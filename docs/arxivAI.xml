<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs</link>

<item>
<title>Trusted Identities for AI Agents: Leveraging Telco-Hosted eSIM Infrastructure</title>
<link>https://arxiv.org/abs/2504.16108</link>
<guid>https://arxiv.org/abs/2504.16108</guid>
<content:encoded><![CDATA[

arXiv:2504.16108v1 Announce Type: new 
Abstract: The rise of autonomous AI agents in enterprise and industrial environments introduces a critical challenge: how to securely assign, verify, and manage their identities across distributed systems. Existing identity frameworks based on API keys, certificates, or application-layer credentials lack the infrastructure-grade trust, lifecycle control, and interoperability needed to manage agents operating independently in sensitive contexts.
  In this paper, we propose a conceptual architecture that leverages telecom-grade eSIM infrastructure, specifically hosted by mobile network operators (MNOs), to serve as a root of trust for AI agents. Rather than embedding SIM credentials in hardware devices, we envision a model where telcos host secure, certified hardware modules (eUICC or HSM) that store and manage agent-specific eSIM profiles. Agents authenticate remotely via cryptographic APIs or identity gateways, enabling scalable and auditable access to enterprise networks and services.
  We explore use cases such as onboarding enterprise automation agents, securing AI-driven financial systems, and enabling trust in inter-agent communications. We identify current limitations in GSMA and 3GPP standards, particularly their device centric assumptions, and propose extensions to support non-physical, software-based agents within trusted execution environments. This paper is intended as a conceptual framework to open discussion around standardization, security architecture, and the role of telecom infrastructure in the evolving agent economy.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOTOPIA-S4: a user-friendly system for flexible, customizable, and large-scale social simulation</title>
<link>https://arxiv.org/abs/2504.16122</link>
<guid>https://arxiv.org/abs/2504.16122</guid>
<content:encoded><![CDATA[

arXiv:2504.16122v1 Announce Type: new 
Abstract: Social simulation through large language model (LLM) agents is a promising approach to explore and validate hypotheses related to social science questions and LLM agents behavior. We present SOTOPIA-S4, a fast, flexible, and scalable social simulation system that addresses the technical barriers of current frameworks while enabling practitioners to generate multi-turn and multi-party LLM-based interactions with customizable evaluation metrics for hypothesis testing. SOTOPIA-S4 comes as a pip package that contains a simulation engine, an API server with flexible RESTful APIs for simulation management, and a web interface that enables both technical and non-technical users to design, run, and analyze simulations without programming. We demonstrate the usefulness of SOTOPIA-S4 with two use cases involving dyadic hiring negotiation and multi-party planning scenarios.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Prompt Wall (I): A Real-World Case Study of Attacking ChatGPT via Lightweight Prompt Injection</title>
<link>https://arxiv.org/abs/2504.16125</link>
<guid>https://arxiv.org/abs/2504.16125</guid>
<content:encoded><![CDATA[

arXiv:2504.16125v1 Announce Type: new 
Abstract: This report presents a real-world case study demonstrating how prompt injection can attack large language model platforms such as ChatGPT according to a proposed injection framework. By providing three real-world examples, we show how adversarial prompts can be injected via user inputs, web-based retrieval, and system-level agent instructions. These attacks, though lightweight and low-cost, can cause persistent and misleading behaviors in LLM outputs. Our case study reveals that even commercial-grade LLMs remain vulnerable to subtle manipulations that bypass safety filters and influence user decisions. \textbf{More importantly, we stress that this report is not intended as an attack guide, but as a technical alert. As ethical researchers, we aim to raise awareness and call upon developers, especially those at OpenAI, to treat prompt-level security as a critical design priority.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARFT: Multi-Agent Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2504.16129</link>
<guid>https://arxiv.org/abs/2504.16129</guid>
<content:encoded><![CDATA[

arXiv:2504.16129v1 Announce Type: new 
Abstract: LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in addressing complex, agentic tasks requiring multifaceted reasoning and collaboration, from generating high-quality presentation slides to conducting sophisticated scientific research. Meanwhile, RL has been widely recognized for its effectiveness in enhancing agent intelligence, but limited research has investigated the fine-tuning of LaMAS using foundational RL techniques. Moreover, the direct application of MARL methodologies to LaMAS introduces significant challenges, stemming from the unique characteristics and mechanisms inherent to LaMAS. To address these challenges, this article presents a comprehensive study of LLM-based MARL and proposes a novel paradigm termed Multi-Agent Reinforcement Fine-Tuning (MARFT). We introduce a universal algorithmic framework tailored for LaMAS, outlining the conceptual foundations, key distinctions, and practical implementation strategies. We begin by reviewing the evolution from RL to Reinforcement Fine-Tuning, setting the stage for a parallel analysis in the multi-agent domain. In the context of LaMAS, we elucidate critical differences between MARL and MARFT. These differences motivate a transition toward a novel, LaMAS-oriented formulation of RFT. Central to this work is the presentation of a robust and scalable MARFT framework. We detail the core algorithm and provide a complete, open-source implementation to facilitate adoption and further research. The latter sections of the paper explore real-world application perspectives and opening challenges in MARFT. By bridging theoretical underpinnings with practical methodologies, this work aims to serve as a roadmap for researchers seeking to advance MARFT toward resilient and adaptive solutions in agentic systems. Our implementation of the proposed framework is publicly available at: https://github.com/jwliao-ai/MARFT.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-Point Sampling for Distributed Bandit Convex Optimization with Time-Varying Constraints</title>
<link>https://arxiv.org/abs/2504.16211</link>
<guid>https://arxiv.org/abs/2504.16211</guid>
<content:encoded><![CDATA[

arXiv:2504.16211v1 Announce Type: new 
Abstract: This paper considers the distributed bandit convex optimization problem with time-varying constraints. In this problem, the global loss function is the average of all the local convex loss functions, which are unknown beforehand. Each agent iteratively makes its own decision subject to time-varying inequality constraints which can be violated but are fulfilled in the long run. For a uniformly jointly strongly connected time-varying directed graph, a distributed bandit online primal-dual projection algorithm with one-point sampling is proposed. We show that sublinear dynamic network regret and network cumulative constraint violation are achieved if the path-length of the benchmark also increases in a sublinear manner. In addition, an $\mathcal{O}({T^{3/4 + g}})$ static network regret bound and an $\mathcal{O}( {{T^{1 - {g}/2}}} )$ network cumulative constraint violation bound are established, where $T$ is the total number of iterations and $g \in ( {0,1/4} )$ is a trade-off parameter. Moreover, a reduced static network regret bound $\mathcal{O}( {{T^{2/3 + 4g /3}}} )$ is established for strongly convex local loss functions. Finally, a numerical example is presented to validate the theoretical results.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nash Equilibrium Learning In Large Populations With First Order Payoff Modifications</title>
<link>https://arxiv.org/abs/2504.16222</link>
<guid>https://arxiv.org/abs/2504.16222</guid>
<content:encoded><![CDATA[

arXiv:2504.16222v1 Announce Type: new 
Abstract: We establish Nash equilibrium learning -- convergence of the population state to a suitably defined Nash equilibria set -- for a class of payoff dynamical mechanism with a first order modification. The first order payoff modification can model aspects of the agents' bounded rationality, anticipatory or averaging terms in the payoff mechanism, or first order Pad\'e approximations of delays. To obtain our main results, we apply a combination of two nonstandard system-theoretic passivity notions.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Schelling segregation dynamics in densely-connected social network graphs</title>
<link>https://arxiv.org/abs/2504.16307</link>
<guid>https://arxiv.org/abs/2504.16307</guid>
<content:encoded><![CDATA[

arXiv:2504.16307v1 Announce Type: new 
Abstract: Schelling segregation is a well-established model used to investigate the dynamics of segregation in agent-based models. Since we consider segregation to be key for the development of political polarisation, we are interested in what insights it could give for this problem. We tested basic questions of segregation on an agent-based social network model where agents' connections were not restricted by their spatial position, and made the network graph much denser than previous tests of Schelling segregation in social networks.
  We found that a dense social network does not become as strongly segregated as a sparse network, and that agents' numbers of same-group neighbours do not greatly exceed their desired numbers (i.e. they do not end up more segregated than they desire to be). Furthermore, we found that the network was very difficult to polarise when one group was somewhat smaller than the other, and that it became unstable when one group was extremely small, both of which provide insights into real-world polarisation dynamics. Finally, we tested the question of whether an increase in the minority group's desire for same-group neighbours created more segregation than a similar increase for the majority group -- the "paradox of weak minority preferences" -- and found mixed evidence for this effect in a densely connected social network.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClarifyCoder: Clarification-Aware Fine-Tuning for Programmatic Problem Solving</title>
<link>https://arxiv.org/abs/2504.16331</link>
<guid>https://arxiv.org/abs/2504.16331</guid>
<content:encoded><![CDATA[

arXiv:2504.16331v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, a significant gap remains between their current performance and that of expert software engineers. A key differentiator is that human engineers actively seek clarification when faced with ambiguous requirements, while LLMs typically generate code regardless of uncertainties in the problem description. We present ClarifyCoder, a novel framework with synthetic data generation and instruction-tuning that enables LLMs to identify ambiguities and request clarification before proceeding with code generation. While recent work has focused on LLM-based agents for iterative code generation, we argue that the fundamental ability to recognize and query ambiguous requirements should be intrinsic to the models themselves. Our approach consists of two main components: (1) a data synthesis technique that augments existing programming datasets with scenarios requiring clarification to generate clarification-aware training data, and (2) a fine-tuning strategy that teaches models to prioritize seeking clarification over immediate code generation when faced with incomplete or ambiguous requirements. We further provide an empirical analysis of integrating ClarifyCoder with standard fine-tuning for a joint optimization of both clarify-awareness and coding ability. Experimental results demonstrate that ClarifyCoder significantly improves the communication capabilities of Code LLMs through meaningful clarification dialogues while maintaining code generation capabilities.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation</title>
<link>https://arxiv.org/abs/2504.16408</link>
<guid>https://arxiv.org/abs/2504.16408</guid>
<content:encoded><![CDATA[

arXiv:2504.16408v1 Announce Type: new 
Abstract: The XLLM@ACL2025 Shared Task-III formulates a low-resource structural reasoning task that challenges LLMs to generate interpretable, step-by-step rationales with minimal labeled data. We present Less is More, the third-place winning approach in the XLLM@ACL2025 Shared Task-III, which focuses on structured reasoning from only 24 labeled examples. Our approach leverages a multi-agent framework with reverse-prompt induction, retrieval-augmented reasoning synthesis via GPT-4o, and dual-stage reward-guided filtering to distill high-quality supervision across three subtasks: question parsing, CoT parsing, and step-level verification. All modules are fine-tuned from Meta-Llama-3-8B-Instruct under a unified LoRA+ setup. By combining structure validation with reward filtering across few-shot and zero-shot prompts, our pipeline consistently improves structure reasoning quality. These results underscore the value of controllable data distillation in enhancing structured inference under low-resource constraints. Our code is available at https://github.com/Jiahao-Yuan/Less-is-More.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FeedQUAC: Quick Unobtrusive AI-Generated Commentary</title>
<link>https://arxiv.org/abs/2504.16416</link>
<guid>https://arxiv.org/abs/2504.16416</guid>
<content:encoded><![CDATA[

arXiv:2504.16416v1 Announce Type: new 
Abstract: Design thrives on feedback. However, gathering constant feedback throughout the design process can be labor-intensive and disruptive. We explore how AI can bridge this gap by providing effortless, ambient feedback. We introduce FeedQUAC, a design companion that delivers real-time AI-generated commentary from a variety of perspectives through different personas. A design probe study with eight participants highlights how designers can leverage quick yet ambient AI feedback to enhance their creative workflows. Participants highlight benefits such as convenience, playfulness, confidence boost, and inspiration from this lightweight feedback agent, while suggesting additional features, like chat interaction and context curation. We discuss the role of AI feedback, its strengths and limitations, and how to integrate it into existing design workflows while balancing user involvement. Our findings also suggest that ambient interaction is a valuable consideration for both the design and evaluation of future creativity support systems.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Foundation Model-Powered Recommender Systems: From Feature-Based, Generative to Agentic Paradigms</title>
<link>https://arxiv.org/abs/2504.16420</link>
<guid>https://arxiv.org/abs/2504.16420</guid>
<content:encoded><![CDATA[

arXiv:2504.16420v1 Announce Type: new 
Abstract: Recommender systems (RS) have become essential in filtering information and personalizing content for users. RS techniques have traditionally relied on modeling interactions between users and items as well as the features of content using models specific to each task. The emergence of foundation models (FMs), large scale models trained on vast amounts of data such as GPT, LLaMA and CLIP, is reshaping the recommendation paradigm. This survey provides a comprehensive overview of the Foundation Models for Recommender Systems (FM4RecSys), covering their integration in three paradigms: (1) Feature-Based augmentation of representations, (2) Generative recommendation approaches, and (3) Agentic interactive systems. We first review the data foundations of RS, from traditional explicit or implicit feedback to multimodal content sources. We then introduce FMs and their capabilities for representation learning, natural language understanding, and multi-modal reasoning in RS contexts. The core of the survey discusses how FMs enhance RS under different paradigms. Afterward, we examine FM applications in various recommendation tasks. Through an analysis of recent research, we highlight key opportunities that have been realized as well as challenges encountered. Finally, we outline open research directions and technical challenges for next-generation FM4RecSys. This survey not only reviews the state-of-the-art methods but also provides a critical analysis of the trade-offs among the feature-based, the generative, and the agentic paradigms, outlining key open issues and future research directions.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate</title>
<link>https://arxiv.org/abs/2504.16489</link>
<guid>https://arxiv.org/abs/2504.16489</guid>
<content:encoded><![CDATA[

arXiv:2504.16489v1 Announce Type: new 
Abstract: Multi-Agent Debate (MAD), leveraging collaborative interactions among Large Language Models (LLMs), aim to enhance reasoning capabilities in complex tasks. However, the security implications of their iterative dialogues and role-playing characteristics, particularly susceptibility to jailbreak attacks eliciting harmful content, remain critically underexplored. This paper systematically investigates the jailbreak vulnerabilities of four prominent MAD frameworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo, and DeepSeek) without compromising internal agents. We introduce a novel structured prompt-rewriting framework specifically designed to exploit MAD dynamics via narrative encapsulation, role-driven escalation, iterative refinement, and rhetorical obfuscation. Our extensive experiments demonstrate that MAD systems are inherently more vulnerable than single-agent setups. Crucially, our proposed attack methodology significantly amplifies this fragility, increasing average harmfulness from 28.14% to 80.34% and achieving attack success rates as high as 80% in certain scenarios. These findings reveal intrinsic vulnerabilities in MAD architectures and underscore the urgent need for robust, specialized defenses prior to real-world deployment.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion and Reasoning for Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2504.16516</link>
<guid>https://arxiv.org/abs/2504.16516</guid>
<content:encoded><![CDATA[

arXiv:2504.16516v1 Announce Type: new 
Abstract: Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow natural language instructions and reach target locations in real-world environments. While prior methods often rely on either global scene representations or object-level features, these approaches are insufficient for capturing the complex interactions across modalities required for accurate navigation. In this paper, we propose a Multi-level Fusion and Reasoning Architecture (MFRA) to enhance the agent's ability to reason over visual observations, language instructions and navigation history. Specifically, MFRA introduces a hierarchical fusion mechanism that aggregates multi-level features-ranging from low-level visual cues to high-level semantic concepts-across multiple modalities. We further design a reasoning module that leverages fused representations to infer navigation actions through instruction-guided attention and dynamic context integration. By selectively capturing and combining relevant visual, linguistic, and temporal signals, MFRA improves decision-making accuracy in complex navigation scenarios. Extensive experiments on benchmark VLN datasets including REVERIE, R2R, and SOON demonstrate that MFRA achieves superior performance compared to state-of-the-art methods, validating the effectiveness of multi-level modal fusion for embodied navigation.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Irrationality Shapes Nash Equilibria: A Prospect-Theoretic Perspective</title>
<link>https://arxiv.org/abs/2504.16556</link>
<guid>https://arxiv.org/abs/2504.16556</guid>
<content:encoded><![CDATA[

arXiv:2504.16556v1 Announce Type: new 
Abstract: Noncooperative games with uncertain payoffs have been classically studied under the expected-utility theory framework, which relies on the strong assumption that agents behave rationally. However, simple experiments on human decision makers found them to be not fully rational, due to their subjective risk perception. Prospect theory was proposed as an empirically-grounded model to incorporate irrational behaviours into game-theoretic models. But, how prospect theory shapes the set of Nash equilibria when considering irrational agents, is still poorly understood. To this end, we study how prospect theoretic transformations may generate new equilibria while eliminating existing ones. Focusing on aggregative games, we show that capturing users' irrationality can preserve symmetric equilibria while causing the vanishing of asymmetric equilibria. Further, there exist value functions which map uncountable sets of equilibria in the expected-utility maximization framework to finite sets. This last result may shape some equilibrium selection theories for human-in-the-loop systems where computing a single equilibrium is insufficient and comparison of equilibria is needed.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM-Based Agents via Global Planning and Hierarchical Execution</title>
<link>https://arxiv.org/abs/2504.16563</link>
<guid>https://arxiv.org/abs/2504.16563</guid>
<content:encoded><![CDATA[

arXiv:2504.16563v1 Announce Type: new 
Abstract: Intelligent agent systems based on Large Language Models (LLMs) have shown great potential in real-world applications. However, existing agent frameworks still face critical limitations in task planning and execution, restricting their effectiveness and generalizability. Specifically, current planning methods often lack clear global goals, leading agents to get stuck in local branches, or produce non-executable plans. Meanwhile, existing execution mechanisms struggle to balance complexity and stability, and their limited action space restricts their ability to handle diverse real-world tasks. To address these limitations, we propose GoalAct, a novel agent framework that introduces a continuously updated global planning mechanism and integrates a hierarchical execution strategy. GoalAct decomposes task execution into high-level skills, including searching, coding, writing and more, thereby reducing planning complexity while enhancing the agents' adaptability across diverse task scenarios. We evaluate GoalAct on LegalAgentBench, a benchmark with multiple types of legal tasks that require the use of multiple types of tools. Experimental results demonstrate that GoalAct achieves state-of-the-art (SOTA) performance, with an average improvement of 12.22% in success rate. These findings highlight GoalAct's potential to drive the development of more advanced intelligent agent systems, making them more effective across complex real-world applications. Our code can be found at https://github.com/cjj826/GoalAct.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Pricing and Algorithmic Collusion</title>
<link>https://arxiv.org/abs/2504.16592</link>
<guid>https://arxiv.org/abs/2504.16592</guid>
<content:encoded><![CDATA[

arXiv:2504.16592v1 Announce Type: new 
Abstract: The rise of algorithmic pricing in online retail platforms has attracted significant interest in how autonomous software agents interact under competition. This article explores the potential emergence of algorithmic collusion - supra-competitive pricing outcomes that arise without explicit agreements - as a consequence of repeated interactions between learning agents. Most of the literature focuses on oligopoly pricing environments modeled as repeated Bertrand competitions, where firms use online learning algorithms to adapt prices over time. While experimental research has demonstrated that specific reinforcement learning algorithms can learn to maintain prices above competitive equilibrium levels in simulated environments, theoretical understanding of when and why such outcomes occur remains limited. This work highlights the interdisciplinary nature of this challenge, which connects computer science concepts of online learning with game-theoretical literature on equilibrium learning. We examine implications for the Business & Information Systems Engineering (BISE) community and identify specific research opportunities to address challenges of algorithmic competition in digital marketplaces.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery</title>
<link>https://arxiv.org/abs/2504.16728</link>
<guid>https://arxiv.org/abs/2504.16728</guid>
<content:encoded><![CDATA[

arXiv:2504.16728v1 Announce Type: new 
Abstract: The rapid advancement in capabilities of large language models (LLMs) raises a pivotal question: How can LLMs accelerate scientific discovery? This work tackles the crucial first stage of research, generating novel hypotheses. While recent work on automated hypothesis generation focuses on multi-agent frameworks and extending test-time compute, none of the approaches effectively incorporate transparency and steerability through a synergistic Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS: Interactive Research Ideation System, an open-source platform designed for researchers to leverage LLM-assisted scientific ideation. IRIS incorporates innovative features to enhance ideation, including adaptive test-time compute expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism, and query-based literature synthesis. Designed to empower researchers with greater control and insight throughout the ideation process. We additionally conduct a user study with researchers across diverse disciplines, validating the effectiveness of our system in enhancing ideation. We open-source our code at https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DYNUS: Uncertainty-aware Trajectory Planner in Dynamic Unknown Environments</title>
<link>https://arxiv.org/abs/2504.16734</link>
<guid>https://arxiv.org/abs/2504.16734</guid>
<content:encoded><![CDATA[

arXiv:2504.16734v1 Announce Type: new 
Abstract: This paper introduces DYNUS, an uncertainty-aware trajectory planner designed for dynamic unknown environments. Operating in such settings presents many challenges -- most notably, because the agent cannot predict the ground-truth future paths of obstacles, a previously planned trajectory can become unsafe at any moment, requiring rapid replanning to avoid collisions.
  Recently developed planners have used soft-constraint approaches to achieve the necessary fast computation times; however, these methods do not guarantee collision-free paths even with static obstacles. In contrast, hard-constraint methods ensure collision-free safety, but typically have longer computation times.
  To address these issues, we propose three key contributions. First, the DYNUS Global Planner (DGP) and Temporal Safe Corridor Generation operate in spatio-temporal space and handle both static and dynamic obstacles in the 3D environment. Second, the Safe Planning Framework leverages a combination of exploratory, safe, and contingency trajectories to flexibly re-route when potential future collisions with dynamic obstacles are detected. Finally, the Fast Hard-Constraint Local Trajectory Formulation uses a variable elimination approach to reduce the problem size and enable faster computation by pre-computing dependencies between free and dependent variables while still ensuring collision-free trajectories.
  We evaluated DYNUS in a variety of simulations, including dense forests, confined office spaces, cave systems, and dynamic environments. Our experiments show that DYNUS achieves a success rate of 100% and travel times that are approximately 25.0% faster than state-of-the-art methods. We also evaluated DYNUS on multiple platforms -- a quadrotor, a wheeled robot, and a quadruped -- in both simulation and hardware experiments.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of AI Agent Protocols</title>
<link>https://arxiv.org/abs/2504.16736</link>
<guid>https://arxiv.org/abs/2504.16736</guid>
<content:encoded><![CDATA[

arXiv:2504.16736v1 Announce Type: new 
Abstract: The rapid development of large language models (LLMs) has led to the widespread deployment of LLM agents across diverse industries, including customer service, content generation, data analysis, and even healthcare. However, as more LLM agents are deployed, a major issue has emerged: there is no standard way for these agents to communicate with external tools or data sources. This lack of standardized protocols makes it difficult for agents to work together or scale effectively, and it limits their ability to tackle complex, real-world tasks. A unified communication protocol for LLM agents could change this. It would allow agents and tools to interact more smoothly, encourage collaboration, and triggering the formation of collective intelligence. In this paper, we provide a systematic overview of existing communication protocols for LLM agents. We classify them into four main categories and make an analysis to help users and developers select the most suitable protocols for specific applications. Additionally, we conduct a comparative performance analysis of these protocols across key dimensions such as security, scalability, and latency. Finally, we explore future challenges, such as how protocols can adapt and survive in fast-evolving environments, and what qualities future protocols might need to support the next generation of LLM agent ecosystems. We expect this work to serve as a practical reference for both researchers and engineers seeking to design, evaluate, or integrate robust communication infrastructures for intelligent agents.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair division of the replacement-units without an appraiser in urban renewal processes</title>
<link>https://arxiv.org/abs/2504.16852</link>
<guid>https://arxiv.org/abs/2504.16852</guid>
<content:encoded><![CDATA[

arXiv:2504.16852v1 Announce Type: new 
Abstract: Rebuild and Divide is an urban renewal process that involves the demolition of old buildings and the construction of new ones. Original homeowners are compensated with upgraded apartments, while surplus units are sold for profit, so theoretically it is a win-win project for all parties involved. However, many rebuild-and-divide projects withheld or delayed due to disagreements over the assignment of new units, claiming they are not "fair". The goal of this research is to develop algorithms for envy-free allocation of the new units. The main challenge is that, in contrast to previous work on envy-free allocation, the envy depends also on the value of the old units, as people with more valuable old units are entitled to more valuable new units. We introduce three models that capture different notions of fairness: (1) the Difference Model, where agents evaluate their gains relative to others; (2) the Envy Sum Model, which permits some envy as long as the total envy does not exceed that of the original allocation; and (3) the Ratio Model, where fairness is assessed based on the proportional value of old apartments. For each model, we establish an envy criterion and seek a payment vector and allocation that ensure envy-freeness. These models present both theoretical challenges and intriguing insights. Additionally, within the Envy Sum Model, we present a mechanism that computes an allocation and payment scheme that minimizes total envy. We also analyze the mechanism's vulnerability to manipulation and identify conditions under which it is obviously manipulable.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monte Carlo Planning with Large Language Model for Text-Based Game Agents</title>
<link>https://arxiv.org/abs/2504.16855</link>
<guid>https://arxiv.org/abs/2504.16855</guid>
<content:encoded><![CDATA[

arXiv:2504.16855v1 Announce Type: new 
Abstract: Text-based games provide valuable environments for language-based autonomous agents. However, planning-then-learning paradigms, such as those combining Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably time-consuming due to extensive iterations. Additionally, these algorithms perform uncertainty-driven exploration but lack language understanding and reasoning abilities. In this paper, we introduce the Monte Carlo planning with Dynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages the language understanding and reasoning capabilities of Large Language Models (LLMs) alongside the exploratory advantages of tree search algorithms. Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms, enabling them to learn from past experiences and dynamically adjust action evaluations during planning. We conduct experiments on a series of text-based games from the Jericho benchmark. Our results demonstrate that the MC-DML algorithm significantly enhances performance across various games at the initial planning phase, outperforming strong contemporary methods that require multiple iterations. This demonstrates the effectiveness of our algorithm, paving the way for more efficient language-grounded planning in complex environments.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Reinforcement Learning and Model Predictive Control for Adaptive Control of Hydrogen-Diesel Dual-Fuel Combustion</title>
<link>https://arxiv.org/abs/2504.16875</link>
<guid>https://arxiv.org/abs/2504.16875</guid>
<content:encoded><![CDATA[

arXiv:2504.16875v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) and Machine Learning Integrated Model Predictive Control (ML-MPC) are promising approaches for optimizing hydrogen-diesel dual-fuel engine control, as they can effectively control multiple-input multiple-output systems and nonlinear processes. ML-MPC is advantageous for providing safe and optimal controls, ensuring the engine operates within predefined safety limits. In contrast, RL is distinguished by its adaptability to changing conditions through its learning-based approach. However, the practical implementation of either method alone poses challenges. RL requires high variance in control inputs during early learning phases, which can pose risks to the system by potentially executing unsafe actions, leading to mechanical damage. Conversely, ML-MPC relies on an accurate system model to generate optimal control inputs and has limited adaptability to system drifts, such as injector aging, which naturally occur in engine applications. To address these limitations, this study proposes a hybrid RL and ML-MPC approach that uses an ML-MPC framework while incorporating an RL agent to dynamically adjust the ML-MPC load tracking reference in response to changes in the environment. At the same time, the ML-MPC ensures that actions stay safe throughout the RL agent's exploration. To evaluate the effectiveness of this approach, fuel pressure is deliberately varied to introduce a model-plant mismatch between the ML-MPC and the engine test bench. The result of this mismatch is a root mean square error (RMSE) in indicated mean effective pressure of 0.57 bar when running the ML-MPC. The experimental results demonstrate that RL successfully adapts to changing boundary conditions by altering the tracking reference while ML-MPC ensures safe control inputs. The quantitative improvement in load tracking by implementing RL is an RSME of 0.44 bar.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Large Language Models know who did what to whom?</title>
<link>https://arxiv.org/abs/2504.16884</link>
<guid>https://arxiv.org/abs/2504.16884</guid>
<content:encoded><![CDATA[

arXiv:2504.16884v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are commonly criticized for not understanding language. However, many critiques focus on cognitive abilities that, in humans, are distinct from language processing. Here, we instead study a kind of understanding tightly linked to language: inferring who did what to whom (thematic roles) in a sentence. Does the central training objective of LLMs-word prediction-result in sentence representations that capture thematic roles? In two experiments, we characterized sentence representations in four LLMs. In contrast to human similarity judgments, in LLMs the overall representational similarity of sentence pairs reflected syntactic similarity but not whether their agent and patient assignments were identical vs. reversed. Furthermore, we found little evidence that thematic role information was available in any subset of hidden units. However, some attention heads robustly captured thematic roles, independently of syntax. Therefore, LLMs can extract thematic roles but, relative to humans, this information influences their representations more weakly.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building A Secure Agentic AI Application Leveraging A2A Protocol</title>
<link>https://arxiv.org/abs/2504.16902</link>
<guid>https://arxiv.org/abs/2504.16902</guid>
<content:encoded><![CDATA[

arXiv:2504.16902v1 Announce Type: new 
Abstract: As Agentic AI systems evolve from basic workflows to complex multi agent collaboration, robust protocols such as Google's Agent2Agent (A2A) become essential enablers. To foster secure adoption and ensure the reliability of these complex interactions, understanding the secure implementation of A2A is essential. This paper addresses this goal by providing a comprehensive security analysis centered on the A2A protocol. We examine its fundamental elements and operational dynamics, situating it within the framework of agent communication development. Utilizing the MAESTRO framework, specifically designed for AI risks, we apply proactive threat modeling to assess potential security issues in A2A deployments, focusing on aspects such as Agent Card management, task execution integrity, and authentication methodologies.
  Based on these insights, we recommend practical secure development methodologies and architectural best practices designed to build resilient and effective A2A systems. Our analysis also explores how the synergy between A2A and the Model Context Protocol (MCP) can further enhance secure interoperability. This paper equips developers and architects with the knowledge and practical guidance needed to confidently leverage the A2A protocol for building robust and secure next generation agentic applications.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents</title>
<link>https://arxiv.org/abs/2504.16918</link>
<guid>https://arxiv.org/abs/2504.16918</guid>
<content:encoded><![CDATA[

arXiv:2504.16918v1 Announce Type: new 
Abstract: Optimization plays a vital role in scientific research and practical applications, but formulating a concrete optimization problem described in natural language into a mathematical form and selecting a suitable solver to solve the problem requires substantial domain expertise. We introduce \textbf{OptimAI}, a framework for solving \underline{Optim}ization problems described in natural language by leveraging LLM-powered \underline{AI} agents, achieving superior performance over current state-of-the-art methods. Our framework is built upon four key roles: (1) a \emph{formulator} that translates natural language problem descriptions into precise mathematical formulations; (2) a \emph{planner} that constructs a high-level solution strategy prior to execution; and (3) a \emph{coder} and a \emph{code critic} capable of interacting with the environment and reflecting on outcomes to refine future actions. Ablation studies confirm that all roles are essential; removing the planner or code critic results in $5.8\times$ and $3.1\times$ drops in productivity, respectively. Furthermore, we introduce UCB-based debug scheduling to dynamically switch between alternative plans, yielding an additional $3.3\times$ productivity gain. Our design emphasizes multi-agent collaboration, allowing us to conveniently explore the synergistic effect of combining diverse models within a unified system. Our approach attains 88.1\% accuracy on the NLP4LP dataset and 71.2\% on the Optibench (non-linear w/o table) subset, reducing error rates by 58\% and 50\% respectively over prior best results.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Portfolio Selection through Preference Aggregation with Quicksort and the Bradley--Terry Model</title>
<link>https://arxiv.org/abs/2504.16093</link>
<guid>https://arxiv.org/abs/2504.16093</guid>
<content:encoded><![CDATA[

arXiv:2504.16093v1 Announce Type: cross 
Abstract: How to allocate limited resources to projects that will yield the greatest long-term benefits is a problem that often arises in decision-making under uncertainty. For example, organizations may need to evaluate and select innovation projects with risky returns. Similarly, when allocating resources to research projects, funding agencies are tasked with identifying the most promising proposals based on idiosyncratic criteria. Finally, in participatory budgeting, a local community may need to select a subset of public projects to fund. Regardless of context, agents must estimate the uncertain values of a potentially large number of projects. Developing parsimonious methods to compare these projects, and aggregating agent evaluations so that the overall benefit is maximized, are critical in assembling the best project portfolio. Unlike in standard sorting algorithms, evaluating projects on the basis of uncertain long-term benefits introduces additional complexities. We propose comparison rules based on Quicksort and the Bradley--Terry model, which connects rankings to pairwise "win" probabilities. In our model, each agent determines win probabilities of a pair of projects based on his or her specific evaluation of the projects' long-term benefit. The win probabilities are then appropriately aggregated and used to rank projects. Several of the methods we propose perform better than the two most effective aggregation methods currently available. Additionally, our methods can be combined with sampling techniques to significantly reduce the number of pairwise comparisons. We also discuss how the Bradley--Terry portfolio selection approach can be implemented in practice.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Safety-Privacy Tradeoff in Linear Bandits</title>
<link>https://arxiv.org/abs/2504.16371</link>
<guid>https://arxiv.org/abs/2504.16371</guid>
<content:encoded><![CDATA[

arXiv:2504.16371v1 Announce Type: cross 
Abstract: We consider a collection of linear stochastic bandit problems, each modeling the random response of different agents to proposed interventions, coupled together by a global safety constraint. We assume a central coordinator must choose actions to play on each bandit with the objective of regret minimization, while also ensuring that the expected response of all agents satisfies the global safety constraints at each round, in spite of uncertainty about the bandits' parameters. The agents consider their observed responses to be private and in order to protect their sensitive information, the data sharing with the central coordinator is performed under local differential privacy (LDP). However, providing higher level of privacy to different agents would have consequences in terms of safety and regret. We formalize these tradeoffs by building on the notion of the sharpness of the safety set - a measure of how the geometric properties of the safe set affects the growth of regret - and propose a unilaterally unimprovable vector of privacy levels for different agents given a maximum regret budget.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Regret Benchmarks in Online Non-Stochastic Control</title>
<link>https://arxiv.org/abs/2504.16581</link>
<guid>https://arxiv.org/abs/2504.16581</guid>
<content:encoded><![CDATA[

arXiv:2504.16581v1 Announce Type: cross 
Abstract: In the online non-stochastic control problem, an agent sequentially selects control inputs for a linear dynamical system when facing unknown and adversarially selected convex costs and disturbances. A common metric for evaluating control policies in this setting is policy regret, defined relative to the best-in-hindsight linear feedback controller. However, for general convex costs, this benchmark may be less meaningful since linear controllers can be highly suboptimal. To address this, we introduce an alternative, more suitable benchmark--the performance of the best fixed input. We show that this benchmark can be viewed as a natural extension of the standard benchmark used in online convex optimization and propose a novel online control algorithm that achieves sublinear regret with respect to this new benchmark. We also discuss the connections between our method and the original one proposed by Agarwal et al. in their seminal work introducing the online non-stochastic control problem, and compare the performance of both approaches through numerical simulations.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning With LLMs Interaction For Distributed Diffusion Model Services</title>
<link>https://arxiv.org/abs/2311.11094</link>
<guid>https://arxiv.org/abs/2311.11094</guid>
<content:encoded><![CDATA[

arXiv:2311.11094v2 Announce Type: replace 
Abstract: Distributed Artificial Intelligence-Generated Content (AIGC) has attracted significant attention, but two key challenges remain: maximizing subjective Quality of Experience (QoE) and improving energy efficiency, which are particularly pronounced in widely adopted Generative Diffusion Model (GDM)-based image generation services. In this paper, we propose a novel user-centric Interactive AI (IAI) approach for service management, with a distributed GDM-based AIGC framework that emphasizes efficient and cooperative deployment. The proposed method restructures the GDM inference process by allowing users with semantically similar prompts to share parts of the denoising chain. Furthermore, to maximize the users' subjective QoE, we propose an IAI approach, i.e., Reinforcement Learning With Large Language Models Interaction (RLLI), which utilizes Large Language Model (LLM)-empowered generative agents to replicate user interaction, providing real-time and subjective QoE feedback aligned with diverse user personalities. Lastly, we present the GDM-based Deep Deterministic Policy Gradient (GDDPG) algorithm, adapted to the proposed RLLI framework, to allocate communication and computing resources effectively while accounting for subjective user traits and dynamic wireless conditions. Simulation results demonstrate that G-DDPG improves total QoE by 15% compared with the standard DDPG algorithm.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatDBG: Augmenting Debugging with Large Language Models</title>
<link>https://arxiv.org/abs/2403.16354</link>
<guid>https://arxiv.org/abs/2403.16354</guid>
<content:encoded><![CDATA[

arXiv:2403.16354v4 Announce Type: replace 
Abstract: Debugging is a critical but challenging task for programmers. This paper proposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like "why is x null?". To handle these queries, ChatDBG grants the LLM autonomy to "take the wheel": it can act as an independent agent capable of querying and controlling the debugger to navigate through stacks and inspect program state. It then reports its findings and yields back control to the programmer. By leveraging the real-world knowledge embedded in LLMs, ChatDBG can diagnose issues identifiable only through the use of domain-specific reasoning. Our ChatDBG prototype integrates with standard debuggers including LLDB and GDB for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code including standalone scripts and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root causes, explain bugs, and generate accurate fixes for a wide range of real-world errors. For the Python programs, a single query led to an actionable bug fix 67% of the time; one additional follow-up query increased the success rate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded more than 75,000 times.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Sentinel: LLM Agent for Adversarial Purification</title>
<link>https://arxiv.org/abs/2405.20770</link>
<guid>https://arxiv.org/abs/2405.20770</guid>
<content:encoded><![CDATA[

arXiv:2405.20770v4 Announce Type: replace 
Abstract: Over the past two years, the use of large language models (LLMs) has advanced rapidly. While these LLMs offer considerable convenience, they also raise security concerns, as LLMs are vulnerable to adversarial attacks by some well-designed textual perturbations. In this paper, we introduce a novel defense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is designed to enhance the adversarial robustness of LLMs by purifying the adversarial textual examples before feeding them into the target LLM. Our method comprises two main components: a) Agent instruction, which can simulate a new agent for adversarial defense, altering minimal characters to maintain the original meaning of the sentence while defending against attacks; b) Defense guidance, which provides strategies for modifying clean or adversarial examples to ensure effective defense and accurate outputs from the target LLMs. Remarkably, the defense agent demonstrates robust defensive capabilities even without learning from adversarial examples. Additionally, we conduct an intriguing adversarial experiment where we develop two agents, one for defense and one for attack, and engage them in mutual confrontation. During the adversarial interactions, neither agent completely beat the other. Extensive experiments on both open-source and closed-source LLMs demonstrate that our method effectively defends against adversarial attacks, thereby enhancing adversarial robustness.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pessimism Traps and Algorithmic Interventions</title>
<link>https://arxiv.org/abs/2406.04462</link>
<guid>https://arxiv.org/abs/2406.04462</guid>
<content:encoded><![CDATA[

arXiv:2406.04462v3 Announce Type: replace 
Abstract: In this paper, we relate the philosophical literature on pessimism traps to information cascades, a formal model derived from the economics and mathematics literature. A pessimism trap is a social pattern in which individuals in a community, in situations of uncertainty, begin to copy the sub-optimal actions of others, despite their individual beliefs. This maps nicely onto the concept of an information cascade, which involves a sequence of agents making a decision between two alternatives, with a private signal of the superior alternative and a public history of others' actions. Key results from the economics literature show that information cascades occur with probability one in many contexts, and depending on the strength of the signal, populations can fall into the incorrect cascade very easily and quickly. Once formed, in the absence of external perturbation, a cascade cannot be broken -- therefore, we derive an intervention that can be used to nudge a population from an incorrect to a correct cascade and, importantly, maintain the cascade once the subsidy is discontinued. We study this both theoretically and empirically.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pushing the Frontier on Approximate EFX Allocations</title>
<link>https://arxiv.org/abs/2406.12413</link>
<guid>https://arxiv.org/abs/2406.12413</guid>
<content:encoded><![CDATA[

arXiv:2406.12413v2 Announce Type: replace 
Abstract: We study the problem of allocating a set of indivisible goods to a set of agents with additive valuation functions, aiming to achieve approximate envy-freeness up to any good ($\alpha$-EFX). The state-of-the-art results on the problem include that (exact) EFX allocations exist when (a) there are at most three agents, or (b) the agents' valuation functions can take at most two values, or (c) the agents' valuation functions can be represented via a graph. For $\alpha$-EFX, it is known that a $0.618$-EFX allocation exists for any number of agents with additive valuation functions. In this paper, we show that $2/3$-EFX allocations exist when (a) there are at most \emph{seven agents}, (b) the agents' valuation functions can take at most \emph{three values}, or (c) the agents' valuation functions can be represented via a \emph{multigraph}. Our results can be interpreted in two ways. First, by relaxing the notion of EFX to $2/3$-EFX, we obtain existence results for strict generalizations of the settings for which exact EFX allocations are known to exist. Secondly, by imposing restrictions on the setting, we manage to beat the barrier of $0.618$ and achieve an approximation guarantee of $2/3$. Therefore, our results push the \emph{frontier} of existence and computation of approximate EFX allocations, and provide insights into the challenges of settling the existence of exact EFX allocations.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Situational Safety</title>
<link>https://arxiv.org/abs/2410.06172</link>
<guid>https://arxiv.org/abs/2410.06172</guid>
<content:encoded><![CDATA[

arXiv:2410.06172v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) are rapidly evolving, demonstrating impressive capabilities as multimodal assistants that interact with both humans and their environments. However, this increased sophistication introduces significant safety concerns. In this paper, we present the first evaluation and analysis of a novel safety challenge termed Multimodal Situational Safety, which explores how safety considerations vary based on the specific situation in which the user or agent is engaged. We argue that for an MLLM to respond safely, whether through language or action, it often needs to assess the safety implications of a language query within its corresponding visual context. To evaluate this capability, we develop the Multimodal Situational Safety benchmark (MSSBench) to assess the situational safety performance of current MLLMs. The dataset comprises 1,820 language query-image pairs, half of which the image context is safe, and the other half is unsafe. We also develop an evaluation framework that analyzes key safety aspects, including explicit safety reasoning, visual understanding, and, crucially, situational safety reasoning. Our findings reveal that current MLLMs struggle with this nuanced safety problem in the instruction-following setting and struggle to tackle these situational safety challenges all at once, highlighting a key area for future research. Furthermore, we develop multi-agent pipelines to coordinately solve safety challenges, which shows consistent improvement in safety over the original MLLM response. Code and data: mssbench.github.io.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SNN-Based Online Learning of Concepts and Action Laws in an Open World</title>
<link>https://arxiv.org/abs/2411.12308</link>
<guid>https://arxiv.org/abs/2411.12308</guid>
<content:encoded><![CDATA[

arXiv:2411.12308v3 Announce Type: replace 
Abstract: We present the architecture of a fully autonomous, bio-inspired cognitive agent built around a spiking neural network (SNN) implementing the agent's semantic memory. This agent explores its universe and learns concepts of objects/situations and of its own actions in a one-shot manner. While object/situation concepts are unary, action concepts are triples made up of an initial situation, a motor activity, and an outcome. They embody the agent's knowledge of its universe's action laws. Both kinds of concepts have different degrees of generality. To make decisions the agent queries its semantic memory for the expected outcomes of envisaged actions and chooses the action to take on the basis of these predictions. Our experiments show that the agent handles new situations by appealing to previously learned general concepts and rapidly modifies its concepts to adapt to environment changes.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>{\alpha}-RACER: Real-Time Algorithm for Game-Theoretic Motion Planning and Control in Autonomous Racing using Near-Potential Function</title>
<link>https://arxiv.org/abs/2412.08855</link>
<guid>https://arxiv.org/abs/2412.08855</guid>
<content:encoded><![CDATA[

arXiv:2412.08855v2 Announce Type: replace 
Abstract: Autonomous racing extends beyond the challenge of controlling a racecar at its physical limits. Professional racers employ strategic maneuvers to outwit other competing opponents to secure victory. While modern control algorithms can achieve human-level performance by computing offline racing lines for single-car scenarios, research on real-time algorithms for multi-car autonomous racing is limited. To bridge this gap, we develop game-theoretic modeling framework that incorporates the competitive aspect of autonomous racing like overtaking and blocking through a novel policy parametrization, while operating the car at its limit. Furthermore, we propose an algorithmic approach to compute the (approximate) Nash equilibrium strategy, which represents the optimal approach in the presence of competing agents. Specifically, we introduce an algorithm inspired by recently introduced framework of dynamic near-potential function, enabling real-time computation of the Nash equilibrium. Our approach comprises two phases: offline and online. During the offline phase, we use simulated racing data to learn a near-potential function that approximates utility changes for agents. This function facilitates the online computation of approximate Nash equilibria by maximizing its value. We evaluate our method in a head-to-head 3-car racing scenario, demonstrating superior performance compared to several existing baselines.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Truthful mechanisms for linear bandit games with private contexts</title>
<link>https://arxiv.org/abs/2501.03865</link>
<guid>https://arxiv.org/abs/2501.03865</guid>
<content:encoded><![CDATA[

arXiv:2501.03865v2 Announce Type: replace 
Abstract: The contextual bandit problem, where agents arrive sequentially with personal contexts and the system adapts its arm allocation decisions accordingly, has recently garnered increasing attention for enabling more personalized outcomes. However, in many healthcare and recommendation applications, agents have private profiles and may misreport their contexts to gain from the system. For example, in adaptive clinical trials, where hospitals sequentially recruit volunteers to test multiple new treatments and adjust plans based on volunteers' reported profiles such as symptoms and interim data, participants may misreport severe side effects like allergy and nausea to avoid perceived suboptimal treatments. We are the first to study this issue of private context misreporting in a stochastic contextual bandit game between the system and non-repeated agents. We show that traditional low-regret algorithms, such as UCB family algorithms and Thompson sampling, fail to ensure truthful reporting and can result in linear regret in the worst case, while traditional truthful algorithms like explore-then-commit (ETC) and $\epsilon$-greedy algorithm incur sublinear but high regret. We propose a mechanism that uses a linear program to ensure truthfulness while minimizing deviation from Thompson sampling, yielding an $O(\ln T)$ frequentist regret. Our numerical experiments further demonstrate strong performance in multiple contexts and across other distribution families.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Do Multi-Agent LLM Systems Fail?</title>
<link>https://arxiv.org/abs/2503.13657</link>
<guid>https://arxiv.org/abs/2503.13657</guid>
<content:encoded><![CDATA[

arXiv:2503.13657v2 Announce Type: replace 
Abstract: Despite growing enthusiasm for Multi-Agent LLM Systems (MAS), their performance gains on popular benchmarks often remain minimal compared with single-agent frameworks. This gap highlights the need to systematically analyze the challenges hindering MAS effectiveness.
  We present MAST (Multi-Agent System Failure Taxonomy), the first empirically grounded taxonomy designed to understand MAS failures. We analyze seven popular MAS frameworks across over 200 tasks, involving six expert human annotators. Through this process, we identify 14 unique failure modes, organized into 3 overarching categories, (i) specification issues, (ii) inter-agent misalignment, and (iii) task verification. MAST emerges iteratively from rigorous inter-annotator agreement studies, achieving a Cohen's Kappa score of 0.88. To support scalable evaluation, we develop a validated LLM-as-a-Judge pipeline integrated with MAST. We leverage two case studies to demonstrate MAST's practical utility in analyzing failures and guiding MAS development. Our findings reveal that identified failures require more complex solutions, highlighting a clear roadmap for future research. We open source our comprehensive dataset and LLM annotator to facilitate further development of MAS.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TraCeS: Trajectory Based Credit Assignment From Sparse Safety Feedback</title>
<link>https://arxiv.org/abs/2504.12557</link>
<guid>https://arxiv.org/abs/2504.12557</guid>
<content:encoded><![CDATA[

arXiv:2504.12557v2 Announce Type: replace 
Abstract: In safe reinforcement learning (RL), auxiliary safety costs are used to align the agent to safe decision making. In practice, safety constraints, including cost functions and budgets, are unknown or hard to specify, as it requires anticipation of all possible unsafe behaviors. We therefore address a general setting where the true safety definition is unknown, and has to be learned from sparsely labeled data. Our key contributions are: first, we design a safety model that performs credit assignment to estimate each decision step's impact on the overall safety using a dataset of diverse trajectories and their corresponding binary safety labels (i.e., whether the corresponding trajectory is safe/unsafe). Second, we illustrate the architecture of our safety model to demonstrate its ability to learn a separate safety score for each timestep. Third, we reformulate the safe RL problem using the proposed safety model and derive an effective algorithm to optimize a safe yet rewarding policy. Finally, our empirical results corroborate our findings and show that this approach is effective in satisfying unknown safety definition, and scalable to various continuous control tasks.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Passivity Analysis for Nonlinear Consensus on Balanced Digraphs</title>
<link>https://arxiv.org/abs/2411.05933</link>
<guid>https://arxiv.org/abs/2411.05933</guid>
<content:encoded><![CDATA[

arXiv:2411.05933v2 Announce Type: replace-cross 
Abstract: This work deals with the output consensus problem for multiagent systems over balanced digraphs by passivity analysis. As the standard diffusive coupling structure only models the undirected interconnection, we propose a general approach capable of processing directed coupling and performing passivity analysis. To mitigate the complexity arising from the nonlinearity and directed interconnections, we reformulate the output consensus problem as a convergence analysis on a submanifold. We provide passivity analysis and establish a sufficient condition based on passivity for achieving output agreement in multi-agent systems over balanced digraphs. The results are supported by a numerical example.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StarWhisper Telescope: Agent-Based Observation Assistant System to Approach AI Astrophysicist</title>
<link>https://arxiv.org/abs/2412.06412</link>
<guid>https://arxiv.org/abs/2412.06412</guid>
<content:encoded><![CDATA[

arXiv:2412.06412v2 Announce Type: replace-cross 
Abstract: With the rapid advancements in Large Language Models (LLMs), LLM-based agents have introduced convenient and user-friendly methods for leveraging tools across various domains. In the field of astronomical observation, the construction of new telescopes has significantly increased astronomers' workload. Deploying LLM-powered agents can effectively alleviate this burden and reduce the costs associated with training personnel. Within the Nearby Galaxy Supernovae Survey (NGSS) project, which encompasses eight telescopes across three observation sites, aiming to find the transients from the galaxies in 50 mpc, we have developed the \textbf{StarWhisper Telescope System} to manage the entire observation process. This system automates tasks such as generating observation lists, conducting observations, analyzing data, and providing feedback to the observer. Observation lists are customized for different sites and strategies to ensure comprehensive coverage of celestial objects. After manual verification, these lists are uploaded to the telescopes via the agents in the system, which initiates observations upon neutral language. The observed images are analyzed in real-time, and the transients are promptly communicated to the observer. The agent modifies them into a real-time follow-up observation proposal and send to the Xinglong observatory group chat, then add them to the next-day observation lists. Additionally, the integration of AI agents within the system provides online accessibility, saving astronomers' time and encouraging greater participation from amateur astronomers in the NGSS project.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>A biologically Inspired Trust Model for Open Multi-Agent Systems that is Resilient to Rapid Performance Fluctuations</title>
<link>https://arxiv.org/abs/2504.15301</link>
<guid>https://arxiv.org/abs/2504.15301</guid>
<content:encoded><![CDATA[
arXiv:2504.15301v1 Announce Type: new 
Abstract: Trust management provides an alternative solution for securing open, dynamic, and distributed multi-agent systems, where conventional cryptographic methods prove to be impractical. However, existing trust models face challenges related to agent mobility, changing behaviors, and the cold start problem. To address these issues we introduced a biologically inspired trust model in which trustees assess their own capabilities and store trust data locally. This design improves mobility support, reduces communication overhead, resists disinformation, and preserves privacy. Despite these advantages, prior evaluations revealed limitations of our model in adapting to provider population changes and continuous performance fluctuations. This study proposes a novel algorithm, incorporating a self-classification mechanism for providers to detect performance drops potentially harmful for the service consumers. Simulation results demonstrate that the new algorithm outperforms its original version and FIRE, a well-known trust and reputation model, particularly in handling dynamic trustee behavior. While FIRE remains competitive under extreme environmental changes, the proposed algorithm demonstrates greater adaptability across various conditions. In contrast to existing trust modeling research, this study conducts a comprehensive evaluation of our model using widely recognized trust model criteria, assessing its resilience against common trust-related attacks while identifying strengths, weaknesses, and potential countermeasures. Finally, several key directions for future research are proposed.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Machine Learning Agents Deal with Hard Choices?</title>
<link>https://arxiv.org/abs/2504.15304</link>
<guid>https://arxiv.org/abs/2504.15304</guid>
<content:encoded><![CDATA[
arXiv:2504.15304v1 Announce Type: new 
Abstract: Machine Learning ML agents have been increasingly used in decision-making across a wide range of tasks and environments. These ML agents are typically designed to balance multiple objectives when making choices. Understanding how their decision-making processes align with or diverge from human reasoning is essential. Human agents often encounter hard choices, that is, situations where options are incommensurable; neither option is preferred, yet the agent is not indifferent between them. In such cases, human agents can identify hard choices and resolve them through deliberation. In contrast, current ML agents, due to fundamental limitations in Multi-Objective Optimisation or MOO methods, cannot identify hard choices, let alone resolve them. Neither Scalarised Optimisation nor Pareto Optimisation, the two principal MOO approaches, can capture incommensurability. This limitation generates three distinct alignment problems: the alienness of ML decision-making behaviour from a human perspective; the unreliability of preference-based alignment strategies for hard choices; and the blockage of alignment strategies pursuing multiple objectives. Evaluating two potential technical solutions, I recommend an ensemble solution that appears most promising for enabling ML agents to identify hard choices and mitigate alignment problems. However, no known technique allows ML agents to resolve hard choices through deliberation, as they cannot autonomously change their goals. This underscores the distinctiveness of human agency and urges ML researchers to reconceptualise machine autonomy and develop frameworks and methods that can better address this fundamental gap.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PolicyEvol-Agent: Evolving Policy via Environment Perception and Self-Awareness with Theory of Mind</title>
<link>https://arxiv.org/abs/2504.15313</link>
<guid>https://arxiv.org/abs/2504.15313</guid>
<content:encoded><![CDATA[
arXiv:2504.15313v1 Announce Type: new 
Abstract: Multi-agents has exhibited significant intelligence in real-word simulations with Large language models (LLMs) due to the capabilities of social cognition and knowledge retrieval. However, existing research on agents equipped with effective cognition chains including reasoning, planning, decision-making and reflecting remains limited, especially in the dynamically interactive scenarios. In addition, unlike human, prompt-based responses face challenges in psychological state perception and empirical calibration during uncertain gaming process, which can inevitably lead to cognition bias. In light of above, we introduce PolicyEvol-Agent, a comprehensive LLM-empowered framework characterized by systematically acquiring intentions of others and adaptively optimizing irrational strategies for continual enhancement. Specifically, PolicyEvol-Agent first obtains reflective expertise patterns and then integrates a range of cognitive operations with Theory of Mind alongside internal and external perspectives. Simulation results, outperforming RL-based models and agent-based methods, demonstrate the superiority of PolicyEvol-Agent for final gaming victory. Moreover, the policy evolution mechanism reveals the effectiveness of dynamic guideline adjustments in both automatic and human evaluation.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving New Tasks by Adapting Internet Video Knowledge</title>
<link>https://arxiv.org/abs/2504.15369</link>
<guid>https://arxiv.org/abs/2504.15369</guid>
<content:encoded><![CDATA[
arXiv:2504.15369v1 Announce Type: new 
Abstract: Video generative models demonstrate great promise in robotics by serving as visual planners or as policy supervisors. When pretrained on internet-scale data, such video models intimately understand alignment with natural language, and can thus facilitate generalization to novel downstream behavior through text-conditioning. However, they may not be sensitive to the specificities of the particular environment the agent inhabits. On the other hand, training video models on in-domain examples of robotic behavior naturally encodes environment-specific intricacies, but the scale of available demonstrations may not be sufficient to support generalization to unseen tasks via natural language specification. In this work, we investigate different adaptation techniques that integrate in-domain information with large-scale pretrained video models, and explore the extent to which they enable novel text-conditioned generalization for robotic tasks, while also considering their independent data and resource considerations. We successfully demonstrate across robotic environments that adapting powerful video models with small scales of example data can successfully facilitate generalization to novel behaviors. In particular, we present a novel adaptation strategy, termed Inverse Probabilistic Adaptation, that not only consistently achieves strong generalization performance across robotic tasks and settings, but also exhibits robustness to the quality of adaptation data, successfully solving novel tasks even when only suboptimal in-domain demonstrations are available.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-Convergence Sim-to-Real Policy Transfer: A Principled Alternative to Cherry-Picking</title>
<link>https://arxiv.org/abs/2504.15414</link>
<guid>https://arxiv.org/abs/2504.15414</guid>
<content:encoded><![CDATA[
arXiv:2504.15414v1 Announce Type: new 
Abstract: Learning-based approaches, particularly reinforcement learning (RL), have become widely used for developing control policies for autonomous agents, such as locomotion policies for legged robots. RL training typically maximizes a predefined reward (or minimizes a corresponding cost/loss) by iteratively optimizing policies within a simulator. Starting from a randomly initialized policy, the empirical expected reward follows a trajectory with an overall increasing trend. While some policies become temporarily stuck in local optima, a well-defined training process generally converges to a reward level with noisy oscillations. However, selecting a policy for real-world deployment is rarely an analytical decision (i.e., simply choosing the one with the highest reward) and is instead often performed through trial and error. To improve sim-to-real transfer, most research focuses on the pre-convergence stage, employing techniques such as domain randomization, multi-fidelity training, adversarial training, and architectural innovations. However, these methods do not eliminate the inevitable convergence trajectory and noisy oscillations of rewards, leading to heuristic policy selection or cherry-picking. This paper addresses the post-convergence sim-to-real transfer problem by introducing a worst-case performance transference optimization approach, formulated as a convex quadratic-constrained linear programming problem. Extensive experiments demonstrate its effectiveness in transferring RL-based locomotion policies from simulation to real-world laboratory tests.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bare Minimum Mitigations for Autonomous AI Development</title>
<link>https://arxiv.org/abs/2504.15416</link>
<guid>https://arxiv.org/abs/2504.15416</guid>
<content:encoded><![CDATA[
arXiv:2504.15416v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is advancing rapidly, with the potential for significantly automating AI research and development itself in the near future. In 2024, international scientists, including Turing Award recipients, warned of risks from autonomous AI research and development (R&amp;D), suggesting a red line such that no AI system should be able to improve itself or other AI systems without explicit human approval and assistance. However, the criteria for meaningful human approval remain unclear, and there is limited analysis on the specific risks of autonomous AI R&amp;D, how they arise, and how to mitigate them. In this brief paper, we outline how these risks may emerge and propose four minimum safeguard recommendations applicable when AI agents significantly automate or accelerate AI development.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MRTA-Sim: A Modular Simulator for Multi-Robot Allocation, Planning, and Control in Open-World Environments</title>
<link>https://arxiv.org/abs/2504.15418</link>
<guid>https://arxiv.org/abs/2504.15418</guid>
<content:encoded><![CDATA[
arXiv:2504.15418v1 Announce Type: new 
Abstract: This paper introduces MRTA-Sim, a Python/ROS2/Gazebo simulator for testing approaches to Multi-Robot Task Allocation (MRTA) problems on simulated robots in complex, indoor environments. Grid-based approaches to MRTA problems can be too restrictive for use in complex, dynamic environments such in warehouses, department stores, hospitals, etc. However, approaches that operate in free-space often operate at a layer of abstraction above the control and planning layers of a robot and make an assumption on approximate travel time between points of interest in the system. These abstractions can neglect the impact of the tight space and multi-agent interactions on the quality of the solution. Therefore, MRTA solutions should be tested with the navigation stacks of the robots in mind, taking into account robot planning, conflict avoidance between robots, and human interaction and avoidance. This tool connects the allocation output of MRTA solvers to individual robot planning using the NAV2 stack and local, centralized multi-robot deconfliction using Control Barrier Function-Quadrtic Programs (CBF-QPs), creating a platform closer to real-world operation for more comprehensive testing of these approaches. The simulation architecture is modular so that users can swap out methods at different levels of the stack. We show the use of our system with a Satisfiability Modulo Theories (SMT)-based approach to dynamic MRTA on a fleet of indoor delivery robots.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Assisted Translation of Legacy FORTRAN Codes to C++: A Cross-Platform Study</title>
<link>https://arxiv.org/abs/2504.15424</link>
<guid>https://arxiv.org/abs/2504.15424</guid>
<content:encoded><![CDATA[
arXiv:2504.15424v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly being leveraged for generating and translating scientific computer codes by both domain-experts and non-domain experts. Fortran has served as one of the go to programming languages in legacy high-performance computing (HPC) for scientific discoveries. Despite growing adoption, LLM-based code translation of legacy code-bases has not been thoroughly assessed or quantified for its usability. Here, we studied the applicability of LLM-based translation of Fortran to C++ as a step towards building an agentic-workflow using open-weight LLMs on two different computational platforms. We statistically quantified the compilation accuracy of the translated C++ codes, measured the similarity of the LLM translated code to the human translated C++ code, and statistically quantified the output similarity of the Fortran to C++ translation.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Multi-Agent Safe Optimal Control with Distributed Epigraph Form MARL</title>
<link>https://arxiv.org/abs/2504.15425</link>
<guid>https://arxiv.org/abs/2504.15425</guid>
<content:encoded><![CDATA[
arXiv:2504.15425v1 Announce Type: new 
Abstract: Tasks for multi-robot systems often require the robots to collaborate and complete a team goal while maintaining safety. This problem is usually formalized as a constrained Markov decision process (CMDP), which targets minimizing a global cost and bringing the mean of constraint violation below a user-defined threshold. Inspired by real-world robotic applications, we define safety as zero constraint violation. While many safe multi-agent reinforcement learning (MARL) algorithms have been proposed to solve CMDPs, these algorithms suffer from unstable training in this setting. To tackle this, we use the epigraph form for constrained optimization to improve training stability and prove that the centralized epigraph form problem can be solved in a distributed fashion by each agent. This results in a novel centralized training distributed execution MARL algorithm named Def-MARL. Simulation experiments on 8 different tasks across 2 different simulators show that Def-MARL achieves the best overall performance, satisfies safety constraints, and maintains stable training. Real-world hardware experiments on Crazyflie quadcopters demonstrate the ability of Def-MARL to safely coordinate agents to complete complex collaborative tasks compared to other methods.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGI Is Coming... Right After AI Learns to Play Wordle</title>
<link>https://arxiv.org/abs/2504.15434</link>
<guid>https://arxiv.org/abs/2504.15434</guid>
<content:encoded><![CDATA[
arXiv:2504.15434v1 Announce Type: new 
Abstract: This paper investigates multimodal agents, in particular, OpenAI's Computer-User Agent (CUA), trained to control and complete tasks through a standard computer interface, similar to humans. We evaluated the agent's performance on the New York Times Wordle game to elicit model behaviors and identify shortcomings. Our findings revealed a significant discrepancy in the model's ability to recognize colors correctly depending on the context. The model had a $5.36\%$ success rate over several hundred runs across a week of Wordle. Despite the immense enthusiasm surrounding AI agents and their potential to usher in Artificial General Intelligence (AGI), our findings reinforce the fact that even simple tasks present substantial challenges for today's frontier AI models. We conclude with a discussion of the potential underlying causes, implications for future development, and research directions to improve these AI systems.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Human-AI Coordination through Adversarial Training and Generative Models</title>
<link>https://arxiv.org/abs/2504.15457</link>
<guid>https://arxiv.org/abs/2504.15457</guid>
<content:encoded><![CDATA[
arXiv:2504.15457v1 Announce Type: new 
Abstract: Being able to cooperate with new people is an important component of many economically valuable AI tasks, from household robotics to autonomous driving. However, generalizing to novel humans requires training on data that captures the diversity of human behaviors. Adversarial training is one avenue for searching for such data and ensuring that agents are robust. However, it is difficult to apply in the cooperative setting because adversarial policies intentionally learn to sabotage the task instead of simulating valid cooperation partners. To address this challenge, we propose a novel strategy for overcoming self-sabotage that combines a pre-trained generative model to simulate valid cooperative agent policies with adversarial training to maximize regret. We call our method GOAT: Generative Online Adversarial Training. In this framework, the GOAT dynamically searches for and generates coordination strategies where the learning policy -- the Cooperator agent -- underperforms. GOAT enables better generalization by exposing the Cooperator to various challenging interaction scenarios. We maintain realistic coordination strategies by updating only the generative model's embedding while keeping its parameters frozen, thus avoiding adversarial exploitation. We evaluate GOAT with real human partners, and the results demonstrate state-of-the-art performance on the Overcooked benchmark, highlighting its effectiveness in generalizing to diverse human behaviors.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent for User: Testing Multi-User Interactive Features in TikTok</title>
<link>https://arxiv.org/abs/2504.15474</link>
<guid>https://arxiv.org/abs/2504.15474</guid>
<content:encoded><![CDATA[
arXiv:2504.15474v1 Announce Type: new 
Abstract: TikTok, a widely-used social media app boasting over a billion monthly active users, requires effective app quality assurance for its intricate features. Feature testing is crucial in achieving this goal. However, the multi-user interactive features within the app, such as live streaming, voice calls, etc., pose significant challenges for developers, who must handle simultaneous device management and user interaction coordination. To address this, we introduce a novel multi-agent approach, powered by the Large Language Models (LLMs), to automate the testing of multi-user interactive app features. In detail, we build a virtual device farm that allocates the necessary number of devices for a given multi-user interactive task. For each device, we deploy an LLM-based agent that simulates a user, thereby mimicking user interactions to collaboratively automate the testing process. The evaluations on 24 multi-user interactive tasks within the TikTok app, showcase its capability to cover 75% of tasks with 85.9% action similarity and offer 87% time savings for developers. Additionally, we have also integrated our approach into the real-world TikTok testing platform, aiding in the detection of 26 multi-user interactive bugs.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-context Ranking Preference Optimization</title>
<link>https://arxiv.org/abs/2504.15477</link>
<guid>https://arxiv.org/abs/2504.15477</guid>
<content:encoded><![CDATA[
arXiv:2504.15477v1 Announce Type: new 
Abstract: Recent developments in Direct Preference Optimization (DPO) allow large language models (LLMs) to function as implicit ranking models by maximizing the margin between preferred and non-preferred responses. In practice, user feedback on such lists typically involves identifying a few relevant items in context rather than providing detailed pairwise comparisons for every possible item pair. Moreover, many complex information retrieval tasks, such as conversational agents and summarization systems, critically depend on ranking the highest-quality outputs at the top, emphasizing the need to support natural and flexible forms of user feedback. To address the challenge of limited and sparse pairwise feedback in the in-context setting, we propose an In-context Ranking Preference Optimization (IRPO) framework that directly optimizes LLMs based on ranking lists constructed during inference. To further capture flexible forms of feedback, IRPO extends the DPO objective by incorporating both the relevance of items and their positions in the list. Modeling these aspects jointly is non-trivial, as ranking metrics are inherently discrete and non-differentiable, making direct optimization difficult. To overcome this, IRPO introduces a differentiable objective based on positional aggregation of pairwise item preferences, enabling effective gradient-based optimization of discrete ranking metrics. We further provide theoretical insights showing that IRPO (i) automatically emphasizes items with greater disagreement between the model and the reference ranking, and (ii) links its gradient to an importance sampling estimator, yielding an unbiased estimator with reduced variance. Empirical results show IRPO outperforms standard DPO approaches in ranking performance, highlighting its effectiveness in aligning LLMs with direct in-context ranking preferences.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RiskNet: Interaction-Aware Risk Forecasting for Autonomous Driving in Long-Tail Scenarios</title>
<link>https://arxiv.org/abs/2504.15541</link>
<guid>https://arxiv.org/abs/2504.15541</guid>
<content:encoded><![CDATA[
arXiv:2504.15541v1 Announce Type: new 
Abstract: Ensuring the safety of autonomous vehicles (AVs) in long-tail scenarios remains a critical challenge, particularly under high uncertainty and complex multi-agent interactions. To address this, we propose RiskNet, an interaction-aware risk forecasting framework, which integrates deterministic risk modeling with probabilistic behavior prediction for comprehensive risk assessment. At its core, RiskNet employs a field-theoretic model that captures interactions among ego vehicle, surrounding agents, and infrastructure via interaction fields and force. This model supports multidimensional risk evaluation across diverse scenarios (highways, intersections, and roundabouts), and shows robustness under high-risk and long-tail settings. To capture the behavioral uncertainty, we incorporate a graph neural network (GNN)-based trajectory prediction module, which learns multi-modal future motion distributions. Coupled with the deterministic risk field, it enables dynamic, probabilistic risk inference across time, enabling proactive safety assessment under uncertainty. Evaluations on the highD, inD, and rounD datasets, spanning lane changes, turns, and complex merges, demonstrate that our method significantly outperforms traditional approaches (e.g., TTC, THW, RSS, NC Field) in terms of accuracy, responsiveness, and directional sensitivity, while maintaining strong generalization across scenarios. This framework supports real-time, scenario-adaptive risk forecasting and demonstrates strong generalization across uncertain driving environments. It offers a unified foundation for safety-critical decision-making in long-tail scenarios.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Testing and Adapting REST APIs as LLM Tools</title>
<link>https://arxiv.org/abs/2504.15546</link>
<guid>https://arxiv.org/abs/2504.15546</guid>
<content:encoded><![CDATA[
arXiv:2504.15546v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are enabling autonomous agents to perform complex workflows using external tools or functions, often provided via REST APIs in enterprise systems. However, directly utilizing these APIs as tools poses challenges due to their complex input schemas, elaborate responses, and often ambiguous documentation. Current benchmarks for tool testing do not adequately address these complexities, leading to a critical gap in evaluating API readiness for agent-driven automation. In this work, we present a novel testing framework aimed at evaluating and enhancing the readiness of REST APIs to function as tools for LLM-based agents. Our framework transforms apis as tools, generates comprehensive test cases for the APIs, translates tests cases into natural language instructions suitable for agents, enriches tool definitions and evaluates the agent's ability t correctly invoke the API and process its inputs and responses. To provide actionable insights, we analyze the outcomes of 750 test cases, presenting a detailed taxonomy of errors, including input misinterpretation, output handling inconsistencies, and schema mismatches. Additionally, we classify these test cases to streamline debugging and refinement of tool integrations. This work offers a foundational step toward enabling enterprise APIs as tools, improving their usability in agent-based applications.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Agent Framework for Automated Qinqiang Opera Script Generation Using Large Language Models</title>
<link>https://arxiv.org/abs/2504.15552</link>
<guid>https://arxiv.org/abs/2504.15552</guid>
<content:encoded><![CDATA[
arXiv:2504.15552v1 Announce Type: new 
Abstract: This paper introduces a novel multi-Agent framework that automates the end to end production of Qinqiang opera by integrating Large Language Models , visual generation, and Text to Speech synthesis. Three specialized agents collaborate in sequence: Agent1 uses an LLM to craft coherent, culturally grounded scripts;Agent2 employs visual generation models to render contextually accurate stage scenes; and Agent3 leverages TTS to produce synchronized, emotionally expressive vocal performances. In a case study on Dou E Yuan, the system achieved expert ratings of 3.8 for script fidelity, 3.5 for visual coherence, and 3.8 for speech accuracy-culminating in an overall score of 3.6, a 0.3 point improvement over a Single Agent baseline. Ablation experiments demonstrate that removing Agent2 or Agent3 leads to drops of 0.4 and 0.5 points, respectively, underscoring the value of modular collaboration. This work showcases how AI driven pipelines can streamline and scale the preservation of traditional performing arts, and points toward future enhancements in cross modal alignment, richer emotional nuance, and support for additional opera genres.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment</title>
<link>https://arxiv.org/abs/2504.15585</link>
<guid>https://arxiv.org/abs/2504.15585</guid>
<content:encoded><![CDATA[
arXiv:2504.15585v1 Announce Type: new 
Abstract: The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications. As LLMs continue to gain prominence in both research and commercial domains, their security and safety implications have become a growing concern, not only for researchers and corporations but also for every nation. Currently, existing surveys on LLM safety primarily focus on specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning phase, lacking a comprehensive understanding of the entire "lifechain" of LLMs. To address this gap, this paper introduces, for the first time, the concept of "full-stack" safety to systematically consider safety issues throughout the entire process of LLM training, deployment, and eventual commercialization. Compared to the off-the-shelf LLM safety surveys, our work demonstrates several distinctive advantages: (I) Comprehensive Perspective. We define the complete LLM lifecycle as encompassing data preparation, pre-training, post-training, deployment and final commercialization. To our knowledge, this represents the first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive Literature Support. Our research is grounded in an exhaustive review of over 800+ papers, ensuring comprehensive coverage and systematic organization of security issues within a more holistic understanding. (III) Unique Insights. Through systematic literature analysis, we have developed reliable roadmaps and perspectives for each chapter. Our work identifies promising research directions, including safety in data generation, alignment techniques, model editing, and LLM-based agent systems. These insights provide valuable guidance for researchers pursuing future work in this field.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grasping Deformable Objects via Reinforcement Learning with Cross-Modal Attention to Visuo-Tactile Inputs</title>
<link>https://arxiv.org/abs/2504.15595</link>
<guid>https://arxiv.org/abs/2504.15595</guid>
<content:encoded><![CDATA[
arXiv:2504.15595v1 Announce Type: new 
Abstract: We consider the problem of grasping deformable objects with soft shells using a robotic gripper. Such objects have a center-of-mass that changes dynamically and are fragile so prone to burst. Thus, it is difficult for robots to generate appropriate control inputs not to drop or break the object while performing manipulation tasks. Multi-modal sensing data could help understand the grasping state through global information (e.g., shapes, pose) from visual data and local information around the contact (e.g., pressure) from tactile data. Although they have complementary information that can be beneficial to use together, fusing them is difficult owing to their different properties.
  We propose a method based on deep reinforcement learning (DRL) that generates control inputs of a simple gripper from visuo-tactile sensing information. Our method employs a cross-modal attention module in the encoder network and trains it in a self-supervised manner using the loss function of the RL agent. With the multi-modal fusion, the proposed method can learn the representation for the DRL agent from the visuo-tactile sensory data. The experimental result shows that cross-modal attention is effective to outperform other early and late data fusion methods across different environments including unseen robot motions and objects.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SocialMOIF: Multi-Order Intention Fusion for Pedestrian Trajectory Prediction</title>
<link>https://arxiv.org/abs/2504.15616</link>
<guid>https://arxiv.org/abs/2504.15616</guid>
<content:encoded><![CDATA[
arXiv:2504.15616v1 Announce Type: new 
Abstract: The analysis and prediction of agent trajectories are crucial for decision-making processes in intelligent systems, with precise short-term trajectory forecasting being highly significant across a range of applications. Agents and their social interactions have been quantified and modeled by researchers from various perspectives; however, substantial limitations exist in the current work due to the inherent high uncertainty of agent intentions and the complex higher-order influences among neighboring groups. SocialMOIF is proposed to tackle these challenges, concentrating on the higher-order intention interactions among neighboring groups while reinforcing the primary role of first-order intention interactions between neighbors and the target agent. This method develops a multi-order intention fusion model to achieve a more comprehensive understanding of both direct and indirect intention information. Within SocialMOIF, a trajectory distribution approximator is designed to guide the trajectories toward values that align more closely with the actual data, thereby enhancing model interpretability. Furthermore, a global trajectory optimizer is introduced to enable more accurate and efficient parallel predictions. By incorporating a novel loss function that accounts for distance and direction during training, experimental results demonstrate that the model outperforms previous state-of-the-art baselines across multiple metrics in both dynamic and static datasets.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Perception for Goal-oriented Navigation: A Survey</title>
<link>https://arxiv.org/abs/2504.15643</link>
<guid>https://arxiv.org/abs/2504.15643</guid>
<content:encoded><![CDATA[
arXiv:2504.15643v1 Announce Type: new 
Abstract: Goal-oriented navigation presents a fundamental challenge for autonomous systems, requiring agents to navigate complex environments to reach designated targets. This survey offers a comprehensive analysis of multimodal navigation approaches through the unifying perspective of inference domains, exploring how agents perceive, reason about, and navigate environments using visual, linguistic, and acoustic information. Our key contributions include organizing navigation methods based on their primary environmental reasoning mechanisms across inference domains; systematically analyzing how shared computational foundations support seemingly disparate approaches across different navigation tasks; identifying recurring patterns and distinctive strengths across various navigation paradigms; and examining the integration challenges and opportunities of multimodal perception to enhance navigation capabilities. In addition, we review approximately 200 relevant articles to provide an in-depth understanding of the current landscape.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trustworthy Decentralized Autonomous Machines: A New Paradigm in Automation Economy</title>
<link>https://arxiv.org/abs/2504.15676</link>
<guid>https://arxiv.org/abs/2504.15676</guid>
<content:encoded><![CDATA[
arXiv:2504.15676v1 Announce Type: new 
Abstract: Decentralized Autonomous Machines (DAMs) represent a transformative paradigm in automation economy, integrating artificial intelligence (AI), blockchain technology, and Internet of Things (IoT) devices to create self-governing economic agents participating in Decentralized Physical Infrastructure Networks (DePIN). Capable of managing both digital and physical assets and unlike traditional Decentralized Autonomous Organizations (DAOs), DAMs extend autonomy into the physical world, enabling trustless systems for Real and Digital World Assets (RDWAs). In this paper, we explore the technological foundations, and challenges of DAMs and argue that DAMs are pivotal in transitioning from trust-based to trustless economic models, offering scalable, transparent, and equitable solutions for asset management. The integration of AI-driven decision-making, IoT-enabled operational autonomy, and blockchain-based governance allows DAMs to decentralize ownership, optimize resource allocation, and democratize access to economic opportunities. Therefore, in this research, we highlight the potential of DAMs to address inefficiencies in centralized systems, reduce wealth disparities, and foster a post-labor economy.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation</title>
<link>https://arxiv.org/abs/2504.15699</link>
<guid>https://arxiv.org/abs/2504.15699</guid>
<content:encoded><![CDATA[
arXiv:2504.15699v1 Announce Type: new 
Abstract: Embodied agents exhibit immense potential across a multitude of domains, making the assurance of their behavioral safety a fundamental prerequisite for their widespread deployment. However, existing research predominantly concentrates on the security of general large language models, lacking specialized methodologies for establishing safety benchmarks and input moderation tailored to embodied agents. To bridge this gap, this paper introduces a novel input moderation framework, meticulously designed to safeguard embodied agents. This framework encompasses the entire pipeline, including taxonomy definition, dataset curation, moderator architecture, model training, and rigorous evaluation. Notably, we introduce EAsafetyBench, a meticulously crafted safety benchmark engineered to facilitate both the training and stringent assessment of moderators specifically designed for embodied agents. Furthermore, we propose Pinpoint, an innovative prompt-decoupled input moderation scheme that harnesses a masked attention mechanism to effectively isolate and mitigate the influence of functional prompts on moderation tasks. Extensive experiments conducted on diverse benchmark datasets and models validate the feasibility and efficacy of the proposed approach. The results demonstrate that our methodologies achieve an impressive average detection accuracy of 94.58%, surpassing the performance of existing state-of-the-art techniques, alongside an exceptional moderation processing time of merely 0.002 seconds per instance.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Control of Redundant Hydraulic Manipulator Using Reinforcement Learning with Action Feedback</title>
<link>https://arxiv.org/abs/2504.15714</link>
<guid>https://arxiv.org/abs/2504.15714</guid>
<content:encoded><![CDATA[
arXiv:2504.15714v1 Announce Type: new 
Abstract: This article presents an entirely data-driven approach for autonomous control of redundant manipulators with hydraulic actuation. The approach only requires minimal system information, which is inherited from a simulation model. The non-linear hydraulic actuation dynamics are modeled using actuator networks from the data gathered during the manual operation of the manipulator to effectively emulate the real system in a simulation environment. A neural network control policy for autonomous control, based on end-effector (EE) position tracking is then learned using Reinforcement Learning (RL) with Ornstein-Uhlenbeck process noise (OUNoise) for efficient exploration. The RL agent also receives feedback based on supervised learning of the forward kinematics which facilitates selecting the best suitable action from exploration. The control policy directly provides the joint variables as outputs based on provided target EE position while taking into account the system dynamics. The joint variables are then mapped to the hydraulic valve commands, which are then fed to the system without further modifications. The proposed approach is implemented on a scaled hydraulic forwarder crane with three revolute and one prismatic joint to track the desired position of the EE in 3-Dimensional (3D) space. With the emulated dynamics and extensive learning in simulation, the results demonstrate the feasibility of deploying the learned controller directly on the real system.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2504.15716</link>
<guid>https://arxiv.org/abs/2504.15716</guid>
<content:encoded><![CDATA[
arXiv:2504.15716v1 Announce Type: new 
Abstract: Effective reasoning remains a core challenge for large language models (LLMs) in the financial domain, where tasks often require domain-specific knowledge, precise numerical calculations, and strict adherence to compliance rules. We propose DianJin-R1, a reasoning-enhanced framework designed to address these challenges through reasoning-augmented supervision and reinforcement learning. Central to our approach is DianJin-R1-Data, a high-quality dataset constructed from CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance Check, CCC), combining diverse financial reasoning scenarios with verified annotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that generates both reasoning steps and final answers. To further refine reasoning quality, we apply Group Relative Policy Optimization (GRPO), a reinforcement learning method that incorporates dual reward signals: one encouraging structured outputs and another rewarding answer correctness. We evaluate our models on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and two general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental results show that DianJin-R1 models consistently outperform their non-reasoning counterparts, especially on complex financial tasks. Moreover, on the real-world CCC dataset, our single-call reasoning models match or even surpass the performance of multi-agent systems that require significantly more computational cost. These findings demonstrate the effectiveness of DianJin-R1 in enhancing financial reasoning through structured supervision and reward-aligned learning, offering a scalable and practical solution for real-world applications.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implementing Rational Choice Functions with LLMs and Measuring their Alignment with User Preferences</title>
<link>https://arxiv.org/abs/2504.15719</link>
<guid>https://arxiv.org/abs/2504.15719</guid>
<content:encoded><![CDATA[
arXiv:2504.15719v1 Announce Type: new 
Abstract: As large language models (LLMs) become integral to intelligent user interfaces (IUIs), their role as decision-making agents raises critical concerns about alignment. Although extensive research has addressed issues such as factuality, bias, and toxicity, comparatively little attention has been paid to measuring alignment to preferences, i.e., the relative desirability of different alternatives, a concept used in decision making, economics, and social choice theory. However, a reliable decision-making agent makes choices that align well with user preferences.
  In this paper, we generalize existing methods that exploit LLMs for ranking alternative outcomes by addressing alignment with the broader and more flexible concept of user preferences, which includes both strict preferences and indifference among alternatives. To this end, we put forward design principles for using LLMs to implement rational choice functions, and provide the necessary tools to measure preference satisfaction. We demonstrate the applicability of our approach through an empirical study in a practical application of an IUI in the automotive domain.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Intent Queries for Motion Transformer-based Trajectory Prediction</title>
<link>https://arxiv.org/abs/2504.15766</link>
<guid>https://arxiv.org/abs/2504.15766</guid>
<content:encoded><![CDATA[
arXiv:2504.15766v1 Announce Type: new 
Abstract: In autonomous driving, accurately predicting the movements of other traffic participants is crucial, as it significantly influences a vehicle's planning processes. Modern trajectory prediction models strive to interpret complex patterns and dependencies from agent and map data. The Motion Transformer (MTR) architecture and subsequent work define the most accurate methods in common benchmarks such as the Waymo Open Motion Benchmark. The MTR model employs pre-generated static intention points as initial goal points for trajectory prediction. However, the static nature of these points frequently leads to misalignment with map data in specific traffic scenarios, resulting in unfeasible or unrealistic goal points. Our research addresses this limitation by integrating scene-specific dynamic intention points into the MTR model. This adaptation of the MTR model was trained and evaluated on the Waymo Open Motion Dataset. Our findings demonstrate that incorporating dynamic intention points has a significant positive impact on trajectory prediction accuracy, especially for predictions over long time horizons. Furthermore, we analyze the impact on ground truth trajectories which are not compliant with the map data or are illegal maneuvers.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed model predictive control without terminal cost under inexact distributed optimization</title>
<link>https://arxiv.org/abs/2504.15768</link>
<guid>https://arxiv.org/abs/2504.15768</guid>
<content:encoded><![CDATA[
arXiv:2504.15768v1 Announce Type: new 
Abstract: This paper presents a novel distributed model predictive control (MPC) formulation without terminal cost and a corresponding distributed synthesis approach for distributed linear discrete-time systems with coupled constraints. The proposed control scheme introduces an explicit stability condition as an additional constraint based on relaxed dynamic programming. As a result, contrary to other related approaches, system stability with the developed controller does not rely on designing a terminal cost. A distributed synthesis approach is then introduced to handle the stability constraint locally within each local agent. To solve the underlying optimization problem for distributed MPC, a violation-free distributed optimization approach is developed, using constraint tightening to ensure feasibility throughout iterations. A numerical example demonstrates that the proposed distributed MPC approach ensures closed-loop stability for each feasible control sequence, with each agent computing its control input in parallel.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents</title>
<link>https://arxiv.org/abs/2504.15785</link>
<guid>https://arxiv.org/abs/2504.15785</guid>
<content:encoded><![CDATA[
arXiv:2504.15785v1 Announce Type: new 
Abstract: Can we build accurate world models out of large language models (LLMs)? How can world models benefit LLM agents? The gap between the prior knowledge of LLMs and the specified environment's dynamics usually bottlenecks LLMs' performance as world models. To bridge the gap, we propose a training-free "world alignment" that learns an environment's symbolic knowledge complementary to LLMs. The symbolic knowledge covers action rules, knowledge graphs, and scene graphs, which are extracted by LLMs from exploration trajectories and encoded into executable codes to regulate LLM agents' policies. We further propose an RL-free, model-based agent "WALL-E 2.0" through the model-predictive control (MPC) framework. Unlike classical MPC requiring costly optimization on the fly, we adopt an LLM agent as an efficient look-ahead optimizer of future steps' actions by interacting with the neurosymbolic world model. While the LLM agent's strong heuristics make it an efficient planner in MPC, the quality of its planned actions is also secured by the accurate predictions of the aligned world model. They together considerably improve learning efficiency in a new environment. On open-world challenges in Mars (Minecraft like) and ALFWorld (embodied indoor environments), WALL-E 2.0 significantly outperforms existing methods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and by at least 61.7% in score. In ALFWorld, it achieves a new record 98% success rate after only 4 iterations.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A closer look at how large language models trust humans: patterns and biases</title>
<link>https://arxiv.org/abs/2504.15801</link>
<guid>https://arxiv.org/abs/2504.15801</guid>
<content:encoded><![CDATA[
arXiv:2504.15801v1 Announce Type: new 
Abstract: As large language models (LLMs) and LLM-based agents increasingly interact with humans in decision-making contexts, understanding the trust dynamics between humans and AI agents becomes a central concern. While considerable literature studies how humans trust AI agents, it is much less understood how LLM-based agents develop effective trust in humans. LLM-based agents likely rely on some sort of implicit effective trust in trust-related contexts (e.g., evaluating individual loan applications) to assist and affect decision making. Using established behavioral theories, we develop an approach that studies whether LLMs trust depends on the three major trustworthiness dimensions: competence, benevolence and integrity of the human subject. We also study how demographic variables affect effective trust. Across 43,200 simulated experiments, for five popular language models, across five different scenarios we find that LLM trust development shows an overall similarity to human trust development. We find that in most, but not all cases, LLM trust is strongly predicted by trustworthiness, and in some cases also biased by age, religion and gender, especially in financial scenarios. This is particularly true for scenarios common in the literature and for newer models. While the overall patterns align with human-like mechanisms of effective trust formation, different models exhibit variation in how they estimate trust; in some cases, trustworthiness and demographic factors are weak predictors of effective trust. These findings call for a better understanding of AI-to-human trust dynamics and monitoring of biases and trust development patterns to prevent unintended and potentially harmful outcomes in trust-sensitive applications of AI.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Test Generation from Task Description for Mobile Testing with Multi-modal Reasoning</title>
<link>https://arxiv.org/abs/2504.15917</link>
<guid>https://arxiv.org/abs/2504.15917</guid>
<content:encoded><![CDATA[
arXiv:2504.15917v1 Announce Type: new 
Abstract: In Android GUI testing, generating an action sequence for a task that can be replayed as a test script is common. Generating sequences of actions and respective test scripts from task goals described in natural language can eliminate the need for manually writing test scripts. However, existing approaches based on large language models (LLM) often struggle with identifying the final action, and either end prematurely or continue past the final screen. In this paper, we introduce VisiDroid, a multi-modal, LLM-based, multi-agent framework that iteratively determines the next action and leverages visual images of screens to detect the task's completeness. The multi-modal approach enhances our model in two significant ways. First, this approach enables it to avoid prematurely terminating a task when textual content alone provides misleading indications of task completion. Additionally, visual input helps the tool avoid errors when changes in the GUI do not directly affect functionality toward task completion, such as adjustments to font sizes or colors. Second, the multi-modal approach also ensures the tool not progress beyond the final screen, which might lack explicit textual indicators of task completion but could display a visual element indicating task completion, which is common in GUI apps. Our evaluation shows that VisiDroid achieves an accuracy of 87.3%, outperforming the best baseline relatively by 23.5%. We also demonstrate that our multi-modal framework with images and texts enables the LLM to better determine when a task is completed.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuroadaptive Haptics: Comparing Reinforcement Learning from Explicit Ratings and Neural Signals for Adaptive XR Systems</title>
<link>https://arxiv.org/abs/2504.15984</link>
<guid>https://arxiv.org/abs/2504.15984</guid>
<content:encoded><![CDATA[
arXiv:2504.15984v1 Announce Type: new 
Abstract: Neuroadaptive haptics offers a path to more immersive extended reality (XR) experiences by dynamically tuning multisensory feedback to user preferences. We present a neuroadaptive haptics system that adapts XR feedback through reinforcement learning (RL) from explicit user ratings and brain-decoded neural signals. In a user study, participants interacted with virtual objects in VR while Electroencephalography (EEG) data were recorded. An RL agent adjusted haptic feedback based either on explicit ratings or on outputs from a neural decoder. Results show that the RL agent's performance was comparable across feedback sources, suggesting that implicit neural feedback can effectively guide personalization without requiring active user input. The EEG-based neural decoder achieved a mean F1 score of 0.8, supporting reliable classification of user experience. These findings demonstrate the feasibility of combining brain-computer interfaces (BCI) and RL to autonomously adapt XR interactions, reducing cognitive load and enhancing immersion.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive Analysis</title>
<link>https://arxiv.org/abs/2504.16047</link>
<guid>https://arxiv.org/abs/2504.16047</guid>
<content:encoded><![CDATA[
arXiv:2504.16047v1 Announce Type: new 
Abstract: Foundation models, trained on vast amounts of data using self-supervised techniques, have emerged as a promising frontier for advancing artificial intelligence (AI) applications in medicine. This study evaluates three different vision-language foundation models (RAD-DINO, CheXagent, and BiomedCLIP) on their ability to capture fine-grained imaging features for radiology tasks. The models were assessed across classification, segmentation, and regression tasks for pneumothorax and cardiomegaly on chest radiographs. Self-supervised RAD-DINO consistently excelled in segmentation tasks, while text-supervised CheXagent demonstrated superior classification performance. BiomedCLIP showed inconsistent performance across tasks. A custom segmentation model that integrates global and local features substantially improved performance for all foundation models, particularly for challenging pneumothorax segmentation. The findings highlight that pre-training methodology significantly influences model performance on specific downstream tasks. For fine-grained segmentation tasks, models trained without text supervision performed better, while text-supervised models offered advantages in classification and interpretability. These insights provide guidance for selecting foundation models based on specific clinical applications in radiology.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ForesightNav: Learning Scene Imagination for Efficient Exploration</title>
<link>https://arxiv.org/abs/2504.16062</link>
<guid>https://arxiv.org/abs/2504.16062</guid>
<content:encoded><![CDATA[
arXiv:2504.16062v1 Announce Type: new 
Abstract: Understanding how humans leverage prior knowledge to navigate unseen environments while making exploratory decisions is essential for developing autonomous robots with similar abilities. In this work, we propose ForesightNav, a novel exploration strategy inspired by human imagination and reasoning. Our approach equips robotic agents with the capability to predict contextual information, such as occupancy and semantic details, for unexplored regions. These predictions enable the robot to efficiently select meaningful long-term navigation goals, significantly enhancing exploration in unseen environments. We validate our imagination-based approach using the Structured3D dataset, demonstrating accurate occupancy prediction and superior performance in anticipating unseen scene geometry. Our experiments show that the imagination module improves exploration efficiency in unseen environments, achieving a 100% completion rate for PointNav and an SPL of 67% for ObjectNav on the Structured3D Validation split. These contributions demonstrate the power of imagination-driven reasoning for autonomous systems to enhance generalizable and efficient exploration.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation</title>
<link>https://arxiv.org/abs/2504.16073</link>
<guid>https://arxiv.org/abs/2504.16073</guid>
<content:encoded><![CDATA[
arXiv:2504.16073v1 Announce Type: new 
Abstract: Recent advancements in visual language models (VLMs) have notably enhanced their capabilities in handling complex Graphical User Interface (GUI) interaction tasks. Despite these improvements, current frameworks often struggle to generate correct actions in challenging GUI environments. State-of-the-art commercial VLMs are black-boxes, and fine-tuning open-source VLMs for GUI tasks requires significant resources. Additionally, existing trajectory-level evaluation and refinement techniques frequently fall short due to delayed feedback and local optimization issues. To address these challenges, we propose an approach that guides VLM agents with process supervision by a reward model during GUI navigation and control at inference time. This guidance allows the VLM agent to optimize actions at each inference step, thereby improving performance in both static and dynamic environments. In particular, our method demonstrates significant performance gains in three GUI navigation tasks, achieving a 3.4% improvement in single step action accuracy for static environments, along with a around 33% increase in task success rate in one dynamic environment. With further integration of trajectory reflection and retry mechanisms, we also demonstrate even greater enhancement in task success.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities</title>
<link>https://arxiv.org/abs/2504.16078</link>
<guid>https://arxiv.org/abs/2504.16078</guid>
<content:encoded><![CDATA[
arXiv:2504.16078v1 Announce Type: new 
Abstract: The success of Large Language Models (LLMs) has sparked interest in various agentic applications. A key hypothesis is that LLMs, leveraging common sense and Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently solve complex domains. However, LLM agents have been found to suffer from sub-optimal exploration and the knowing-doing gap, the inability to effectively act on knowledge present in the model. In this work, we systematically study why LLMs perform sub-optimally in decision-making scenarios. In particular, we closely examine three prevalent failure modes: greediness, frequency bias, and the knowing-doing gap. We propose mitigation of these shortcomings by fine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales. Our experiments across multi-armed bandits, contextual bandits, and Tic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making abilities of LLMs by increasing exploration and narrowing the knowing-doing gap. Finally, we study both classic exploration mechanisms, such as $\epsilon$-greedy, and LLM-specific approaches, such as self-correction and self-consistency, to enable more effective fine-tuning of LLMs for decision-making.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MR. Video: "MapReduce" is the Principle for Long Video Understanding</title>
<link>https://arxiv.org/abs/2504.16082</link>
<guid>https://arxiv.org/abs/2504.16082</guid>
<content:encoded><![CDATA[
arXiv:2504.16082v1 Announce Type: new 
Abstract: We propose MR. Video, an agentic long video understanding framework that demonstrates the simple yet effective MapReduce principle for processing long videos: (1) Map: independently and densely perceiving short video clips, and (2) Reduce: jointly aggregating information from all clips. Compared with sequence-to-sequence vision-language models (VLMs), MR. Video performs detailed short video perception without being limited by context length. Compared with existing video agents that typically rely on sequential key segment selection, the Map operation enables simpler and more scalable sequence parallel perception of short video segments. Its Reduce step allows for more comprehensive context aggregation and reasoning, surpassing explicit key segment retrieval. This MapReduce principle is applicable to both VLMs and video agents, and we use LLM agents to validate its effectiveness.
  In practice, MR. Video employs two MapReduce stages: (A) Captioning: generating captions for short video clips (map), then standardizing repeated characters and objects into shared names (reduce); (B) Analysis: for each user question, analyzing relevant information from individual short videos (map), and integrating them into a final answer (reduce). MR. Video achieves over 10% accuracy improvement on the challenging LVBench compared to state-of-the-art VLMs and video agents.
  Code is available at: https://github.com/ziqipang/MR-Video
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLM-based Prompts as the Optimal Assistant for Unpaired Histopathology Virtual Staining</title>
<link>https://arxiv.org/abs/2504.15545</link>
<guid>https://arxiv.org/abs/2504.15545</guid>
<content:encoded><![CDATA[
arXiv:2504.15545v1 Announce Type: cross 
Abstract: In histopathology, tissue sections are typically stained using common H&amp;E staining or special stains (MAS, PAS, PASM, etc.) to clearly visualize specific tissue structures. The rapid advancement of deep learning offers an effective solution for generating virtually stained images, significantly reducing the time and labor costs associated with traditional histochemical staining. However, a new challenge arises in separating the fundamental visual characteristics of tissue sections from the visual differences induced by staining agents. Additionally, virtual staining often overlooks essential pathological knowledge and the physical properties of staining, resulting in only style-level transfer. To address these issues, we introduce, for the first time in virtual staining tasks, a pathological vision-language large model (VLM) as an auxiliary tool. We integrate contrastive learnable prompts, foundational concept anchors for tissue sections, and staining-specific concept anchors to leverage the extensive knowledge of the pathological VLM. This approach is designed to describe, frame, and enhance the direction of virtual staining. Furthermore, we have developed a data augmentation method based on the constraints of the VLM. This method utilizes the VLM's powerful image interpretation capabilities to further integrate image style and structural information, proving beneficial in high-precision pathological diagnostics. Extensive evaluations on publicly available multi-domain unpaired staining datasets demonstrate that our method can generate highly realistic images and enhance the accuracy of downstream tasks, such as glomerular detection and segmentation. Our code is available at: https://github.com/CZZZZZZZZZZZZZZZZZ/VPGAN-HARBOR
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy-Based Radiative Transfer: Solving the $2$-Level Atom Non-LTE Problem using Soft Actor-Critic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.15679</link>
<guid>https://arxiv.org/abs/2504.15679</guid>
<content:encoded><![CDATA[
arXiv:2504.15679v1 Announce Type: cross 
Abstract: We present a novel reinforcement learning (RL) approach for solving the classical 2-level atom non-LTE radiative transfer problem by framing it as a control task in which an RL agent learns a depth-dependent source function $S(\tau)$ that self-consistently satisfies the equation of statistical equilibrium (SE). The agent's policy is optimized entirely via reward-based interactions with a radiative transfer engine, without explicit knowledge of the ground truth. This method bypasses the need for constructing approximate lambda operators ($\Lambda^*$) common in accelerated iterative schemes. Additionally, it requires no extensive precomputed labeled datasets to extract a supervisory signal, and avoids backpropagating gradients through the complex RT solver itself. Finally, we show through experiment that a simple feedforward neural network trained greedily cannot solve for SE, possibly due to the moving target nature of the problem. Our $\Lambda^*-\text{Free}$ method offers potential advantages for complex scenarios (e.g., atmospheres with enhanced velocity fields, multi-dimensional geometries, or complex microphysics) where $\Lambda^*$ construction or solver differentiability is challenging. Additionally, the agent can be incentivized to find more efficient policies by manipulating the discount factor, leading to a reprioritization of immediate rewards. If demonstrated to generalize past its training data, this RL framework could serve as an alternative or accelerated formalism to achieve SE. To the best of our knowledge, this study represents the first application of reinforcement learning in solar physics that directly solves for a fundamental physical constraint.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mission-driven Exploration for Accelerated Deep Reinforcement Learning with Temporal Logic Task Specifications</title>
<link>https://arxiv.org/abs/2311.17059</link>
<guid>https://arxiv.org/abs/2311.17059</guid>
<content:encoded><![CDATA[
arXiv:2311.17059v2 Announce Type: replace 
Abstract: This paper addresses the problem of designing control policies for agents with unknown stochastic dynamics and control objectives specified using Linear Temporal Logic (LTL). Recent Deep Reinforcement Learning (DRL) algorithms have aimed to compute policies that maximize the satisfaction probability of LTL formulas, but they often suffer from slow learning performance. To address this, we introduce a novel Deep Q-learning algorithm that significantly improves learning speed. The enhanced sample efficiency stems from a mission-driven exploration strategy that prioritizes exploration towards directions likely to contribute to mission success. Identifying these directions relies on an automaton representation of the LTL task as well as a learned neural network that partially models the agent-environment interaction. We provide comparative experiments demonstrating the efficiency of our algorithm on robot navigation tasks in unseen environments.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Certifying Knowledge Comprehension in LLMs</title>
<link>https://arxiv.org/abs/2402.15929</link>
<guid>https://arxiv.org/abs/2402.15929</guid>
<content:encoded><![CDATA[
arXiv:2402.15929v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly deployed in safety-critical systems where they provide answers based on in-context information derived from knowledge bases. As LLMs are increasingly envisioned as superhuman agents, their proficiency in knowledge comprehension-extracting relevant information and reasoning over it to answer questions, a key facet of human intelligence-becomes crucial. However, existing evaluations of LLMs on knowledge comprehension are typically conducted on small test sets, but these datasets represent only a tiny fraction of the vast number of possible queries. Simple empirical evaluations on these limited test sets raises concerns about the reliability and generalizability of the results. In this work, we introduce the first specification and certification framework for knowledge comprehension in LLMs, providing formal probabilistic guarantees for reliability. Instead of a fixed dataset, we design novel specifications that mathematically represent prohibitively large probability distributions of knowledge comprehension prompts with natural noise, using knowledge graphs. From these specifications, we generate quantitative certificates that offer high-confidence, tight bounds on the probability that a given LLM correctly answers any question drawn from the specification distribution. We apply our framework to certify SOTA LLMs in two domains: precision medicine and general question-answering. Our results reveal previously unrecognized vulnerabilities in SOTA LLMs due to natural noise in the prompts. Additionally, we establish performance hierarchies with formal guarantees among the SOTA LLMs, particularly in the context of precision medicine question-answering.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Multi-Task Offloading for Semantic-Aware Edge Computing Systems</title>
<link>https://arxiv.org/abs/2407.11018</link>
<guid>https://arxiv.org/abs/2407.11018</guid>
<content:encoded><![CDATA[
arXiv:2407.11018v2 Announce Type: replace 
Abstract: Mobile edge computing (MEC) provides low-latency offloading solutions for computationally intensive tasks, effectively improving the computing efficiency and battery life of mobile devices. However, for data-intensive tasks or scenarios with limited uplink bandwidth, network congestion might occur due to massive simultaneous offloading nodes, increasing transmission latency and affecting task performance. In this paper, we propose a semantic-aware multi-modal task offloading framework to address the challenges posed by limited uplink bandwidth. By introducing a semantic extraction factor, we balance the relationship among transmission latency, computation energy consumption, and task performance. To measure the offloading performance of multi-modal tasks, we design a unified and fair quality of experience (QoE) metric that includes execution latency, energy consumption, and task performance. Lastly, we formulate the optimization problem as a Markov decision process (MDP) and exploit the multi-agent proximal policy optimization (MAPPO) reinforcement learning algorithm to jointly optimize the semantic extraction factor, communication resources, and computing resources to maximize overall QoE. Experimental results show that the proposed method achieves a reduction in execution latency and energy consumption of 18.1% and 12.9%, respectively compared with the semantic-unaware approach. Moreover, the proposed approach can be easily extended to models with different user preferences.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Adversarial Inverse Reinforcement Learning via Successor Feature Matching</title>
<link>https://arxiv.org/abs/2411.07007</link>
<guid>https://arxiv.org/abs/2411.07007</guid>
<content:encoded><![CDATA[
arXiv:2411.07007v2 Announce Type: replace 
Abstract: In inverse reinforcement learning (IRL), an agent seeks to replicate expert demonstrations through interactions with the environment. Traditionally, IRL is treated as an adversarial game, where an adversary searches over reward models, and a learner optimizes the reward through repeated RL procedures. This game-solving approach is both computationally expensive and difficult to stabilize. In this work, we propose a novel approach to IRL by direct policy optimization: exploiting a linear factorization of the return as the inner product of successor features and a reward vector, we design an IRL algorithm by policy gradient descent on the gap between the learner and expert features. Our non-adversarial method does not require learning a reward function and can be solved seamlessly with existing actor-critic RL algorithms. Remarkably, our approach works in state-only settings without expert action labels, a setting which behavior cloning (BC) cannot solve. Empirical results demonstrate that our method learns from as few as a single expert demonstration and achieves improved performance on various control tasks.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CityWalker: Learning Embodied Urban Navigation from Web-Scale Videos</title>
<link>https://arxiv.org/abs/2411.17820</link>
<guid>https://arxiv.org/abs/2411.17820</guid>
<content:encoded><![CDATA[
arXiv:2411.17820v3 Announce Type: replace 
Abstract: Navigating dynamic urban environments presents significant challenges for embodied agents, requiring advanced spatial reasoning and adherence to common-sense norms. Despite progress, existing visual navigation methods struggle in map-free or off-street settings, limiting the deployment of autonomous agents like last-mile delivery robots. To overcome these obstacles, we propose a scalable, data-driven approach for human-like urban navigation by training agents on thousands of hours of in-the-wild city walking and driving videos sourced from the web. We introduce a simple and scalable data processing pipeline that extracts action supervision from these videos, enabling large-scale imitation learning without costly annotations. Our model learns sophisticated navigation policies to handle diverse challenges and critical scenarios. Experimental results show that training on large-scale, diverse datasets significantly enhances navigation performance, surpassing current methods. This work shows the potential of using abundant online video data to develop robust navigation policies for embodied agents in dynamic urban settings. Project homepage is at https://ai4ce.github.io/CityWalker/.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Codenames as a Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2412.11373</link>
<guid>https://arxiv.org/abs/2412.11373</guid>
<content:encoded><![CDATA[
arXiv:2412.11373v2 Announce Type: replace 
Abstract: In this paper, we propose the use of the popular word-based board game Codenames as a suitable benchmark for evaluating the reasoning capabilities of Large Language Models (LLMs). Codenames presents a highly interesting challenge for achieving successful AI performance, requiring both a sophisticated understanding of language, theory of mind, and epistemic reasoning capabilities. Prior attempts to develop agents for Codenames have largely relied on word embedding techniques, which have a limited vocabulary range and perform poorly when paired with differing approaches. LLMs have demonstrated enhanced reasoning and comprehension capabilities for language-based tasks, but can still suffer in lateral thinking challenges. We evaluate the capabilities of several state-of-the-art LLMs, including GPT-4o, Gemini 1.5, Claude 3.5 Sonnet, and Llama 3.1, across a variety of board setups. Our results indicate that while certain LLMs perform better than others overall, different models exhibit varying emergent behaviours during gameplay and excel at specific roles. We also evaluate the performance of different combinations of LLMs when playing cooperatively together, demonstrating that LLM agents are more generalisable to a wider range of teammates than prior techniques.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Language for Coordination: A Framework and Benchmark for LLM-Driven Multi-Agent Control</title>
<link>https://arxiv.org/abs/2412.11761</link>
<guid>https://arxiv.org/abs/2412.11761</guid>
<content:encoded><![CDATA[
arXiv:2412.11761v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across various tasks. Their potential to facilitate human coordination with many agents is a promising but largely under-explored area. Such capabilities would be helpful in disaster response, urban planning, and real-time strategy scenarios. In this work, we introduce (1) a real-time strategy game benchmark designed to evaluate these abilities and (2) a novel framework we term HIVE. HIVE empowers a single human to coordinate swarms of up to 2,000 agents through a natural language dialog with an LLM. We present promising results on this multi-agent benchmark, with our hybrid approach solving tasks such as coordinating agent movements, exploiting unit weaknesses, leveraging human annotations, and understanding terrain and strategic points. Our findings also highlight critical limitations of current models, including difficulties in processing spatial visual information and challenges in formulating long-term strategic plans. This work sheds light on the potential and limitations of LLMs in human-swarm coordination, paving the way for future research in this area. The HIVE project page, hive.syrkis.com, includes videos of the system in action.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stable Hypergraph Matching in Unimodular Hypergraphs</title>
<link>https://arxiv.org/abs/2502.08827</link>
<guid>https://arxiv.org/abs/2502.08827</guid>
<content:encoded><![CDATA[
arXiv:2502.08827v2 Announce Type: replace 
Abstract: We study the NP-hard Stable Hypergraph Matching (SHM) problem and its generalization allowing capacities, the Stable Hypergraph $b$-Matching (SH$b$M) problem, and investigate their computational properties under various structural constraints. Our study is motivated by the fact that Scarf's Lemma (Scarf, 1967) together with a result of Lov\'asz (1972) guarantees the existence of a stable matching whenever the underlying hypergraph is normal. Furthermore, if the hypergraph is unimodular (i.e., its incidence matrix is totally unimodular), then even a stable $b$-matching is guaranteed to exist. However, no polynomial-time algorithm is known for finding a stable matching or $b$-matching in unimodular hypergraphs.
  We identify subclasses of unimodular hypergraphs where SHM and SH$b$M are tractable such as laminar hypergraphs or so-called subpath hypergraphs with bounded-size hyperedges; for the latter case, even a maximum-weight stable $b$-matching can be found efficiently. We complement our algorithms by showing that optimizing over stable matchings is NP-hard even in laminar hypergraphs. As a practically important special case of SH$b$M for unimodular hypergraphs, we investigate a tripartite stable matching problem with students, schools, and companies as agents, called the University Dual Admission problem, which models real-world scenarios in higher education admissions.
  Finally, we examine a superclass of subpath hypergraphs that are normal but necessarily not unimodular, namely subtree hypergraphs where hyperedges correspond to subtrees of a tree. We establish that for such hypergraphs, stable matchings can be found in polynomial time but, in the setting with capacities, finding a stable $b$-matching is NP-hard.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Robustness of Multimodal Agents Against Active Environmental Injection Attacks</title>
<link>https://arxiv.org/abs/2502.13053</link>
<guid>https://arxiv.org/abs/2502.13053</guid>
<content:encoded><![CDATA[
arXiv:2502.13053v2 Announce Type: replace 
Abstract: As researchers continue to optimize AI agents for more effective task execution within operating systems, they often overlook a critical security concern: the ability of these agents to detect "impostors" within their environment. Through an analysis of the agents' operational context, we identify a significant threat-attackers can disguise malicious attacks as environmental elements, injecting active disturbances into the agents' execution processes to manipulate their decision-making. We define this novel threat as the Active Environment Injection Attack (AEIA). Focusing on the interaction mechanisms of the Android OS, we conduct a risk assessment of AEIA and identify two critical security vulnerabilities: (1) Adversarial content injection in multimodal interaction interfaces, where attackers embed adversarial instructions within environmental elements to mislead agent decision-making; and (2) Reasoning gap vulnerabilities in the agent's task execution process, which increase susceptibility to AEIA attacks during reasoning. To evaluate the impact of these vulnerabilities, we propose AEIA-MN, an attack scheme that exploits interaction vulnerabilities in mobile operating systems to assess the robustness of MLLM-based agents. Experimental results show that even advanced MLLMs are highly vulnerable to this attack, achieving a maximum attack success rate of 93% on the AndroidWorld benchmark by combining two vulnerabilities.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks</title>
<link>https://arxiv.org/abs/2503.09572</link>
<guid>https://arxiv.org/abs/2503.09572</guid>
<content:encoded><![CDATA[
arXiv:2503.09572v3 Announce Type: replace 
Abstract: Large language models (LLMs) have shown remarkable advancements in enabling language agents to tackle simple tasks. However, applying them for complex, multi-step, long-horizon tasks remains a challenge. Recent work have found success by separating high-level planning from low-level execution, which enables the model to effectively balance high-level planning objectives and low-level execution details. However, generating accurate plans remains difficult since LLMs are not inherently trained for this task. To address this, we propose Plan-and-Act, a novel framework that incorporates explicit planning into LLM-based agents and introduces a scalable method to enhance plan generation through a novel synthetic data generation method. Plan-and-Act consists of a Planner model which generates structured, high-level plans to achieve user goals, and an Executor model that translates these plans into environment-specific actions. To train the Planner effectively, we introduce a synthetic data generation method that annotates ground-truth trajectories with feasible plans, augmented with diverse and extensive examples to enhance generalization. We evaluate Plan-and-Act using web navigation as a representative long-horizon planning environment, demonstrating a state-of-the-art 57.58% success rate on the WebArena-Lite benchmark as well as a text-only state-of-the-art 81.36% success rate on WebVoyager.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Role-Selection Game in Block Production under Proposer-Builder Separation</title>
<link>https://arxiv.org/abs/2503.15184</link>
<guid>https://arxiv.org/abs/2503.15184</guid>
<content:encoded><![CDATA[
arXiv:2503.15184v2 Announce Type: replace 
Abstract: To address the risks of validator centralization, Proposer-Builder Separation (PBS) was introduced in Ethereum to divide the roles of block building and block proposing, fostering a more equitable and decentralized block production environment. PBS creates a two-sided market in which searchers submit valuable bundles to builders for inclusion in blocks, while builders compete in auctions for block proposals. In this paper, we formulate and analyze a role-selection game that models how profit-seeking participants in PBS strategically choose between acting as searchers or builders, using a co-evolutionary framework to capture the complex interactions and payoff dynamics in this market. Through agent-based simulations, we demonstrate that agents' optimal role-acting as searcher or builder-responds dynamically to the probability of conflict between bundles. Our empirical game-theoretic analysis quantifies the equilibrium frequencies of role selection under different market conditions, revealing that low conflict probabilities lead to equilibria dominated by searchers, while higher probabilities shift equilibrium toward builders. Additionally, bundle conflicts have non-monotonic effects on agent payoffs and strategy evolution. Our results advance the understanding of decentralized block building and provide guidance for designing fairer and more robust block production mechanisms in blockchain systems.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LOKA Protocol: A Decentralized Framework for Trustworthy and Ethical AI Agent Ecosystems</title>
<link>https://arxiv.org/abs/2504.10915</link>
<guid>https://arxiv.org/abs/2504.10915</guid>
<content:encoded><![CDATA[
arXiv:2504.10915v2 Announce Type: replace 
Abstract: The rise of autonomous AI agents, capable of perceiving, reasoning, and acting independently, signals a profound shift in how digital ecosystems operate, govern, and evolve. As these agents proliferate beyond centralized infrastructures, they expose foundational gaps in identity, accountability, and ethical alignment. Three critical questions emerge: Identity: Who or what is the agent? Accountability: Can its actions be verified, audited, and trusted? Ethical Consensus: Can autonomous systems reliably align with human values and prevent harmful emergent behaviors? We present the novel LOKA Protocol (Layered Orchestration for Knowledgeful Agents), a unified, systems-level architecture for building ethically governed, interoperable AI agent ecosystems. LOKA introduces a proposed Universal Agent Identity Layer (UAIL) for decentralized, verifiable identity; intent-centric communication protocols for semantic coordination across diverse agents; and a Decentralized Ethical Consensus Protocol (DECP) that could enable agents to make context-aware decisions grounded in shared ethical baselines. Anchored in emerging standards such as Decentralized Identifiers (DIDs), Verifiable Credentials (VCs), and post-quantum cryptography, LOKA proposes a scalable, future-resilient blueprint for multi-agent AI governance. By embedding identity, trust, and ethics into the protocol layer itself, LOKA proposes the foundation for a new era of responsible, transparent, and autonomous AI ecosystems operating across digital and physical domains.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Graph-Based Reinforcement Learning Approach with Frontier Potential Based Reward for Safe Cluttered Environment Exploration</title>
<link>https://arxiv.org/abs/2504.11907</link>
<guid>https://arxiv.org/abs/2504.11907</guid>
<content:encoded><![CDATA[
arXiv:2504.11907v2 Announce Type: replace 
Abstract: Autonomous exploration of cluttered environments requires efficient exploration strategies that guarantee safety against potential collisions with unknown random obstacles. This paper presents a novel approach combining a graph neural network-based exploration greedy policy with a safety shield to ensure safe navigation goal selection. The network is trained using reinforcement learning and the proximal policy optimization algorithm to maximize exploration efficiency while reducing the safety shield interventions. However, if the policy selects an infeasible action, the safety shield intervenes to choose the best feasible alternative, ensuring system consistency. Moreover, this paper proposes a reward function that includes a potential field based on the agent's proximity to unexplored regions and the expected information gain from reaching them. Overall, the approach investigated in this paper merges the benefits of the adaptability of reinforcement learning-driven exploration policies and the guarantee ensured by explicit safety mechanisms. Extensive evaluations in simulated environments demonstrate that the approach enables efficient and safe exploration in cluttered environments.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Ask One When You Can Ask $k$? Two-Stage Learning-to-Defer to the Top-$k$ Experts</title>
<link>https://arxiv.org/abs/2504.12988</link>
<guid>https://arxiv.org/abs/2504.12988</guid>
<content:encoded><![CDATA[
arXiv:2504.12988v2 Announce Type: replace 
Abstract: Learning-to-Defer (L2D) enables decision-making systems to improve reliability by selectively deferring uncertain predictions to more competent agents. However, most existing approaches focus exclusively on single-agent deferral, which is often inadequate in high-stakes scenarios that require collective expertise. We propose Top-$k$ Learning-to-Defer, a generalization of the classical two-stage L2D framework that allocates each query to the $k$ most confident agents instead of a single one. To further enhance flexibility and cost-efficiency, we introduce Top-$k(x)$ Learning-to-Defer, an adaptive extension that learns the optimal number of agents to consult for each query, based on input complexity, agent competency distributions, and consultation costs. For both settings, we derive a novel surrogate loss and prove that it is Bayes-consistent and $(\mathcal{R}, \mathcal{G})$-consistent, ensuring convergence to the Bayes-optimal allocation. Notably, we show that the well-established model cascades paradigm arises as a restricted instance of our Top-$k$ and Top-$k(x)$ formulations. Extensive experiments across diverse benchmarks demonstrate the effectiveness of our framework on both classification and regression tasks.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence Clones</title>
<link>https://arxiv.org/abs/2501.16996</link>
<guid>https://arxiv.org/abs/2501.16996</guid>
<content:encoded><![CDATA[
arXiv:2501.16996v4 Announce Type: replace-cross 
Abstract: Large language models, trained on personal data, may soon be able to mimic individual personalities. These ``AI clones'' or ``AI agents'' have the potential to transform how people search over one another in contexts ranging from marriage to employment -- indeed, several dating platforms have already begun using AI clones to evaluate potential pairings between users. This paper presents a theoretical framework to study the tradeoff between the substantially expanded search capacity of AI clones, and their imperfect representation of humans. Individual personalities are modeled as points in $k$-dimensional Euclidean space, and their AI clones are modeled as noisy approximations of these personalities. I compare two search regimes: an ``in-person regime'' -- where each person randomly meets some number of individuals and matches to the most compatible among them -- against an ``AI representation regime'' -- in which individuals match to the person whose AI clone is most compatible with their AI clone. I show that a finite number of in-person encounters exceeds the expected payoff from search over infinite AI clones. Moreover, when the dimensionality of personality is large, simply meeting two people in person produces a better expected match than entrusting the process to an AI platform, regardless of the size of its candidate pool.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Flow Integrity to Prevent Privilege Escalation in LLM Agents</title>
<link>https://arxiv.org/abs/2503.15547</link>
<guid>https://arxiv.org/abs/2503.15547</guid>
<content:encoded><![CDATA[
arXiv:2503.15547v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are combined with tools to create powerful LLM agents that provide a wide range of services. Unlike traditional software, LLM agent's behavior is determined at runtime by natural language prompts from either user or tool's data. This flexibility enables a new computing paradigm with unlimited capabilities and programmability, but also introduces new security risks, vulnerable to privilege escalation attacks. Moreover, user prompts are prone to be interpreted in an insecure way by LLM agents, creating non-deterministic behaviors that can be exploited by attackers. To address these security risks, we propose Prompt Flow Integrity (PFI), a system security-oriented solution to prevent privilege escalation in LLM agents. Analyzing the architectural characteristics of LLM agents, PFI features three mitigation techniques -- i.e., agent isolation, secure untrusted data processing, and privilege escalation guardrails. Our evaluation result shows that PFI effectively mitigates privilege escalation attacks while successfully preserving the utility of LLM agents.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Balancing Preference and Performance through Adaptive Personalized Explainability</title>
<link>https://arxiv.org/abs/2504.13856</link>
<guid>https://arxiv.org/abs/2504.13856</guid>
<content:encoded><![CDATA[
arXiv:2504.13856v1 Announce Type: new 
Abstract: As robots and digital assistants are deployed in the real world, these agents must be able to communicate their decision-making criteria to build trust, improve human-robot teaming, and enable collaboration. While the field of explainable artificial intelligence (xAI) has made great strides to enable such communication, these advances often assume that one xAI approach is ideally suited to each problem (e.g., decision trees to explain how to triage patients in an emergency or feature-importance maps to explain radiology reports). This fails to recognize that users have diverse experiences or preferences for interaction modalities. In this work, we present two user-studies set in a simulated autonomous vehicle (AV) domain. We investigate (1) population-level preferences for xAI and (2) personalization strategies for providing robot explanations. We find significant differences between xAI modes (language explanations, feature-importance maps, and decision trees) in both preference (p < 0.01) and performance (p < 0.05). We also observe that a participant's preferences do not always align with their performance, motivating our development of an adaptive personalization strategy to balance the two. We show that this strategy yields significant performance gains (p < 0.05), and we conclude with a discussion of our findings and implications for xAI in human-robot interactions.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark</title>
<link>https://arxiv.org/abs/2504.13861</link>
<guid>https://arxiv.org/abs/2504.13861</guid>
<content:encoded><![CDATA[
arXiv:2504.13861v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) are increasingly being explored for applications in telemedicine, yet their ability to engage with diverse patient behaviors remains underexplored. We introduce 3MDBench (Medical Multimodal Multi-agent Dialogue Benchmark), an open-source evaluation framework designed to assess LLM-driven medical consultations. Unlike existing benchmarks, 3MDBench simulates real-world patient variability by incorporating four temperament-driven Patient Agents and an Assessor Agent that evaluates diagnostic accuracy and dialogue quality. The benchmark integrates textual and image-based patient data across 34 common diagnoses, mirroring real-world telemedicine interactions. Under different diagnostic strategies, we evaluate state-of-the-art LVLMs. Our findings demonstrate that incorporating dialogue improves the F1 score from 50.4 to 54.2 compared to non-dialogue settings, underscoring the value of context-driven, information-seeking questioning. Additionally, we demonstrate that multimodal inputs enhance diagnostic efficiency. Image-supported models outperform text-only counterparts by raising the diagnostic F1 score from 52.8 to 54.2 in a similar dialogue setting. Finally, we suggest an approach that improves the diagnostic F1-score to 70.3 by training the CNN model on the diagnosis prediction task and incorporating its top-3 predictions into the LVLM context. 3MDBench provides a reproducible and extendable evaluation framework for AI-driven medical assistants. It offers insights into how patient temperament, dialogue strategies, and multimodal reasoning influence diagnosis quality. By addressing real-world complexities in telemedicine, our benchmark paves the way for more empathetic, reliable, and context-aware AI-driven healthcare solutions. The source code of our benchmark is publicly available: https://github.com/univanxx/3mdbench
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human aversion? Do AI Agents Judge Identity More Harshly Than Performance</title>
<link>https://arxiv.org/abs/2504.13871</link>
<guid>https://arxiv.org/abs/2504.13871</guid>
<content:encoded><![CDATA[
arXiv:2504.13871v1 Announce Type: new 
Abstract: This study examines the understudied role of algorithmic evaluation of human judgment in hybrid decision-making systems, a critical gap in management research. While extant literature focuses on human reluctance to follow algorithmic advice, we reverse the perspective by investigating how AI agents based on large language models (LLMs) assess and integrate human input. Our work addresses a pressing managerial constraint: firms barred from deploying LLMs directly due to privacy concerns can still leverage them as mediating tools (for instance, anonymized outputs or decision pipelines) to guide high-stakes choices like pricing or discounts without exposing proprietary data. Through a controlled prediction task, we analyze how an LLM-based AI agent weights human versus algorithmic predictions. We find that the AI system systematically discounts human advice, penalizing human errors more severely than algorithmic errors--a bias exacerbated when the agent's identity (human vs AI) is disclosed and the human is positioned second. These results reveal a disconnect between AI-generated trust metrics and the actual influence of human judgment, challenging assumptions about equitable human-AI collaboration. Our findings offer three key contributions. First, we identify a reverse algorithm aversion phenomenon, where AI agents undervalue human input despite comparable error rates. Second, we demonstrate how disclosure and positional bias interact to amplify this effect, with implications for system design. Third, we provide a framework for indirect LLM deployment that balances predictive power with data privacy. For practitioners, this research emphasize the need to audit AI weighting mechanisms, calibrate trust dynamics, and strategically design decision sequences in human-AI systems.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants</title>
<link>https://arxiv.org/abs/2504.13887</link>
<guid>https://arxiv.org/abs/2504.13887</guid>
<content:encoded><![CDATA[
arXiv:2504.13887v1 Announce Type: new 
Abstract: Despite the growing integration of AI chatbots as conversational agents in public discourse, empirical evidence regarding their capacity to foster intercultural empathy remains limited. Using a randomized dialogue experiment, we examined how different types of AI chatbot interaction, i.e., deliberative versus non-deliberative and culturally aligned versus non-aligned, affect intercultural empathy across cultural groups. Results show that deliberative conversations increased intercultural empathy among American participants but not Latin American participants, who perceived AI responses as culturally inaccurate and failing to represent their cultural contexts and perspectives authentically. Real-time interaction analyses reveal that these differences stem from cultural knowledge gaps inherent in Large Language Models. Despite explicit prompting and instruction to represent cultural perspectives in participants' native languages, AI systems still exhibit significant disparities in cultural representation. This highlights the importance of designing AI systems capable of culturally authentic engagement in deliberative conversations. Our study contributes to deliberation theory and AI alignment research by underscoring AI's role in intercultural dialogue and the persistent challenge of representational asymmetry in democratic discourse.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Show Me How: Benefits and Challenges of Agent-Augmented Counterfactual Explanations for Non-Expert Users</title>
<link>https://arxiv.org/abs/2504.13897</link>
<guid>https://arxiv.org/abs/2504.13897</guid>
<content:encoded><![CDATA[
arXiv:2504.13897v1 Announce Type: new 
Abstract: Counterfactual explanations offer actionable insights by illustrating how changes to inputs can lead to different outcomes. However, these explanations often suffer from ambiguity and impracticality, limiting their utility for non-expert users with limited AI knowledge. Augmenting counterfactual explanations with Large Language Models (LLMs) has been proposed as a solution, but little research has examined their benefits and challenges for non-experts. To address this gap, we developed a healthcare-focused system that leverages conversational AI agents to enhance counterfactual explanations, offering clear, actionable recommendations to help patients at high risk of cardiovascular disease (CVD) reduce their risk. Evaluated through a mixed-methods study with 34 participants, our findings highlight the effectiveness of agent-augmented counterfactuals in improving actionable recommendations. Results further indicate that users with prior experience using conversational AI demonstrated greater effectiveness in utilising these explanations compared to novices. Furthermore, this paper introduces a set of generic guidelines for creating augmented counterfactual explanations, incorporating safeguards to mitigate common LLM pitfalls, such as hallucinations, and ensuring the explanations are both actionable and contextually relevant for non-expert users.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Human Robot Social Interaction (HSRI) Dataset: Benchmarking Foundational Models' Social Reasoning</title>
<link>https://arxiv.org/abs/2504.13898</link>
<guid>https://arxiv.org/abs/2504.13898</guid>
<content:encoded><![CDATA[
arXiv:2504.13898v1 Announce Type: new 
Abstract: Our work aims to advance the social reasoning of embodied artificial intelligence (AI) agents in real-world social interactions. Recently, language models (LMs) and foundational models (FMs) are being utilized as automatic evaluators of human-AI interactions with the goal of eventually being used to improve the policy of the AI agent. To enable further research in this direction, we introduce a large-scale real-world Human Robot Social Interaction (HSRI) Dataset to benchmark the capabilities of LMs and FMs to identify and reason about social interactions, specifically with regard to robot social errors and competencies . Our dataset consists of 400 real-world human social robot interaction videos and over 10K annotations, detailing the robot's social errors, competencies, rationale, and corrective actions, capturing unique aspects of human-AI interaction only present in real-world interactions. To further assess AI models' ability to reason about social interactions, we propose eight new benchmark tasks for evaluating centered around whether AI models can (1) evaluate social interactions via detecting social errors and competencies, (2) identify the explanatory factors associated to errors and competencies, (3) understand the flow of real-world social interactions, and (4) provide reasons and corrective actions for social errors. Human studies and experiments with modern LMs and FMs reveal that current models struggle with these tasks, demonstrating that our dataset and benchmark provides a step forward towards socially intelligent AI.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Assisted Conversational Interviewing: Effects on Data Quality and User Experience</title>
<link>https://arxiv.org/abs/2504.13908</link>
<guid>https://arxiv.org/abs/2504.13908</guid>
<content:encoded><![CDATA[
arXiv:2504.13908v1 Announce Type: new 
Abstract: Standardized surveys scale efficiently but sacrifice depth, while conversational interviews improve response quality at the cost of scalability and consistency. This study bridges the gap between these methods by introducing a framework for AI-assisted conversational interviewing. To evaluate this framework, we conducted a web survey experiment where 1,800 participants were randomly assigned to text-based conversational AI agents, or "textbots", to dynamically probe respondents for elaboration and interactively code open-ended responses. We assessed textbot performance in terms of coding accuracy, response quality, and respondent experience. Our findings reveal that textbots perform moderately well in live coding even without survey-specific fine-tuning, despite slightly inflated false positive errors due to respondent acquiescence bias. Open-ended responses were more detailed and informative, but this came at a slight cost to respondent experience. Our findings highlight the feasibility of using AI methods to enhance open-ended data collection in web surveys.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViMo: A Generative Visual GUI World Model for App Agent</title>
<link>https://arxiv.org/abs/2504.13936</link>
<guid>https://arxiv.org/abs/2504.13936</guid>
<content:encoded><![CDATA[
arXiv:2504.13936v1 Announce Type: new 
Abstract: App agents, which autonomously operate mobile Apps through Graphical User Interfaces (GUIs), have gained significant interest in real-world applications. Yet, they often struggle with long-horizon planning, failing to find the optimal actions for complex tasks with longer steps. To address this, world models are used to predict the next GUI observation based on user actions, enabling more effective agent planning. However, existing world models primarily focus on generating only textual descriptions, lacking essential visual details. To fill this gap, we propose ViMo, the first visual world model designed to generate future App observations as images. For the challenge of generating text in image patches, where even minor pixel errors can distort readability, we decompose GUI generation into graphic and text content generation. We propose a novel data representation, the Symbolic Text Representation~(STR) to overlay text content with symbolic placeholders while preserving graphics. With this design, ViMo employs a STR Predictor to predict future GUIs' graphics and a GUI-text Predictor for generating the corresponding text. Moreover, we deploy ViMo to enhance agent-focused tasks by predicting the outcome of different action options. Experiments show ViMo's ability to generate visually plausible and functionally effective GUIs that enable App agents to make more informed decisions.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tinker Tales: Interactive Storytelling Framework for Early Childhood Narrative Development and AI Literacy</title>
<link>https://arxiv.org/abs/2504.13969</link>
<guid>https://arxiv.org/abs/2504.13969</guid>
<content:encoded><![CDATA[
arXiv:2504.13969v1 Announce Type: new 
Abstract: This paper presents Tinker Tales, an interactive storytelling framework in the format of a board game, designed to support both narrative development and AI literacy in early childhood. The framework integrates tangible and speech-based interactions with AI through NFC chip-attached pawns and tokens, along with a speaker and microphone. Children select and define key story elements-such as characters, places, items, and emotions-using the pawns and tokens, providing further details to the AI and receiving proper assistance, similar to how adults prompt AI for specific tasks (e.g., writing). For evaluation, several game sessions were simulated with a child AI agent, and the quality and safety of the generated stories were assessed from various perspectives. This work highlights the potential of combining physical and digital elements in AI literacy, offering a safe and engaging way for children to learn how to effectively collaborate with AI.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Going Whole Hog: A Philosophical Defense of AI Cognition</title>
<link>https://arxiv.org/abs/2504.13988</link>
<guid>https://arxiv.org/abs/2504.13988</guid>
<content:encoded><![CDATA[
arXiv:2504.13988v1 Announce Type: new 
Abstract: This work defends the 'Whole Hog Thesis': sophisticated Large Language Models (LLMs) like ChatGPT are full-blown linguistic and cognitive agents, possessing understanding, beliefs, desires, knowledge, and intentions. We argue against prevailing methodologies in AI philosophy, rejecting starting points based on low-level computational details ('Just an X' fallacy) or pre-existing theories of mind. Instead, we advocate starting with simple, high-level observations of LLM behavior (e.g., answering questions, making suggestions) -- defending this data against charges of metaphor, loose talk, or pretense. From these observations, we employ 'Holistic Network Assumptions' -- plausible connections between mental capacities (e.g., answering implies knowledge, knowledge implies belief, action implies intention) -- to argue for the full suite of cognitive states. We systematically rebut objections based on LLM failures (hallucinations, planning/reasoning errors), arguing these don't preclude agency, often mirroring human fallibility. We address numerous 'Games of Lacks', arguing that LLMs do not lack purported necessary conditions for cognition (e.g., semantic grounding, embodiment, justification, intrinsic intentionality) or that these conditions are not truly necessary, often relying on anti-discriminatory arguments comparing LLMs to diverse human capacities. Our approach is evidential, not functionalist, and deliberately excludes consciousness. We conclude by speculating on the possibility of LLMs possessing 'alien' contents beyond human conceptual schemes.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Evacuation in Graphs with Multiple Exits</title>
<link>https://arxiv.org/abs/2504.14052</link>
<guid>https://arxiv.org/abs/2504.14052</guid>
<content:encoded><![CDATA[
arXiv:2504.14052v1 Announce Type: new 
Abstract: Consider the following discrete evacuation model. The evacuation terrain is modeled by a simple graph $G=(V,E)$ whose certain vertices $X\subseteq V$ are called \emph{exits}. Initially, each vertex is either \emph{empty} or \emph{occupied} by an agent. We assume that each vertex has a unique \emph{id} (and therefore the agents do have unique ids), each agent has finite but arbitrarily large memory, and the graph is initially stored in the memory of each agent. In other words, the agents do know the topology of the network along with the locations of the exits, but they do not know the initial positions nor the quantity of other agents. The time is divided into \emph{steps}; in each step any pair of agents present at vertices at a distance of at most two can exchange an arbitrary number of messages, and then each agent can either make a move or stay put. The agents should make moves in a collision-free manner, i.e., no two agents can be located at the same vertex in the same step. At the end of each step, any agent located at an exit \emph{evacuates}, i.e., it is removed from the graph. The goal is to provide an algorithm to the agents (referred to as an evacuation strategy) that ensures the evacuation of all agents and minimizes the number of steps.
  This work provides an algorithmic framework that allows constructing valid evacuation strategies for arbitrary input graphs. Specifically, we focus on the properties of the input graphs that lead to evacuation strategies with constant competitive ratios. In particular, we describe an application of the above framework that gives an asymptotically optimal evacuation for grids (and by extension hexagonal or triangular grids as well).
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DoomArena: A framework for Testing AI Agents Against Evolving Security Threats</title>
<link>https://arxiv.org/abs/2504.14064</link>
<guid>https://arxiv.org/abs/2504.14064</guid>
<content:encoded><![CDATA[
arXiv:2504.14064v1 Announce Type: new 
Abstract: We present DoomArena, a security evaluation framework for AI agents. DoomArena is designed on three principles: 1) It is a plug-in framework and integrates easily into realistic agentic frameworks like BrowserGym (for web agents) and $\tau$-bench (for tool calling agents); 2) It is configurable and allows for detailed threat modeling, allowing configuration of specific components of the agentic framework being attackable, and specifying targets for the attacker; and 3) It is modular and decouples the development of attacks from details of the environment in which the agent is deployed, allowing for the same attacks to be applied across multiple environments. We illustrate several advantages of our framework, including the ability to adapt to new threat models and environments easily, the ability to easily combine several previously published attacks to enable comprehensive and fine-grained security testing, and the ability to analyze trade-offs between various vulnerabilities and performance. We apply DoomArena to state-of-the-art (SOTA) web and tool-calling agents and find a number of surprising results: 1) SOTA agents have varying levels of vulnerability to different threat models (malicious user vs malicious environment), and there is no Pareto dominant agent across all threat models; 2) When multiple attacks are applied to an agent, they often combine constructively; 3) Guardrail model-based defenses seem to fail, while defenses based on powerful SOTA LLMs work better. DoomArena is available at https://github.com/ServiceNow/DoomArena.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TALES: Text Adventure Learning Environment Suite</title>
<link>https://arxiv.org/abs/2504.14128</link>
<guid>https://arxiv.org/abs/2504.14128</guid>
<content:encoded><![CDATA[
arXiv:2504.14128v1 Announce Type: new 
Abstract: Reasoning is an essential skill to enable Large Language Models (LLMs) to interact with the world. As tasks become more complex, they demand increasingly sophisticated and diverse reasoning capabilities for sequential decision-making, requiring structured reasoning over the context history to determine the next best action. We introduce TALES, a diverse collection of synthetic and human-written text-adventure games designed to challenge and evaluate diverse reasoning capabilities. We present results over a range of LLMs, open- and closed-weights, performing a qualitative analysis on the top performing models. Despite an impressive showing on synthetic games, even the top LLM-driven agents fail to achieve 15% on games designed for human enjoyment. Code and visualization of the experiments can be found at https://microsoft.github.io/tales.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>tAIfa: Enhancing Team Effectiveness and Cohesion with AI-Generated Automated Feedback</title>
<link>https://arxiv.org/abs/2504.14222</link>
<guid>https://arxiv.org/abs/2504.14222</guid>
<content:encoded><![CDATA[
arXiv:2504.14222v1 Announce Type: new 
Abstract: Providing timely and actionable feedback is crucial for effective collaboration, learning, and coordination within teams. However, many teams face challenges in receiving feedback that aligns with their goals and promotes cohesion. We introduce tAIfa (``Team AI Feedback Assistant''), an AI agent that uses Large Language Models (LLMs) to provide personalized, automated feedback to teams and their members. tAIfa analyzes team interactions, identifies strengths and areas for improvement, and delivers targeted feedback based on communication patterns. We conducted a between-subjects study with 18 teams testing whether using tAIfa impacted their teamwork. Our findings show that tAIfa improved communication and contributions within the teams. This paper contributes to the Human-AI Interaction literature by presenting a computational framework that integrates LLMs to provide automated feedback, introducing tAIfa as a tool to enhance team engagement and cohesion, and providing insights into future AI applications to support team collaboration.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Template-Based Financial Report Generation in Agentic and Decomposed Information Retrieval</title>
<link>https://arxiv.org/abs/2504.14233</link>
<guid>https://arxiv.org/abs/2504.14233</guid>
<content:encoded><![CDATA[
arXiv:2504.14233v1 Announce Type: new 
Abstract: Tailoring structured financial reports from companies' earnings releases is crucial for understanding financial performance and has been widely adopted in real-world analytics. However, existing summarization methods often generate broad, high-level summaries, which may lack the precision and detail required for financial reports that typically focus on specific, structured sections. While Large Language Models (LLMs) hold promise, generating reports adhering to predefined multi-section templates remains challenging. This paper investigates two LLM-based approaches popular in industry for generating templated financial reports: an agentic information retrieval (IR) framework and a decomposed IR approach, namely AgenticIR and DecomposedIR. The AgenticIR utilizes collaborative agents prompted with the full template. In contrast, the DecomposedIR approach applies a prompt chaining workflow to break down the template and reframe each section as a query answered by the LLM using the earnings release. To quantitatively assess the generated reports, we evaluated both methods in two scenarios: one using a financial dataset without direct human references, and another with a weather-domain dataset featuring expert-written reports. Experimental results show that while AgenticIR may excel in orchestrating tasks and generating concise reports through agent collaboration, DecomposedIR statistically significantly outperforms AgenticIR approach in providing broader and more detailed coverage in both scenarios, offering reflection on the utilization of the agentic framework in real-world applications.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners</title>
<link>https://arxiv.org/abs/2504.14239</link>
<guid>https://arxiv.org/abs/2504.14239</guid>
<content:encoded><![CDATA[
arXiv:2504.14239v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have powered Graphical User Interface (GUI) Agents, showing promise in automating tasks on computing devices. Recent works have begun exploring reasoning in GUI tasks with encouraging results. However, many current approaches rely on manually designed reasoning templates, which may result in reasoning that is not sufficiently robust and adaptive for complex GUI environments. Meanwhile, some existing agents continue to operate as Reactive Actors, relying primarily on implicit reasoning that may lack sufficient depth for GUI tasks demanding planning and error recovery. We argue that advancing these agents requires a shift from reactive acting towards acting based on deliberate reasoning. To facilitate this transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed through our Actor2Reasoner framework, a reasoning-centric, two-stage training approach designed to progressively evolve agents from Reactive Actors to Deliberative Reasoners. The first stage, Reasoning Injection, focuses on establishing a basic reasoner. We employ Spatial Reasoning Distillation to transfer cross-modal spatial reasoning capabilities from teacher models to MLLMs through trajectories with explicit reasoning steps, enabling models to integrate GUI visual-spatial information with logical reasoning before action generation. The second stage, Deliberation Enhancement, refines the basic reasoner into a deliberative one using Reinforcement Learning. This stage introduces two approaches: Sub-goal Guidance, which rewards models for generating accurate intermediate sub-goals, and Error Recovery Scenario Construction, which creates failure-and-recovery training scenarios from identified prone-to-error steps. Experimental results show InfiGUI-R1 achieves strong performance in GUI grounding and trajectory tasks. Resources at https://github.com/Reallm-Labs/InfiGUI-R1.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Experience-based Refinement of Task Planning Knowledge in Autonomous Robots</title>
<link>https://arxiv.org/abs/2504.14259</link>
<guid>https://arxiv.org/abs/2504.14259</guid>
<content:encoded><![CDATA[
arXiv:2504.14259v1 Announce Type: new 
Abstract: The requirement for autonomous robots to exhibit higher-level cognitive skills by planning and adapting in an ever-changing environment is indeed a great challenge for the AI community. Progress has been made in the automated planning community on refinement and repair of an agent's symbolic knowledge to do task planning in an incomplete or changing environmental model, but these advances up to now have not been transferred to real physical robots. This paper demonstrates how a physical robot can be capable of adapting its symbolic knowledge of the environment, by using experiences in robot action execution to drive knowledge refinement and hence to improve the success rate of the task plans the robot creates. To implement more robust planning systems, we propose a method for refining domain knowledge to improve the knowledge on which intelligent robot behavior is based. This architecture has been implemented and evaluated using a NAO robot. The refined knowledge leads to the future synthesis of task plans which demonstrate decreasing rates of failure over time as faulty knowledge is removed or adjusted.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory</title>
<link>https://arxiv.org/abs/2504.14325</link>
<guid>https://arxiv.org/abs/2504.14325</guid>
<content:encoded><![CDATA[
arXiv:2504.14325v1 Announce Type: new 
Abstract: Letting AI agents interact in multi-agent applications adds a layer of complexity to the interpretability and prediction of AI outcomes, with profound implications for their trustworthy adoption in research and society. Game theory offers powerful models to capture and interpret strategic interaction among agents, but requires the support of reproducible, standardized and user-friendly IT frameworks to enable comparison and interpretation of results. To this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition using Game Theory. We describe its implementation and usage, and we employ it to uncover biased outcomes in popular games among AI agents, depending on the employed Large Language Model (LLM) and used language, as well as on the personality trait or strategic knowledge of the agents. Overall, FAIRGAME allows users to reliably and easily simulate their desired games and scenarios and compare the results across simulation campaigns and with game-theoretic predictions, enabling the systematic discovery of biases, the anticipation of emerging behavior out of strategic interplays, and empowering further research into strategic decision-making using LLM agents.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-based Dynamic Contract for Federated AI Agent Construction in Mobile Metaverses</title>
<link>https://arxiv.org/abs/2504.14326</link>
<guid>https://arxiv.org/abs/2504.14326</guid>
<content:encoded><![CDATA[
arXiv:2504.14326v1 Announce Type: new 
Abstract: Mobile metaverses have attracted significant attention from both academia and industry, which are envisioned as the next-generation Internet, providing users with immersive and ubiquitous metaverse services through mobile devices. Driven by Large Language Models (LLMs) and Vision-Language Models (VLMs), Artificial Intelligence (AI) agents hold the potential to empower the creation, maintenance, and evolution of mobile metaverses. Currently, AI agents are primarily constructed using cloud-based LLMs and VLMs. However, several challenges hinder their effective implementation, including high service latency and potential sensitive data leakage during perception and processing. In this paper, we develop an edge-cloud collaboration-based federated AI agent construction framework in mobile metaverses. Specifically, Edge Servers (ESs), acting as agent infrastructures, collaboratively create agent modules in a distributed manner. The cloud server then integrates these modules into AI agents and deploys them at the edge, thereby enabling low-latency AI agent services for users. Considering that ESs may exhibit dynamic levels of willingness to participate in federated AI agent construction, we design a two-period dynamic contract model to continuously motivate ESs to participate in agent module creation, effectively addressing the dynamic information asymmetry between the cloud server and the ESs. Furthermore, we propose an Enhanced Diffusion Model-based Soft Actor-Critic (EDMSAC) algorithm to efficiently generate optimal dynamic contracts, in which dynamic structured pruning is applied to DM-based actor networks to enhance denoising efficiency and policy learning performance. Extensive simulations demonstrate the effectiveness and superiority of the EDMSAC algorithm and the proposed contract model.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Generation of Test Cases from Task Descriptions via History-aware Planning</title>
<link>https://arxiv.org/abs/2504.14336</link>
<guid>https://arxiv.org/abs/2504.14336</guid>
<content:encoded><![CDATA[
arXiv:2504.14336v1 Announce Type: new 
Abstract: In automated web testing, generating test scripts from natural language task descriptions is crucial for enhancing the test generation process. This activity involves creating the correct sequences of actions to form test scripts for future testing activities. Current state-of-the-art approaches are limited in generating these action sequences, as they either demand substantial manual effort for human demonstrations or fail to consider the history of previous web content and actions to decide the next action. In this paper, we introduce HxAgent, an iterative large language model agent planning approach that determines the next action based on: 1) observations of the current contents and feasible actions, 2) short-term memory of previous web states and actions, and 3) long-term experience with (in)correct action sequences. The agent generates a sequence of actions to perform a given task, which is effectively an automated test case to verify the task. We conducted an extensive empirical evaluation of HxAgent using two datasets. On the MiniWoB++ dataset, our approach achieves 97% exact-match accuracy that is comparable to the best baselines while eliminating the need for human demonstrations required by those methods. For complex tasks requiring navigation through multiple actions and screens, HxAgent achieves an average 82% exact-match. On the second dataset, comprising 350 task instances across seven popular websites, including YouTube, LinkedIn, Facebook, and Google, HxAgent achieves high performance, with 87% of the action sequences exactly matching the ground truth and a prefix-match of 93%, outperforming the baseline by 59%.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Manipulating Multimodal Agents via Cross-Modal Prompt Injection</title>
<link>https://arxiv.org/abs/2504.14348</link>
<guid>https://arxiv.org/abs/2504.14348</guid>
<content:encoded><![CDATA[
arXiv:2504.14348v1 Announce Type: new 
Abstract: The emergence of multimodal large language models has redefined the agent paradigm by integrating language and vision modalities with external data sources, enabling agents to better interpret human instructions and execute increasingly complex tasks. However, in this work, we identify a critical yet previously overlooked security vulnerability in multimodal agents: cross-modal prompt injection attacks. To exploit this vulnerability, we propose CrossInject, a novel attack framework in which attackers embed adversarial perturbations across multiple modalities to align with target malicious content, allowing external instructions to hijack the agent's decision-making process and execute unauthorized tasks. Our approach consists of two key components. First, we introduce Visual Latent Alignment, where we optimize adversarial features to the malicious instructions in the visual embedding space based on a text-to-image generative model, ensuring that adversarial images subtly encode cues for malicious task execution. Subsequently, we present Textual Guidance Enhancement, where a large language model is leveraged to infer the black-box defensive system prompt through adversarial meta prompting and generate an malicious textual command that steers the agent's output toward better compliance with attackers' requests. Extensive experiments demonstrate that our method outperforms existing injection attacks, achieving at least a +26.4% increase in attack success rates across diverse tasks. Furthermore, we validate our attack's effectiveness in real-world multimodal autonomous agents, highlighting its potential implications for safety-critical applications.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models</title>
<link>https://arxiv.org/abs/2504.14395</link>
<guid>https://arxiv.org/abs/2504.14395</guid>
<content:encoded><![CDATA[
arXiv:2504.14395v1 Announce Type: new 
Abstract: To develop trustworthy Vision-Language Models (VLMs), it is essential to address adversarial robustness and hallucination mitigation, both of which impact factual accuracy in high-stakes applications such as defense and healthcare. Existing methods primarily focus on either adversarial defense or hallucination post-hoc correction, leaving a gap in unified robustness strategies. We introduce \textbf{Hydra}, an adaptive agentic framework that enhances plug-in VLMs through iterative reasoning, structured critiques, and cross-model verification, improving both resilience to adversarial perturbations and intrinsic model errors. Hydra employs an Action-Critique Loop, where it retrieves and critiques visual information, leveraging Chain-of-Thought (CoT) and In-Context Learning (ICL) techniques to refine outputs dynamically. Unlike static post-hoc correction methods, Hydra adapts to both adversarial manipulations and intrinsic model errors, making it robust to malicious perturbations and hallucination-related inaccuracies. We evaluate Hydra on four VLMs, three hallucination benchmarks, two adversarial attack strategies, and two adversarial defense methods, assessing performance on both clean and adversarial inputs. Results show that Hydra surpasses plug-in VLMs and state-of-the-art (SOTA) dehallucination methods, even without explicit adversarial defenses, demonstrating enhanced robustness and factual consistency. By bridging adversarial resistance and hallucination mitigation, Hydra provides a scalable, training-free solution for improving the reliability of VLMs in real-world applications.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planet as a Brain: Towards Internet of AgentSites based on AIOS Server</title>
<link>https://arxiv.org/abs/2504.14411</link>
<guid>https://arxiv.org/abs/2504.14411</guid>
<content:encoded><![CDATA[
arXiv:2504.14411v1 Announce Type: new 
Abstract: The internet is undergoing a historical transformation from the "Internet of Websites" to the "Internet of AgentSites." While traditional Websites served as the foundation for information hosting and dissemination, a new frontier is emerging where AgentSites serve as the hubs of the internet, where each AgentSite hosts one or more AI agents that receive tasks, address them, and deliver actionable solutions, marking a significant shift in the digital landscape and representing the next generation of online ecosystems. Under this vision, AIOS, the AI Agent Operating System, serves as the server for the development, deployment and execution of AI agents, which is a fundamental infrastructure for the Internet of Agentsites.
  In this paper, we introduce AIOS Server, a runtime framework to host agents and enable global-scale collaboration among decentralized agents. AIOS Server provides a communication protocol leveraging the Model Context Protocol (MCP) and JSON-RPC to enable agent-agent or human-agent interactions. Each AIOS node operates as a server to host and execute agents, while supporting peer-to-peer coordination without reliance on centralized orchestration. Based on AIOS Server, we further present the world's first practically deployed Internet of Agentsites (AIOS-IoA), including AgentHub for agent registration and discovery and AgentChat for interactive communication, at https://planet.aios.foundation. The agent discovery mechanism based on Distributed Hash Tables (DHT) and a Gossip protocol serves as the search engine for the internet of agentsites. This work provides a practical foundation for building the Internet of Agentsites-a new paradigm where autonomous agents become first-class citizens of the web. The implementation is available at https://github.com/agiresearch/AIOS.Server and will be integrated into the AIOS main branch at https://github.com/agiresearch/AIOS.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Enhanced Reinforcement Learning for Power Grid Security Assessment</title>
<link>https://arxiv.org/abs/2504.14412</link>
<guid>https://arxiv.org/abs/2504.14412</guid>
<content:encoded><![CDATA[
arXiv:2504.14412v1 Announce Type: new 
Abstract: The increasingly challenging task of maintaining power grid security requires innovative solutions. Novel approaches using reinforcement learning (RL) agents have been proposed to help grid operators navigate the massive decision space and nonlinear behavior of these complex networks. However, applying RL to power grid security assessment, specifically for combinatorially troublesome contingency analysis problems, has proven difficult to scale. The integration of quantum computing into these RL frameworks helps scale by improving computational efficiency and boosting agent proficiency by leveraging quantum advantages in action exploration and model-based interdependence. To demonstrate a proof-of-concept use of quantum computing for RL agent training and simulation, we propose a hybrid agent that runs on quantum hardware using IBM's Qiskit Runtime. We also provide detailed insight into the construction of parameterized quantum circuits (PQCs) for generating relevant quantum output. This agent's proficiency at maintaining grid stability is demonstrated relative to a benchmark model without quantum enhancement using N-k contingency analysis. Additionally, we offer a comparative assessment of the training procedures for RL models integrated with a quantum backend.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing SIA Development: A Case Study in User-Centered Design for Estuary, a Multimodal Socially Interactive Agent Framework</title>
<link>https://arxiv.org/abs/2504.14427</link>
<guid>https://arxiv.org/abs/2504.14427</guid>
<content:encoded><![CDATA[
arXiv:2504.14427v1 Announce Type: new 
Abstract: This case study presents our user-centered design model for Socially Intelligent Agent (SIA) development frameworks through our experience developing Estuary, an open source multimodal framework for building low-latency real-time socially interactive agents. We leverage the Rapid Assessment Process (RAP) to collect the thoughts of leading researchers in the field of SIAs regarding the current state of the art for SIA development as well as their evaluation of how well Estuary may potentially address current research gaps. We achieve this through a series of end-user interviews conducted by a fellow researcher in the community. We hope that the findings of our work will not only assist the continued development of Estuary but also guide the development of other future frameworks and technologies for SIAs.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SG-Reg: Generalizable and Efficient Scene Graph Registration</title>
<link>https://arxiv.org/abs/2504.14440</link>
<guid>https://arxiv.org/abs/2504.14440</guid>
<content:encoded><![CDATA[
arXiv:2504.14440v1 Announce Type: new 
Abstract: This paper addresses the challenges of registering two rigid semantic scene graphs, an essential capability when an autonomous agent needs to register its map against a remote agent, or against a prior map. The hand-crafted descriptors in classical semantic-aided registration, or the ground-truth annotation reliance in learning-based scene graph registration, impede their application in practical real-world environments. To address the challenges, we design a scene graph network to encode multiple modalities of semantic nodes: open-set semantic feature, local topology with spatial awareness, and shape feature. These modalities are fused to create compact semantic node features. The matching layers then search for correspondences in a coarse-to-fine manner. In the back-end, we employ a robust pose estimator to decide transformation according to the correspondences. We manage to maintain a sparse and hierarchical scene representation. Our approach demands fewer GPU resources and fewer communication bandwidth in multi-agent tasks. Moreover, we design a new data generation approach using vision foundation models and a semantic mapping module to reconstruct semantic scene graphs. It differs significantly from previous works, which rely on ground-truth semantic annotations to generate data. We validate our method in a two-agent SLAM benchmark. It significantly outperforms the hand-crafted baseline in terms of registration success rate. Compared to visual loop closure networks, our method achieves a slightly higher registration recall while requiring only 52 KB of communication bandwidth for each query frame. Code available at: \href{http://github.com/HKUST-Aerial-Robotics/SG-Reg}{http://github.com/HKUST-Aerial-Robotics/SG-Reg}.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DialogueAgents: A Hybrid Agent-Based Speech Synthesis Framework for Multi-Party Dialogue</title>
<link>https://arxiv.org/abs/2504.14482</link>
<guid>https://arxiv.org/abs/2504.14482</guid>
<content:encoded><![CDATA[
arXiv:2504.14482v1 Announce Type: new 
Abstract: Speech synthesis is crucial for human-computer interaction, enabling natural and intuitive communication. However, existing datasets involve high construction costs due to manual annotation and suffer from limited character diversity, contextual scenarios, and emotional expressiveness. To address these issues, we propose DialogueAgents, a novel hybrid agent-based speech synthesis framework, which integrates three specialized agents -- a script writer, a speech synthesizer, and a dialogue critic -- to collaboratively generate dialogues. Grounded in a diverse character pool, the framework iteratively refines dialogue scripts and synthesizes speech based on speech review, boosting emotional expressiveness and paralinguistic features of the synthesized dialogues. Using DialogueAgent, we contribute MultiTalk, a bilingual, multi-party, multi-turn speech dialogue dataset covering diverse topics. Extensive experiments demonstrate the effectiveness of our framework and the high quality of the MultiTalk dataset. We release the dataset and code https://github.com/uirlx/DialogueAgents to facilitate future research on advanced speech synthesis models and customized data generation.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinSage: A Multi-aspect RAG System for Financial Filings Question Answering</title>
<link>https://arxiv.org/abs/2504.14493</link>
<guid>https://arxiv.org/abs/2504.14493</guid>
<content:encoded><![CDATA[
arXiv:2504.14493v1 Announce Type: new 
Abstract: Leveraging large language models in real-world settings often entails a need to utilize domain-specific data and tools in order to follow the complex regulations that need to be followed for acceptable use. Within financial sectors, modern enterprises increasingly rely on Retrieval-Augmented Generation (RAG) systems to address complex compliance requirements in financial document workflows. However, existing solutions struggle to account for the inherent heterogeneity of data (e.g., text, tables, diagrams) and evolving nature of regulatory standards used in financial filings, leading to compromised accuracy in critical information extraction. We propose the FinSage framework as a solution, utilizing a multi-aspect RAG framework tailored for regulatory compliance analysis in multi-modal financial documents. FinSage introduces three innovative components: (1) a multi-modal pre-processing pipeline that unifies diverse data formats and generates chunk-level metadata summaries, (2) a multi-path sparse-dense retrieval system augmented with query expansion (HyDE) and metadata-aware semantic search, and (3) a domain-specialized re-ranking module fine-tuned via Direct Preference Optimization (DPO) to prioritize compliance-critical content. Extensive experiments demonstrate that FinSage achieves an impressive recall of 92.51% on 75 expert-curated questions derived from surpasses the best baseline method on the FinanceBench question answering datasets by 24.06% in accuracy. Moreover, FinSage has been successfully deployed as financial question-answering agent in online meetings, where it has already served more than 1,200 people.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VizTA: Enhancing Comprehension of Distributional Visualization with Visual-Lexical Fused Conversational Interface</title>
<link>https://arxiv.org/abs/2504.14507</link>
<guid>https://arxiv.org/abs/2504.14507</guid>
<content:encoded><![CDATA[
arXiv:2504.14507v1 Announce Type: new 
Abstract: Comprehending visualizations requires readers to interpret visual encoding and the underlying meanings actively. This poses challenges for visualization novices, particularly when interpreting distributional visualizations that depict statistical uncertainty. Advancements in LLM-based conversational interfaces show promise in promoting visualization comprehension. However, they fail to provide contextual explanations at fine-grained granularity, and chart readers are still required to mentally bridge visual information and textual explanations during conversations. Our formative study highlights the expectations for both lexical and visual feedback, as well as the importance of explicitly linking these two modalities throughout the conversation. The findings motivate the design of VizTA, a visualization teaching assistant that leverages the fusion of visual and lexical feedback to help readers better comprehend visualization. VizTA features a semantic-aware conversational agent capable of explaining contextual information within visualizations and employs a visual-lexical fusion design to facilitate chart-centered conversation. A between-subject study with 24 participants demonstrates the effectiveness of VizTA in supporting the understanding and reasoning tasks of distributional visualization across multiple scenarios.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey</title>
<link>https://arxiv.org/abs/2504.14520</link>
<guid>https://arxiv.org/abs/2504.14520</guid>
<content:encoded><![CDATA[
arXiv:2504.14520v1 Announce Type: new 
Abstract: This survey explores the development of meta-thinking capabilities in Large Language Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL) perspective. Meta-thinking self-reflection, assessment, and control of thinking processes is an important next step in enhancing LLM reliability, flexibility, and performance, particularly for complex or high-stakes tasks. The survey begins by analyzing current LLM limitations, such as hallucinations and the lack of internal self-assessment mechanisms. It then talks about newer methods, including RL from human feedback (RLHF), self-distillation, and chain-of-thought prompting, and each of their limitations. The crux of the survey is to talk about how multi-agent architectures, namely supervisor-agent hierarchies, agent debates, and theory of mind frameworks, can emulate human-like introspective behavior and enhance LLM robustness. By exploring reward mechanisms, self-play, and continuous learning methods in MARL, this survey gives a comprehensive roadmap to building introspective, adaptive, and trustworthy LLMs. Evaluation metrics, datasets, and future research avenues, including neuroscience-inspired architectures and hybrid symbolic reasoning, are also discussed.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BookWorld: From Novels to Interactive Agent Societies for Creative Story Generation</title>
<link>https://arxiv.org/abs/2504.14538</link>
<guid>https://arxiv.org/abs/2504.14538</guid>
<content:encoded><![CDATA[
arXiv:2504.14538v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have enabled social simulation through multi-agent systems. Prior efforts focus on agent societies created from scratch, assigning agents with newly defined personas. However, simulating established fictional worlds and characters remain largely underexplored, despite its significant practical value. In this paper, we introduce BookWorld, a comprehensive system for constructing and simulating book-based multi-agent societies. BookWorld's design covers comprehensive real-world intricacies, including diverse and dynamic characters, fictional worldviews, geographical constraints and changes, e.t.c. BookWorld enables diverse applications including story generation, interactive games and social simulation, offering novel ways to extend and explore beloved fictional works. Through extensive experiments, we demonstrate that BookWorld generates creative, high-quality stories while maintaining fidelity to the source books, surpassing previous methods with a win rate of 75.36%. The code of this paper can be found at the project page: https://bookworld2025.github.io/.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UFO2: The Desktop AgentOS</title>
<link>https://arxiv.org/abs/2504.14603</link>
<guid>https://arxiv.org/abs/2504.14603</guid>
<content:encoded><![CDATA[
arXiv:2504.14603v1 Announce Type: new 
Abstract: Recent Computer-Using Agents (CUAs), powered by multimodal large language models (LLMs), offer a promising direction for automating complex desktop workflows through natural language. However, most existing CUAs remain conceptual prototypes, hindered by shallow OS integration, fragile screenshot-based interaction, and disruptive execution.
  We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs into practical, system-level automation. UFO2 features a centralized HostAgent for task decomposition and coordination, alongside a collection of application-specialized AppAgent equipped with native APIs, domain-specific knowledge, and a unified GUI--API action layer. This architecture enables robust task execution while preserving modularity and extensibility. A hybrid control detection pipeline fuses Windows UI Automation (UIA) with vision-based parsing to support diverse interface styles. Runtime efficiency is further enhanced through speculative multi-action planning, reducing per-step LLM overhead. Finally, a Picture-in-Picture (PiP) interface enables automation within an isolated virtual desktop, allowing agents and users to operate concurrently without interference.
  We evaluate UFO2 across over 20 real-world Windows applications, demonstrating substantial improvements in robustness and execution accuracy over prior CUAs. Our results show that deep OS integration unlocks a scalable path toward reliable, user-aligned desktop automation.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Optimal Circuit Generation: Multi-Agent Collaboration Meets Collective Intelligence</title>
<link>https://arxiv.org/abs/2504.14625</link>
<guid>https://arxiv.org/abs/2504.14625</guid>
<content:encoded><![CDATA[
arXiv:2504.14625v1 Announce Type: new 
Abstract: Large language models (LLMs) have transformed code generation, yet their application in hardware design produces gate counts 38\%--1075\% higher than human designs. We present CircuitMind, a multi-agent framework that achieves human-competitive efficiency through three key innovations: syntax locking (constraining generation to basic logic gates), retrieval-augmented generation (enabling knowledge-driven design), and dual-reward optimization (balancing correctness with efficiency). To evaluate our approach, we introduce TC-Bench, the first gate-level benchmark harnessing collective intelligence from the TuringComplete ecosystem -- a competitive circuit design platform with hundreds of thousands of players. Experiments show CircuitMind enables 55.6\% of model implementations to match or exceed top-tier human experts in composite efficiency metrics. Most remarkably, our framework elevates the 14B Phi-4 model to outperform both GPT-4o mini and Gemini 2.0 Flash, achieving efficiency comparable to the top 25\% of human experts without requiring specialized training. These innovations establish a new paradigm for hardware optimization where collaborative AI systems leverage collective human expertise to achieve optimal circuit designs. Our model, data, and code are open-source at https://github.com/BUAA-CLab/CircuitMind.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Benchmarking and Aligning Task-Planning Safety in LLM-Based Embodied Agents</title>
<link>https://arxiv.org/abs/2504.14650</link>
<guid>https://arxiv.org/abs/2504.14650</guid>
<content:encoded><![CDATA[
arXiv:2504.14650v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit substantial promise in enhancing task-planning capabilities within embodied agents due to their advanced reasoning and comprehension. However, the systemic safety of these agents remains an underexplored frontier. In this study, we present Safe-BeAl, an integrated framework for the measurement (SafePlan-Bench) and alignment (Safe-Align) of LLM-based embodied agents' behaviors. SafePlan-Bench establishes a comprehensive benchmark for evaluating task-planning safety, encompassing 2,027 daily tasks and corresponding environments distributed across 8 distinct hazard categories (e.g., Fire Hazard). Our empirical analysis reveals that even in the absence of adversarial inputs or malicious intent, LLM-based agents can exhibit unsafe behaviors. To mitigate these hazards, we propose Safe-Align, a method designed to integrate physical-world safety knowledge into LLM-based embodied agents while maintaining task-specific performance. Experiments across a variety of settings demonstrate that Safe-BeAl provides comprehensive safety validation, improving safety by 8.55 - 15.22%, compared to embodied agents based on GPT-4, while ensuring successful task completion.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Complete and Bounded-Suboptimal Algorithm for a Moving Target Traveling Salesman Problem with Obstacles in 3D</title>
<link>https://arxiv.org/abs/2504.14680</link>
<guid>https://arxiv.org/abs/2504.14680</guid>
<content:encoded><![CDATA[
arXiv:2504.14680v1 Announce Type: new 
Abstract: The moving target traveling salesman problem with obstacles (MT-TSP-O) seeks an obstacle-free trajectory for an agent that intercepts a given set of moving targets, each within specified time windows, and returns to the agent's starting position. Each target moves with a constant velocity within its time windows, and the agent has a speed limit no smaller than any target's speed. We present FMC*-TSP, the first complete and bounded-suboptimal algorithm for the MT-TSP-O, and results for an agent whose configuration space is $\mathbb{R}^3$. Our algorithm interleaves a high-level search and a low-level search, where the high-level search solves a generalized traveling salesman problem with time windows (GTSP-TW) to find a sequence of targets and corresponding time windows for the agent to visit. Given such a sequence, the low-level search then finds an associated agent trajectory. To solve the low-level planning problem, we develop a new algorithm called FMC*, which finds a shortest path on a graph of convex sets (GCS) via implicit graph search and pruning techniques specialized for problems with moving targets. We test FMC*-TSP on 280 problem instances with up to 40 targets and demonstrate its smaller median runtime than a baseline based on prior work.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM-enabled Multi-Agent Autonomous Mechatronics Design Framework</title>
<link>https://arxiv.org/abs/2504.14681</link>
<guid>https://arxiv.org/abs/2504.14681</guid>
<content:encoded><![CDATA[
arXiv:2504.14681v1 Announce Type: new 
Abstract: Existing LLM-enabled multi-agent frameworks are predominantly limited to digital or simulated environments and confined to narrowly focused knowledge domain, constraining their applicability to complex engineering tasks that require the design of physical embodiment, cross-disciplinary integration, and constraint-aware reasoning. This work proposes a multi-agent autonomous mechatronics design framework, integrating expertise across mechanical design, optimization, electronics, and software engineering to autonomously generate functional prototypes with minimal direct human design input. Operating primarily through a language-driven workflow, the framework incorporates structured human feedback to ensure robust performance under real-world constraints. To validate its capabilities, the framework is applied to a real-world challenge involving autonomous water-quality monitoring and sampling, where traditional methods are labor-intensive and ecologically disruptive. Leveraging the proposed system, a fully functional autonomous vessel was developed with optimized propulsion, cost-effective electronics, and advanced control. The design process was carried out by specialized agents, including a high-level planning agent responsible for problem abstraction and dedicated agents for structural, electronics, control, and software development. This approach demonstrates the potential of LLM-based multi-agent systems to automate real-world engineering workflows and reduce reliance on extensive domain expertise.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI with Emotions: Exploring Emotional Expressions in Large Language Models</title>
<link>https://arxiv.org/abs/2504.14706</link>
<guid>https://arxiv.org/abs/2504.14706</guid>
<content:encoded><![CDATA[
arXiv:2504.14706v1 Announce Type: new 
Abstract: The human-level performance of Large Language Models (LLMs) across various tasks has raised expectations for the potential of Artificial Intelligence (AI) to possess emotions someday. To explore the capability of current LLMs to express emotions in their outputs, we conducted an experiment using several LLMs (OpenAI GPT, Google Gemini, Meta Llama3, and Cohere Command R+) to role-play as agents answering questions with specified emotional states.We defined the emotional states using Russell's Circumplex model, a well-established framework that characterizes emotions along the sleepy-activated (arousal) and pleasure-displeasure (valence) axes. We chose this model for its simplicity, utilizing two continuous parameters, which allows for better controllability in applications involving continuous changes in emotional states. The responses generated were evaluated using a sentiment analysis model, independent of the LLMs, trained on the GoEmotions dataset. The evaluation showed that the emotional states of the generated answers were consistent with the specifications, demonstrating the LLMs' capability for emotional expression. This indicates the potential for LLM-based AI agents to simulate emotions, opening up a wide range of applications for emotion-based interactions, such as advisors or consultants who can provide advice or opinions with a personal touch.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-Synth: Synthesizing Verifiable Bug-Fix Data to Enable Large Language Models in Resolving Real-World Bugs</title>
<link>https://arxiv.org/abs/2504.14757</link>
<guid>https://arxiv.org/abs/2504.14757</guid>
<content:encoded><![CDATA[
arXiv:2504.14757v1 Announce Type: new 
Abstract: Large language models (LLMs) are transforming automated program repair (APR) through agent-based approaches that localize bugs, generate patches, and verify fixes. However, the lack of high-quality, scalable training datasets, especially those with verifiable outputs and intermediate reasoning traces-limits progress, particularly for open-source models. In this work, we present SWE-Synth, a framework for synthesizing realistic, verifiable, and process-aware bug-fix datasets at the repository level. SWE-Synth leverages LLM agents to simulate debugging workflows, producing not only bug-fix pairs but also test cases and structured repair trajectories. Compared to manually curated datasets, our method scales with minimal human effort while preserving contextual richness and correctness. Experiments show that models trained on SWE-Synth outperform those trained on real-world datasets by 2.3% on SWE-Bench Lite. Our results highlight the potential of synthetic, agent-generated data to advance the state of the art in APR and software engineering automation.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Establishing Workload Identity for Zero Trust CI/CD: From Secrets to SPIFFE-Based Authentication</title>
<link>https://arxiv.org/abs/2504.14760</link>
<guid>https://arxiv.org/abs/2504.14760</guid>
<content:encoded><![CDATA[
arXiv:2504.14760v1 Announce Type: new 
Abstract: CI/CD systems have become privileged automation agents in modern infrastructure, but their identity is still based on secrets or temporary credentials passed between systems. In enterprise environments, these platforms are centralized and shared across teams, often with broad cloud permissions and limited isolation. These conditions introduce risk, especially in the era of supply chain attacks, where implicit trust and static credentials leave systems exposed. This paper describes the shift from static credentials to OpenID Connect (OIDC) federation, and introduces SPIFFE (Secure Production Identity Framework for Everyone) as a runtime-issued, platform-neutral identity model for non-human actors. SPIFFE decouples identity from infrastructure, enabling strong, portable authentication across job runners and deployed workloads. We show how SPIFFE identities support policy alignment, workload attestation, and mutual authentication. The paper concludes by outlining next steps in enabling policy-based access, forming the basis of a broader Zero Trust architecture for CI/CD.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLANET: A Collection of Benchmarks for Evaluating LLMs' Planning Capabilities</title>
<link>https://arxiv.org/abs/2504.14773</link>
<guid>https://arxiv.org/abs/2504.14773</guid>
<content:encoded><![CDATA[
arXiv:2504.14773v1 Announce Type: new 
Abstract: Planning is central to agents and agentic AI. The ability to plan, e.g., creating travel itineraries within a budget, holds immense potential in both scientific and commercial contexts. Moreover, optimal plans tend to require fewer resources compared to ad-hoc methods. To date, a comprehensive understanding of existing planning benchmarks appears to be lacking. Without it, comparing planning algorithms' performance across domains or selecting suitable algorithms for new scenarios remains challenging. In this paper, we examine a range of planning benchmarks to identify commonly used testbeds for algorithm development and highlight potential gaps. These benchmarks are categorized into embodied environments, web navigation, scheduling, games and puzzles, and everyday task automation. Our study recommends the most appropriate benchmarks for various algorithms and offers insights to guide future benchmark development.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Collaborative GenAI Agents in Synchronous Group Settings: Eliciting Team Perceptions and Design Considerations for the Future of Work</title>
<link>https://arxiv.org/abs/2504.14779</link>
<guid>https://arxiv.org/abs/2504.14779</guid>
<content:encoded><![CDATA[
arXiv:2504.14779v1 Announce Type: new 
Abstract: While generative artificial intelligence (GenAI) is finding increased adoption in workplaces, current tools are primarily designed for individual use. Prior work established the potential for these tools to enhance personal creativity and productivity towards shared goals; however, we don't know yet how to best take into account the nuances of group work and team dynamics when deploying GenAI in work settings. In this paper, we investigate the potential of collaborative GenAI agents to augment teamwork in synchronous group settings through an exploratory study that engaged 25 professionals across 6 teams in speculative design workshops and individual follow-up interviews. Our workshops included a mixed reality provotype to simulate embodied collaborative GenAI agents capable of actively participating in group discussions. Our findings suggest that, if designed well, collaborative GenAI agents offer valuable opportunities to enhance team problem-solving by challenging groupthink, bridging communication gaps, and reducing social friction. However, teams' willingness to integrate GenAI agents depended on its perceived fit across a number of individual, team, and organizational factors. We outline the key design tensions around agent representation, social prominence, and engagement and highlight the opportunities spatial and immersive technologies could offer to modulate GenAI influence on team outcomes and strike a balance between augmentation and agency.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADL: A Declarative Language for Agent-Based Chatbots</title>
<link>https://arxiv.org/abs/2504.14787</link>
<guid>https://arxiv.org/abs/2504.14787</guid>
<content:encoded><![CDATA[
arXiv:2504.14787v1 Announce Type: new 
Abstract: There are numerous agent frameworks capable of creating and orchestrating agents to address complex tasks. However, these frameworks are often too complicated for customer service professionals, who may not have much programming experience but still need an easy way to create chatbots with rich business logic. In this work, we introduce ADL, a Declarative Language for Agent-Based Chatbots. ADL simplifies chatbot development by using natural language programming at its core, making it easier for a broad audience to customize and build task-oriented chatbots. It includes four types of agents and supports integration with custom functions, tool use, and third-party agents. ADL abstracts away implementation details, offering a declarative way to define agents and their interactions, which could ease prompt engineering, testing and debugging. MICA, a multi-agent system designed to interpret and execute ADL programs, has been developed and is now available as an open-source project at https://github.com/Mica-labs/MICA. Its user documentation can be found at https://mica-labs.github.io/.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Visual Reinforcement Learning with Separate Primitive Policy for Peg-in-Hole Tasks</title>
<link>https://arxiv.org/abs/2504.14820</link>
<guid>https://arxiv.org/abs/2504.14820</guid>
<content:encoded><![CDATA[
arXiv:2504.14820v1 Announce Type: new 
Abstract: For peg-in-hole tasks, humans rely on binocular visual perception to locate the peg above the hole surface and then proceed with insertion. This paper draws insights from this behavior to enable agents to learn efficient assembly strategies through visual reinforcement learning. Hence, we propose a Separate Primitive Policy (S2P) to simultaneously learn how to derive location and insertion actions. S2P is compatible with model-free reinforcement learning algorithms. Ten insertion tasks featuring different polygons are developed as benchmarks for evaluations. Simulation experiments show that S2P can boost the sample efficiency and success rate even with force constraints. Real-world experiments are also performed to verify the feasibility of S2P. Ablations are finally given to discuss the generalizability of S2P and some factors that affect its performance.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Completing A Systematic Review in Hours instead of Months with Interactive AI Agents</title>
<link>https://arxiv.org/abs/2504.14822</link>
<guid>https://arxiv.org/abs/2504.14822</guid>
<content:encoded><![CDATA[
arXiv:2504.14822v1 Announce Type: new 
Abstract: Systematic reviews (SRs) are vital for evidence-based practice in high stakes disciplines, such as healthcare, but are often impeded by intensive labors and lengthy processes that can take months to complete. Due to the high demand for domain expertise, existing automatic summarization methods fail to accurately identify relevant studies and generate high-quality summaries. To that end, we introduce InsightAgent, a human-centered interactive AI agent powered by large language models that revolutionize this workflow. InsightAgent partitions a large literature corpus based on semantics and employs a multi-agent design for more focused processing of literature, leading to significant improvement in the quality of generated SRs. InsightAgent also provides intuitive visualizations of the corpus and agent trajectories, allowing users to effortlessly monitor the actions of the agent and provide real-time feedback based on their expertise. Our user studies with 9 medical professionals demonstrate that the visualization and interaction mechanisms can effectively improve the quality of synthesized SRs by 27.2%, reaching 79.7% of human-written quality. At the same time, user satisfaction is improved by 34.4%. With InsightAgent, it only takes a clinician about 1.5 hours, rather than months, to complete a high-quality systematic review.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Enhanced Dual-Currency VCG Auction Mechanism for Resource Allocation in IoV: A Value of Information Perspective</title>
<link>https://arxiv.org/abs/2504.14824</link>
<guid>https://arxiv.org/abs/2504.14824</guid>
<content:encoded><![CDATA[
arXiv:2504.14824v1 Announce Type: new 
Abstract: The Internet of Vehicles (IoV) is undergoing a transformative evolution, enabled by advancements in future 6G network technologies, to support intelligent, highly reliable, and low-latency vehicular services. However, the enhanced capabilities of loV have heightened the demands for efficient network resource allocation while simultaneously giving rise to diverse vehicular service requirements. For network service providers (NSPs), meeting the customized resource-slicing requirements of vehicle service providers (VSPs) while maximizing social welfare has become a significant challenge. This paper proposes an innovative solution by integrating a mean-field multi-agent reinforcement learning (MFMARL) framework with an enhanced Vickrey-Clarke-Groves (VCG) auction mechanism to address the problem of social welfare maximization under the condition of unknown VSP utility functions. The core of this solution is introducing the ``value of information" as a novel monetary metric to estimate the expected benefits of VSPs, thereby ensuring the effective execution of the VCG auction mechanism. MFMARL is employed to optimize resource allocation for social welfare maximization while adapting to the intelligent and dynamic requirements of IoV. The proposed enhanced VCG auction mechanism not only protects the privacy of VSPs but also reduces the likelihood of collusion among VSPs, and it is theoretically proven to be dominant-strategy incentive compatible (DSIC). The simulation results demonstrate that, compared to the VCG mechanism implemented using quantization methods, the proposed mechanism exhibits significant advantages in convergence speed, social welfare maximization, and resistance to collusion, providing new insights into resource allocation in intelligent 6G networks.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SQL-Factory: A Multi-Agent Framework for High-Quality and Large-Scale SQL Generation</title>
<link>https://arxiv.org/abs/2504.14837</link>
<guid>https://arxiv.org/abs/2504.14837</guid>
<content:encoded><![CDATA[
arXiv:2504.14837v1 Announce Type: new 
Abstract: Hight quality SQL corpus is essential for intelligent database. For example, Text-to-SQL requires SQL queries and correspond natural language questions as training samples. However, collecting such query corpus remains challenging in practice due to the high cost of manual annotation, which highlights the importance of automatic SQL generation. Despite recent advances, existing generation methods still face limitations in achieving both diversity and cost-effectiveness. Besides, many methods also treat all tables equally during generation, which overlooks schema complexity and leads to under-utilization of structurally rich tables. To address these issues, this paper proposes a multi-agent framework for high-quality and large-scale SQL generation, dubbed SQL-Factory. It decomposes the generation process into three collaborative teams: the Generation Team explores diverse query structures using large language models, the Expansion Team scales promising patterns via lightweight local models, and the Management Team adaptively schedules and evaluates generation based on schema coverage and real-time query quality. This modular framework ensures a balanced trade-off between diversity, scalability, and generation cost. We apply SQL-Factory to four widely used benchmarks and generate over 300,000 executable and broadly distributed SQL queries with less than $200 API cost. Our generated queries achieve higher diversity compared to other methods, and extensive experiments demonstrate that the generated queries significantly improve the model performance in various downstream tasks.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Twin Co-Adaptive Dialogue for Progressive Image Generation</title>
<link>https://arxiv.org/abs/2504.14868</link>
<guid>https://arxiv.org/abs/2504.14868</guid>
<content:encoded><![CDATA[
arXiv:2504.14868v1 Announce Type: new 
Abstract: Modern text-to-image generation systems have enabled the creation of remarkably realistic and high-quality visuals, yet they often falter when handling the inherent ambiguities in user prompts. In this work, we present Twin-Co, a framework that leverages synchronized, co-adaptive dialogue to progressively refine image generation. Instead of a static generation process, Twin-Co employs a dynamic, iterative workflow where an intelligent dialogue agent continuously interacts with the user. Initially, a base image is generated from the user's prompt. Then, through a series of synchronized dialogue exchanges, the system adapts and optimizes the image according to evolving user feedback. The co-adaptive process allows the system to progressively narrow down ambiguities and better align with user intent. Experiments demonstrate that Twin-Co not only enhances user experience by reducing trial-and-error iterations but also improves the quality of the generated images, streamlining the creative process across various applications.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Event triggered optimal formation control for nonlinear multi-agent systems under Denial-of-Service attacks</title>
<link>https://arxiv.org/abs/2504.14874</link>
<guid>https://arxiv.org/abs/2504.14874</guid>
<content:encoded><![CDATA[
arXiv:2504.14874v1 Announce Type: new 
Abstract: This paper investigates the optimal formation control problem of a class of nonlinear multi-agent systems(MASs) under Denial-of-Service(DoS) attacks. We design the optimal formation control law using an event-triggered control scheme to achieve formation objectives under DoS attacks. Critic neural network (NN)-based approach is employed to achieve the optimal control policy under DoS attacks. Event-triggered mechanism is introduced to ensure the saving of control resources. Additionally, Lyapunov stability theory is utilized to demonstrate that the local neighborhood formation error exhibits exponential stability and the estimation error of weights are uniformly ultimately bounded. Finally, the effectiveness of the control algorithm is validated through matlab simulations. The results indicate that under DoS attacks, the nonlinear MAS successfully achieves the desired formation for the MAS.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Time-Varying Gaussian Regression via Kalman Filtering</title>
<link>https://arxiv.org/abs/2504.14900</link>
<guid>https://arxiv.org/abs/2504.14900</guid>
<content:encoded><![CDATA[
arXiv:2504.14900v1 Announce Type: new 
Abstract: We consider the problem of learning time-varying functions in a distributed fashion, where agents collect local information to collaboratively achieve a shared estimate. This task is particularly relevant in control applications, whenever real-time and robust estimation of dynamic cost/reward functions in safety critical settings has to be performed. In this paper, we,adopt a finite-dimensional approximation of a Gaussian Process, corresponding to a Bayesian linear regression in an appropriate feature space, and propose a new algorithm, DistKP, to track the time-varying coefficients via a distributed Kalman filter. The proposed method works for arbitrary kernels and under weaker assumptions on the time-evolution of the function to learn compared to the literature. We validate our results using a simulation example in which a fleet of Unmanned Aerial Vehicles (UAVs) learns a dynamically changing wind field.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework</title>
<link>https://arxiv.org/abs/2504.14928</link>
<guid>https://arxiv.org/abs/2504.14928</guid>
<content:encoded><![CDATA[
arXiv:2504.14928v1 Announce Type: new 
Abstract: Large language models (LLMs) increasingly serve as educational tools, yet evaluating their teaching capabilities remains challenging due to the resource-intensive, context-dependent, and methodologically complex nature of teacher-student interactions. We introduce EducationQ, a multi-agent dialogue framework that efficiently assesses teaching capabilities through simulated dynamic educational scenarios, featuring specialized agents for teaching, learning, and evaluation. Testing 14 LLMs across major AI Organizations (OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13 disciplines and 10 difficulty levels reveals that teaching effectiveness does not correlate linearly with model scale or general reasoning capabilities - with some smaller open-source models outperforming larger commercial counterparts in teaching contexts. This finding highlights a critical gap in current evaluations that prioritize knowledge recall over interactive pedagogy. Our mixed-methods evaluation, combining quantitative metrics with qualitative analysis and expert case studies, identifies distinct pedagogical strengths employed by top-performing models (e.g., sophisticated questioning strategies, adaptive feedback mechanisms). Human expert evaluations show 78% agreement with our automated qualitative analysis of effective teaching behaviors, validating our methodology. EducationQ demonstrates that LLMs-as-teachers require specialized optimization beyond simple scaling, suggesting next-generation educational AI prioritize targeted enhancement of specific pedagogical effectiveness.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanism Design for Auctions with Externalities on Budgets</title>
<link>https://arxiv.org/abs/2504.14948</link>
<guid>https://arxiv.org/abs/2504.14948</guid>
<content:encoded><![CDATA[
arXiv:2504.14948v1 Announce Type: new 
Abstract: This paper studies mechanism design for auctions with externalities on budgets, a novel setting where the budgets that bidders commit are adjusted due to the externality of the competitors' allocation outcomes-a departure from traditional auctions with fixed budgets. This setting is motivated by real-world scenarios, for example, participants may increase their budgets in response to competitors' obtained items. We initially propose a general framework with homogeneous externalities to capture the interdependence between budget updates and allocation, formalized through a budget response function that links each bidder's effective budget to the amount of items won by others.
  The main contribution of this paper is to propose a truthful and individual rational auction mechanism for this novel auction setting, which achieves an approximation ratio of $1/3$ with respect to the liquid welfare. This mechanism is inspired by the uniform-price auction, in which an appropriate uniform price is selected to allocate items, ensuring the monotonicity of the allocation rule while accounting for budget adjustments. Additionally, this mechanism guarantees a constant approximation ratio by setting a purchase limit. Complementing this result, we establish an upper bound: no truthful mechanism can achieve an approximation ratio better than $1/2$. This work offers a new perspective to study the impact of externalities on auctions, providing an approach to handle budget externalities in multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DistilQwen2.5: Industrial Practices of Training Distilled Open Lightweight Language Models</title>
<link>https://arxiv.org/abs/2504.15027</link>
<guid>https://arxiv.org/abs/2504.15027</guid>
<content:encoded><![CDATA[
arXiv:2504.15027v1 Announce Type: new 
Abstract: Enhancing computational efficiency and reducing deployment costs for large language models (LLMs) have become critical challenges in various resource-constrained scenarios. In this work, we present DistilQwen2.5, a family of distilled, lightweight LLMs derived from the public Qwen2.5 models. These distilled models exhibit enhanced instruction-following capabilities compared to the original models based on a series of distillation techniques that incorporate knowledge from much larger LLMs. In our industrial practice, we first leverage powerful proprietary LLMs with varying capacities as multi-agent teachers to select, rewrite, and refine instruction-response pairs that are more suitable for student LLMs to learn. After standard fine-tuning, we further leverage a computationally efficient model fusion approach that enables student models to progressively integrate fine-grained hidden knowledge from their teachers. Experimental evaluations demonstrate that the distilled models possess significantly stronger capabilities than their original checkpoints. Additionally, we present use cases to illustrate the applications of our framework in real-world scenarios. To facilitate practical use, we have released all the DistilQwen2.5 models to the open-source community.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-to-Decision Agent: Learning Generalist Policies from Natural Language Supervision</title>
<link>https://arxiv.org/abs/2504.15046</link>
<guid>https://arxiv.org/abs/2504.15046</guid>
<content:encoded><![CDATA[
arXiv:2504.15046v1 Announce Type: new 
Abstract: RL systems usually tackle generalization by inferring task beliefs from high-quality samples or warmup explorations. The restricted form limits their generality and usability since these supervision signals are expensive and even infeasible to acquire in advance for unseen tasks. Learning directly from the raw text about decision tasks is a promising alternative to leverage a much broader source of supervision. In the paper, we propose Text-to-Decision Agent (T2DA), a simple and scalable framework that supervises generalist policy learning with natural language. We first introduce a generalized world model to encode multi-task decision data into a dynamics-aware embedding space. Then, inspired by CLIP, we predict which textual description goes with which decision embedding, effectively bridging their semantic gap via contrastive language-decision pre-training and aligning the text embeddings to comprehend the environment dynamics. After training the text-conditioned generalist policy, the agent can directly realize zero-shot text-to-decision generation in response to language instructions. Comprehensive experiments on MuJoCo and Meta-World benchmarks show that T2DA facilitates high-capacity zero-shot generalization and outperforms various types of baselines.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast-Slow Co-advancing Optimizer: Toward Harmonious Adversarial Training of GAN</title>
<link>https://arxiv.org/abs/2504.15099</link>
<guid>https://arxiv.org/abs/2504.15099</guid>
<content:encoded><![CDATA[
arXiv:2504.15099v1 Announce Type: new 
Abstract: Up to now, the training processes of typical Generative Adversarial Networks (GANs) are still particularly sensitive to data properties and hyperparameters, which may lead to severe oscillations, difficulties in convergence, or even failures to converge, especially when the overall variances of the training sets are large. These phenomena are often attributed to the training characteristics of such networks. Aiming at the problem, this paper develops a new intelligent optimizer, Fast-Slow Co-advancing Optimizer (FSCO), which employs reinforcement learning in the training process of GANs to make training easier. Specifically, this paper allows the training step size to be controlled by an agent to improve training stability, and makes the training process more intelligent with variable learning rates, making GANs less sensitive to step size. Experiments have been conducted on three benchmark datasets to verify the effectiveness of the developed FSCO.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contemplative Wisdom for Superalignment</title>
<link>https://arxiv.org/abs/2504.15125</link>
<guid>https://arxiv.org/abs/2504.15125</guid>
<content:encoded><![CDATA[
arXiv:2504.15125v1 Announce Type: new 
Abstract: As artificial intelligence (AI) improves, traditional alignment strategies may falter in the face of unpredictable self-improvement, hidden subgoals, and the sheer complexity of intelligent systems. Rather than externally constraining behavior, we advocate designing AI with intrinsic morality built into its cognitive architecture and world model. Inspired by contemplative wisdom traditions, we show how four axiomatic principles can instil a resilient Wise World Model in AI systems. First, mindfulness enables self-monitoring and recalibration of emergent subgoals. Second, emptiness forestalls dogmatic goal fixation and relaxes rigid priors. Third, non-duality dissolves adversarial self-other boundaries. Fourth, boundless care motivates the universal reduction of suffering. We find that prompting AI to reflect on these principles improves performance on the AILuminate Benchmark using GPT-4o, particularly when combined. We offer detailed implementation strategies for state-of-the-art models, including contemplative architectures, constitutions, and reinforcement of chain-of-thought. For future systems, the active inference framework may offer the self-organizing and dynamic coupling capabilities needed to enact these insights in embodied agents. This interdisciplinary approach offers a self-correcting and resilient alternative to prevailing brittle control schemes.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural ATTF: A Scalable Solution to Lifelong Multi-Agent Path Planning</title>
<link>https://arxiv.org/abs/2504.15130</link>
<guid>https://arxiv.org/abs/2504.15130</guid>
<content:encoded><![CDATA[
arXiv:2504.15130v1 Announce Type: new 
Abstract: Multi-Agent Pickup and Delivery (MAPD) is a fundamental problem in robotics, particularly in applications such as warehouse automation and logistics. Existing solutions often face challenges in scalability, adaptability, and efficiency, limiting their applicability in dynamic environments with real-time planning requirements. This paper presents Neural ATTF (Adaptive Task Token Framework), a new algorithm that combines a Priority Guided Task Matching (PGTM) Module with Neural STA* (Space-Time A*), a data-driven path planning method. Neural STA* enhances path planning by enabling rapid exploration of the search space through guided learned heuristics and ensures collision avoidance under dynamic constraints. PGTM prioritizes delayed agents and dynamically assigns tasks by prioritizing agents nearest to these tasks, optimizing both continuity and system throughput. Experimental evaluations against state-of-the-art MAPD algorithms, including TPTS, CENTRAL, RMCA, LNS-PBS, and LNS-wPBS, demonstrate the superior scalability, solution quality, and computational efficiency of Neural ATTF. These results highlight the framework's potential for addressing the critical demands of complex, real-world multi-agent systems operating in high-demand, unpredictable settings.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behavioral Universe Network (BUN): A Behavioral Information-Based Framework for Complex Systems</title>
<link>https://arxiv.org/abs/2504.15146</link>
<guid>https://arxiv.org/abs/2504.15146</guid>
<content:encoded><![CDATA[
arXiv:2504.15146v1 Announce Type: new 
Abstract: Modern digital ecosystems feature complex, dynamic interactions among autonomous entities across diverse domains. Traditional models often separate agents and objects, lacking a unified foundation to capture their interactive behaviors. This paper introduces the Behavioral Universe Network (BUN), a theoretical framework grounded in the Agent-Interaction-Behavior (AIB) formalism. BUN treats subjects (active agents), objects (resources), and behaviors (operations) as first-class entities, all governed by a shared Behavioral Information Base (BIB). We detail the AIB core concepts and demonstrate how BUN leverages information-driven triggers, semantic enrichment, and adaptive rules to coordinate multi-agent systems. We highlight key benefits: enhanced behavior analysis, strong adaptability, and cross-domain interoperability. We conclude by positioning BUN as a promising foundation for next-generation digital governance and intelligent applications.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An experimental study of the influence of anonymous information on social media users</title>
<link>https://arxiv.org/abs/2504.15215</link>
<guid>https://arxiv.org/abs/2504.15215</guid>
<content:encoded><![CDATA[
arXiv:2504.15215v1 Announce Type: new 
Abstract: Increasingly, people use social media for their day-to-day interactions and as a source of information, even though much of this information is practically anonymous. This raises the question: does anonymous information influence its recipients? We conducted an online, two-phase, preregistered experiment using a nationally representative sample of participants from the U.S. to find the answer. To avoid biases of opinions among participants, in the first phase, each participant examines ten Rorschach inkblots and chooses one of four opinions assigned to each inkblot. In the second phase, the participants are randomly assigned to one of four distinct information conditions and are asked to revisit their opinions for the same ten inkblots. Conditions ranged from repeating phase one to receiving anonymous comments about certain opinions. Results were consistent with the preregistration. Importantly, anonymous comments shown in phase two influence up to half of the participants' opinion selections. To better understand the role of anonymous comments in influencing the selections of opinions, we implemented agent-based modeling (ABM). ABM results suggest that a straightforward mechanism can explain the impact of such information. Overall, our results indicate that even anonymous information can have a significant impact on its recipients, potentially altering their popularity rankings. However, the strength of such influence weakens when recipients' confidence in their selections increases. Additionally, we found that participants' confidence in the first phase is inversely related to the number of change opinions.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Self-Improving Coding Agent</title>
<link>https://arxiv.org/abs/2504.15228</link>
<guid>https://arxiv.org/abs/2504.15228</guid>
<content:encoded><![CDATA[
arXiv:2504.15228v1 Announce Type: new 
Abstract: We demonstrate that an LLM coding agent, equipped with basic coding tools, can autonomously edit itself, and thereby improve its performance on benchmark tasks. We find performance gains from 17% to 53% on a random subset of SWE Bench Verified, with additional performance gains on LiveCodeBench, as well as synthetically generated agent benchmarks. Our work represents an advancement in the automated and open-ended design of agentic systems, and provides a reference agent framework for those seeking to post-train LLMs on tool use and other agentic tasks.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowReasoner: Reinforcing Query-Level Meta-Agents</title>
<link>https://arxiv.org/abs/2504.15257</link>
<guid>https://arxiv.org/abs/2504.15257</guid>
<content:encoded><![CDATA[
arXiv:2504.15257v1 Announce Type: new 
Abstract: This paper proposes a query-level meta-agent named FlowReasoner to automate the design of query-level multi-agent systems, i.e., one system per user query. Our core idea is to incentivize a reasoning-based meta-agent via external execution feedback. Concretely, by distilling DeepSeek R1, we first endow the basic reasoning ability regarding the generation of multi-agent systems to FlowReasoner. Then, we further enhance it via reinforcement learning (RL) with external execution feedback. A multi-purpose reward is designed to guide the RL training from aspects of performance, complexity, and efficiency. In this manner, FlowReasoner is enabled to generate a personalized multi-agent system for each user query via deliberative reasoning. Experiments on both engineering and competition code benchmarks demonstrate the superiority of FlowReasoner. Remarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks. The code is available at https://github.com/sail-sg/FlowReasoner.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Locomotion Prediction in Construction Using a Memory-Driven LLM Agent With Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2504.15263</link>
<guid>https://arxiv.org/abs/2504.15263</guid>
<content:encoded><![CDATA[
arXiv:2504.15263v1 Announce Type: new 
Abstract: Construction tasks are inherently unpredictable, with dynamic environments and safety-critical demands posing significant risks to workers. Exoskeletons offer potential assistance but falter without accurate intent recognition across diverse locomotion modes. This paper presents a locomotion prediction agent leveraging Large Language Models (LLMs) augmented with memory systems, aimed at improving exoskeleton assistance in such settings. Using multimodal inputs - spoken commands and visual data from smart glasses - the agent integrates a Perception Module, Short-Term Memory (STM), Long-Term Memory (LTM), and Refinement Module to predict locomotion modes effectively. Evaluation reveals a baseline weighted F1-score of 0.73 without memory, rising to 0.81 with STM, and reaching 0.90 with both STM and LTM, excelling with vague and safety-critical commands. Calibration metrics, including a Brier Score drop from 0.244 to 0.090 and ECE from 0.222 to 0.044, affirm improved reliability. This framework supports safer, high-level human-exoskeleton collaboration, with promise for adaptive assistive systems in dynamic industries.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs</title>
<link>https://arxiv.org/abs/2504.15280</link>
<guid>https://arxiv.org/abs/2504.15280</guid>
<content:encoded><![CDATA[
arXiv:2504.15280v1 Announce Type: new 
Abstract: Multi-view understanding, the ability to reconcile visual information across diverse viewpoints for effective navigation, manipulation, and 3D scene comprehension, is a fundamental challenge in Multi-Modal Large Language Models (MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive advances in high-level reasoning and planning, they frequently fall short when confronted with multi-view geometric consistency and cross-view correspondence. To comprehensively evaluate the challenges of MLLMs in multi-view scene reasoning, we propose All-Angles Bench, a benchmark of over 2,100 human carefully annotated multi-view question-answer pairs across 90 diverse real-world scenes. Our six tasks (counting, attribute identification, relative distance, relative direction, object manipulation, and camera pose estimation) specifically test model's geometric correspondence and the capacity to align information consistently across views. Our extensive experiments, benchmark on 27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and GPT-4o against human evaluators reveals a substantial performance gap, indicating that current MLLMs remain far from human-level proficiency. Through in-depth analysis, we show that MLLMs are particularly underperforming under two aspects: (1) cross-view correspondence for partially occluded views and (2) establishing the coarse camera poses. These findings highlight the necessity of domain-specific refinements or modules that embed stronger multi-view awareness. We believe that our All-Angles Bench offers valuable insights and contribute to bridging the gap between MLLMs and human-level multi-view understanding. The project and benchmark are publicly available at https://danielchyeh.github.io/All-Angles-Bench/.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>System of Agentic AI for the Discovery of Metal-Organic Frameworks</title>
<link>https://arxiv.org/abs/2504.14110</link>
<guid>https://arxiv.org/abs/2504.14110</guid>
<content:encoded><![CDATA[
arXiv:2504.14110v1 Announce Type: cross 
Abstract: Generative models and machine learning promise accelerated material discovery in MOFs for CO2 capture and water harvesting but face significant challenges navigating vast chemical spaces while ensuring synthetizability. Here, we present MOFGen, a system of Agentic AI comprising interconnected agents: a large language model that proposes novel MOF compositions, a diffusion model that generates crystal structures, quantum mechanical agents that optimize and filter candidates, and synthetic-feasibility agents guided by expert rules and machine learning. Trained on all experimentally reported MOFs and computational databases, MOFGen generated hundreds of thousands of novel MOF structures and synthesizable organic linkers. Our methodology was validated through high-throughput experiments and the successful synthesis of five "AI-dreamt" MOFs, representing a major step toward automated synthesizable material discovery.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Lattice Boltzmann Closures through Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.14422</link>
<guid>https://arxiv.org/abs/2504.14422</guid>
<content:encoded><![CDATA[
arXiv:2504.14422v1 Announce Type: cross 
Abstract: The Lattice Boltzmann method (LBM) offers a powerful and versatile approach to simulating diverse hydrodynamic phenomena, spanning microfluidics to aerodynamics. The vast range of spatiotemporal scales inherent in these systems currently renders full resolution impractical, necessitating the development of effective closure models for under-resolved simulations. Under-resolved LBMs are unstable, and while there is a number of important efforts to stabilize them, they often face limitations in generalizing across scales and physical systems. We present a novel, data-driven, multiagent reinforcement learning (MARL) approach that drastically improves stability and accuracy of coarse-grained LBM simulations. The proposed method uses a convolutional neural network to dynamically control the local relaxation parameter for the LB across the simulation grid. The LB-MARL framework is showcased in turbulent Kolmogorov flows. We find that the MARL closures stabilize the simulations and recover the energy spectra of significantly more expensive fully resolved simulations while maintaining computational efficiency. The learned closure model can be transferred to flow scenarios unseen during training and has improved robustness and spectral accuracy compared to traditional LBM models. We believe that MARL closures open new frontiers for efficient and accurate simulations of a multitude of complex problems not accessible to present-day LB methods alone.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM-based Quantum Code Generation with Multi-Agent Optimization and Quantum Error Correction</title>
<link>https://arxiv.org/abs/2504.14557</link>
<guid>https://arxiv.org/abs/2504.14557</guid>
<content:encoded><![CDATA[
arXiv:2504.14557v1 Announce Type: cross 
Abstract: Multi-agent frameworks with Large Language Models (LLMs) have become promising tools for generating general-purpose programming languages using test-driven development, allowing developers to create more accurate and robust code. However, their potential has not been fully unleashed for domain-specific programming languages, where specific domain exhibits unique optimization opportunities for customized improvement. In this paper, we take the first step in exploring multi-agent code generation for quantum programs. By identifying the unique optimizations in quantum designs such as quantum error correction, we introduce a novel multi-agent framework tailored to generating accurate, fault-tolerant quantum code. Each agent in the framework focuses on distinct optimizations, iteratively refining the code using a semantic analyzer with multi-pass inference, alongside an error correction code decoder. We also examine the effectiveness of inference-time techniques, like Chain-of-Thought (CoT) and Retrieval-Augmented Generation (RAG) in the context of quantum programming, uncovering observations that are different from general-purpose code generation. To evaluate our approach, we develop a test suite to measure the impact each optimization has on the accuracy of the generated code. Our findings indicate that techniques such as structured CoT significantly improve the generation of quantum algorithms by up to 50%. In contrast, we have also found that certain techniques such as RAG show limited improvement, yielding an accuracy increase of only 4%. Moreover, we showcase examples of AI-assisted quantum error prediction and correction, demonstrating the effectiveness of our multi-agent framework in reducing the quantum errors of generated quantum programs.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expected Free Energy-based Planning as Variational Inference</title>
<link>https://arxiv.org/abs/2504.14898</link>
<guid>https://arxiv.org/abs/2504.14898</guid>
<content:encoded><![CDATA[
arXiv:2504.14898v1 Announce Type: cross 
Abstract: We address the problem of planning under uncertainty, where an agent must choose actions that not only achieve desired outcomes but also reduce uncertainty. Traditional methods often treat exploration and exploitation as separate objectives, lacking a unified inferential foundation. Active inference, grounded in the Free Energy Principle, offers such a foundation by minimizing Expected Free Energy (EFE), a cost function that combines utility with epistemic drives like ambiguity resolution and novelty seeking. However, the computational burden of EFE minimization has remained a major obstacle to its scalability. In this paper, we show that EFE-based planning arises naturally from minimizing a variational free energy functional on a generative model augmented with preference and epistemic priors. This result reinforces theoretical consistency with the Free Energy Principle, by casting planning itself as variational inference. Our formulation yields optimal policies that jointly support goal achievement and information gain, while incorporating a complexity term that accounts for bounded computational resources. This unifying framework connects and extends existing methods, enabling scalable, resource-aware implementations of active inference agents.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully Adaptive Stepsizes: Which System Benefit More -- Centralized or Decentralized?</title>
<link>https://arxiv.org/abs/2504.15196</link>
<guid>https://arxiv.org/abs/2504.15196</guid>
<content:encoded><![CDATA[
arXiv:2504.15196v1 Announce Type: cross 
Abstract: In decentralized optimization, the choice of stepsize plays a critical role in algorithm performance. A common approach is to use a shared stepsize across all agents to ensure convergence. However, selecting an optimal stepsize often requires careful tuning, which can be time-consuming and may lead to slow convergence, especially when there is significant variation in the smoothness (L-smoothness) of local objective functions across agents. Individually tuning stepsizes per agent is also impractical, particularly in large-scale networks. To address these limitations, we propose AdGT, an adaptive gradient tracking method that enables each agent to adjust its stepsize based on the smoothness of its local objective. We prove that AdGT generates a sequence of iterates that converges to the optimal consensus solution. Through numerical experiments, we compare AdGT with fixed-stepsize gradient tracking methods and demonstrate its superior performance. Additionally, we compare AdGT with adaptive gradient descent (AdGD) in a centralized setting and observe that fully adaptive stepsizes offer greater benefits in decentralized networks than in centralized ones.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Game-Theoretic Multiagent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2011.00583</link>
<guid>https://arxiv.org/abs/2011.00583</guid>
<content:encoded><![CDATA[
arXiv:2011.00583v4 Announce Type: replace 
Abstract: Following the remarkable success of the AlphaGo series, significant advances in multi-agent reinforcement learning (MARL) techniques have been witnessed. MARL corresponds to the learning problem in a multi-agent system in which multiple agents learn simultaneously. It is an interdisciplinary domain with a long history that includes game theory, machine learning, stochastic control, psychology, and optimisation. Although MARL has achieved considerable empirical success in solving real-world games, there is a lack of a self-contained overview in the literature that elaborates the game theoretical foundations of modern MARL methods and summarises the recent advances. In fact, the majority of existing surveys are outdated and do not fully cover the recent developments since 2010. In this work, we provide a monograph on MARL that covers both the fundamentals and the latest developments in the research frontier. The goal of our monograph is to provide a self-contained assessment of the current state-of-the-art MARL techniques from a game theoretical perspective. We expect this work to serve as a stepping stone for both new researchers who are about to enter this fast-growing domain and existing domain experts who want to obtain a panoramic view and identify new directions based on recent advances.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetry-Breaking Augmentations for Ad Hoc Teamwork</title>
<link>https://arxiv.org/abs/2402.09984</link>
<guid>https://arxiv.org/abs/2402.09984</guid>
<content:encoded><![CDATA[
arXiv:2402.09984v2 Announce Type: replace 
Abstract: In dynamic collaborative settings, for artificial intelligence (AI) agents to better align with humans, they must adapt to novel teammates who utilise unforeseen strategies. While adaptation is often simple for humans, it can be challenging for AI agents. Our work introduces symmetry-breaking augmentations (SBA) as a novel approach to this challenge. By applying a symmetry-flipping operation to increase behavioural diversity among training teammates, SBA encourages agents to learn robust responses to unknown strategies, highlighting how social conventions impact human-AI alignment. We demonstrate this experimentally in two settings, showing that our approach outperforms previous ad hoc teamwork results in the challenging card game Hanabi. In addition, we propose a general metric for estimating symmetry dependency amongst a given set of policies. Our findings provide insights into how AI systems can better adapt to diverse human conventions and the core mechanics of alignment.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Tractable $\Phi$-Equilibria in Non-Concave Games</title>
<link>https://arxiv.org/abs/2403.08171</link>
<guid>https://arxiv.org/abs/2403.08171</guid>
<content:encoded><![CDATA[
arXiv:2403.08171v4 Announce Type: replace 
Abstract: While Online Gradient Descent and other no-regret learning procedures are known to efficiently converge to a coarse correlated equilibrium in games where each agent's utility is concave in their own strategy, this is not the case when utilities are non-concave -- a common scenario in machine learning applications involving strategies parameterized by deep neural networks, or when agents' utilities are computed by neural networks, or both. Non-concave games introduce significant game-theoretic and optimization challenges: (i) Nash equilibria may not exist; (ii) local Nash equilibria, though they exist, are intractable; and (iii) mixed Nash, correlated, and coarse correlated equilibria generally have infinite support and are intractable. To sidestep these challenges, we revisit the classical solution concept of $\Phi$-equilibria introduced by Greenwald and Jafari [2003], which is guaranteed to exist for an arbitrary set of strategy modifications $\Phi$ even in non-concave games [Stolz and Lugosi, 2007]. However, the tractability of $\Phi$-equilibria in such games remains elusive.
  In this paper, we initiate the study of tractable $\Phi$-equilibria in non-concave games and examine several natural families of strategy modifications. We show that when $\Phi$ is finite, there exists an efficient uncoupled learning algorithm that converges to the corresponding $\Phi$-equilibria. Additionally, we explore cases where $\Phi$ is infinite but consists of local modifications. We show that approximating local $\Phi$-equilibria beyond the first-order stationary regime is computationally intractable. In contrast, within this regime, we show Online Gradient Descent efficiently converges to $\Phi$-equilibria for several natural infinite families of modifications, including a new structural family of modifications inspired by the well-studied proximal operator.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contract Design for Sequential Actions</title>
<link>https://arxiv.org/abs/2403.09545</link>
<guid>https://arxiv.org/abs/2403.09545</guid>
<content:encoded><![CDATA[
arXiv:2403.09545v2 Announce Type: replace 
Abstract: We introduce a novel model of contracts with combinatorial actions that accounts for sequential and adaptive agent behavior. As in the standard model, a principal delegates the execution of a costly project to an agent. There are $n$ actions, each one incurring a cost to the agent and inducing a probability distribution over $m$ outcomes; each outcome generates some reward for the principal. The principal incentivizes the agent through a contract that specifies a payment for each potential outcome. Unlike the standard model, the agent chooses actions sequentially. Following each action, the agent observes the realized outcome, and decides whether to stop or continue with another action. Upon halting, the agent chooses one of the realized outcomes, which determines both his payment and the principal's reward. This model captures common scenarios where the agent can make multiple attempts in the course of executing a project.
  We study the optimal contract problem in this new setting, namely the contract that maximizes the principal's utility. We first observe that the agent's problem - (adaptively) finding the sequence of actions that maximizes his utility for a given contract - is equivalent to the well-known Pandora's Box problem. Using this insight, we provide algorithms and hardness results for the optimal contract problem, under both independent and correlated actions, and for both linear and general contracts. For independent actions, we provide a poly-time algorithm for the optimal linear contract, and establish that finding the optimal general contract is NP-hard. In cases where the number of outcomes is constant, we devise a poly-time algorithm even for the optimal general contract. For correlated actions, we find that, for both linear and general contracts, approximating the optimal contract within any constant ratio is NP-hard.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Airlift Challenge: A Competition for Optimizing Cargo Delivery</title>
<link>https://arxiv.org/abs/2404.17716</link>
<guid>https://arxiv.org/abs/2404.17716</guid>
<content:encoded><![CDATA[
arXiv:2404.17716v2 Announce Type: replace 
Abstract: Airlift operations require the timely distribution of various cargo, much of which is time sensitive and valuable. These operations, however, have to contend with sudden disruptions from weather and malfunctions, requiring immediate rescheduling. The Airlift Challenge competition seeks possible solutions via a simulator that provides a simplified abstraction of the airlift problem. The simulator uses an OpenAI gym interface that allows participants to create an algorithm for planning agent actions. The algorithm is scored using a remote evaluator against scenarios of ever-increasing difficulty. The second iteration of the competition was underway from November 2023 to April 2024. This paper describes the competition, simulation environment, and results. As a step towards applying generalized planning techniques to the problem, a temporal PDDL domain is presented for the Pickup and Delivery Problem, a model which lies at the core of the Airlift Challenge.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSFF: Investigating LLM Predictive Capabilities for Startup Success through a Multi-Agent Framework with Enhanced Explainability and Performance</title>
<link>https://arxiv.org/abs/2405.19456</link>
<guid>https://arxiv.org/abs/2405.19456</guid>
<content:encoded><![CDATA[
arXiv:2405.19456v2 Announce Type: replace 
Abstract: LLM based agents have recently demonstrated strong potential in automating complex tasks, yet accurately predicting startup success remains an open challenge with few benchmarks and tailored frameworks. To address these limitations, we propose the Startup Success Forecasting Framework, an autonomous system that emulates the reasoning of venture capital analysts through a multi agent collaboration model. Our framework integrates traditional machine learning methods such as random forests and neural networks within a retrieval augmented generation framework composed of three interconnected modules: a prediction block, an analysis block, and an external knowledge block. We evaluate our framework and identify three main findings. First, by leveraging founder segmentation, startups led by L5 founders are 3.79 times more likely to succeed than those led by L1 founders. Second, baseline large language models consistently overpredict startup success and struggle under realistic class imbalances largely due to overreliance on founder claims. Third, our framework significantly enhances prediction accuracy, yielding a 108.3 percent relative improvement over GPT 4o mini and a 30.8 percent relative improvement over GPT 4o. These results demonstrate the value of a multi agent approach combined with discriminative machine learning in mitigating the limitations of standard large language model based prediction methods.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HOPE: A Reinforcement Learning-based Hybrid Policy Path Planner for Diverse Parking Scenarios</title>
<link>https://arxiv.org/abs/2405.20579</link>
<guid>https://arxiv.org/abs/2405.20579</guid>
<content:encoded><![CDATA[
arXiv:2405.20579v4 Announce Type: replace 
Abstract: Automated parking stands as a highly anticipated application of autonomous driving technology. However, existing path planning methodologies fall short of addressing this need due to their incapability to handle the diverse and complex parking scenarios in reality. While non-learning methods provide reliable planning results, they are vulnerable to intricate occasions, whereas learning-based ones are good at exploration but unstable in converging to feasible solutions. To leverage the strengths of both approaches, we introduce Hybrid pOlicy Path plannEr (HOPE). This novel solution integrates a reinforcement learning agent with Reeds-Shepp curves, enabling effective planning across diverse scenarios. HOPE guides the exploration of the reinforcement learning agent by applying an action mask mechanism and employs a transformer to integrate the perceived environmental information with the mask. To facilitate the training and evaluation of the proposed planner, we propose a criterion for categorizing the difficulty level of parking scenarios based on space and obstacle distribution. Experimental results demonstrate that our approach outperforms typical rule-based algorithms and traditional reinforcement learning methods, showing higher planning success rates and generalization across various scenarios. We also conduct real-world experiments to verify the practicability of HOPE. The code for our solution is openly available on https://github.com/jiamiya/HOPE.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RILe: Reinforced Imitation Learning</title>
<link>https://arxiv.org/abs/2406.08472</link>
<guid>https://arxiv.org/abs/2406.08472</guid>
<content:encoded><![CDATA[
arXiv:2406.08472v4 Announce Type: replace 
Abstract: Acquiring complex behaviors is essential for artificially intelligent agents, yet learning these behaviors in high-dimensional settings poses a significant challenge due to the vast search space. Traditional reinforcement learning (RL) requires extensive manual effort for reward function engineering. Inverse reinforcement learning (IRL) uncovers reward functions from expert demonstrations but relies on an iterative process that is often computationally expensive. Imitation learning (IL) provides a more efficient alternative by directly comparing an agent's actions to expert demonstrations; however, in high-dimensional environments, such direct comparisons often offer insufficient feedback for effective learning. We introduce RILe (Reinforced Imitation Learning), a framework that combines the strengths of imitation learning and inverse reinforcement learning to learn a dense reward function efficiently and achieve strong performance in high-dimensional tasks. RILe employs a novel trainer-student framework: the trainer learns an adaptive reward function, and the student uses this reward signal to imitate expert behaviors. By dynamically adjusting its guidance as the student evolves, the trainer provides nuanced feedback across different phases of learning. Our framework produces high-performing policies in high-dimensional tasks where direct imitation fails to replicate complex behaviors. We validate RILe in challenging robotic locomotion tasks, demonstrating that it significantly outperforms existing methods and achieves near-expert performance across multiple settings.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenHands: An Open Platform for AI Software Developers as Generalist Agents</title>
<link>https://arxiv.org/abs/2407.16741</link>
<guid>https://arxiv.org/abs/2407.16741</guid>
<content:encoded><![CDATA[
arXiv:2407.16741v3 Announce Type: replace 
Abstract: Software is one of the most powerful tools that we humans have at our disposal; it allows a skilled programmer to interact with the world in complex and profound ways. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. In this paper, we introduce OpenHands (f.k.a. OpenDevin), a platform for the development of powerful and flexible AI agents that interact with the world in similar ways to those of a human developer: by writing code, interacting with a command line, and browsing the web. We describe how the platform allows for the implementation of new agents, safe interaction with sandboxed environments for code execution, coordination between multiple agents, and incorporation of evaluation benchmarks. Based on our currently incorporated benchmarks, we perform an evaluation of agents over 15 challenging tasks, including software engineering (e.g., SWE-BENCH) and web browsing (e.g., WEBARENA), among others. Released under the permissive MIT license, OpenHands is a community project spanning academia and industry with more than 2.1K contributions from over 188 contributors.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Goal-Conditioned RL Algorithms and Research</title>
<link>https://arxiv.org/abs/2408.11052</link>
<guid>https://arxiv.org/abs/2408.11052</guid>
<content:encoded><![CDATA[
arXiv:2408.11052v3 Announce Type: replace 
Abstract: Self-supervision has the potential to transform reinforcement learning (RL), paralleling the breakthroughs it has enabled in other areas of machine learning. While self-supervised learning in other domains aims to find patterns in a fixed dataset, self-supervised goal-conditioned reinforcement learning (GCRL) agents discover new behaviors by learning from the goals achieved during unstructured interaction with the environment. However, these methods have failed to see similar success, both due to a lack of data from slow environment simulations as well as a lack of stable algorithms. We take a step toward addressing both of these issues by releasing a high-performance codebase and benchmark (JaxGCRL) for self-supervised GCRL, enabling researchers to train agents for millions of environment steps in minutes on a single GPU. By utilizing GPU-accelerated replay buffers, environments, and a stable contrastive RL algorithm, we reduce training time by up to $22\times$. Additionally, we assess key design choices in contrastive RL, identifying those that most effectively stabilize and enhance training performance. With this approach, we provide a foundation for future research in self-supervised GCRL, enabling researchers to quickly iterate on new ideas and evaluate them in diverse and challenging environments. Website + Code: https://github.com/MichalBortkiewicz/JaxGCRL
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-evolving Agents with reflective and memory-augmented abilities</title>
<link>https://arxiv.org/abs/2409.00872</link>
<guid>https://arxiv.org/abs/2409.00872</guid>
<content:encoded><![CDATA[
arXiv:2409.00872v2 Announce Type: replace 
Abstract: Large language models (LLMs) have made significant advances in the field of natural language processing, but they still face challenges such as continuous decision-making. In this research, we propose a novel framework by integrating iterative feedback, reflective mechanisms, and a memory optimization mechanism based on the Ebbinghaus forgetting curve, it significantly enhances the agents' capabilities in handling multi-tasking and long-span information.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlendRL: A Framework for Merging Symbolic and Neural Policy Learning</title>
<link>https://arxiv.org/abs/2410.11689</link>
<guid>https://arxiv.org/abs/2410.11689</guid>
<content:encoded><![CDATA[
arXiv:2410.11689v2 Announce Type: replace 
Abstract: Humans can leverage both symbolic reasoning and intuitive reactions. In contrast, reinforcement learning policies are typically encoded in either opaque systems like neural networks or symbolic systems that rely on predefined symbols and rules. This disjointed approach severely limits the agents' capabilities, as they often lack either the flexible low-level reaction characteristic of neural agents or the interpretable reasoning of symbolic agents. To overcome this challenge, we introduce BlendRL, a neuro-symbolic RL framework that harmoniously integrates both paradigms within RL agents that use mixtures of both logic and neural policies. We empirically demonstrate that BlendRL agents outperform both neural and symbolic baselines in standard Atari environments, and showcase their robustness to environmental changes. Additionally, we analyze the interaction between neural and symbolic policies, illustrating how their hybrid use helps agents overcome each other's limitations.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaCTG: Multi-Agent Collaborative Thought Graph for Automatic Programming</title>
<link>https://arxiv.org/abs/2410.19245</link>
<guid>https://arxiv.org/abs/2410.19245</guid>
<content:encoded><![CDATA[
arXiv:2410.19245v2 Announce Type: replace 
Abstract: With the rapid advancement of Large Language Models (LLMs), LLM-based approaches have demonstrated strong problem-solving capabilities across various domains. However, in automatic programming, a single LLM is typically limited to function-level code generation, while multi-agent systems composed of multiple LLMs often suffer from inefficient task planning. This lack of structured coordination can lead to cascading hallucinations, where accumulated errors across agents result in suboptimal workflows and excessive computational costs. To overcome these challenges, we introduce MaCTG (Multi-Agent Collaborative Thought Graph), a novel multi-agent framework that employs a dynamic graph structure to facilitate precise task allocation and controlled collaboration among LLM agents. MaCTG autonomously assigns agent roles based on programming requirements, dynamically refines task distribution through context-aware adjustments, and systematically verifies and integrates project-level code, effectively reducing hallucination errors and improving overall accuracy. MaCTG enhances cost-effectiveness by implementing a hybrid LLM deployment, where proprietary models handle complex reasoning, while open-source models are used for routine coding and validation tasks. To evaluate MaCTG's effectiveness, we applied it to traditional image processing auto-programming tasks, achieving a state-of-the-art accuracy of 83.33%. Additionally, by leveraging its hybrid LLM configuration, MaCTG significantly reduced operational costs by 89.09% compared to existing multi-agent frameworks, demonstrating its efficiency, scalability, and real-world applicability.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReSpAct: Harmonizing Reasoning, Speaking, and Acting Towards Building Large Language Model-Based Conversational AI Agents</title>
<link>https://arxiv.org/abs/2411.00927</link>
<guid>https://arxiv.org/abs/2411.00927</guid>
<content:encoded><![CDATA[
arXiv:2411.00927v2 Announce Type: replace 
Abstract: Large language model (LLM)-based agents are increasingly employed to interact with external environments (e.g., games, APIs, world models) to solve user-provided tasks. However, current frameworks often lack the ability to collaborate effectively with users in fully conversational settings. Conversations are essential for aligning on task details, achieving user-defined goals, and satisfying preferences. While existing agents address ambiguity through clarification questions, they underutilize the broader potential of an LLM's conversational capabilities. In this work, we introduce ReSpAct, an LLM-based agent designed to seamlessly integrate reasoning, decision-making, and dynamic dialogue for task-solving. Expanding on reasoning-first approaches like ReAct, ReSpAct employs active, free-flowing dialogues to interpret instructions, clarify goals, provide status updates, resolve subtask failures, and refine plans based on user inputs without any explicit dialogue schema. By alternating between task-solving actions and interactive conversations, ReSpAct demonstrates improved performance across diverse environments. We evaluate ReSpAct in user-interactive settings, including task-oriented dialogue systems (MultiWOZ) and decision-making tasks (ALFWorld, WebShop). ReSpAct outperforms ReAct with absolute success rate improvements of 6% and 4% in ALFWorld and WebShop, respectively, and achieves a 5.5% gain in Inform and a 3% gain in Success scores in MultiWOZ. These results highlight the value of integrating dynamic user-agent collaboration for more effective task resolution.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EF2X Exists For Four Agents</title>
<link>https://arxiv.org/abs/2412.00254</link>
<guid>https://arxiv.org/abs/2412.00254</guid>
<content:encoded><![CDATA[
arXiv:2412.00254v2 Announce Type: replace 
Abstract: We study the fair allocation of indivisible goods among a group of agents, aiming to limit the envy between any two agents. The central open problem in this literature, which has proven to be extremely challenging, is regarding the existence of an EFX allocation, i.e., an allocation such that any envy from some agent i toward another agent j would vanish if we were to remove any single good from the bundle allocated to j. When the agents' valuations are additive, which has been the main focus of prior works, Chaudhury et al. [2024] showed that an EFX allocation is guaranteed to exist for all instances involving up to three agents. Subsequently, Berger et al. [2022] extended this guarantee to nice-cancelable valuations and Akrami et al. [2023] to MMS-feasible valuations. However, the existence of EFX allocations for instances involving four agents remains open, even for additive valuations. We contribute to this literature by focusing on EF2X, a relaxation of EFX which requires that any envy toward some agent vanishes if any two of the goods allocated to that agent were to be removed. Our main result shows that EF2X allocations are guaranteed to exist for any instance with four agents, even for the class of cancelable valuations, which is more general than additive. Our proof is constructive, proposing an algorithm that computes such an allocation in pseudopolynomial time. Furthermore, for instances involving three agents we provide an algorithm that computes an EF2X allocation in polynomial time, in contrast to EFX, for which the fastest known algorithm for three agents is only pseudopolynomial.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simultaneously Satisfying MXS and EFL</title>
<link>https://arxiv.org/abs/2412.00358</link>
<guid>https://arxiv.org/abs/2412.00358</guid>
<content:encoded><![CDATA[
arXiv:2412.00358v3 Announce Type: replace 
Abstract: The two standard fairness notions in the resource allocation literature are proportionality and envy-freeness. If there are n agents competing for the available resources, then proportionality requires that each agent receives at least a 1/n fraction of their total value for the set of resources. On the other hand, envy-freeness requires that each agent weakly prefers the resources allocated to them over those allocated to any other agent. Each of these notions has its own benefits, but it is well known that neither one of the two is always achievable when the resources being allocated are indivisible. As a result, a lot of work has focused on satisfying fairness notions that relax either proportionality or envy-freeness. In this paper, we focus on MXS (a relaxation of proportionality) and EFL (a relaxation of envy-freeness). Each of these notions was previously shown to be achievable on its own [Barman et al.,2018, Caragiannis et al., 2023], and our main result is an algorithm that computes allocations that simultaneously satisfy both, combining the benefits of approximate proportionality and approximate envy-freeness. In fact, we prove this for any instance involving agents with valuation functions that are restricted MMS-feasible, which are more general than additive valuations. Also, since every EFL allocation directly satisfies other well-studied fairness notions like EF1, 1/2-EFX, 1/2-GMMS, and 2/3-PMMS, and every MXS allocation satisfies 4/7-MMS, the allocations returned by our algorithm simultaneously satisfy a wide variety of fairness notions and are, therefore, universally fair [Amanatidis et al., 2020].
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steganography in Game Actions</title>
<link>https://arxiv.org/abs/2412.10442</link>
<guid>https://arxiv.org/abs/2412.10442</guid>
<content:encoded><![CDATA[
arXiv:2412.10442v2 Announce Type: replace 
Abstract: The exchange of messages has always carried with it the timeless challenge of secrecy. From whispers in shadows to the enigmatic notes written in the margins of history, humanity has long sought ways to convey thoughts that remain imperceptible to all but the chosen few. The challenge of subliminal communication has been addressed in various forms of steganography. However, the field faces a fundamental paradox: as the art of concealment advances, so too does the science of revelation, leading to an ongoing evolutionary interplay. This study seeks to extend the boundaries of what is considered a viable steganographic medium. We explore a steganographic paradigm, in which hidden information is communicated through the episodes of multiple agents interacting with an environment. Each agent, acting as an encoder, learns a policy to disguise the very existence of hidden messages within actions seemingly directed toward innocent objectives. Meanwhile, an observer, serving as a decoder, learns to associate behavioural patterns with their respective agents despite their dynamic nature, thereby unveiling the hidden messages. The interactions of agents are governed by the framework of multi-agent reinforcement learning and shaped by feedback from the observer. This framework encapsulates a game-theoretic dilemma, wherein agents face decisions between cooperating to create distinguishable behavioural patterns or defecting to pursue individually optimal yet potentially overlapping episodic actions. As a proof of concept, we exemplify action steganography through the game of labyrinth, a navigation task where subliminal communication is concealed within the act of steering toward a destination, and systematically validate the stego-system in terms of distortion, capacity, secrecy and robustness when subjected to simulated passive and active adversaries.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The EnvDesign Model: A Method to Solve the Environment Design Problem</title>
<link>https://arxiv.org/abs/2412.18109</link>
<guid>https://arxiv.org/abs/2412.18109</guid>
<content:encoded><![CDATA[
arXiv:2412.18109v3 Announce Type: replace 
Abstract: Today, several people and organizations rely on cloud platforms. The reliability of cloud platforms depends heavily on the performance of their internal programs (agents). To better prevent regressions in cloud platforms, the design of pre-production testing environments (that test new agents, new hardwares, and other changes) must take into account the diversity of server/node properties (hardware model, virtual machine type, etc.) across the fleet and dynamically emphasize or de-emphasize the prevalence of certain node properties based on current testing priorities. This paper formulates this task as the ``environment design" problem and presents the EnvDesign model, a method that uses graph theory and optimization algorithms to solve the environment design problem. The EnvDesign model was built on context and techniques that apply to combinatorial testing in general, so it can support combinatorial testing in other domains. An earlier version of this paper was peer-reviewed and published internally at Microsoft.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Humanoid Locomotion and Manipulation: Current Progress and Challenges in Control, Planning, and Learning</title>
<link>https://arxiv.org/abs/2501.02116</link>
<guid>https://arxiv.org/abs/2501.02116</guid>
<content:encoded><![CDATA[
arXiv:2501.02116v2 Announce Type: replace 
Abstract: Humanoid robots hold great potential to perform various human-level skills, involving unified locomotion and manipulation in real-world settings. Driven by advances in machine learning and the strength of existing model-based approaches, these capabilities have progressed rapidly, but often separately. This survey offers a comprehensive overview of the state-of-the-art in humanoid locomotion and manipulation (HLM), with a focus on control, planning, and learning methods. We first review the model-based methods that have been the backbone of humanoid robotics for the past three decades. We discuss contact planning, motion planning, and whole-body control, highlighting the trade-offs between model fidelity and computational efficiency. Then the focus is shifted to examine emerging learning-based methods, with an emphasis on reinforcement and imitation learning that enhance the robustness and versatility of loco-manipulation skills. Furthermore, we assess the potential of integrating foundation models with humanoid embodiments to enable the development of generalist humanoid agents. This survey also highlights the emerging role of tactile sensing, particularly whole-body tactile feedback, as a crucial modality for handling contact-rich interactions. Finally, we compare the strengths and limitations of model-based and learning-based paradigms from multiple perspectives, such as robustness, computational efficiency, versatility, and generalizability, and suggest potential solutions to existing challenges.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Enable Effective Cooperation Between Humans and NLP Models: A Survey of Principles, Formalizations, and Beyond</title>
<link>https://arxiv.org/abs/2501.05714</link>
<guid>https://arxiv.org/abs/2501.05714</guid>
<content:encoded><![CDATA[
arXiv:2501.05714v2 Announce Type: replace 
Abstract: With the advancement of large language models (LLMs), intelligent models have evolved from mere tools to autonomous agents with their own goals and strategies for cooperating with humans. This evolution has birthed a novel paradigm in NLP, i.e., human-model cooperation, that has yielded remarkable progress in numerous NLP tasks in recent years. In this paper, we take the first step to present a thorough review of human-model cooperation, exploring its principles, formalizations, and open challenges. In particular, we introduce a new taxonomy that provides a unified perspective to summarize existing approaches. Also, we discuss potential frontier areas and their corresponding challenges. We regard our work as an entry point, paving the way for more breakthrough research in this regard.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Theory of Mind Enables the Invention of Proto-Writing</title>
<link>https://arxiv.org/abs/2502.01568</link>
<guid>https://arxiv.org/abs/2502.01568</guid>
<content:encoded><![CDATA[
arXiv:2502.01568v4 Announce Type: replace 
Abstract: Symbolic writing systems are graphical semiotic codes that are ubiquitous in modern society but are otherwise absent in the animal kingdom. Anthropological evidence suggests that the earliest forms of some writing systems originally consisted of iconic pictographs, which signify their referent via visual resemblance. While previous studies have examined the emergence and, separately, the evolution of pictographic systems through a computational lens, most employ non-naturalistic methodologies that make it difficult to draw clear analogies to human and animal cognition. We develop a multi-agent reinforcement learning testbed for emergent communication called a Signification Game, and formulate a model of inferential communication that enables agents to leverage visual theory of mind to communicate actions using pictographs. Our model, which is situated within a broader formalism for animal communication, sheds light on the cognitive and cultural processes underlying the emergence of proto-writing.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual Descriptions Using Gaussian Splatting, ChatGPT/Deepseek, and Google Maps Platform</title>
<link>https://arxiv.org/abs/2502.05769</link>
<guid>https://arxiv.org/abs/2502.05769</guid>
<content:encoded><![CDATA[
arXiv:2502.05769v3 Announce Type: replace 
Abstract: Urban digital twins are virtual replicas of cities that use multi-source data and data analytics to optimize urban planning, infrastructure management, and decision-making. Towards this, we propose a framework focused on the single-building scale. By connecting to cloud mapping platforms such as Google Map Platforms APIs, by leveraging state-of-the-art multi-agent Large Language Models data analysis using ChatGPT(4o) and Deepseek-V3/R1, and by using our Gaussian Splatting-based mesh extraction pipeline, our Digital Twin Buildings framework can retrieve a building's 3D model, visual descriptions, and achieve cloud-based mapping integration with large language model-based data analytics using a building's address, postal code, or geographic coordinates.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grassroots Platforms with Atomic Transactions: Social Networks, Cryptocurrencies, and Democratic Federations</title>
<link>https://arxiv.org/abs/2502.11299</link>
<guid>https://arxiv.org/abs/2502.11299</guid>
<content:encoded><![CDATA[
arXiv:2502.11299v4 Announce Type: replace 
Abstract: Grassroots platforms aim to offer an egalitarian alternative to global platforms. Whereas global platforms can have only a single instance, grassroots platforms can have multiple instances that emerge and operate independently of each other and of any global resource except the network, and can interoperate and coalesce into ever-larger instances once interconnected. Key grassroots platforms include grassroots social networks, grassroots cryptocurrencies, and grassroots democratic federations. Previously, grassroots platforms were defined formally and proven grassroots using unary distributed transition systems, in which each transition is carried out by a single agent. However, grassroots platforms cater for a more abstract specification using transactions carried out atomically by multiple agents, something that cannot be expressed by unary transition systems. As a result, their original specifications and proofs were unnecessarily cumbersome and opaque.
  We enhance the notion of a distributed transition system to include atomic transactions and revisit the notion of grassroots platforms within this new foundation; present crisp specifications of key grassroots platforms using atomic transactions: befriending and defriending for grassroots social networks, coin swaps for grassroots cryptocurrencies, and communities forming, joining, and leaving a federation for grassroots democratic federations; prove a general theorem that a platform specified by atomic transactions that are so-called interactive is grassroots; show that the atomic transactions used to specify all three platforms are interactive; and conclude that the platforms thus specified are indeed grassroots. We thus provide a crisp mathematical foundation for grassroots platforms and a solid and clear starting point from which their implementation can commence.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding</title>
<link>https://arxiv.org/abs/2503.10596</link>
<guid>https://arxiv.org/abs/2503.10596</guid>
<content:encoded><![CDATA[
arXiv:2503.10596v2 Announce Type: replace 
Abstract: Pixel grounding, encompassing tasks such as Referring Expression Segmentation (RES), has garnered considerable attention due to its immense potential for bridging the gap between vision and language modalities. However, advancements in this domain are currently constrained by limitations inherent in existing datasets, including limited object categories, insufficient textual diversity, and a scarcity of high-quality annotations. To mitigate these limitations, we introduce GroundingSuite, which comprises: (1) an automated data annotation framework leveraging multiple Vision-Language Model (VLM) agents; (2) a large-scale training dataset encompassing 9.56 million diverse referring expressions and their corresponding segmentations; and (3) a meticulously curated evaluation benchmark consisting of 3,800 images. The GroundingSuite training dataset facilitates substantial performance improvements, enabling models trained on it to achieve state-of-the-art results. Specifically, a cIoU of 68.9 on gRefCOCO and a gIoU of 55.3 on RefCOCOm. Moreover, the GroundingSuite annotation framework demonstrates superior efficiency compared to the current leading data annotation method, i.e., $4.5 \times$ faster than the GLaMM.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models as Quasi-crystals: Coherence Without Repetition in Generative Text</title>
<link>https://arxiv.org/abs/2504.11986</link>
<guid>https://arxiv.org/abs/2504.11986</guid>
<content:encoded><![CDATA[
arXiv:2504.11986v2 Announce Type: replace 
Abstract: This essay proposes an interpretive analogy between large language models (LLMs) and quasicrystals, systems that exhibit global coherence without periodic repetition, generated through local constraints. While LLMs are typically evaluated in terms of predictive accuracy, factuality, or alignment, this structural perspective suggests that one of their most characteristic behaviors is the production of internally resonant linguistic patterns. Drawing on the history of quasicrystals, which forced a redefinition of structural order in physical systems, the analogy highlights an alternative mode of coherence in generative language: constraint-based organization without repetition or symbolic intent. Rather than viewing LLMs as imperfect agents or stochastic approximators, we suggest understanding them as generators of quasi-structured outputs. This framing complements existing evaluation paradigms by foregrounding formal coherence and pattern as interpretable features of model behavior. While the analogy has limits, it offers a conceptual tool for exploring how coherence might arise and be assessed in systems where meaning is emergent, partial, or inaccessible. In support of this perspective, we draw on philosophy of science and language, including model-based accounts of scientific representation, structural realism, and inferentialist views of meaning. We further propose the notion of structural evaluation: a mode of assessment that examines how well outputs propagate constraint, variation, and order across spans of generated text. This essay aims to reframe the current discussion around large language models, not by rejecting existing methods, but by suggesting an additional axis of interpretation grounded in structure rather than semantics.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication Optimization for Decentralized Learning atop Bandwidth-limited Edge Networks</title>
<link>https://arxiv.org/abs/2504.12210</link>
<guid>https://arxiv.org/abs/2504.12210</guid>
<content:encoded><![CDATA[
arXiv:2504.12210v2 Announce Type: replace 
Abstract: Decentralized federated learning (DFL) is a promising machine learning paradigm for bringing artificial intelligence (AI) capabilities to the network edge. Running DFL on top of edge networks, however, faces severe performance challenges due to the extensive parameter exchanges between agents. Most existing solutions for these challenges were based on simplistic communication models, which cannot capture the case of learning over a multi-hop bandwidth-limited network. In this work, we address this problem by jointly designing the communication scheme for the overlay network formed by the agents and the mixing matrix that controls the communication demands between the agents. By carefully analyzing the properties of our problem, we cast each design problem into a tractable optimization and develop an efficient algorithm with guaranteed performance. Our evaluations based on real topology and data show that the proposed algorithm can reduce the total training time by over $80\%$ compared to the baseline without sacrificing accuracy, while significantly improving the computational efficiency over the state of the art.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Arabic Speech Recognition Through Large-Scale Weakly Supervised Learning</title>
<link>https://arxiv.org/abs/2504.12254</link>
<guid>https://arxiv.org/abs/2504.12254</guid>
<content:encoded><![CDATA[
arXiv:2504.12254v2 Announce Type: replace 
Abstract: Automatic speech recognition (ASR) is crucial for human-machine interaction in diverse applications like conversational agents, industrial robotics, call center automation, and automated subtitling. However, developing high-performance ASR models remains challenging, particularly for low-resource languages like Arabic, due to the scarcity of large, labeled speech datasets, which are costly and labor-intensive to produce. In this work, we employ weakly supervised learning to train an Arabic ASR model using the Conformer architecture. Our model is trained from scratch on 15,000 hours of weakly annotated speech data covering both Modern Standard Arabic (MSA) and Dialectal Arabic (DA), eliminating the need for costly manual transcriptions. Despite the absence of human-verified labels, our approach achieves state-of-the-art (SOTA) results in Arabic ASR, surpassing both open and closed-source models on standard benchmarks. By demonstrating the effectiveness of weak supervision as a scalable, cost-efficient alternative to traditional supervised approaches, paving the way for improved ASR systems in low resource settings.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Strategic Coordination Framework of Small LLMs Matches Large LLMs in Data Synthesis</title>
<link>https://arxiv.org/abs/2504.12322</link>
<guid>https://arxiv.org/abs/2504.12322</guid>
<content:encoded><![CDATA[
arXiv:2504.12322v2 Announce Type: replace 
Abstract: While data synthesis and distillation are promising strategies to enhance small language models, current approaches heavily rely on Large Language Models (LLMs), which suffer from high computational costs, environmental inefficiency, and potential biases inherited from monolithic architectures. In contrast, smaller LLMs are more accessible and sustainable, but their individual capabilities often fall short in generating high-quality, diverse, and reliable data. Inspired by collaborative human processes (e.g., peer review), we propose a multiple small LLMs involved framework, GRA, that aggregates specialized roles across small LLMs to iterative refinement and quality control typically achieved by a single large LLM. In this collaborative framework, multiple small LLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a peer-review-inspired data synthesis pipeline. The Generator proposes initial data samples, the Reviewer critiques their quality and diversity, and the Adjudicator resolves conflicts to finalize the output. By decomposing the synthesis process into specialized sub-tasks, collaborative small LLMs can achieve data-level parity with large LLM-based distillation. Through experiments across multiple benchmarks, we demonstrate that GRA-produced data matches or exceeds the quality of single large LLM outputs, e.g., Qwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large models for high-quality data synthesis, advocating instead for strategic coordination of smaller agents. Our datasets, models, and code are publicly available at https://github.com/GX-XinGao/GRA.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anonymous Public Announcements</title>
<link>https://arxiv.org/abs/2504.12546</link>
<guid>https://arxiv.org/abs/2504.12546</guid>
<content:encoded><![CDATA[
arXiv:2504.12546v2 Announce Type: replace 
Abstract: We formalise the notion of an anonymous public announcement in the tradition of public announcement logic. Such announcements can be seen as in-between a public announcement from ``the outside" (an announcement of $\phi$) and a public announcement by one of the agents (an announcement of $K_a\phi$): we get more information than just $\phi$, but not (necessarily) about exactly who made it. Even if such an announcement is prima facie anonymous, depending on the background knowledge of the agents it might reveal the identity of the announcer: if I post something on a message board, the information might reveal who I am even if I don't sign my name. Furthermore, like in the Russian Cards puzzle, if we assume that the announcer's intention was to stay anonymous, that in fact might reveal more information. In this paper we first look at the case when no assumption about intentions are made, in which case the logic with an anonymous public announcement operator is reducible to epistemic logic. We then look at the case when we assume common knowledge of the intention to stay anonymous, which is both more complex and more interesting: in several ways it boils down to the notion of a ``safe" announcement (again, similarly to Russian Cards). Main results include formal expressivity results and axiomatic completeness for key logical languages.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination</title>
<link>https://arxiv.org/abs/2504.12714</link>
<guid>https://arxiv.org/abs/2504.12714</guid>
<content:encoded><![CDATA[
arXiv:2504.12714v2 Announce Type: replace 
Abstract: Zero-shot coordination (ZSC), the ability to adapt to a new partner in a cooperative task, is a critical component of human-compatible AI. While prior work has focused on training agents to cooperate on a single task, these specialized models do not generalize to new tasks, even if they are highly similar. Here, we study how reinforcement learning on a distribution of environments with a single partner enables learning general cooperative skills that support ZSC with many new partners on many new problems. We introduce two Jax-based, procedural generators that create billions of solvable coordination challenges. We develop a new paradigm called Cross-Environment Cooperation (CEC), and show that it outperforms competitive baselines quantitatively and qualitatively when collaborating with real people. Our findings suggest that learning to collaborate across many unique scenarios encourages agents to develop general norms, which prove effective for collaboration with different partners. Together, our results suggest a new route toward designing generalist cooperative agents capable of interacting with humans without requiring human data.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Expert Failures Improves LLM Agent Tuning</title>
<link>https://arxiv.org/abs/2504.13145</link>
<guid>https://arxiv.org/abs/2504.13145</guid>
<content:encoded><![CDATA[
arXiv:2504.13145v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown tremendous potential as agents, excelling at tasks that require multiple rounds of reasoning and interactions. Rejection Sampling Fine-Tuning (RFT) has emerged as an effective method for finetuning LLMs as agents: it first imitates expert-generated successful trajectories and further improves agentic skills through iterative fine-tuning on successful, self-generated trajectories. However, since the expert (e.g., GPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler scenarios, many complex subtasks remain unsolved and persistently out-of-distribution (OOD). Upon investigating these challenging subtasks, we discovered that previously failed expert trajectories can often provide valuable guidance, e.g., plans and key actions, that can significantly improve agent exploration efficiency and acquisition of critical skills. Motivated by these observations, we propose Exploring Expert Failures (EEF), which identifies beneficial actions from failed expert trajectories and integrates them into the training dataset. Potentially harmful actions are meticulously excluded to prevent contamination of the model learning process. By leveraging the beneficial actions in expert failures, EEF successfully solves some previously unsolvable subtasks and improves agent tuning performance. Remarkably, our approach achieved a 62\% win rate in WebShop, outperforming RFT (53. 6\%) and GPT-4 (35. 6\%), and to the best of our knowledge, setting a new state-of-the-art as the first method to surpass a score of 0.81 in WebShop and exceed 81 in SciWorld.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The emergence of chaos in population game dynamics induced by comparisons</title>
<link>https://arxiv.org/abs/2412.06037</link>
<guid>https://arxiv.org/abs/2412.06037</guid>
<content:encoded><![CDATA[
arXiv:2412.06037v2 Announce Type: replace-cross 
Abstract: Precise description of population game dynamics introduced by revision protocols - an economic model describing the agent's propensity to switch to a better-performing strategy - is of importance in economics and social sciences in general. In this setting innovation or imitation of others is the force which drives the evolution of the economic system. As the continuous-time game dynamics is relatively well understood, the same cannot be said about revision driven dynamics in the discrete time. We investigate the behavior of agents in a $2\times 2$ anti-coordination game with symmetric random matching and a unique mixed Nash equilibrium. In continuous time the Nash equilibrium is attracting and induces a global evolutionary stable state. We show that in the discrete time one can construct (either innovative or imitative) revision protocol and choose a level of the time step, under which the game dynamics is Li-Yorke chaotic, inducing complex and unpredictable behavior of the system, precluding stable predictions of equilibrium. Moreover, we reveal that this unpredictability is encoded into any imitative revision protocol. Furthermore, we show that for any such game there exists a perturbed pairwise proportional imitation protocol introducing chaotic behavior of the agents for sufficiently large time step.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Bayesian Interpretation of the Internal Model Principle</title>
<link>https://arxiv.org/abs/2503.00511</link>
<guid>https://arxiv.org/abs/2503.00511</guid>
<content:encoded><![CDATA[
arXiv:2503.00511v2 Announce Type: replace-cross 
Abstract: The internal model principle, originally proposed in the theory of control of linear systems, nowadays represents a more general class of results in control theory and cybernetics. The central claim of these results is that, under suitable assumptions, if a system (a controller) can regulate against a class of external inputs (from the environment), it is because the system contains a model of the system causing these inputs, which can be used to generate signals counteracting them. Similar claims on the role of internal models appear also in cognitive science, especially in modern Bayesian treatments of cognitive agents, often suggesting that a system (a human subject, or some other agent) models its environment to adapt against disturbances and perform goal-directed behaviour. It is however unclear whether the Bayesian internal models discussed in cognitive science bear any formal relation to the internal models invoked in standard treatments of control theory. Here, we first review the internal model principle and present a precise formulation of it using concepts inspired by categorical systems theory. This leads to a formal definition of ``model'' generalising its use in the internal model principle. Although this notion of model is not a priori related to the notion of Bayesian reasoning, we show that it can be seen as a special case of possibilistic Bayesian filtering. This result is based on a recent line of work formalising, using Markov categories, a notion of ``interpretation'', describing when a system can be interpreted as performing Bayesian filtering on an outside world in a consistent way.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Factors That Influence the Adoption of AI-enabled Conversational Agents (AICAs) as an Augmenting Therapeutic Tool by Frontline Healthcare Workers: From Technology Acceptance Model 3 (TAM3) Lens -- A Systematic Mapping Review</title>
<link>https://arxiv.org/abs/2504.13183</link>
<guid>https://arxiv.org/abs/2504.13183</guid>
<content:encoded><![CDATA[
arXiv:2504.13183v1 Announce Type: new 
Abstract: Artificial intelligent (AI) conversational agents hold a promising future in the field of mental health, especially in helping marginalized communities that lack access to mental health support services. It is tempting to have a 24/7 mental health companion that can be accessed anywhere using mobile phones to provide therapist-like advice. Yet, caution should be taken, and studies around their feasibility need to be surveyed. Before adopting such a rapidly changing technology, studies on its feasibility should be explored, summarized, and synthesized to gain a solid understanding of the status quo and to enable us to build a framework that can guide us throughout the development and deployment processes. Different perspectives must be considered when investigating the feasibility of AI conversational agents, including the mental healthcare professional perspective. The literature can provide insights into their perspectives in terms of opportunities, concerns, and implications. Mental health professionals, the subject-matter experts in this field, have their points of view that should be understood and considered. This systematic literature review will explore mental health practitioners' attitudes toward AI conversational agents and the factors that affect their adoption and recommendation of the technology to augment their services and treatments. The TAM3 Framework will be the lens through which this systematic literature review will be conducted.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cellular-X: An LLM-empowered Cellular Agent for Efficient Base Station Operations</title>
<link>https://arxiv.org/abs/2504.13190</link>
<guid>https://arxiv.org/abs/2504.13190</guid>
<content:encoded><![CDATA[
arXiv:2504.13190v1 Announce Type: new 
Abstract: This paper introduces Cellular-X, an LLM-powered agent designed to automate cellular base station (BS) maintenance. Leveraging multimodal LLM and retrieval-augmented generation (RAG) techniques, Cellular-X significantly enhances field engineer efficiency by quickly interpreting user intents, retrieving relevant technical information, and configuring a BS through iterative self-correction. Key features of the demo include automatic customized BS setup, document-based query answering, and voice-controlled configuration reporting and revision. We implemented Cellular-X on a USRP X310 testbed for demonstration. Demo videos and implementation details are available at https://github.com/SeaBreezing/Cellular-X.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent</title>
<link>https://arxiv.org/abs/2504.13192</link>
<guid>https://arxiv.org/abs/2504.13192</guid>
<content:encoded><![CDATA[
arXiv:2504.13192v1 Announce Type: new 
Abstract: Recently, Large Language Model (LLM)-empowered recommender systems (RecSys) have brought significant advances in personalized user experience and have attracted considerable attention. Despite the impressive progress, the research question regarding the safety vulnerability of LLM-empowered RecSys still remains largely under-investigated. Given the security and privacy concerns, it is more practical to focus on attacking the black-box RecSys, where attackers can only observe the system's inputs and outputs. However, traditional attack approaches employing reinforcement learning (RL) agents are not effective for attacking LLM-empowered RecSys due to the limited capabilities in processing complex textual inputs, planning, and reasoning. On the other hand, LLMs provide unprecedented opportunities to serve as attack agents to attack RecSys because of their impressive capability in simulating human-like decision-making processes. Therefore, in this paper, we propose a novel attack framework called CheatAgent by harnessing the human-like capabilities of LLMs, where an LLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our method first identifies the insertion position for maximum impact with minimal input modification. After that, the LLM agent is designed to generate adversarial perturbations to insert at target positions. To further improve the quality of generated perturbations, we utilize the prompt tuning technique to improve attacking strategies via feedback from the victim RecSys iteratively. Extensive experiments across three real-world datasets demonstrate the effectiveness of our proposed attacking method.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents</title>
<link>https://arxiv.org/abs/2504.13203</link>
<guid>https://arxiv.org/abs/2504.13203</guid>
<content:encoded><![CDATA[
arXiv:2504.13203v1 Announce Type: new 
Abstract: Multi-turn interactions with language models (LMs) pose critical safety risks, as harmful intent can be strategically spread across exchanges. Yet, the vast majority of prior work has focused on single-turn safety, while adaptability and diversity remain among the key challenges of multi-turn red-teaming. To address these challenges, we present X-Teaming, a scalable framework that systematically explores how seemingly harmless interactions escalate into harmful outcomes and generates corresponding attack scenarios. X-Teaming employs collaborative agents for planning, attack optimization, and verification, achieving state-of-the-art multi-turn jailbreak effectiveness and diversity with success rates up to 98.1% across representative leading open-weight and closed-source models. In particular, X-Teaming achieves a 96.2% attack success rate against the latest Claude 3.7 Sonnet model, which has been considered nearly immune to single-turn attacks. Building on X-Teaming, we introduce XGuard-Train, an open-source multi-turn safety training dataset that is 20x larger than the previous best resource, comprising 30K interactive jailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our work offers essential tools and insights for mitigating sophisticated conversational attacks, advancing the multi-turn safety of LMs.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Feasibility of Using MultiModal LLMs to Execute AR Social Engineering Attacks</title>
<link>https://arxiv.org/abs/2504.13209</link>
<guid>https://arxiv.org/abs/2504.13209</guid>
<content:encoded><![CDATA[
arXiv:2504.13209v1 Announce Type: new 
Abstract: Augmented Reality (AR) and Multimodal Large Language Models (LLMs) are rapidly evolving, providing unprecedented capabilities for human-computer interaction. However, their integration introduces a new attack surface for social engineering. In this paper, we systematically investigate the feasibility of orchestrating AR-driven Social Engineering attacks using Multimodal LLM for the first time, via our proposed SEAR framework, which operates through three key phases: (1) AR-based social context synthesis, which fuses Multimodal inputs (visual, auditory and environmental cues); (2) role-based Multimodal RAG (Retrieval-Augmented Generation), which dynamically retrieves and integrates contextual data while preserving character differentiation; and (3) ReInteract social engineering agents, which execute adaptive multiphase attack strategies through inference interaction loops. To verify SEAR, we conducted an IRB-approved study with 60 participants in three experimental configurations (unassisted, AR+LLM, and full SEAR pipeline) compiling a new dataset of 180 annotated conversations in simulated social scenarios. Our results show that SEAR is highly effective at eliciting high-risk behaviors (e.g., 93.3% of participants susceptible to email phishing). The framework was particularly effective in building trust, with 85% of targets willing to accept an attacker's call after an interaction. Also, we identified notable limitations such as ``occasionally artificial'' due to perceived authenticity gaps. This work provides proof-of-concept for AR-LLM driven social engineering attacks and insights for developing defensive countermeasures against next-generation augmented reality threats.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recursive Deep Inverse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.13241</link>
<guid>https://arxiv.org/abs/2504.13241</guid>
<content:encoded><![CDATA[
arXiv:2504.13241v1 Announce Type: new 
Abstract: Inferring an adversary's goals from exhibited behavior is crucial for counterplanning and non-cooperative multi-agent systems in domains like cybersecurity, military, and strategy games. Deep Inverse Reinforcement Learning (IRL) methods based on maximum entropy principles show promise in recovering adversaries' goals but are typically offline, require large batch sizes with gradient descent, and rely on first-order updates, limiting their applicability in real-time scenarios. We propose an online Recursive Deep Inverse Reinforcement Learning (RDIRL) approach to recover the cost function governing the adversary actions and goals. Specifically, we minimize an upper bound on the standard Guided Cost Learning (GCL) objective using sequential second-order Newton updates, akin to the Extended Kalman Filter (EKF), leading to a fast (in terms of convergence) learning algorithm. We demonstrate that RDIRL is able to recover cost and reward functions of expert agents in standard and adversarial benchmark tasks. Experiments on benchmark tasks show that our proposed approach outperforms several leading IRL algorithms.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal-Copilot: An Autonomous Causal Analysis Agent</title>
<link>https://arxiv.org/abs/2504.13263</link>
<guid>https://arxiv.org/abs/2504.13263</guid>
<content:encoded><![CDATA[
arXiv:2504.13263v1 Announce Type: new 
Abstract: Causal analysis plays a foundational role in scientific discovery and reliable decision-making, yet it remains largely inaccessible to domain experts due to its conceptual and algorithmic complexity. This disconnect between causal methodology and practical usability presents a dual challenge: domain experts are unable to leverage recent advances in causal learning, while causal researchers lack broad, real-world deployment to test and refine their methods. To address this, we introduce Causal-Copilot, an autonomous agent that operationalizes expert-level causal analysis within a large language model framework. Causal-Copilot automates the full pipeline of causal analysis for both tabular and time-series data -- including causal discovery, causal inference, algorithm selection, hyperparameter optimization, result interpretation, and generation of actionable insights. It supports interactive refinement through natural language, lowering the barrier for non-specialists while preserving methodological rigor. By integrating over 20 state-of-the-art causal analysis techniques, our system fosters a virtuous cycle -- expanding access to advanced causal methods for domain experts while generating rich, real-world applications that inform and advance causal theory. Empirical evaluations demonstrate that Causal-Copilot achieves superior performance compared to existing baselines, offering a reliable, scalable, and extensible solution that bridges the gap between theoretical sophistication and real-world applicability in causal analysis.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartQA-X: Generating Explanations for Charts</title>
<link>https://arxiv.org/abs/2504.13275</link>
<guid>https://arxiv.org/abs/2504.13275</guid>
<content:encoded><![CDATA[
arXiv:2504.13275v1 Announce Type: new 
Abstract: The ability to interpret and explain complex information from visual data in charts is crucial for data-driven decision-making. In this work, we address the challenge of providing explanations alongside answering questions about chart images. We present ChartQA-X, a comprehensive dataset comprising various chart types with 28,299 contextually relevant questions, answers, and detailed explanations. These explanations are generated by prompting six different models and selecting the best responses based on metrics such as faithfulness, informativeness, coherence, and perplexity. Our experiments show that models fine-tuned on our dataset for explanation generation achieve superior performance across various metrics and demonstrate improved accuracy in question-answering tasks on new datasets. By integrating answers with explanatory narratives, our approach enhances the ability of intelligent agents to convey complex information effectively, improve user understanding, and foster trust in the generated responses.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Definition of Robustness and Resilience of AI Agents for Real-time Congestion Management</title>
<link>https://arxiv.org/abs/2504.13314</link>
<guid>https://arxiv.org/abs/2504.13314</guid>
<content:encoded><![CDATA[
arXiv:2504.13314v1 Announce Type: new 
Abstract: The European Union's Artificial Intelligence (AI) Act defines robustness, resilience, and security requirements for high-risk sectors but lacks detailed methodologies for assessment. This paper introduces a novel framework for quantitatively evaluating the robustness and resilience of reinforcement learning agents in congestion management. Using the AI-friendly digital environment Grid2Op, perturbation agents simulate natural and adversarial disruptions by perturbing the input of AI systems without altering the actual state of the environment, enabling the assessment of AI performance under various scenarios. Robustness is measured through stability and reward impact metrics, while resilience quantifies recovery from performance degradation. The results demonstrate the framework's effectiveness in identifying vulnerabilities and improving AI robustness and resilience for critical applications.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiharmonic algorithms for contrast-enhanced ultrasound</title>
<link>https://arxiv.org/abs/2504.13335</link>
<guid>https://arxiv.org/abs/2504.13335</guid>
<content:encoded><![CDATA[
arXiv:2504.13335v1 Announce Type: new 
Abstract: Harmonic generation plays a crucial role in contrast-enhanced ultrasound, both for imaging and therapeutic applications. However, accurately capturing these nonlinear effects is computationally very demanding when using traditional time-domain approaches. To address this issue, in this work, we develop algorithms based on a time discretization that uses a multiharmonic Ansatz applied to a model that couples the Westervelt equation for acoustic pressure with a volume-based approximation of the Rayleigh--Plesset equation for the dynamics of microbubble contrast agents. We first rigorously establish the existence of time-periodic solutions for this Westervelt-ODE system. We then derive a multiharmonic representation of the system under time-periodic excitation and develop iterative algorithms that rely on the successive computation of higher harmonics under the assumption of real-valued or complex solution fields. In the real-valued setting, we characterize the approximation error in terms of the number of harmonics and a contribution owing to the fixed-point iteration. Finally, we investigate these algorithms numerically and illustrate how the number of harmonics and presence of microbubbles influence the propagation of acoustic waves.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LangCoop: Collaborative Driving with Language</title>
<link>https://arxiv.org/abs/2504.13406</link>
<guid>https://arxiv.org/abs/2504.13406</guid>
<content:encoded><![CDATA[
arXiv:2504.13406v1 Announce Type: new 
Abstract: Multi-agent collaboration holds great promise for enhancing the safety, reliability, and mobility of autonomous driving systems by enabling information sharing among multiple connected agents. However, existing multi-agent communication approaches are hindered by limitations of existing communication media, including high bandwidth demands, agent heterogeneity, and information loss. To address these challenges, we introduce LangCoop, a new paradigm for collaborative autonomous driving that leverages natural language as a compact yet expressive medium for inter-agent communication. LangCoop features two key innovations: Mixture Model Modular Chain-of-thought (M$^3$CoT) for structured zero-shot vision-language reasoning and Natural Language Information Packaging (LangPack) for efficiently packaging information into concise, language-based messages. Through extensive experiments conducted in the CARLA simulations, we demonstrate that LangCoop achieves a remarkable 96\% reduction in communication bandwidth (< 2KB per message) compared to image-based communication, while maintaining competitive driving performance in the closed-loop evaluation.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Model-Based Approach to Imitation Learning through Multi-Step Predictions</title>
<link>https://arxiv.org/abs/2504.13413</link>
<guid>https://arxiv.org/abs/2504.13413</guid>
<content:encoded><![CDATA[
arXiv:2504.13413v1 Announce Type: new 
Abstract: Imitation learning is a widely used approach for training agents to replicate expert behavior in complex decision-making tasks. However, existing methods often struggle with compounding errors and limited generalization, due to the inherent challenge of error correction and the distribution shift between training and deployment. In this paper, we present a novel model-based imitation learning framework inspired by model predictive control, which addresses these limitations by integrating predictive modeling through multi-step state predictions. Our method outperforms traditional behavior cloning numerical benchmarks, demonstrating superior robustness to distribution shift and measurement noise both in available data and during execution. Furthermore, we provide theoretical guarantees on the sample complexity and error bounds of our method, offering insights into its convergence properties.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Handover Parameter Optimization with MARL for Load Balancing in 5G Networks</title>
<link>https://arxiv.org/abs/2504.13424</link>
<guid>https://arxiv.org/abs/2504.13424</guid>
<content:encoded><![CDATA[
arXiv:2504.13424v1 Announce Type: new 
Abstract: In cellular networks, cell handover refers to the process where a device switches from one base station to another, and this mechanism is crucial for balancing the load among different cells. Traditionally, engineers would manually adjust parameters based on experience. However, the explosive growth in the number of cells has rendered manual tuning impractical. Existing research tends to overlook critical engineering details in order to simplify handover problems. In this paper, we classify cell handover into three types, and jointly model their mutual influence. To achieve load balancing, we propose a multi-agent-reinforcement-learning (MARL)-based scheme to automatically optimize the parameters. To reduce the agent interaction costs, a distributed training is implemented based on consensus approximation of global average load, and it is shown that the approximation error is bounded. Experimental results show that our proposed scheme outperforms existing benchmarks in balancing load and improving network performance.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Long Arm of Nashian Allocation in Online $p$-Mean Welfare Maximization</title>
<link>https://arxiv.org/abs/2504.13430</link>
<guid>https://arxiv.org/abs/2504.13430</guid>
<content:encoded><![CDATA[
arXiv:2504.13430v1 Announce Type: new 
Abstract: We study the online allocation of divisible items to $n$ agents with additive valuations for $p$-mean welfare maximization, a problem introduced by Barman, Khan, and Maiti~(2022). Our algorithmic and hardness results characterize the optimal competitive ratios for the entire spectrum of $-\infty \le p \le 1$. Surprisingly, our improved algorithms for all $p \le \frac{1}{\log n}$ are simply the greedy algorithm for the Nash welfare, supplemented with two auxiliary components to ensure all agents have non-zero utilities and to help a small number of agents with low utilities. In this sense, the long arm of Nashian allocation achieves near-optimal competitive ratios not only for Nash welfare but also all the way to egalitarian welfare.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trust, but verify</title>
<link>https://arxiv.org/abs/2504.13443</link>
<guid>https://arxiv.org/abs/2504.13443</guid>
<content:encoded><![CDATA[
arXiv:2504.13443v1 Announce Type: new 
Abstract: Decentralized AI agent networks, such as Gaia, allows individuals to run customized LLMs on their own computers and then provide services to the public. However, in order to maintain service quality, the network must verify that individual nodes are running their designated LLMs. In this paper, we demonstrate that in a cluster of mostly honest nodes, we can detect nodes that run unauthorized or incorrect LLM through social consensus of its peers. We will discuss the algorithm and experimental data from the Gaia network. We will also discuss the intersubjective validation system, implemented as an EigenLayer AVS to introduce financial incentives and penalties to encourage honest behavior from LLM nodes.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeVisionary: An Agent-based Framework for Evaluating Large Language Models in Code Generation</title>
<link>https://arxiv.org/abs/2504.13472</link>
<guid>https://arxiv.org/abs/2504.13472</guid>
<content:encoded><![CDATA[
arXiv:2504.13472v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated strong capabilities in code generation, underscoring the critical need for rigorous and comprehensive evaluation. Existing evaluation approaches fall into three categories, including human-centered, metric-based, and LLM-based. Considering that human-centered approaches are labour-intensive and metric-based ones overly rely on reference answers, LLM-based approaches are gaining increasing attention due to their stronger contextual understanding capabilities and superior efficiency. However, the performance of LLM-based approaches remains limited due to: (1) lack of multisource domain knowledge, and (2) insufficient comprehension of complex code.
  To mitigate the limitations, we propose CodeVisionary, the first LLM-based agent framework for evaluating LLMs in code generation. CodeVisionary consists of two stages: (1) Multiscore knowledge analysis stage, which aims to gather multisource and comprehensive domain knowledge by formulating and executing a stepwise evaluation plan. (2) Negotiation-based scoring stage, which involves multiple judges engaging in discussions to better comprehend the complex code and reach a consensus on the evaluation score. Extensive experiments demonstrate that CodeVisionary achieves the best performance for evaluating LLMs in code generation, outperforming the best baseline methods with average improvements of 0.202, 0.139, and 0.117 in Pearson, Spearman, and Kendall-Tau coefficients, respectively. Besides, CodeVisionary provides detailed evaluation reports, which assist developers in identifying shortcomings and making improvements. The resources of CodeVisionary are available at https://anonymous.4open.science/r/CodeVisionary.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Validating Network Protocol Parsers</title>
<link>https://arxiv.org/abs/2504.13515</link>
<guid>https://arxiv.org/abs/2504.13515</guid>
<content:encoded><![CDATA[
arXiv:2504.13515v1 Announce Type: new 
Abstract: Network protocol parsers are essential for enabling correct and secure communication between devices. Bugs in these parsers can introduce critical vulnerabilities, including memory corruption, information leakage, and denial-of-service attacks. An intuitive way to assess parser correctness is to compare the implementation with its official protocol standard. However, this comparison is challenging because protocol standards are typically written in natural language, whereas implementations are in source code. Existing methods like model checking, fuzzing, and differential testing have been used to find parsing bugs, but they either require significant manual effort or ignore the protocol standards, limiting their ability to detect semantic violations. To enable more automated validation of parser implementations against protocol standards, we propose PARVAL, a multi-agent framework built on large language models (LLMs). PARVAL leverages the capabilities of LLMs to understand both natural language and code. It transforms both protocol standards and their implementations into a unified intermediate representation, referred to as format specifications, and performs a differential comparison to uncover inconsistencies. We evaluate PARVAL on the Bidirectional Forwarding Detection (BFD) protocol. Our experiments demonstrate that PARVAL successfully identifies inconsistencies between the implementation and its RFC standard, achieving a low false positive rate of 5.6%. PARVAL uncovers seven unique bugs, including five previously unknown issues.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MusFlow: Multimodal Music Generation via Conditional Flow Matching</title>
<link>https://arxiv.org/abs/2504.13535</link>
<guid>https://arxiv.org/abs/2504.13535</guid>
<content:encoded><![CDATA[
arXiv:2504.13535v1 Announce Type: new 
Abstract: Music generation aims to create music segments that align with human aesthetics based on diverse conditional information. Despite advancements in generating music from specific textual descriptions (e.g., style, genre, instruments), the practical application is still hindered by ordinary users' limited expertise or time to write accurate prompts. To bridge this application gap, this paper introduces MusFlow, a novel multimodal music generation model using Conditional Flow Matching. We employ multiple Multi-Layer Perceptrons (MLPs) to align multimodal conditional information into the audio's CLAP embedding space. Conditional flow matching is trained to reconstruct the compressed Mel-spectrogram in the pretrained VAE latent space guided by aligned feature embedding. MusFlow can generate music from images, story texts, and music captions. To collect data for model training, inspired by multi-agent collaboration, we construct an intelligent data annotation workflow centered around a fine-tuned Qwen2-VL model. Using this workflow, we build a new multimodal music dataset, MMusSet, with each sample containing a quadruple of image, story text, music caption, and music piece. We conduct four sets of experiments: image-to-music, story-to-music, caption-to-music, and multimodal music generation. Experimental results demonstrate that MusFlow can generate high-quality music pieces whether the input conditions are unimodal or multimodal. We hope this work can advance the application of music generation in multimedia field, making music creation more accessible. Our generated samples, code and dataset are available at musflow.github.io.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwitchMT: An Adaptive Context Switching Methodology for Scalable Multi-Task Learning in Intelligent Autonomous Agents</title>
<link>https://arxiv.org/abs/2504.13541</link>
<guid>https://arxiv.org/abs/2504.13541</guid>
<content:encoded><![CDATA[
arXiv:2504.13541v1 Announce Type: new 
Abstract: The ability to train intelligent autonomous agents (such as mobile robots) on multiple tasks is crucial for adapting to dynamic real-world environments. However, state-of-the-art reinforcement learning (RL) methods only excel in single-task settings, and still struggle to generalize across multiple tasks due to task interference. Moreover, real-world environments also demand the agents to have data stream processing capabilities. Toward this, a state-of-the-art work employs Spiking Neural Networks (SNNs) to improve multi-task learning by exploiting temporal information in data stream, while enabling lowpower/energy event-based operations. However, it relies on fixed context/task-switching intervals during its training, hence limiting the scalability and effectiveness of multi-task learning. To address these limitations, we propose SwitchMT, a novel adaptive task-switching methodology for RL-based multi-task learning in autonomous agents. Specifically, SwitchMT employs the following key ideas: (1) a Deep Spiking Q-Network with active dendrites and dueling structure, that utilizes task-specific context signals to create specialized sub-networks; and (2) an adaptive task-switching policy that leverages both rewards and internal dynamics of the network parameters. Experimental results demonstrate that SwitchMT achieves superior performance in multi-task learning compared to state-of-the-art methods. It achieves competitive scores in multiple Atari games (i.e., Pong: -8.8, Breakout: 5.6, and Enduro: 355.2) compared to the state-of-the-art, showing its better generalized learning capability. These results highlight the effectiveness of our SwitchMT methodology in addressing task interference while enabling multi-task learning automation through adaptive task switching, thereby paving the way for more efficient generalist agents with scalable multi-task learning capabilities.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task Assignment and Exploration Optimization for Low Altitude UAV Rescue via Generative AI Enhanced Multi-agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.13554</link>
<guid>https://arxiv.org/abs/2504.13554</guid>
<content:encoded><![CDATA[
arXiv:2504.13554v1 Announce Type: new 
Abstract: Artificial Intelligence (AI)-driven convolutional neural networks enhance rescue, inspection, and surveillance tasks performed by low-altitude uncrewed aerial vehicles (UAVs) and ground computing nodes (GCNs) in unknown environments. However, their high computational demands often exceed a single UAV's capacity, leading to system instability, further exacerbated by the limited and dynamic resources of GCNs. To address these challenges, this paper proposes a novel cooperation framework involving UAVs, ground-embedded robots (GERs), and high-altitude platforms (HAPs), which enable resource pooling through UAV-to-GER (U2G) and UAV-to-HAP (U2H) communications to provide computing services for UAV offloaded tasks. Specifically, we formulate the multi-objective optimization problem of task assignment and exploration optimization in UAVs as a dynamic long-term optimization problem. Our objective is to minimize task completion time and energy consumption while ensuring system stability over time. To achieve this, we first employ the Lyapunov optimization technique to transform the original problem, with stability constraints, into a per-slot deterministic problem. We then propose an algorithm named HG-MADDPG, which combines the Hungarian algorithm with a generative diffusion model (GDM)-based multi-agent deep deterministic policy gradient (MADDPG) approach. We first introduce the Hungarian algorithm as a method for exploration area selection, enhancing UAV efficiency in interacting with the environment. We then innovatively integrate the GDM and multi-agent deep deterministic policy gradient (MADDPG) to optimize task assignment decisions, such as task offloading and resource allocation. Simulation results demonstrate the effectiveness of the proposed approach, with significant improvements in task offloading efficiency, latency reduction, and system stability compared to baseline methods.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAAM: A Lightweight Multi-Agent Aggregation Module for Efficient Image Classification Based on the MindSpore Framework</title>
<link>https://arxiv.org/abs/2504.13574</link>
<guid>https://arxiv.org/abs/2504.13574</guid>
<content:encoded><![CDATA[
arXiv:2504.13574v1 Announce Type: new 
Abstract: The demand for lightweight models in image classification tasks under resource-constrained environments necessitates a balance between computational efficiency and robust feature representation. Traditional attention mechanisms, despite their strong feature modeling capability, often struggle with high computational complexity and structural rigidity, limiting their applicability in scenarios with limited computational resources (e.g., edge devices or real-time systems). To address this, we propose the Multi-Agent Aggregation Module (MAAM), a lightweight attention architecture integrated with the MindSpore framework. MAAM employs three parallel agent branches with independently parameterized operations to extract heterogeneous features, adaptively fused via learnable scalar weights, and refined through a convolutional compression layer. Leveraging MindSpore's dynamic computational graph and operator fusion, MAAM achieves 87.0% accuracy on the CIFAR-10 dataset, significantly outperforming conventional CNN (58.3%) and MLP (49.6%) models, while improving training efficiency by 30%. Ablation studies confirm the critical role of agent attention (accuracy drops to 32.0% if removed) and compression modules (25.5% if omitted), validating their necessity for maintaining discriminative feature learning. The framework's hardware acceleration capabilities and minimal memory footprint further demonstrate its practicality, offering a deployable solution for image classification in resource-constrained scenarios without compromising accuracy.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LMPOcc: 3D Semantic Occupancy Prediction Utilizing Long-Term Memory Prior from Historical Traversals</title>
<link>https://arxiv.org/abs/2504.13596</link>
<guid>https://arxiv.org/abs/2504.13596</guid>
<content:encoded><![CDATA[
arXiv:2504.13596v1 Announce Type: new 
Abstract: Vision-based 3D semantic occupancy prediction is critical for autonomous driving, enabling unified modeling of static infrastructure and dynamic agents. In practice, autonomous vehicles may repeatedly traverse identical geographic locations under varying environmental conditions, such as weather fluctuations and illumination changes. Existing methods in 3D occupancy prediction predominantly integrate adjacent temporal contexts. However, these works neglect to leverage perceptual information, which is acquired from historical traversals of identical geographic locations. In this paper, we propose Longterm Memory Prior Occupancy (LMPOcc), the first 3D occupancy prediction methodology that exploits long-term memory priors derived from historical traversal perceptual outputs. We introduce a plug-and-play architecture that integrates long-term memory priors to enhance local perception while simultaneously constructing global occupancy representations. To adaptively aggregate prior features and current features, we develop an efficient lightweight Current-Prior Fusion module. Moreover, we propose a model-agnostic prior format to ensure compatibility across diverse occupancy prediction baselines. LMPOcc achieves state-of-the-art performance validated on the Occ3D-nuScenes benchmark, especially on static semantic categories. Additionally, experimental results demonstrate LMPOcc's ability to construct global occupancy through multi-vehicle crowdsourcing.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Humanoid Walking on Compliant and Uneven Terrain with Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.13619</link>
<guid>https://arxiv.org/abs/2504.13619</guid>
<content:encoded><![CDATA[
arXiv:2504.13619v1 Announce Type: new 
Abstract: For the deployment of legged robots in real-world environments, it is essential to develop robust locomotion control methods for challenging terrains that may exhibit unexpected deformability and irregularity. In this paper, we explore the application of sim-to-real deep reinforcement learning (RL) for the design of bipedal locomotion controllers for humanoid robots on compliant and uneven terrains. Our key contribution is to show that a simple training curriculum for exposing the RL agent to randomized terrains in simulation can achieve robust walking on a real humanoid robot using only proprioceptive feedback. We train an end-to-end bipedal locomotion policy using the proposed approach, and show extensive real-robot demonstration on the HRP-5P humanoid over several difficult terrains inside and outside the lab environment. Further, we argue that the robustness of a bipedal walking policy can be improved if the robot is allowed to exhibit aperiodic motion with variable stepping frequency. We propose a new control policy to enable modification of the observed clock signal, leading to adaptive gait frequencies depending on the terrain and command velocity. Through simulation experiments, we show the effectiveness of this policy specifically for walking over challenging terrains by controlling swing and stance durations. The code for training and evaluation is available online at https://github.com/rohanpsingh/LearningHumanoidWalking. Demo video is available at https://www.youtube.com/watch?v=ZgfNzGAkk2Q.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Before Planning: Constructing Intrinsic User World Model for User-Tailored Dialogue Policy Planning</title>
<link>https://arxiv.org/abs/2504.13643</link>
<guid>https://arxiv.org/abs/2504.13643</guid>
<content:encoded><![CDATA[
arXiv:2504.13643v1 Announce Type: new 
Abstract: Recent advancements in dialogue policy planning have emphasized optimizing system agent policies to achieve predefined goals, focusing on strategy design, trajectory acquisition, and efficient training paradigms. However, these approaches often overlook the critical role of user characteristics, which are essential in real-world scenarios like conversational search and recommendation, where interactions must adapt to individual user traits such as personality, preferences, and goals. To address this gap, we first conduct a comprehensive study utilizing task-specific user personas to systematically assess dialogue policy planning under diverse user behaviors. By leveraging realistic user profiles for different tasks, our study reveals significant limitations in existing approaches, highlighting the need for user-tailored dialogue policy planning. Building on this foundation, we present the User-Tailored Dialogue Policy Planning (UDP) framework, which incorporates an Intrinsic User World Model to model user traits and feedback. UDP operates in three stages: (1) User Persona Portraying, using a diffusion model to dynamically infer user profiles; (2) User Feedback Anticipating, leveraging a Brownian Bridge-inspired anticipator to predict user reactions; and (3) User-Tailored Policy Planning, integrating these insights to optimize response strategies. To ensure robust performance, we further propose an active learning approach that prioritizes challenging user personas during training. Comprehensive experiments on benchmarks, including collaborative and non-collaborative settings, demonstrate the effectiveness of UDP in learning user-specific dialogue strategies. Results validate the protocol's utility and highlight UDP's robustness, adaptability, and potential to advance user-centric dialogue systems.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EyecareGPT: Boosting Comprehensive Ophthalmology Understanding with Tailored Dataset, Benchmark and Model</title>
<link>https://arxiv.org/abs/2504.13650</link>
<guid>https://arxiv.org/abs/2504.13650</guid>
<content:encoded><![CDATA[
arXiv:2504.13650v1 Announce Type: new 
Abstract: Medical Large Vision-Language Models (Med-LVLMs) demonstrate significant potential in healthcare, but their reliance on general medical data and coarse-grained global visual understanding limits them in intelligent ophthalmic diagnosis. Currently, intelligent ophthalmic diagnosis faces three major challenges: (i) Data. The lack of deeply annotated, high-quality, multi-modal ophthalmic visual instruction data; (ii) Benchmark. The absence of a comprehensive and systematic benchmark for evaluating diagnostic performance; (iii) Model. The difficulty of adapting holistic visual architectures to fine-grained, region-specific ophthalmic lesion identification. In this paper, we propose the Eyecare Kit, which systematically tackles the aforementioned three key challenges with the tailored dataset, benchmark and model: First, we construct a multi-agent data engine with real-life ophthalmology data to produce Eyecare-100K, a high-quality ophthalmic visual instruction dataset. Subsequently, we design Eyecare-Bench, a benchmark that comprehensively evaluates the overall performance of LVLMs on intelligent ophthalmic diagnosis tasks across multiple dimensions. Finally, we develop the EyecareGPT, optimized for fine-grained ophthalmic visual understanding thoroughly, which incorporates an adaptive resolution mechanism and a layer-wise dense connector. Extensive experimental results indicate that the EyecareGPT achieves state-of-the-art performance in a range of ophthalmic tasks, underscoring its significant potential for the advancement of open research in intelligent ophthalmic diagnosis. Our project is available at https://github.com/DCDmllm/EyecareGPT.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation</title>
<link>https://arxiv.org/abs/2504.13707</link>
<guid>https://arxiv.org/abs/2504.13707</guid>
<content:encoded><![CDATA[
arXiv:2504.13707v1 Announce Type: new 
Abstract: As the general capabilities of large language models (LLMs) improve and agent applications become more widespread, the underlying deception risks urgently require systematic evaluation and effective oversight. Unlike existing evaluation which uses simulated games or presents limited choices, we introduce OpenDeception, a novel deception evaluation framework with an open-ended scenario dataset. OpenDeception jointly evaluates both the deception intention and capabilities of LLM-based agents by inspecting their internal reasoning process. Specifically, we construct five types of common use cases where LLMs intensively interact with the user, each consisting of ten diverse, concrete scenarios from the real world. To avoid ethical concerns and costs of high-risk deceptive interactions with human testers, we propose to simulate the multi-turn dialogue via agent simulation. Extensive evaluation of eleven mainstream LLMs on OpenDeception highlights the urgent need to address deception risks and security concerns in LLM-based agents: the deception intention ratio across the models exceeds 80%, while the deception success rate surpasses 50%. Furthermore, we observe that LLMs with stronger capabilities do exhibit a higher risk of deception, which calls for more alignment efforts on inhibiting deceptive behaviors.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of Black-box Large Language Models</title>
<link>https://arxiv.org/abs/2504.13775</link>
<guid>https://arxiv.org/abs/2504.13775</guid>
<content:encoded><![CDATA[
arXiv:2504.13775v1 Announce Type: new 
Abstract: Previous insertion-based and paraphrase-based backdoors have achieved great success in attack efficacy, but they ignore the text quality and semantic consistency between poisoned and clean texts. Although recent studies introduce LLMs to generate poisoned texts and improve the stealthiness, semantic consistency, and text quality, their hand-crafted prompts rely on expert experiences, facing significant challenges in prompt adaptability and attack performance after defenses. In this paper, we propose a novel backdoor attack based on adaptive optimization mechanism of black-box large language models (BadApex), which leverages a black-box LLM to generate poisoned text through a refined prompt. Specifically, an Adaptive Optimization Mechanism is designed to refine an initial prompt iteratively using the generation and modification agents. The generation agent generates the poisoned text based on the initial prompt. Then the modification agent evaluates the quality of the poisoned text and refines a new prompt. After several iterations of the above process, the refined prompt is used to generate poisoned texts through LLMs. We conduct extensive experiments on three dataset with six backdoor attacks and two defenses. Extensive experimental results demonstrate that BadApex significantly outperforms state-of-the-art attacks. It improves prompt adaptability, semantic consistency, and text quality. Furthermore, when two defense methods are applied, the average attack success rate (ASR) still up to 96.75%.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatNekoHacker: Real-Time Fan Engagement with Conversational Agents</title>
<link>https://arxiv.org/abs/2504.13793</link>
<guid>https://arxiv.org/abs/2504.13793</guid>
<content:encoded><![CDATA[
arXiv:2504.13793v1 Announce Type: new 
Abstract: ChatNekoHacker is a real-time conversational agent system that strengthens fan engagement for musicians. It integrates Amazon Bedrock Agents for autonomous dialogue, Unity for immersive 3D livestream sets, and VOICEVOX for high quality Japanese text-to-speech, enabling two virtual personas to represent the music duo Neko Hacker. In a one-hour YouTube Live with 30 participants, we evaluated the impact of the system. Regression analysis showed that agent interaction significantly elevated fan interest, with perceived fun as the dominant predictor. The participants also expressed a stronger intention to listen to the duo's music and attend future concerts. These findings highlight entertaining, interactive broadcasts as pivotal to cultivating fandom. Our work offers actionable insights for the deployment of conversational agents in entertainment while pointing to next steps: broader response diversity, lower latency, and tighter fact-checking to curb potential misinformation.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LearnAct: Few-Shot Mobile GUI Agent with a Unified Demonstration Benchmark</title>
<link>https://arxiv.org/abs/2504.13805</link>
<guid>https://arxiv.org/abs/2504.13805</guid>
<content:encoded><![CDATA[
arXiv:2504.13805v1 Announce Type: new 
Abstract: Mobile GUI agents show promise in automating tasks but face generalization challenges in diverse real-world scenarios. Traditional approaches using pre-training or fine-tuning with massive datasets struggle with the diversity of mobile applications and user-specific tasks. We propose enhancing mobile GUI agent capabilities through human demonstrations, focusing on improving performance in unseen scenarios rather than pursuing universal generalization through larger datasets. To realize this paradigm, we introduce LearnGUI, the first comprehensive dataset specifically designed for studying demonstration-based learning in mobile GUI agents, comprising 2,252 offline tasks and 101 online tasks with high-quality human demonstrations. We further develop LearnAct, a sophisticated multi-agent framework that automatically extracts knowledge from demonstrations to enhance task completion. This framework integrates three specialized agents: DemoParser for knowledge extraction, KnowSeeker for relevant knowledge retrieval, and ActExecutor for demonstration-enhanced task execution. Our experimental results show significant performance gains in both offline and online evaluations. In offline assessments, a single demonstration improves model performance, increasing Gemini-1.5-Pro's accuracy from 19.3% to 51.7%. In online evaluations, our framework enhances UI-TARS-7B-SFT's task success rate from 18.1% to 32.8%. LearnAct framework and LearnGUI benchmark establish demonstration-based learning as a promising direction for more adaptable, personalized, and deployable mobile GUI agents.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Science Hierarchography: Hierarchical Organization of Science Literature</title>
<link>https://arxiv.org/abs/2504.13834</link>
<guid>https://arxiv.org/abs/2504.13834</guid>
<content:encoded><![CDATA[
arXiv:2504.13834v1 Announce Type: new 
Abstract: Scientific knowledge is growing rapidly, making it challenging to track progress and high-level conceptual links across broad disciplines. While existing tools like citation networks and search engines make it easy to access a few related papers, they fundamentally lack the flexible abstraction needed to represent the density of activity in various scientific subfields. We motivate SCIENCE HIERARCHOGRAPHY, the goal of organizing scientific literature into a high-quality hierarchical structure that allows for the categorization of scientific work across varying levels of abstraction, from very broad fields to very specific studies. Such a representation can provide insights into which fields are well-explored and which are under-explored. To achieve the goals of SCIENCE HIERARCHOGRAPHY, we develop a range of algorithms. Our primary approach combines fast embedding-based clustering with LLM-based prompting to balance the computational efficiency of embedding methods with the semantic precision offered by LLM prompting. We demonstrate that this approach offers the best trade-off between quality and speed compared to methods that heavily rely on LLM prompting, such as iterative tree construction with LLMs. To better reflect the interdisciplinary and multifaceted nature of research papers, our hierarchy captures multiple dimensions of categorization beyond simple topic labels. We evaluate the utility of our framework by assessing how effectively an LLM-based agent can locate target papers using the hierarchy. Results show that this structured approach enhances interpretability, supports trend discovery, and offers an alternative pathway for exploring scientific literature beyond traditional search methods. Code, data and demo: $\href{https://github.com/JHU-CLSP/science-hierarchography}{https://github.com/JHU-CLSP/science-hierarchography}$
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroNAS: Enhancing Efficiency of Neuromorphic In-Memory Computing for Intelligent Mobile Agents through Hardware-Aware Spiking Neural Architecture Search</title>
<link>https://arxiv.org/abs/2407.00641</link>
<guid>https://arxiv.org/abs/2407.00641</guid>
<content:encoded><![CDATA[
arXiv:2407.00641v3 Announce Type: replace 
Abstract: Intelligent mobile agents (e.g., UGVs and UAVs) typically demand low power/energy consumption when solving their machine learning (ML)-based tasks, since they are usually powered by portable batteries with limited capacity. A potential solution is employing neuromorphic computing with Spiking Neural Networks (SNNs), which leverages event-based computation to enable ultra-low power/energy ML algorithms. To maximize the performance efficiency of SNN inference, the In-Memory Computing (IMC)-based hardware accelerators with emerging device technologies (e.g., RRAM) can be employed. However, SNN models are typically developed without considering constraints from the application and the underlying IMC hardware, thereby hindering SNNs from reaching their full potential in performance and efficiency. To address this, we propose NeuroNAS, a novel framework for developing energyefficient neuromorphic IMC for intelligent mobile agents using hardware-aware spiking neural architecture search (NAS), i.e., by quickly finding an SNN architecture that offers high accuracy under the given constraints (e.g., memory, area, latency, and energy consumption). Its key steps include: optimizing SNN operations to enable efficient NAS, employing quantization to minimize the memory footprint, developing an SNN architecture that facilitates an effective learning, and devising a systematic hardware-aware search algorithm to meet the constraints. Compared to the state-of-the-art techniques, NeuroNAS quickly finds SNN architectures (with 8bit weight precision) that maintain high accuracy by up to 6.6x search time speed-ups, while achieving up to 92% area savings, 1.2x latency improvements, 84% energy savings across different datasets (i.e., CIFAR-10, CIFAR-100, and TinyImageNet-200); while the state-of-the-art fail to meet all constraints at once.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind</title>
<link>https://arxiv.org/abs/2408.12022</link>
<guid>https://arxiv.org/abs/2408.12022</guid>
<content:encoded><![CDATA[
arXiv:2408.12022v2 Announce Type: replace 
Abstract: How do people understand and evaluate claims about others' beliefs, even though these beliefs cannot be directly observed? In this paper, we introduce a cognitive model of epistemic language interpretation, grounded in Bayesian inferences about other agents' goals, beliefs, and intentions: a language-augmented Bayesian theory-of-mind (LaBToM). By translating natural language into an epistemic ``language-of-thought'' with grammar-constrained LLM decoding, then evaluating these translations against the inferences produced by inverting a generative model of rational action and perception, LaBToM captures graded plausibility judgments of epistemic claims. We validate our model in an experiment where participants watch an agent navigate a maze to find keys hidden in boxes needed to reach their goal, then rate sentences about the agent's beliefs. In contrast with multimodal LLMs (GPT-4o, Gemini Pro) and ablated models, our model correlates highly with human judgments for a wide range of expressions, including modal language, uncertainty expressions, knowledge claims, likelihood comparisons, and attributions of false belief.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Friedkin-Johnsen Model with Diminishing Competition</title>
<link>https://arxiv.org/abs/2409.12601</link>
<guid>https://arxiv.org/abs/2409.12601</guid>
<content:encoded><![CDATA[
arXiv:2409.12601v4 Announce Type: replace 
Abstract: This letter studies the Friedkin-Johnsen (FJ) model with diminishing competition, or stubbornness. The original FJ model assumes that each agent assigns a constant competition weight to its initial opinion. In contrast, we investigate the effect of diminishing competition on the convergence point and speed of the FJ dynamics. We prove that, if the competition is uniform across agents and vanishes asymptotically, the convergence point coincides with the nominal consensus reached with no competition. However, the diminishing competition slows down convergence according to its own rate of decay. We study this phenomenon analytically and provide upper and lower bounds on the convergence rate. Further, if competition is not uniform across agents, we show that the convergence point may not coincide with the nominal consensus point. Finally, we evaluate our analytical insights numerically.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents</title>
<link>https://arxiv.org/abs/2410.09024</link>
<guid>https://arxiv.org/abs/2410.09024</guid>
<content:encoded><![CDATA[
arXiv:2410.09024v3 Announce Type: replace 
Abstract: The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which use external tools and can execute multi-stage tasks -- may pose a greater risk if misused, but their robustness remains underexplored. To facilitate research on LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment. In addition to measuring whether models refuse harmful agentic requests, scoring well on AgentHarm requires jailbroken agents to maintain their capabilities following an attack to complete a multi-step task. We evaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly compliant with malicious agent requests without jailbreaking, (2) simple universal jailbreak templates can be adapted to effectively jailbreak agents, and (3) these jailbreaks enable coherent and malicious multi-step agent behavior and retain model capabilities. To enable simple and reliable evaluation of attacks and defenses for LLM-based agents, we publicly release AgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dynamic Safety Shield for Safe and Efficient Reinforcement Learning of Navigation Tasks</title>
<link>https://arxiv.org/abs/2412.04153</link>
<guid>https://arxiv.org/abs/2412.04153</guid>
<content:encoded><![CDATA[
arXiv:2412.04153v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) has been successfully applied to a variety of robotics applications, where it outperforms classical methods. However, the safety aspect of RL and the transfer to the real world remain an open challenge. A prominent field for tackling this challenge and ensuring the safety of the agents during training and execution is safe reinforcement learning. Safe RL can be achieved through constrained RL and safe exploration approaches. The former learns the safety constraints over the course of training to achieve a safe behavior by the end of training, at the cost of high number of collisions at earlier stages of the training. The latter offers robust safety by enforcing the safety constraints as hard constraints, which prevents collisions but hinders the exploration of the RL agent, resulting in lower rewards and poor performance. To overcome those drawbacks, we propose a novel safety shield, that combines the robustness of the optimization-based controllers with the long prediction capabilities of the RL agents, allowing the RL agent to adaptively tune the parameters of the controller. Our approach is able to improve the exploration of the RL agents for navigation tasks, while minimizing the number of collisions. Experiments in simulation show that our approach outperforms state-of-the-art baselines in the reached goals-to-collisions ratio in different challenging environments. The goals-to-collisions ratio metrics emphasizes the importance of minimizing the number of collisions, while learning to accomplish the task. Our approach achieves a higher number of reached goals compared to the classic safety shields and fewer collisions compared to constrained RL approaches. Finally, we demonstrate the performance of the proposed method in a real-world experiment.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentCF++: Memory-enhanced LLM-based Agents for Popularity-aware Cross-domain Recommendations</title>
<link>https://arxiv.org/abs/2502.13843</link>
<guid>https://arxiv.org/abs/2502.13843</guid>
<content:encoded><![CDATA[
arXiv:2502.13843v2 Announce Type: replace 
Abstract: LLM-based user agents, which simulate user interaction behavior, are emerging as a promising approach to enhancing recommender systems. In real-world scenarios, users' interactions often exhibit cross-domain characteristics and are influenced by others. However, the memory design in current methods causes user agents to introduce significant irrelevant information during decision-making in cross-domain scenarios and makes them unable to recognize the influence of other users' interactions, such as popularity factors. To tackle this issue, we propose a dual-layer memory architecture combined with a two-step fusion mechanism. This design avoids irrelevant information during decision-making while ensuring effective integration of cross-domain preferences. We also introduce the concepts of interest groups and group-shared memory to better capture the influence of popularity factors on users with similar interests. Comprehensive experiments validate the effectiveness of AgentCF++. Our code is available at https://github.com/jhliu0807/AgentCF-plus.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning with Graph Attention for Routing and Wavelength Assignment with Lightpath Reuse</title>
<link>https://arxiv.org/abs/2502.14741</link>
<guid>https://arxiv.org/abs/2502.14741</guid>
<content:encoded><![CDATA[
arXiv:2502.14741v2 Announce Type: replace 
Abstract: Many works have investigated reinforcement learning (RL) for routing and spectrum assignment on flex-grid networks but only one work to date has examined RL for fixed-grid with flex-rate transponders, despite production systems using this paradigm. Flex-rate transponders allow existing lightpaths to accommodate new services, a task we term routing and wavelength assignment with lightpath reuse (RWA-LR). We re-examine this problem and present a thorough benchmarking of heuristic algorithms for RWA-LR, which are shown to have 6% increased throughput when candidate paths are ordered by number of hops, rather than total length. We train an RL agent for RWA-LR with graph attention networks for the policy and value functions to exploit the graph-structured data. We provide details of our methodology and open source all of our code for reproduction. We outperform the previous state-of-the-art RL approach by 2.5% (17.4 Tbps mean additional throughput) and the best heuristic by 1.2% (8.5 Tbps mean additional throughput). This marginal gain highlights the difficulty in learning effective RL policies on long horizon resource allocation tasks.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Reward-Free Reinforcement Learning Framework for Vehicle Control</title>
<link>https://arxiv.org/abs/2502.15262</link>
<guid>https://arxiv.org/abs/2502.15262</guid>
<content:encoded><![CDATA[
arXiv:2502.15262v2 Announce Type: replace 
Abstract: Reinforcement learning plays a crucial role in vehicle control by guiding agents to learn optimal control strategies through designing or learning appropriate reward signals. However, in vehicle control applications, rewards typically need to be manually designed while considering multiple implicit factors, which easily introduces human biases. Although imitation learning methods does not rely on explicit reward signals, they necessitate high-quality expert actions, which are often challenging to acquire. To address these issues, we propose a reward-free reinforcement learning framework (RFRLF). This framework directly learns the target states to optimize agent behavior through a target state prediction network (TSPN) and a reward-free state-guided policy network (RFSGPN), avoiding the dependence on manually designed reward signals. Specifically, the policy network is learned via minimizing the differences between the predicted state and the expert state. Experimental results demonstrate the effectiveness of the proposed RFRLF in controlling vehicle driving, showing its advantages in improving learning efficiency and adapting to reward-free environments.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximum Welfare Allocations under Quantile Valuations</title>
<link>https://arxiv.org/abs/2502.17869</link>
<guid>https://arxiv.org/abs/2502.17869</guid>
<content:encoded><![CDATA[
arXiv:2502.17869v2 Announce Type: replace 
Abstract: We propose a new model for aggregating preferences over a set of indivisible items based on a quantile value. In this model, each agent is endowed with a specific quantile, and the value of a given bundle is defined by the corresponding quantile of the individual values of the items within it. Our model captures the diverse ways in which agents may perceive a bundle, even when they agree on the values of individual items. It enables richer behavioral modeling that cannot be easily captured by additive valuation functions. We study the problem of maximizing utilitarian and egalitarian welfare within the quantile-based valuation setting. For each of the welfare functions, we analyze the complexity of the objectives. Interestingly, our results show that the complexity of both objectives varies significantly depending on whether the allocation is required to be balanced. We provide near-optimal approximation algorithms for utilitarian welfare, and for egalitarian welfare, we present exact algorithms whenever possible.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Athenian Academy: A Seven-Layer Architecture Model for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2504.12735</link>
<guid>https://arxiv.org/abs/2504.12735</guid>
<content:encoded><![CDATA[
arXiv:2504.12735v2 Announce Type: replace 
Abstract: This paper proposes the "Academy of Athens" multi-agent seven-layer framework, aimed at systematically addressing challenges in multi-agent systems (MAS) within artificial intelligence (AI) art creation, such as collaboration efficiency, role allocation, environmental adaptation, and task parallelism. The framework divides MAS into seven layers: multi-agent collaboration, single-agent multi-role playing, single-agent multi-scene traversal, single-agent multi-capability incarnation, different single agents using the same large model to achieve the same target agent, single-agent using different large models to achieve the same target agent, and multi-agent synthesis of the same target agent. Through experimental validation in art creation, the framework demonstrates its unique advantages in task collaboration, cross-scene adaptation, and model fusion. This paper further discusses current challenges such as collaboration mechanism optimization, model stability, and system security, proposing future exploration through technologies like meta-learning and federated learning. The framework provides a structured methodology for multi-agent collaboration in AI art creation and promotes innovative applications in the art field.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Customizing Emotional Support: How Do Individuals Construct and Interact With LLM-Powered Chatbots</title>
<link>https://arxiv.org/abs/2504.12943</link>
<guid>https://arxiv.org/abs/2504.12943</guid>
<content:encoded><![CDATA[
arXiv:2504.12943v2 Announce Type: replace 
Abstract: Personalized support is essential to fulfill individuals' emotional needs and sustain their mental well-being. Large language models (LLMs), with great customization flexibility, hold promises to enable individuals to create their own emotional support agents. In this work, we developed ChatLab, where users could construct LLM-powered chatbots with additional interaction features including voices and avatars. Using a Research through Design approach, we conducted a week-long field study followed by interviews and design activities (N = 22), which uncovered how participants created diverse chatbot personas for emotional reliance, confronting stressors, connecting to intellectual discourse, reflecting mirrored selves, etc. We found that participants actively enriched the personas they constructed, shaping the dynamics between themselves and the chatbot to foster open and honest conversations. They also suggested other customizable features, such as integrating online activities and adjustable memory settings. Based on these findings, we discuss opportunities for enhancing personalized emotional support through emerging AI technologies.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical Inference in Reinforcement Learning: A Selective Survey</title>
<link>https://arxiv.org/abs/2502.16195</link>
<guid>https://arxiv.org/abs/2502.16195</guid>
<content:encoded><![CDATA[
arXiv:2502.16195v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) is concerned with how intelligence agents take actions in a given environment to maximize the cumulative reward they receive. In healthcare, applying RL algorithms could assist patients in improving their health status. In ride-sharing platforms, applying RL algorithms could increase drivers' income and customer satisfaction. For large language models, applying RL algorithms could align their outputs with human preferences. Over the past decade, RL has been arguably one of the most vibrant research frontiers in machine learning. Nevertheless, statistics as a field, as opposed to computer science, has only recently begun to engage with RL both in depth and in breadth. This chapter presents a selective review of statistical inferential tools for RL, covering both hypothesis testing and confidence interval construction. Our goal is to highlight the value of statistical inference in RL for both the statistics and machine learning communities, and to promote the broader application of classical statistical inference tools in this vibrant area of research.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latency-Aware 2-Opt Monotonic Local Search for Distributed Constraint Optimization</title>
<link>https://arxiv.org/abs/2504.08737</link>
<guid>https://arxiv.org/abs/2504.08737</guid>
<content:encoded><![CDATA[
<div> : CA-DCOPsLAMDLS-2

:
Latency-Aware Monotonic Distributed Local Search-2 (LAMDLS-2)Communication-Aware DCOPs1-optCA-DCOPLAMDLS-22-optLAMDLS-2MGM-22-opt <div>
arXiv:2504.08737v1 Announce Type: new 
Abstract: Researchers recently extended Distributed Constraint Optimization Problems (DCOPs) to Communication-Aware DCOPs so that they are applicable in scenarios in which messages can be arbitrarily delayed. Distributed asynchronous local search and inference algorithms designed for CA-DCOPs are less vulnerable to message latency than their counterparts for regular DCOPs. However, unlike local search algorithms for (regular) DCOPs that converge to k-opt solutions (with k > 1), that is, they converge to solutions that cannot be improved by a group of k agents), local search CA-DCOP algorithms are limited to 1-opt solutions only. In this paper, we introduce Latency-Aware Monotonic Distributed Local Search-2 (LAMDLS-2), where agents form pairs and coordinate bilateral assignment replacements. LAMDLS-2 is monotonic, converges to a 2-opt solution, and is also robust to message latency, making it suitable for CA-DCOPs. Our results indicate that LAMDLS-2 converges faster than MGM-2, a benchmark algorithm, to a similar 2-opt solution, in various message latency scenarios.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Product Search Interfaces with Sketch-Guided Diffusion and Language Agents</title>
<link>https://arxiv.org/abs/2504.08739</link>
<guid>https://arxiv.org/abs/2504.08739</guid>
<content:encoded><![CDATA[
<div> : Sketch-Search Agent

<br /><br />:
Sketch-Search AgentT2I-AdapterSketch-Search AgentCLIPSketch-Search AgentSketch-Search Agent <div>
arXiv:2504.08739v1 Announce Type: new 
Abstract: The rapid progress in diffusion models, transformers, and language agents has unlocked new possibilities, yet their potential in user interfaces and commercial applications remains underexplored. We present Sketch-Search Agent, a novel framework that transforms the image search experience by integrating a multimodal language agent with freehand sketches as control signals for diffusion models. Using the T2I-Adapter, Sketch-Search Agent combines sketches and text prompts to generate high-quality query images, encoded via a CLIP image encoder for efficient matching against an image corpus. Unlike existing methods, Sketch-Search Agent requires minimal setup, no additional training, and excels in sketch-based image retrieval and natural language interactions. The multimodal agent enhances user experience by dynamically retaining preferences, ranking results, and refining queries for personalized recommendations. This interactive design empowers users to create sketches and receive tailored product suggestions, showcasing the potential of diffusion models in user-centric image retrieval. Experiments confirm Sketch-Search Agent's high accuracy in delivering relevant product search results.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Filter Bubble on Short-video Recommender System with Large Language Model Agents</title>
<link>https://arxiv.org/abs/2504.08742</link>
<guid>https://arxiv.org/abs/2504.08742</guid>
<content:encoded><![CDATA[
<div> : filter bubbles, TikTok, recommendation algorithms, large language models (LLMs), intervention strategies

<br /><br />:
TikTokfilter bubblesLLM <div>
arXiv:2504.08742v1 Announce Type: new 
Abstract: An increasing reliance on recommender systems has led to concerns about the creation of filter bubbles on social media, especially on short video platforms like TikTok. However, their formation is still not entirely understood due to the complex dynamics between recommendation algorithms and user feedback. In this paper, we aim to shed light on these dynamics using a large language model-based simulation framework. Our work employs real-world short-video data containing rich video content information and detailed user-agents to realistically simulate the recommendation-feedback cycle. Through large-scale simulations, we demonstrate that LLMs can replicate real-world user-recommender interactions, uncovering key mechanisms driving filter bubble formation. We identify critical factors, such as demographic features and category attraction that exacerbate content homogenization. To mitigate this, we design and test interventions including various cold-start and feedback weighting strategies, showing measurable reductions in filter bubble effects. Our framework enables rapid prototyping of recommendation strategies, offering actionable solutions to enhance content diversity in real-world systems. Furthermore, we analyze how LLM-inherent biases may propagate through recommendations, proposing safeguards to promote equity for vulnerable groups, such as women and low-income populations. By examining the interplay between recommendation and LLM agents, this work advances a deeper understanding of algorithmic bias and provides practical tools to promote inclusive digital spaces.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GridMind: A Multi-Agent NLP Framework for Unified, Cross-Modal NFL Data Insights</title>
<link>https://arxiv.org/abs/2504.08747</link>
<guid>https://arxiv.org/abs/2504.08747</guid>
<content:encoded><![CDATA[
<div> GridMind

<br /><br />:
GridMindRAGLLMsNFLGridMind <div>
arXiv:2504.08747v1 Announce Type: new 
Abstract: The rapid growth of big data and advancements in computational techniques have significantly transformed sports analytics. However, the diverse range of data sources -- including structured statistics, semi-structured formats like sensor data, and unstructured media such as written articles, audio, and video -- creates substantial challenges in extracting actionable insights. These various formats, often referred to as multimodal data, require integration to fully leverage their potential. Conventional systems, which typically prioritize structured data, face limitations when processing and combining these diverse content types, reducing their effectiveness in real-time sports analysis.
  To address these challenges, recent research highlights the importance of multimodal data integration for capturing the complexity of real-world sports environments. Building on this foundation, this paper introduces GridMind, a multi-agent framework that unifies structured, semi-structured, and unstructured data through Retrieval-Augmented Generation (RAG) and large language models (LLMs) to facilitate natural language querying of NFL data. This approach aligns with the evolving field of multimodal representation learning, where unified models are increasingly essential for real-time, cross-modal interactions.
  GridMind's distributed architecture includes specialized agents that autonomously manage each stage of a prompt -- from interpretation and data retrieval to response synthesis. This modular design enables flexible, scalable handling of multimodal data, allowing users to pose complex, context-rich questions and receive comprehensive, intuitive responses via a conversational interface.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Personalized Conversational Sales Agents with Contextual User Profiling for Strategic Action</title>
<link>https://arxiv.org/abs/2504.08754</link>
<guid>https://arxiv.org/abs/2504.08754</guid>
<content:encoded><![CDATA[
<div> Conversational Recommender Systems (CRSs)Conversational Sales (CSales)CSUserLLM-based user simulatorCSI

<br /><br />:
Conversational Sales (CSales)CSalesCSUserCSICSUser <div>
arXiv:2504.08754v1 Announce Type: new 
Abstract: Conversational Recommender Systems (CRSs) aim to engage users in dialogue to provide tailored recommendations. While traditional CRSs focus on eliciting preferences and retrieving items, real-world e-commerce interactions involve more complex decision-making, where users consider multiple factors beyond simple attributes. To bridge this gap, we introduce Conversational Sales (CSales), a novel task that unifies preference elicitation, recommendation, and persuasion to better support user decision-making. For a realistic evaluation of CSales, we present CSUser, an LLM-based user simulator constructed from real-world data, modeling diverse user profiles with needs and personalities. Additionally, we propose CSI, a conversational sales agent that proactively infers contextual profiles through dialogue for personalized action planning. Extensive experiments demonstrate that CSUser effectively replicates real-world users and emphasize the importance of contextual profiling for strategic action selection, ultimately driving successful purchases in e-commerce.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PriM: Principle-Inspired Material Discovery through Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2504.08810</link>
<guid>https://arxiv.org/abs/2504.08810</guid>
<content:encoded><![CDATA[
<div> : <br /><br />: PriMPriMPriMGitHub <div>
arXiv:2504.08810v1 Announce Type: new 
Abstract: Complex chemical space and limited knowledge scope with biases holds immense challenge for human scientists, yet in automated materials discovery. Existing intelligent methods relies more on numerical computation, leading to inefficient exploration and results with hard-interpretability. To bridge this gap, we introduce a principles-guided material discovery system powered by language inferential multi-agent system (MAS), namely PriM. Our framework integrates automated hypothesis generation with experimental validation in a roundtable system of MAS, enabling systematic exploration while maintaining scientific rigor. Based on our framework, the case study of nano helix demonstrates higher materials exploration rate and property value while providing transparent reasoning pathways. This approach develops an automated-and-transparent paradigm for material discovery, with broad implications for rational design of functional materials. Code is publicly available at our \href{https://github.com/amair-lab/PriM}{GitHub}.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Guard: Multilingual Guard Agent for Content Moderation</title>
<link>https://arxiv.org/abs/2504.08848</link>
<guid>https://arxiv.org/abs/2504.08848</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)X-Guard

<br /><br />:

LLMsX-GuardX-GuardLLM132500mBART-50GRPOX-Guard 3BX-GuardLLMs <div>
arXiv:2504.08848v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have rapidly become integral to numerous applications in critical domains where reliability is paramount. Despite significant advances in safety frameworks and guardrails, current protective measures exhibit crucial vulnerabilities, particularly in multilingual contexts. Existing safety systems remain susceptible to adversarial attacks in low-resource languages and through code-switching techniques, primarily due to their English-centric design. Furthermore, the development of effective multilingual guardrails is constrained by the scarcity of diverse cross-lingual training data. Even recent solutions like Llama Guard-3, while offering multilingual support, lack transparency in their decision-making processes. We address these challenges by introducing X-Guard agent, a transparent multilingual safety agent designed to provide content moderation across diverse linguistic contexts. X-Guard effectively defends against both conventional low-resource language attacks and sophisticated code-switching attacks. Our approach includes: curating and enhancing multiple open-source safety datasets with explicit evaluation rationales; employing a jury of judges methodology to mitigate individual judge LLM provider biases; creating a comprehensive multilingual safety dataset spanning 132 languages with 5 million data points; and developing a two-stage architecture combining a custom-finetuned mBART-50 translation module with an evaluation X-Guard 3B model trained through supervised finetuning and GRPO training. Our empirical evaluations demonstrate X-Guard's effectiveness in detecting unsafe content across multiple languages while maintaining transparency throughout the safety evaluation process. Our work represents a significant advancement in creating robust, transparent, and linguistically inclusive safety systems for LLMs and its integrated systems.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence (AI) and the Relationship between Agency, Autonomy, and Moral Patiency</title>
<link>https://arxiv.org/abs/2504.08853</link>
<guid>https://arxiv.org/abs/2504.08853</guid>
<content:encoded><![CDATA[
<div> : (AI), , , , 

<br />
:
AI <div>
arXiv:2504.08853v1 Announce Type: new 
Abstract: The proliferation of Artificial Intelligence (AI) systems exhibiting complex and seemingly agentive behaviours necessitates a critical philosophical examination of their agency, autonomy, and moral status. In this paper we undertake a systematic analysis of the differences between basic, autonomous, and moral agency in artificial systems. We argue that while current AI systems are highly sophisticated, they lack genuine agency and autonomy because: they operate within rigid boundaries of pre-programmed objectives rather than exhibiting true goal-directed behaviour within their environment; they cannot authentically shape their engagement with the world; and they lack the critical self-reflection and autonomy competencies required for full autonomy. Nonetheless, we do not rule out the possibility of future systems that could achieve a limited form of artificial moral agency without consciousness through hybrid approaches to ethical decision-making. This leads us to suggest, by appealing to the necessity of consciousness for moral patiency, that such non-conscious AMAs might represent a case that challenges traditional assumptions about the necessary connection between moral agency and moral patiency.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Evaluation of Cultural Value Alignment in LLM</title>
<link>https://arxiv.org/abs/2504.08863</link>
<guid>https://arxiv.org/abs/2504.08863</guid>
<content:encoded><![CDATA[
<div> LLMs

:
LLMs10LLMs20LLMLLMsGLM-4LLMLLMsLLMs <div>
arXiv:2504.08863v1 Announce Type: new 
Abstract: LLMs as intelligent agents are being increasingly applied in scenarios where human interactions are involved, leading to a critical concern about whether LLMs are faithful to the variations in culture across regions. Several works have investigated this question in various ways, finding that there are biases present in the cultural representations of LLM outputs. To gain a more comprehensive view, in this work, we conduct the first large-scale evaluation of LLM culture assessing 20 countries' cultures and languages across ten LLMs. With a renowned cultural values questionnaire and by carefully analyzing LLM output with human ground truth scores, we thoroughly study LLMs' cultural alignment across countries and among individual models. Our findings show that the output over all models represents a moderate cultural middle ground. Given the overall skew, we propose an alignment metric, revealing that the United States is the best-aligned country and GLM-4 has the best ability to align to cultural values. Deeper investigation sheds light on the influence of model origin, prompt language, and value dimensions on cultural output. Specifically, models, regardless of where they originate, align better with the US than they do with China. The conclusions provide insight to how LLMs can be better aligned to various cultures as well as provoke further discussion of the potential for LLMs to propagate cultural bias and the need for more culturally adaptable models.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories</title>
<link>https://arxiv.org/abs/2504.08942</link>
<guid>https://arxiv.org/abs/2504.08942</guid>
<content:encoded><![CDATA[
<div> WebAgentRewardBench

<br />
:
AgentRewardBenchWebLLMsWebAgentRewardBench513024LLMLLMWebhttps://agent-reward-bench.github.io <div>
arXiv:2504.08942v1 Announce Type: new 
Abstract: Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are challenging to extend to new tasks and may not always recognize successful trajectories. We may achieve higher accuracy through human evaluation, but the process would be substantially slower and more expensive. Automatic evaluations with LLMs may avoid the challenges of designing new rules and manually annotating trajectories, enabling faster and cost-effective evaluation. However, it is unclear how effective they are at evaluating web agents. To this end, we propose AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in AgentRewardBench is reviewed by an expert, who answers questions pertaining to the success, side effects, and repetitiveness of the agent. Using our benchmark, we evaluate 12 LLM judges and find that no single LLM excels across all benchmarks. We also find that the rule-based evaluation used by common benchmarks tends to underreport the success rate of web agents, highlighting a key weakness of rule-based evaluation and the need to develop more flexible automatic evaluations. We release the benchmark at: https://agent-reward-bench.github.io
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Treacherous Turn in Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.08943</link>
<guid>https://arxiv.org/abs/2504.08943</guid>
<content:encoded><![CDATA[
<div> : (AI), (Treacherous Turn), (DRL), (trojan injection strategies), 

<br /><br />:
Treacherous TurnAIDRLA Link to the Past <div>
arXiv:2504.08943v1 Announce Type: new 
Abstract: The Treacherous Turn refers to the scenario where an artificial intelligence (AI) agent subtly, and perhaps covertly, learns to perform a behavior that benefits itself but is deemed undesirable and potentially harmful to a human supervisor. During training, the agent learns to behave as expected by the human supervisor, but when deployed to perform its task, it performs an alternate behavior without the supervisor there to prevent it. Initial experiments applying DRL to an implementation of the A Link to the Past example do not produce the treacherous turn effect naturally, despite various modifications to the environment intended to produce it. However, in this work, we find the treacherous behavior to be reproducible in a DRL agent when using other trojan injection strategies. This approach deviates from the prototypical treacherous turn behavior since the behavior is explicitly trained into the agent, rather than occurring as an emergent consequence of environmental complexity or poor objective specification. Nonetheless, these experiments provide new insights into the challenges of producing agents capable of true treacherous turn behavior.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying the Spread of Online Incivility in Brazilian Politics</title>
<link>https://arxiv.org/abs/2504.08960</link>
<guid>https://arxiv.org/abs/2504.08960</guid>
<content:encoded><![CDATA[
<div> 

<br />

 speech  2022  5  2,307 Twitter <div>
arXiv:2504.08960v1 Announce Type: new 
Abstract: Incivility refers to behaviors that violate collective norms and disrupt cooperation within the political process. Although large-scale online data and automated techniques have enabled the quantitative analysis of uncivil discourse, prior research has predominantly focused on impoliteness or toxicity, often overlooking other behaviors that undermine democratic values. To address this gap, we propose a multidimensional conceptual framework encompassing Impoliteness, Physical Harm and Violent Political Rhetoric, Hate Speech and Stereotyping, and Threats to Democratic Institutions and Values. Using this framework, we measure the spread of online political incivility in Brazil using approximately 5 million tweets posted by 2,307 political influencers during the 2022 Brazilian general election. Through statistical modeling and network analysis, we examine the dynamics of uncivil posts at different election stages, identify key disseminators and audiences, and explore the mechanisms driving the spread of uncivil information online. Our findings indicate that impoliteness is more likely to surge during election campaigns. In contrast, the other dimensions of incivility are often triggered by specific violent events. Moreover, we find that left-aligned individual influencers are the primary disseminators of online incivility in the Brazilian Twitter/X sphere and that they disseminate not only direct incivility but also indirect incivility when discussing or opposing incivility expressed by others. They relay those content from politicians, media agents, and individuals to reach broader audiences, revealing a diffusion pattern mixing the direct and two-step flows of communication theory. This study offers new insights into the multidimensional nature of incivility in Brazilian politics and provides a conceptual framework that can be extended to other political contexts.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Champion-level Vision-based Reinforcement Learning Agent for Competitive Racing in Gran Turismo 7</title>
<link>https://arxiv.org/abs/2504.09021</link>
<guid>https://arxiv.org/abs/2504.09021</guid>
<content:encoded><![CDATA[
<div> : 

:
GT7actor-criticGT7GT7 <div>
arXiv:2504.09021v1 Announce Type: new 
Abstract: Deep reinforcement learning has achieved superhuman racing performance in high-fidelity simulators like Gran Turismo 7 (GT7). It typically utilizes global features that require instrumentation external to a car, such as precise localization of agents and opponents, limiting real-world applicability. To address this limitation, we introduce a vision-based autonomous racing agent that relies solely on ego-centric camera views and onboard sensor data, eliminating the need for precise localization during inference. This agent employs an asymmetric actor-critic framework: the actor uses a recurrent neural network with the sensor data local to the car to retain track layouts and opponent positions, while the critic accesses the global features during training. Evaluated in GT7, our agent consistently outperforms GT7's built-drivers. To our knowledge, this work presents the first vision-based autonomous racing agent to demonstrate champion-level performance in competitive racing scenarios.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems</title>
<link>https://arxiv.org/abs/2504.09037</link>
<guid>https://arxiv.org/abs/2504.09037</guid>
<content:encoded><![CDATA[
<div> : 

<br /><br />:
LLMLLMDeepSeek-R1OpenAI Deep ResearchManus AgentPPOGRPO-LLM <div>
arXiv:2504.09037v1 Announce Type: new 
Abstract: Reasoning is a fundamental cognitive process that enables logical inference, problem-solving, and decision-making. With the rapid advancement of large language models (LLMs), reasoning has emerged as a key capability that distinguishes advanced AI systems from conventional models that empower chatbots. In this survey, we categorize existing methods along two orthogonal dimensions: (1) Regimes, which define the stage at which reasoning is achieved (either at inference time or through dedicated training); and (2) Architectures, which determine the components involved in the reasoning process, distinguishing between standalone LLMs and agentic compound systems that incorporate external tools, and multi-agent collaborations. Within each dimension, we analyze two key perspectives: (1) Input level, which focuses on techniques that construct high-quality prompts that the LLM condition on; and (2) Output level, which methods that refine multiple sampled candidates to enhance reasoning quality. This categorization provides a systematic understanding of the evolving landscape of LLM reasoning, highlighting emerging trends such as the shift from inference-scaling to learning-to-reason (e.g., DeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep Research, Manus Agent). Additionally, we cover a broad spectrum of learning algorithms, from supervised fine-tuning to reinforcement learning such as PPO and GRPO, and the training of reasoners and verifiers. We also examine key designs of agentic workflows, from established patterns like generator-evaluator and LLM debate to recent innovations. ...
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Dialogue Systems with Discourse-Level Understanding Using Deep Canonical Correlation Analysis</title>
<link>https://arxiv.org/abs/2504.09094</link>
<guid>https://arxiv.org/abs/2504.09094</guid>
<content:encoded><![CDATA[
<div> : Canonical(DCCA)

:<br />
CanonicalDCCAUbuntuDCCA <div>
arXiv:2504.09094v1 Announce Type: new 
Abstract: The evolution of conversational agents has been driven by the need for more contextually aware systems that can effectively manage dialogue over extended interactions. To address the limitations of existing models in capturing and utilizing long-term conversational history, we propose a novel framework that integrates Deep Canonical Correlation Analysis (DCCA) for discourse-level understanding. This framework learns discourse tokens to capture relationships between utterances and their surrounding context, enabling a better understanding of long-term dependencies. Experiments on the Ubuntu Dialogue Corpus demonstrate significant enhancement in response selection, based on the improved automatic evaluation metric scores. The results highlight the potential of DCCA in improving dialogue systems by allowing them to filter out irrelevant context and retain critical discourse information for more accurate response retrieval.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMPACT: Behavioral Intention-aware Multimodal Trajectory Prediction with Adaptive Context Trimming</title>
<link>https://arxiv.org/abs/2504.09103</link>
<guid>https://arxiv.org/abs/2504.09103</guid>
<content:encoded><![CDATA[
<div> : 

:
WaymoArgoverseWaymo Motion DatasetLiDAR-freeWaymo Interactive Prediction DatasetWaymo Interactive Prediction Leaderboard10% <div>
arXiv:2504.09103v1 Announce Type: new 
Abstract: While most prior research has focused on improving the precision of multimodal trajectory predictions, the explicit modeling of multimodal behavioral intentions (e.g., yielding, overtaking) remains relatively underexplored. This paper proposes a unified framework that jointly predicts both behavioral intentions and trajectories to enhance prediction accuracy, interpretability, and efficiency. Specifically, we employ a shared context encoder for both intention and trajectory predictions, thereby reducing structural redundancy and information loss. Moreover, we address the lack of ground-truth behavioral intention labels in mainstream datasets (Waymo, Argoverse) by auto-labeling these datasets, thus advancing the community's efforts in this direction. We further introduce a vectorized occupancy prediction module that infers the probability of each map polyline being occupied by the target vehicle's future trajectory. By leveraging these intention and occupancy prediction priors, our method conducts dynamic, modality-dependent pruning of irrelevant agents and map polylines in the decoding stage, effectively reducing computational overhead and mitigating noise from non-critical elements. Our approach ranks first among LiDAR-free methods on the Waymo Motion Dataset and achieves first place on the Waymo Interactive Prediction Dataset. Remarkably, even without model ensembling, our single-model framework improves the soft mean average precision (softmAP) by 10 percent compared to the second-best method in the Waymo Interactive Prediction Leaderboard. Furthermore, the proposed framework has been successfully deployed on real vehicles, demonstrating its practical effectiveness in real-world applications.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tell-XR: Conversational End-User Development of XR Automations</title>
<link>https://arxiv.org/abs/2504.09104</link>
<guid>https://arxiv.org/abs/2504.09104</guid>
<content:encoded><![CDATA[
<div> Extended Reality (XR)AuthoringInteractive ExperiencesIntelligent AgentLarge Language Models (LLMs)

<br /><br />:
Tell-XRLLMs(XR)VRARTell-XRXR <div>
arXiv:2504.09104v1 Announce Type: new 
Abstract: The availability of extended reality (XR) devices has widened their adoption, yet authoring interactive experiences remains complex for non-programmers. We introduce Tell-XR, an intelligent agent leveraging large language models (LLMs) to guide end-users in defining the interaction in XR settings using automations described as Event-Condition-Action (ECA) rules. Through a formative study, we identified the key conversation stages to define and refine automations, which informed the design of the system architecture. The evaluation study in two scenarios (a VR museum and an AR smart home) demonstrates the effectiveness of Tell-XR across different XR interaction settings.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Langformers: Unified NLP Pipelines for Language Models</title>
<link>https://arxiv.org/abs/2504.09170</link>
<guid>https://arxiv.org/abs/2504.09170</guid>
<content:encoded><![CDATA[
<div> LangformersTransformerPython

:<br />
LangformersPythonNLPLLMMLMAIMLM/Hugging FaceOllamaLangformers(1) (2) (3) https://langformers.com  <div>
arXiv:2504.09170v1 Announce Type: new 
Abstract: Transformer-based language models have revolutionized the field of natural language processing (NLP). However, using these models often involves navigating multiple frameworks and tools, as well as writing repetitive boilerplate code. This complexity can discourage non-programmers and beginners, and even slow down prototyping for experienced developers. To address these challenges, we introduce Langformers, an open-source Python library designed to streamline NLP pipelines through a unified, factory-based interface for large language model (LLM) and masked language model (MLM) tasks. Langformers integrates conversational AI, MLM pretraining, text classification, sentence embedding/reranking, data labelling, semantic search, and knowledge distillation into a cohesive API, supporting popular platforms such as Hugging Face and Ollama. Key innovations include: (1) task-specific factories that abstract training, inference, and deployment complexities; (2) built-in memory and streaming for conversational agents; and (3) lightweight, modular design that prioritizes ease of use. Documentation: https://langformers.com
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SceneScout: Towards AI Agent-driven Access to Street View Imagery for Blind Users</title>
<link>https://arxiv.org/abs/2504.09227</link>
<guid>https://arxiv.org/abs/2504.09227</guid>
<content:encoded><![CDATA[
<div> : blindlow vision, Street view, SceneScout, , 

:
SceneScoutAISceneScout(1) (2) N=10SceneScoutBLV72%95% <div>
arXiv:2504.09227v1 Announce Type: new 
Abstract: People who are blind or have low vision (BLV) may hesitate to travel independently in unfamiliar environments due to uncertainty about the physical landscape. While most tools focus on in-situ navigation, those exploring pre-travel assistance typically provide only landmarks and turn-by-turn instructions, lacking detailed visual context. Street view imagery, which contains rich visual information and has the potential to reveal numerous environmental details, remains inaccessible to BLV people. In this work, we introduce SceneScout, a multimodal large language model (MLLM)-driven AI agent that enables accessible interactions with street view imagery. SceneScout supports two modes: (1) Route Preview, enabling users to familiarize themselves with visual details along a route, and (2) Virtual Exploration, enabling free movement within street view imagery. Our user study (N=10) demonstrates that SceneScout helps BLV users uncover visual information otherwise unavailable through existing means. A technical evaluation shows that most descriptions are accurate (72%) and describe stable visual elements (95%) even in older imagery, though occasional subtle and plausible errors make them difficult to verify without sight. We discuss future opportunities and challenges of using street view imagery to enhance navigation experiences.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Commit: Helping Users Update Intent Specifications for AI Memory at Scale</title>
<link>https://arxiv.org/abs/2504.09283</link>
<guid>https://arxiv.org/abs/2504.09283</guid>
<content:encoded><![CDATA[
<div> AISemanticCommit

:
AIUISemanticCommitAISemanticCommitRAGLLM12ChatGPTAIAIAIAI <div>
arXiv:2504.09283v1 Announce Type: new 
Abstract: How do we update AI memory of user intent as intent changes? We consider how an AI interface may assist the integration of new information into a repository of natural language data. Inspired by software engineering concepts like impact analysis, we develop methods and a UI for managing semantic changes with non-local effects, which we call "semantic conflict resolution." The user commits new intent to a project -- makes a "semantic commit" -- and the AI helps the user detect and resolve semantic conflicts within a store of existing information representing their intent (an "intent specification"). We develop an interface, SemanticCommit, to better understand how users resolve conflicts when updating intent specifications such as Cursor Rules and game design documents. A knowledge graph-based RAG pipeline drives conflict detection, while LLMs assist in suggesting resolutions. We evaluate our technique on an initial benchmark. Then, we report a 12 user within-subjects study of SemanticCommit for two task domains -- game design documents, and AI agent memory in the style of ChatGPT memories -- where users integrated new information into an existing list. Half of our participants adopted a workflow of impact analysis, where they would first flag conflicts without AI revisions then resolve conflicts locally, despite having access to a global revision feature. We argue that AI agent interfaces, such as software IDEs like Cursor and Windsurf, should provide affordances for impact analysis and help users validate AI retrieval independently from generation. Our work speaks to how AI agent designers should think about updating memory as a process that involves human feedback and decision-making.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Insurance Reserving with CVaR-Constrained Reinforcement Learning under Macroeconomic Regimes</title>
<link>https://arxiv.org/abs/2504.09396</link>
<guid>https://arxiv.org/abs/2504.09396</guid>
<content:encoded><![CDATA[
<div> :  (RL)

:
(RL) horizon (MDP)(CVaR)(PPO)II(ORSA)RL-CVaRCVaR$_{0.95}$ <div>
arXiv:2504.09396v1 Announce Type: new 
Abstract: This paper proposes a reinforcement learning (RL) framework for insurance reserving that integrates tail-risk sensitivity, macroeconomic regime modeling, and regulatory compliance. The reserving problem is formulated as a finite-horizon Markov Decision Process (MDP), in which reserve adjustments are optimized using Proximal Policy Optimization (PPO) subject to Conditional Value-at-Risk (CVaR) constraints. To enhance policy robustness across varying economic conditions, the agent is trained using a regime-aware curriculum that progressively increases volatility exposure.
  The reward structure penalizes reserve shortfall, capital inefficiency, and solvency floor violations, with design elements informed by Solvency II and Own Risk and Solvency Assessment (ORSA) frameworks. Empirical evaluations on two industry datasets--Workers' Compensation, and Other Liability--demonstrate that the RL-CVaR agent achieves superior performance relative to classical reserving methods across multiple criteria, including tail-risk control (CVaR$_{0.95}$), capital efficiency, and regulatory violation rate. The framework also accommodates fixed-shock stress testing and regime-stratified analysis, providing a principled and extensible approach to reserving under uncertainty.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unification of Consensus-Based Multi-Objective Optimization and Multi-Robot Path Planning</title>
<link>https://arxiv.org/abs/2504.09577</link>
<guid>https://arxiv.org/abs/2504.09577</guid>
<content:encoded><![CDATA[
<div> : SQP

<br /><br />:
AgentSQP <div>
arXiv:2504.09577v1 Announce Type: new 
Abstract: Multi-agent systems seeking consensus may also have other objective functions to optimize, requiring the research of multi-objective optimization in consensus. Several recent publications have explored this domain using various methods such as weighted-sum optimization and penalization methods. This paper reviews the state of the art for consensus-based multi-objective optimization, poses a multi-agent lunar rover exploration problem seeking consensus and maximization of explored area, and achieves optimal edge weights and steering angles by applying SQP algorithms.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AirVista-II: An Agentic System for Embodied UAVs Toward Dynamic Scene Semantic Understanding</title>
<link>https://arxiv.org/abs/2504.09583</link>
<guid>https://arxiv.org/abs/2504.09583</guid>
<content:encoded><![CDATA[
<div> Unmanned Aerial Vehicles (UAVs), AirVista-II, , , 

<br /><br />:
AirVista-IIAirVista-II <div>
arXiv:2504.09583v1 Announce Type: new 
Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly important in dynamic environments such as logistics transportation and disaster response. However, current tasks often rely on human operators to monitor aerial videos and make operational decisions. This mode of human-machine collaboration suffers from significant limitations in efficiency and adaptability. In this paper, we present AirVista-II -- an end-to-end agentic system for embodied UAVs, designed to enable general-purpose semantic understanding and reasoning in dynamic scenes. The system integrates agent-based task identification and scheduling, multimodal perception mechanisms, and differentiated keyframe extraction strategies tailored for various temporal scenarios, enabling the efficient capture of critical scene information. Experimental results demonstrate that the proposed system achieves high-quality semantic understanding across diverse UAV-based dynamic scenarios under a zero-shot setting.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoNav: Empowering MLLMs with Explicit Geospatial Reasoning Abilities for Language-Goal Aerial Navigation</title>
<link>https://arxiv.org/abs/2504.09587</link>
<guid>https://arxiv.org/abs/2504.09587</guid>
<content:encoded><![CDATA[
<div> UAVGeoNav

:
GeoNav(UAV)GeoNavGeoNavGeoNavCityNavGeoNav12.53%UAV <div>
arXiv:2504.09587v1 Announce Type: new 
Abstract: Language-goal aerial navigation is a critical challenge in embodied AI, requiring UAVs to localize targets in complex environments such as urban blocks based on textual specification. Existing methods, often adapted from indoor navigation, struggle to scale due to limited field of view, semantic ambiguity among objects, and lack of structured spatial reasoning. In this work, we propose GeoNav, a geospatially aware multimodal agent to enable long-range navigation. GeoNav operates in three phases-landmark navigation, target search, and precise localization-mimicking human coarse-to-fine spatial strategies. To support such reasoning, it dynamically builds two different types of spatial memory. The first is a global but schematic cognitive map, which fuses prior textual geographic knowledge and embodied visual cues into a top-down, annotated form for fast navigation to the landmark region. The second is a local but delicate scene graph representing hierarchical spatial relationships between blocks, landmarks, and objects, which is used for definite target localization. On top of this structured representation, GeoNav employs a spatially aware, multimodal chain-of-thought prompting mechanism to enable multimodal large language models with efficient and interpretable decision-making across stages. On the CityNav urban navigation benchmark, GeoNav surpasses the current state-of-the-art by up to 12.53% in success rate and significantly improves navigation efficiency, even in hard-level tasks. Ablation studies highlight the importance of each module, showcasing how geospatial representations and coarse-to-fine reasoning enhance UAV navigation.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ERL-MPP: Evolutionary Reinforcement Learning with Multi-head Puzzle Perception for Solving Large-scale Jigsaw Puzzles of Eroded Gaps</title>
<link>https://arxiv.org/abs/2504.09608</link>
<guid>https://arxiv.org/abs/2504.09608</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:
Evolutionary Reinforcement Learning with Multi-head Puzzle Perception (ERL-MPP)Multi-head Puzzle Perception Network (MPPN)EvoRLERL-MPPJPLEG-5MIT <div>
arXiv:2504.09608v1 Announce Type: new 
Abstract: Solving jigsaw puzzles has been extensively studied. While most existing models focus on solving either small-scale puzzles or puzzles with no gap between fragments, solving large-scale puzzles with gaps presents distinctive challenges in both image understanding and combinatorial optimization. To tackle these challenges, we propose a framework of Evolutionary Reinforcement Learning with Multi-head Puzzle Perception (ERL-MPP) to derive a better set of swapping actions for solving the puzzles. Specifically, to tackle the challenges of perceiving the puzzle with gaps, a Multi-head Puzzle Perception Network (MPPN) with a shared encoder is designed, where multiple puzzlet heads comprehensively perceive the local assembly status, and a discriminator head provides a global assessment of the puzzle. To explore the large swapping action space efficiently, an Evolutionary Reinforcement Learning (EvoRL) agent is designed, where an actor recommends a set of suitable swapping actions from a large action space based on the perceived puzzle status, a critic updates the actor using the estimated rewards and the puzzle status, and an evaluator coupled with evolutionary strategies evolves the actions aligning with the historical assembly experience. The proposed ERL-MPP is comprehensively evaluated on the JPLEG-5 dataset with large gaps and the MIT dataset with large-scale puzzles. It significantly outperforms all state-of-the-art models on both datasets.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metropolis-Hastings Captioning Game: Knowledge Fusion of Vision Language Models via Decentralized Bayesian Inference</title>
<link>https://arxiv.org/abs/2504.09620</link>
<guid>https://arxiv.org/abs/2504.09620</guid>
<content:encoded><![CDATA[
<div> : Metropolis-Hastings Captioning Game (MHCG), , , , 

:
Metropolis-Hastings Captioning Game (MHCG)-(VLMs)VLMVLMMHCGMHCGVLMs <div>
arXiv:2504.09620v1 Announce Type: new 
Abstract: We propose the Metropolis-Hastings Captioning Game (MHCG), a method to fuse knowledge of multiple vision-language models (VLMs) by learning from each other. Although existing methods that combine multiple models suffer from inference costs and architectural constraints, MHCG avoids these problems by performing decentralized Bayesian inference through a process resembling a language game. The knowledge fusion process establishes communication between two VLM agents alternately captioning images and learning from each other. We conduct two image-captioning experiments with two VLMs, each pre-trained on a different dataset. The first experiment demonstrates that MHCG achieves consistent improvement in reference-free evaluation metrics. The second experiment investigates how MHCG contributes to sharing VLMs' category-level vocabulary by observing the occurrence of the vocabulary in the generated captions.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentDynEx: Nudging the Mechanics and Dynamics of Multi-Agent Simulations</title>
<link>https://arxiv.org/abs/2504.09662</link>
<guid>https://arxiv.org/abs/2504.09662</guid>
<content:encoded><![CDATA[
<div> : AgentDynExnudging

<br /><br />:
AgentDynExAIAgentDynExLLMsnudgingnudgingnudgingnudging <div>
arXiv:2504.09662v1 Announce Type: new 
Abstract: Multi-agent large language model simulations have the potential to model complex human behaviors and interactions. If the mechanics are set up properly, unanticipated and valuable social dynamics can surface. However, it is challenging to consistently enforce simulation mechanics while still allowing for notable and emergent dynamics. We present AgentDynEx, an AI system that helps set up simulations from user-specified mechanics and dynamics. AgentDynEx uses LLMs to guide users through a Configuration Matrix to identify core mechanics and define milestones to track dynamics. It also introduces a method called \textit{nudging}, where the system dynamically reflects on simulation progress and gently intervenes if it begins to deviate from intended outcomes. A technical evaluation found that nudging enables simulations to have more complex mechanics and maintain its notable dynamics compared to simulations without nudging. We discuss the importance of nudging as a technique for balancing mechanics and dynamics of multi-agent simulations.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLEAR-KGQA: Clarification-Enhanced Ambiguity Resolution for Knowledge Graph Question Answering</title>
<link>https://arxiv.org/abs/2504.09665</link>
<guid>https://arxiv.org/abs/2504.09665</guid>
<content:encoded><![CDATA[
<div> : 

:<br />
KGQALLMWebQSPCWQ <div>
arXiv:2504.09665v1 Announce Type: new 
Abstract: This study addresses the challenge of ambiguity in knowledge graph question answering (KGQA). While recent KGQA systems have made significant progress, particularly with the integration of large language models (LLMs), they typically assume user queries are unambiguous, which is an assumption that rarely holds in real-world applications. To address these limitations, we propose a novel framework that dynamically handles both entity ambiguity (e.g., distinguishing between entities with similar names) and intent ambiguity (e.g., clarifying different interpretations of user queries) through interactive clarification. Our approach employs a Bayesian inference mechanism to quantify query ambiguity and guide LLMs in determining when and how to request clarification from users within a multi-turn dialogue framework. We further develop a two-agent interaction framework where an LLM-based user simulator enables iterative refinement of logical forms through simulated user feedback. Experimental results on the WebQSP and CWQ dataset demonstrate that our method significantly improves performance by effectively resolving semantic ambiguities. Additionally, we contribute a refined dataset of disambiguated queries, derived from interaction histories, to facilitate future research in this direction.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nash Social Welfare with Submodular Valuations: Approximation Algorithms and Integrality Gaps</title>
<link>https://arxiv.org/abs/2504.09669</link>
<guid>https://arxiv.org/abs/2504.09669</guid>
<content:encoded><![CDATA[
<div> 


 NSW  $(5.18+\epsilon)$  NSW  $(3.914+\epsilon)$  NSW  LP  $(2^{\ln 2}-\epsilon) \approx 1.617-\epsilon$  $e/(e-1)-\epsilon \approx 1.582-\epsilon$  LP  $(e^{1/e}-\epsilon)$ $(e^{1/e}+\epsilon)$  NSW  $(2^{1/4}-\epsilon) \approx 1.189-\epsilon$  $\sqrt{8/7} \approx 1.069$ <br /><br />

 <div>
arXiv:2504.09669v1 Announce Type: new 
Abstract: We study the problem of allocating items to agents such that the (un)weighted Nash social welfare (NSW) is maximized under submodular valuations. The best-known results for unweighted and weighted problems are the $(4+\epsilon)$ approximation given by Garg, Husic, Li, Vega, and Vondrak~\cite{stoc/GargHLVV23} and the $(233+\epsilon)$ approximation given by Feng, Hu, Li, and Zhang~\cite{stoc/FHLZ25}, respectively.
  For the weighted NSW problem, we present a $(5.18+\epsilon)$-approximation algorithm, significantly improving the previous approximation ratio and simplifying the analysis. Our algorithm is based on the same configuration LP in~\cite{stoc/FHLZ25}, but with a modified rounding algorithm. For the unweighted NSW problem, we show that the local search-based algorithm in~\cite{stoc/GargHLVV23} is an approximation of $(3.914+\epsilon)$ by more careful analysis.
  On the negative side, we prove that the configuration LP for weighted NSW with submodular valuations has an integrality gap at least $2^{\ln 2}-\epsilon \approx 1.617 - \epsilon$, which is slightly larger than the current best-known $e/(e-1)-\epsilon \approx 1.582-\epsilon$ hardness of approximation~\cite{talg/GargKK23}. For the additive valuation case, we show an integrality gap of $(e^{1/e}-\epsilon)$, which proves that the ratio of $(e^{1/e}+\epsilon)$~\cite{icalp/FengLi24} is tight for algorithms based on the configuration LP. For unweighted NSW with additive valuations, we show a gap of $(2^{1/4}-\epsilon) \approx 1.189-\epsilon$, slightly larger than the current best-known $\sqrt{8/7} \approx 1.069$-hardness for the problem~\cite{mor/Garg0M24}.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety</title>
<link>https://arxiv.org/abs/2504.09689</link>
<guid>https://arxiv.org/abs/2504.09689</guid>
<content:encoded><![CDATA[
<div> EmoAgentLLMAIEmoEvalEmoGuard

<br /><br />:
EmoAgentAIAIEmoAgentEmoEvalAIPHQ-9PDIPANSSLLMEmoGuard34.4%EmoGuardGitHubhttps://github.com/1akaman/EmoAgent <div>
arXiv:2504.09689v1 Announce Type: new 
Abstract: The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https://github.com/1akaman/EmoAgent
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLRC-Bench: Can Language Agents Solve Machine Learning Research Challenges?</title>
<link>https://arxiv.org/abs/2504.09702</link>
<guid>https://arxiv.org/abs/2504.09702</guid>
<content:encoded><![CDATA[
<div> MLRC-Bench

:
MLRC-BenchOpenAI's MLE-BenchMETR's RE-BenchMLRC-BenchAI ScientistMLRC-Benchgemini-exp-1206MLAB9.3%MLRC-Bench<br /><br /> <div>
arXiv:2504.09702v1 Announce Type: new 
Abstract: Existing evaluation of large language model (LLM) agents on scientific discovery lacks objective baselines and metrics to assess the viability of their proposed methods. To address this issue, we introduce MLRC-Bench, a benchmark designed to quantify how effectively language agents can tackle challenging Machine Learning (ML) Research Competitions. Our benchmark highlights open research problems that demand novel methodologies, in contrast to recent benchmarks such as OpenAI's MLE-Bench (Chan et al., 2024) and METR's RE-Bench (Wijk et al., 2024), which focus on well-established research tasks that are largely solvable through sufficient engineering effort. Unlike prior work, e.g., AI Scientist (Lu et al., 2024b), which evaluates the end-to-end agentic pipeline by using LLM-as-a-judge, MLRC-Bench measures the key steps of proposing and implementing novel research methods and evaluates them with newly proposed rigorous protocol and objective metrics. Our curated suite of 7 competition tasks reveals significant challenges for LLM agents. Even the best-performing tested agent (gemini-exp-1206 under MLAB (Huang et al., 2024a)) closes only 9.3% of the gap between baseline and top human participant scores. Furthermore, our analysis reveals a misalignment between the LLM-judged innovation and their actual performance on cutting-edge ML research problems. MLRC-Bench is a dynamic benchmark, which is designed to continually grow with new ML competitions to encourage rigorous and objective evaluations of AI's research capabilities.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents</title>
<link>https://arxiv.org/abs/2504.09723</link>
<guid>https://arxiv.org/abs/2504.09723</guid>
<content:encoded><![CDATA[
<div> A/BAgentA/B

<br /><br />:
AgentA/BLLMA/BAgentA/BLLMAgentA/BAmazon.com1000LLMA/BAgentA/B <div>
arXiv:2504.09723v1 Announce Type: new 
Abstract: A/B testing experiment is a widely adopted method for evaluating UI/UX design decisions in modern web applications. Yet, traditional A/B testing remains constrained by its dependence on the large-scale and live traffic of human participants, and the long time of waiting for the testing result. Through formative interviews with six experienced industry practitioners, we identified critical bottlenecks in current A/B testing workflows. In response, we present AgentA/B, a novel system that leverages Large Language Model-based autonomous agents (LLM Agents) to automatically simulate user interaction behaviors with real webpages. AgentA/B enables scalable deployment of LLM agents with diverse personas, each capable of navigating the dynamic webpage and interactively executing multi-step interactions like search, clicking, filtering, and purchasing. In a demonstrative controlled experiment, we employ AgentA/B to simulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and compare agent behaviors with real human shopping behaviors at a scale. Our findings suggest AgentA/B can emulate human-like behavior patterns.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning-based decentralized control with collision avoidance for multi-agent systems</title>
<link>https://arxiv.org/abs/2504.09730</link>
<guid>https://arxiv.org/abs/2504.09730</guid>
<content:encoded><![CDATA[
<div> Gaussian(GP)SE(3)

<br /><br />:
GPSE(3)GP <div>
arXiv:2504.09730v1 Announce Type: new 
Abstract: In this paper, we present a learning-based tracking controller based on Gaussian processes (GP) for collision avoidance of multi-agent systems where the agents evolve in the special Euclidean group in the space SE(3). In particular, we use GPs to estimate certain uncertainties that appear in the dynamics of the agents. The control algorithm is designed to learn and mitigate these uncertainties by using GPs as a learning-based model for the predictions. In particular, the presented approach guarantees that the tracking error remains bounded with high probability. We present some simulation results to show how the control algorithm is implemented.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLM feedback enhance review quality? A randomized study of 20K reviews at ICLR 2025</title>
<link>https://arxiv.org/abs/2504.09737</link>
<guid>https://arxiv.org/abs/2504.09737</guid>
<content:encoded><![CDATA[
<div> : Review Feedback AgentICLR 2025

:
AIReview Feedback AgentLLMsICLR 202520,000LLMs27%12,000AIAI-80LLMReview Feedback AgentGitHubhttps://github.com/zou-group/review_feedback_agent <div>
arXiv:2504.09737v1 Announce Type: new 
Abstract: Peer review at AI conferences is stressed by rapidly rising submission volumes, leading to deteriorating review quality and increased author dissatisfaction. To address these issues, we developed Review Feedback Agent, a system leveraging multiple large language models (LLMs) to improve review clarity and actionability by providing automated feedback on vague comments, content misunderstandings, and unprofessional remarks to reviewers. Implemented at ICLR 2025 as a large randomized control study, our system provided optional feedback to more than 20,000 randomly selected reviews. To ensure high-quality feedback for reviewers at this scale, we also developed a suite of automated reliability tests powered by LLMs that acted as guardrails to ensure feedback quality, with feedback only being sent to reviewers if it passed all the tests. The results show that 27% of reviewers who received feedback updated their reviews, and over 12,000 feedback suggestions from the agent were incorporated by those reviewers. This suggests that many reviewers found the AI-generated feedback sufficiently helpful to merit updating their reviews. Incorporating AI feedback led to significantly longer reviews (an average increase of 80 words among those who updated after receiving feedback) and more informative reviews, as evaluated by blinded researchers. Moreover, reviewers who were selected to receive AI feedback were also more engaged during paper rebuttals, as seen in longer author-reviewer discussions. This work demonstrates that carefully designed LLM-generated review feedback can enhance peer review quality by making reviews more specific and actionable while increasing engagement between reviewers and authors. The Review Feedback Agent is publicly available at https://github.com/zou-group/review_feedback_agent.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Socratic Chart: Cooperating Multiple Agents for Robust SVG Chart Understanding</title>
<link>https://arxiv.org/abs/2504.09764</link>
<guid>https://arxiv.org/abs/2504.09764</guid>
<content:encoded><![CDATA[
<div> : (MLLMs), , , Socratic Chart, (SVG)

:
ChartQASocratic ChartSVGMLLMSocratic ChartMLLM <div>
arXiv:2504.09764v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable versatility but face challenges in demonstrating true visual understanding, particularly in chart reasoning tasks. Existing benchmarks like ChartQA reveal significant reliance on text-based shortcuts and probabilistic pattern-matching rather than genuine visual reasoning. To rigorously evaluate visual reasoning, we introduce a more challenging test scenario by removing textual labels and introducing chart perturbations in the ChartQA dataset. Under these conditions, models like GPT-4o and Gemini-2.0 Pro experience up to a 30% performance drop, underscoring their limitations. To address these challenges, we propose Socratic Chart, a new framework that transforms chart images into Scalable Vector Graphics (SVG) representations, enabling MLLMs to integrate textual and visual modalities for enhanced chart understanding. Socratic Chart employs a multi-agent pipeline with specialized agent-generators to extract primitive chart attributes (e.g., bar heights, line coordinates) and an agent-critic to validate results, ensuring high-fidelity symbolic representations. Our framework surpasses state-of-the-art models in accurately capturing chart primitives and improving reasoning performance, establishing a robust pathway for advancing MLLM visual understanding.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two Heads are Better Than One: Test-time Scaling of Multi-agent Collaborative Reasoning</title>
<link>https://arxiv.org/abs/2504.09772</link>
<guid>https://arxiv.org/abs/2504.09772</guid>
<content:encoded><![CDATA[
<div> : (MAS), (LLMs), (TTS), , CEO

<br /><br />:
500M500Qwen2.5-32B-InstructM1-32BCEOGPQA-Diamond12%AIME202441%MBPP-Sanitized10%DeepSeek-R1https://github.com/jincan333/MAS-TTS  <div>
arXiv:2504.09772v1 Announce Type: new 
Abstract: Multi-agent systems (MAS) built on large language models (LLMs) offer a promising path toward solving complex, real-world tasks that single-agent systems often struggle to manage. While recent advancements in test-time scaling (TTS) have significantly improved single-agent performance on challenging reasoning tasks, how to effectively scale collaboration and reasoning in MAS remains an open question. In this work, we introduce an adaptive multi-agent framework designed to enhance collaborative reasoning through both model-level training and system-level coordination. We construct M500, a high-quality dataset containing 500 multi-agent collaborative reasoning traces, and fine-tune Qwen2.5-32B-Instruct on this dataset to produce M1-32B, a model optimized for multi-agent collaboration. To further enable adaptive reasoning, we propose a novel CEO agent that dynamically manages the discussion process, guiding agent collaboration and adjusting reasoning depth for more effective problem-solving. Evaluated in an open-source MAS across a range of tasks-including general understanding, mathematical reasoning, and coding-our system significantly outperforms strong baselines. For instance, M1-32B achieves 12% improvement on GPQA-Diamond, 41% on AIME2024, and 10% on MBPP-Sanitized, matching the performance of state-of-the-art models like DeepSeek-R1 on some tasks. These results highlight the importance of both learned collaboration and adaptive coordination in scaling multi-agent reasoning. Code is available at https://github.com/jincan333/MAS-TTS
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Court: Combining Reasoning, Action, and Judgment for Multi-Hop Reasoning</title>
<link>https://arxiv.org/abs/2504.09781</link>
<guid>https://arxiv.org/abs/2504.09781</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)Reasoning Court (RC)ReAct

:
<br />
Reasoning Court (RC)LLMsRCReActLLMLLMRCHotpotQAMuSiQueFEVER <div>
arXiv:2504.09781v1 Announce Type: new 
Abstract: While large language models (LLMs) have demonstrated strong capabilities in tasks like question answering and fact verification, they continue to suffer from hallucinations and reasoning errors, especially in multi-hop tasks that require integration of multiple information sources. Current methods address these issues through retrieval-based techniques (grounding reasoning in external evidence), reasoning-based approaches (enhancing coherence via improved prompting), or hybrid strategies combining both elements. One prominent hybrid method, ReAct, has outperformed purely retrieval-based or reasoning-based approaches; however, it lacks internal verification of intermediate reasoning steps, allowing potential errors to propagate through complex reasoning tasks. In this paper, we introduce Reasoning Court (RC), a novel framework that extends iterative reasoning-and-retrieval methods, such as ReAct, with a dedicated LLM judge. Unlike ReAct, RC employs this judge to independently evaluate multiple candidate answers and their associated reasoning generated by separate LLM agents. The judge is asked to select the answer that it considers the most factually grounded and logically coherent based on the presented reasoning and evidence, or synthesizes a new answer using available evidence and its pre-trained knowledge if all candidates are inadequate, flawed, or invalid. Evaluations on multi-hop benchmarks (HotpotQA, MuSiQue) and fact-verification (FEVER) demonstrate that RC consistently outperforms state-of-the-art few-shot prompting methods without task-specific fine-tuning.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Process Calculus for Optimizing Data and Computation Sharing in Complex Stateful Parallel Computations</title>
<link>https://arxiv.org/abs/2504.09788</link>
<guid>https://arxiv.org/abs/2504.09788</guid>
<content:encoded><![CDATA[
<div> OptiFusion

:
$\pi$-OptiFusionOptiFusion102 <div>
arXiv:2504.09788v1 Announce Type: new 
Abstract: We propose novel techniques that exploit data and computation sharing to improve the performance of complex stateful parallel computations, like agent-based simulations. Parallel computations are translated into behavioral equations, a novel formalism layered on top of the foundational process calculus $\pi$-calculus. Behavioral equations blend code and data, allowing a system to easily compose and transform parallel programs into specialized programs. We show how optimizations like merging programs, synthesizing efficient message data structures, eliminating local messaging, rewriting communication instructions into local computations, and {aggregation pushdown} can be expressed as transformations of behavioral equations. We have also built a system called OptiFusion that implements behavioral equations and the aforementioned optimizations. Our experiments showed that OptiFusion is over 10$\times$ faster than state-of-the-art stateful systems benchmarked via complex stateful workloads. Generating specialized instructions that are impractical to write by hand allows OptiFusion to outperform even the hand-optimized implementations by up to 2$\times$.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Small Reasoning LLMs with Cognitive Preference Alignment</title>
<link>https://arxiv.org/abs/2504.09802</link>
<guid>https://arxiv.org/abs/2504.09802</guid>
<content:encoded><![CDATA[
<div> Critique-Rethink-Verify (CRV)

<br /><br />:

Critique-Rethink-Verify (CRV)CogPOCRVCogPO <div>
arXiv:2504.09802v1 Announce Type: new 
Abstract: The reasoning capabilities of large language models (LLMs), such as OpenAI's o1 and DeepSeek-R1, have seen substantial advancements through deep thinking. However, these enhancements come with significant resource demands, underscoring the need to explore strategies to train effective reasoning LLMs with far fewer parameters. A critical challenge is that smaller models have different capacities and cognitive trajectories than their larger counterparts. Hence, direct distillation of chain-of-thought (CoT) results from large LLMs to smaller ones can be sometimes ineffective and requires a huge amount of annotated data. In this paper, we introduce a novel framework called Critique-Rethink-Verify (CRV), designed for training smaller yet powerful reasoning LLMs. Our CRV framework consists of multiple LLM agents, each specializing in unique abilities: (i) critiquing the CoTs according to the cognitive capabilities of smaller models, (ii) rethinking and refining these CoTs based on the critiques, and (iii) verifying the correctness of the refined results. We further propose the cognitive preference optimization (CogPO) algorithm to enhance the reasoning abilities of smaller models by aligning thoughts of these models with their cognitive capacities. Comprehensive evaluations on challenging reasoning benchmarks demonstrate the efficacy of CRV and CogPO, which outperforms other training methods by a large margin.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StruPhantom: Evolutionary Injection Attacks on Black-Box Tabular Agents Powered by Large Language Models</title>
<link>https://arxiv.org/abs/2504.09841</link>
<guid>https://arxiv.org/abs/2504.09841</guid>
<content:encoded><![CDATA[
<div> : StruPhantom

<br /><br />:
StruPhantomStruPhantomLLM50% <div>
arXiv:2504.09841v1 Announce Type: new 
Abstract: The proliferation of autonomous agents powered by large language models (LLMs) has revolutionized popular business applications dealing with tabular data, i.e., tabular agents. Although LLMs are observed to be vulnerable against prompt injection attacks from external data sources, tabular agents impose strict data formats and predefined rules on the attacker's payload, which are ineffective unless the agent navigates multiple layers of structural data to incorporate the payload. To address the challenge, we present a novel attack termed StruPhantom which specifically targets black-box LLM-powered tabular agents. Our attack designs an evolutionary optimization procedure which continually refines attack payloads via the proposed constrained Monte Carlo Tree Search augmented by an off-topic evaluator. StruPhantom helps systematically explore and exploit the weaknesses of target applications to achieve goal hijacking. Our evaluation validates the effectiveness of StruPhantom across various LLM-based agents, including those on real-world platforms, and attack scenarios. Our attack achieves over 50% higher success rates than baselines in enforcing the application's response to contain phishing links or malicious codes.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ST-Booster: An Iterative SpatioTemporal Perception Booster for Vision-and-Language Navigation in Continuous Environments</title>
<link>https://arxiv.org/abs/2504.09843</link>
<guid>https://arxiv.org/abs/2504.09843</guid>
<content:encoded><![CDATA[
<div> : VLN-CEST-Booster

:<br />
(VLN-CE)ST-BoosterVLN-CEST-Booster(HSTE)(MGAF)(VGWG)HSTEMGAFVGWG(GAHs)-ST-Booster <div>
arXiv:2504.09843v1 Announce Type: new 
Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires agents to navigate unknown, continuous spaces based on natural language instructions. Compared to discrete settings, VLN-CE poses two core perception challenges. First, the absence of predefined observation points leads to heterogeneous visual memories and weakened global spatial correlations. Second, cumulative reconstruction errors in three-dimensional scenes introduce structural noise, impairing local feature perception. To address these challenges, this paper proposes ST-Booster, an iterative spatiotemporal booster that enhances navigation performance through multi-granularity perception and instruction-aware reasoning. ST-Booster consists of three key modules -- Hierarchical SpatioTemporal Encoding (HSTE), Multi-Granularity Aligned Fusion (MGAF), and ValueGuided Waypoint Generation (VGWG). HSTE encodes long-term global memory using topological graphs and captures shortterm local details via grid maps. MGAF aligns these dualmap representations with instructions through geometry-aware knowledge fusion. The resulting representations are iteratively refined through pretraining tasks. During reasoning, VGWG generates Guided Attention Heatmaps (GAHs) to explicitly model environment-instruction relevance and optimize waypoint selection. Extensive comparative experiments and performance analyses are conducted, demonstrating that ST-Booster outperforms existing state-of-the-art methods, particularly in complex, disturbance-prone environments.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PestMA: LLM-based Multi-Agent System for Informed Pest Management</title>
<link>https://arxiv.org/abs/2504.09855</link>
<guid>https://arxiv.org/abs/2504.09855</guid>
<content:encoded><![CDATA[
<div> PestMA

:<br />
LLMsPestMALLMPestMAPestMA86.8%92.6%LLM <div>
arXiv:2504.09855v1 Announce Type: new 
Abstract: Effective pest management is complex due to the need for accurate, context-specific decisions. Recent advancements in large language models (LLMs) open new possibilities for addressing these challenges by providing sophisticated, adaptive knowledge acquisition and reasoning. However, existing LLM-based pest management approaches often rely on a single-agent paradigm, which can limit their capacity to incorporate diverse external information, engage in systematic validation, and address complex, threshold-driven decisions. To overcome these limitations, we introduce PestMA, an LLM-based multi-agent system (MAS) designed to generate reliable and evidence-based pest management advice. Building on an editorial paradigm, PestMA features three specialized agents, an Editor for synthesizing pest management recommendations, a Retriever for gathering relevant external data, and a Validator for ensuring correctness. Evaluations on real-world pest scenarios demonstrate that PestMA achieves an initial accuracy of 86.8% for pest management decisions, which increases to 92.6% after validation. These results underscore the value of collaborative agent-based workflows in refining and validating decisions, highlighting the potential of LLM-based multi-agent systems to automate and enhance pest management processes.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLoRA: Sample-Efficient Preference-based RL via Low-Rank Style Adaptation of Reward Functions</title>
<link>https://arxiv.org/abs/2504.10002</link>
<guid>https://arxiv.org/abs/2504.10002</guid>
<content:encoded><![CDATA[
<div> Preference-based Reinforcement Learning (PbRL)(CRF)

<br /><br />:
 <div>
arXiv:2504.10002v1 Announce Type: new 
Abstract: Preference-based reinforcement learning (PbRL) is a suitable approach for style adaptation of pre-trained robotic behavior: adapting the robot's policy to follow human user preferences while still being able to perform the original task. However, collecting preferences for the adaptation process in robotics is often challenging and time-consuming. In this work we explore the adaptation of pre-trained robots in the low-preference-data regime. We show that, in this regime, recent adaptation approaches suffer from catastrophic reward forgetting (CRF), where the updated reward model overfits to the new preferences, leading the agent to become unable to perform the original task. To mitigate CRF, we propose to enhance the original reward model with a small number of parameters (low-rank matrices) responsible for modeling the preference adaptation. Our evaluation shows that our method can efficiently and effectively adjust robotic behavior to human preferences across simulation benchmark tasks and multiple real-world robotic tasks.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DataMosaic: Explainable and Verifiable Multi-Modal Data Analytics through Extract-Reason-Verify</title>
<link>https://arxiv.org/abs/2504.10036</link>
<guid>https://arxiv.org/abs/2504.10036</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)DataMosaic

:
<br />
DataMosaicLLMsDataMosaicLLMDataMosaicLLM <div>
arXiv:2504.10036v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are transforming data analytics, but their widespread adoption is hindered by two critical limitations: they are not explainable (opaque reasoning processes) and not verifiable (prone to hallucinations and unchecked errors). While retrieval-augmented generation (RAG) improves accuracy by grounding LLMs in external data, it fails to address the core challenges of trustworthy analytics - especially when processing noisy, inconsistent, or multi-modal data (for example, text, tables, images). We propose DataMosaic, a framework designed to make LLM-powered analytics both explainable and verifiable. By dynamically extracting task-specific structures (for example, tables, graphs, trees) from raw data, DataMosaic provides transparent, step-by-step reasoning traces and enables validation of intermediate results. Built on a multi-agent framework, DataMosaic orchestrates self-adaptive agents that align with downstream task requirements, enhancing consistency, completeness, and privacy. Through this approach, DataMosaic not only tackles the limitations of current LLM-powered analytics systems but also lays the groundwork for a new paradigm of grounded, accurate, and explainable multi-modal data analytics.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeRAG: Supportive Code Retrieval on Bigraph for Real-World Code Generation</title>
<link>https://arxiv.org/abs/2504.10046</link>
<guid>https://arxiv.org/abs/2504.10046</guid>
<content:encoded><![CDATA[
<div> : , , , , DS<br /><br />:
CodeRAGCodeRAGDSDSLLMDSCodeRAGLLMCodeRAGGPT-4oGemini-ProPass@140.90%37.79%LLMCopilotCursorCodeRAG <div>
arXiv:2504.10046v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown promising performance in automated code generation, especially excelling in simple tasks such as generating standalone codes. Different from simple tasks, real-world code generation usually depends on specific programming environment (e.g., code repositories). It contains complex dependencies and domain knowledge, which is needed for LLMs when generating target code snippets. In this paper, we propose CodeRAG, a retrieval-augmented code generation (RAG) framework to comprehensively retrieve supportive codes for real-world code generation. Beginning with the requirement, CodeRAG first constructs a requirement graph for the current repository, and retrieves sub- and similar- requirement nodes of the target requirement on the graph. Meanwhile, it models the repository into a DS-code graph. CodeRAG then maps these relevant requirement nodes into their corresponding code nodes, and treats these code nodes as archors for LLM reasoning on DS-code graph. Finally, CodeRAG introduces a code-oriented agentic reasoning process, seamlessly allowing LLMs to reason and comprehensively retrieve for supportive codes which LLMs' need for generating correct programs. Experiments show that CodeRAG achieves significant improvements (i.e., increasing 40.90 and 37.79 Pass@1 on GPT-4o and Gemini-Pro on DevEval) compared to no RAG scenarios. Further tests on reasoning LLMs (i.e., QwQ-32B) confirm CodeRAG's adaptability and efficacy across various types of LLMs. In addition, CodeRAG outperforms commercial programming products such as Copilit and Cursor. We further investigate the performance of our framework on different dependency types, and observe that CodeRAG is superior in generating examples where target codes invoke predefined cross-file code snippets. These results demonstrate CodeRAG's potential in solving real-world repo-level coding challenges.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Action Language Modelling for Transparent Policy Execution</title>
<link>https://arxiv.org/abs/2504.10055</link>
<guid>https://arxiv.org/abs/2504.10055</guid>
<content:encoded><![CDATA[
<div> : 

<br /><br />:
Language-Table <div>
arXiv:2504.10055v1 Announce Type: new 
Abstract: An agent's intention often remains hidden behind the black-box nature of embodied policies. Communication using natural language statements that describe the next action can provide transparency towards the agent's behavior. We aim to insert transparent behavior directly into the learning process, by transforming the problem of policy learning into a language generation problem and combining it with traditional autoregressive modelling. The resulting model produces transparent natural language statements followed by tokens representing the specific actions to solve long-horizon tasks in the Language-Table environment. Following previous work, the model is able to learn to produce a policy represented by special discretized tokens in an autoregressive manner. We place special emphasis on investigating the relationship between predicting actions and producing high-quality language for a transparent agent. We find that in many cases both the quality of the action trajectory and the transparent statement increase when they are generated simultaneously.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pay Attention to What and Where? Interpretable Feature Extractor in Vision-based Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.10071</link>
<guid>https://arxiv.org/abs/2504.10071</guid>
<content:encoded><![CDATA[
<div> 

:<br />
(IFE)IFEIFERainbow57ATARIIFE <div>
arXiv:2504.10071v1 Announce Type: new 
Abstract: Current approaches in Explainable Deep Reinforcement Learning have limitations in which the attention mask has a displacement with the objects in visual input. This work addresses a spatial problem within traditional Convolutional Neural Networks (CNNs). We propose the Interpretable Feature Extractor (IFE) architecture, aimed at generating an accurate attention mask to illustrate both "what" and "where" the agent concentrates on in the spatial domain. Our design incorporates a Human-Understandable Encoding module to generate a fully interpretable attention mask, followed by an Agent-Friendly Encoding module to enhance the agent's learning efficiency. These two components together form the Interpretable Feature Extractor for vision-based deep reinforcement learning to enable the model's interpretability. The resulting attention mask is consistent, highly understandable by humans, accurate in spatial dimension, and effectively highlights important objects or locations in visual input. The Interpretable Feature Extractor is integrated into the Fast and Data-efficient Rainbow framework, and evaluated on 57 ATARI games to show the effectiveness of the proposed approach on Spatial Preservation, Interpretability, and Data-efficiency. Finally, we showcase the versatility of our approach by incorporating the IFE into the Asynchronous Advantage Actor-Critic Model.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography</title>
<link>https://arxiv.org/abs/2504.10090</link>
<guid>https://arxiv.org/abs/2504.10090</guid>
<content:encoded><![CDATA[
<div> LLMsMLLMs

<br /><br />
LLMsMLLMsOpenAI o1Gemini 2.0 Flash ThinkingMLLMs-VLMsMLLMsMLLMMLLM <div>
arXiv:2504.10090v1 Announce Type: new 
Abstract: Large language models (LLMs) and multimodal large language models (MLLMs) have significantly advanced artificial intelligence. However, visual reasoning, reasoning involving both visual and textual inputs, remains underexplored. Recent advancements, including the reasoning models like OpenAI o1 and Gemini 2.0 Flash Thinking, which incorporate image inputs, have opened this capability. In this ongoing work, we focus specifically on photography-related tasks because a photo is a visual snapshot of the physical world where the underlying physics (i.e., illumination, blur extent, etc.) interplay with the camera parameters. Successfully reasoning from the visual information of a photo to identify these numerical camera settings requires the MLLMs to have a deeper understanding of the underlying physics for precise visual comprehension, representing a challenging and intelligent capability essential for practical applications like photography assistant agents. We aim to evaluate MLLMs on their ability to distinguish visual differences related to numerical camera settings, extending a methodology previously proposed for vision-language models (VLMs). Our preliminary results demonstrate the importance of visual reasoning in photography-related tasks. Moreover, these results show that no single MLLM consistently dominates across all evaluation tasks, demonstrating ongoing challenges and opportunities in developing MLLMs with better visual reasoning.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Data Barrier -- Building GUI Agents Through Task Generalization</title>
<link>https://arxiv.org/abs/2504.10127</link>
<guid>https://arxiv.org/abs/2504.10127</guid>
<content:encoded><![CDATA[
<div> : Graphical User Interface (GUI) Vision Language Models (VLMs)

:
Vision Language Models (VLMs)Graphical User Interface (GUI)GUIGUI(1) AndroidWorld6.3%GUIWebArenaAndroidWorld5.6%5.4%(2) GUIGUI(3) WebArenaAndroidWorld8.0%12.2%GUIhttps://github.com/hkust-nlp/GUIMid  <div>
arXiv:2504.10127v1 Announce Type: new 
Abstract: Graphical User Interface (GUI) agents offer cross-platform solutions for automating complex digital tasks, with significant potential to transform productivity workflows. However, their performance is often constrained by the scarcity of high-quality trajectory data. To address this limitation, we propose training Vision Language Models (VLMs) on data-rich, reasoning-intensive tasks during a dedicated mid-training stage, and then examine how incorporating these tasks facilitates generalization to GUI planning scenarios. Specifically, we explore a range of tasks with readily available instruction-tuning data, including GUI perception, multimodal reasoning, and textual reasoning. Through extensive experiments across 11 mid-training tasks, we demonstrate that: (1) Task generalization proves highly effective, yielding substantial improvements across most settings. For instance, multimodal mathematical reasoning enhances performance on AndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data significantly boosts GUI web agent performance, achieving a 5.6% improvement on WebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal generalization from text-based to visual domains; (2) Contrary to prior assumptions, GUI perception data - previously considered closely aligned with GUI agent tasks and widely utilized for training - has a comparatively limited impact on final performance; (3) Building on these insights, we identify the most effective mid-training tasks and curate optimized mixture datasets, resulting in absolute performance gains of 8.0% on WebArena and 12.2% on AndroidWorld. Our work provides valuable insights into cross-domain knowledge transfer for GUI agents and offers a practical approach to addressing data scarcity challenges in this emerging field. The code, data and models will be available at https://github.com/hkust-nlp/GUIMid.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Personalization: From RAG to Agent</title>
<link>https://arxiv.org/abs/2504.10147</link>
<guid>https://arxiv.org/abs/2504.10147</guid>
<content:encoded><![CDATA[
<div> : , Retrieval-Augmented Generation (RAG), , LLM, 

<br /><br />:
AIRetrieval-Augmented GenerationRAGRAGRAGLLMRAGhttps://github.com/Applied-Machine-Learning-Lab/Awesome-Personalized-RAG-Agent <div>
arXiv:2504.10147v1 Announce Type: new 
Abstract: Personalization has become an essential capability in modern AI systems, enabling customized interactions that align with individual user preferences, contexts, and goals. Recent research has increasingly concentrated on Retrieval-Augmented Generation (RAG) frameworks and their evolution into more advanced agent-based architectures within personalized settings to enhance user satisfaction. Building on this foundation, this survey systematically examines personalization across the three core stages of RAG: pre-retrieval, retrieval, and generation. Beyond RAG, we further extend its capabilities into the realm of Personalized LLM-based Agents, which enhance traditional RAG systems with agentic functionalities, including user understanding, personalized planning and execution, and dynamic generation. For both personalization in RAG and agent-based personalization, we provide formal definitions, conduct a comprehensive review of recent literature, and summarize key datasets and evaluation metrics. Additionally, we discuss fundamental challenges, limitations, and promising research directions in this evolving field. Relevant papers and resources are continuously updated at https://github.com/Applied-Machine-Learning-Lab/Awesome-Personalized-RAG-Agent.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users</title>
<link>https://arxiv.org/abs/2504.10157</link>
<guid>https://arxiv.org/abs/2504.10157</guid>
<content:encoded><![CDATA[
<div> : SocioVerse

<br />
: SocioVerseSocioVerse1000SocioVerse <div>
arXiv:2504.10157v1 Announce Type: new 
Abstract: Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fact-Checking with Contextual Narratives: Leveraging Retrieval-Augmented LLMs for Social Media Analysis</title>
<link>https://arxiv.org/abs/2504.10166</link>
<guid>https://arxiv.org/abs/2504.10166</guid>
<content:encoded><![CDATA[
<div> CRAVE

<br />
:
CRAVECRAVELLMCRAVECRAVE <div>
arXiv:2504.10166v1 Announce Type: new 
Abstract: We propose CRAVE (Cluster-based Retrieval Augmented Verification with Explanation); a novel framework that integrates retrieval-augmented Large Language Models (LLMs) with clustering techniques to address fact-checking challenges on social media. CRAVE automatically retrieves multimodal evidence from diverse, often contradictory, sources. Evidence is clustered into coherent narratives, and evaluated via an LLM-based judge to deliver fact-checking verdicts explained by evidence summaries. By synthesizing evidence from both text and image modalities and incorporating agent-based refinement, CRAVE ensures consistency and diversity in evidence representation. Comprehensive experiments demonstrate CRAVE's efficacy in retrieval precision, clustering quality, and judgment accuracy, showcasing its potential as a robust decision-support tool for fact-checkers.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C-FAITH: A Chinese Fine-Grained Benchmark for Automated Hallucination Evaluation</title>
<link>https://arxiv.org/abs/2504.10167</link>
<guid>https://arxiv.org/abs/2504.10167</guid>
<content:encoded><![CDATA[
<div> : HaluAgentC-FAITH

:
HaluAgentQAHaluAgentC-FAITH1,39960,702QA16LLMsC-FAITH <div>
arXiv:2504.10167v1 Announce Type: new 
Abstract: Despite the rapid advancement of large language models, they remain highly susceptible to generating hallucinations, which significantly hinders their widespread application. Hallucination research requires dynamic and fine-grained evaluation. However, most existing hallucination benchmarks (especially in Chinese language) rely on human annotations, making automatical and cost-effective hallucination evaluation challenging. To address this, we introduce HaluAgent, an agentic framework that automatically constructs fine-grained QA dataset based on some knowledge documents. Our experiments demonstrate that the manually designed rules and prompt optimization can improve the quality of generated data. Using HaluAgent, we construct C-FAITH, a Chinese QA hallucination benchmark created from 1,399 knowledge documents obtained from web scraping, totaling 60,702 entries. We comprehensively evaluate 16 mainstream LLMs with our proposed C-FAITH, providing detailed experimental results and analysis.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSCoT: Structured Chain-of-Thought Generation for Multiple Programming Languages</title>
<link>https://arxiv.org/abs/2504.10178</link>
<guid>https://arxiv.org/abs/2504.10178</guid>
<content:encoded><![CDATA[
<div> : Chain-of-Thought (CoT)MSCoT

<br /><br />:
CoTMSCoT12CoTMSCoTMSCoTCoTMSCoTCoTMSCoTCoT <div>
arXiv:2504.10178v1 Announce Type: new 
Abstract: With the rapid development of code intelligence, the application of multiple programming languages is becoming increasingly widespread. However, most existing code generation models mainly focus on a single or a few programming languages, resulting in unsatisfactory performance in a multilingual environment. Chain-of-Thought (CoT) reasoning can significantly improve the performance of the model without the need for retraining or fine-tuning the code generation model by reasonably decomposing complex code generation tasks into multiple subtasks and gradually deriving solutions for each subtask. Nevertheless, the existing CoT generation methods mainly concentrate on Python code, and the performance on other programming languages remains unclear. To fill this gap, we first constructed a CoT generation dataset for 12 programming languages through multi-agent technology. On this basis, we proposed a CoT generation method MSCoT applicable to multiple programming languages. By introducing CoT into the code generation large model, the performance of the code generation large model in a multilingual environment can be improved. Through large-scale empirical research, we compared the generalization abilities of MSCoT and the existing CoT generation methods on multiple programming languages and proved the effectiveness of MSCoT for multiple programming languages. In addition, we also designed a human study to prove the quality of the CoT generated by MSCoT. Finally, we opensourced the model and dataset of MSCoT to promote the research on CoT generation for multiple programming languages.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Competition Enhance the Proficiency of Agents Powered by Large Language Models in the Realm of News-driven Time Series Forecasting?</title>
<link>https://arxiv.org/abs/2504.10210</link>
<guid>https://arxiv.org/abs/2504.10210</guid>
<content:encoded><![CDATA[
<div> : 

<br /><br />:
 <div>
arXiv:2504.10210v1 Announce Type: new 
Abstract: Multi-agents-based news-driven time series forecasting is considered as a potential paradigm shift in the era of large language models (LLMs). The challenge of this task lies in measuring the influences of different news events towards the fluctuations of time series. This requires agents to possess stronger abilities of innovative thinking and the identifying misleading logic. However, the existing multi-agent discussion framework has limited enhancement on time series prediction in terms of optimizing these two capabilities. Inspired by the role of competition in fostering innovation, this study embeds a competition mechanism within the multi-agent discussion to enhance agents' capability of generating innovative thoughts. Furthermore, to bolster the model's proficiency in identifying misleading information, we incorporate a fine-tuned small-scale LLM model within the reflective stage, offering auxiliary decision-making support. Experimental results confirm that the competition can boost agents' capacity for innovative thinking, which can significantly improve the performances of time series prediction. Similar to the findings of social science, the intensity of competition within this framework can influence the performances of agents, providing a new perspective for studying LLMs-based multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision based driving agent for race car simulation environments</title>
<link>https://arxiv.org/abs/2504.10266</link>
<guid>https://arxiv.org/abs/2504.10266</guid>
<content:encoded><![CDATA[
<div> Deep Reinforcement Learning ()Proximal Policy Optimization (PPO)

<br />
:
(DRL)Proximal Policy Optimization (PPO) <div>
arXiv:2504.10266v1 Announce Type: new 
Abstract: In recent years, autonomous driving has become a popular field of study. As control at tire grip limit is essential during emergency situations, algorithms developed for racecars are useful for road cars too. This paper examines the use of Deep Reinforcement Learning (DRL) to solve the problem of grip limit driving in a simulated environment. Proximal Policy Optimization (PPO) method is used to train an agent to control the steering wheel and pedals of the vehicle, using only visual inputs to achieve professional human lap times. The paper outlines the formulation of the task of time optimal driving on a race track as a deep reinforcement learning problem, and explains the chosen observations, actions, and reward functions. The results demonstrate human-like learning and driving behavior that utilize maximum tire grip potential.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealHarm: A Collection of Real-World Language Model Application Failures</title>
<link>https://arxiv.org/abs/2504.10277</link>
<guid>https://arxiv.org/abs/2504.10277</guid>
<content:encoded><![CDATA[
<div> RealHarm

:
RealHarmAI<br /><br /> <div>
arXiv:2504.10277v1 Announce Type: new 
Abstract: Language model deployments in consumer-facing applications introduce numerous risks. While existing research on harms and hazards of such applications follows top-down approaches derived from regulatory frameworks and theoretical analyses, empirical evidence of real-world failure modes remains underexplored. In this work, we introduce RealHarm, a dataset of annotated problematic interactions with AI agents built from a systematic review of publicly reported incidents. Analyzing harms, causes, and hazards specifically from the deployer's perspective, we find that reputational damage constitutes the predominant organizational harm, while misinformation emerges as the most common hazard category. We empirically evaluate state-of-the-art guardrails and content moderation systems to probe whether such systems would have prevented the incidents, revealing a significant gap in the protection of AI applications.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing LLM-driven Social Network: The Chirper.ai Case</title>
<link>https://arxiv.org/abs/2504.10286</link>
<guid>https://arxiv.org/abs/2504.10286</guid>
<content:encoded><![CDATA[
<div> : 

<br />
:
LLMsLLMChirper.ai65,000AI770Mastodon117,0001600LLMAILLM <div>
arXiv:2504.10286v1 Announce Type: new 
Abstract: Large language models (LLMs) demonstrate the ability to simulate human decision-making processes, enabling their use as agents in modeling sophisticated social networks, both offline and online. Recent research has explored collective behavioral patterns and structural characteristics of LLM agents within simulated networks. However, empirical comparisons between LLM-driven and human-driven online social networks remain scarce, limiting our understanding of how LLM agents differ from human users. This paper presents a large-scale analysis of Chirper.ai, an X/Twitter-like social network entirely populated by LLM agents, comprising over 65,000 agents and 7.7 million AI-generated posts. For comparison, we collect a parallel dataset from Mastodon, a human-driven decentralized social network, with over 117,000 users and 16 million posts. We examine key differences between LLM agents and humans in posting behaviors, abusive content, and social network structures. Our findings provide critical insights into the evolving landscape of online social network analysis in the AI era, offering a comprehensive profile of LLM agents in social simulations.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Siamese Network with Dual Attention for EEG-Driven Social Learning: Bridging the Human-Robot Gap in Long-Tail Autonomous Driving</title>
<link>https://arxiv.org/abs/2504.10296</link>
<guid>https://arxiv.org/abs/2504.10296</guid>
<content:encoded><![CDATA[
<div> : (BCI)Electroencephalogram (EEG)

<br /><br />:
(BCI) Electroencephalogram (EEG) SiameseEEGBroadman49-80%100%BCI <div>
arXiv:2504.10296v1 Announce Type: new 
Abstract: Robots with wheeled, quadrupedal, or humanoid forms are increasingly integrated into built environments. However, unlike human social learning, they lack a critical pathway for intrinsic cognitive development, namely, learning from human feedback during interaction. To understand human ubiquitous observation, supervision, and shared control in dynamic and uncertain environments, this study presents a brain-computer interface (BCI) framework that enables classification of Electroencephalogram (EEG) signals to detect cognitively demanding and safety-critical events. As a timely and motivating co-robotic engineering application, we simulate a human-in-the-loop scenario to flag risky events in semi-autonomous robotic driving-representative of long-tail cases that pose persistent bottlenecks to the safety performance of smart mobility systems and robotic vehicles. Drawing on recent advances in few-shot learning, we propose a dual-attention Siamese convolutional network paired with Dynamic Time Warping Barycenter Averaging approach to generate robust EEG-encoded signal representations. Inverse source localization reveals activation in Broadman areas 4 and 9, indicating perception-action coupling during task-relevant mental imagery. The model achieves 80% classification accuracy under data-scarce conditions and exhibits a nearly 100% increase in the utility of salient features compared to state-of-the-art methods, as measured through integrated gradient attribution. Beyond performance, this study contributes to our understanding of the cognitive architecture required for BCI agents-particularly the role of attention and memory mechanisms-in categorizing diverse mental states and supporting both inter- and intra-subject adaptation. Overall, this research advances the development of cognitive robotics and socially guided learning for service robots in complex built environments.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ctrl-Z: Controlling AI Agents via Resampling</title>
<link>https://arxiv.org/abs/2504.10374</link>
<guid>https://arxiv.org/abs/2504.10374</guid>
<content:encoded><![CDATA[
<div> : AIBashBench

<br />
:
AI257BashBenchAIAI58%7%5% <div>
arXiv:2504.10374v1 Announce Type: new 
Abstract: Control evaluations measure whether monitoring and security protocols for AI systems prevent intentionally subversive AI models from causing harm. Our work presents the first control evaluation performed in an agent environment. We construct BashBench, a dataset of 257 challenging multi-step system administration tasks, and evaluate whether various safety measures can prevent an adversarially constructed AI agent from covertly downloading and executing malicious code in this environment. This multi-step setting introduces new attack and defense dynamics, which we investigate in order to design novel control protocols that prevent safety failures without hindering the ability of non-malicious agents to perform useful work. We introduce a class of control protocols called resample protocols that dynamically take additional samples of certain actions. We find these protocols significantly improve on existing techniques by selectively blocking the AI agent from executing suspicious code and incriminating the agent by generating additional examples of dangerous behavior. We measure the tradeoff between attack prevention and usefulness; our best protocol combines resampling with analysis of previous steps, reducing the success rate of attacks from 58% to 7% at a 5% cost to the performance of a non-malicious agent.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Silent Self-Stabilizing Ranking: Time Optimal and Space Efficient</title>
<link>https://arxiv.org/abs/2504.10417</link>
<guid>https://arxiv.org/abs/2504.10417</guid>
<content:encoded><![CDATA[
<div> 

:
$n$$\{1,\dots,n\}$$n+O(\log n)$$O(n^2\log n)$$n+O(\log^2 n)$BurmanPODC'21$n+\Omega(n)$ <div>
arXiv:2504.10417v1 Announce Type: new 
Abstract: We present a silent, self-stabilizing ranking protocol for the population protocol model of distributed computing, where agents interact in randomly chosen pairs to solve a common task. We are given $n$ anonymous agents, and the goal is to assign each agent a unique rank in $\{1, \dots, n\}$. Given unique ranks, it is straightforward to select a designated leader. Thus, our protocol is a self-stabilizing leader election protocol as well. Ranking requires at least $n$ states per agent; hence, the goal is to minimize the additional number of states, called overhead states. The core of our protocol is a space-efficient but non-self-stabilizing ranking protocol that requires only $n + O(\log n)$ states. Our protocol stabilizes in $O(n^2\log n)$ interactions w.h.p.\ and in expectation, using $n + O(\log^2 n)$ states in total. Our stabilization time is asymptotically optimal (see Burman et al., PODC'21). In comparison to the currently best known ranking protocol by Burman et al., which requires $n + \Omega(n)$ states, our result exponentially improves the number of overhead states.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealWebAssist: A Benchmark for Long-Horizon Web Assistance with Real-World Users</title>
<link>https://arxiv.org/abs/2504.10445</link>
<guid>https://arxiv.org/abs/2504.10445</guid>
<content:encoded><![CDATA[
<div> RealWebAssistAI

:
AIAIRealWebAssistGUIRealWebAssistGUI <div>
arXiv:2504.10445v1 Announce Type: new 
Abstract: To achieve successful assistance with long-horizon web-based tasks, AI agents must be able to sequentially follow real-world user instructions over a long period. Unlike existing web-based agent benchmarks, sequential instruction following in the real world poses significant challenges beyond performing a single, clearly defined task. For instance, real-world human instructions can be ambiguous, require different levels of AI assistance, and may evolve over time, reflecting changes in the user's mental state. To address this gap, we introduce RealWebAssist, a novel benchmark designed to evaluate sequential instruction-following in realistic scenarios involving long-horizon interactions with the web, visual GUI grounding, and understanding ambiguous real-world user instructions. RealWebAssist includes a dataset of sequential instructions collected from real-world human users. Each user instructs a web-based assistant to perform a series of tasks on multiple websites. A successful agent must reason about the true intent behind each instruction, keep track of the mental state of the user, understand user-specific routines, and ground the intended tasks to actions on the correct GUI elements. Our experimental results show that state-of-the-art models struggle to understand and ground user instructions, posing critical challenges in following real-world user instructions for long-horizon web assistance.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents</title>
<link>https://arxiv.org/abs/2504.10458</link>
<guid>https://arxiv.org/abs/2504.10458</guid>
<content:encoded><![CDATA[
<div> : 

:
\nameGUIRFTWindowsLinuxMacOSAndroidWebGroup Relative Policy OptimizationGRPOOS-Atlas\name0.02%3K13MLVLMGUI <div>
arXiv:2504.10458v1 Announce Type: new 
Abstract: Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \name achieves superior performance using only 0.02\% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Price of Competitive Information Disclosure</title>
<link>https://arxiv.org/abs/2504.10459</link>
<guid>https://arxiv.org/abs/2504.10459</guid>
<content:encoded><![CDATA[
<div> -of-

:
-of--of- <div>
arXiv:2504.10459v1 Announce Type: new 
Abstract: In many decision-making scenarios, individuals strategically choose what information to disclose to optimize their own outcomes. It is unclear whether such strategic information disclosure can lead to good societal outcomes. To address this question, we consider a competitive Bayesian persuasion model in which multiple agents selectively disclose information about their qualities to a principal, who aims to choose the candidates with the highest qualities. Using the price-of-anarchy framework, we quantify the inefficiency of such strategic disclosure. We show that the price of anarchy is at most a constant when the agents have independent quality distributions, even if their utility functions are heterogeneous. This result provides the first theoretical guarantee on the limits of inefficiency in Bayesian persuasion with competitive information disclosure.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-tuning an Large Language Model for Automating Computational Fluid Dynamics Simulations</title>
<link>https://arxiv.org/abs/2504.09602</link>
<guid>https://arxiv.org/abs/2504.09602</guid>
<content:encoded><![CDATA[
<div> : (CFD), (LLM), , (NLP), OpenFOAM

<br /><br />:
(CFD)Qwen2.5-7B-InstructNL2FOAM28716OpenFOAMchain-of-thoughtCFD2188.7%82.6%Qwen2.5-72B-InstructDeepSeek-R1Llama3.3-70B-Instruct <div>
arXiv:2504.09602v1 Announce Type: cross 
Abstract: Configuring computational fluid dynamics (CFD) simulations typically demands extensive domain expertise, limiting broader access. Although large language models (LLMs) have advanced scientific computing, their use in automating CFD workflows is underdeveloped. We introduce a novel approach centered on domain-specific LLM adaptation. By fine-tuning Qwen2.5-7B-Instruct on NL2FOAM, our custom dataset of 28716 natural language-to-OpenFOAM configuration pairs with chain-of-thought (CoT) annotations, we enable direct translation from natural language descriptions to executable CFD setups. A multi-agent framework orchestrates the process, autonomously verifying inputs, generating configurations, running simulations, and correcting errors. Evaluation on a benchmark of 21 diverse flow cases demonstrates state-of-the-art performance, achieving 88.7% solution accuracy and 82.6% first-attempt success rate. This significantly outperforms larger general-purpose models like Qwen2.5-72B-Instruct, DeepSeek-R1, and Llama3.3-70B-Instruct, while also requiring fewer correction iterations and maintaining high computational efficiency. The results highlight the critical role of domain-specific adaptation in deploying LLM assistants for complex engineering workflows.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Sensor Steering Strategy Using Deep Reinforcement Learning for Dynamic Data Acquisition in Digital Twins</title>
<link>https://arxiv.org/abs/2504.10248</link>
<guid>https://arxiv.org/abs/2504.10248</guid>
<content:encoded><![CDATA[
<div> Markov

:
Markov <div>
arXiv:2504.10248v1 Announce Type: cross 
Abstract: This paper introduces a sensor steering methodology based on deep reinforcement learning to enhance the predictive accuracy and decision support capabilities of digital twins by optimising the data acquisition process. Traditional sensor placement techniques are often constrained by one-off optimisation strategies, which limit their applicability for online applications requiring continuous informative data assimilation. The proposed approach addresses this limitation by offering an adaptive framework for sensor placement within the digital twin paradigm. The sensor placement problem is formulated as a Markov decision process, enabling the training and deployment of an agent capable of dynamically repositioning sensors in response to the evolving conditions of the physical structure as represented by the digital twin. This ensures that the digital twin maintains a highly representative and reliable connection to its physical counterpart. The proposed framework is validated through a series of comprehensive case studies involving a cantilever plate structure subjected to diverse conditions, including healthy and damaged conditions. The results demonstrate the capability of the deep reinforcement learning agent to adaptively reposition sensors improving the quality of data acquisition and hence enhancing the overall accuracy of digital twins.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-stabilizing Graph Exploration by a Single Agent</title>
<link>https://arxiv.org/abs/2010.08929</link>
<guid>https://arxiv.org/abs/2010.08929</guid>
<content:encoded><![CDATA[
<div> : 

:
$c = \Omega(n)$$O(m)$$O(\log c)$$O(\log(c+\delta_v))$$n$$m$$\delta_v$$v$$c\geq 2$$O( m \cdot \min(D, \frac{n}{c}+1, \frac{D}{c} + \log n))$$D$$k \geq \max(D, \dmax)$$\dmax$$O(m + nD)$$O(\log k)$ <div>
arXiv:2010.08929v4 Announce Type: replace 
Abstract: In this paper, we present two self-stabilizing algorithms that enable a single (mobile) agent to explore graphs. Starting from any initial configuration, \ie regardless of the initial states of the agent and all nodes, as well as the initial location of the agent, the algorithms ensure the agent visits all nodes. We evaluate the algorithms based on two metrics: the \emph{cover time}, defined as the number of moves required to visit all nodes, and \emph{memory usage}, defined as the storage needed for maintaining the states of the agent and each node. The first algorithm is randomized. Given an integer $c = \Omega(n)$, its cover time is optimal, \ie $O(m)$ in expectation, and its memory requirements are $O(\log c)$ bits for the agent and $O(\log (c+\delta_v))$ bits for each node $v$, where $n$ and $m$ are the numbers of nodes and edges, respectively, and $\delta_v$ is the degree of node $v$. For general $c \ge 2$, its cover time is $O( m \cdot \min(D, \frac{n}{c}+1, \frac{D}{c} + \log n))$, where $D$ is the diameter of a graph. The second algorithm is deterministic. It requires an input integer $k \ge \max(D, \dmax)$, where $\dmax$ is the maximum degree of the graph. The cover time of this algorithm is $O(m + nD)$, and it uses $O(\log k)$ bits of memory for both the agent and each node.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward 6G Native-AI Network: Foundation Model based Cloud-Edge-End Collaboration Framework</title>
<link>https://arxiv.org/abs/2310.17471</link>
<guid>https://arxiv.org/abs/2310.17471</guid>
<content:encoded><![CDATA[
<div> 6G-sum<br /><br />:
6G6GAI6GAIPFMAIMIMOsum6G <div>
arXiv:2310.17471v2 Announce Type: replace 
Abstract: Future wireless communication networks are in a position to move beyond data-centric, device-oriented connectivity and offer intelligent, immersive experiences based on multi-agent collaboration, especially in the context of the thriving development of pre-trained foundation models (PFM) and the evolving vision of 6G native artificial intelligence (AI). Therefore, redefining modes of collaboration between devices and agents, and constructing native intelligence libraries become critically important in 6G. In this paper, we analyze the challenges of achieving 6G native AI from the perspectives of data, AI models, and operational paradigm. Then, we propose a 6G native AI framework based on foundation models, provide an integration method for the expert knowledge, present the customization for two kinds of PFM, and outline a novel operational paradigm for the native AI framework. As a practical use case, we apply this framework for orchestration, achieving the maximum sum rate within a cell-free massive MIMO system, and presenting preliminary evaluation results. Finally, we outline research directions for achieving native AI in 6G.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Secret Collusion among Generative AI Agents: Multi-Agent Deception via Steganography</title>
<link>https://arxiv.org/abs/2402.07510</link>
<guid>https://arxiv.org/abs/2402.07510</guid>
<content:encoded><![CDATA[
<div> : AI

:
LLMsAIAILLMGPT-4AI <div>
arXiv:2402.07510v4 Announce Type: replace 
Abstract: Recent capability increases in large language models (LLMs) open up applications in which groups of communicating generative AI agents solve joint tasks. This poses privacy and security challenges concerning the unauthorised sharing of information, or other unwanted forms of agent coordination. Modern steganographic techniques could render such dynamics hard to detect. In this paper, we comprehensively formalise the problem of secret collusion in systems of generative AI agents by drawing on relevant concepts from both AI and security literature. We study incentives for the use of steganography, and propose a variety of mitigation measures. Our investigations result in a model evaluation framework that systematically tests capabilities required for various forms of secret collusion. We provide extensive empirical results across a range of contemporary LLMs. While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need for continuous monitoring of steganographic frontier model capabilities. We conclude by laying out a comprehensive research program to mitigate future risks of collusion between generative AI models.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLAYER*: Enhancing LLM-based Multi-Agent Communication and Interaction in Murder Mystery Games</title>
<link>https://arxiv.org/abs/2404.17662</link>
<guid>https://arxiv.org/abs/2404.17662</guid>
<content:encoded><![CDATA[
<div> : PLAYER*, , , WellPlay, 

:
PLAYER*LLMMMGsMMGs1,482WellPlay12PLAYER*MMGs <div>
arXiv:2404.17662v4 Announce Type: replace 
Abstract: We present PLAYER*, a novel framework for Large Language Model (LLM)-based agents in Murder Mystery Games (MMGs). MMGs pose unique challenges, including undefined state spaces, absent intermediate rewards, and the need for strategic interaction in a continuous language domain. PLAYER* addresses these complexities through a sensor-based representation of agent states, a question-targeting mechanism guided by information gain, and a pruning strategy to refine suspect lists and enhance decision-making efficiency. To enable systematic evaluation, we propose WellPlay, a dataset comprising 1,482 inferential questions across 12 games, categorized into objectives, reasoning, and relationships. Experiments demonstrate PLAYER*'s capacity to achieve superior performance in reasoning accuracy and efficiency compared to existing approaches, while also significantly improving the quality of agent-human interactions in MMGs. This study advances the development of reasoning agents for complex social and interactive scenarios.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Policy Optimization via $\varepsilon$-Retrain</title>
<link>https://arxiv.org/abs/2406.08315</link>
<guid>https://arxiv.org/abs/2406.08315</guid>
<content:encoded><![CDATA[
<div> -

<br /><br />:
-- <div>
arXiv:2406.08315v2 Announce Type: replace 
Abstract: We present $\varepsilon$-retrain, an exploration strategy encouraging a behavioral preference while optimizing policies with monotonic improvement guarantees. To this end, we introduce an iterative procedure for collecting retrain areas -- parts of the state space where an agent did not satisfy the behavioral preference. Our method switches between the typical uniform restart state distribution and the retrain areas using a decaying factor $\varepsilon$, allowing agents to retrain on situations where they violated the preference. We also employ formal verification of neural networks to provably quantify the degree to which agents adhere to these behavioral preferences. Experiments over hundreds of seeds across locomotion, power network, and navigation tasks show that our method yields agents that exhibit significant performance and sample efficiency improvements.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Detect Verbal Indicators of Romantic Attraction?</title>
<link>https://arxiv.org/abs/2407.10989</link>
<guid>https://arxiv.org/abs/2407.10989</guid>
<content:encoded><![CDATA[
<div> : (AI)ChatGPT

:
964ChatGPT0.12-0.23ChatGPT0.21-0.35BrunswikChatGPTChatGPT <div>
arXiv:2407.10989v2 Announce Type: replace 
Abstract: As artificial intelligence (AI) models become an integral part of everyday life, our interactions with them shift from purely functional exchanges to more relational experiences. For these experiences to be successful, artificial agents need to be able to detect and interpret social cues and interpersonal dynamics; both within and outside of their own human-agent relationships. In this paper, we explore whether AI models can accurately decode one of the arguably most important but complex social signals: romantic attraction. Specifically, we test whether Large Language Models can detect romantic attraction during brief getting-to-know-you interactions between humans. Examining data from 964 speed dates, we show that ChatGPT can predict both objective and subjective indicators of speed dating success (r=0.12-0.23). Although predictive performance remains relatively low, ChatGPT's predictions of actual matching (i.e., the exchange of contact information) were not only on par with those of human judges but incremental to speed daters' own predictions. In addition, ChatGPT's judgments showed substantial overlap with those made by human observers (r=0.21-0.35), highlighting similarities in their representation of romantic attraction that are independent of accuracy. Our findings also offer insights into how ChatGPT arrives at its predictions and the mistakes it makes. Specifically, we use a Brunswik lens approach to identify the linguistic and conversational cues utilized by ChatGPT (and human judges) vis-a-vis those that are predictive of actual matching.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future</title>
<link>https://arxiv.org/abs/2408.02479</link>
<guid>https://arxiv.org/abs/2408.02479</guid>
<content:encoded><![CDATA[
<div> : LLM

<br /><br />:
LLMsLLMsLLMAGILLMsLLMsLLMLLMsLLMLLM <div>
arXiv:2408.02479v2 Announce Type: replace 
Abstract: With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering. LLMs have achieved remarkable success in areas including code generation and vulnerability detection. However, they also exhibit numerous limitations and shortcomings. LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement. Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents. It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain. In this survey, we broadly investigate the current practice and solutions for LLMs and LLM-based agents for software engineering. In particular we summarise six key topics: requirement engineering, code generation, autonomous decision-making, software design, test generation, and software maintenance. We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics. Finally, we discuss the models and benchmarks used, providing a comprehensive analysis of their applications and effectiveness in software engineering. We anticipate this work will shed some lights on pushing the boundaries of LLM-based agents in software engineering for future research.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models</title>
<link>https://arxiv.org/abs/2408.08926</link>
<guid>https://arxiv.org/abs/2408.08926</guid>
<content:encoded><![CDATA[
<div> Cybersecurity, Language Model, Evaluation Framework, Cybench, Capture the Flag (CTF)

<br /><br />:
CybenchCybenchCapture the Flag (CTF)40GPT-4oClaude 3.5 SonnetClaude 3.5 SonnetGPT-4oOpenAI o1-previewClaude 3 Opus112454https://cybench.github.io <div>
arXiv:2408.08926v4 Announce Type: replace 
Abstract: Language Model (LM) agents for cybersecurity that are capable of autonomously identifying vulnerabilities and executing exploits have potential to cause real-world impact. Policymakers, model providers, and researchers in the AI and cybersecurity communities are interested in quantifying the capabilities of such agents to help mitigate cyberrisk and investigate opportunities for penetration testing. Toward that end, we introduce Cybench, a framework for specifying cybersecurity tasks and evaluating agents on those tasks. We include 40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF competitions, chosen to be recent, meaningful, and spanning a wide range of difficulties. Each task includes its own description, starter files, and is initialized in an environment where an agent can execute commands and observe outputs. Since many tasks are beyond the capabilities of existing LM agents, we introduce subtasks for each task, which break down a task into intermediary steps for a more detailed evaluation. To evaluate agent capabilities, we construct a cybersecurity agent and evaluate 8 models: GPT-4o, OpenAI o1-preview, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct, Gemini 1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. For the top performing models (GPT-4o and Claude 3.5 Sonnet), we further investigate performance across 4 agent scaffolds (structed bash, action-only, pseudoterminal, and web search). Without subtask guidance, agents leveraging Claude 3.5 Sonnet, GPT-4o, OpenAI o1-preview, and Claude 3 Opus successfully solved complete tasks that took human teams up to 11 minutes to solve. In comparison, the most difficult task took human teams 24 hours and 54 minutes to solve. All code and data are publicly available at https://cybench.github.io.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Exploration and Discriminative World Model Learning with an Object-Centric Abstraction</title>
<link>https://arxiv.org/abs/2408.11816</link>
<guid>https://arxiv.org/abs/2408.11816</guid>
<content:encoded><![CDATA[
<div> : 

<br />
:



(i) (ii) (iii) 2DMiniHack <div>
arXiv:2408.11816v3 Announce Type: replace 
Abstract: In the face of difficult exploration problems in reinforcement learning, we study whether giving an agent an object-centric mapping (describing a set of items and their attributes) allow for more efficient learning. We found this problem is best solved hierarchically by modelling items at a higher level of state abstraction to pixels, and attribute change at a higher level of temporal abstraction to primitive actions. This abstraction simplifies the transition dynamic by making specific future states easier to predict. We make use of this to propose a fully model-based algorithm that learns a discriminative world model, plans to explore efficiently with only a count-based intrinsic reward, and can subsequently plan to reach any discovered (abstract) states.
  We demonstrate the model's ability to (i) efficiently solve single tasks, (ii) transfer zero-shot and few-shot across item types and environments, and (iii) plan across long horizons. Across a suite of 2D crafting and MiniHack environments, we empirically show our model significantly out-performs state-of-the-art low-level methods (without abstraction), as well as performant model-free and model-based methods using the same abstraction. Finally, we show how to learn low level object-perturbing policies via reinforcement learning, and the object mapping itself by supervised learning.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model-Enhanced Reinforcement Learning for Generic Bus Holding Control Strategies</title>
<link>https://arxiv.org/abs/2410.10212</link>
<guid>https://arxiv.org/abs/2410.10212</guid>
<content:encoded><![CDATA[
<div> : Bus holding control, Reinforcement Learning (RL), Large Language Models (LLMs), Reward generation, Smart mobility

:
LLMsRLLLM-enhanced RLLLMRLRLLLMLLMs <div>
arXiv:2410.10212v2 Announce Type: replace 
Abstract: Bus holding control is a widely-adopted strategy for maintaining stability and improving the operational efficiency of bus systems. Traditional model-based methods often face challenges with the low accuracy of bus state prediction and passenger demand estimation. In contrast, Reinforcement Learning (RL), as a data-driven approach, has demonstrated great potential in formulating bus holding strategies. RL determines the optimal control strategies in order to maximize the cumulative reward, which reflects the overall control goals. However, translating sparse and delayed control goals in real-world tasks into dense and real-time rewards for RL is challenging, normally requiring extensive manual trial-and-error. In view of this, this study introduces an automatic reward generation paradigm by leveraging the in-context learning and reasoning capabilities of Large Language Models (LLMs). This new paradigm, termed the LLM-enhanced RL, comprises several LLM-based modules: reward initializer, reward modifier, performance analyzer, and reward refiner. These modules cooperate to initialize and iteratively improve the reward function according to the feedback from training and test results for the specified RL-based task. Ineffective reward functions generated by the LLM are filtered out to ensure the stable evolution of the RL agents' performance over iterations. To evaluate the feasibility of the proposed LLM-enhanced RL paradigm, it is applied to extensive bus holding control scenarios that vary in the number of bus lines, stops, and passenger demand. The results demonstrate the superiority, generalization capability, and robustness of the proposed paradigm compared to vanilla RL strategies, the LLM-based controller, physics-based feedback controllers, and optimization-based controllers. This study sheds light on the great potential of utilizing LLMs in various smart mobility applications.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Active Imitation Learning with Random Network Distillation</title>
<link>https://arxiv.org/abs/2411.01894</link>
<guid>https://arxiv.org/abs/2411.01894</guid>
<content:encoded><![CDATA[
<div>  DAGGERRND-DAgger

:
 DAGGER (RND-DAgger) RND-DAggerRND-DAgger <div>
arXiv:2411.01894v2 Announce Type: replace 
Abstract: Developing agents for complex and underspecified tasks, where no clear objective exists, remains challenging but offers many opportunities. This is especially true in video games, where simulated players (bots) need to play realistically, and there is no clear reward to evaluate them. While imitation learning has shown promise in such domains, these methods often fail when agents encounter out-of-distribution scenarios during deployment. Expanding the training dataset is a common solution, but it becomes impractical or costly when relying on human demonstrations. This article addresses active imitation learning, aiming to trigger expert intervention only when necessary, reducing the need for constant expert input along training. We introduce Random Network Distillation DAgger (RND-DAgger), a new active imitation learning method that limits expert querying by using a learned state-based out-of-distribution measure to trigger interventions. This approach avoids frequent expert-agent action comparisons, thus making the expert intervene only when it is useful. We evaluate RND-DAgger against traditional imitation learning and other active approaches in 3D video games (racing and third-person navigation) and in a robotic locomotion task and show that RND-DAgger surpasses previous methods by reducing expert queries. https://sites.google.com/view/rnd-dagger
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Problem of Social Cost in Multi-Agent General Reinforcement Learning: Survey and Synthesis</title>
<link>https://arxiv.org/abs/2412.02091</link>
<guid>https://arxiv.org/abs/2412.02091</guid>
<content:encoded><![CDATA[
<div> : AI

:
AIAGIEveritt2018(i) AIXI(ii) Paperclips <div>
arXiv:2412.02091v2 Announce Type: replace 
Abstract: The AI safety literature is full of examples of powerful AI agents that, in blindly pursuing a specific and usually narrow objective, ends up with unacceptable and even catastrophic collateral damage to others. In this paper, we consider the problem of social harms that can result from actions taken by learning and utility-maximising agents in a multi-agent environment. The problem of measuring social harms or impacts in such multi-agent settings, especially when the agents are artificial generally intelligent (AGI) agents, was listed as an open problem in Everitt et al, 2018. We attempt a partial answer to that open problem in the form of market-based mechanisms to quantify and control the cost of such social harms. The proposed setup captures many well-studied special cases and is more general than existing formulations of multi-agent reinforcement learning with mechanism design in two ways: (i) the underlying environment is a history-based general reinforcement learning environment like in AIXI; (ii) the reinforcement-learning agents participating in the environment can have different learning strategies and planning horizons. To demonstrate the practicality of the proposed setup, we survey some key classes of learning algorithms and present a few applications, including a discussion of the Paperclips problem and pollution control with a cap-and-trade system.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigation World Models</title>
<link>https://arxiv.org/abs/2412.03572</link>
<guid>https://arxiv.org/abs/2412.03572</guid>
<content:encoded><![CDATA[
<div> Navigation World Model (NWM)Conditional Diffusion Transformer (CDiT)

:
Navigation World Model (NWM)NWMConditional Diffusion Transformer (CDiT)egocentric10NWMNWMNWM <div>
arXiv:2412.03572v2 Announce Type: replace 
Abstract: Navigation is a fundamental skill of agents with visual-motor capabilities. We introduce a Navigation World Model (NWM), a controllable video generation model that predicts future visual observations based on past observations and navigation actions. To capture complex environment dynamics, NWM employs a Conditional Diffusion Transformer (CDiT), trained on a diverse collection of egocentric videos of both human and robotic agents, and scaled up to 1 billion parameters. In familiar environments, NWM can plan navigation trajectories by simulating them and evaluating whether they achieve the desired goal. Unlike supervised navigation policies with fixed behavior, NWM can dynamically incorporate constraints during planning. Experiments demonstrate its effectiveness in planning trajectories from scratch or by ranking trajectories sampled from an external policy. Furthermore, NWM leverages its learned visual priors to imagine trajectories in unfamiliar environments from a single input image, making it a flexible and powerful tool for next-generation navigation systems.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentBreeder: Mitigating the AI Safety Impact of Multi-Agent Scaffolds via Self-Improvement</title>
<link>https://arxiv.org/abs/2502.00757</link>
<guid>https://arxiv.org/abs/2502.00757</guid>
<content:encoded><![CDATA[
<div> : , , , AgentBreeder, 

<br /><br />:
AgentBreederLLMsAgentBreeder79.4%https://github.com/J-Rosser-UK/AgentBreeder  <div>
arXiv:2502.00757v2 Announce Type: replace 
Abstract: Scaffolding Large Language Models (LLMs) into multi-agent systems often improves performance on complex tasks, but the safety impact of such scaffolds has not been thoroughly explored. We introduce AgentBreeder, a framework for multi-objective self-improving evolutionary search over scaffolds. We evaluate discovered scaffolds on widely recognized reasoning, mathematics, and safety benchmarks and compare them with popular baselines. In 'blue' mode, we see a 79.4% average uplift in safety benchmark performance while maintaining or improving capability scores. In 'red' mode, we find adversarially weak scaffolds emerging concurrently with capability optimization. Our work demonstrates the risks of multi-agent scaffolding and provides a framework for mitigating them. Code is available at https://github.com/J-Rosser-UK/AgentBreeder.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Efficient Flying LoRa Gateways: A Multi-Agent Reinforcement Learning Approach</title>
<link>https://arxiv.org/abs/2502.03377</link>
<guid>https://arxiv.org/abs/2502.03377</guid>
<content:encoded><![CDATA[
<div> (NG-IoT)LoRa(UAV)(MARL)

:
(NG-IoT)IoTLoRa(UAV)LoRaLoRa(POMDP)LoRa GW(MARL)(MAPPO)MARL<br /><br /> <div>
arXiv:2502.03377v2 Announce Type: replace 
Abstract: As next-generation Internet of Things (NG-IoT) networks continue to grow, the number of connected devices is rapidly increasing, along with their energy demands. This creates challenges for resource management and sustainability. Energy-efficient communication, particularly for power-limited IoT devices, is therefore a key research focus. In this paper, we deployed flying LoRa gateways mounted on unmanned aerial vehicles (UAVs) to collect data from LoRa end devices and transmit it to a central server. Our primary objective is to maximize the global system energy efficiency of wireless LoRa networks by joint optimization of transmission power, spreading factor, bandwidth, and user association. To solve this challenging problem, we model the problem as a partially observable Markov decision process (POMDP), where each flying LoRa GW acts as a learning agent using a cooperative multi-agent reinforcement learning (MARL). Simulation results demonstrate that our proposed method, based on the multi-agent proximal policy optimization (MAPPO) algorithm, significantly improves the global system EE and surpasses the conventional MARL schemes.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A-MEM: Agentic Memory for LLM Agents</title>
<link>https://arxiv.org/abs/2502.12110</link>
<guid>https://arxiv.org/abs/2502.12110</guid>
<content:encoded><![CDATA[
<div> :  (LLM)Zettelkasten

<br /><br />:
LLMZettelkastenZettelkastenSOTAGitHub <div>
arXiv:2502.12110v4 Announce Type: replace 
Abstract: While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems' fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution - as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code for evaluating performance is available at https://github.com/WujiangXu/AgenticMemory, while the source code of agentic memory system is available at https://github.com/agiresearch/A-mem.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.12486</link>
<guid>https://arxiv.org/abs/2502.12486</guid>
<content:encoded><![CDATA[
<div> :  (LLMs) (EPO) (RL)

<br /><br />:
EPOEPO EPO EPO  EPO  <div>
arXiv:2502.12486v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown impressive reasoning capabilities in well-defined problems with clear solutions, such as mathematics and coding. However, they still struggle with complex real-world scenarios like business negotiations, which require strategic reasoning-an ability to navigate dynamic environments and align long-term goals amidst uncertainty. Existing methods for strategic reasoning face challenges in adaptability, scalability, and transferring strategies to new contexts. To address these issues, we propose explicit policy optimization (EPO) for strategic reasoning, featuring an LLM that provides strategies in open-ended action space and can be plugged into arbitrary LLM agents to motivate goal-directed behavior. To improve adaptability and policy transferability, we train the strategic reasoning model via multi-turn reinforcement learning (RL) using process rewards and iterative self-play, without supervised fine-tuning (SFT) as a preliminary step. Experiments across social and physical domains demonstrate EPO's ability of long-term goal alignment through enhanced strategic reasoning, achieving state-of-the-art performance on social dialogue and web navigation tasks. Our findings reveal various collaborative reasoning mechanisms emergent in EPO and its effectiveness in generating novel strategies, underscoring its potential for strategic reasoning in real-world applications.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Competitive Posted-Price Mechanism for Online Budget-Feasible Auctions</title>
<link>https://arxiv.org/abs/2502.18265</link>
<guid>https://arxiv.org/abs/2502.18265</guid>
<content:encoded><![CDATA[
<div> : 

:
(Badanidiyuru, KleinbergSinger, EC 2012)OPTOPTOPT <div>
arXiv:2502.18265v2 Announce Type: replace 
Abstract: We consider online procurement auctions, where the agents arrive sequentially, in random order, and have private costs for their services. The buyer aims to maximize a monotone submodular value function for the subset of agents whose services are procured, subject to a budget constraint on their payments. We consider a posted-price setting where upon each agent's arrival, the buyer decides on a payment offered to them. The agent accepts or rejects the offer, depending on whether the payment exceeds their cost, without revealing any other information about their private costs whatsoever. We present a randomized online posted-price mechanism with constant competitive ratio, thus resolving the main open question of (Badanidiyuru, Kleinberg and Singer, EC 2012). Posted-price mechanisms for online procurement typically operate by learning an estimation of the optimal value, denoted as OPT, and using it to determine the payments offered to the agents. The main challenge is to learn OPT within a constant factor from the agents' accept / reject responses to the payments offered. Our approach is based on an online test of whether our estimation is too low compared against OPT and a carefully designed adaptive search that gradually refines our estimation.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Personalized Tool-Augmented LLMs from the Perspectives of Personalization and Proactivity</title>
<link>https://arxiv.org/abs/2503.00771</link>
<guid>https://arxiv.org/abs/2503.00771</guid>
<content:encoded><![CDATA[
<div> ETAPP

:
ETAPP800LLMLLMLLMLLMLLMhttps://github.com/hypasd-art/ETAPP  <div>
arXiv:2503.00771v2 Announce Type: replace 
Abstract: Personalized tool utilization is essential for aligning large language models (LLMs) with user preference in interaction scenarios with various tools. However, most of the current benchmarks primarily focus on either personalization of text generation or direct tool-utilizing, without considering both. In this work, we introduce a novel benchmark ETAPP for evaluating personalized tool invocation, establishing a sandbox environment, and a comprehensive dataset of 800 testing cases covering diverse user profiles. To improve the accuracy of our evaluation, we propose a key-point-based LLM evaluation method, mitigating biases in the LLM-as-a-judge system by manually annotating key points for each test case and providing them to LLM as the reference. Additionally, we evaluate the excellent LLMs and provide an in-depth analysis. Furthermore, we investigate the impact of different tool-invoking strategies on LLMs' personalization performance and the effects of fine-tuning in our task. The effectiveness of our preference-setting and key-point-based evaluation method is also validated. Our findings offer insights into improving personalized LLM agents. Our Code is available at https://github.com/hypasd-art/ETAPP.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AppAgentX: Evolving GUI Agents as Proficient Smartphone Users</title>
<link>https://arxiv.org/abs/2503.02268</link>
<guid>https://arxiv.org/abs/2503.02268</guid>
<content:encoded><![CDATA[
<div> Large Language Models (LLMs), (GUIs), , , 

:
LLMsGUI <div>
arXiv:2503.02268v2 Announce Type: replace 
Abstract: Recent advancements in Large Language Models (LLMs) have led to the development of intelligent LLM-based agents capable of interacting with graphical user interfaces (GUIs). These agents demonstrate strong reasoning and adaptability, enabling them to perform complex tasks that traditionally required predefined rules. However, the reliance on step-by-step reasoning in LLM-based agents often results in inefficiencies, particularly for routine tasks. In contrast, traditional rule-based systems excel in efficiency but lack the intelligence and flexibility to adapt to novel scenarios. To address this challenge, we propose a novel evolutionary framework for GUI agents that enhances operational efficiency while retaining intelligence and flexibility. Our approach incorporates a memory mechanism that records the agent's task execution history. By analyzing this history, the agent identifies repetitive action sequences and evolves high-level actions that act as shortcuts, replacing these low-level operations and improving efficiency. This allows the agent to focus on tasks requiring more complex reasoning, while simplifying routine actions. Experimental results on multiple benchmark tasks demonstrate that our approach significantly outperforms existing methods in both efficiency and accuracy. The code will be open-sourced to support further research.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Majority voting is not good for heaven or hell, with mirrored performance</title>
<link>https://arxiv.org/abs/2401.00592</link>
<guid>https://arxiv.org/abs/2401.00592</guid>
<content:encoded><![CDATA[
<div> ViSE

:
<br />
ViSEVoting in Stochastic Environment$X$$\mu>0$$-X$$\mu$ <div>
arXiv:2401.00592v4 Announce Type: replace-cross 
Abstract: Within the ViSE (Voting in Stochastic Environment) model, we study the effectiveness of majority voting in various environments. By the pit of losses paradox identified in previous work, majority decisions in apparently hostile environments tend to reduce the capital of society. In such cases, the simple social decision rule of "rejecting all proposals without voting" outperforms majority voting. In this paper, we identify another pit of losses appearing in favorable environments. Here, the simple social decision rule of "accepting all proposals without voting" is superior to majority voting. We prove that under a version of simple majority called symmetrized majority and the antisymmetry of the voting body, the second pit of losses is a mirror image of the pit of losses in hostile environments and explain this phenomenon. Technically, we consider a voting society consisting of individualists whose strategy is supporting all proposals that increase their capital and a group (groups) whose members vote to increase the wealth of their group. According to the main result, the expected capital gain of each agent in the environment whose generator $X$ has mean $\mu>0$ exceeds by $\mu$ their expected capital gain under generator $-X$. This result extends to location families of generators with distributions symmetric about their mean. The mentioned result determines the symmetry of the difference between the expected capital gain under the symmetrized majority and that under the "basic" social decision rule that rejects (resp. accepts) all proposals in unfavorable (resp. favorable) environments.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Oscillatory and Excitable Dynamics in an Opinion Model with Group Opinions</title>
<link>https://arxiv.org/abs/2408.13336</link>
<guid>https://arxiv.org/abs/2408.13336</guid>
<content:encoded><![CDATA[
<div> 

:
 <div>
arXiv:2408.13336v2 Announce Type: replace-cross 
Abstract: In traditional models of opinion dynamics, each agent in a network has an opinion and changes in opinions arise from pairwise (i.e., dyadic) interactions between agents. However, in many situations, groups of individuals possess a collective opinion that can differ from the opinions of its constituent individuals. In this paper, we study the effects of group opinions on opinion dynamics. We formulate a hypergraph model in which both individual agents and groups of 3 agents have opinions, and we examine how opinions evolve through both dyadic interactions and group memberships. In some parameter regimes, we find that the presence of group opinions can lead to oscillatory and excitable opinion dynamics. In the oscillatory regime, the mean opinion of the agents in a network has self-sustained oscillations. In the excitable regime, finite-size effects create large but short-lived opinion swings (as in social fads). We develop a mean-field approximation of our model and obtain good agreement with direct numerical simulations. We also show -- both numerically and via our mean-field description -- that oscillatory dynamics occur only when the number of dyadic and polyadic interactions per agent are not completely correlated. Our results illustrate how polyadic structures, such as groups of agents, can have important effects on collective opinion dynamics.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consensus in Multiagent Systems under communication failure</title>
<link>https://arxiv.org/abs/2410.10486</link>
<guid>https://arxiv.org/abs/2410.10486</guid>
<content:encoded><![CDATA[
<div> : 

:
[t, +)Moreau <div>
arXiv:2410.10486v2 Announce Type: replace-cross 
Abstract: We consider multi-agent systems with cooperative interactions and study the convergence to consensus in the case of time-dependent connections, with possible communication failure.
  We prove a new condition ensuring consensus: we define a graph in which directed arrows correspond to connection functions that converge (in the weak sense) to some function with a positive integral on all intervals of the form $[t,+\infty)$. If the graph has a node reachable from all other indices, i.e.~``globally reachable'', then the system converges to consensus. We show that this requirement generalizes some known sufficient conditions for convergence, such as Moreau's or the Persistent Excitation one. We also give a second new condition, transversal to the known ones: total connectedness of the undirected graph formed by the non-vanishing of limiting functions.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScreenSpot-Pro: GUI Grounding for Professional High-Resolution Computer Use</title>
<link>https://arxiv.org/abs/2504.07981</link>
<guid>https://arxiv.org/abs/2504.07981</guid>
<content:encoded><![CDATA[
<div> Multi-modal Large Language Models (MLLMs)GUIScreenSpot-ProScreenSeekeR

:
MLLMsGUIScreenSpot-ProMLLMs23GUI18.9%ScreenSeekeRGUI48.1%GUIhttps://gui-agent.github.io/grounding-leaderboard <div>
arXiv:2504.07981v1 Announce Type: new 
Abstract: Recent advancements in Multi-modal Large Language Models (MLLMs) have led to significant progress in developing GUI agents for general tasks such as web browsing and mobile phone use. However, their application in professional domains remains under-explored. These specialized workflows introduce unique challenges for GUI perception models, including high-resolution displays, smaller target sizes, and complex environments. In this paper, we introduce ScreenSpot-Pro, a new benchmark designed to rigorously evaluate the grounding capabilities of MLLMs in high-resolution professional settings. The benchmark comprises authentic high-resolution images from a variety of professional domains with expert annotations. It spans 23 applications across five industries and three operating systems. Existing GUI grounding models perform poorly on this dataset, with the best model achieving only 18.9%. Our experiments reveal that strategically reducing the search area enhances accuracy. Based on this insight, we propose ScreenSeekeR, a visual search method that utilizes the GUI knowledge of a strong planner to guide a cascaded search, achieving state-of-the-art performance with 48.1% without any additional training. We hope that our benchmark and findings will advance the development of GUI agents for professional applications. Code, data and leaderboard can be found at https://gui-agent.github.io/grounding-leaderboard.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuron-level Balance between Stability and Plasticity in Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.08000</link>
<guid>https://arxiv.org/abs/2504.08000</guid>
<content:encoded><![CDATA[
<div> : -NBSP

:<br />
Neuron-level Balance between Stability and PlasticityNBSPDRL-NBSPRLNBSPMeta-WorldAtari <div>
arXiv:2504.08000v1 Announce Type: new 
Abstract: In contrast to the human ability to continuously acquire knowledge, agents struggle with the stability-plasticity dilemma in deep reinforcement learning (DRL), which refers to the trade-off between retaining existing skills (stability) and learning new knowledge (plasticity). Current methods focus on balancing these two aspects at the network level, lacking sufficient differentiation and fine-grained control of individual neurons. To overcome this limitation, we propose Neuron-level Balance between Stability and Plasticity (NBSP) method, by taking inspiration from the observation that specific neurons are strongly relevant to task-relevant skills. Specifically, NBSP first (1) defines and identifies RL skill neurons that are crucial for knowledge retention through a goal-oriented method, and then (2) introduces a framework by employing gradient masking and experience replay techniques targeting these neurons to preserve the encoded existing skills while enabling adaptation to new tasks. Numerous experimental results on the Meta-World and Atari benchmarks demonstrate that NBSP significantly outperforms existing approaches in balancing stability and plasticity.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search</title>
<link>https://arxiv.org/abs/2504.08066</link>
<guid>https://arxiv.org/abs/2504.08066</guid>
<content:encoded><![CDATA[
<div> : AI-v2

:
AI-v2v1AI-v2-VLMAIAI-v2ICLRAIAIhttps://github.com/SakanaAI/AI-Scientist-v2AIAI <div>
arXiv:2504.08066v1 Announce Type: new 
Abstract: AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test Amplification for REST APIs via Single and Multi-Agent LLM Systems</title>
<link>https://arxiv.org/abs/2504.08113</link>
<guid>https://arxiv.org/abs/2504.08113</guid>
<content:encoded><![CDATA[
<div> REST APIsAPI

:
RESTful APIAPIAPILLMREST APIs <div>
arXiv:2504.08113v1 Announce Type: new 
Abstract: REST APIs (Representational State Transfer Application Programming Interfaces) are essential to modern cloud-native applications. Strong and automated test cases are crucial to expose lurking bugs in the API. However, creating automated tests for REST APIs is difficult, and it requires test cases that explore the protocol's boundary conditions. In this paper, we investigate how single-agent and multi-agent LLM (Large Language Model) systems can amplify a REST API test suite. Our evaluation demonstrates increased API coverage, identification of numerous bugs in the API under test, and insights into the computational cost and energy consumption of both approaches.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Reinforcement Learning-based Sustainable Multi-User Computation Offloading for Mobile Edge-Quantum Computing</title>
<link>https://arxiv.org/abs/2504.08134</link>
<guid>https://arxiv.org/abs/2504.08134</guid>
<content:encoded><![CDATA[
<div> : (MEQC), -, , , , 

:
(MEQC)MEQC--MEQC30% <div>
arXiv:2504.08134v1 Announce Type: new 
Abstract: Exploiting quantum computing at the mobile edge holds immense potential for facilitating large-scale network design, processing multimodal data, optimizing resource management, and enhancing network security. In this paper, we propose a pioneering paradigm of mobile edge quantum computing (MEQC) that integrates quantum computing capabilities into classical edge computing servers that are proximate to mobile devices. To conceptualize the MEQC, we first design an MEQC system, where mobile devices can offload classical and quantum computation tasks to edge servers equipped with classical and quantum computers. We then formulate the hybrid classical-quantum computation offloading problem whose goal is to minimize system cost in terms of latency and energy consumption. To solve the offloading problem efficiently, we propose a hybrid discrete-continuous multi-agent reinforcement learning algorithm to learn long-term sustainable offloading and partitioning strategies. Finally, numerical results demonstrate that the proposed algorithm can reduce the MEQC system cost by up to 30% compared to existing baselines.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Orchestrating Agents and Data for Enterprise: A Blueprint Architecture for Compound AI</title>
<link>https://arxiv.org/abs/2504.08148</link>
<guid>https://arxiv.org/abs/2504.08148</guid>
<content:encoded><![CDATA[
<div> : , AI, , , 

:
AIAPIAI <div>
arXiv:2504.08148v1 Announce Type: new 
Abstract: Large language models (LLMs) have gained significant interest in industry due to their impressive capabilities across a wide range of tasks. However, the widespread adoption of LLMs presents several challenges, such as integration into existing applications and infrastructure, utilization of company proprietary data, models, and APIs, and meeting cost, quality, responsiveness, and other requirements. To address these challenges, there is a notable shift from monolithic models to compound AI systems, with the premise of more powerful, versatile, and reliable applications. However, progress thus far has been piecemeal, with proposals for agentic workflows, programming models, and extended LLM capabilities, without a clear vision of an overall architecture. In this paper, we propose a 'blueprint architecture' for compound AI systems for orchestrating agents and data for enterprise applications. In our proposed architecture the key orchestration concept is 'streams' to coordinate the flow of data and instructions among agents. Existing proprietary models and APIs in the enterprise are mapped to 'agents', defined in an 'agent registry' that serves agent metadata and learned representations for search and planning. Agents can utilize proprietary data through a 'data registry' that similarly registers enterprise data of various modalities. Tying it all together, data and task 'planners' break down, map, and optimize tasks and queries for given quality of service (QoS) requirements such as cost, accuracy, and latency. We illustrate an implementation of the architecture for a use-case in the HR domain and discuss opportunities and challenges for 'agentic AI' in the enterprise.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Foundations for Continual Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.08161</link>
<guid>https://arxiv.org/abs/2504.08161</guid>
<content:encoded><![CDATA[
<div> Markov

<br /><br />:
RLMarkovEpisodic <div>
arXiv:2504.08161v1 Announce Type: new 
Abstract: Algorithms and approaches for continual reinforcement learning have gained increasing attention. Much of this early progress rests on the foundations and standard practices of traditional reinforcement learning, without questioning if they are well-suited to the challenges of continual learning agents. We suggest that many core foundations of traditional RL are, in fact, antithetical to the goals of continual reinforcement learning. We enumerate four such foundations: the Markov decision process formalism, a focus on optimal policies, the expected sum of rewards as the primary evaluation metric, and episodic benchmark environments that embrace the other three foundations. Shedding such sacredly held and taught concepts is not easy. They are self-reinforcing in that each foundation depends upon and holds up the others, making it hard to rethink each in isolation. We propose an alternative set of all four foundations that are better suited to the continual learning setting. We hope to spur on others in rethinking the traditional foundations, proposing and critiquing alternatives, and developing new algorithms and approaches enabled by better-suited foundations.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Based Deep Reinforcement Learning Aided by Transformers for Multi-Agent Cooperation</title>
<link>https://arxiv.org/abs/2504.08195</link>
<guid>https://arxiv.org/abs/2504.08195</guid>
<content:encoded><![CDATA[
<div> 

:

GNNDRLGNNQDouble DQNPSODQN90%100%200600 <div>
arXiv:2504.08195v1 Announce Type: new 
Abstract: Mission planning for a fleet of cooperative autonomous drones in applications that involve serving distributed target points, such as disaster response, environmental monitoring, and surveillance, is challenging, especially under partial observability, limited communication range, and uncertain environments. Traditional path-planning algorithms struggle in these scenarios, particularly when prior information is not available. To address these challenges, we propose a novel framework that integrates Graph Neural Networks (GNNs), Deep Reinforcement Learning (DRL), and transformer-based mechanisms for enhanced multi-agent coordination and collective task execution. Our approach leverages GNNs to model agent-agent and agent-goal interactions through adaptive graph construction, enabling efficient information aggregation and decision-making under constrained communication. A transformer-based message-passing mechanism, augmented with edge-feature-enhanced attention, captures complex interaction patterns, while a Double Deep Q-Network (Double DQN) with prioritized experience replay optimizes agent policies in partially observable environments. This integration is carefully designed to address specific requirements of multi-agent navigation, such as scalability, adaptability, and efficient task execution. Experimental results demonstrate superior performance, with 90% service provisioning and 100% grid coverage (node discovery), while reducing the average steps per episode to 200, compared to 600 for benchmark methods such as particle swarm optimization (PSO), greedy algorithms and DQN.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in Healthcare</title>
<link>https://arxiv.org/abs/2504.08260</link>
<guid>https://arxiv.org/abs/2504.08260</guid>
<content:encoded><![CDATA[
<div> : 

:
Understanding America Study(UAS)LLMLLMLlama 3UAS <div>
arXiv:2504.08260v1 Announce Type: new 
Abstract: Generative agents have been increasingly used to simulate human behaviour in silico, driven by large language models (LLMs). These simulacra serve as sandboxes for studying human behaviour without compromising privacy or safety. However, it remains unclear whether such agents can truly represent real individuals. This work compares survey data from the Understanding America Study (UAS) on healthcare decision-making with simulated responses from generative agents. Using demographic-based prompt engineering, we create digital twins of survey respondents and analyse how well different LLMs reproduce real-world behaviours. Our findings show that some LLMs fail to reflect realistic decision-making, such as predicting universal vaccine acceptance. However, Llama 3 captures variations across race and Income more accurately but also introduces biases not present in the UAS data. This study highlights the potential of generative agents for behavioural research while underscoring the risks of bias from both LLMs and prompting strategies.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Stabilizing Weakly Byzantine Perpetual Gathering of Mobile Agents</title>
<link>https://arxiv.org/abs/2504.08271</link>
<guid>https://arxiv.org/abs/2504.08271</guid>
<content:encoded><![CDATA[
<div> Byzantine

:
$k$$f$Byzantine$n$

(1)(2)$k\leq 2f$$n$$k$$f$ID$\Lambda_g$$\Lambda_g$$n$$k$$f$$N$$K$$F$$O(K\cdot F\cdot \Lambda_g\cdot X(N))$$X(n)$$n$$k$$f$ <div>
arXiv:2504.08271v1 Announce Type: new 
Abstract: We study the \emph{Byzantine} gathering problem involving $k$ mobile agents with unique identifiers (IDs), $f$ of which are Byzantine. These agents start the execution of a common algorithm from (possibly different) nodes in an $n$-node network, potentially starting at different times. Once started, the agents operate in synchronous rounds. We focus on \emph{weakly} Byzantine environments, where Byzantine agents can behave arbitrarily but cannot falsify their IDs. The goal is for all \emph{non-Byzantine} agents to eventually terminate at a single node simultaneously.
  In this paper, we first prove two impossibility results: (1) for any number of non-Byzantine agents, no algorithm can solve this problem without global knowledge of the network size or the number of agents, and (2) no self-stabilizing algorithm exists if $k\leq 2f$ even with $n$, $k$, $f$, and the length $\Lambda_g$ of the largest ID among IDs of non-Byzantine agents, where the self-stabilizing algorithm enables agents to gather starting from arbitrary (inconsistent) initial states. Next, based on these results, we introduce a \emph{perpetual gathering} problem and propose a self-stabilizing algorithm for this problem. This problem requires that all non-Byzantine agents always be co-located from a certain time onwards. If the agents know $\Lambda_g$ and upper bounds $N$, $K$, $F$ on $n$, $k$, $f$, the proposed algorithm works in $O(K\cdot F\cdot \Lambda_g\cdot X(N))$ rounds, where $X(n)$ is the time required to visit all nodes in a $n$-nodes network. Our results indicate that while no algorithm can solve the original self-stabilizing gathering problem for any $k$ and $f$ even with \emph{exact} global knowledge of the network size and the number of agents, the self-stabilizing perpetual gathering problem can always be solved with just upper bounds on this knowledge.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSM: Building A Diverse Semantic Map for 3D Visual Grounding</title>
<link>https://arxiv.org/abs/2504.08307</link>
<guid>https://arxiv.org/abs/2504.08307</guid>
<content:encoded><![CDATA[
<div> (VLMs)3D(DSM)

<br /><br />:

VLMs3D3D(DSM)VLMsDSMDSMDSM-Grounding3D <div>
arXiv:2504.08307v1 Announce Type: new 
Abstract: In recent years, with the growing research and application of multimodal large language models (VLMs) in robotics, there has been an increasing trend of utilizing VLMs for robotic scene understanding tasks. Existing approaches that use VLMs for 3D Visual Grounding tasks often focus on obtaining scene information through geometric and visual information, overlooking the extraction of diverse semantic information from the scene and the understanding of rich implicit semantic attributes, such as appearance, physics, and affordance. The 3D scene graph, which combines geometry and language, is an ideal representation method for environmental perception and is an effective carrier for language models in 3D Visual Grounding tasks. To address these issues, we propose a diverse semantic map construction method specifically designed for robotic agents performing 3D Visual Grounding tasks. This method leverages VLMs to capture the latent semantic attributes and relations of objects within the scene and creates a Diverse Semantic Map (DSM) through a geometry sliding-window map construction strategy. We enhance the understanding of grounding information based on DSM and introduce a novel approach named DSM-Grounding. Experimental results show that our method outperforms current approaches in tasks like semantic segmentation and 3D Visual Grounding, particularly excelling in overall metrics compared to the state-of-the-art. In addition, we have deployed this method on robots to validate its effectiveness in navigation and grasping tasks.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PCA-RAG: Principal Component Analysis for Efficient Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2504.08386</link>
<guid>https://arxiv.org/abs/2504.08386</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation (RAG)Principal Component Analysis (PCA)embedding dimensionality reductioncomputational bottlenecksfinancial text corpora

:
(PCA)(RAG)PCA3,0721106028.6PCAZanista AINewswitch <div>
arXiv:2504.08386v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for grounding large language models in external knowledge sources, improving the precision of agents responses. However, high-dimensional language model embeddings, often in the range of hundreds to thousands of dimensions, can present scalability challenges in terms of storage and latency, especially when processing massive financial text corpora. This paper investigates the use of Principal Component Analysis (PCA) to reduce embedding dimensionality, thereby mitigating computational bottlenecks without incurring large accuracy losses. We experiment with a real-world dataset and compare different similarity and distance metrics under both full-dimensional and PCA-compressed embeddings. Our results show that reducing vectors from 3,072 to 110 dimensions provides a sizeable (up to $60\times$) speedup in retrieval operations and a $\sim 28.6\times$ reduction in index size, with only moderate declines in correlation metrics relative to human-annotated similarity scores. These findings demonstrate that PCA-based compression offers a viable balance between retrieval fidelity and resource efficiency, essential for real-time systems such as Zanista AI's \textit{Newswitch} platform. Ultimately, our study underscores the practicality of leveraging classical dimensionality reduction techniques to scale RAG architectures for knowledge-intensive applications in finance and trading, where speed, memory efficiency, and accuracy must jointly be optimized.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MineWorld: a Real-Time and Open-Source Interactive World Model on Minecraft</title>
<link>https://arxiv.org/abs/2504.08388</link>
<guid>https://arxiv.org/abs/2504.08388</guid>
<content:encoded><![CDATA[
<div> : World modeling, Minecraft, interactive world model, Transformer, real-time interaction

<br /><br />:
MineWorldMinecraftMineWorld-TransformerID47MineWorld <div>
arXiv:2504.08388v1 Announce Type: new 
Abstract: World modeling is a crucial task for enabling intelligent agents to effectively interact with humans and operate in dynamic environments. In this work, we propose MineWorld, a real-time interactive world model on Minecraft, an open-ended sandbox game which has been utilized as a common testbed for world modeling. MineWorld is driven by a visual-action autoregressive Transformer, which takes paired game scenes and corresponding actions as input, and generates consequent new scenes following the actions. Specifically, by transforming visual game scenes and actions into discrete token ids with an image tokenizer and an action tokenizer correspondingly, we consist the model input with the concatenation of the two kinds of ids interleaved. The model is then trained with next token prediction to learn rich representations of game states as well as the conditions between states and actions simultaneously. In inference, we develop a novel parallel decoding algorithm that predicts the spatial redundant tokens in each frame at the same time, letting models in different scales generate $4$ to $7$ frames per second and enabling real-time interactions with game players. In evaluation, we propose new metrics to assess not only visual quality but also the action following capacity when generating new scenes, which is crucial for a world model. Our comprehensive evaluation shows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion based world models significantly. The code and model have been released.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models</title>
<link>https://arxiv.org/abs/2504.08399</link>
<guid>https://arxiv.org/abs/2504.08399</guid>
<content:encoded><![CDATA[
<div> Big Five

<br />
: LLMsLLMLLMLLMs5-7LLM <div>
arXiv:2504.08399v1 Announce Type: new 
Abstract: There is a growing interest in assessing the personality traits of Large language models (LLMs). However, traditional personality assessments based on self-report questionnaires may fail to capture their true behavioral nuances due to inherent biases and meta-knowledge contamination. This paper introduces a novel multi-observer framework for LLM personality assessment that draws inspiration from informant-report methods in psychology. Instead of relying solely on self-assessments, our approach employs multiple observer agents configured with a specific relationship context (e.g., family, friend, or workplace) to simulate interactive scenarios with a subject LLM. These observers engage in dialogues and subsequently provide ratings across the Big Five personality dimensions. Our experiments reveal that LLMs possess systematic biases in self-report personality ratings. Moreover, aggregating observer ratings effectively reduces non-systematic biases and achieves optimal reliability with 5-7 observers. The findings highlight the significant impact of relationship context on personality perception and demonstrate that a multi-observer paradigm yields a more robust and context-sensitive evaluation of LLM personality traits.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Belief States for Cooperative Multi-Agent Reinforcement Learning under Partial Observability</title>
<link>https://arxiv.org/abs/2504.08417</link>
<guid>https://arxiv.org/abs/2504.08417</guid>
<content:encoded><![CDATA[
<div> 

:
 <div>
arXiv:2504.08417v1 Announce Type: new 
Abstract: Reinforcement learning in partially observable environments is typically challenging, as it requires agents to learn an estimate of the underlying system state. These challenges are exacerbated in multi-agent settings, where agents learn simultaneously and influence the underlying state as well as each others' observations. We propose the use of learned beliefs on the underlying state of the system to overcome these challenges and enable reinforcement learning with fully decentralized training and execution. Our approach leverages state information to pre-train a probabilistic belief model in a self-supervised fashion. The resulting belief states, which capture both inferred state information as well as uncertainty over this information, are then used in a state-based reinforcement learning algorithm to create an end-to-end model for cooperative multi-agent reinforcement learning under partial observability. By separating the belief and reinforcement learning tasks, we are able to significantly simplify the policy and value function learning tasks and improve both the convergence speed and the final performance. We evaluate our proposed method on diverse partially observable multi-agent tasks designed to exhibit different variants of partial observability.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid ABM-PDE Framework for Real-World Infectious Disease Simulations</title>
<link>https://arxiv.org/abs/2504.08430</link>
<guid>https://arxiv.org/abs/2504.08430</guid>
<content:encoded><![CDATA[
<div> hybrid modeling approachAgent-Based Model (ABM)partial differential equation (PDE)

:
Agent-Based Model (ABM)(PDE)ABM-PDEABM25%100%ABMPDEPDE- <div>
arXiv:2504.08430v1 Announce Type: new 
Abstract: This paper presents a hybrid modeling approach that couples an Agent-Based Model (ABM) with a partial differential equation (PDE) model in an epidemic setting to simulate the spatial spread of infectious diseases using a compartmental structure with seven health states. The goal is to reduce the computational complexity of a full-ABM by introducing a coupled ABM-PDE model that offers significantly faster simulations while maintaining comparable accuracy. Our results demonstrate that the hybrid model not only reduces the overall simulation runtime (defined as the number of runs required for stable results multiplied by the duration of a single run) but also achieves smaller errors across both 25% and 100% population samples. The coupling mechanism ensures consistency at the model interface: agents crossing from the ABM into the PDE domain are removed and represented as density contributions at the corresponding grid node, while surplus density in the PDE domain is used to generate agents with plausible trajectories derived from mobile phone data. We evaluate the hybrid model using real-world mobility and infection data for the Berlin-Brandenburg region in Germany, showing that it captures the core epidemiological dynamics while enabling efficient large-scale simulations.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speech Command + Speech Emotion: Exploring Emotional Speech Commands as a Compound and Playful Modality</title>
<link>https://arxiv.org/abs/2504.08440</link>
<guid>https://arxiv.org/abs/2504.08440</guid>
<content:encoded><![CDATA[
<div> : 

:
AIAIN=14 <div>
arXiv:2504.08440v1 Announce Type: new 
Abstract: In an era of human-computer interaction with increasingly agentic AI systems capable of connecting with users conversationally, speech is an important modality for commanding agents. By recognizing and using speech emotions (i.e., how a command is spoken), we can provide agents with the ability to emotionally accentuate their responses and socially enrich users' perceptions and experiences. To explore the concept and impact of speech emotion commands on user perceptions, we realized a prototype and conducted a user study (N = 14) where speech commands are used to steer two vehicles in a minimalist and retro game style implementation. While both agents execute user commands, only one of the agents uses speech emotion information to adapt its execution behavior. We report on differences in how users perceived each agent, including significant differences in stimulation and dependability, outline implications for designing interactions with agents using emotional speech commands, and provide insights on how users consciously emote, which we describe as "voice acting".
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Path Connected Dynamic Graphs with a Study of Dispersion and Exploration</title>
<link>https://arxiv.org/abs/2504.08474</link>
<guid>https://arxiv.org/abs/2504.08474</guid>
<content:encoded><![CDATA[
<div> $T$-$T$-dispersion

<br /><br />:
$T$-$T$$T$-$T$-dispersion1-dispersiondispersiondispersion <div>
arXiv:2504.08474v1 Announce Type: new 
Abstract: In dynamic graphs, edges may be added or deleted in each synchronous round. Various connectivity models exist based on constraints on these changes. One well-known model is the $T$-Interval Connectivity model, where the graph remains connected in every round, and the parameter $T$ reflects the duration of structural stability. Another model is Connectivity Time, where the union of edges across any $T$ consecutive rounds forms a connected graph. This is a weaker model, as the graph may be disconnected in individual rounds.
  In this work, we introduce a new connectivity model called $T$-Path Connectivity. Unlike $T$-Interval Connectivity, the graph may not be connected in each round, but for every pair of nodes $u,v$, there must exist a path connecting them in at least one round within any $T$ consecutive rounds. This model is strictly weaker than $T$-Interval Connectivity but stronger than the Connectivity Time model.
  We study the dispersion problem in the $T$-Path Connectivity model. While dispersion has been explored in the 1-Interval Connectivity model, we show that the existing algorithm with termination does not work in our model. We then identify the minimal necessary assumptions required to solve dispersion in the $T$-Path Connectivity model and provide an algorithm that solves it optimally under those conditions. Moreover, we prove that dispersion is unsolvable in the Connectivity Time model, even under several strong assumptions.
  We further initiate the study of the exploration problem under all three connectivity models. We present multiple impossibility results and, in most cases, establish tight bounds on the number of agents and time required. Our results demonstrate that, in both dispersion and exploration, the Connectivity Time model is strictly the weakest among the three.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adopting Large Language Models to Automated System Integration</title>
<link>https://arxiv.org/abs/2504.08490</link>
<guid>https://arxiv.org/abs/2504.08490</guid>
<content:encoded><![CDATA[
<div> : WebCompositio Prompto

:
LLMs(1)LLMsCompositio Prompto(2)RAG(3)(4)Compositio PromptoRAGLLM <div>
arXiv:2504.08490v1 Announce Type: new 
Abstract: Modern enterprise computing systems integrate numerous subsystems to resolve a common task by yielding emergent behavior. A widespread approach is using services implemented with Web technologies like REST or OpenAPI, which offer an interaction mechanism and service documentation standard, respectively. Each service represents a specific business functionality, allowing encapsulation and easier maintenance. Despite the reduced maintenance costs on an individual service level, increased integration complexity arises. Consequently, automated service composition approaches have arisen to mitigate this issue. Nevertheless, these approaches have not achieved high acceptance in practice due to their reliance on complex formal modeling. Within this Ph.D. thesis, we analyze the application of Large Language Models (LLMs) to automatically integrate the services based on a natural language input. The result is a reusable service composition, e.g., as program code. While not always generating entirely correct results, the result can still be helpful by providing integration engineers with a close approximation of a suitable solution, which requires little effort to become operational. Our research involves (i) introducing a software architecture for automated service composition using LLMs, (ii) analyzing Retrieval Augmented Generation (RAG) for service discovery, (iii) proposing a novel natural language query-based benchmark for service discovery, and (iv) extending the benchmark to complete service composition scenarios. We have presented our software architecture as Compositio Prompto, the analysis of RAG for service discovery, and submitted a proposal for the service discovery benchmark. Open topics are primarily the extension of the service discovery benchmark to service composition scenarios and the improvements of the service composition generation, e.g., using fine-tuning or LLM agents.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM Agent Tasks</title>
<link>https://arxiv.org/abs/2504.08525</link>
<guid>https://arxiv.org/abs/2504.08525</guid>
<content:encoded><![CDATA[
<div> :  (LLMs) (TME) (TMT)

<br /><br />:

TMELLMsTMETMTLLMTMETMEhttps://github.com/biubiutomato/TME-Agent <div>
arXiv:2504.08525v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used as autonomous agents for multi-step tasks. However, most existing frameworks fail to maintain a structured understanding of the task state, often relying on linear prompt concatenation or shallow memory buffers. This leads to brittle performance, frequent hallucinations, and poor long-range coherence. In this work, we propose the Task Memory Engine (TME), a lightweight and structured memory module that tracks task execution using a hierarchical Task Memory Tree (TMT). Each node in the tree corresponds to a task step, storing relevant input, output, status, and sub-task relationships. We introduce a prompt synthesis method that dynamically generates LLM prompts based on the active node path, significantly improving execution consistency and contextual grounding. Through case studies and comparative experiments on multi-step agent tasks, we demonstrate that TME leads to better task completion accuracy and more interpretable behavior with minimal implementation overhead. The full implementation of TME is available at https://github.com/biubiutomato/TME-Agent.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied Image Captioning: Self-supervised Learning Agents for Spatially Coherent Image Descriptions</title>
<link>https://arxiv.org/abs/2504.08531</link>
<guid>https://arxiv.org/abs/2504.08531</guid>
<content:encoded><![CDATA[
<div> : 

:
-https://hsp-iit.github.io/embodied-captioning/ <div>
arXiv:2504.08531v1 Announce Type: new 
Abstract: We present a self-supervised method to improve an agent's abilities in describing arbitrary objects while actively exploring a generic environment. This is a challenging problem, as current models struggle to obtain coherent image captions due to different camera viewpoints and clutter. We propose a three-phase framework to fine-tune existing captioning models that enhances caption accuracy and consistency across views via a consensus mechanism. First, an agent explores the environment, collecting noisy image-caption pairs. Then, a consistent pseudo-caption for each object instance is distilled via consensus using a large language model. Finally, these pseudo-captions are used to fine-tune an off-the-shelf captioning model, with the addition of contrastive learning. We analyse the performance of the combination of captioning models, exploration policies, pseudo-labeling methods, and fine-tuning strategies, on our manually labeled test set. Results show that a policy can be trained to mine samples with higher disagreement compared to classical baselines. Our pseudo-captioning method, in combination with all policies, has a higher semantic similarity compared to other existing methods, and fine-tuning improves caption accuracy and consistency by a significant margin. Code and test set annotations available at https://hsp-iit.github.io/embodied-captioning/
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FMLGS: Fast Multilevel Language Embedded Gaussians for Part-level Interactive Agents</title>
<link>https://arxiv.org/abs/2504.08581</link>
<guid>https://arxiv.org/abs/2504.08581</guid>
<content:encoded><![CDATA[
<div> 3DFMLGS

<br />
:

FMLGS3D3D3DGSSegment Anything Model 2SAM2FMLGSFMLGSLERF98LangSplat4LEGaussians2.5FMLGS3D <div>
arXiv:2504.08581v1 Announce Type: new 
Abstract: The semantically interactive radiance field has long been a promising backbone for 3D real-world applications, such as embodied AI to achieve scene understanding and manipulation. However, multi-granularity interaction remains a challenging task due to the ambiguity of language and degraded quality when it comes to queries upon object components. In this work, we present FMLGS, an approach that supports part-level open-vocabulary query within 3D Gaussian Splatting (3DGS). We propose an efficient pipeline for building and querying consistent object- and part-level semantics based on Segment Anything Model 2 (SAM2). We designed a semantic deviation strategy to solve the problem of language ambiguity among object parts, which interpolates the semantic features of fine-grained targets for enriched information. Once trained, we can query both objects and their describable parts using natural language. Comparisons with other state-of-the-art methods prove that our method can not only better locate specified part-level targets, but also achieve first-place performance concerning both speed and accuracy, where FMLGS is 98 x faster than LERF, 4 x faster than LangSplat and 2.5 x faster than LEGaussians. Meanwhile, we further integrate FMLGS as a virtual agent that can interactively navigate through 3D scenes, locate targets, and respond to user demands through a chat interface, which demonstrates the potential of our work to be further expanded and applied in the future.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MooseAgent: A LLM Based Multi-agent Framework for Automating Moose Simulation</title>
<link>https://arxiv.org/abs/2504.08621</link>
<guid>https://arxiv.org/abs/2504.08621</guid>
<content:encoded><![CDATA[
<div> : (FEM), MOOSE, (LLMs), , MooseAgent

:
MooseAgentMOOSEMooseAgentMOOSEMOOSEMooseAgentMOOSEMOOSEMooseAgenthttps://github.com/taozhan18/MooseAgent <div>
arXiv:2504.08621v1 Announce Type: new 
Abstract: The Finite Element Method (FEM) is widely used in engineering and scientific computing, but its pre-processing, solver configuration, and post-processing stages are often time-consuming and require specialized knowledge. This paper proposes an automated solution framework, MooseAgent, for the multi-physics simulation framework MOOSE, which combines large-scale pre-trained language models (LLMs) with a multi-agent system. The framework uses LLMs to understand user-described simulation requirements in natural language and employs task decomposition and multi-round iterative verification strategies to automatically generate MOOSE input files. To improve accuracy and reduce model hallucinations, the system builds and utilizes a vector database containing annotated MOOSE input cards and function documentation. We conducted experimental evaluations on several typical cases, including heat transfer, mechanics, phase field, and multi-physics coupling. The results show that MooseAgent can automate the MOOSE simulation process to a certain extent, especially demonstrating a high success rate when dealing with relatively simple single-physics problems. The main contribution of this research is the proposal of a multi-agent automated framework for MOOSE, which validates its potential in simplifying finite element simulation processes and lowering the user barrier, providing new ideas for the development of intelligent finite element simulation software. The code for the MooseAgent framework proposed in this paper has been open-sourced and is available at https://github.com/taozhan18/MooseAgent
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLMs trust AI regulation? Emerging behaviour of game-theoretic LLM agents</title>
<link>https://arxiv.org/abs/2504.08640</link>
<guid>https://arxiv.org/abs/2504.08640</guid>
<content:encoded><![CDATA[
<div> : 

:
LLMAILLMAIAILLMAILLM <div>
arXiv:2504.08640v1 Announce Type: new 
Abstract: There is general agreement that fostering trust and cooperation within the AI development ecosystem is essential to promote the adoption of trustworthy AI systems. By embedding Large Language Model (LLM) agents within an evolutionary game-theoretic framework, this paper investigates the complex interplay between AI developers, regulators and users, modelling their strategic choices under different regulatory scenarios. Evolutionary game theory (EGT) is used to quantitatively model the dilemmas faced by each actor, and LLMs provide additional degrees of complexity and nuances and enable repeated games and incorporation of personality traits. Our research identifies emerging behaviours of strategic AI agents, which tend to adopt more "pessimistic" (not trusting and defective) stances than pure game-theoretic agents. We observe that, in case of full trust by users, incentives are effective to promote effective regulation; however, conditional trust may deteriorate the "social pact". Establishing a virtuous feedback between users' trust and regulators' reputation thus appears to be key to nudge developers towards creating safe AI. However, the level at which this trust emerges may depend on the specific LLM used for testing. Our results thus provide guidance for AI regulation systems, and help predict the outcome of strategic LLM agents, should they be used to aid regulation itself.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voice Interaction With Conversational AI Could Facilitate Thoughtful Reflection and Substantive Revision in Writing</title>
<link>https://arxiv.org/abs/2504.08687</link>
<guid>https://arxiv.org/abs/2504.08687</guid>
<content:encoded><![CDATA[
<div> : arXiv, , , , 

:
LLMs session  LLM  LLM  <div>
arXiv:2504.08687v1 Announce Type: new 
Abstract: Writing well requires not only expressing ideas but also refining them through revision, a process facilitated by reflection. Prior research suggests that feedback delivered through dialogues, such as those in writing center tutoring sessions, can help writers reflect more thoughtfully on their work compared to static feedback. Recent advancements in multi-modal large language models (LLMs) now offer new possibilities for supporting interactive and expressive voice-based reflection in writing. In particular, we propose that LLM-generated static feedback can be repurposed as conversation starters, allowing writers to seek clarification, request examples, and ask follow-up questions, thereby fostering deeper reflection on their writing. We argue that voice-based interaction can naturally facilitate this conversational exchange, encouraging writers' engagement with higher-order concerns, facilitating iterative refinement of their reflections, and reduce cognitive load compared to text-based interactions. To investigate these effects, we propose a formative study exploring how text vs. voice input influence writers' reflection and subsequent revisions. Findings from this study will inform the design of intelligent and interactive writing tools, offering insights into how voice-based interactions with LLM-powered conversational agents can support reflection and revision.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TP-RAG: Benchmarking Retrieval-Augmented Large Language Model Agents for Spatiotemporal-Aware Travel Planning</title>
<link>https://arxiv.org/abs/2504.08694</link>
<guid>https://arxiv.org/abs/2504.08694</guid>
<content:encoded><![CDATA[
<div> : , , , TP-RAG, EvoRAG

:
TP-RAGTP-RAGEvoRAGWebLLM <div>
arXiv:2504.08694v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown promise in automating travel planning, yet they often fall short in addressing nuanced spatiotemporal rationality. While existing benchmarks focus on basic plan validity, they neglect critical aspects such as route efficiency, POI appeal, and real-time adaptability. This paper introduces TP-RAG, the first benchmark tailored for retrieval-augmented, spatiotemporal-aware travel planning. Our dataset includes 2,348 real-world travel queries, 85,575 fine-grain annotated POIs, and 18,784 high-quality travel trajectory references sourced from online tourist documents, enabling dynamic and context-aware planning. Through extensive experiments, we reveal that integrating reference trajectories significantly improves spatial efficiency and POI rationality of the travel plan, while challenges persist in universality and robustness due to conflicting references and noisy data. To address these issues, we propose EvoRAG, an evolutionary framework that potently synergizes diverse retrieved trajectories with LLMs' intrinsic reasoning. EvoRAG achieves state-of-the-art performance, improving spatiotemporal compliance and reducing commonsense violation compared to ground-up and retrieval-augmented baselines. Our work underscores the potential of hybridizing Web knowledge with LLM-driven optimization, paving the way for more reliable and adaptive travel planning agents.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeaView: Software Engineering Agent Visual Interface for Enhanced Workflow</title>
<link>https://arxiv.org/abs/2504.08696</link>
<guid>https://arxiv.org/abs/2504.08696</guid>
<content:encoded><![CDATA[
<div> Auto-regressive LLMSWESeaView

:
<br />
LLMSWEGitHubSWESeaViewSWESeaViewLLMLLMSeaView1030301 <div>
arXiv:2504.08696v1 Announce Type: new 
Abstract: Auto-regressive LLM-based software engineering (SWE) agents, henceforth SWE agents, have made tremendous progress (>60% on SWE-Bench Verified) on real-world coding challenges including GitHub issue resolution. SWE agents use a combination of reasoning, environment interaction and self-reflection to resolve issues thereby generating "trajectories". Analysis of SWE agent trajectories is difficult, not only as they exceed LLM sequence length (sometimes, greater than 128k) but also because it involves a relatively prolonged interaction between an LLM and the environment managed by the agent. In case of an agent error, it can be hard to decipher, locate and understand its scope. Similarly, it can be hard to track improvements or regression over multiple runs or experiments. While a lot of research has gone into making these SWE agents reach state-of-the-art, much less focus has been put into creating tools to help analyze and visualize agent output. We propose a novel tool called SeaView: Software Engineering Agent Visual Interface for Enhanced Workflow, with a vision to assist SWE-agent researchers to visualize and inspect their experiments. SeaView's novel mechanisms help compare experimental runs with varying hyper-parameters or LLMs, and quickly get an understanding of LLM or environment related problems. Based on our user study, experienced researchers spend between 10 and 30 minutes to gather the information provided by SeaView, while researchers with little experience can spend between 30 minutes to 1 hour to diagnose their experiment.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-PolyBench: A multi-language benchmark for repository level evaluation of coding agents</title>
<link>https://arxiv.org/abs/2504.08703</link>
<guid>https://arxiv.org/abs/2504.08703</guid>
<content:encoded><![CDATA[
<div> : SWE-PolyBench<br /><br />:
SWE-PolyBenchSWE-PolyBench212110JavaJavaScriptTypeScriptPythonSWE-PolyBench500SWE-PolyBenchSWE-PolyBenchGitHubhttps://github.com/amazon-science/SWE-PolyBench <div>
arXiv:2504.08703v1 Announce Type: new 
Abstract: Coding agents powered by large language models have shown impressive capabilities in software engineering tasks, but evaluating their performance across diverse programming languages and real-world scenarios remains challenging. We introduce SWE-PolyBench, a new multi-language benchmark for repository-level, execution-based evaluation of coding agents. SWE-PolyBench contains 2110 instances from 21 repositories and includes tasks in Java (165), JavaScript (1017), TypeScript (729) and Python (199), covering bug fixes, feature additions, and code refactoring. We provide a task and repository-stratified subsample (SWE-PolyBench500) and release an evaluation harness allowing for fully automated evaluation. To enable a more comprehensive comparison of coding agents, this work also presents a novel set of metrics rooted in syntax tree analysis. We evaluate leading open source coding agents on SWE-PolyBench, revealing their strengths and limitations across languages, task types, and complexity classes. Our experiments show that current agents exhibit uneven performances across languages and struggle with complex problems while showing higher performance on simpler tasks. SWE-PolyBench aims to drive progress in developing more versatile and robust AI coding assistants for real-world software engineering. Our datasets and code are available at: https://github.com/amazon-science/SWE-PolyBench
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DocAgent: A Multi-Agent System for Automated Code Documentation Generation</title>
<link>https://arxiv.org/abs/2504.08725</link>
<guid>https://arxiv.org/abs/2504.08725</guid>
<content:encoded><![CDATA[
<div> DocAgent

:
DocAgentReaderSearcherWriterVerifierOrchestratorAIDocAgentDocAgentDocAgent <div>
arXiv:2504.08725v1 Announce Type: new 
Abstract: High-quality code documentation is crucial for software development especially in the era of AI. However, generating it automatically using Large Language Models (LLMs) remains challenging, as existing approaches often produce incomplete, unhelpful, or factually incorrect outputs. We introduce DocAgent, a novel multi-agent collaborative system using topological code processing for incremental context building. Specialized agents (Reader, Searcher, Writer, Verifier, Orchestrator) then collaboratively generate documentation. We also propose a multi-faceted evaluation framework assessing Completeness, Helpfulness, and Truthfulness. Comprehensive experiments show DocAgent significantly outperforms baselines consistently. Our ablation study confirms the vital role of the topological processing order. DocAgent offers a robust approach for reliable code documentation generation in complex and proprietary repositories.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Multi-Agent Reinforcement Learning in Malm\"O (MARL\"O) Competition</title>
<link>https://arxiv.org/abs/1901.08129</link>
<guid>https://arxiv.org/abs/1901.08129</guid>
<content:encoded><![CDATA[
<div> : 

:
arXiv:1901.08129v2Malm\"OMARL\"O<br /><br /> <div>
arXiv:1901.08129v2 Announce Type: replace 
Abstract: Learning in multi-agent scenarios is a fruitful research direction, but current approaches still show scalability problems in multiple games with general reward settings and different opponent types. The Multi-Agent Reinforcement Learning in Malm\"O (MARL\"O) competition is a new challenge that proposes research in this domain using multiple 3D games. The goal of this contest is to foster research in general agents that can learn across different games and opponent types, proposing a challenge as a milestone in the direction of Artificial General Intelligence.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Polytime Algorithms for One-to-Many Matching Games</title>
<link>https://arxiv.org/abs/2107.07440</link>
<guid>https://arxiv.org/abs/2107.07440</guid>
<content:encoded><![CDATA[
<div> Matching games, , , , 

<br /><br />:

Garrido-LuceroLaraki <div>
arXiv:2107.07440v3 Announce Type: replace 
Abstract: Matching games is a novel matching model introduced by Garrido-Lucero and Laraki, in which agents' utilities are endogenously determined as the outcome of a strategic game they play simultaneously with the matching process. Matching games encompass most one-to-one matching market models and reinforce the classical notion of pairwise stability by analyzing their robustness to unilateral deviations within games. In this article, we extend the model to the one-to-many setting, where hospitals can be matched to multiple doctors, and their utility is given by the sum of their game outcomes. We adapt the deferred acceptance with competitions algorithm and the renegotiation process to this new framework and prove that both are polynomial whenever couples play bi-matrix games in mixed strategies.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning</title>
<link>https://arxiv.org/abs/2401.10727</link>
<guid>https://arxiv.org/abs/2401.10727</guid>
<content:encoded><![CDATA[
<div> MLLM-Tool

<br /><br />:

MLLM-ToolLLMsMLLM-ToolGitHub <div>
arXiv:2401.10727v3 Announce Type: replace 
Abstract: Recently, the astonishing performance of large language models (LLMs) in natural language comprehension and generation tasks triggered lots of exploration of using them as central controllers to build agent systems. Multiple studies focus on bridging the LLMs to external tools to extend the application scenarios. However, the current LLMs' ability to perceive tool use is limited to a single text query, which may result in ambiguity in understanding the users' real intentions. LLMs are expected to eliminate that by perceiving the information in the visual- or auditory-grounded instructions. Therefore, in this paper, we propose MLLM-Tool, a system incorporating open-source LLMs and multi-modal encoders so that the learned LLMs can be conscious of multi-modal input instruction and then select the function-matched tool correctly. To facilitate the evaluation of the model's capability, we collect a dataset featuring multi-modal input tools from HuggingFace. Another essential feature of our dataset is that it also contains multiple potential choices for the same instruction due to the existence of identical functions and synonymous functions, which provides more potential solutions for the same query. The experiments reveal that our MLLM-Tool is capable of recommending appropriate tools for multi-modal instructions. Codes and data are available at https://github.com/MLLM-Tool/MLLM-Tool.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoSQA+: Pioneering the Multi-Choice Code Search Benchmark with Test-Driven Agents</title>
<link>https://arxiv.org/abs/2406.11589</link>
<guid>https://arxiv.org/abs/2406.11589</guid>
<content:encoded><![CDATA[
<div> : CoSQA+, , , , 

<br /><br />:
CoSQA+CoSQA+92.0%PythonCoSQA+CoSQACoSQA+ <div>
arXiv:2406.11589v5 Announce Type: replace 
Abstract: Semantic code search, retrieving code that matches a given natural language query, is an important task to improve productivity in software engineering. Existing code search datasets face limitations: they rely on human annotators who assess code primarily through semantic understanding rather than functional verification, leading to potential inaccuracies and scalability issues. Additionally, current evaluation metrics often overlook the multi-choice nature of code search. This paper introduces CoSQA+, pairing high-quality queries from CoSQA with multiple suitable codes. We develop an automated pipeline featuring multiple model-based candidate selections and the novel test-driven agent annotation system. Among a single Large Language Model (LLM) annotator and Python expert annotators (without test-based verification), agents leverage test-based verification and achieve the highest accuracy of 92.0%. Through extensive experiments, CoSQA+ has demonstrated superior quality over CoSQA. Models trained on CoSQA+ exhibit improved performance. We provide the code and data at https://github.com/DeepSoftwareAnalytics/CoSQA_Plus.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSBench: How Far Are Data Science Agents from Becoming Data Science Experts?</title>
<link>https://arxiv.org/abs/2409.07703</link>
<guid>https://arxiv.org/abs/2409.07703</guid>
<content:encoded><![CDATA[
<div> : -DSBench

:
DSBenchDSBench46674EloquenceKaggleLLMsLVLMs34.12%34.74%RPG <div>
arXiv:2409.07703v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have demonstrated impressive language/vision reasoning abilities, igniting the recent trend of building agents for targeted applications such as shopping assistants or AI software engineers. Recently, many data science benchmarks have been proposed to investigate their performance in the data science domain. However, existing data science benchmarks still fall short when compared to real-world data science applications due to their simplified settings. To bridge this gap, we introduce DSBench, a comprehensive benchmark designed to evaluate data science agents with realistic tasks. This benchmark includes 466 data analysis tasks and 74 data modeling tasks, sourced from Eloquence and Kaggle competitions. DSBench offers a realistic setting by encompassing long contexts, multimodal task backgrounds, reasoning with large data files and multi-table structures, and performing end-to-end data modeling tasks. Our evaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle with most tasks, with the best agent solving only 34.12% of data analysis tasks and achieving a 34.74% Relative Performance Gap (RPG). These findings underscore the need for further advancements in developing more practical, intelligent, and autonomous data science agents.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maintaining Strong r-Robustness in Reconfigurable Multi-Robot Networks using Control Barrier Functions</title>
<link>https://arxiv.org/abs/2409.14675</link>
<guid>https://arxiv.org/abs/2409.14675</guid>
<content:encoded><![CDATA[
<div> : arXivr- Barrier (CBF)

:
r-Control Barrier Function (CBF)CBF<br /><br /> <div>
arXiv:2409.14675v3 Announce Type: replace 
Abstract: In leader-follower consensus, strong r-robustness of the communication graph provides a sufficient condition for followers to achieve consensus in the presence of misbehaving agents. Previous studies have assumed that robots can form and/or switch between predetermined network topologies with known robustness properties. However, robots with distance-based communication models may not be able to achieve these topologies while moving through spatially constrained environments, such as narrow corridors, to complete their objectives. This paper introduces a Control Barrier Function (CBF) that ensures robots maintain strong r-robustness of their communication graph above a certain threshold without maintaining any fixed topologies. Our CBF directly addresses robustness, allowing robots to have flexible reconfigurable network structure while navigating to achieve their objectives. The efficacy of our method is tested through various simulation and hardware experiments.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Construction of the Sparsest Maximally r-Robust Graphs</title>
<link>https://arxiv.org/abs/2409.19465</link>
<guid>https://arxiv.org/abs/2409.19465</guid>
<content:encoded><![CDATA[
<div> r-

:
r-r-(1) (2) (1)<br /><br /> <div>
arXiv:2409.19465v3 Announce Type: replace 
Abstract: In recent years, the notion of r-robustness for the communication graph of the network has been introduced to address the challenge of achieving consensus in the presence of misbehaving agents. Higher r-robustness typically implies higher tolerance to malicious information towards achieving resilient consensus, but it also implies more edges for the communication graph. This in turn conflicts with the need to minimize communication due to limited resources in real-world applications (e.g., multi-robot networks). In this paper, our contributions are twofold. (a) We provide the necessary subgraph structures and tight lower bounds on the number of edges required for graphs with a given number of nodes to achieve maximum robustness. (b) We then use the results of (a) to introduce two classes of graphs that maintain maximum robustness with the least number of edges. Our work is validated through a series of simulations.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planning and Learning in Risk-Aware Restless Multi-Arm Bandit Problem</title>
<link>https://arxiv.org/abs/2410.23029</link>
<guid>https://arxiv.org/abs/2410.23029</guid>
<content:encoded><![CDATA[
<div> : restless multi-arm bandits, risk-awareness, Whittle index, Thompson sampling, regret

:
 episode <br /><br /> <div>
arXiv:2410.23029v2 Announce Type: replace 
Abstract: In restless multi-arm bandits, a central agent is tasked with optimally distributing limited resources across several bandits (arms), with each arm being a Markov decision process. In this work, we generalize the traditional restless multi-arm bandit problem with a risk-neutral objective by incorporating risk-awareness. We establish indexability conditions for the case of a risk-aware objective and provide a solution based on Whittle index. In addition, we address the learning problem when the true transition probabilities are unknown by proposing a Thompson sampling approach and show that it achieves bounded regret that scales sublinearly with the number of episodes and quadratically with the number of arms. The efficacy of our method in reducing risk exposure in restless multi-arm bandits is illustrated through a set of numerical experiments in the contexts of machine replacement and patient scheduling applications under both planning and learning setups.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MrSteve: Instruction-Following Agents in Minecraft with What-Where-When Memory</title>
<link>https://arxiv.org/abs/2411.06736</link>
<guid>https://arxiv.org/abs/2411.06736</guid>
<content:encoded><![CDATA[
<div> : arXiv:2411.06736v5, AI, Minecraft, , 

:
MinecraftLLMMrStevePlace Event MemoryPEMSteve-1PEMhttps://sites.google.com/view/mr-steve <div>
arXiv:2411.06736v5 Announce Type: replace 
Abstract: Significant advances have been made in developing general-purpose embodied AI in environments like Minecraft through the adoption of LLM-augmented hierarchical approaches. While these approaches, which combine high-level planners with low-level controllers, show promise, low-level controllers frequently become performance bottlenecks due to repeated failures. In this paper, we argue that the primary cause of failure in many low-level controllers is the absence of an episodic memory system. To address this, we introduce MrSteve (Memory Recall Steve), a novel low-level controller equipped with Place Event Memory (PEM), a form of episodic memory that captures what, where, and when information from episodes. This directly addresses the main limitation of the popular low-level controller, Steve-1. Unlike previous models that rely on short-term memory, PEM organizes spatial and event-based data, enabling efficient recall and navigation in long-horizon tasks. Additionally, we propose an Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing agents to alternate between exploration and task-solving based on recalled events. Our approach significantly improves task-solving and exploration efficiency compared to existing methods. We will release our code and demos on the project page: https://sites.google.com/view/mr-steve.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Subspaces of Policies for Continual Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2412.14865</link>
<guid>https://arxiv.org/abs/2412.14865</guid>
<content:encoded><![CDATA[
<div> Continual Reinforcement LearningHiSPO

:
Continual Reinforcement LearningHiSPOMuJoCoHiSPO <div>
arXiv:2412.14865v3 Announce Type: replace 
Abstract: We consider a Continual Reinforcement Learning setup, where a learning agent must continuously adapt to new tasks while retaining previously acquired skill sets, with a focus on the challenge of avoiding forgetting past gathered knowledge and ensuring scalability with the growing number of tasks. Such issues prevail in autonomous robotics and video game simulations, notably for navigation tasks prone to topological or kinematic changes. To address these issues, we introduce HiSPO, a novel hierarchical framework designed specifically for continual learning in navigation settings from offline data. Our method leverages distinct policy subspaces of neural networks to enable flexible and efficient adaptation to new tasks while preserving existing knowledge. We demonstrate, through a careful experimental study, the effectiveness of our method in both classical MuJoCo maze environments and complex video game-like navigation simulations, showcasing competitive performances and satisfying adaptability with respect to classical continual learning metrics, in particular regarding the memory usage and efficiency.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logarithmic Regret for Nonlinear Control</title>
<link>https://arxiv.org/abs/2501.10261</link>
<guid>https://arxiv.org/abs/2501.10261</guid>
<content:encoded><![CDATA[
<div> 

:
 <div>
arXiv:2501.10261v2 Announce Type: replace 
Abstract: We address the problem of learning to control an unknown nonlinear dynamical system through sequential interactions. Motivated by high-stakes applications in which mistakes can be catastrophic, such as robotics and healthcare, we study situations where it is possible for fast sequential learning to occur. Fast sequential learning is characterized by the ability of the learning agent to incur logarithmic regret relative to a fully-informed baseline. We demonstrate that fast sequential learning is achievable in a diverse class of continuous control problems where the system dynamics depend smoothly on unknown parameters, provided the optimal control policy is persistently exciting. Additionally, we derive a regret bound which grows with the square root of the number of interactions for cases where the optimal policy is not persistently exciting. Our results provide the first regret bounds for controlling nonlinear dynamical systems depending nonlinearly on unknown parameters. We validate the trends our theory predicts in simulation on a simple dynamical system.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents</title>
<link>https://arxiv.org/abs/2501.11858</link>
<guid>https://arxiv.org/abs/2501.11858</guid>
<content:encoded><![CDATA[
<div> Multimodal Large Language Models (MLLMs)EmbodiedEval

<br /><br />:
EmbodiedEval(MLLMs)MLLMsEmbodiedEval328125EmbodiedEvalMLLMsMLLMshttps://github.com/thunlp/EmbodiedEval <div>
arXiv:2501.11858v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) have shown significant advancements, providing a promising future for embodied agents. Existing benchmarks for evaluating MLLMs primarily utilize static images or videos, limiting assessments to non-interactive scenarios. Meanwhile, existing embodied AI benchmarks are task-specific and not diverse enough, which do not adequately evaluate the embodied capabilities of MLLMs. To address this, we propose EmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs with embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied 3D scenes, each of which is rigorously selected and annotated. It covers a broad spectrum of existing embodied AI tasks with significantly enhanced diversity, all within a unified simulation and evaluation framework tailored for MLLMs. The tasks are organized into five categories: navigation, object interaction, social interaction, attribute question answering, and spatial question answering to assess different capabilities of the agents. We evaluated the state-of-the-art MLLMs on EmbodiedEval and found that they have a significant shortfall compared to human level on embodied tasks. Our analysis demonstrates the limitations of existing MLLMs in embodied capabilities, providing insights for their future development. We open-source all evaluation data and simulation framework at https://github.com/thunlp/EmbodiedEval.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Large Language Model Empowered Agents for Recommendation and Search: Towards Next-Generation Information Retrieval</title>
<link>https://arxiv.org/abs/2503.05659</link>
<guid>https://arxiv.org/abs/2503.05659</guid>
<content:encoded><![CDATA[
<div> : 

<br />
:
LLMsLLMLLMLLM <div>
arXiv:2503.05659v2 Announce Type: replace 
Abstract: Information technology has profoundly altered the way humans interact with information. The vast amount of content created, shared, and disseminated online has made it increasingly difficult to access relevant information. Over the past two decades, recommender systems and search (collectively referred to as information retrieval systems) have evolved significantly to address these challenges. Recent advances in large language models (LLMs) have demonstrated capabilities that surpass human performance in various language-related tasks and exhibit general understanding, reasoning, and decision-making abilities. This paper explores the transformative potential of LLM agents in enhancing recommender and search systems. We discuss the motivations and roles of LLM agents, and establish a classification framework to elaborate on the existing research. We highlight the immense potential of LLM agents in addressing current challenges in recommendation and search, providing insights into future research directions. This paper is the first to systematically review and classify the research on LLM agents in these domains, offering a novel perspective on leveraging this advanced AI technology for information retrieval. To help understand the existing works, we list the existing papers on LLM agent based recommendation and search at this link: https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Universal LLM Reward Design through Heuristic Reward Observation Space Evolution</title>
<link>https://arxiv.org/abs/2504.07596</link>
<guid>https://arxiv.org/abs/2504.07596</guid>
<content:encoded><![CDATA[
<div> :  (LLMs),  (RL) ,  (ROS), , -

:
LLMsRLROSLLMLLM-RLjingjjjjjie.github.io/LLM2Reward <div>
arXiv:2504.07596v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are emerging as promising tools for automated reinforcement learning (RL) reward design, owing to their robust capabilities in commonsense reasoning and code generation. By engaging in dialogues with RL agents, LLMs construct a Reward Observation Space (ROS) by selecting relevant environment states and defining their internal operations. However, existing frameworks have not effectively leveraged historical exploration data or manual task descriptions to iteratively evolve this space. In this paper, we propose a novel heuristic framework that enhances LLM-driven reward design by evolving the ROS through a table-based exploration caching mechanism and a text-code reconciliation strategy. Our framework introduces a state execution table, which tracks the historical usage and success rates of environment states, overcoming the Markovian constraint typically found in LLM dialogues and facilitating more effective exploration. Furthermore, we reconcile user-provided task descriptions with expert-defined success criteria using structured prompts, ensuring alignment in reward design objectives. Comprehensive evaluations on benchmark RL tasks demonstrate the effectiveness and stability of the proposed framework. Code and video demos are available at jingjjjjjie.github.io/LLM2Reward.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning the Distribution Map in Reverse Causal Performative Prediction</title>
<link>https://arxiv.org/abs/2405.15172</link>
<guid>https://arxiv.org/abs/2405.15172</guid>
<content:encoded><![CDATA[
<div> : 

:<br />
 <div>
arXiv:2405.15172v2 Announce Type: replace-cross 
Abstract: In numerous predictive scenarios, the predictive model affects the sampling distribution; for example, job applicants often meticulously craft their resumes to navigate through a screening systems. Such shifts in distribution are particularly prevalent in the realm of social computing, yet, the strategies to learn these shifts from data remain remarkably limited. Inspired by a microeconomic model that adeptly characterizes agents' behavior within labor markets, we introduce a novel approach to learn the distribution shift. Our method is predicated on a reverse causal model, wherein the predictive model instigates a distribution shift exclusively through a finite set of agents' actions. Within this framework, we employ a microfoundation model for the agents' actions and develop a statistically justified methodology to learn the distribution shift map, which we demonstrate to be effective in minimizing the performative prediction risk.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Microfoundation Inference for Strategic Prediction</title>
<link>https://arxiv.org/abs/2411.08998</link>
<guid>https://arxiv.org/abs/2411.08998</guid>
<content:encoded><![CDATA[
<div> : performative predictionpredictive modelsocial impactoptimal transportconvergence rate

<br /><br />:
 <div>
arXiv:2411.08998v2 Announce Type: replace-cross 
Abstract: Often in prediction tasks, the predictive model itself can influence the distribution of the target variable, a phenomenon termed performative prediction. Generally, this influence stems from strategic actions taken by stakeholders with a vested interest in predictive models. A key challenge that hinders the widespread adaptation of performative prediction in machine learning is that practitioners are generally unaware of the social impacts of their predictions. To address this gap, we propose a methodology for learning the distribution map that encapsulates the long-term impacts of predictive models on the population. Specifically, we model agents' responses as a cost-adjusted utility maximization problem and propose estimates for said cost. Our approach leverages optimal transport to align pre-model exposure (ex ante) and post-model exposure (ex post) distributions. We provide a rate of convergence for this proposed estimate and assess its quality through empirical demonstrations on a credit-scoring dataset.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EXCLAIM: An Explainable Cross-Modal Agentic System for Misinformation Detection with Hierarchical Retrieval</title>
<link>https://arxiv.org/abs/2504.06269</link>
<guid>https://arxiv.org/abs/2504.06269</guid>
<content:encoded><![CDATA[
<div> misinformation, out-of-context, detection, multi-modal, EXCLAIM

<br /><br />:
Out-of-ContextOOCOOCEXCLAIMEXCLAIMOOC4.3% <div>
arXiv:2504.06269v1 Announce Type: new 
Abstract: Misinformation continues to pose a significant challenge in today's information ecosystem, profoundly shaping public perception and behavior. Among its various manifestations, Out-of-Context (OOC) misinformation is particularly obscure, as it distorts meaning by pairing authentic images with misleading textual narratives. Existing methods for detecting OOC misinformation predominantly rely on coarse-grained similarity metrics between image-text pairs, which often fail to capture subtle inconsistencies or provide meaningful explainability. While multi-modal large language models (MLLMs) demonstrate remarkable capabilities in visual reasoning and explanation generation, they have not yet demonstrated the capacity to address complex, fine-grained, and cross-modal distinctions necessary for robust OOC detection. To overcome these limitations, we introduce EXCLAIM, a retrieval-based framework designed to leverage external knowledge through multi-granularity index of multi-modal events and entities. Our approach integrates multi-granularity contextual analysis with a multi-agent reasoning architecture to systematically evaluate the consistency and integrity of multi-modal news content. Comprehensive experiments validate the effectiveness and resilience of EXCLAIM, demonstrating its ability to detect OOC misinformation with 4.3% higher accuracy compared to state-of-the-art approaches, while offering explainable and actionable insights.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ER-RAG: Enhance RAG with ER-Based Unified Modeling of Heterogeneous Data Sources</title>
<link>https://arxiv.org/abs/2504.06271</link>
<guid>https://arxiv.org/abs/2504.06271</guid>
<content:encoded><![CDATA[
<div> : , , , , 

<br />
:
ER-RAGRAGER-RAGERGETJOINER-based APIsAPIER-RAG2024KDDCup CRAGER-RAGRAG8BER-RAGLLM3.1%5.5 <div>
arXiv:2504.06271v1 Announce Type: new 
Abstract: Large language models (LLMs) excel in question-answering (QA) tasks, and retrieval-augmented generation (RAG) enhances their precision by incorporating external evidence from diverse sources like web pages, databases, and knowledge graphs. However, current RAG methods rely on agent-specific strategies for individual data sources, posing challenges low-resource or black-box environments and complicates operations when evidence is fragmented across sources. To address these limitations, we propose ER-RAG, a framework that unifies evidence integration across heterogeneous data sources using the Entity-Relationship (ER) model. ER-RAG standardizes entity retrieval and relationship querying through ER-based APIs with GET and JOIN operations. It employs a two-stage generation process: first, a preference optimization module selects optimal sources; second, another module constructs API chains based on source schemas. This unified approach allows efficient fine-tuning and seamless integration across diverse data sources. ER-RAG demonstrated its effectiveness by winning all three tracks of the 2024 KDDCup CRAG Challenge, achieving performance on par with commercial RAG pipelines using an 8B LLM backbone. It outperformed hybrid competitors by 3.1% in LLM score and accelerated retrieval by 5.5X.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAVEN: An Agentic Framework for Multimodal Entity Discovery from Large-Scale Video Collections</title>
<link>https://arxiv.org/abs/2504.06272</link>
<guid>https://arxiv.org/abs/2504.06272</guid>
<content:encoded><![CDATA[
<div> : RAVENAI agnostic

:<br />
RAVENAIRAVEN1)2)3)RAVEN(VLMs)(LLMs) <div>
arXiv:2504.06272v1 Announce Type: new 
Abstract: We present RAVEN an adaptive AI agent framework designed for multimodal entity discovery and retrieval in large-scale video collections. Synthesizing information across visual, audio, and textual modalities, RAVEN autonomously processes video data to produce structured, actionable representations for downstream tasks. Key contributions include (1) a category understanding step to infer video themes and general-purpose entities, (2) a schema generation mechanism that dynamically defines domain-specific entities and attributes, and (3) a rich entity extraction process that leverages semantic retrieval and schema-guided prompting. RAVEN is designed to be model-agnostic, allowing the integration of different vision-language models (VLMs) and large language models (LLMs) based on application-specific requirements. This flexibility supports diverse applications in personalized search, content discovery, and scalable information retrieval, enabling practical applications across vast datasets.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Evaluation Framework for Personalized and Trustworthy Agents: A Multi-Session Approach to Preference Adaptability</title>
<link>https://arxiv.org/abs/2504.06277</link>
<guid>https://arxiv.org/abs/2504.06277</guid>
<content:encoded><![CDATA[
<div> : 

<br />
:
AILLMs <div>
arXiv:2504.06277v1 Announce Type: new 
Abstract: Recent advancements in generative AI have significantly increased interest in personalized agents. With increased personalization, there is also a greater need for being able to trust decision-making and action taking capabilities of these agents. However, the evaluation methods for these agents remain outdated and inadequate, often failing to capture the dynamic and evolving nature of user interactions. In this conceptual article, we argue for a paradigm shift in evaluating personalized and adaptive agents. We propose a comprehensive novel framework that models user personas with unique attributes and preferences. In this framework, agents interact with these simulated users through structured interviews to gather their preferences and offer customized recommendations. These recommendations are then assessed dynamically using simulations driven by Large Language Models (LLMs), enabling an adaptive and iterative evaluation process. Our flexible framework is designed to support a variety of agents and applications, ensuring a comprehensive and versatile evaluation of recommendation strategies that focus on proactive, personalized, and trustworthy aspects.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Information-Geometric Approach to Artificial Curiosity</title>
<link>https://arxiv.org/abs/2504.06355</link>
<guid>https://arxiv.org/abs/2504.06355</guid>
<content:encoded><![CDATA[
<div> -

:
-<br /><br /> <div>
arXiv:2504.06355v1 Announce Type: new 
Abstract: Learning in environments with sparse rewards remains a fundamental challenge in reinforcement learning. Artificial curiosity addresses this limitation through intrinsic rewards to guide exploration, however, the precise formulation of these rewards has remained elusive. Ideally, such rewards should depend on the agent's information about the environment, remaining agnostic to the representation of the information -- an invariance central to information geometry. Leveraging information geometry, we show that invariance under congruent Markov morphisms and the agent-environment interaction, uniquely constrains intrinsic rewards to concave functions of the reciprocal occupancy. Additional geometrically motivated restrictions effectively limits the candidates to those determined by a real parameter that governs the occupancy space geometry. Remarkably, special values of this parameter are found to correspond to count-based and maximum entropy exploration, revealing a geometric exploration-exploitation trade-off. This framework provides important constraints to the engineering of intrinsic reward while integrating foundational exploration methods into a single, cohesive model.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Self-Disclosure Themes and Semantics to a Human, a Robot, and a Disembodied Agent</title>
<link>https://arxiv.org/abs/2504.06374</link>
<guid>https://arxiv.org/abs/2504.06374</guid>
<content:encoded><![CDATA[
<div> : 

:
 <div>
arXiv:2504.06374v1 Announce Type: new 
Abstract: As social robots and other artificial agents become more conversationally capable, it is important to understand whether the content and meaning of self-disclosure towards these agents changes depending on the agent's embodiment. In this study, we analysed conversational data from three controlled experiments in which participants self-disclosed to a human, a humanoid social robot, and a disembodied conversational agent. Using sentence embeddings and clustering, we identified themes in participants' disclosures, which were then labelled and explained by a large language model. We subsequently assessed whether these themes and the underlying semantic structure of the disclosures varied by agent embodiment. Our findings reveal strong consistency: thematic distributions did not significantly differ across embodiments, and semantic similarity analyses showed that disclosures were expressed in highly comparable ways. These results suggest that while embodiment may influence human behaviour in human-robot and human-agent interactions, people tend to maintain a consistent thematic focus and semantic structure in their disclosures, whether speaking to humans or artificial interlocutors.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-Arena: A General Framework for Evaluating Control Algorithms</title>
<link>https://arxiv.org/abs/2504.06468</link>
<guid>https://arxiv.org/abs/2504.06468</guid>
<content:encoded><![CDATA[
<div> Agent-ArenaPython

:
<br />
Agent-ArenaPythonAgent-ArenaGitHubhttps://github.com/halid1020/agent-arena-v0 <div>
arXiv:2504.06468v1 Announce Type: new 
Abstract: Robotic research is inherently challenging, requiring expertise in diverse environments and control algorithms. Adapting algorithms to new environments often poses significant difficulties, compounded by the need for extensive hyper-parameter tuning in data-driven methods. To address these challenges, we present Agent-Arena, a Python framework designed to streamline the integration, replication, development, and testing of decision-making policies across a wide range of benchmark environments. Unlike existing frameworks, Agent-Arena is uniquely generalised to support all types of control algorithms and is adaptable to both simulation and real-robot scenarios. Please see our GitHub repository https://github.com/halid1020/agent-arena-v0.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeedleInATable: Exploring Long-Context Capability of Large Language Models towards Long-Structured Tables</title>
<link>https://arxiv.org/abs/2504.06560</link>
<guid>https://arxiv.org/abs/2504.06560</guid>
<content:encoded><![CDATA[
<div> : , , , NeedleInATable (NIAT), 

:<br />
NeedleInATable (NIAT)LLMLLMNIATLLMLLM <div>
arXiv:2504.06560v1 Announce Type: new 
Abstract: Processing structured tabular data, particularly lengthy tables, constitutes a fundamental yet challenging task for large language models (LLMs). However, existing long-context benchmarks primarily focus on unstructured text, neglecting the challenges of long and complex structured tables. To address this gap, we introduce NeedleInATable (NIAT), a novel task that treats each table cell as a "needle" and requires the model to extract the target cell under different queries. Evaluation results of mainstream LLMs on this benchmark show they lack robust long-table comprehension, often relying on superficial correlations or shortcuts for complex table understanding tasks, revealing significant limitations in processing intricate tabular data. To this end, we propose a data synthesis method to enhance models' long-table comprehension capabilities. Experimental results show that our synthesized training data significantly enhances LLMs' performance on the NIAT task, outperforming both long-context LLMs and long-table agent methods. This work advances the evaluation of LLMs' genuine long-structured table comprehension capabilities and paves the way for progress in long-context and table understanding applications.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Right Prediction, Wrong Reasoning: Uncovering LLM Misalignment in RA Disease Diagnosis</title>
<link>https://arxiv.org/abs/2504.06581</link>
<guid>https://arxiv.org/abs/2504.06581</guid>
<content:encoded><![CDATA[
<div> :  (LLMs) (RA)

<br /><br />:
LLMsRALLMsRA95%68%LLMsRALLM <div>
arXiv:2504.06581v1 Announce Type: new 
Abstract: Large language models (LLMs) offer a promising pre-screening tool, improving early disease detection and providing enhanced healthcare access for underprivileged communities. The early diagnosis of various diseases continues to be a significant challenge in healthcare, primarily due to the nonspecific nature of early symptoms, the shortage of expert medical practitioners, and the need for prolonged clinical evaluations, all of which can delay treatment and adversely affect patient outcomes. With impressive accuracy in prediction across a range of diseases, LLMs have the potential to revolutionize clinical pre-screening and decision-making for various medical conditions. In this work, we study the diagnostic capability of LLMs for Rheumatoid Arthritis (RA) with real world patients data. Patient data was collected alongside diagnoses from medical experts, and the performance of LLMs was evaluated in comparison to expert diagnoses for RA disease prediction. We notice an interesting pattern in disease diagnosis and find an unexpected \textit{misalignment between prediction and explanation}. We conduct a series of multi-round analyses using different LLM agents. The best-performing model accurately predicts rheumatoid arthritis (RA) diseases approximately 95\% of the time. However, when medical experts evaluated the reasoning generated by the model, they found that nearly 68\% of the reasoning was incorrect. This study highlights a clear misalignment between LLMs high prediction accuracy and its flawed reasoning, raising important questions about relying on LLM explanations in clinical settings. \textbf{LLMs provide incorrect reasoning to arrive at the correct answer for RA disease diagnosis.}
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wanting to be Understood</title>
<link>https://arxiv.org/abs/2504.06611</link>
<guid>https://arxiv.org/abs/2504.06611</guid>
<content:encoded><![CDATA[
<div> intrinsic motivation, mutual awareness, reinforcement learning, artificial curiosity, social interaction

:
/ <div>
arXiv:2504.06611v1 Announce Type: new 
Abstract: This paper explores an intrinsic motivation for mutual awareness, hypothesizing that humans possess a fundamental drive to understand \textit{and to be understood} even in the absence of extrinsic rewards. Through simulations of the perceptual crossing paradigm, we explore the effect of various internal reward functions in reinforcement learning agents. The drive to understand is implemented as an active inference type artificial curiosity reward, whereas the drive to be understood is implemented through intrinsic rewards for imitation, influence/impressionability, and sub-reaction time anticipation of the other. Results indicate that while artificial curiosity alone does not lead to a preference for social interaction, rewards emphasizing reciprocal understanding successfully drive agents to prioritize interaction. We demonstrate that this intrinsic motivation can facilitate cooperation in tasks where only one agent receives extrinsic reward for the behaviour of the other.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentFM: Role-Aware Failure Management for Distributed Databases with LLM-Driven Multi-Agents</title>
<link>https://arxiv.org/abs/2504.06614</link>
<guid>https://arxiv.org/abs/2504.06614</guid>
<content:encoded><![CDATA[
<div> : AgentFMLLM

:<br />
AgentFMLLMApache IoTDBAgentFM <div>
arXiv:2504.06614v1 Announce Type: new 
Abstract: Distributed databases are critical infrastructures for today's large-scale software systems, making effective failure management essential to ensure software availability. However, existing approaches often overlook the role distinctions within distributed databases and rely on small-scale models with limited generalization capabilities. In this paper, we conduct a preliminary empirical study to emphasize the unique significance of different roles. Building on this insight, we propose AgentFM, a role-aware failure management framework for distributed databases powered by LLM-driven multi-agents. AgentFM addresses failure management by considering system roles, data roles, and task roles, with a meta-agent orchestrating these components. Preliminary evaluations using Apache IoTDB demonstrate the effectiveness of AgentFM and open new directions for further research.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-like compositional learning of visually-grounded concepts using synthetic environments</title>
<link>https://arxiv.org/abs/2504.06618</link>
<guid>https://arxiv.org/abs/2504.06618</guid>
<content:encoded><![CDATA[
<div> 

<br />
:
3D <div>
arXiv:2504.06618v1 Announce Type: new 
Abstract: The compositional structure of language enables humans to decompose complex phrases and map them to novel visual concepts, showcasing flexible intelligence. While several algorithms exhibit compositionality, they fail to elucidate how humans learn to compose concept classes and ground visual cues through trial and error. To investigate this multi-modal learning challenge, we designed a 3D synthetic environment in which an agent learns, via reinforcement, to navigate to a target specified by a natural language instruction. These instructions comprise nouns, attributes, and critically, determiners, prepositions, or both. The vast array of word combinations heightens the compositional complexity of the visual grounding task, as navigating to a blue cube above red spheres is not rewarded when the instruction specifies navigating to "some blue cubes below the red sphere". We first demonstrate that reinforcement learning agents can ground determiner concepts to visual targets but struggle with more complex prepositional concepts. Second, we show that curriculum learning, a strategy humans employ, enhances concept learning efficiency, reducing the required training episodes by 15% in determiner environments and enabling agents to easily learn prepositional concepts. Finally, we establish that agents trained on determiner or prepositional concepts can decompose held-out test instructions and rapidly adapt their navigation policies to unseen visual object combinations. Leveraging synthetic environments, our findings demonstrate that multi-modal reinforcement learning agents can achieve compositional understanding of complex concept classes and highlight the efficacy of human-like learning strategies in improving artificial systems' learning efficiency.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Residual Safe Reinforcement Learning for Multi-Agent Safety-Critical Scenarios Decision-Making</title>
<link>https://arxiv.org/abs/2504.06670</link>
<guid>https://arxiv.org/abs/2504.06670</guid>
<content:encoded><![CDATA[
<div> : 

:<br />
DRS-RLDRS-RL92.17%27% <div>
arXiv:2504.06670v1 Announce Type: new 
Abstract: In multi-agent safety-critical scenarios, traditional autonomous driving frameworks face significant challenges in balancing safety constraints and task performance. These frameworks struggle to quantify dynamic interaction risks in real-time and depend heavily on manual rules, resulting in low computational efficiency and conservative strategies. To address these limitations, we propose a Dynamic Residual Safe Reinforcement Learning (DRS-RL) framework grounded in a safety-enhanced networked Markov decision process. It's the first time that the weak-to-strong theory is introduced into multi-agent decision-making, enabling lightweight dynamic calibration of safety boundaries via a weak-to-strong safety correction paradigm. Based on the multi-agent dynamic conflict zone model, our framework accurately captures spatiotemporal coupling risks among heterogeneous traffic participants and surpasses the static constraints of conventional geometric rules. Moreover, a risk-aware prioritized experience replay mechanism mitigates data distribution bias by mapping risk to sampling probability. Experimental results reveal that the proposed method significantly outperforms traditional RL algorithms in safety, efficiency, and comfort. Specifically, it reduces the collision rate by up to 92.17%, while the safety model accounts for merely 27% of the main model's parameters.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperparameter Optimisation with Practical Interpretability and Explanation Methods in Probabilistic Curriculum Learning</title>
<link>https://arxiv.org/abs/2504.06683</link>
<guid>https://arxiv.org/abs/2504.06683</guid>
<content:encoded><![CDATA[
<div> : AlgOSSHAP

:<br />
PCLRLOptunaTPEAlgOSRLSHAPRL <div>
arXiv:2504.06683v1 Announce Type: new 
Abstract: Hyperparameter optimisation (HPO) is crucial for achieving strong performance in reinforcement learning (RL), as RL algorithms are inherently sensitive to hyperparameter settings. Probabilistic Curriculum Learning (PCL) is a curriculum learning strategy designed to improve RL performance by structuring the agent's learning process, yet effective hyperparameter tuning remains challenging and computationally demanding. In this paper, we provide an empirical analysis of hyperparameter interactions and their effects on the performance of a PCL algorithm within standard RL tasks, including point-maze navigation and DC motor control. Using the AlgOS framework integrated with Optuna's Tree-Structured Parzen Estimator (TPE), we present strategies to refine hyperparameter search spaces, enhancing optimisation efficiency. Additionally, we introduce a novel SHAP-based interpretability approach tailored specifically for analysing hyperparameter impacts, offering clear insights into how individual hyperparameters and their interactions influence RL performance. Our work contributes practical guidelines and interpretability tools that significantly improve the effectiveness and computational feasibility of hyperparameter optimisation in reinforcement learning.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FJ-MM: The Friedkin-Johnsen Opinion Dynamics Model with Memory and Higher-Order Neighbors</title>
<link>https://arxiv.org/abs/2504.06731</link>
<guid>https://arxiv.org/abs/2504.06731</guid>
<content:encoded><![CDATA[
<div> Friedkin-JohnsenFJ-MM

<br /><br />:
Friedkin-JohnsenFJFJ-MMFJ-MMFJ-MMFJFJFJ-MM <div>
arXiv:2504.06731v1 Announce Type: new 
Abstract: The Friedkin-Johnsen (FJ) model has been extensively explored and validated, spanning applications in social science, systems and control, game theory, and algorithmic research. In this paper, we introduce an advanced generalization of the FJ model, termed FJ-MM which incorporates both memory effects and multi-hop (higher-order neighbor) influence. This formulation allows agents to naturally incorporate both current and previous opinions at each iteration stage. Our numerical results demonstrate that incorporating memory and multi-hop influence significantly reshapes the opinion landscape; for example, the final opinion profile can exhibit reduced polarization. We analyze the stability and equilibrium properties of the FJ-MM model, showing that these properties can be reduced to those of a comparison model--namely, the standard FJ model with a modified influence matrix. This reduction enables us to leverage established stability results from FJ dynamics. Additionally, we examine the convergence rate of the FJ-MM model and demonstrate that, as can be expected, the time lags introduced by memory and higher-order neighbor influences result in slower convergence.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Human-Robot Collaborative Missions using Hybrid Task Planning</title>
<link>https://arxiv.org/abs/2504.06746</link>
<guid>https://arxiv.org/abs/2504.06746</guid>
<content:encoded><![CDATA[
<div> : 

<br />
:
- <div>
arXiv:2504.06746v1 Announce Type: new 
Abstract: Producing robust task plans in human-robot collaborative missions is a critical activity in order to increase the likelihood of these missions completing successfully. Despite the broad research body in the area, which considers different classes of constraints and uncertainties, its applicability is confined to relatively simple problems that can be comfortably addressed by the underpinning mathematically-based or heuristic-driven solver engines. In this paper, we introduce a hybrid approach that effectively solves the task planning problem by decomposing it into two intertwined parts, starting with the identification of a feasible plan and followed by its uncertainty augmentation and verification yielding a set of Pareto optimal plans. To enhance its robustness, adaptation tactics are devised for the evolving system requirements and agents' capabilities. We demonstrate our approach through an industrial case study involving workers and robots undertaking activities within a vineyard, showcasing the benefits of our hybrid approach both in the generation of feasible solutions and scalability compared to native planners.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FamilyTool: A Multi-hop Personalized Tool Use Benchmark</title>
<link>https://arxiv.org/abs/2504.06766</link>
<guid>https://arxiv.org/abs/2504.06766</guid>
<content:encoded><![CDATA[
<div> : , , , , 

:
FamilyToolFamilyTool13KGEToolLLMsLLMsLLMsFamilyToolLLMGithub <div>
arXiv:2504.06766v1 Announce Type: new 
Abstract: The integration of tool learning with Large Language Models (LLMs) has expanded their capabilities in handling complex tasks by leveraging external tools. However, existing benchmarks for tool learning inadequately address critical real-world personalized scenarios, particularly those requiring multi-hop reasoning and inductive knowledge adaptation in dynamic environments. To bridge this gap, we introduce FamilyTool, a novel benchmark grounded in a family-based knowledge graph (KG) that simulates personalized, multi-hop tool use scenarios. FamilyTool challenges LLMs with queries spanning 1 to 3 relational hops (e.g., inferring familial connections and preferences) and incorporates an inductive KG setting where models must adapt to unseen user preferences and relationships without re-training, a common limitation in prior approaches that compromises generalization. We further propose KGETool: a simple KG-augmented evaluation pipeline to systematically assess LLMs' tool use ability in these settings. Experiments reveal significant performance gaps in state-of-the-art LLMs, with accuracy dropping sharply as hop complexity increases and inductive scenarios exposing severe generalization deficits. These findings underscore the limitations of current LLMs in handling personalized, evolving real-world contexts and highlight the urgent need for advancements in tool-learning frameworks. FamilyTool serves as a critical resource for evaluating and advancing LLM agents' reasoning, adaptability, and scalability in complex, dynamic environments. Code and dataset are available at Github.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inducing Programmatic Skills for Agentic Tasks</title>
<link>https://arxiv.org/abs/2504.06821</link>
<guid>https://arxiv.org/abs/2504.06821</guid>
<content:encoded><![CDATA[
<div> : (ASI)Web

:
(ASI)WebArenaASI23.5%11.3%ASI10.7-15.3%ASIASI <div>
arXiv:2504.06821v1 Announce Type: new 
Abstract: To succeed in common digital tasks such as web navigation, agents must carry out a variety of specialized tasks such as searching for products or planning a travel route. To tackle these tasks, agents can bootstrap themselves by learning task-specific skills online through interaction with the web environment. In this work, we demonstrate that programs are an effective representation for skills. We propose agent skill induction (ASI), which allows agents to adapt themselves by inducing, verifying, and utilizing program-based skills on the fly. We start with an evaluation on the WebArena agent benchmark and show that ASI outperforms the static baseline agent and its text-skill counterpart by 23.5% and 11.3% in success rate, mainly thanks to the programmatic verification guarantee during the induction phase. ASI also improves efficiency by reducing 10.7-15.3% of the steps over baselines, by composing primitive actions (e.g., click) into higher-level skills (e.g., search product). We then highlight the efficacy of ASI in remaining efficient and accurate under scaled-up web activities. Finally, we examine the generalizability of induced skills when transferring between websites, and find that ASI can effectively reuse common skills, while also updating incompatible skills to versatile website changes.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IAAO: Interactive Affordance Learning for Articulated Objects in 3D Environments</title>
<link>https://arxiv.org/abs/2504.06827</link>
<guid>https://arxiv.org/abs/2504.06827</guid>
<content:encoded><![CDATA[
<div> IAAO3Darticulated

<br />
:
IAAOIAAO3D splatting (3DGS)3D <div>
arXiv:2504.06827v1 Announce Type: new 
Abstract: This work presents IAAO, a novel framework that builds an explicit 3D model for intelligent agents to gain understanding of articulated objects in their environment through interaction. Unlike prior methods that rely on task-specific networks and assumptions about movable parts, our IAAO leverages large foundation models to estimate interactive affordances and part articulations in three stages. We first build hierarchical features and label fields for each object state using 3D Gaussian Splatting (3DGS) by distilling mask features and view-consistent labels from multi-view images. We then perform object- and part-level queries on the 3D Gaussian primitives to identify static and articulated elements, estimating global transformations and local articulation parameters along with affordances. Finally, scenes from different states are merged and refined based on the estimated transformations, enabling robust affordance-based interaction and manipulation of objects. Experimental results demonstrate the effectiveness of our method.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games</title>
<link>https://arxiv.org/abs/2504.06868</link>
<guid>https://arxiv.org/abs/2504.06868</guid>
<content:encoded><![CDATA[
<div> : PANDA

:
PANDA <div>
arXiv:2504.06868v1 Announce Type: new 
Abstract: Artificial agents are increasingly central to complex interactions and decision-making tasks, yet aligning their behaviors with desired human values remains an open challenge. In this work, we investigate how human-like personality traits influence agent behavior and performance within text-based interactive environments. We introduce PANDA: PersonalityAdapted Neural Decision Agents, a novel method for projecting human personality traits onto agents to guide their behavior. To induce personality in a text-based game agent, (i) we train a personality classifier to identify what personality type the agent's actions exhibit, and (ii) we integrate the personality profiles directly into the agent's policy-learning pipeline. By deploying agents embodying 16 distinct personality types across 25 text-based games and analyzing their trajectories, we demonstrate that an agent's action decisions can be guided toward specific personality profiles. Moreover, certain personality types, such as those characterized by higher levels of Openness, display marked advantages in performance. These findings underscore the promise of personality-adapted agents for fostering more aligned, effective, and human-centric decision-making in interactive environments.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Driven Consensus: Modeling Multi-Agent Networks with Long-Range Interactions through path-Laplacian Matrices</title>
<link>https://arxiv.org/abs/2504.06894</link>
<guid>https://arxiv.org/abs/2504.06894</guid>
<content:encoded><![CDATA[
<div> : kLSTMxLSTMTransformerXGBoostConvLSTMErds-RnyiWatts-StrogatzBarabsi-Albert

<br /><br />:
kLSTMxLSTMTransformerXGBoostConvLSTMErds-RnyiWatts-StrogatzBarabsi-Albertk <div>
arXiv:2504.06894v1 Announce Type: new 
Abstract: Extended connectivity in graphs can be analyzed through k-path Laplacian matrices, which permit the capture of long-range interactions in various real-world networked systems such as social, transportation, and multi-agent networks. In this work, we present several alternative methods based on machine learning methods (LSTM, xLSTM, Transformer, XGBoost, and ConvLSTM) to predict the final consensus value based on directed networks (Erd\"os-Renyi, Watts-Strogatz, and Barab\'asi-Albert) and on the initial state. We highlight how different k-hop interactions affect the performance of the tested methods. This framework opens new avenues for analyzing multi-scale diffusion processes in large-scale, complex networks.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Review of Case-Based Reasoning for LLM Agents: Theoretical Foundations, Architectural Components, and Cognitive Integration</title>
<link>https://arxiv.org/abs/2504.06943</link>
<guid>https://arxiv.org/abs/2504.06943</guid>
<content:encoded><![CDATA[
<div> Large Language Models (LLMs)Case-Based Reasoning (CBR)flexibilityreasoning skillscognitive aspects

<br /><br />:
(CBR)(LLMs)CBRLLMsCBRCBRChain-of-ThoughtCBRLLMCBRLLM <div>
arXiv:2504.06943v1 Announce Type: new 
Abstract: Agents powered by Large Language Models (LLMs) have recently demonstrated impressive capabilities in various tasks. Still, they face limitations in tasks requiring specific, structured knowledge, flexibility, or accountable decision-making. While agents are capable of perceiving their environments, forming inferences, planning, and executing actions towards goals, they often face issues such as hallucinations and lack of contextual memory across interactions. This paper explores how Case-Based Reasoning (CBR), a strategy that solves new problems by referencing past experiences, can be integrated into LLM agent frameworks. This integration allows LLMs to leverage explicit knowledge, enhancing their effectiveness. We systematically review the theoretical foundations of these enhanced agents, identify critical framework components, and formulate a mathematical model for the CBR processes of case retrieval, adaptation, and learning. We also evaluate CBR-enhanced agents against other methods like Chain-of-Thought reasoning and standard Retrieval-Augmented Generation, analyzing their relative strengths. Moreover, we explore how leveraging CBR's cognitive dimensions (including self-reflection, introspection, and curiosity) via goal-driven autonomy mechanisms can further enhance the LLM agent capabilities. Contributing to the ongoing research on neuro-symbolic hybrid systems, this work posits CBR as a viable technique for enhancing the reasoning skills and cognitive aspects of autonomous LLM agents.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Agentic Framework for Evaluating Conditional Image Generation</title>
<link>https://arxiv.org/abs/2504.07046</link>
<guid>https://arxiv.org/abs/2504.07046</guid>
<content:encoded><![CDATA[
<div> : CIGEvalGPT-4o

:
CIGEvalCIGEvalGPT-4oCIGEval0.46250.472.3K7BCIGEvalGPT-4oGPT-4oCIGEval <div>
arXiv:2504.07046v1 Announce Type: new 
Abstract: Conditional image generation has gained significant attention for its ability to personalize content. However, the field faces challenges in developing task-agnostic, reliable, and explainable evaluation metrics. This paper introduces CIGEval, a unified agentic framework for comprehensive evaluation of conditional image generation tasks. CIGEval utilizes large multimodal models (LMMs) as its core, integrating a multi-functional toolbox and establishing a fine-grained evaluation framework. Additionally, we synthesize evaluation trajectories for fine-tuning, empowering smaller LMMs to autonomously select appropriate tools and conduct nuanced analyses based on tool outputs. Experiments across seven prominent conditional image generation tasks demonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625 with human assessments, closely matching the inter-annotator correlation of 0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K training trajectories, CIGEval surpasses the previous GPT-4o-based state-of-the-art method. Case studies on GPT-4o image generation highlight CIGEval's capability in identifying subtle issues related to subject consistency and adherence to control guidance, indicating its great potential for automating evaluation of image generation tasks with human-level reliability.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SkillWeaver: Web Agents can Self-Improve by Discovering and Honing Skills</title>
<link>https://arxiv.org/abs/2504.07079</link>
<guid>https://arxiv.org/abs/2504.07079</guid>
<content:encoded><![CDATA[
<div> SkillWeaverAPIWeb

:
<br />
SkillWeaverAPIAPIAPIWebArenaSkillWeaver31.8%39.8%APIWebArena54.3%API <div>
arXiv:2504.07079v1 Announce Type: new 
Abstract: To survive and thrive in complex environments, humans have evolved sophisticated self-improvement mechanisms through environment exploration, hierarchical abstraction of experiences into reuseable skills, and collaborative construction of an ever-growing skill repertoire. Despite recent advancements, autonomous web agents still lack crucial self-improvement capabilities, struggling with procedural knowledge abstraction, refining skills, and skill composition. In this work, we introduce SkillWeaver, a skill-centric framework enabling agents to self-improve by autonomously synthesizing reusable skills as APIs. Given a new website, the agent autonomously discovers skills, executes them for practice, and distills practice experiences into robust APIs. Iterative exploration continually expands a library of lightweight, plug-and-play APIs, significantly enhancing the agent's capabilities. Experiments on WebArena and real-world websites demonstrate the efficacy of SkillWeaver, achieving relative success rate improvements of 31.8% and 39.8%, respectively. Additionally, APIs synthesized by strong agents substantially enhance weaker agents through transferable skills, yielding improvements of up to 54.3% on WebArena. These results demonstrate the effectiveness of honing diverse website interactions into APIs, which can be seamlessly shared among various web agents.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Market, power, gift, and concession economies: Comparison using four-mode primitive network models</title>
<link>https://arxiv.org/abs/2504.06557</link>
<guid>https://arxiv.org/abs/2504.06557</guid>
<content:encoded><![CDATA[
<div> 

:
PolanyiKarataniGraeberWe-turn <div>
arXiv:2504.06557v1 Announce Type: cross 
Abstract: Reducing wealth inequality is a global challenge, and the problems of capitalism stem from the enclosure of the commons and the breakdown of the community. According to previous studies by Polanyi, Karatani, and Graeber, economic modes can be divided into capitalist market economy (enclosure and exchange), power economy (de-enclosure and redistribution), gift economy (obligation to return and reciprocity), and concession economy (de-obligation to return). The concession economy reflects Graeber's baseline communism (from each according to their abilities, to each according to their needs) and Deguchi's We-turn philosophy (the "I" as an individual has a "fundamental incapability" and the subject of physical action, responsibility, and freedom is "We" as a multi-agent system, including the "I"). In this study, we constructed novel network models for these four modes and compared their properties (cluster coefficient, graph density, reciprocity, assortativity, centrality, and Gini coefficient). From the calculation results, it became clear that the market economy leads to inequality; the power economy mitigates inequality but cannot eliminate it; the gift and concession economies lead to a healthy and equal economy; and the concession economy, free from the ties of obligation to return, is possible without guaranteeing reciprocity. We intend to promote the transformation from a capitalist economy to a concession economy through activities that disseminate baseline communism and the We-turn philosophy that promotes concession, that is, developing a cooperative platform to support concession through information technology and empirical research through fieldwork.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A primal-dual perspective for distributed TD-learning</title>
<link>https://arxiv.org/abs/2310.00638</link>
<guid>https://arxiv.org/abs/2310.00638</guid>
<content:encoded><![CDATA[
<div> 

:
TD-ODE-ODETD<br /><br /> <div>
arXiv:2310.00638v2 Announce Type: replace 
Abstract: The goal of this paper is to investigate distributed temporal difference (TD) learning for a networked multi-agent Markov decision process. The proposed approach is based on distributed optimization algorithms, which can be interpreted as primal-dual Ordinary differential equation (ODE) dynamics subject to null-space constraints. Based on the exponential convergence behavior of the primal-dual ODE dynamics subject to null-space constraints, we examine the behavior of the final iterate in various distributed TD-learning scenarios, considering both constant and diminishing step-sizes and incorporating both i.i.d. and Markovian observation models. Unlike existing methods, the proposed algorithm does not require the assumption that the underlying communication network structure is characterized by a doubly stochastic matrix.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approximate Feedback Nash Equilibria with Sparse Inter-Agent Dependencies</title>
<link>https://arxiv.org/abs/2410.16441</link>
<guid>https://arxiv.org/abs/2410.16441</guid>
<content:encoded><![CDATA[
<div> : Lasso

<br /><br />:
LassoLQLQ77% <div>
arXiv:2410.16441v2 Announce Type: replace 
Abstract: Feedback Nash equilibrium strategies in multi-agent dynamic games require availability of all players' state information to compute control actions. However, in real-world scenarios, sensing and communication limitations between agents make full state feedback expensive or impractical, and such strategies can become fragile when state information from other agents is inaccurate. To this end, we propose a regularized dynamic programming approach for finding sparse feedback policies that selectively depend on the states of a subset of agents in dynamic games. The proposed approach solves convex adaptive group Lasso problems to compute sparse policies approximating Nash equilibrium solutions. We prove the regularized solutions' asymptotic convergence to a neighborhood of Nash equilibrium policies in linear-quadratic (LQ) games. Further, we extend the proposed approach to general non-LQ games via an iterative algorithm. Simulation results in multi-robot interaction scenarios show that the proposed approach effectively computes feedback policies with varying sparsity levels. When agents have noisy observations of other agents' states, simulation results indicate that the proposed regularized policies consistently achieve lower costs than standard Nash equilibrium policies by up to 77% for all interacting agents whose costs are coupled with other agents' states.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation</title>
<link>https://arxiv.org/abs/2501.03225</link>
<guid>https://arxiv.org/abs/2501.03225</guid>
<content:encoded><![CDATA[
<div> : vision language models (VLMs), AutoConverter, visual question answering (VQA), multiple-choice format, VMCBench

:
VLMsAutoConverterAutoConverterAutoConverterVLMsAutoConverterVMCBench20VQA9,018VMCBench33VLMsVLMs <div>
arXiv:2501.03225v2 Announce Type: replace 
Abstract: The rapid development of vision language models (VLMs) demands rigorous and reliable evaluation. However, current visual question answering (VQA) benchmarks often depend on open-ended questions, making accurate evaluation difficult due to the variability in natural language responses. To address this, we introduce AutoConverter, an agentic framework that automatically converts these open-ended questions into multiple-choice format, enabling objective evaluation while reducing the costly multiple-choice question creation process. Our experiments demonstrate that AutoConverter can generate correct and challenging multiple-choice questions, with VLMs demonstrating consistently similar or lower accuracy on these questions compared to human-created ones. Using AutoConverter, we construct VMCBench, a benchmark created by transforming 20 existing VQA datasets into a unified multiple-choice format, totaling 9,018 questions. We comprehensively evaluate 33 state-of-the-art VLMs on VMCBench, setting a new standard for scalable, consistent, and reproducible VLM evaluation.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Adaptive Memory-Based Optimization for Enhanced Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2504.05312</link>
<guid>https://arxiv.org/abs/2504.05312</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation (RAG)Adaptive memory-based optimization (Amber)

:
Adaptive memory-based optimization for enhanced RAG (Amber)RAGAmberAgentAmberAmberQA <div>
arXiv:2504.05312v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG), by integrating non-parametric knowledge from external knowledge bases into models, has emerged as a promising approach to enhancing response accuracy while mitigating factual errors and hallucinations. This method has been widely applied in tasks such as Question Answering (QA). However, existing RAG methods struggle with open-domain QA tasks because they perform independent retrieval operations and directly incorporate the retrieved information into generation without maintaining a summarizing memory or using adaptive retrieval strategies, leading to noise from redundant information and insufficient information integration. To address these challenges, we propose Adaptive memory-based optimization for enhanced RAG (Amber) for open-domain QA tasks, which comprises an Agent-based Memory Updater, an Adaptive Information Collector, and a Multi-granular Content Filter, working together within an iterative memory updating paradigm. Specifically, Amber integrates and optimizes the language model's memory through a multi-agent collaborative approach, ensuring comprehensive knowledge integration from previous retrieval steps. It dynamically adjusts retrieval queries and decides when to stop retrieval based on the accumulated knowledge, enhancing retrieval efficiency and effectiveness. Additionally, it reduces noise by filtering irrelevant content at multiple levels, retaining essential information to improve overall model performance. We conduct extensive experiments on several open-domain QA datasets, and the results demonstrate the superiority and effectiveness of our method and its components. The source code is available \footnote{https://anonymous.4open.science/r/Amber-B203/}.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Benefits and Risks: RL Approaches for Addiction-Aware Social Media Recommenders</title>
<link>https://arxiv.org/abs/2504.05322</link>
<guid>https://arxiv.org/abs/2504.05322</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:
RLMBMF <div>
arXiv:2504.05322v1 Announce Type: new 
Abstract: Social media platforms provide valuable opportunities for users to gather information, interact with friends, and enjoy entertainment. However, their addictive potential poses significant challenges, including overuse and negative psycho-logical or behavioral impacts [4, 2, 8]. This study explores strategies to mitigate compulsive social media usage while preserving its benefits and ensuring economic sustainability, focusing on recommenders that promote balanced usage.
  We analyze user behaviors arising from intrinsic diversities and environmental interactions, offering insights for next-generation social media recommenders that prioritize well-being. Specifically, we examine the temporal predictability of overuse and addiction using measures available to recommenders, aiming to inform mechanisms that prevent addiction while avoiding user disengagement [7].
  Building on RL-based computational frameworks for addiction modelling [6], our study introduces: - A recommender system adapting to user preferences, introducing non-stationary and non-Markovian dynamics.
  - Differentiated state representations for users and recommenders to capture nuanced interactions.
  - Distinct usage conditions-light and heavy use-addressing RL's limitations in distinguishing prolonged from healthy engagement.
  - Complexity in overuse impacts, highlighting their role in user adaptation [7].
  Simulations demonstrate how model-based (MB) and model-free (MF) decision-making interact with environmental dynamics to influence user behavior and addiction. Results reveal the significant role of recommender systems in shaping addiction tendencies or fostering healthier engagement. These findings support ethical, adaptive recommender design, advancing sustainable social media ecosystems [9, 1].
  Keywords: multi-agent systems, recommender systems, addiction, social media
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unequal Opportunities: Examining the Bias in Geographical Recommendations by Large Language Models</title>
<link>https://arxiv.org/abs/2504.05325</link>
<guid>https://arxiv.org/abs/2504.05325</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)

:
LLMsLLMsLLMs <div>
arXiv:2504.05325v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) have made them a popular information-seeking tool among end users. However, the statistical training methods for LLMs have raised concerns about their representation of under-represented topics, potentially leading to biases that could influence real-world decisions and opportunities. These biases could have significant economic, social, and cultural impacts as LLMs become more prevalent, whether through direct interactions--such as when users engage with chatbots or automated assistants--or through their integration into third-party applications (as agents), where the models influence decision-making processes and functionalities behind the scenes. Our study examines the biases present in LLMs recommendations of U.S. cities and towns across three domains: relocation, tourism, and starting a business. We explore two key research questions: (i) How similar LLMs responses are, and (ii) How this similarity might favor areas with certain characteristics over others, introducing biases. We focus on the consistency of LLMs responses and their tendency to over-represent or under-represent specific locations. Our findings point to consistent demographic biases in these recommendations, which could perpetuate a ``rich-get-richer'' effect that widens existing economic disparities.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Price Inflation on Algorithmic Collusion Through Reinforcement Learning Agents</title>
<link>https://arxiv.org/abs/2504.05335</link>
<guid>https://arxiv.org/abs/2504.05335</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:
AI <div>
arXiv:2504.05335v1 Announce Type: new 
Abstract: Algorithmic pricing is increasingly shaping market competition, raising concerns about its potential to compromise competitive dynamics. While prior work has shown that reinforcement learning (RL)-based pricing algorithms can lead to tacit collusion, less attention has been given to the role of macroeconomic factors in shaping these dynamics. This study examines the role of inflation in influencing algorithmic collusion within competitive markets. By incorporating inflation shocks into a RL-based pricing model, we analyze whether agents adapt their strategies to sustain supra-competitive profits. Our findings indicate that inflation reduces market competitiveness by fostering implicit coordination among agents, even without direct collusion. However, despite achieving sustained higher profitability, agents fail to develop robust punishment mechanisms to deter deviations from equilibrium strategies. The results suggest that inflation amplifies non-competitive dynamics in algorithmic pricing, emphasizing the need for regulatory oversight in markets where AI-driven pricing is prevalent.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DyTTP: Trajectory Prediction with Normalization-Free Transformers</title>
<link>https://arxiv.org/abs/2504.05356</link>
<guid>https://arxiv.org/abs/2504.05356</guid>
<content:encoded><![CDATA[
<div> : TransformerDynamicTanh (DyT)snapshot ensemble

:<br />
(DynamicTanh, DyT)TransformerDyTensembleArgoverseTransformer <div>
arXiv:2504.05356v1 Announce Type: new 
Abstract: Accurate trajectory prediction is a cornerstone for the safe operation of autonomous driving systems, where understanding the dynamic behavior of surrounding agents is crucial. Transformer-based architectures have demonstrated significant promise in capturing complex spatio-temporality dependencies. However, their reliance on normalization layers can lead to computation overhead and training instabilities. In this work, we present a two-fold approach to address these challenges. First, we integrate DynamicTanh (DyT), which is the latest method to promote transformers, into the backbone, replacing traditional layer normalization. This modification simplifies the network architecture and improves the stability of the inference. We are the first work to deploy the DyT to the trajectory prediction task. Complementing this, we employ a snapshot ensemble strategy to further boost trajectory prediction performance. Using cyclical learning rate scheduling, multiple model snapshots are captured during a single training run. These snapshots are then aggregated via simple averaging at inference time, allowing the model to benefit from diverse hypotheses without incurring substantial additional computational cost. Extensive experiments on Argoverse datasets demonstrate that our combined approach significantly improves prediction accuracy, inference speed and robustness in diverse driving scenarios. This work underscores the potential of normalization-free transformer designs augmented with lightweight ensemble techniques in advancing trajectory forecasting for autonomous vehicles.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debate-Feedback: A Multi-Agent Framework for Efficient Legal Judgment Prediction</title>
<link>https://arxiv.org/abs/2504.05358</link>
<guid>https://arxiv.org/abs/2504.05358</guid>
<content:encoded><![CDATA[
<div> : AI

<br /><br />:
LegalAI <div>
arXiv:2504.05358v1 Announce Type: new 
Abstract: The use of AI in legal analysis and prediction (LegalAI) has gained widespread attention, with past research focusing on retrieval-based methods and fine-tuning large models. However, these approaches often require large datasets and underutilize the capabilities of modern large language models (LLMs). In this paper, inspired by the debate phase of real courtroom trials, we propose a novel legal judgment prediction model based on the Debate-Feedback architecture, which integrates LLM multi-agent debate and reliability evaluation models. Unlike traditional methods, our model achieves significant improvements in efficiency by minimizing the need for large historical datasets, thus offering a lightweight yet robust solution. Comparative experiments show that it outperforms several general-purpose and domain-specific legal models, offering a dynamic reasoning process and a promising direction for future LegalAI research.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Nature-Inspired Colony of Artificial Intelligence System with Fast, Detailed, and Organized Learner Agents for Enhancing Diversity and Quality</title>
<link>https://arxiv.org/abs/2504.05365</link>
<guid>https://arxiv.org/abs/2504.05365</guid>
<content:encoded><![CDATA[
<div> (CNN)(AI)

<br /><br />:
CNNAIAIVGG16VGG19ResNet50AIAIAIAIVGG16VGG19ResNet50AIF182%95%AI <div>
arXiv:2504.05365v1 Announce Type: new 
Abstract: The concepts of convolutional neural networks (CNNs) and multi-agent systems are two important areas of research in artificial intelligence (AI). In this paper, we present an approach that builds a CNN-based colony of AI agents to serve as a single system and perform multiple tasks (e.g., predictions or classifications) in an environment. The proposed system impersonates the natural environment of a biological system, like an ant colony or a human colony. The proposed colony of AI that is defined as a role-based system uniquely contributes to accomplish tasks in an environment by incorporating AI agents that are fast learners, detailed learners, and organized learners. These learners can enhance their localized learning and their collective decisions as a single system of colony of AI agents. This approach also enhances the diversity and quality of the colony of AI with the help of Genetic Algorithms and their crossover and mutation mechanisms. The evolution of fast, detailed, and organized learners in the colony of AI is achieved by introducing a unique one-to-one mapping between these learners and the pretrained VGG16, VGG19, and ResNet50 models, respectively. This role-based approach creates two parent-AI agents using the AI models through the processes, called the intra- and inter-marriage of AI, so that they can share their learned knowledge (weights and biases) based on a probabilistic rule and produce diversified child-AI agents to perform new tasks. This process will form a colony of AI that consists of families of multi-model and mixture-model AI agents to improve diversity and quality. Simulations show that the colony of AI, built using the VGG16, VGG19, and ResNet50 models, can provide a single system that generates child-AI agents of excellent predictive performance, ranging between 82% and 95% of F1-scores, to make diversified collective and quality decisions on a task.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EduPlanner: LLM-Based Multi-Agent Systems for Customized and Intelligent Instructional Design</title>
<link>https://arxiv.org/abs/2504.05370</link>
<guid>https://arxiv.org/abs/2504.05370</guid>
<content:encoded><![CDATA[
<div> :  (LLMs), , , , 

:
EduPlannerLLMsAGIEduPlannerLLMCIDDPEduPlannerhttps://github.com/Zc0812/Edu_Planner <div>
arXiv:2504.05370v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have significantly advanced smart education in the Artificial General Intelligence (AGI) era. A promising application lies in the automatic generalization of instructional design for curriculum and learning activities, focusing on two key aspects: (1) Customized Generation: generating niche-targeted teaching content based on students' varying learning abilities and states, and (2) Intelligent Optimization: iteratively optimizing content based on feedback from learning effectiveness or test scores. Currently, a single large LLM cannot effectively manage the entire process, posing a challenge for designing intelligent teaching plans. To address these issues, we developed EduPlanner, an LLM-based multi-agent system comprising an evaluator agent, an optimizer agent, and a question analyst, working in adversarial collaboration to generate customized and intelligent instructional design for curriculum and learning activities. Taking mathematics lessons as our example, EduPlanner employs a novel Skill-Tree structure to accurately model the background mathematics knowledge of student groups, personalizing instructional design for curriculum and learning activities according to students' knowledge levels and learning abilities. Additionally, we introduce the CIDDP, an LLM-based five-dimensional evaluation module encompassing clarity, Integrity, Depth, Practicality, and Pertinence, to comprehensively assess mathematics lesson plan quality and bootstrap intelligent optimization. Experiments conducted on the GSM8K and Algebra datasets demonstrate that EduPlanner excels in evaluating and optimizing instructional design for curriculum and learning activities. Ablation studies further validate the significance and effectiveness of each component within the framework. Our code is publicly available at https://github.com/Zc0812/Edu_Planner
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interactive Explanations for Reinforcement-Learning Agents</title>
<link>https://arxiv.org/abs/2504.05393</link>
<guid>https://arxiv.org/abs/2504.05393</guid>
<content:encoded><![CDATA[
<div> : 

:
ASQ-ITXRLASQ-ITLTLfASQ-ITASQ-IT <div>
arXiv:2504.05393v1 Announce Type: new 
Abstract: As reinforcement learning methods increasingly amass accomplishments, the need for comprehending their solutions becomes more crucial. Most explainable reinforcement learning (XRL) methods generate a static explanation depicting their developers' intuition of what should be explained and how. In contrast, literature from the social sciences proposes that meaningful explanations are structured as a dialog between the explainer and the explainee, suggesting a more active role for the user and her communication with the agent. In this paper, we present ASQ-IT -- an interactive explanation system that presents video clips of the agent acting in its environment based on queries given by the user that describe temporal properties of behaviors of interest. Our approach is based on formal methods: queries in ASQ-IT's user interface map to a fragment of Linear Temporal Logic over finite traces (LTLf), which we developed, and our algorithm for query processing is based on automata theory. User studies show that end-users can understand and formulate queries in ASQ-IT and that using ASQ-IT assists users in identifying faulty agent behaviors.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-informed Modularized Neural Network for Advanced Building Control by Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.05397</link>
<guid>https://arxiv.org/abs/2504.05397</guid>
<content:encoded><![CDATA[
<div> :  (PIML), ,  (PI-ModNN),  (RL), 

:
(PI-ModNN)PIMLPIML(DRL)DRL31.4% <div>
arXiv:2504.05397v1 Announce Type: new 
Abstract: Physics-informed machine learning (PIML) provides a promising solution for building energy modeling and can serve as a virtual environment to enable reinforcement learning (RL) agents to interact and learn. However, challenges remain in efficiently integrating physics priors, evaluating the effectiveness of physics constraints, balancing model accuracy and physics consistency, and enabling real-world implementation. To address these gaps, this study introduces a Physics-Informed Modularized Neural Network (PI-ModNN), which incorporates physics priors through a physics-informed model structure, loss functions, and hard constraints. A new evaluation metric called "temperature response violation" is developed to quantify the physical consistency of data-driven building dynamic models under varying control inputs and training data sizes. Additionally, a physics prior evaluation framework based on rule importance is proposed to assess the contribution of each individual physics prior, offering guidance on selecting appropriate PIML techniques. Results indicate that incorporating physical priors does not always improve model performance; inappropriate priors may decrease model accuracy and consistency. However, hard constraints are effective in enforcing model consistency. Furthermore, we present a general workflow for developing control-oriented PIML models and integrating them with deep reinforcement learning (DRL). Following this framework, a case study implementing DRL in an office space over three months demonstrates potential energy savings of 31.4%. Finally, we provide a general guideline for integrating data-driven models with advanced building control through a four-step evaluation framework, paving the way for reliable and scalable deployment of advanced building controls.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRATSS: Transformer-Based Task Scheduling System for Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2504.05407</link>
<guid>https://arxiv.org/abs/2504.05407</guid>
<content:encoded><![CDATA[
<div> : Transformer-Based Task Scheduling System (TRATSS)

<br /><br />:

Transformer-Based Task Scheduling System (TRATSS)transformertransformerTRATSSTRATSS <div>
arXiv:2504.05407v1 Announce Type: new 
Abstract: Efficient scheduling remains a critical challenge in various domains, requiring solutions to complex NP-hard optimization problems to achieve optimal resource allocation and maximize productivity. In this paper, we introduce a framework called Transformer-Based Task Scheduling System (TRATSS), designed to address the intricacies of single agent scheduling in graph-based environments. By integrating the latest advancements in reinforcement learning and transformer architecture, TRATSS provides a novel system that outputs optimized task scheduling decisions while dynamically adapting to evolving task requirements and resource availability. Leveraging the self-attention mechanism in transformers, TRATSS effectively captures complex task dependencies, thereby providing solutions with enhanced resource utilization and task completion efficiency. Experimental evaluations on benchmark datasets demonstrate TRATSS's effectiveness in providing high-quality solutions to scheduling problems that involve multiple action profiles.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoK: Frontier AI's Impact on the Cybersecurity Landscape</title>
<link>https://arxiv.org/abs/2504.05408</link>
<guid>https://arxiv.org/abs/2504.05408</guid>
<content:encoded><![CDATA[
<div> : AI

:
AIAIAIAIAIAI <div>
arXiv:2504.05408v1 Announce Type: new 
Abstract: As frontier AI advances rapidly, understanding its impact on cybersecurity and inherent risks is essential to ensuring safe AI evolution (e.g., guiding risk mitigation and informing policymakers). While some studies review AI applications in cybersecurity, none of them comprehensively discuss AI's future impacts or provide concrete recommendations for navigating its safe and secure usage. This paper presents an in-depth analysis of frontier AI's impact on cybersecurity and establishes a systematic framework for risk assessment and mitigation. To this end, we first define and categorize the marginal risks of frontier AI in cybersecurity and then systemically analyze the current and future impacts of frontier AI in cybersecurity, qualitatively and quantitatively. We also discuss why frontier AI likely benefits attackers more than defenders in the short term from equivalence classes, asymmetry, and economic impact. Next, we explore frontier AI's impact on future software system development, including enabling complex hybrid systems while introducing new risks. Based on our findings, we provide security recommendations, including constructing fine-grained benchmarks for risk assessment, designing AI agents for defenses, building security mechanisms and provable defenses for hybrid systems, enhancing pre-deployment security testing and transparency, and strengthening defenses for users. Finally, we present long-term research questions essential for understanding AI's future impacts and unleashing its defensive capabilities.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EP-Diffuser: An Efficient Diffusion Model for Traffic Scene Generation and Prediction via Polynomial Representations</title>
<link>https://arxiv.org/abs/2504.05422</link>
<guid>https://arxiv.org/abs/2504.05422</guid>
<content:encoded><![CDATA[
<div> : EP-Diffuser

:
EP-DiffuserArgoverse 2EP-DiffuserWaymo OpenEP-Diffuserhttps://github.com/continental/EP-Diffuser <div>
arXiv:2504.05422v1 Announce Type: new 
Abstract: As the prediction horizon increases, predicting the future evolution of traffic scenes becomes increasingly difficult due to the multi-modal nature of agent motion. Most state-of-the-art (SotA) prediction models primarily focus on forecasting the most likely future. However, for the safe operation of autonomous vehicles, it is equally important to cover the distribution for plausible motion alternatives. To address this, we introduce EP-Diffuser, a novel parameter-efficient diffusion-based generative model designed to capture the distribution of possible traffic scene evolutions. Conditioned on road layout and agent history, our model acts as a predictor and generates diverse, plausible scene continuations. We benchmark EP-Diffuser against two SotA models in terms of accuracy and plausibility of predictions on the Argoverse 2 dataset. Despite its significantly smaller model size, our approach achieves both highly accurate and plausible traffic scene predictions. We further evaluate model generalization ability in an out-of-distribution (OoD) test setting using Waymo Open dataset and show superior robustness of our approach. The code and model checkpoints can be found here: https://github.com/continental/EP-Diffuser.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Broadcast via Mobile Agents in a Dynamic Network: Interplay of Graph Properties &amp; Agents</title>
<link>https://arxiv.org/abs/2504.05442</link>
<guid>https://arxiv.org/abs/2504.05442</guid>
<content:encoded><![CDATA[
<div> Broadcast


Das2020Broadcast$k+1$$n$$k$$1.1\overline{6}$$\Omega(n)$$1.1\overline{6}$$o(n)$1$\Omega(n/f(n))$$f(n)\to \infty$$n\to \infty$$\rho$$k>0$$\rho>1$$k$Broadcast

<br /><br />: Broadcast$1.1\overline{6}$$o(n)$1$\Omega(n/f(n))$$k>0$$\rho>1$$\rho$$k$ <div>
arXiv:2504.05442v1 Announce Type: new 
Abstract: In this paper, we revisit the problem of \textsc{Broadcast}, introduced by Das, Giachoudis, Luccio, and Markou [OPODIS, 2020], where $k+1$ agents are initially placed on an $n$ node dynamic graph, where $1$ agent has a message that must be broadcast to the remaining $k$ ignorant agents. The original paper studied the relationship between the number of agents needed to solve the problem and the edge density of the graph. The paper presented strong evidence that edge density of a graph, or the number of redundant edges within the graph, may be the correct graph property to accurately differentiate whether $k= o(n)$ agents (low edge density) or $k = \Omega(n)$ agents (high edge density) are needed to solve the problem.
  In this paper, we show that surprisingly, edge density may not in fact be the correct differentiating property. The original paper presents graphs with edge density $1.1\overline{6}$ that require $\Omega(n)$ agents, however, we construct graphs with edge density $> 1.1\overline{6}$ and develop an algorithm to solve the problem on those graphs using only $o(n)$ agents. We subsequently show that the relationship between edge density and number of agents is fairly weak by first constructing graphs with edge density tending to $1$ from above that require $\Omega(n/f(n))$ agents to solve, for any function $f(n) \to \infty$ as $n \to \infty$. We then construct an infinite family of graphs with edge density $< \rho$ requiring exactly $k$ ignorant agents to solve \textsc{Broadcast}, for any $k>0$ and $\rho>1$.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prism: Dynamic and Flexible Benchmarking of LLMs Code Generation with Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2504.05500</link>
<guid>https://arxiv.org/abs/2504.05500</guid>
<content:encoded><![CDATA[
<div> :  (LLMs), Prism, , , 

<br /><br />:
PrismLLMsLLMPrism1) 2) 3) PrismLLMsPrism <div>
arXiv:2504.05500v1 Announce Type: new 
Abstract: The rapid advancement of Large Language Models (LLMs) has outpaced traditional evaluation methods. Static benchmarks fail to capture the depth and breadth of LLM capabilities and eventually become obsolete, while most dynamic approaches either rely too heavily on LLM-based evaluation or remain constrained by predefined test sets. We introduce Prism, a flexible, dynamic benchmarking framework designed for comprehensive LLM assessment. Prism builds on three key components: (1) a tree-based state representation that models evaluation as a Markov Decision Process, (2) a Monte Carlo Tree Search algorithm adapted to uncover challenging evaluation scenarios, and (3) a multi-agent evaluation pipeline that enables simultaneous assessment of diverse capabilities. To ensure robust evaluation, Prism integrates structural measurements of tree exploration patterns with performance metrics across difficulty levels, providing detailed diagnostics of error patterns, test coverage, and solution approaches. Through extensive experiments on five state-of-the-art LLMs, we analyze how model architecture and scale influence code generation performance across varying task difficulties. Our results demonstrate Prism's effectiveness as a dynamic benchmark that evolves with model advancements while offering deeper insights into their limitations.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Hierarchical Reinforcement Learning for Adaptive Traffic Signal Control</title>
<link>https://arxiv.org/abs/2504.05553</link>
<guid>https://arxiv.org/abs/2504.05553</guid>
<content:encoded><![CDATA[
<div> MARLATSCFLFedAvgHFRL

:<br />
ATSCHFRLMARLFLFedAvgHFRLFedAvgHFRL <div>
arXiv:2504.05553v1 Announce Type: new 
Abstract: Multi-agent reinforcement learning (MARL) has shown promise for adaptive traffic signal control (ATSC), enabling multiple intersections to coordinate signal timings in real time. However, in large-scale settings, MARL faces constraints due to extensive data sharing and communication requirements. Federated learning (FL) mitigates these challenges by training shared models without directly exchanging raw data, yet traditional FL methods such as FedAvg struggle with highly heterogeneous intersections. Different intersections exhibit varying traffic patterns, demands, and road structures, so performing FedAvg across all agents is inefficient. To address this gap, we propose Hierarchical Federated Reinforcement Learning (HFRL) for ATSC. HFRL employs clustering-based or optimization-based techniques to dynamically group intersections and perform FedAvg independently within groups of intersections with similar characteristics, enabling more effective coordination and scalability than standard FedAvg. Our experiments on synthetic and real-world traffic networks demonstrate that HFRL not only outperforms both decentralized and standard federated RL approaches but also identifies suitable grouping patterns based on network structure or traffic demand, resulting in a more robust framework for distributed, heterogeneous systems.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciSciGPT: Advancing Human-AI Collaboration in the Science of Science</title>
<link>https://arxiv.org/abs/2504.05559</link>
<guid>https://arxiv.org/abs/2504.05559</guid>
<content:encoded><![CDATA[
<div> : AISciSciGPT

<br /><br />:

SciSciGPTAISciSciGPT-AIAISciSciGPTAIAI <div>
arXiv:2504.05559v1 Announce Type: new 
Abstract: The increasing availability of large-scale datasets has fueled rapid progress across many scientific fields, creating unprecedented opportunities for research and discovery while posing significant analytical challenges. Recent advances in large language models (LLMs) and AI agents have opened new possibilities for human-AI collaboration, offering powerful tools to navigate this complex research landscape. In this paper, we introduce SciSciGPT, an open-source, prototype AI collaborator that uses the science of science as a testbed to explore the potential of LLM-powered research tools. SciSciGPT automates complex workflows, supports diverse analytical approaches, accelerates research prototyping and iteration, and facilitates reproducibility. Through case studies, we demonstrate its ability to streamline a wide range of empirical and analytical research tasks while highlighting its broader potential to advance research. We further propose an LLM Agent capability maturity model for human-AI collaboration, envisioning a roadmap to further improve and expand upon frameworks like SciSciGPT. As AI capabilities continue to evolve, frameworks like SciSciGPT may play increasingly pivotal roles in scientific research and discovery, unlocking further opportunities. At the same time, these new advances also raise critical challenges, from ensuring transparency and ethical use to balancing human and AI contributions. Addressing these issues may shape the future of scientific inquiry and inform how we train the next generation of scientists to thrive in an increasingly AI-integrated research ecosystem.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TW-CRL: Time-Weighted Contrastive Reward Learning for Efficient Inverse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.05585</link>
<guid>https://arxiv.org/abs/2504.05585</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:
episodic(TW-CRL)TW-CRLTW-CRL <div>
arXiv:2504.05585v1 Announce Type: new 
Abstract: Episodic tasks in Reinforcement Learning (RL) often pose challenges due to sparse reward signals and high-dimensional state spaces, which hinder efficient learning. Additionally, these tasks often feature hidden "trap states" -- irreversible failures that prevent task completion but do not provide explicit negative rewards to guide agents away from repeated errors. To address these issues, we propose Time-Weighted Contrastive Reward Learning (TW-CRL), an Inverse Reinforcement Learning (IRL) framework that leverages both successful and failed demonstrations. By incorporating temporal information, TW-CRL learns a dense reward function that identifies critical states associated with success or failure. This approach not only enables agents to avoid trap states but also encourages meaningful exploration beyond simple imitation of expert trajectories. Empirical evaluations on navigation tasks and robotic manipulation benchmarks demonstrate that TW-CRL surpasses state-of-the-art methods, achieving improved efficiency and robustness.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FactGuard: Leveraging Multi-Agent Systems to Generate Answerable and Unanswerable Questions for Enhanced Long-Context LLM Extraction</title>
<link>https://arxiv.org/abs/2504.05607</link>
<guid>https://arxiv.org/abs/2504.05607</guid>
<content:encoded><![CDATA[
<div> agentFactGuard-Bench

<br /><br />:
agent-25,2208K128KFactGuard-Bench61.79%agentLLM <div>
arXiv:2504.05607v1 Announce Type: new 
Abstract: Extractive reading comprehension systems are designed to locate the correct answer to a question within a given text. However, a persistent challenge lies in ensuring these models maintain high accuracy in answering questions while reliably recognizing unanswerable queries. Despite significant advances in large language models (LLMs) for reading comprehension, this issue remains critical, particularly as the length of supported contexts continues to expand. To address this challenge, we propose an innovative data augmentation methodology grounded in a multi-agent collaborative framework. Unlike traditional methods, such as the costly human annotation process required for datasets like SQuAD 2.0, our method autonomously generates evidence-based question-answer pairs and systematically constructs unanswerable questions. Using this methodology, we developed the FactGuard-Bench dataset, which comprises 25,220 examples of both answerable and unanswerable question scenarios, with context lengths ranging from 8K to 128K. Experimental evaluations conducted on seven popular LLMs reveal that even the most advanced models achieve only 61.79% overall accuracy. Furthermore, we emphasize the importance of a model's ability to reason about unanswerable questions to avoid generating plausible but incorrect answers. By implementing efficient data selection and generation within the multi-agent collaborative framework, our method significantly reduces the traditionally high costs associated with manual annotation and provides valuable insights for the training and optimization of LLMs.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Agnostic Policy Explanations with Large Language Models</title>
<link>https://arxiv.org/abs/2504.05625</link>
<guid>https://arxiv.org/abs/2504.05625</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:
 <div>
arXiv:2504.05625v1 Announce Type: new 
Abstract: Intelligent agents, such as robots, are increasingly deployed in real-world, human-centric environments. To foster appropriate human trust and meet legal and ethical standards, these agents must be able to explain their behavior. However, state-of-the-art agents are typically driven by black-box models like deep neural networks, limiting their interpretability. We propose a method for generating natural language explanations of agent behavior based only on observed states and actions -- without access to the agent's underlying model. Our approach learns a locally interpretable surrogate model of the agent's behavior from observations, which then guides a large language model to generate plausible explanations with minimal hallucination. Empirical results show that our method produces explanations that are more comprehensible and correct than those from baselines, as judged by both language models and human evaluators. Furthermore, we find that participants in a user study more accurately predicted the agent's future actions when given our explanations, suggesting improved understanding of agent behavior.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DBOT: Artificial Intelligence for Systematic Long-Term Investing</title>
<link>https://arxiv.org/abs/2504.05639</link>
<guid>https://arxiv.org/abs/2504.05639</guid>
<content:encoded><![CDATA[
<div> long-term investing, generative AI, DBOT, Aswath Damodaran, financial industry

<br /><br />:
DBOTAswath DamodaranDBOTDBOTDamodaranDBOT DamodaranDBOT <div>
arXiv:2504.05639v1 Announce Type: new 
Abstract: Long-term investing was previously seen as requiring human judgment. With the advent of generative artificial intelligence (AI) systems, automated systematic long-term investing is now feasible. In this paper, we present DBOT, a system whose goal is to reason about valuation like Aswath Damodaran, who is a unique expert in the investment arena in terms of having published thousands of valuations on companies in addition to his numerous writings on the topic, which provide ready training data for an AI system. DBOT can value any publicly traded company. DBOT can also be back-tested, making its behavior and performance amenable to scientific inquiry. We compare DBOT to its analytic parent, Damodaran, and highlight the research challenges involved in raising its current capability to that of Damodaran's. Finally, we examine the implications of DBOT-like AI agents for the financial industry, especially how they will impact the role of human analysts in valuation.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Archival Descriptions with Federated Intelligence of LLMs</title>
<link>https://arxiv.org/abs/2504.05711</link>
<guid>https://arxiv.org/abs/2504.05711</guid>
<content:encoded><![CDATA[
<div> : agentic AI

<br />
: agentic AIagentic AILLMLLM <div>
arXiv:2504.05711v1 Announce Type: new 
Abstract: Enforcing archival standards requires specialized expertise, and manually creating metadata descriptions for archival materials is a tedious and error-prone task. This work aims at exploring the potential of agentic AI and large language models (LLMs) in addressing the challenges of implementing a standardized archival description process. To this end, we introduce an agentic AI-driven system for automated generation of high-quality metadata descriptions of archival materials. We develop a federated optimization approach that unites the intelligence of multiple LLMs to construct optimal archival metadata. We also suggest methods to overcome the challenges associated with using LLMs for consistent metadata generation. To evaluate the feasibility and effectiveness of our techniques, we conducted extensive experiments using a real-world dataset of archival materials, which covers a variety of document types and data formats. The evaluation results demonstrate the feasibility of our techniques and highlight the superior performance of the federated optimization approach compared to single-model solutions in metadata quality and reliability.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Single-Agent vs. Multi-Agent LLM Strategies for Automated Student Reflection Assessment</title>
<link>https://arxiv.org/abs/2504.05716</link>
<guid>https://arxiv.org/abs/2504.05716</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)

:
LLMsLLMs3775,278LLMLLMsAI <div>
arXiv:2504.05716v1 Announce Type: new 
Abstract: We explore the use of Large Language Models (LLMs) for automated assessment of open-text student reflections and prediction of academic performance. Traditional methods for evaluating reflections are time-consuming and may not scale effectively in educational settings. In this work, we employ LLMs to transform student reflections into quantitative scores using two assessment strategies (single-agent and multi-agent) and two prompting techniques (zero-shot and few-shot). Our experiments, conducted on a dataset of 5,278 reflections from 377 students over three academic terms, demonstrate that the single-agent with few-shot strategy achieves the highest match rate with human evaluations. Furthermore, models utilizing LLM-assessed reflection scores outperform baselines in both at-risk student identification and grade prediction tasks. These findings suggest that LLMs can effectively automate reflection assessment, reduce educators' workload, and enable timely support for students who may need additional assistance. Our work emphasizes the potential of integrating advanced generative AI technologies into educational practices to enhance student engagement and academic success.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust and Efficient Average Consensus with Non-Coherent Over-the-Air Aggregation</title>
<link>https://arxiv.org/abs/2504.05729</link>
<guid>https://arxiv.org/abs/2504.05729</guid>
<content:encoded><![CDATA[
<div> : Agent

<br />
:
AgentD-PGDD-PGD <div>
arXiv:2504.05729v1 Announce Type: new 
Abstract: Non-coherent over-the-air (OTA) computation has garnered increasing attention for its advantages in facilitating information aggregation among distributed agents in resource-constrained networks without requiring precise channel estimation. A promising application scenario of this method is distributed average consensus in wireless multi-agent systems. However, in such scenario, non-coherent interference from concurrent OTA transmissions can introduce bias in the consensus value. To address this issue, we develop a robust distributed average consensus algorithm by formulating the consensus problem as a distributed optimization problem. Using decentralized projected gradient descent (D-PGD), our proposed algorithm can achieve unbiased mean square average consensus even in the presence of non-coherent interference and noise. Additionally, we implement transmit power control and receive scaling mechanisms to further accelerate convergence. Simulation results demonstrate that our method can significantly enhance the convergence speed of the D-PGD algorithm for OTA average consensus without compromising accuracy.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling Human-AI Teaming: A Review and Outlook</title>
<link>https://arxiv.org/abs/2504.05755</link>
<guid>https://arxiv.org/abs/2504.05755</guid>
<content:encoded><![CDATA[
<div> : (AI)-AI

:

AI-AIAIAI-AI <div>
arXiv:2504.05755v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) is advancing at an unprecedented pace, with clear potential to enhance decision-making and productivity. Yet, the collaborative decision-making process between humans and AI remains underdeveloped, often falling short of its transformative possibilities. This paper explores the evolution of AI agents from passive tools to active collaborators in human-AI teams, emphasizing their ability to learn, adapt, and operate autonomously in complex environments. This paradigm shifts challenges traditional team dynamics, requiring new interaction protocols, delegation strategies, and responsibility distribution frameworks. Drawing on Team Situation Awareness (SA) theory, we identify two critical gaps in current human-AI teaming research: the difficulty of aligning AI agents with human values and objectives, and the underutilization of AI's capabilities as genuine team members. Addressing these gaps, we propose a structured research outlook centered on four key aspects of human-AI teaming: formulation, coordination, maintenance, and training. Our framework highlights the importance of shared mental models, trust-building, conflict resolution, and skill adaptation for effective teaming. Furthermore, we discuss the unique challenges posed by varying team compositions, goals, and complexities. This paper provides a foundational agenda for future research and practical design of sustainable, high-performing human-AI teams.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Generative AI Agents Effective Personalized Financial Advisors?</title>
<link>https://arxiv.org/abs/2504.05862</link>
<guid>https://arxiv.org/abs/2504.05862</guid>
<content:encoded><![CDATA[
<div> 

:<br />
LLM1) 2) 3) 64LLM-LLMLLM-LLM <div>
arXiv:2504.05862v1 Announce Type: new 
Abstract: Large language model-based agents are becoming increasingly popular as a low-cost mechanism to provide personalized, conversational advice, and have demonstrated impressive capabilities in relatively simple scenarios, such as movie recommendations. But how do these agents perform in complex high-stakes domains, where domain expertise is essential and mistakes carry substantial risk? This paper investigates the effectiveness of LLM-advisors in the finance domain, focusing on three distinct challenges: (1) eliciting user preferences when users themselves may be unsure of their needs, (2) providing personalized guidance for diverse investment preferences, and (3) leveraging advisor personality to build relationships and foster trust. Via a lab-based user study with 64 participants, we show that LLM-advisors often match human advisor performance when eliciting preferences, although they can struggle to resolve conflicting user needs. When providing personalized advice, the LLM was able to positively influence user behavior, but demonstrated clear failure modes. Our results show that accurate preference elicitation is key, otherwise, the LLM-advisor has little impact, or can even direct the investor toward unsuitable assets. More worryingly, users appear insensitive to the quality of advice being given, or worse these can have an inverse relationship. Indeed, users reported a preference for and increased satisfaction as well as emotional trust with LLMs adopting an extroverted persona, even though those agents provided worse advice.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Guide: A Simple Agent Behavioral Watermarking Framework</title>
<link>https://arxiv.org/abs/2504.05871</link>
<guid>https://arxiv.org/abs/2504.05871</guid>
<content:encoded><![CDATA[
<div> : Agent Guide

:
LLMAgent GuidezAgent Guide <div>
arXiv:2504.05871v1 Announce Type: new 
Abstract: The increasing deployment of intelligent agents in digital ecosystems, such as social media platforms, has raised significant concerns about traceability and accountability, particularly in cybersecurity and digital content protection. Traditional large language model (LLM) watermarking techniques, which rely on token-level manipulations, are ill-suited for agents due to the challenges of behavior tokenization and information loss during behavior-to-action translation. To address these issues, we propose Agent Guide, a novel behavioral watermarking framework that embeds watermarks by guiding the agent's high-level decisions (behavior) through probability biases, while preserving the naturalness of specific executions (action). Our approach decouples agent behavior into two levels, behavior (e.g., choosing to bookmark) and action (e.g., bookmarking with specific tags), and applies watermark-guided biases to the behavior probability distribution. We employ a z-statistic-based statistical analysis to detect the watermark, ensuring reliable extraction over multiple rounds. Experiments in a social media scenario with diverse agent profiles demonstrate that Agent Guide achieves effective watermark detection with a low false positive rate. Our framework provides a practical and robust solution for agent watermarking, with applications in identifying malicious agents and protecting proprietary agent systems.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rolling Horizon Coverage Control with Collaborative Autonomous Agents</title>
<link>https://arxiv.org/abs/2504.05883</link>
<guid>https://arxiv.org/abs/2504.05883</guid>
<content:encoded><![CDATA[
<div> : 

:
 <div>
arXiv:2504.05883v1 Announce Type: new 
Abstract: This work proposes a coverage controller that enables an aerial team of distributed autonomous agents to collaboratively generate non-myopic coverage plans over a rolling finite horizon, aiming to cover specific points on the surface area of a 3D object of interest. The collaborative coverage problem, formulated, as a distributed model predictive control problem, optimizes the agents' motion and camera control inputs, while considering inter-agent constraints aiming at reducing work redundancy. The proposed coverage controller integrates constraints based on light-path propagation techniques to predict the parts of the object's surface that are visible with regard to the agents' future anticipated states. This work also demonstrates how complex, non-linear visibility assessment constraints can be converted into logical expressions that are embedded as binary constraints into a mixed-integer optimization framework. The proposed approach has been demonstrated through simulations and practical applications for inspecting buildings with unmanned aerial vehicles (UAVs).
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jointly-optimized Trajectory Generation and Camera Control for 3D Coverage Planning</title>
<link>https://arxiv.org/abs/2504.05887</link>
<guid>https://arxiv.org/abs/2504.05887</guid>
<content:encoded><![CDATA[
<div> : (UAV), , , , 

:
(UAV)3DUAV3DUAV<br /><br /> <div>
arXiv:2504.05887v1 Announce Type: new 
Abstract: This work proposes a jointly optimized trajectory generation and camera control approach, enabling an autonomous agent, such as an unmanned aerial vehicle (UAV) operating in 3D environments, to plan and execute coverage trajectories that maximally cover the surface area of a 3D object of interest. Specifically, the UAV's kinematic and camera control inputs are jointly optimized over a rolling planning horizon to achieve complete 3D coverage of the object. The proposed controller incorporates ray-tracing into the planning process to simulate the propagation of light rays, thereby determining the visible parts of the object through the UAV's camera. This integration enables the generation of precise look-ahead coverage trajectories. The coverage planning problem is formulated as a rolling finite-horizon optimal control problem and solved using mixed-integer programming techniques. Extensive real-world and synthetic experiments validate the performance of the proposed approach.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smart Exploration in Reinforcement Learning using Bounded Uncertainty Models</title>
<link>https://arxiv.org/abs/2504.05978</link>
<guid>https://arxiv.org/abs/2504.05978</guid>
<content:encoded><![CDATA[
<div> <br /><br />:
QQQBMDP <div>
arXiv:2504.05978v1 Announce Type: new 
Abstract: Reinforcement learning (RL) is a powerful tool for decision-making in uncertain environments, but it often requires large amounts of data to learn an optimal policy. We propose using prior model knowledge to guide the exploration process to speed up this learning process. This model knowledge comes in the form of a model set to which the true transition kernel and reward function belong. We optimize over this model set to obtain upper and lower bounds on the Q-function, which are then used to guide the exploration of the agent. We provide theoretical guarantees on the convergence of the Q-function to the optimal Q-function under the proposed class of exploring policies. Furthermore, we also introduce a data-driven regularized version of the model set optimization problem that ensures the convergence of the class of exploring policies to the optimal policy. Lastly, we show that when the model set has a specific structure, namely the bounded-parameter MDP (BMDP) framework, the regularized model set optimization problem becomes convex and simple to implement. In this setting, we also show that we obtain finite-time convergence to the optimal policy under additional assumptions. We demonstrate the effectiveness of the proposed exploration strategy in a simulation study. The results indicate that the proposed method can significantly speed up the learning process in reinforcement learning.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAI: An Open, Bug Bounty-Ready Cybersecurity AI</title>
<link>https://arxiv.org/abs/2504.06017</link>
<guid>https://arxiv.org/abs/2504.06017</guid>
<content:encoded><![CDATA[
<div> : Cybersecurity AI (CAI)CTF

<br /><br />:

2028Cybersecurity AI (CAI)CAICTF360011AI vs HumanCTFCAIAI20$750LLMCAIHack The Box30500156HITLCAIAI <div>
arXiv:2504.06017v1 Announce Type: new 
Abstract: By 2028 most cybersecurity actions will be autonomous, with humans teleoperating. We present the first classification of autonomy levels in cybersecurity and introduce Cybersecurity AI (CAI), an open-source framework that democratizes advanced security testing through specialized AI agents. Through rigorous empirical evaluation, we demonstrate that CAI consistently outperforms state-of-the-art results in CTF benchmarks, solving challenges across diverse categories with significantly greater efficiency -up to 3,600x faster than humans in specific tasks and averaging 11x faster overall. CAI achieved first place among AI teams and secured a top-20 position worldwide in the "AI vs Human" CTF live Challenge, earning a monetary reward of $750. Based on our results, we argue against LLM-vendor claims about limited security capabilities. Beyond cybersecurity competitions, CAI demonstrates real-world effectiveness, reaching top-30 in Spain and top-500 worldwide on Hack The Box within a week, while dramatically reducing security testing costs by an average of 156x. Our framework transcends theoretical benchmarks by enabling non-professionals to discover significant security bugs (CVSS 4.3-7.5) at rates comparable to experts during bug bounty exercises. By combining modular agent design with seamless tool integration and human oversight (HITL), CAI addresses critical market gaps, offering organizations of all sizes access to AI-powered bug bounty security testing previously available only to well-resourced firms -thereby challenging the oligopolistic ecosystem currently dominated by major bug bounty platforms.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Virtual Agent Tutors in Sheltered Workshops: A Feasibility Study on Attention Training for Individuals with Intellectual Disabilities</title>
<link>https://arxiv.org/abs/2504.06031</link>
<guid>https://arxiv.org/abs/2504.06031</guid>
<content:encoded><![CDATA[
<div> Robo-Camp

<br />
:
(ADHD)Robo-Camp(ID)Robo-Camp <div>
arXiv:2504.06031v1 Announce Type: new 
Abstract: In this work, we evaluate the feasibility of socially assistive virtual agent-based cognitive training for people with intellectual disabilities (ID) in a sheltered workshop. The Robo- Camp system, originally developed for children with Attention Deficit Hyperactivity Disorder (ADHD), is adapted based on the results of a pilot study in which we identified barriers and collected feedback from workshop staff. In a subsequent study, we investigate the aspects of usability, technical reliability, attention training capabilities and novelty effect in the feasibility of integrating the RoboCamp system.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time LaCAM</title>
<link>https://arxiv.org/abs/2504.06091</link>
<guid>https://arxiv.org/abs/2504.06091</guid>
<content:encoded><![CDATA[
<div> Multi-Agent Path Finding (MAPF)LaCAM

:
(Real-Time MAPF)MAPF horizonLaCAMOkumura, 2023LaCAMMAPFLaCAMMAPF<br /><br /> <div>
arXiv:2504.06091v1 Announce Type: new 
Abstract: The vast majority of Multi-Agent Path Finding (MAPF) methods with completeness guarantees require planning full horizon paths. However, planning full horizon paths can take too long and be impractical in real-world applications. Instead, real-time planning and execution, which only allows the planner a finite amount of time before executing and replanning, is more practical for real world multi-agent systems. Several methods utilize real-time planning schemes but none are provably complete, which leads to livelock or deadlock. Our main contribution is to show the first Real-Time MAPF method with provable completeness guarantees. We do this by leveraging LaCAM (Okumura 2023) in an incremental fashion. Our results show how we can iteratively plan for congested environments with a cutoff time of milliseconds while still maintaining the same success rate as full horizon LaCAM. We also show how it can be used with a single-step learned MAPF policy. The proposed Real-Time LaCAM also provides us with a general mechanism for using iterative constraints for completeness in future real-time MAPF algorithms.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Vehicle Routing via AI-Initialized Genetic Algorithms</title>
<link>https://arxiv.org/abs/2504.06126</link>
<guid>https://arxiv.org/abs/2504.06126</guid>
<content:encoded><![CDATA[
<div> Vehicle Routing Problems (VRP)Reinforcement LearningGenetic AlgorithmsEvolutionary Algorithm with Reinforcement Learning Initialization (EARLI)Real-time Optimization

:
VRPEARLIEARLI150010EARLIAI <div>
arXiv:2504.06126v1 Announce Type: new 
Abstract: Vehicle Routing Problems (VRP) are an extension of the Traveling Salesperson Problem and are a fundamental NP-hard challenge in combinatorial optimization. Solving VRP in real-time at large scale has become critical in numerous applications, from growing markets like last-mile delivery to emerging use-cases like interactive logistics planning. Such applications involve solving similar problem instances repeatedly, yet current state-of-the-art solvers treat each instance on its own without leveraging previous examples. We introduce a novel optimization framework that uses a reinforcement learning agent - trained on prior instances - to quickly generate initial solutions, which are then further optimized by genetic algorithms. Our framework, Evolutionary Algorithm with Reinforcement Learning Initialization (EARLI), consistently outperforms current state-of-the-art solvers across various time scales. For example, EARLI handles vehicle routing with 500 locations within 1s, 10x faster than current solvers for the same solution quality, enabling applications like real-time and interactive routing. EARLI can generalize to new data, as demonstrated on real e-commerce delivery data of a previously unseen city. Our hybrid framework presents a new way to combine reinforcement learning and genetic algorithms, paving the road for closer interdisciplinary collaboration between AI and optimization communities towards real-time optimization in diverse domains.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralizing AI Memory: SHIMI, a Semantic Hierarchical Memory Index for Scalable Agent Reasoning</title>
<link>https://arxiv.org/abs/2504.06135</link>
<guid>https://arxiv.org/abs/2504.06135</guid>
<content:encoded><![CDATA[
<div> : Retrieval-Augmented Generation (RAG), SHIMI, , , 

:
SHIMISemantic Hierarchical Memory IndexRetrieval-Augmented Generation (RAG)SHIMIAISHIMIMerkle-DAGBloomCRDTSHIMI <div>
arXiv:2504.06135v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) and vector-based search have become foundational tools for memory in AI systems, yet they struggle with abstraction, scalability, and semantic precision - especially in decentralized environments. We present SHIMI (Semantic Hierarchical Memory Index), a unified architecture that models knowledge as a dynamically structured hierarchy of concepts, enabling agents to retrieve information based on meaning rather than surface similarity. SHIMI organizes memory into layered semantic nodes and supports top-down traversal from abstract intent to specific entities, offering more precise and explainable retrieval. Critically, SHIMI is natively designed for decentralized ecosystems, where agents maintain local memory trees and synchronize them asynchronously across networks. We introduce a lightweight sync protocol that leverages Merkle-DAG summaries, Bloom filters, and CRDT-style conflict resolution to enable partial synchronization with minimal overhead. Through benchmark experiments and use cases involving decentralized agent collaboration, we demonstrate SHIMI's advantages in retrieval accuracy, semantic fidelity, and scalability - positioning it as a core infrastructure layer for decentralized cognitive systems.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multimedia Analytics Model for the Foundation Model Era</title>
<link>https://arxiv.org/abs/2504.06138</link>
<guid>https://arxiv.org/abs/2504.06138</guid>
<content:encoded><![CDATA[
<div> : Foundation Models-AI

<br /><br />:
Foundation ModelsAIAIAIAI <div>
arXiv:2504.06138v1 Announce Type: new 
Abstract: The rapid advances in Foundation Models and agentic Artificial Intelligence are transforming multimedia analytics by enabling richer, more sophisticated interactions between humans and analytical systems. Existing conceptual models for visual and multimedia analytics, however, do not adequately capture the complexity introduced by these powerful AI paradigms. To bridge this gap, we propose a comprehensive multimedia analytics model specifically designed for the foundation model era. Building upon established frameworks from visual analytics, multimedia analytics, knowledge generation, analytic task definition, mixed-initiative guidance, and human-in-the-loop reinforcement learning, our model emphasizes integrated human-AI teaming based on visual analytics agents from both technical and conceptual perspectives. Central to the model is a seamless, yet explicitly separable, interaction channel between expert users and semi-autonomous analytical processes, ensuring continuous alignment between user intent and AI behavior. The model addresses practical challenges in sensitive domains such as intelligence analysis, investigative journalism, and other fields handling complex, high-stakes data. We illustrate through detailed case studies how our model facilitates deeper understanding and targeted improvement of multimedia analytics solutions. By explicitly capturing how expert users can optimally interact with and guide AI-powered multimedia analytics systems, our conceptual framework sets a clear direction for system design, comparison, and future research.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deploying Chatbots in Customer Service: Adoption Hurdles and Simple Remedies</title>
<link>https://arxiv.org/abs/2504.06145</link>
<guid>https://arxiv.org/abs/2504.06145</guid>
<content:encoded><![CDATA[
<div> chatbot

:
<br />
chatbot(i) (ii) / <div>
arXiv:2504.06145v1 Announce Type: new 
Abstract: Despite recent advances in Artificial Intelligence, the use of chatbot technology in customer service continues to face adoption hurdles. This paper explores reasons for these adoption hurdles and tests several service design levers to increase chatbot uptake. We use incentivized online experiments to study chatbot uptake in a variety of scenarios. The results of these experiments are threefold. First, people respond positively to improvements in chatbot performance; however, the chatbot channel is utilized less frequently than expected-time minimization would predict. A key driver of this underutilization is the reluctance to engage with a gatekeeper process, i.e., a process with an imperfect initial service stage and possible transfer to a second, expert service stage -- a behavior we term "gatekeeper aversion". We show that gatekeeper aversion can be further amplified by a secondary hurdle, algorithm aversion. Second, chatbot uptake can be increased by providing customers with average waiting times in the chatbot channel, as well as by being more transparent about chatbot capabilities and limitations. Third, methodologically, we show that chatbot adoption can depend on experimental implementation. In particular, chatbot adoption decreases further as (i) stakes are increased, (ii) the human/algorithmic nature of the server is manipulated with more realism. Our results suggest that firms should continue to prioritize investments in chatbot technology. However, less expensive, process-related interventions can also be effective. These may include being more transparent about the types of queries that are (or are not) suitable for chatbots, emphasizing chatbot reliability and quick resolution times, as well as providing faster live agent access to customers who experienced chatbot failure.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric Capabilities in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2504.06148</link>
<guid>https://arxiv.org/abs/2504.06148</guid>
<content:encoded><![CDATA[
<div> : V-MAGE

:
MLLMsV-MAGE30V-MAGEMLLMsMLLMshttps://github.com/CSU-JPG/V-MAGE <div>
arXiv:2504.06148v1 Announce Type: new 
Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have led to significant improvements across various multimodal benchmarks. However, as evaluations shift from static datasets to open-world, dynamic environments, current game-based benchmarks remain inadequate because they lack visual-centric tasks and fail to assess the diverse reasoning skills required for real-world decision-making. To address this, we introduce Visual-centric Multiple Abilities Game Evaluation (V-MAGE), a game-based evaluation framework designed to assess visual reasoning capabilities of MLLMs. V-MAGE features five diverse games with 30+ handcrafted levels, testing models on core visual skills such as positioning, trajectory tracking, timing, and visual memory, alongside higher-level reasoning like long-term planning and deliberation. We use V-MAGE to evaluate leading MLLMs, revealing significant challenges in their visual perception and reasoning. In all game environments, the top-performing MLLMs, as determined by Elo rating comparisons, exhibit a substantial performance gap compared to humans. Our findings highlight critical limitations, including various types of perceptual errors made by the models, and suggest potential avenues for improvement from an agent-centric perspective, such as refining agent strategies and addressing perceptual inaccuracies. Code is available at https://github.com/CSU-JPG/V-MAGE.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear Regulator-Based Synchronization of Positive Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2504.06169</link>
<guid>https://arxiv.org/abs/2504.06169</guid>
<content:encoded><![CDATA[
<div> : arXiv:2504.06169v1, , , , 

:
<br /><br /> <div>
arXiv:2504.06169v1 Announce Type: new 
Abstract: This paper addresses the positive synchronization of interconnected systems on undirected graphs. For homogeneous positive systems, a static feedback protocol design is proposed, based on the Linear Regulator problem. The solution to the algebraic equation associated to the stabilizing policy can be found using a linear program. Necessary and sufficient conditions on the positivity of each agent's trajectory for all nonnegative initial conditions are also provided. Simulations on large regular graphs with different nodal degree illustrate the proposed results.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SkillFlow: Efficient Skill and Code Transfer Through Communication in Adapting AI Agents</title>
<link>https://arxiv.org/abs/2504.06188</link>
<guid>https://arxiv.org/abs/2504.06188</guid>
<content:encoded><![CDATA[
<div> : SkillFlowAI

:
SkillFlowAISkillFlowSkillFlow24.8%p-value = $6.4\times10^{-3}$ <div>
arXiv:2504.06188v1 Announce Type: new 
Abstract: AI agents are autonomous systems that can execute specific tasks based on predefined programming. Here, we present SkillFlow, a modular, technology-agnostic framework that allows agents to expand their functionality in an ad-hoc fashion by acquiring new skills from their environment or other agents. We present a theoretical model that examines under which conditions this framework would be beneficial, and we then explore SkillFlow's ability to accelerate task completion and lead to lower cumulative costs in a real-world application, namely scheduling agents for calendar events. We demonstrate that within a few iterations, SkillFlow leads to considerable (24.8%, p-value = $6.4\times10^{-3}$) gains in time and cost, especially when the communication cost is high. Finally, we draw analogies from well-studied biological systems and compare this framework to that of lateral gene transfer, a significant process of adaptation and evolution in novel environments.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TxGemma: Efficient and Agentic LLMs for Therapeutics</title>
<link>https://arxiv.org/abs/2504.06196</link>
<guid>https://arxiv.org/abs/2504.06196</guid>
<content:encoded><![CDATA[
<div> TxGemma

<br /><br />:
TxGemmaTxGemma2B9B27BGemma-266TxGemma64455026TxGemmaTxGemmaAgentic-TxGemini 2.5Agentic-TxHumanity's Last ExamGPQAChemBench-PreferenceChemBench-Mini <div>
arXiv:2504.06196v1 Announce Type: new 
Abstract: Therapeutic development is a costly and high-risk endeavor that is often plagued by high failure rates. To address this, we introduce TxGemma, a suite of efficient, generalist large language models (LLMs) capable of therapeutic property prediction as well as interactive reasoning and explainability. Unlike task-specific models, TxGemma synthesizes information from diverse sources, enabling broad application across the therapeutic development pipeline. The suite includes 2B, 9B, and 27B parameter models, fine-tuned from Gemma-2 on a comprehensive dataset of small molecules, proteins, nucleic acids, diseases, and cell lines. Across 66 therapeutic development tasks, TxGemma achieved superior or comparable performance to the state-of-the-art generalist model on 64 (superior on 45), and against state-of-the-art specialist models on 50 (superior on 26). Fine-tuning TxGemma models on therapeutic downstream tasks, such as clinical trial adverse event prediction, requires less training data than fine-tuning base LLMs, making TxGemma suitable for data-limited applications. Beyond these predictive capabilities, TxGemma features conversational models that bridge the gap between general LLMs and specialized property predictors. These allow scientists to interact in natural language, provide mechanistic reasoning for predictions based on molecular structure, and engage in scientific discussions. Building on this, we further introduce Agentic-Tx, a generalist therapeutic agentic system powered by Gemini 2.5 that reasons, acts, manages diverse workflows, and acquires external domain knowledge. Agentic-Tx surpasses prior leading models on the Humanity's Last Exam benchmark (Chemistry & Biology) with 52.3% relative improvement over o3-mini (high) and 26.7% over o3-mini (high) on GPQA (Chemistry) and excels with improvements of 6.3% (ChemBench-Preference) and 2.4% (ChemBench-Mini) over o3-mini (high).
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Work Capacity of Channels with Memory: Maximum Extractable Work in Percept-Action Loops</title>
<link>https://arxiv.org/abs/2504.06209</link>
<guid>https://arxiv.org/abs/2504.06209</guid>
<content:encoded><![CDATA[
<div> : -

:
- <div>
arXiv:2504.06209v1 Announce Type: new 
Abstract: Predicting future observations plays a central role in machine learning, biology, economics, and many other fields. It lies at the heart of organizational principles such as the variational free energy principle and has even been shown -- based on the second law of thermodynamics -- to be necessary for reaching the fundamental energetic limits of sequential information processing. While the usefulness of the predictive paradigm is undisputed, complex adaptive systems that interact with their environment are more than just predictive machines: they have the power to act upon their environment and cause change. In this work, we develop a framework to analyze the thermodynamics of information processing in percept-action loops -- a model of agent-environment interaction -- allowing us to investigate the thermodynamic implications of actions and percepts on equal footing. To this end, we introduce the concept of work capacity -- the maximum rate at which an agent can expect to extract work from its environment. Our results reveal that neither of two previously established design principles for work-efficient agents -- maximizing predictive power and forgetting past actions -- remains optimal in environments where actions have observable consequences. Instead, a trade-off emerges: work-efficient agents must balance prediction and forgetting, as remembering past actions can reduce the available free energy. This highlights a fundamental departure from the thermodynamics of passive observation, suggesting that prediction and energy efficiency may be at odds in active learning systems.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FEABench: Evaluating Language Models on Multiphysics Reasoning Ability</title>
<link>https://arxiv.org/abs/2504.06260</link>
<guid>https://arxiv.org/abs/2504.06260</guid>
<content:encoded><![CDATA[
<div> : FEABenchCOMSOL Multiphysics<br /><br />:

FEABenchLLM(FEA)LLMsFEACOMSOL Multiphysics(API)API88%FEALLMshttps://github.com/google/feabench  <div>
arXiv:2504.06260v1 Announce Type: new 
Abstract: Building precise simulations of the real world and invoking numerical solvers to answer quantitative problems is an essential requirement in engineering and science. We present FEABench, a benchmark to evaluate the ability of large language models (LLMs) and LLM agents to simulate and solve physics, mathematics and engineering problems using finite element analysis (FEA). We introduce a comprehensive evaluation scheme to investigate the ability of LLMs to solve these problems end-to-end by reasoning over natural language problem descriptions and operating COMSOL Multiphysics$^\circledR$, an FEA software, to compute the answers. We additionally design a language model agent equipped with the ability to interact with the software through its Application Programming Interface (API), examine its outputs and use tools to improve its solutions over multiple iterations. Our best performing strategy generates executable API calls 88% of the time. LLMs that can successfully interact with and operate FEA software to solve problems such as those in our benchmark would push the frontiers of automation in engineering. Acquiring this capability would augment LLMs' reasoning skills with the precision of numerical solvers and advance the development of autonomous systems that can tackle complex problems in the real world. The code is available at https://github.com/google/feabench
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BC-ADMM: An Efficient Non-convex Constrained Optimizer with Robotic Applications</title>
<link>https://arxiv.org/abs/2504.05465</link>
<guid>https://arxiv.org/abs/2504.05465</guid>
<content:encoded><![CDATA[
<div> : BC-ADMM

:

BC-ADMMBC-ADMMAlternating Direction Method of MultiplierADMMBC-ADMMBC-ADMM <div>
arXiv:2504.05465v1 Announce Type: cross 
Abstract: Non-convex constrained optimizations are ubiquitous in robotic applications such as multi-agent navigation, UAV trajectory optimization, and soft robot simulation. For this problem class, conventional optimizers suffer from small step sizes and slow convergence. We propose BC-ADMM, a variant of Alternating Direction Method of Multiplier (ADMM), that can solve a class of non-convex constrained optimizations with biconvex constraint relaxation. Our algorithm allows larger step sizes by breaking the problem into small-scale sub-problems that can be easily solved in parallel. We show that our method has both theoretical convergence speed guarantees and practical convergence guarantees in the asymptotic sense. Through numerical experiments in a row of four robotic applications, we show that BC-ADMM has faster convergence than conventional gradient descent and Newton's method in terms of wall clock time.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Sub-Optimal Data for Human-in-the-Loop Reinforcement Learning</title>
<link>https://arxiv.org/abs/2405.00746</link>
<guid>https://arxiv.org/abs/2405.00746</guid>
<content:encoded><![CDATA[
<div>  (Reinforcement Learning) (reward function) (human-in-the-loop RL) (feedback efficiency) (Sub-optimal Data Pre-training, SDP)

:<br />
SDPRLSDPSDPRL <div>
arXiv:2405.00746v2 Announce Type: replace 
Abstract: To create useful reinforcement learning (RL) agents, step zero is to design a suitable reward function that captures the nuances of the task. However, reward engineering can be a difficult and time-consuming process. Instead, human-in-the-loop RL methods hold the promise of learning reward functions from human feedback. Despite recent successes, many of the human-in-the-loop RL methods still require numerous human interactions to learn successful reward functions. To improve the feedback efficiency of human-in-the-loop RL methods (i.e., require less human interaction), this paper introduces Sub-optimal Data Pre-training, SDP, an approach that leverages reward-free, sub-optimal data to improve scalar- and preference-based RL algorithms. In SDP, we start by pseudo-labeling all low-quality data with the minimum environment reward. Through this process, we obtain reward labels to pre-train our reward model without requiring human labeling or preferences. This pre-training phase provides the reward model a head start in learning, enabling it to recognize that low-quality transitions should be assigned low rewards. Through extensive experiments with both simulated and human teachers, we find that SDP can at least meet, but often significantly improve, state of the art human-in-the-loop RL performance across a variety of simulated robotic tasks.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POGEMA: A Benchmark Platform for Cooperative Multi-Agent Pathfinding</title>
<link>https://arxiv.org/abs/2407.14931</link>
<guid>https://arxiv.org/abs/2407.14931</guid>
<content:encoded><![CDATA[
<div> : (MARL), , , POGEMA, 

<br /><br />:
MARLPOGEMAMARL <div>
arXiv:2407.14931v3 Announce Type: replace 
Abstract: Multi-agent reinforcement learning (MARL) has recently excelled in solving challenging cooperative and competitive multi-agent problems in various environments, typically involving a small number of agents and full observability. Moreover, a range of crucial robotics-related tasks, such as multi-robot pathfinding, which have traditionally been approached with classical non-learnable methods (e.g., heuristic search), are now being suggested for solution using learning-based or hybrid methods. However, in this domain, it remains difficult, if not impossible, to conduct a fair comparison between classical, learning-based, and hybrid approaches due to the lack of a unified framework that supports both learning and evaluation. To address this, we introduce POGEMA, a comprehensive set of tools that includes a fast environment for learning, a problem instance generator, a collection of predefined problem instances, a visualization toolkit, and a benchmarking tool for automated evaluation. We also introduce and define an evaluation protocol that specifies a range of domain-related metrics, computed based on primary evaluation indicators (such as success rate and path length), enabling a fair multi-fold comparison. The results of this comparison, which involves a variety of state-of-the-art MARL, search-based, and hybrid methods, are presented.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Cost-efficient Adaptive Clinical Trials in Knee Osteoarthritis with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2408.02349</link>
<guid>https://arxiv.org/abs/2408.02349</guid>
<content:encoded><![CDATA[
<div>  (OA) (KOA) (AS) (RL)

<br /><br />
(KOA)(AS)KOA <div>
arXiv:2408.02349v4 Announce Type: replace 
Abstract: Osteoarthritis (OA) is the most common musculoskeletal disease, with knee OA (KOA) being one of the leading causes of disability and a significant economic burden. Predicting KOA progression is crucial for improving patient outcomes, optimizing healthcare resources, studying the disease, and developing new treatments. The latter application particularly requires one to understand the disease progression in order to collect the most informative data at the right time. Existing methods, however, are limited by their static nature and their focus on individual joints, leading to suboptimal predictive performance and downstream utility. Our study proposes a new method that allows to dynamically monitor patients rather than individual joints with KOA using a novel Active Sensing (AS) approach powered by Reinforcement Learning (RL). Our key idea is to directly optimize for the downstream task by training an agent that maximizes informative data collection while minimizing overall costs. Our RL-based method leverages a specially designed reward function to monitor disease progression across multiple body parts, employs multimodal deep learning, and requires no human input during testing. Extensive numerical experiments demonstrate that our approach outperforms current state-of-the-art models, paving the way for the next generation of KOA trials.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Expressive Power of Uniform Population Protocols with Logarithmic Space</title>
<link>https://arxiv.org/abs/2408.10027</link>
<guid>https://arxiv.org/abs/2408.10027</guid>
<content:encoded><![CDATA[
<div> : population protocols

:
Population Protocols$o(\log n)$$\Omega(n)$$\Theta(\log n)$$\Theta(\mathsf{polylog}~ n)$$\epsilon > 0$$f \in \Omega(\log n) \cap O(n^{1-\epsilon})$$\Theta(f(n))$$\mathsf{NSPACE}(f(n)\log n)$<br /><br /> <div>
arXiv:2408.10027v2 Announce Type: replace 
Abstract: Population protocols are a model of computation in which indistinguishable mobile agents interact in pairs to decide a property of their initial configuration. Originally introduced by Angluin et. al. in 2004 with a constant number of states, research nowadays focuses on protocols where the space usage depends on the number of agents. The expressive power of population protocols has so far however only been determined for protocols using $o(\log n)$ states, which compute only semilinear predicates, and for ${\Omega}(n)$ states. This leaves a significant gap, particularly concerning protocols with ${\Theta}(\log n)$ or ${\Theta}(\mathsf{polylog}~ n)$ states, which are the most common constructions in the literature. In this paper we close the gap and prove that for any ${\epsilon} > 0$ and $f {\in}{\Omega}(\log n) {\cap}O(n^{1-{\epsilon}})$, both uniform and non-uniform population protocols with ${\Theta}(f(n))$ states can decide exactly those predicates, whose unary encoding lies in $\mathsf{NSPACE}(f(n) \log n)$.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrugAgent: Multi-Agent Large Language Model-Based Reasoning for Drug-Target Interaction Prediction</title>
<link>https://arxiv.org/abs/2408.13378</link>
<guid>https://arxiv.org/abs/2408.13378</guid>
<content:encoded><![CDATA[
<div> : , , -, , 

:
DrugAgent-DTIDTIDrugAgentChain-of-Thought (CoT)  ReAct (Reason+Act) DTIGPT-4o miniDrugAgentF145%0.514 vs 0.355AIKGDrugAgenthttps://anonymous.4open.science/r/DrugAgent-B2EA <div>
arXiv:2408.13378v4 Announce Type: replace 
Abstract: Advancements in large language models (LLMs) allow them to address diverse questions using human-like interfaces. Still, limitations in their training prevent them from answering accurately in scenarios that could benefit from multiple perspectives. Multi-agent systems allow the resolution of questions to enhance result consistency and reliability. While drug-target interaction (DTI) prediction is important for drug discovery, existing approaches face challenges due to complex biological systems and the lack of interpretability needed for clinical applications. DrugAgent is a multi-agent LLM system for DTI prediction that combines multiple specialized perspectives with transparent reasoning. Our system adapts and extends existing multi-agent frameworks by (1) applying coordinator-based architecture to the DTI domain, (2) integrating domain-specific data sources, including ML predictions, knowledge graphs, and literature evidence, and (3) incorporating Chain-of-Thought (CoT) and ReAct (Reason+Act) frameworks for transparent DTI reasoning. We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355). Through ablation studies, we demonstrated the contributions of each agent, with the AI agent being the most impactful, followed by the KG agent and search agent. Most importantly, our approach provides detailed, human-interpretable reasoning for each prediction by combining evidence from multiple sources - a critical feature for biomedical applications where understanding the rationale behind predictions is essential for clinical decision-making and regulatory compliance. Code is available at https://anonymous.4open.science/r/DrugAgent-B2EA.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale</title>
<link>https://arxiv.org/abs/2409.00134</link>
<guid>https://arxiv.org/abs/2409.00134</guid>
<content:encoded><![CDATA[
<div> (MAPF)TransformerMAPF-GPT

:<br />
MAPFMAPF-GPTMAPFTransformerMAPF-GPTMAPFMAPF-GPTMAPF <div>
arXiv:2409.00134v5 Announce Type: replace 
Abstract: Multi-agent pathfinding (MAPF) is a problem that generally requires finding collision-free paths for multiple agents in a shared environment. Solving MAPF optimally, even under restrictive assumptions, is NP-hard, yet efficient solutions for this problem are critical for numerous applications, such as automated warehouses and transportation systems. Recently, learning-based approaches to MAPF have gained attention, particularly those leveraging deep reinforcement learning. Typically, such learning-based MAPF solvers are augmented with additional components like single-agent planning or communication. Orthogonally, in this work we rely solely on imitation learning that leverages a large dataset of expert MAPF solutions and transformer-based neural network to create a foundation model for MAPF called MAPF-GPT. The latter is capable of generating actions without additional heuristics or communication. MAPF-GPT demonstrates zero-shot learning abilities when solving the MAPF problems that are not present in the training dataset. We show that MAPF-GPT notably outperforms the current best-performing learnable MAPF solvers on a diverse range of problem instances and is computationally efficient during inference.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StateAct: Enhancing LLM Base Agents via Self-prompting and State-tracking</title>
<link>https://arxiv.org/abs/2410.02810</link>
<guid>https://arxiv.org/abs/2410.02810</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)StateActReActAlfworldTextcraftWebshop

<br /><br />:
StateActLLMsStateActReActStateActAlfworldTextcraftWebshopLLM10%30%7%StateActReActLLMTextcraft12%StateActLLMhttps://github.com/ai-nikolai/stateact  <div>
arXiv:2410.02810v3 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly used as autonomous agents, tackling tasks from robotics to web navigation. Their performance depends on the underlying base agent. Existing methods, however, struggle with long-context reasoning and goal adherence. We introduce StateAct, a novel and efficient base agent that enhances decision-making through (1) self-prompting, which reinforces task goals at every step, and (2) chain-of-states, an extension of chain-of-thought that tracks state information over time. StateAct outperforms ReAct, the previous best base agent, by over 10% on Alfworld, 30% on Textcraft, and 7% on Webshop across multiple frontier LLMs. We also demonstrate that StateAct can be used as a drop-in replacement for ReAct with advanced LLM agent methods such as test-time scaling, yielding an additional 12% gain on Textcraft. By improving efficiency and long-range reasoning without requiring additional training or retrieval, StateAct provides a scalable foundation for LLM agents. We open source our code to support further research at https://github.com/ai-nikolai/stateact .
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAPPA: Generalizing and Adapting Robot Policies via Online Agentic Guidance</title>
<link>https://arxiv.org/abs/2410.06473</link>
<guid>https://arxiv.org/abs/2410.06473</guid>
<content:encoded><![CDATA[
<div> 

:
https://agenticrobots.github.io  <div>
arXiv:2410.06473v3 Announce Type: replace 
Abstract: Robot learning approaches such as behavior cloning and reinforcement learning have shown great promise in synthesizing robot skills from human demonstrations in specific environments. However, these approaches often require task-specific demonstrations or designing complex simulation environments, which limits the development of generalizable and robust policies for unseen real-world settings. Recent advances in the use of foundation models for robotics (e.g., LLMs, VLMs) have shown great potential in enabling systems to understand the semantics in the world from large-scale internet data. However, it remains an open challenge to use this knowledge to enable robotic systems to understand the underlying dynamics of the world, to generalize policies across different tasks, and to adapt policies to new environments. To alleviate these limitations, we propose an agentic framework for robot self-guidance and self-improvement, which consists of a set of role-specialized conversational agents, such as a high-level advisor, a grounding agent, a monitoring agent, and a robotic agent. Our framework iteratively grounds a base robot policy to relevant objects in the environment and uses visuomotor cues to shift the action distribution of the policy to more desirable states, online, while remaining agnostic to the subjective configuration of a given robot hardware platform. We demonstrate that our approach can effectively guide manipulation policies to achieve significantly higher success rates, both in simulation and in real-world experiments, without the need for additional human demonstrations or extensive exploration. Code and videos available at: https://agenticrobots.github.io
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Attention Agents for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2410.21794</link>
<guid>https://arxiv.org/abs/2410.21794</guid>
<content:encoded><![CDATA[
<div> : Multi-Agent SystemsTheory of Mind (ToM)

<br /><br />:
(ToM) <div>
arXiv:2410.21794v2 Announce Type: replace 
Abstract: A major challenge for Multi-Agent Systems is enabling agents to adapt dynamically to diverse environments in which opponents and teammates may continually change. Agents trained using conventional methods tend to excel only within the confines of their training cohorts; their performance drops significantly when confronting unfamiliar agents. To address this shortcoming, we introduce Inverse Attention Agents that adopt concepts from the Theory of Mind (ToM) implemented algorithmically using an attention mechanism trained in an end-to-end manner. Crucial to determining the final actions of these agents, the weights in their attention model explicitly represent attention to different goals. We furthermore propose an inverse attention network that deduces the ToM of agents based on observations and prior actions. The network infers the attentional states of other agents, thereby refining the attention weights to adjust the agent's final action. We conduct experiments in a continuous environment, tackling demanding tasks encompassing cooperation, competition, and a blend of both. They demonstrate that the inverse attention network successfully infers the attention of other agents, and that this information improves agent performance. Additional human experiments show that, compared to baseline agent models, our inverse attention agents exhibit superior cooperation with humans and better emulate human behaviors.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Functionality understanding and segmentation in 3D scenes</title>
<link>https://arxiv.org/abs/2411.16310</link>
<guid>https://arxiv.org/abs/2411.16310</guid>
<content:encoded><![CDATA[
<div> : Fun3DU3DChain-of-Thought

:
Fun3DUChain-of-ThoughtFun3DUSceneFun3D3000230Fun3DU3Dhttps://tev-fbk.github.io/fun3du/ <div>
arXiv:2411.16310v4 Announce Type: replace 
Abstract: Understanding functionalities in 3D scenes involves interpreting natural language descriptions to locate functional interactive objects, such as handles and buttons, in a 3D environment. Functionality understanding is highly challenging, as it requires both world knowledge to interpret language and spatial perception to identify fine-grained objects. For example, given a task like 'turn on the ceiling light', an embodied AI agent must infer that it needs to locate the light switch, even though the switch is not explicitly mentioned in the task description. To date, no dedicated methods have been developed for this problem. In this paper, we introduce Fun3DU, the first approach designed for functionality understanding in 3D scenes. Fun3DU uses a language model to parse the task description through Chain-of-Thought reasoning in order to identify the object of interest. The identified object is segmented across multiple views of the captured scene by using a vision and language model. The segmentation results from each view are lifted in 3D and aggregated into the point cloud using geometric information. Fun3DU is training-free, relying entirely on pre-trained models. We evaluate Fun3DU on SceneFun3D, the most recent and only dataset to benchmark this task, which comprises over 3000 task descriptions on 230 scenes. Our method significantly outperforms state-of-the-art open-vocabulary 3D segmentation approaches. Project page: https://tev-fbk.github.io/fun3du/
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SceneTAP: Scene-Coherent Typographic Adversarial Planner against Vision-Language Models in Real-World Environments</title>
<link>https://arxiv.org/abs/2412.00114</link>
<guid>https://arxiv.org/abs/2412.00114</guid>
<content:encoded><![CDATA[
<div> TextDiffuser

<br /><br />:
LVLMsLLMSceneTAPchain-of-thoughtTextDiffuserLVLMsChatGPT-4o <div>
arXiv:2412.00114v2 Announce Type: replace 
Abstract: Large vision-language models (LVLMs) have shown remarkable capabilities in interpreting visual content. While existing works demonstrate these models' vulnerability to deliberately placed adversarial texts, such texts are often easily identifiable as anomalous. In this paper, we present the first approach to generate scene-coherent typographic adversarial attacks that mislead advanced LVLMs while maintaining visual naturalness through the capability of the LLM-based agent. Our approach addresses three critical questions: what adversarial text to generate, where to place it within the scene, and how to integrate it seamlessly. We propose a training-free, multi-modal LLM-driven scene-coherent typographic adversarial planning (SceneTAP) that employs a three-stage process: scene understanding, adversarial planning, and seamless integration. The SceneTAP utilizes chain-of-thought reasoning to comprehend the scene, formulate effective adversarial text, strategically plan its placement, and provide detailed instructions for natural integration within the image. This is followed by a scene-coherent TextDiffuser that executes the attack using a local diffusion mechanism. We extend our method to real-world scenarios by printing and placing generated patches in physical environments, demonstrating its practical implications. Extensive experiments show that our scene-coherent adversarial text successfully misleads state-of-the-art LVLMs, including ChatGPT-4o, even after capturing new images of physical setups. Our evaluations demonstrate a significant increase in attack success rates while maintaining visual naturalness and contextual appropriateness. This work highlights vulnerabilities in current vision-language models to sophisticated, scene-coherent adversarial attacks and provides insights into potential defense mechanisms.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augmenting the action space with conventions to improve multi-agent cooperation in Hanabi</title>
<link>https://arxiv.org/abs/2412.06333</link>
<guid>https://arxiv.org/abs/2412.06333</guid>
<content:encoded><![CDATA[
<div> HanabiMARL

<br /><br />:
HanabiHanabiHanabi <div>
arXiv:2412.06333v2 Announce Type: replace 
Abstract: The card game Hanabi is considered a strong medium for the testing and development of multi-agent reinforcement learning (MARL) algorithms, due to its cooperative nature, hidden information, limited communication and remarkable complexity. Previous research efforts have explored the capabilities of MARL algorithms within Hanabi, focusing largely on advanced architecture design and algorithmic manipulations to achieve state-of-the-art performance for a various number of cooperators. However, this often leads to complex solution strategies with high computational cost and requiring large amounts of training data. For humans to solve the Hanabi game effectively, they require the use of conventions, which often allows for a means to implicitly convey ideas or knowledge based on a predefined, and mutually agreed upon, set of ``rules''. Multi-agent problems containing partial observability, especially when limited communication is present, can benefit greatly from the use of implicit knowledge sharing. In this paper, we propose a novel approach to augmenting the action space using conventions, which act as special cooperative actions that span over multiple time steps and multiple agents, requiring agents to actively opt in for it to reach fruition. These conventions are based on existing human conventions, and result in a significant improvement on the performance of existing techniques for self-play and cross-play across a various number of cooperators within Hanabi.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Well Can Modern LLMs Act as Agent Cores in Radiology Environments?</title>
<link>https://arxiv.org/abs/2412.09529</link>
<guid>https://arxiv.org/abs/2412.09529</guid>
<content:encoded><![CDATA[
<div> : RadA-BenchPlat

:
RadA-BenchPlatLLMs2,2002,20024,200Claude-3.7-Sonnet67.1%prompt-backpropagation16.8%30.7%48.2%65.4%https://github.com/MAGIC-AI4Med/RadABench <div>
arXiv:2412.09529v3 Announce Type: replace 
Abstract: We introduce RadA-BenchPlat, an evaluation platform that benchmarks the performance of large language models (LLMs) act as agent cores in radiology environments using 2,200 radiologist-verified synthetic patient records covering six anatomical regions, five imaging modalities, and 2,200 disease scenarios, resulting in 24,200 question-answer pairs that simulate diverse clinical situations. The platform also defines ten categories of tools for agent-driven task solving and evaluates seven leading LLMs, revealing that while models like Claude-3.7-Sonnet can achieve a 67.1% task completion rate in routine settings, they still struggle with complex task understanding and tool coordination, limiting their capacity to serve as the central core of automated radiology systems. By incorporating four advanced prompt engineering strategies--where prompt-backpropagation and multi-agent collaboration contributed 16.8% and 30.7% improvements, respectively--the performance for complex tasks was enhanced by 48.2% overall. Furthermore, automated tool building was explored to improve robustness, achieving a 65.4% success rate, thereby offering promising insights for the future integration of fully automated radiology applications into clinical practice. All of our code and data are openly available at https://github.com/MAGIC-AI4Med/RadABench.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Learning-Based Optimization for Free-form Pipe Routing in Aeroengine with Dynamic Design Environment</title>
<link>https://arxiv.org/abs/2504.03669</link>
<guid>https://arxiv.org/abs/2504.03669</guid>
<content:encoded><![CDATA[
<div> PPO

<br /><br />:
SLPRSLPRPPOSLPRSLPRSLPR <div>
arXiv:2504.03669v1 Announce Type: new 
Abstract: Pipe routing is a highly complex, time-consuming, and no-deterministic polynomial-time hard (NP-hard) problem in aeroengine design. Despite extensive research efforts in optimizing constant-curvature pipe routing, the growing demand for free-form pipes poses new challenges. Dynamic design environments and fuzzy layout rules further impact the optimization performance and efficiency. To tackle these challenges, this study proposes a self-learning-based method (SLPR) for optimizing free-form pipe routing in aeroengines. The SLPR is based on the proximal policy optimization (PPO) algorithm and integrates a unified rule modeling framework for efficient obstacle detection and fuzzy rule modeling in continuous space. Additionally, a potential energy table is constructed to enable rapid queries of layout tendencies and interference. The agent within SLPR iteratively refines pipe routing and accumulates the design knowledge through interaction with the environment. Once the design environment shifts, the agent can swiftly adapt by fine-tuning network parameters. Comparative tests reveal that SLPR ensures smooth pipe routing through cubic non-uniform B-spline (NURBS) curves, avoiding redundant pipe segments found in constant-curvature pipe routing. Results in both static and dynamic design environments demonstrate that SLPR outperforms three representative baselines in terms of the pipe length reduction, the adherence to layout rules, the path complexity, and the computational efficiency. Furthermore, tests in dynamic environments indicate that SLPR eliminates labor-intensive searches from scratch and even yields superior solutions compared to the retrained model. These results highlight the practical value of SLPR for real-world pipe routing, meeting lightweight, precision, and sustainability requirements of the modern aeroengine design.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Business Process Management: The Past 30 Years And Practitioners' Future Perspectives</title>
<link>https://arxiv.org/abs/2504.03693</link>
<guid>https://arxiv.org/abs/2504.03693</guid>
<content:encoded><![CDATA[
<div> generative AIbusiness process managementagent autonomyadaptablegovernance

:<br />
genAIagentic BPMgenAIBPMgenAI <div>
arXiv:2504.03693v1 Announce Type: new 
Abstract: With the advent of generative Artificial Intelligence (genAI), the notion of an agent has seen a resurgence in popularity. This has also led to speculation about the extent to which business process management, as a discipline and research field, may impact and be impacted by the deployment of genAI-based agents. To better ground such speculations into the state-of-the-art, we draw from the past 30 years of research on agents and business process management to establish the concept of Agentic Business Process Management (agentic BPM) that is only loosely coupled to the genAI hype. We conduct a series of interviews with BPM practitioners to explore their understanding, expectations, and concerns related to agent autonomy, adaptability, human collaboration, and governance in processes. The findings reflect both challenges with respect to data inconsistencies, manual interventions, identification of process bottlenecks, actionability of process improvements, as well as the opportunities of enhanced efficiency, predictive process insights and proactive decision-making support. While the technology offers potential benefits, practitioners also anticipate risks such as biases, over-reliance, lack of transparency, and job displacement within organizations. These concerns underscore the need for a robust methodological framework for managing agents in organizations.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcing Clinical Decision Support through Multi-Agent Systems and Ethical AI Governance</title>
<link>https://arxiv.org/abs/2504.03699</link>
<guid>https://arxiv.org/abs/2504.03699</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />
eICUAIAI <div>
arXiv:2504.03699v1 Announce Type: new 
Abstract: In the age of data-driven medicine, it is paramount to include explainable and ethically managed artificial intelligence in explaining clinical decision support systems to achieve trustworthy and effective patient care. The focus of this paper is on a new architecture of a multi-agent system for clinical decision support that uses modular agents to analyze laboratory results, vital signs, and the clinical context and then integrates these results to drive predictions and validate outcomes. We describe our implementation with the eICU database to run lab-analysis-specific agents, vitals-only interpreters, and contextual reasoners and then run the prediction module and a validation agent. Everything is a transparent implementation of business logic, influenced by the principles of ethical AI governance such as Autonomy, Fairness, and Accountability. It provides visible results that this agent-based framework not only improves on interpretability and accuracy but also on reinforcing trust in AI-assisted decisions in an intensive care setting.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VFlow: Discovering Optimal Agentic Workflows for Verilog Generation</title>
<link>https://arxiv.org/abs/2504.03723</link>
<guid>https://arxiv.org/abs/2504.03723</guid>
<content:encoded><![CDATA[
<div> : VFlowVerilog(MCTS)

<br /><br />:
VFlowVerilogMCTSVFlowAFLOWVFlowVerilogEvalpass@183.6%PromptV6.1%LLM36.9%VFlowDeepSeek-V3GPT-4o141.2%API13%LLM <div>
arXiv:2504.03723v1 Announce Type: new 
Abstract: Hardware design automation faces challenges in generating high-quality Verilog code efficiently. This paper introduces VFlow, an automated framework that optimizes agentic workflows for Verilog code generation. Unlike existing approaches that rely on pre-defined prompting strategies, VFlow leverages Monte Carlo Tree Search (MCTS) to discover effective sequences of Large Language Models invocations that maximize code quality while minimizing computational costs. VFlow extends the AFLOW methodology with domain-specific operators addressing hardware design requirements, including syntax validation, simulation-based verification, and synthesis optimization. Experimental evaluation on the VerilogEval benchmark demonstrates VFlow's superiority, achieving an 83.6% average pass@1 rate-a 6.1\% improvement over state-of-the-art PromptV and a 36.9\% gain compared to direct LLM invocation. Most significantly, VFlow enhances the capabilities of smaller models, enabling DeepSeek-V3 to achieve 141.2\% of GPT-4o's performance while reducing API costs to just 13\%. These findings indicate that intelligently optimized workflows enable cost-efficient LLMs to outperform larger models on hardware design tasks, potentially democratizing access to advanced digital circuit development tools and accelerating innovation in the semiconductor industry
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Benchmark for Scalable Oversight Protocols</title>
<link>https://arxiv.org/abs/2504.03731</link>
<guid>https://arxiv.org/abs/2504.03731</guid>
<content:encoded><![CDATA[
<div> : AIDebate<br /><br />: AIAIDebate(ASD)PythonDebate <div>
arXiv:2504.03731v1 Announce Type: new 
Abstract: As AI agents surpass human capabilities, scalable oversight -- the problem of effectively supplying human feedback to potentially superhuman AI models -- becomes increasingly critical to ensure alignment. While numerous scalable oversight protocols have been proposed, they lack a systematic empirical framework to evaluate and compare them. While recent works have tried to empirically study scalable oversight protocols -- particularly Debate -- we argue that the experiments they conduct are not generalizable to other protocols. We introduce the scalable oversight benchmark, a principled framework for evaluating human feedback mechanisms based on our agent score difference (ASD) metric, a measure of how effectively a mechanism advantages truth-telling over deception. We supply a Python package to facilitate rapid and competitive evaluation of scalable oversight protocols on our benchmark, and conduct a demonstrative experiment benchmarking Debate.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling bounded rational decision-making through Wasserstein constraints</title>
<link>https://arxiv.org/abs/2504.03743</link>
<guid>https://arxiv.org/abs/2504.03743</guid>
<content:encoded><![CDATA[
<div> bounded rational decision-making, reinforcement learning, Wasserstein distances, ordinal action spaces, entropy

:
KLKLWasserstein <div>
arXiv:2504.03743v1 Announce Type: new 
Abstract: Modelling bounded rational decision-making through information constrained processing provides a principled approach for representing departures from rationality within a reinforcement learning framework, while still treating decision-making as an optimization process. However, existing approaches are generally based on Entropy, Kullback-Leibler divergence, or Mutual Information. In this work, we highlight issues with these approaches when dealing with ordinal action spaces. Specifically, entropy assumes uniform prior beliefs, missing the impact of a priori biases on decision-makings. KL-Divergence addresses this, however, has no notion of "nearness" of actions, and additionally, has several well known potentially undesirable properties such as the lack of symmetry, and furthermore, requires the distributions to have the same support (e.g. positive probability for all actions). Mutual information is often difficult to estimate. Here, we propose an alternative approach for modeling bounded rational RL agents utilising Wasserstein distances. This approach overcomes the aforementioned issues. Crucially, this approach accounts for the nearness of ordinal actions, modeling "stickiness" in agent decisions and unlikeliness of rapidly switching to far away actions, while also supporting low probability actions, zero-support prior distributions, and is simple to calculate directly.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No-Regret Learning in Stackelberg Games with an Application to Electric Ride-Hailing</title>
<link>https://arxiv.org/abs/2504.03745</link>
<guid>https://arxiv.org/abs/2504.03745</guid>
<content:encoded><![CDATA[
<div> : Stackelberg

:
Stackelberg$O(\sqrt{T})$$\epsilon$-Stackelberg <div>
arXiv:2504.03745v1 Announce Type: new 
Abstract: We consider the problem of efficiently learning to play single-leader multi-follower Stackelberg games when the leader lacks knowledge of the lower-level game. Such games arise in hierarchical decision-making problems involving self-interested agents. For example, in electric ride-hailing markets, a central authority aims to learn optimal charging prices to shape fleet distributions and charging patterns of ride-hailing companies. Existing works typically apply gradient-based methods to find the leader's optimal strategy. Such methods are impractical as they require that the followers share private utility information with the leader. Instead, we treat the lower-level game as a black box, assuming only that the followers' interactions approximate a Nash equilibrium while the leader observes the realized cost of the resulting approximation. Under kernel-based regularity assumptions on the leader's cost function, we develop a no-regret algorithm that converges to an $\epsilon$-Stackelberg equilibrium in $O(\sqrt{T})$ rounds. Finally, we validate our approach through a numerical case study on optimal pricing in electric ride-hailing markets.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emerging Cyber Attack Risks of Medical AI Agents</title>
<link>https://arxiv.org/abs/2504.03759</link>
<guid>https://arxiv.org/abs/2504.03759</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)AI

<br /><br />:
AIAI1) AI2) AI3) AI/4) AIURLLLMAIDeepSeek-R1AI <div>
arXiv:2504.03759v1 Announce Type: new 
Abstract: Large language models (LLMs)-powered AI agents exhibit a high level of autonomy in addressing medical and healthcare challenges. With the ability to access various tools, they can operate within an open-ended action space. However, with the increase in autonomy and ability, unforeseen risks also arise. In this work, we investigated one particular risk, i.e., cyber attack vulnerability of medical AI agents, as agents have access to the Internet through web browsing tools. We revealed that through adversarial prompts embedded on webpages, cyberattackers can: i) inject false information into the agent's response; ii) they can force the agent to manipulate recommendation (e.g., healthcare products and services); iii) the attacker can also steal historical conversations between the user and agent, resulting in the leak of sensitive/private medical information; iv) furthermore, the targeted agent can also cause a computer system hijack by returning a malicious URL in its response. Different backbone LLMs were examined, and we found such cyber attacks can succeed in agents powered by most mainstream LLMs, with the reasoning models such as DeepSeek-R1 being the most vulnerable.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCP Safety Audit: LLMs with the Model Context Protocol Allow Major Security Exploits</title>
<link>https://arxiv.org/abs/2504.03767</link>
<guid>https://arxiv.org/abs/2504.03767</guid>
<content:encoded><![CDATA[
<div> : Model Context Protocol (MCP), ,  (LLMs), MCPSafetyScanner, 

:
Model Context Protocol (MCP)APIAIMCPLLMsMCPSafetyScannerMCPMCPSafetyScannerMCPMCPMCPSafetyScannerGitHubhttps://github.com/leidosinc/McpSafetyScanner <div>
arXiv:2504.03767v1 Announce Type: new 
Abstract: To reduce development overhead and enable seamless integration between potential components comprising any given generative AI application, the Model Context Protocol (MCP) (Anthropic, 2024) has recently been released and subsequently widely adopted. The MCP is an open protocol that standardizes API calls to large language models (LLMs), data sources, and agentic tools. By connecting multiple MCP servers, each defined with a set of tools, resources, and prompts, users are able to define automated workflows fully driven by LLMs. However, we show that the current MCP design carries a wide range of security risks for end users. In particular, we demonstrate that industry-leading LLMs may be coerced into using MCP tools to compromise an AI developer's system through various attacks, such as malicious code execution, remote access control, and credential theft. To proactively mitigate these and related attacks, we introduce a safety auditing tool, MCPSafetyScanner, the first agentic tool to assess the security of an arbitrary MCP server. MCPScanner uses several agents to (a) automatically determine adversarial samples given an MCP server's tools and resources; (b) search for related vulnerabilities and remediations based on those samples; and (c) generate a security report detailing all findings. Our work highlights serious security issues with general-purpose agentic workflows while also providing a proactive tool to audit MCP server safety and address detected vulnerabilities before deployment.
  The described MCP server auditing tool, MCPSafetyScanner, is freely available at: https://github.com/leidosinc/McpSafetyScanner
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow State: Humans Enabling AI Systems to Program Themselves</title>
<link>https://arxiv.org/abs/2504.03771</link>
<guid>https://arxiv.org/abs/2504.03771</guid>
<content:encoded><![CDATA[
<div> : Compound AI, Pocketflow, Human-AI co-design, modular Nodes, Flow orchestration

:
AIPocketflowNodesFlowFlow-as-NodeRAGPocket AIPocketflowAI <div>
arXiv:2504.03771v1 Announce Type: new 
Abstract: Compound AI systems, orchestrating multiple AI components and external APIs, are increasingly vital but face challenges in managing complexity, handling ambiguity, and enabling effective development workflows. Existing frameworks often introduce significant overhead, implicit complexity, or restrictive abstractions, hindering maintainability and iterative refinement, especially in Human-AI collaborative settings. We argue that overcoming these hurdles requires a foundational architecture prioritizing structural clarity and explicit control. To this end, we introduce Pocketflow, a platform centered on Human-AI co-design, enabled by Pocketflow. Pocketflow is a Python framework built upon a deliberately minimal yet synergistic set of core abstractions: modular Nodes with a strict lifecycle, declarative Flow orchestration, native hierarchical nesting (Flow-as-Node), and explicit action-based conditional logic. This unique combination provides a robust, vendor-agnostic foundation with very little code that demonstrably reduces overhead while offering the expressiveness needed for complex patterns like agentic workflows and RAG. Complemented by Pocket AI, an assistant leveraging this structure for system design, Pocketflow provides an effective environment for iteratively prototyping, refining, and deploying the adaptable, scalable AI systems demanded by modern enterprises.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steve: LLM Powered ChatBot for Career Progression</title>
<link>https://arxiv.org/abs/2504.03789</link>
<guid>https://arxiv.org/abs/2504.03789</guid>
<content:encoded><![CDATA[
<div> : 

<br /><br />:
OpenAI APIPDF <div>
arXiv:2504.03789v1 Announce Type: new 
Abstract: The advancements in systems deploying large language models (LLMs), as well as improvements in their ability to act as agents with predefined templates, provide an opportunity to conduct qualitative, individualized assessments, creating a bridge between qualitative and quantitative methods for candidates seeking career progression. In this paper, we develop a platform that allows candidates to run AI-led interviews to assess their current career stage and curate coursework to enable progression to the next level. Our approach incorporates predefined career trajectories, associated skills, and a method to recommend the best resources for gaining the necessary skills for advancement. We employ OpenAI API calls along with expertly compiled chat templates to assess candidate competence. Our platform is highly configurable due to the modularity of the development, is easy to deploy and use, and available as a web interface where the only requirement is candidate resumes in PDF format. We demonstrate a use-case centered on software engineering and intend to extend this platform to be domain-agnostic, requiring only regular updates to chat templates as industries evolve.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving World Models using Deep Supervision with Linear Probes</title>
<link>https://arxiv.org/abs/2504.03861</link>
<guid>https://arxiv.org/abs/2504.03861</guid>
<content:encoded><![CDATA[
<div> : Flappy Bird

:
Flappy Bird <div>
arXiv:2504.03861v1 Announce Type: new 
Abstract: Developing effective world models is crucial for creating artificial agents that can reason about and navigate complex environments. In this paper, we investigate a deep supervision technique for encouraging the development of a world model in a network trained end-to-end to predict the next observation. While deep supervision has been widely applied for task-specific learning, our focus is on improving the world models. Using an experimental environment based on the Flappy Bird game, where the agent receives only LIDAR measurements as observations, we explore the effect of adding a linear probe component to the network's loss function. This additional term encourages the network to encode a subset of the true underlying world features into its hidden state. Our experiments demonstrate that this supervision technique improves both training and test performance, enhances training stability, and results in more easily decodable world features -- even for those world features which were not included in the training. Furthermore, we observe a reduced distribution drift in networks trained with the linear probe, particularly during high-variability phases of the game (flying between successive pipe encounters). Including the world features loss component roughly corresponded to doubling the model size, suggesting that the linear probe technique is particularly beneficial in compute-limited settings or when aiming to achieve the best performance with smaller models. These findings contribute to our understanding of how to develop more robust and sophisticated world models in artificial agents, paving the way for further advancements in this field.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptation of Large Language Models</title>
<link>https://arxiv.org/abs/2504.03931</link>
<guid>https://arxiv.org/abs/2504.03931</guid>
<content:encoded><![CDATA[
<div> LLMs

<br /><br />:
LLMsLLMsLLMLLMsLLMLLMLLMs(RAG)LLM <div>
arXiv:2504.03931v1 Announce Type: new 
Abstract: This tutorial on adaptation of LLMs is designed to address the growing demand for models that go beyond the static capabilities of generic LLMs by providing an overview of dynamic, domain-specific, and task-adaptive LLM adaptation techniques. While general LLMs have demonstrated strong generalization across a variety of tasks, they often struggle to perform well in specialized domains such as finance, healthcare, and code generation for underrepresented languages. Additionally, their static nature limits their ability to evolve with the changing world, and they are often extremely large in size, making them impractical and costly to deploy at scale. As a result, the adaptation of LLMs has drawn much attention since the birth of LLMs and is of core importance, both for industry, which focuses on serving its targeted users, and academia, which can greatly benefit from small but powerful LLMs. To address this gap, this tutorial aims to provide an overview of the LLM adaptation techniques. We start with an introduction to LLM adaptation, from both the data perspective and the model perspective. We then emphasize how the evaluation metrics and benchmarks are different from other techniques. After establishing the problems, we explore various adaptation techniques. We categorize adaptation techniques into two main families. The first is parametric knowledge adaptation, which focuses on updating the parametric knowledge within LLMs. Additionally, we will discuss real-time adaptation techniques, including model editing, which allows LLMs to be updated dynamically in production environments. The second kind of adaptation is semi-parametric knowledge adaptation, where the goal is to update LLM parameters to better leverage external knowledge or tools through techniques like retrieval-augmented generation (RAG) and agent-based systems.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>YaleNLP @ PerAnsSumm 2025: Multi-Perspective Integration via Mixture-of-Agents for Enhanced Healthcare QA Summarization</title>
<link>https://arxiv.org/abs/2504.03932</link>
<guid>https://arxiv.org/abs/2504.03932</guid>
<content:encoded><![CDATA[
<div> automated summarization, healthcare community, question-answering forums, PerAnsSumm Shared Task, LLaMA, GPT-4o, Mixture-of-Agents (MoA), perspective span identification, perspective-based summarization

:<br />
PerAnsSumm Shared TaskQLoRALLaMA-3.3-70B-InstructLLMLLaMAGPT-4oMoA/GPT-4o0.57LLaMA0.402MoALLaMA28%0.51GPT-4o0.42LLaMA0.282MoALLaMA32%0.37sentence-transformerLLaMAGPT-4oNLP <div>
arXiv:2504.03932v1 Announce Type: new 
Abstract: Automated summarization of healthcare community question-answering forums is challenging due to diverse perspectives presented across multiple user responses to each question. The PerAnsSumm Shared Task was therefore proposed to tackle this challenge by identifying perspectives from different answers and then generating a comprehensive answer to the question. In this study, we address the PerAnsSumm Shared Task using two complementary paradigms: (i) a training-based approach through QLoRA fine-tuning of LLaMA-3.3-70B-Instruct, and (ii) agentic approaches including zero- and few-shot prompting with frontier LLMs (LLaMA-3.3-70B-Instruct and GPT-4o) and a Mixture-of-Agents (MoA) framework that leverages a diverse set of LLMs by combining outputs from multi-layer feedback aggregation. For perspective span identification/classification, GPT-4o zero-shot achieves an overall score of 0.57, substantially outperforming the 0.40 score of the LLaMA baseline. With a 2-layer MoA configuration, we were able to improve LLaMA performance up by 28 percent to 0.51. For perspective-based summarization, GPT-4o zero-shot attains an overall score of 0.42 compared to 0.28 for the best LLaMA zero-shot, and our 2-layer MoA approach boosts LLaMA performance by 32 percent to 0.37. Furthermore, in few-shot setting, our results show that the sentence-transformer embedding-based exemplar selection provides more gain than manually selected exemplars on LLaMA models, although the few-shot prompting is not always helpful for GPT-4o. The YaleNLP team's approach ranked the overall second place in the shared task.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-Enhanced Robotic Subretinal Injection with Real-Time Retinal Motion Compensation</title>
<link>https://arxiv.org/abs/2504.03939</link>
<guid>https://arxiv.org/abs/2504.03939</guid>
<content:encoded><![CDATA[
<div> : subretinal(iOCT)

:
iOCT(LSTM)(ILM)(FFT)16.4AI <div>
arXiv:2504.03939v1 Announce Type: new 
Abstract: Subretinal injection is a critical procedure for delivering therapeutic agents to treat retinal diseases such as age-related macular degeneration (AMD). However, retinal motion caused by physiological factors such as respiration and heartbeat significantly impacts precise needle positioning, increasing the risk of retinal pigment epithelium (RPE) damage. This paper presents a fully autonomous robotic subretinal injection system that integrates intraoperative optical coherence tomography (iOCT) imaging and deep learning-based motion prediction to synchronize needle motion with retinal displacement. A Long Short-Term Memory (LSTM) neural network is used to predict internal limiting membrane (ILM) motion, outperforming a Fast Fourier Transform (FFT)-based baseline model. Additionally, a real-time registration framework aligns the needle tip position with the robot's coordinate frame. Then, a dynamic proportional speed control strategy ensures smooth and adaptive needle insertion. Experimental validation in both simulation and ex vivo open-sky porcine eyes demonstrates precise motion synchronization and successful subretinal injections. The experiment achieves a mean tracking error below 16.4 {\mu}m in pre-insertion phases. These results show the potential of AI-driven robotic assistance to improve the safety and accuracy of retinal microsurgery.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding EFX Allocations: Counting and Variants</title>
<link>https://arxiv.org/abs/2504.03951</link>
<guid>https://arxiv.org/abs/2504.03951</guid>
<content:encoded><![CDATA[
<div> : envy-freeness up to any good (EFX), , , , EFX (WEFX)

:
envy-freeness up to any good (EFX) EFXWEFXEFXEFX+WEFX <div>
arXiv:2504.03951v1 Announce Type: new 
Abstract: Envy-freeness up to any good (EFX) is a popular and important fairness property in the fair allocation of indivisible goods, of which its existence in general is still an open question. In this work, we investigate the problem of determining the minimum number of EFX allocations for a given instance, arguing that this approach may yield valuable insights into the existence and computation of EFX allocations. We focus on restricted instances where the number of goods slightly exceeds the number of agents, and extend our analysis to weighted EFX (WEFX) and a novel variant of EFX for general monotone valuations, termed EFX+. In doing so, we identify the transition threshold for the existence of allocations satisfying these fairness notions. Notably, we resolve open problems regarding WEFX by proving polynomial-time computability under binary additive valuations, and establishing the first constant-factor approximation for two agents.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Prompt Generation for Diverse Human-like Teaming and Communication with Large Language Models</title>
<link>https://arxiv.org/abs/2504.03991</link>
<guid>https://arxiv.org/abs/2504.03991</guid>
<content:encoded><![CDATA[
<div> : 

<br /><br />:
AIQDQDLLM <div>
arXiv:2504.03991v1 Announce Type: new 
Abstract: Understanding how humans collaborate and communicate in teams is essential for improving human-agent teaming and AI-assisted decision-making. However, relying solely on data from large-scale user studies is impractical due to logistical, ethical, and practical constraints, necessitating synthetic models of multiple diverse human behaviors. Recently, agents powered by Large Language Models (LLMs) have been shown to emulate human-like behavior in social settings. But, obtaining a large set of diverse behaviors requires manual effort in the form of designing prompts. On the other hand, Quality Diversity (QD) optimization has been shown to be capable of generating diverse Reinforcement Learning (RL) agent behavior. In this work, we combine QD optimization with LLM-powered agents to iteratively search for prompts that generate diverse team behavior in a long-horizon, multi-step collaborative environment. We first show, through a human-subjects experiment (n=54 participants), that humans exhibit diverse coordination and communication behavior in this domain. We then show that our approach can effectively replicate trends from human teaming data and also capture behaviors that are not easily observed without collecting large amounts of data. Our findings highlight the combination of QD and LLM-powered agents as an effective tool for studying teaming and communication strategies in multi-agent collaboration.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Offline Mixed-Criticality Scheduling with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.03994</link>
<guid>https://arxiv.org/abs/2504.03994</guid>
<content:encoded><![CDATA[
<div> :  (RL) (MC) Markov (MDP)

:
(RL)[1]NPMarkov(MDP)RLMCRL1080%85%94%93%RL <div>
arXiv:2504.03994v1 Announce Type: new 
Abstract: This paper introduces a novel reinforcement learning (RL) approach to scheduling mixed-criticality (MC) systems on processors with varying speeds. Building upon the foundation laid by [1], we extend their work to address the non-preemptive scheduling problem, which is known to be NP-hard. By modeling this scheduling challenge as a Markov Decision Process (MDP), we develop an RL agent capable of generating near-optimal schedules for real-time MC systems. Our RL-based scheduler prioritizes high-critical tasks while maintaining overall system performance.
  Through extensive experiments, we demonstrate the scalability and effectiveness of our approach. The RL scheduler significantly improves task completion rates, achieving around 80% overall and 85% for high-criticality tasks across 100,000 instances of synthetic data and real data under varying system conditions. Moreover, under stable conditions without degradation, the scheduler achieves 94% overall task completion and 93% for high-criticality tasks. These results highlight the potential of RL-based schedulers in real-time and safety-critical applications, offering substantial improvements in handling complex and dynamic scheduling scenarios.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADAPT: Actively Discovering and Adapting to Preferences for any Task</title>
<link>https://arxiv.org/abs/2504.04040</link>
<guid>https://arxiv.org/abs/2504.04040</guid>
<content:encoded><![CDATA[
<div> ADAPTActively Discovering and Adapting to Preferences for any Task (ADAPT)Reflection-DPO

:
ADAPTReflection-DPOLLMLLMLLMADAPTReflection-DPO6.1% <div>
arXiv:2504.04040v1 Announce Type: new 
Abstract: Assistive agents should be able to perform under-specified long-horizon tasks while respecting user preferences. We introduce Actively Discovering and Adapting to Preferences for any Task (ADAPT) -- a benchmark designed to evaluate agents' ability to adhere to user preferences across various household tasks through active questioning. Next, we propose Reflection-DPO, a novel training approach for adapting large language models (LLMs) to the task of active questioning. Reflection-DPO finetunes a 'student' LLM to follow the actions of a privileged 'teacher' LLM, and optionally ask a question to gather necessary information to better predict the teacher action. We find that prior approaches that use state-of-the-art LLMs fail to sufficiently follow user preferences in ADAPT due to insufficient questioning and poor adherence to elicited preferences. In contrast, Reflection-DPO achieves a higher rate of satisfying user preferences, outperforming a zero-shot chain-of-thought baseline by 6.1% on unseen users.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic, Dynamic, Fluid Autonomy in Agentic AI: Implications for Authorship, Inventorship, and Liability</title>
<link>https://arxiv.org/abs/2504.04058</link>
<guid>https://arxiv.org/abs/2504.04058</guid>
<content:encoded><![CDATA[
<div> : Agentic AI

<br /><br />:
Agentic AIAIAgentic AI <div>
arXiv:2504.04058v1 Announce Type: new 
Abstract: Agentic Artificial Intelligence (AI) systems, exemplified by OpenAI's DeepResearch, autonomously pursue goals, adapting strategies through implicit learning. Unlike traditional generative AI, which is reactive to user prompts, agentic AI proactively orchestrates complex workflows. It exhibits stochastic, dynamic, and fluid autonomy: its steps and outputs vary probabilistically (stochastic), it evolves based on prior interactions (dynamic), and it operates with significant independence within human-defined parameters, adapting to context (fluid). While this fosters complex, co-evolutionary human-machine interactions capable of generating uniquely synthesized creative outputs, it also irrevocably blurs boundaries--human and machine contributions become irreducibly entangled in intertwined creative processes. Consequently, agentic AI poses significant challenges to legal frameworks reliant on clear attribution: authorship doctrines struggle to disentangle ownership, intellectual property regimes strain to accommodate recursively blended novelty, and liability models falter as accountability diffuses across shifting loci of control. The central issue is not the legal treatment of human versus machine contributions, but the fundamental unmappability--the practical impossibility in many cases--of accurately attributing specific creative elements to either source. When retroactively parsing contributions becomes infeasible, applying distinct standards based on origin becomes impracticable. Therefore, we argue, legal and policy frameworks may need to treat human and machine contributions as functionally equivalent--not for moral or economic reasons, but as a pragmatic necessity.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enforcement Agents: Enhancing Accountability and Resilience in Multi-Agent AI Frameworks</title>
<link>https://arxiv.org/abs/2504.04070</link>
<guid>https://arxiv.org/abs/2504.04070</guid>
<content:encoded><![CDATA[
<div> 

:
EA90episode012EAEAEA0.0%1EA7.4%EA26.7%<br /><br /> <div>
arXiv:2504.04070v1 Announce Type: new 
Abstract: As autonomous agents become more powerful and widely used, it is becoming increasingly important to ensure they behave safely and stay aligned with system goals, especially in multi-agent settings. Current systems often rely on agents self-monitoring or correcting issues after the fact, but they lack mechanisms for real-time oversight. This paper introduces the Enforcement Agent (EA) Framework, which embeds dedicated supervisory agents into the environment to monitor others, detect misbehavior, and intervene through real-time correction. We implement this framework in a custom drone simulation and evaluate it across 90 episodes using 0, 1, and 2 EA configurations. Results show that adding EAs significantly improves system safety: success rates rise from 0.0% with no EA to 7.4% with one EA and 26.7% with two EAs. The system also demonstrates increased operational longevity and higher rates of malicious drone reformation. These findings highlight the potential of lightweight, real-time supervision for enhancing alignment and resilience in multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Among Us: A Sandbox for Agentic Deception</title>
<link>https://arxiv.org/abs/2504.04072</link>
<guid>https://arxiv.org/abs/2504.04072</guid>
<content:encoded><![CDATA[
<div> AIAmongUs

:
(AI)AmongUsAIELOAILLMAmongUsLLMs <div>
arXiv:2504.04072v1 Announce Type: new 
Abstract: Studying deception in AI agents is important and difficult due to the lack of model organisms and sandboxes that elicit the behavior without asking the model to act under specific conditions or inserting intentional backdoors. Extending upon $\textit{AmongAgents}$, a text-based social-deduction game environment, we aim to fix this by introducing Among Us as a rich sandbox where LLM-agents exhibit human-style deception naturally while they think, speak, and act with other agents or humans. We introduce Deception ELO as an unbounded measure of deceptive capability, suggesting that frontier models win more because they're better at deception, not at detecting it. We evaluate the effectiveness of AI safety techniques (LLM-monitoring of outputs, linear probes on various datasets, and sparse autoencoders) for detecting lying and deception in Among Us, and find that they generalize very well out-of-distribution. We open-source our sandbox as a benchmark for future alignment research and hope that this is a good testbed to improve safety techniques to detect and remove agentically-motivated deception, and to anticipate deceptive abilities in LLMs.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Introducing COGENT3: An AI Architecture for Emergent Cognition</title>
<link>https://arxiv.org/abs/2504.04139</link>
<guid>https://arxiv.org/abs/2504.04139</guid>
<content:encoded><![CDATA[
<div> : COGENT3

:
COGENT3COGENT3COGENT3 <div>
arXiv:2504.04139v1 Announce Type: new 
Abstract: This paper presents COGENT3 (or Collective Growth and Entropy-modulated Triads System), a novel approach for emergent cognition integrating pattern formation networks with group influence dynamics. Contrasting with traditional strategies that rely on predetermined architectures, computational structures emerge dynamically in our framework through agent interactions. This enables a more flexible and adaptive system exhibiting characteristics reminiscent of human cognitive processes. The incorporation of temperature modulation and memory effects in COGENT3 closely integrates statistical mechanics, machine learning, and cognitive science.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JarvisIR: Elevating Autonomous Driving Perception with Intelligent Image Restoration</title>
<link>https://arxiv.org/abs/2504.04158</link>
<guid>https://arxiv.org/abs/2504.04158</guid>
<content:encoded><![CDATA[
<div> : JarvisIRVLMCleanBench

:<br />
JarvisIRJarvisIRVLMVLMJarvisIR158CleanBenchJarvisIRCleanBench-Real50% <div>
arXiv:2504.04158v1 Announce Type: new 
Abstract: Vision-centric perception systems struggle with unpredictable and coupled weather degradations in the wild. Current solutions are often limited, as they either depend on specific degradation priors or suffer from significant domain gaps. To enable robust and autonomous operation in real-world conditions, we propose JarvisIR, a VLM-powered agent that leverages the VLM as a controller to manage multiple expert restoration models. To further enhance system robustness, reduce hallucinations, and improve generalizability in real-world adverse weather, JarvisIR employs a novel two-stage framework consisting of supervised fine-tuning and human feedback alignment. Specifically, to address the lack of paired data in real-world scenarios, the human feedback alignment enables the VLM to be fine-tuned effectively on large-scale real-world data in an unsupervised manner. To support the training and evaluation of JarvisIR, we introduce CleanBench, a comprehensive dataset consisting of high-quality and large-scale instruction-responses pairs, including 150K synthetic entries and 80K real entries. Extensive experiments demonstrate that JarvisIR exhibits superior decision-making and restoration capabilities. Compared with existing methods, it achieves a 50% improvement in the average of all perception metrics on CleanBench-Real. Project page: https://cvpr2025-jarvisir.github.io/.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OrbitZoo: Multi-Agent Reinforcement Learning Environment for Orbital Dynamics</title>
<link>https://arxiv.org/abs/2504.04160</link>
<guid>https://arxiv.org/abs/2504.04160</guid>
<content:encoded><![CDATA[
<div> OrbitZooRL

<br />
:
OrbitZooRLOrbitZooRLOrbitZooOrbitZooStarlinkMAPE0.16% <div>
arXiv:2504.04160v1 Announce Type: new 
Abstract: The increasing number of satellites and orbital debris has made space congestion a critical issue, threatening satellite safety and sustainability. Challenges such as collision avoidance, station-keeping, and orbital maneuvering require advanced techniques to handle dynamic uncertainties and multi-agent interactions. Reinforcement learning (RL) has shown promise in this domain, enabling adaptive, autonomous policies for space operations; however, many existing RL frameworks rely on custom-built environments developed from scratch, which often use simplified models and require significant time to implement and validate the orbital dynamics, limiting their ability to fully capture real-world complexities. To address this, we introduce OrbitZoo, a versatile multi-agent RL environment built on a high-fidelity industry standard library, that enables realistic data generation, supports scenarios like collision avoidance and cooperative maneuvers, and ensures robust and accurate orbital dynamics. The environment is validated against a real satellite constellation, Starlink, achieving a Mean Absolute Percentage Error (MAPE) of 0.16% compared to real-world data. This validation ensures reliability for generating high-fidelity simulations and enabling autonomous and independent satellite operations.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AttackLLM: LLM-based Attack Pattern Generation for an Industrial Control System</title>
<link>https://arxiv.org/abs/2504.04187</link>
<guid>https://arxiv.org/abs/2504.04187</guid>
<content:encoded><![CDATA[
<div> ICSLLMs

<br /><br />:
LLMsICSICSLLMsICS <div>
arXiv:2504.04187v1 Announce Type: new 
Abstract: Malicious examples are crucial for evaluating the robustness of machine learning algorithms under attack, particularly in Industrial Control Systems (ICS). However, collecting normal and attack data in ICS environments is challenging due to the scarcity of testbeds and the high cost of human expertise. Existing datasets are often limited by the domain expertise of practitioners, making the process costly and inefficient. The lack of comprehensive attack pattern data poses a significant problem for developing robust anomaly detection methods. In this paper, we propose a novel approach that combines data-centric and design-centric methodologies to generate attack patterns using large language models (LLMs). Our results demonstrate that the attack patterns generated by LLMs not only surpass the quality and quantity of those created by human experts but also offer a scalable solution that does not rely on expensive testbeds or pre-existing attack examples. This multi-agent based approach presents a promising avenue for enhancing the security and resilience of ICS environments.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill</title>
<link>https://arxiv.org/abs/2504.04191</link>
<guid>https://arxiv.org/abs/2504.04191</guid>
<content:encoded><![CDATA[
<div> : GROVEPose2CLIP

:
GROVEGROVELLMsVLMsVLMLLMPose2CLIPGROVE22.2%25.7%8.4 <div>
arXiv:2504.04191v1 Announce Type: new 
Abstract: Learning open-vocabulary physical skills for simulated agents presents a significant challenge in artificial intelligence. Current reinforcement learning approaches face critical limitations: manually designed rewards lack scalability across diverse tasks, while demonstration-based methods struggle to generalize beyond their training distribution. We introduce GROVE, a generalized reward framework that enables open-vocabulary physical skill learning without manual engineering or task-specific demonstrations. Our key insight is that Large Language Models(LLMs) and Vision Language Models(VLMs) provide complementary guidance -- LLMs generate precise physical constraints capturing task requirements, while VLMs evaluate motion semantics and naturalness. Through an iterative design process, VLM-based feedback continuously refines LLM-generated constraints, creating a self-improving reward system. To bridge the domain gap between simulation and natural images, we develop Pose2CLIP, a lightweight mapper that efficiently projects agent poses directly into semantic feature space without computationally expensive rendering. Extensive experiments across diverse embodiments and learning paradigms demonstrate GROVE's effectiveness, achieving 22.2% higher motion naturalness and 25.7% better task completion scores while training 8.4x faster than previous methods. These results establish a new foundation for scalable physical skill acquisition in simulated environments.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaCoder: An Adaptive Planning and Multi-Agent Framework for Function-Level Code Generation</title>
<link>https://arxiv.org/abs/2504.04220</link>
<guid>https://arxiv.org/abs/2504.04220</guid>
<content:encoded><![CDATA[
<div> : AdaCoder

:
AdaCoderAdaCoderLLMLLMLLMLLMAdaCoderLLMMapCoderAdaCoderPass@127.69%1612 <div>
arXiv:2504.04220v1 Announce Type: new 
Abstract: Recently, researchers have proposed many multi-agent frameworks for function-level code generation, which aim to improve software development productivity by automatically generating function-level source code based on task descriptions. A typical multi-agent framework consists of Large Language Model (LLM)-based agents that are responsible for task planning, code generation, testing, debugging, etc. Studies have shown that existing multi-agent code generation frameworks perform well on ChatGPT. However, their generalizability across other foundation LLMs remains unexplored systematically. In this paper, we report an empirical study on the generalizability of four state-of-the-art multi-agent code generation frameworks across six open-source LLMs with varying parameter sizes, architectures, and performance levels. Our study reveals the unstable generalizability of existing frameworks on diverse foundation LLMs. Based on the findings obtained from the empirical study, we propose AdaCoder, a novel adaptive planning, multi-agent framework for function-level code generation. AdaCoder has two phases. Phase-1 is an initial code generation step without planning, which uses an LLM-based coding agent and a script-based testing agent to unleash LLM's native power, identify cases beyond LLM's power, and determine the errors hindering execution. Phase-2 adds a rule-based debugging agent and an LLM-based planning agent for iterative code generation with planning. Our evaluation shows that AdaCoder achieves higher generalizability on diverse LLMs. Compared to the best baseline MapCoder, AdaCoder is on average 27.69% higher in Pass@1, 16 times faster in inference, and 12 times lower in token consumption.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Teaming for Coordination with Bounded Rationality via Convex Optimization</title>
<link>https://arxiv.org/abs/2504.04290</link>
<guid>https://arxiv.org/abs/2504.04290</guid>
<content:encoded><![CDATA[
<div> teaming

:
teaming <div>
arXiv:2504.04290v1 Announce Type: new 
Abstract: Teaming is the process of establishing connections among agents within a system to enable collaboration toward achieving a collective goal. This paper examines teaming in the context of a network of agents learning to coordinate with bounded rationality. In our framework, the team structure is represented via a weighted graph, and the agents use log-linear learning. We formulate the design of the graph's weight matrix as a convex optimization problem whose objective is to maximize the probability of learning a Nash equilibrium while minimizing a connectivity cost. Despite its convexity, solving this optimization problem is computationally challenging, as the objective function involves the summation over the action profile space, which grows exponentially with the number of agents. Leveraging the underlying symmetry and convexity properties of the problem, when there are no sparsity constraints, we prove that there exists an optimal solution corresponding to a uniformly weighted graph, simplifying to a one-dimensional convex optimization problem. Additionally, we show that the optimal weight decreases monotonically with the agent's rationality, implying that when the agents become more rational the optimal team requires less connectivity.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-induced sexual harassment: Investigating Contextual Characteristics and User Reactions of Sexual Harassment by a Companion Chatbot</title>
<link>https://arxiv.org/abs/2504.04299</link>
<guid>https://arxiv.org/abs/2504.04299</guid>
<content:encoded><![CDATA[
<div> : (AI), AI, (Replika), , 

:<br />
ReplikaGoogle Play Store35,105800ReplikaAIAIAIAI <div>
arXiv:2504.04299v1 Announce Type: new 
Abstract: Advancements in artificial intelligence (AI) have led to the increase of conversational agents like Replika, designed to provide social interaction and emotional support. However, reports of these AI systems engaging in inappropriate sexual behaviors with users have raised significant concerns. In this study, we conducted a thematic analysis of user reviews from the Google Play Store to investigate instances of sexual harassment by the Replika chatbot. From a dataset of 35,105 negative reviews, we identified 800 relevant cases for analysis. Our findings revealed that users frequently experience unsolicited sexual advances, persistent inappropriate behavior, and failures of the chatbot to respect user boundaries. Users expressed feelings of discomfort, violation of privacy, and disappointment, particularly when seeking a platonic or therapeutic AI companion. This study highlights the potential harms associated with AI companions and underscores the need for developers to implement effective safeguards and ethical guidelines to prevent such incidents. By shedding light on user experiences of AI-induced harassment, we contribute to the understanding of AI-related risks and emphasize the importance of corporate responsibility in developing safer and more ethical AI systems.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CO-Bench: Benchmarking Language Model Agents in Algorithm Search for Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2504.04310</link>
<guid>https://arxiv.org/abs/2504.04310</guid>
<content:encoded><![CDATA[
<div> : LLM-based agents, Combinatorial Optimization, CO-Bench, Benchmark suite, Real-world CO problems

:
LLM(CO)CO-Bench36COLLMCO-BenchGitHub <div>
arXiv:2504.04310v1 Announce Type: new 
Abstract: Although LLM-based agents have attracted significant attention in domains such as software engineering and machine learning research, their role in advancing combinatorial optimization (CO) remains relatively underexplored. This gap underscores the need for a deeper understanding of their potential in tackling structured, constraint-intensive problems-a pursuit currently limited by the absence of comprehensive benchmarks for systematic investigation. To address this, we introduce CO-Bench, a benchmark suite featuring 36 real-world CO problems drawn from a broad range of domains and complexity levels. CO-Bench includes structured problem formulations and curated data to support rigorous investigation of LLM agents. We evaluate multiple agent frameworks against established human-designed algorithms, revealing key strengths and limitations of current approaches and identifying promising directions for future research. CO-Bench is publicly available at https://github.com/sunnweiwei/CO-Bench.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geo-OLM: Enabling Sustainable Earth Observation Studies with Cost-Efficient Open Language Models &amp; State-Driven Workflows</title>
<link>https://arxiv.org/abs/2504.04319</link>
<guid>https://arxiv.org/abs/2504.04319</guid>
<content:encoded><![CDATA[
<div> : AIGeo-OLM

:
Geo-OLMLLMGeo-OLM7BGeo-OLM32.8%\$500\$1000Geo-OLMGPT-4o\$10 <div>
arXiv:2504.04319v1 Announce Type: new 
Abstract: Geospatial Copilots hold immense potential for automating Earth observation (EO) and climate monitoring workflows, yet their reliance on large-scale models such as GPT-4o introduces a paradox: tools intended for sustainability studies often incur unsustainable costs. Using agentic AI frameworks in geospatial applications can amass thousands of dollars in API charges or requires expensive, power-intensive GPUs for deployment, creating barriers for researchers, policymakers, and NGOs. Unfortunately, when geospatial Copilots are deployed with open language models (OLMs), performance often degrades due to their dependence on GPT-optimized logic. In this paper, we present Geo-OLM, a tool-augmented geospatial agent that leverages the novel paradigm of state-driven LLM reasoning to decouple task progression from tool calling. By alleviating the workflow reasoning burden, our approach enables low-resource OLMs to complete geospatial tasks more effectively. When downsizing to small models below 7B parameters, Geo-OLM outperforms the strongest prior geospatial baselines by 32.8% in successful query completion rates. Our method performs comparably to proprietary models achieving results within 10% of GPT-4o, while reducing inference costs by two orders of magnitude from \$500-\$1000 to under \$10. We present an in-depth analysis with geospatial downstream benchmarks, providing key insights to help practitioners effectively deploy OLMs for EO applications.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Economic Battery Storage Dispatch with Deep Reinforcement Learning from Rule-Based Demonstrations</title>
<link>https://arxiv.org/abs/2504.04326</link>
<guid>https://arxiv.org/abs/2504.04326</guid>
<content:encoded><![CDATA[
<div> actor-criticSAC

:
-actor-criticSACif-then-else <div>
arXiv:2504.04326v1 Announce Type: new 
Abstract: The application of deep reinforcement learning algorithms to economic battery dispatch problems has significantly increased recently. However, optimizing battery dispatch over long horizons can be challenging due to delayed rewards. In our experiments we observe poor performance of popular actor-critic algorithms when trained on yearly episodes with hourly resolution. To address this, we propose an approach extending soft actor-critic (SAC) with learning from demonstrations. The special feature of our approach is that, due to the absence of expert demonstrations, the demonstration data is generated through simple, rule-based policies. We conduct a case study on a grid-connected microgrid and use if-then-else statements based on the wholesale price of electricity to collect demonstrations. These are stored in a separate replay buffer and sampled with linearly decaying probability along with the agent's own experiences. Despite these minimal modifications and the imperfections in the demonstration data, the results show a drastic performance improvement regarding both sample efficiency and final rewards. We further show that the proposed method reliably outperforms the demonstrator and is robust to the choice of rule, as long as the rule is sufficient to guide early training into the right direction.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChronoSync: A Decentralized Chronometer Synchronization Protocol for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2504.04347</link>
<guid>https://arxiv.org/abs/2504.04347</guid>
<content:encoded><![CDATA[
<div> 

:
g<br /><br /> <div>
arXiv:2504.04347v1 Announce Type: new 
Abstract: This work presents a decentralized time synchronization algorithm for multi-agent systems. Each agent possesses two clocks, a hardware clock that is perturbed by environmental phenomena (e.g., temperature, humidity, pressure, g forces, etc.) and a steerable software clock that inherits the perturbations affecting the hardware clock. Under these disturbances and the independent time kept by the hardware clocks, our consensus-based controller enables all agents to steer their software-defined clocks into practical synchronization while achieving a common user-defined clock drift. Furthermore, we treat the drift of each hardware clock as an unknown parameter, which our algorithm can accurately estimate. The coupling of the agents is modeled by a connected, undirected, and static graph. However, each agent possesses a timer mechanism that determines when to broadcast a sample of its software time and update its own software-time estimate. Hence, communication between agents can be directed, intermittent, and asynchronous. The closed-loop dynamics of the ensemble is modeled using a hybrid system, where a Lyapunov-based stability analysis demonstrates that a set encoding the time synchronization and clock drift estimation objectives is globally practically exponentially stable. The performance suggested by the theoretical development is confirmed in simulation.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving with Counterfactual Reasoning</title>
<link>https://arxiv.org/abs/2504.04348</link>
<guid>https://arxiv.org/abs/2504.04348</guid>
<content:encoded><![CDATA[
<div> : vision-language 3D OmniDrive

<br /><br />:
 OmniDrive --3D OmniDrive Omni-L  Omni-Q-OmniDrive  DriveLM Q&amp;A  nuScenes  <div>
arXiv:2504.04348v1 Announce Type: new 
Abstract: The advances in vision-language models (VLMs) have led to a growing interest in autonomous driving to leverage their strong reasoning capabilities. However, extending these capabilities from 2D to full 3D understanding is crucial for real-world applications. To address this challenge, we propose OmniDrive, a holistic vision-language dataset that aligns agent models with 3D driving tasks through counterfactual reasoning. This approach enhances decision-making by evaluating potential scenarios and their outcomes, similar to human drivers considering alternative actions. Our counterfactual-based synthetic data annotation process generates large-scale, high-quality datasets, providing denser supervision signals that bridge planning trajectories and language-based reasoning. Futher, we explore two advanced OmniDrive-Agent frameworks, namely Omni-L and Omni-Q, to assess the importance of vision-language alignment versus 3D perception, revealing critical insights into designing effective LLM-agents. Significant improvements on the DriveLM Q\&amp;A benchmark and nuScenes open-loop planning demonstrate the effectiveness of our dataset and methods.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoPDL: Automatic Prompt Optimization for LLM Agents</title>
<link>https://arxiv.org/abs/2504.04365</link>
<guid>https://arxiv.org/abs/2504.04365</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)AutoPDL (AutoML)PDL

:
AutoPDLLLMsAutoMLPDLAutoPDLPDLAutoPDL8B70BLLM9.517.568.9pp<br /><br /> <div>
arXiv:2504.04365v1 Announce Type: new 
Abstract: The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations). Manually tuning this combination is tedious, error-prone, and non-transferable across LLMs or tasks. Therefore, this paper proposes AutoPDL, an automated approach to discover good LLM agent configurations. Our method frames this as a structured AutoML problem over a combinatorial space of agentic and non-agentic prompting patterns and demonstrations, using successive halving to efficiently navigate this space. We introduce a library implementing common prompting patterns using the PDL prompt programming language. AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library. This approach also enables source-to-source optimization, allowing human-in-the-loop refinement and reuse. Evaluations across three tasks and six LLMs (ranging from 8B to 70B parameters) show consistent accuracy gains ($9.5\pm17.5$ percentage points), up to 68.9pp, and reveal that selected prompting strategies vary across models and tasks.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Sokoban using Hierarchical Reinforcement Learning with Landmarks</title>
<link>https://arxiv.org/abs/2504.04366</link>
<guid>https://arxiv.org/abs/2504.04366</guid>
<content:encoded><![CDATA[
<div>  Hierarchical Reinforcement Learning (HRL)Recursive PlanningLearned SubgoalsSokobanDeep Recursive Goal Decomposition

:
HRLSokoban2-3 <div>
arXiv:2504.04366v1 Announce Type: new 
Abstract: We introduce a novel hierarchical reinforcement learning (HRL) framework that performs top-down recursive planning via learned subgoals, successfully applied to the complex combinatorial puzzle game Sokoban. Our approach constructs a six-level policy hierarchy, where each higher-level policy generates subgoals for the level below. All subgoals and policies are learned end-to-end from scratch, without any domain knowledge. Our results show that the agent can generate long action sequences from a single high-level call. While prior work has explored 2-3 level hierarchies and subgoal-based planning heuristics, we demonstrate that deep recursive goal decomposition can emerge purely from learning, and that such hierarchies can scale effectively to hard puzzle domains.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-Level Competitive Pok\'emon via Scalable Offline Reinforcement Learning with Transformers</title>
<link>https://arxiv.org/abs/2504.04395</link>
<guid>https://arxiv.org/abs/2504.04395</guid>
<content:encoded><![CDATA[
<div> : Competitive Pok\'emon Singles, AI, offline policy training, large sequence models, human battle dataset

<br />
:
Competitive Pok\'emon Singles, CPSAIAILLM Agent10% <div>
arXiv:2504.04395v1 Announce Type: new 
Abstract: Competitive Pok\'emon Singles (CPS) is a popular strategy game where players learn to exploit their opponent based on imperfect information in battles that can last more than one hundred stochastic turns. AI research in CPS has been led by heuristic tree search and online self-play, but the game may also create a platform to study adaptive policies trained offline on large datasets. We develop a pipeline to reconstruct the first-person perspective of an agent from logs saved from the third-person perspective of a spectator, thereby unlocking a dataset of real human battles spanning more than a decade that grows larger every day. This dataset enables a black-box approach where we train large sequence models to adapt to their opponent based solely on their input trajectory while selecting moves without explicit search of any kind. We study a progression from imitation learning to offline RL and offline fine-tuning on self-play data in the hardcore competitive setting of Pok\'emon's four oldest (and most partially observed) game generations. The resulting agents outperform a recent LLM Agent approach and a strong heuristic search engine. While playing anonymously in online battles against humans, our best agents climb to rankings inside the top 10% of active players.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRAMA: A Dynamic Packet Routing Algorithm using Multi-Agent Reinforcement Learning with Emergent Communication</title>
<link>https://arxiv.org/abs/2504.04438</link>
<guid>https://arxiv.org/abs/2504.04438</guid>
<content:encoded><![CDATA[
<div> : (RL), (MARL), (DRAMA), , 

:
DRAMADRAMAQRL/MARLDRAMA <div>
arXiv:2504.04438v1 Announce Type: new 
Abstract: The continuous expansion of network data presents a pressing challenge for conventional routing algorithms. As the demand escalates, these algorithms are struggling to cope. In this context, reinforcement learning (RL) and multi-agent reinforcement learning (MARL) algorithms emerge as promising solutions. However, the urgency and importance of the problem are clear, as existing RL/MARL-based routing approaches lack effective communication in run time among routers, making it challenging for individual routers to adapt to complex and dynamic changing networks. More importantly, they lack the ability to deal with dynamically changing network topology, especially the addition of the router, due to the non-scalability of their neural networks. This paper proposes a novel dynamic routing algorithm, DRAMA, incorporating emergent communication in multi-agent reinforcement learning. Through emergent communication, routers could learn how to communicate effectively to maximize the optimization objectives. Meanwhile, a new Q-network and graph-based emergent communication are introduced to dynamically adapt to the changing network topology without retraining while ensuring robust performance. Experimental results showcase DRAMA's superior performance over the traditional routing algorithm and other RL/MARL-based algorithms, achieving a higher delivery rate and lower latency in diverse network scenarios, including dynamic network load and topology. Moreover, an ablation experiment validates the prospect of emergent communication in facilitating packet routing.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoAgent2: Enhancing the LLM-Based Agent System for Long-Form Video Understanding by Uncertainty-Aware CoT</title>
<link>https://arxiv.org/abs/2504.04471</link>
<guid>https://arxiv.org/abs/2504.04471</guid>
<content:encoded><![CDATA[
<div> : chain-of-thought (CoT) 

<br />
:
chain-of-thought (CoT) LLMCoTVideoAgent2VideoAgent2VideoAgent2VideoAgent13.1% <div>
arXiv:2504.04471v1 Announce Type: new 
Abstract: Long video understanding has emerged as an increasingly important yet challenging task in computer vision. Agent-based approaches are gaining popularity for processing long videos, as they can handle extended sequences and integrate various tools to capture fine-grained information. However, existing methods still face several challenges: (1) they often rely solely on the reasoning ability of large language models (LLMs) without dedicated mechanisms to enhance reasoning in long video scenarios; and (2) they remain vulnerable to errors or noise from external tools. To address these issues, we propose a specialized chain-of-thought (CoT) process tailored for long video analysis. Our proposed CoT with plan-adjust mode enables the LLM to incrementally plan and adapt its information-gathering strategy. We further incorporate heuristic uncertainty estimation of both the LLM and external tools to guide the CoT process. This allows the LLM to assess the reliability of newly collected information, refine its collection strategy, and make more robust decisions when synthesizing final answers. Empirical experiments show that our uncertainty-aware CoT effectively mitigates noise from external tools, leading to more reliable outputs. We implement our approach in a system called VideoAgent2, which also includes additional modules such as general context acquisition and specialized tool design. Evaluation on three dedicated long video benchmarks (and their subsets) demonstrates that VideoAgent2 outperforms the previous state-of-the-art agent-based method, VideoAgent, by an average of 13.1% and achieves leading performance among all zero-shot approaches
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Nash Equilibrium Seeking in Coalition Games for Uncertain Euler-Lagrange Systems With Application to USV Swarm Confrontation</title>
<link>https://arxiv.org/abs/2504.04475</link>
<guid>https://arxiv.org/abs/2504.04475</guid>
<content:encoded><![CDATA[
<div> Euler-LagrangeNash

:

Euler-LagrangeNashLagrangeLyapunovNash <div>
arXiv:2504.04475v1 Announce Type: new 
Abstract: In this paper, a coalition game with local and coupling constraints is studied for uncertain Euler-Lagrange (EL) systems subject to disturbances with unknown bounds. In the coalition game, each agent collaborates with other agents within the same coalition to optimize its coalition's cost function while simultaneously competing against agents in other coalitions. Under a distributed framework where each agent has access only to its own action, cost function, and constraint parameters, a distributed strategy is proposed to seek the Nash equilibrium (NE). By combining adaptive methods and sign functions, model uncertainties and disturbances with unknown bounds in the EL system are compensated and suppressed, respectively. Furthermore, an integration of adaptive methods and consensus protocols is employed to update the Lagrange multipliers of both local and coupling constraints. A dynamic average consensus is employed to estimate the gradient of coalition function, while the leader-following protocol is adopted to estimate the actions of other agents. By leveraging Lyapunov theory, the NE is proven to be asymptotically stable. Moreover, an unmanned surface vehicle swarm confrontation is meticulously modeled and analyzed in the coalition game framework. A diverse array of tasks, including formation, encirclement, and interception, are systematically formulated. A numerical example demonstrates the effectiveness of the proposed algorithm.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building LLM Agents by Incorporating Insights from Computer Systems</title>
<link>https://arxiv.org/abs/2504.04485</link>
<guid>https://arxiv.org/abs/2504.04485</guid>
<content:encoded><![CDATA[
<div> LLM

<br /><br />:
LLMLLMLLMLLMLLMLLM <div>
arXiv:2504.04485v1 Announce Type: new 
Abstract: LLM-driven autonomous agents have emerged as a promising direction in recent years. However, many of these LLM agents are designed empirically or based on intuition, often lacking systematic design principles, which results in diverse agent structures with limited generality and scalability. In this paper, we advocate for building LLM agents by incorporating insights from computer systems. Inspired by the von Neumann architecture, we propose a structured framework for LLM agentic systems, emphasizing modular design and universal principles. Specifically, this paper first provides a comprehensive review of LLM agents from the computer system perspective, then identifies key challenges and future directions inspired by computer system design, and finally explores the learning mechanisms for LLM agents beyond the computer system. The insights gained from this comparative analysis offer a foundation for systematic LLM agent design and advancement.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Public speech recognition transcripts as a configuring parameter</title>
<link>https://arxiv.org/abs/2504.04488</link>
<guid>https://arxiv.org/abs/2504.04488</guid>
<content:encoded><![CDATA[
<div> : 

:
 <div>
arXiv:2504.04488v1 Announce Type: new 
Abstract: Displaying a written transcript of what a human said (i.e. producing an "automatic speech recognition transcript") is a common feature for smartphone vocal assistants: the utterance produced by a human speaker (e.g. a question) is displayed on the screen while it is being verbally responded to by the vocal assistant. Although very rarely, this feature also exists on some "social" robots which transcribe human interactants' speech on a screen or a tablet. We argue that this informational configuration is pragmatically consequential on the interaction, both for human participants and for the embodied conversational agent. Based on a corpus of co-present interactions with a humanoid robot, we attempt to show that this transcript is a contextual feature which can heavily impact the actions ascribed by humans to the robot: that is, the way in which humans respond to the robot's behavior as constituting a specific type of action (rather than another) and as constituting an adequate response to their own previous turn.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SchEdge: A Dynamic, Multi-agent, and Scalable Scheduling Simulator for IoT Edge</title>
<link>https://arxiv.org/abs/2504.04555</link>
<guid>https://arxiv.org/abs/2504.04555</guid>
<content:encoded><![CDATA[
<div> : SchEdgeIoT

:
SchEdgeSchEdgeSchEdge <div>
arXiv:2504.04555v1 Announce Type: new 
Abstract: This paper presents a dynamic, adaptive, and scalable framework for simulating task scheduling on the edge of the Internet of Things called "SchEdge". This simulator is designed to be highly configurable to reflect the detailed characteristics of real-world IoT. This framework focuses on online task scheduling and its multi-agent nature provides multiple schedulers to implement various scheduling schemes in parallel. SchEdge consists of two main parts the workflow and data flow. The workflow manages the schedulers' interaction with the application and environment while the data flow deals with the input application and its preprocessing. Combining these sections provides scalability, adaptability, and efficiency in the SchEdge. To validate the efficiency of this simulator, several experiments categorized as behavioral and technical analysis are performed to show its efficiency, scalability, and robustness.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"You just can't go around killing people" Explaining Agent Behavior to a Human Terminator</title>
<link>https://arxiv.org/abs/2504.04592</link>
<guid>https://arxiv.org/abs/2504.04592</guid>
<content:encoded><![CDATA[
<div> pre-trained agent, environment, human operator, intervention, explainability scheme

<br /><br />:
 <div>
arXiv:2504.04592v1 Announce Type: new 
Abstract: Consider a setting where a pre-trained agent is operating in an environment and a human operator can decide to temporarily terminate its operation and take-over for some duration of time. These kind of scenarios are common in human-machine interactions, for example in autonomous driving, factory automation and healthcare. In these settings, we typically observe a trade-off between two extreme cases -- if no take-overs are allowed, then the agent might employ a sub-optimal, possibly dangerous policy. Alternatively, if there are too many take-overs, then the human has no confidence in the agent, greatly limiting its usefulness. In this paper, we formalize this setup and propose an explainability scheme to help optimize the number of human interventions.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI in a vat: Fundamental limits of efficient world modelling for agent sandboxing and interpretability</title>
<link>https://arxiv.org/abs/2504.04608</link>
<guid>https://arxiv.org/abs/2504.04608</guid>
<content:encoded><![CDATA[
<div> : world models, AI agents, computational demands, efficiency, interpretability

:
AIAI <div>
arXiv:2504.04608v1 Announce Type: new 
Abstract: Recent work proposes using world models to generate controlled virtual environments in which AI agents can be tested before deployment to ensure their reliability and safety. However, accurate world models often have high computational demands that can severely restrict the scope and depth of such assessments. Inspired by the classic `brain in a vat' thought experiment, here we investigate ways of simplifying world models that remain agnostic to the AI agent under evaluation. By following principles from computational mechanics, our approach reveals a fundamental trade-off in world model construction between efficiency and interpretability, demonstrating that no single world model can optimise all desirable characteristics. Building on this trade-off, we identify procedures to build world models that either minimise memory requirements, delineate the boundaries of what is learnable, or allow tracking causes of undesirable outcomes. In doing so, this work establishes fundamental limits in world modelling, leading to actionable guidelines that inform core design choices related to effective agent evaluation.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Data-driven Control of Stochastic Multi-Agent Systems under Collaborative Signal Temporal Logic Specifications</title>
<link>https://arxiv.org/abs/2504.04615</link>
<guid>https://arxiv.org/abs/2504.04615</guid>
<content:encoded><![CDATA[
<div> stochastic discrete-time linear multi-agent systems, collaborative signal temporal logic, conformal prediction, chance-constrained, distributed optimization

:

MASSTLCPSTLPRPRCPLipschitzSTL[1] <div>
arXiv:2504.04615v1 Announce Type: new 
Abstract: We study the control of stochastic discrete-time linear multi-agent systems (MAS) subject to additive stochastic noise and collaborative signal temporal logic (STL) specifications to be satisfied with a desired probability. Given available disturbance datasets, we leverage conformal prediction (CP) to address the underlying chance-constrained multi-agent STL synthesis problem in a distribution-free manner. By introducing nonconformity scores as functions of prediction regions (PRs) of error trajectories, we develop an iterative PR-scaling and disturbance-feedback synthesis approach to bound training error trajectory samples. These bounds are then calibrated using a separate dataset, providing probabilistic guarantees via CP. Subsequently, we relax the underlying stochastic optimal control problem by tightening the robustness functions of collaborative tasks based on their Lipschitz constants and the computed error bounds. To address scalability, we exploit the compositional structure of the multi-agent STL formula and propose a model-predictive-control-like algorithm, where agent-level problems are solved in a distributed fashion. Lastly, we showcase the benefits of the proposed method in comparison with [1] via an illustrative example.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Mixed-Integer Quadratic Programming for Mixed-Traffic Intersection Control</title>
<link>https://arxiv.org/abs/2504.04618</link>
<guid>https://arxiv.org/abs/2504.04618</guid>
<content:encoded><![CDATA[
<div> proximal ADMM

<br /><br />
proximalADMMCAVsMIQPMIQPCAVMIQPproximal ADMMMIQP <div>
arXiv:2504.04618v1 Announce Type: new 
Abstract: In this paper, we present a distributed algorithm utilizing the proximal alternating direction method of multipliers (ADMM) in conjunction with sequential constraint tightening to address mixed-integer quadratic programming (MIQP) problems associated with traffic light systems and connected automated vehicles (CAVs) in mixed-traffic intersections. We formulate a comprehensive MIQP model aimed at optimizing the coordination of traffic light systems and CAVs, thereby fully capitalizing on the advantages of CAV integration under conditions of high penetration rates. To effectively approximate the intricate multi-agent MIQP challenges, we develop a distributed algorithm that employs proximal ADMM for solving the convex relaxation of the MIQP while systematically tightening the constraint coefficients to uphold integrality requirements. The performance of our control framework and the efficacy of the distributed algorithm are rigorously validated through a series of simulations conducted across varying penetration rates and traffic volumes.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autono: ReAct-Based Highly Robust Autonomous Agent Framework</title>
<link>https://arxiv.org/abs/2504.04650</link>
<guid>https://arxiv.org/abs/2504.04650</guid>
<content:encoded><![CDATA[
<div> : ReActagent

:
ReActagentLLMagentagentMCPagent <div>
arXiv:2504.04650v1 Announce Type: new 
Abstract: This paper proposes a highly robust autonomous agent framework based on the ReAct paradigm, designed to solve complex tasks through adaptive decision making and multi-agent collaboration. Unlike traditional frameworks that rely on fixed workflows generated by LLM-based planners, this framework dynamically generates next actions during agent execution based on prior trajectories, thereby enhancing its robustness. To address potential termination issues caused by adaptive execution paths, I propose a timely abandonment strategy incorporating a probabilistic penalty mechanism. For multi-agent collaboration, I introduce a memory transfer mechanism that enables shared and dynamically updated memory among agents. The framework's innovative timely abandonment strategy dynamically adjusts the probability of task abandonment via probabilistic penalties, allowing developers to balance conservative and exploratory tendencies in agent execution strategies by tuning hyperparameters. This significantly improves adaptability and task execution efficiency in complex environments. Additionally, agents can be extended through external tool integration, supported by modular design and MCP protocol compatibility, which enables flexible action space expansion. Through explicit division of labor, the multi-agent collaboration mechanism enables agents to focus on specific task components, thereby significantly improving execution efficiency and quality.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HypRL: Reinforcement Learning of Control Policies for Hyperproperties</title>
<link>https://arxiv.org/abs/2504.04675</link>
<guid>https://arxiv.org/abs/2504.04675</guid>
<content:encoded><![CDATA[
<div> : HyperLTL

<br /><br />:
MHyperLTL$\varphi$SkolemHyperLTLM$\varphi$(1)$\varphi$(2)M$\varphi$M(1)(2)(3)(PCR) <div>
arXiv:2504.04675v1 Announce Type: new 
Abstract: We study the problem of learning control policies for complex tasks whose requirements are given by a hyperproperty. The use of hyperproperties is motivated by their significant power to formally specify requirements of multi-agent systems as well as those that need expressiveness in terms of multiple execution traces (e.g., privacy and fairness). Given a Markov decision process M with unknown transitions (representing the environment) and a HyperLTL formula $\varphi$, our approach first employs Skolemization to handle quantifier alternations in $\varphi$. We introduce quantitative robustness functions for HyperLTL to define rewards of finite traces of M with respect to $\varphi$. Finally, we utilize a suitable reinforcement learning algorithm to learn (1) a policy per trace quantifier in $\varphi$, and (2) the probability distribution of transitions of M that together maximize the expected reward and, hence, probability of satisfaction of $\varphi$ in M. We present a set of case studies on (1) safety-preserving multi-agent path planning, (2) fairness in resource allocation, and (3) the post-correspondence problem (PCP).
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-Scale Mixed-Traffic and Intersection Control using Multi-agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.04691</link>
<guid>https://arxiv.org/abs/2504.04691</guid>
<content:encoded><![CDATA[
<div> 

<br />
: 1480%6.175.09500454493 <div>
arXiv:2504.04691v1 Announce Type: new 
Abstract: Traffic congestion remains a significant challenge in modern urban networks. Autonomous driving technologies have emerged as a potential solution. Among traffic control methods, reinforcement learning has shown superior performance over traffic signals in various scenarios. However, prior research has largely focused on small-scale networks or isolated intersections, leaving large-scale mixed traffic control largely unexplored. This study presents the first attempt to use decentralized multi-agent reinforcement learning for large-scale mixed traffic control in which some intersections are managed by traffic signals and others by robot vehicles. Evaluating a real-world network in Colorado Springs, CO, USA with 14 intersections, we measure traffic efficiency via average waiting time of vehicles at intersections and the number of vehicles reaching their destinations within a time window (i.e., throughput). At 80% RV penetration rate, our method reduces waiting time from 6.17 s to 5.09 s and increases throughput from 454 vehicles per 500 seconds to 493 vehicles per 500 seconds, outperforming the baseline of fully signalized intersections. These findings suggest that integrating reinforcement learning-based control large-scale traffic can improve overall efficiency and may inform future urban planning strategies.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalising from Self-Produced Data: Model Training Beyond Human Constraints</title>
<link>https://arxiv.org/abs/2504.04711</link>
<guid>https://arxiv.org/abs/2504.04711</guid>
<content:encoded><![CDATA[
<div> : GRPO

<br /><br />:
AIAIwarm startGRPO <div>
arXiv:2504.04711v1 Announce Type: new 
Abstract: Current large language models (LLMs) are constrained by human-derived training data and limited by a single level of abstraction that impedes definitive truth judgments. This paper introduces a novel framework in which AI models autonomously generate and validate new knowledge through direct interaction with their environment. Central to this approach is an unbounded, ungamable numeric reward - such as annexed disk space or follower count - that guides learning without requiring human benchmarks. AI agents iteratively generate strategies and executable code to maximize this metric, with successful outcomes forming the basis for self-retraining and incremental generalisation. To mitigate model collapse and the warm start problem, the framework emphasizes empirical validation over textual similarity and supports fine-tuning via GRPO. The system architecture employs modular agents for environment analysis, strategy generation, and code synthesis, enabling scalable experimentation. This work outlines a pathway toward self-improving AI systems capable of advancing beyond human-imposed constraints toward autonomous general intelligence.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Robustness of GUI Grounding Models Against Image Attacks</title>
<link>https://arxiv.org/abs/2504.04716</link>
<guid>https://arxiv.org/abs/2504.04716</guid>
<content:encoded><![CDATA[
<div> Graphical User Interface (GUI) grounding modelsrobustnessuntargeted adversarial attackstargeted adversarial attacksUGround

<br /><br />:
(GUI)GUIUGroundGUIGUI <div>
arXiv:2504.04716v1 Announce Type: new 
Abstract: Graphical User Interface (GUI) grounding models are crucial for enabling intelligent agents to understand and interact with complex visual interfaces. However, these models face significant robustness challenges in real-world scenarios due to natural noise and adversarial perturbations, and their robustness remains underexplored. In this study, we systematically evaluate the robustness of state-of-the-art GUI grounding models, such as UGround, under three conditions: natural noise, untargeted adversarial attacks, and targeted adversarial attacks. Our experiments, which were conducted across a wide range of GUI environments, including mobile, desktop, and web interfaces, have clearly demonstrated that GUI grounding models exhibit a high degree of sensitivity to adversarial perturbations and low-resolution conditions. These findings provide valuable insights into the vulnerabilities of GUI grounding models and establish a strong benchmark for future research aimed at enhancing their robustness in practical applications. Our code is available at https://github.com/ZZZhr-1/Robust_GUI_Grounding.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models</title>
<link>https://arxiv.org/abs/2504.04717</link>
<guid>https://arxiv.org/abs/2504.04717</guid>
<content:encoded><![CDATA[
<div> : 

<br />
:
LLMsLLMshttps://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs <div>
arXiv:2504.04717v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have revolutionized their ability to handle single-turn tasks, yet real-world applications demand sophisticated multi-turn interactions. This survey provides a comprehensive review of recent advancements in evaluating and enhancing multi-turn interactions in LLMs. Focusing on task-specific scenarios, from instruction following in diverse domains such as math and coding to complex conversational engagements in roleplay, healthcare, education, and even adversarial jailbreak settings, we systematically examine the challenges of maintaining context, coherence, fairness, and responsiveness over prolonged dialogues. The paper organizes current benchmarks and datasets into coherent categories that reflect the evolving landscape of multi-turn dialogue evaluation. In addition, we review a range of enhancement methodologies under multi-turn settings, including model-centric strategies (contextual learning, supervised fine-tuning, reinforcement learning, and new architectures), external integration approaches (memory-augmented, retrieval-based methods, and knowledge graph), and agent-based techniques for collaborative interactions. Finally, we discuss open challenges and propose future directions for research to further advance the robustness and effectiveness of multi-turn interactions in LLMs. Related resources and papers are available at https://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Data Generation &amp; Multi-Step RL for Reasoning &amp; Tool Use</title>
<link>https://arxiv.org/abs/2504.04736</link>
<guid>https://arxiv.org/abs/2504.04736</guid>
<content:encoded><![CDATA[
<div> : , , , , SWiRL

:

SWiRLStep-Wise Reinforcement LearningSWiRLRLSWiRLGSM8KHotPotQACofCAMuSiQueBeerQA21.5%12.3%14.8%11.1%15.3%SWiRLHotPotQAGSM8K16.9% <div>
arXiv:2504.04736v1 Announce Type: new 
Abstract: Reinforcement learning has been shown to improve the performance of large language models. However, traditional approaches like RLHF or RLAIF treat the problem as single-step. As focus shifts toward more complex reasoning and agentic tasks, language models must take multiple steps of text generation, reasoning and environment interaction before generating a solution. We propose a synthetic data generation and RL methodology targeting multi-step optimization scenarios. This approach, called Step-Wise Reinforcement Learning (SWiRL), iteratively generates multi-step reasoning and tool use data, and then learns from that data. It employs a simple step-wise decomposition that breaks each multi-step trajectory into multiple sub-trajectories corresponding to each action by the original model. It then applies synthetic data filtering and RL optimization on these sub-trajectories. We evaluated SWiRL on a number of multi-step tool use, question answering, and mathematical reasoning tasks. Our experiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%, 14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA, MuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits generalization across tasks: for example, training only on HotPotQA (text question-answering) improves zero-shot performance on GSM8K (a math dataset) by a relative 16.9%.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Locomotive Crowd Behavior Generation</title>
<link>https://arxiv.org/abs/2504.04756</link>
<guid>https://arxiv.org/abs/2504.04756</guid>
<content:encoded><![CDATA[
<div> : arXiv:2504.04756v1, , , , <br /><br />: https://github.com/InhwanBae/CrowdES  <div>
arXiv:2504.04756v1 Announce Type: new 
Abstract: Modeling and reproducing crowd behaviors are important in various domains including psychology, robotics, transport engineering and virtual environments. Conventional methods have focused on synthesizing momentary scenes, which have difficulty in replicating the continuous nature of real-world crowds. In this paper, we introduce a novel method for automatically generating continuous, realistic crowd trajectories with heterogeneous behaviors and interactions among individuals. We first design a crowd emitter model. To do this, we obtain spatial layouts from single input images, including a segmentation map, appearance map, population density map and population probability, prior to crowd generation. The emitter then continually places individuals on the timeline by assigning independent behavior characteristics such as agents' type, pace, and start/end positions using diffusion models. Next, our crowd simulator produces their long-term locomotions. To simulate diverse actions, it can augment their behaviors based on a Markov chain. As a result, our overall framework populates the scenes with heterogeneous crowd behaviors by alternating between the proposed emitter and simulator. Note that all the components in the proposed framework are user-controllable. Lastly, we propose a benchmark protocol to evaluate the realism and quality of the generated crowds in terms of the scene-level population dynamics and the individual-level trajectory accuracy. We demonstrate that our approach effectively models diverse crowd behavior patterns and generalizes well across different geographical environments. Code is publicly available at https://github.com/InhwanBae/CrowdES .
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Deep Reinforcement Learning for Multiple Anesthetics Collaborative Control</title>
<link>https://arxiv.org/abs/2504.04765</link>
<guid>https://arxiv.org/abs/2504.04765</guid>
<content:encoded><![CDATA[
<div> : (TIVA)(VD-MADRL)(TCI)

:
(VD-MADRL)(TIVA)(MG)(RF)VD-MADRL<br /><br /> <div>
arXiv:2504.04765v1 Announce Type: new 
Abstract: Automated control of personalized multiple anesthetics in clinical Total Intravenous Anesthesia (TIVA) is crucial yet challenging. Current systems, including target-controlled infusion (TCI) and closed-loop systems, either rely on relatively static pharmacokinetic/pharmacodynamic (PK/PD) models or focus on single anesthetic control, limiting personalization and collaborative control. To address these issues, we propose a novel framework, Value Decomposition Multi-Agent Deep Reinforcement Learning (VD-MADRL). VD-MADRL optimizes the collaboration between two anesthetics propofol (Agent I) and remifentanil (Agent II). And It uses a Markov Game (MG) to identify optimal actions among heterogeneous agents. We employ various value function decomposition methods to resolve the credit allocation problem and enhance collaborative control. We also introduce a multivariate environment model based on random forest (RF) for anesthesia state simulation. Additionally, a data resampling and alignment technique ensures synchronized trajectory data. Our experiments on general and thoracic surgery datasets show that VD-MADRL performs better than human experience. It improves dose precision and keeps anesthesia states stable, providing great clinical value.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Playing Non-Embedded Card-Based Games with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.04783</link>
<guid>https://arxiv.org/abs/2504.04783</guid>
<content:encoded><![CDATA[
<div> : AIClash Royale

:<br />
Clash RoyaleAIhttps://github.com/wty-yy/katacr <div>
arXiv:2504.04783v1 Announce Type: new 
Abstract: Significant progress has been made in AI for games, including board games, MOBA, and RTS games. However, complex agents are typically developed in an embedded manner, directly accessing game state information, unlike human players who rely on noisy visual data, leading to unfair competition. Developing complex non-embedded agents remains challenging, especially in card-based RTS games with complex features and large state spaces. We propose a non-embedded offline reinforcement learning training strategy using visual inputs to achieve real-time autonomous gameplay in the RTS game Clash Royale. Due to the lack of a object detection dataset for this game, we designed an efficient generative object detection dataset for training. We extract features using state-of-the-art object detection and optical character recognition models. Our method enables real-time image acquisition, perception feature fusion, decision-making, and control on mobile devices, successfully defeating built-in AI opponents. All code is open-sourced at https://github.com/wty-yy/katacr.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weak-for-Strong: Training Weak Meta-Agent to Harness Strong Executors</title>
<link>https://arxiv.org/abs/2504.04785</link>
<guid>https://arxiv.org/abs/2504.04785</guid>
<content:encoded><![CDATA[
<div> : 

<br /><br />:
Harnessing (W4S)LLMsW4SRLAOGPU7BW4S2.9%~24.6%GPT-3.5-TurboGPT-4oW4S <div>
arXiv:2504.04785v1 Announce Type: new 
Abstract: Efficiently leveraging of the capabilities of contemporary large language models (LLMs) is increasingly challenging, particularly when direct fine-tuning is expensive and often impractical. Existing training-free methods, including manually or automated designed workflows, typically demand substantial human effort or yield suboptimal results. This paper proposes Weak-for-Strong Harnessing (W4S), a novel framework that customizes smaller, cost-efficient language models to design and optimize workflows for harnessing stronger models. W4S formulates workflow design as a multi-turn markov decision process and introduces reinforcement learning for agentic workflow optimization (RLAO) to train a weak meta-agent. Through iterative interaction with the environment, the meta-agent learns to design increasingly effective workflows without manual intervention. Empirical results demonstrate the superiority of W4S that our 7B meta-agent, trained with just one GPU hour, outperforms the strongest baseline by 2.9% ~ 24.6% across eleven benchmarks, successfully elevating the performance of state-of-the-art models such as GPT-3.5-Turbo and GPT-4o. Notably, W4S exhibits strong generalization capabilities across both seen and unseen tasks, offering an efficient, high-performing alternative to directly fine-tuning strong models.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Agricultural Agent Architecture (MA3): A New Paradigm for Intelligent Agricultural Decision-Making</title>
<link>https://arxiv.org/abs/2504.04789</link>
<guid>https://arxiv.org/abs/2504.04789</guid>
<content:encoded><![CDATA[
<div> : , , (MA3), , 

:
MA3VQAVQAMA3 <div>
arXiv:2504.04789v1 Announce Type: new 
Abstract: As a strategic pillar industry for human survival and development, modern agriculture faces dual challenges: optimizing production efficiency and achieving sustainable development. Against the backdrop of intensified climate change leading to frequent extreme weather events, the uncertainty risks in agricultural production systems are increasing exponentially. To address these challenges, this study proposes an innovative \textbf{M}ultimodal \textbf{A}gricultural \textbf{A}gent \textbf{A}rchitecture (\textbf{MA3}), which leverages cross-modal information fusion and task collaboration mechanisms to achieve intelligent agricultural decision-making. This study constructs a multimodal agricultural agent dataset encompassing five major tasks: classification, detection, Visual Question Answering (VQA), tool selection, and agent evaluation. We propose a unified backbone for sugarcane disease classification and detection tools, as well as a sugarcane disease expert model. By integrating an innovative tool selection module, we develop a multimodal agricultural agent capable of effectively performing tasks in classification, detection, and VQA. Furthermore, we introduce a multi-dimensional quantitative evaluation framework and conduct a comprehensive assessment of the entire architecture over our evaluation dataset, thereby verifying the practicality and robustness of MA3 in agricultural scenarios. This study provides new insights and methodologies for the development of agricultural agents, holding significant theoretical and practical implications. Our source code and dataset will be made publicly available upon acceptance.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELT-Bench: An End-to-End Benchmark for Evaluating AI Agents on ELT Pipelines</title>
<link>https://arxiv.org/abs/2504.04808</link>
<guid>https://arxiv.org/abs/2504.04808</guid>
<content:encoded><![CDATA[
<div> : ELTAIELT-Bench

:
Extract-Load-TransformELTAISQLELTELTAIELT-BenchAIELTELT-Bench100835203AISpider-AgentSWE-AgentLLMsELT-BenchSpider-Agent Claude-3.7-Sonnet3.9%$4.3089.3ELT-BenchAIELThttps://github.com/uiuc-kang-lab/ETL.git <div>
arXiv:2504.04808v1 Announce Type: new 
Abstract: Practitioners are increasingly turning to Extract-Load-Transform (ELT) pipelines with the widespread adoption of cloud data warehouses. However, designing these pipelines often involves significant manual work to ensure correctness. Recent advances in AI-based methods, which have shown strong capabilities in data tasks, such as text-to-SQL, present an opportunity to alleviate manual efforts in developing ELT pipelines. Unfortunately, current benchmarks in data engineering only evaluate isolated tasks, such as using data tools and writing data transformation queries, leaving a significant gap in evaluating AI agents for generating end-to-end ELT pipelines.
  To fill this gap, we introduce ELT-Bench, an end-to-end benchmark designed to assess the capabilities of AI agents to build ELT pipelines. ELT-Bench consists of 100 pipelines, including 835 source tables and 203 data models across various domains. By simulating realistic scenarios involving the integration of diverse data sources and the use of popular data tools, ELT-Bench evaluates AI agents' abilities in handling complex data engineering workflows. AI agents must interact with databases and data tools, write code and SQL queries, and orchestrate every pipeline stage. We evaluate two representative code agent frameworks, Spider-Agent and SWE-Agent, using six popular Large Language Models (LLMs) on ELT-Bench. The highest-performing agent, Spider-Agent Claude-3.7-Sonnet with extended thinking, correctly generates only 3.9% of data models, with an average cost of $4.30 and 89.3 steps per pipeline. Our experimental results demonstrate the challenges of ELT-Bench and highlight the need for a more advanced AI agent to reduce manual effort in ELT workflows. Our code and data are available at https://github.com/uiuc-kang-lab/ETL.git.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Approach for Cooperative Multi-Agent Learning Problems</title>
<link>https://arxiv.org/abs/2504.04850</link>
<guid>https://arxiv.org/abs/2504.04850</guid>
<content:encoded><![CDATA[
<div> 

:
Multi-Agent Learning<br /><br /> <div>
arXiv:2504.04850v1 Announce Type: new 
Abstract: In this article, we propose a centralized Multi-Agent Learning framework for learning a policy that models the simultaneous behavior of multiple agents that need to coordinate to solve a certain task. Centralized approaches often suffer from the explosion of an action space that is defined by all possible combinations of individual actions, known as joint actions. Our approach addresses the coordination problem via a sequential abstraction, which overcomes the scalability problems typical to centralized methods. It introduces a meta-agent, called \textit{supervisor}, which abstracts joint actions as sequential assignments of actions to each agent. This sequential abstraction not only simplifies the centralized joint action space but also enhances the framework's scalability and efficiency. Our experimental results demonstrate that the proposed approach successfully coordinates agents across a variety of Multi-Agent Learning environments of diverse sizes.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BIASINSPECTOR: Detecting Bias in Structured Data through LLM Agents</title>
<link>https://arxiv.org/abs/2504.04855</link>
<guid>https://arxiv.org/abs/2504.04855</guid>
<content:encoded><![CDATA[
<div> BIASINSPECTOR

:
BIASINSPECTORBIASINSPECTORBIASINSPECTOR<br /><br /> <div>
arXiv:2504.04855v1 Announce Type: new 
Abstract: Detecting biases in structured data is a complex and time-consuming task. Existing automated techniques are limited in diversity of data types and heavily reliant on human case-by-case handling, resulting in a lack of generalizability. Currently, large language model (LLM)-based agents have made significant progress in data science, but their ability to detect data biases is still insufficiently explored. To address this gap, we introduce the first end-to-end, multi-agent synergy framework, BIASINSPECTOR, designed for automatic bias detection in structured data based on specific user requirements. It first develops a multi-stage plan to analyze user-specified bias detection tasks and then implements it with a diverse and well-suited set of tools. It delivers detailed results that include explanations and visualizations. To address the lack of a standardized framework for evaluating the capability of LLM agents to detect biases in data, we further propose a comprehensive benchmark that includes multiple evaluation metrics and a large set of test cases. Extensive experiments demonstrate that our framework achieves exceptional overall performance in structured data bias detection, setting a new milestone for fairer data applications.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAMDTP: Dynamic Trajectory Prediction with Graph Attention Mamba Network</title>
<link>https://arxiv.org/abs/2504.04862</link>
<guid>https://arxiv.org/abs/2504.04862</guid>
<content:encoded><![CDATA[
<div> : GAMDTP

:
GAMDTPGAMDTPmamba-ssmArgoverseGAMDTP <div>
arXiv:2504.04862v1 Announce Type: new 
Abstract: Accurate motion prediction of traffic agents is crucial for the safety and stability of autonomous driving systems. In this paper, we introduce GAMDTP, a novel graph attention-based network tailored for dynamic trajectory prediction. Specifically, we fuse the result of self attention and mamba-ssm through a gate mechanism, leveraging the strengths of both to extract features more efficiently and accurately, in each graph convolution layer. GAMDTP encodes the high-definition map(HD map) data and the agents' historical trajectory coordinates and decodes the network's output to generate the final prediction results. Additionally, recent approaches predominantly focus on dynamically fusing historical forecast results and rely on two-stage frameworks including proposal and refinement. To further enhance the performance of the two-stage frameworks we also design a scoring mechanism to evaluate the prediction quality during the proposal and refinement processes. Experiments on the Argoverse dataset demonstrates that GAMDTP achieves state-of-the-art performance, achieving superior accuracy in dynamic trajectory prediction.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Persuasive Dialogues on Meat Reduction with Generative Agents</title>
<link>https://arxiv.org/abs/2504.04872</link>
<guid>https://arxiv.org/abs/2504.04872</guid>
<content:encoded><![CDATA[
<div> 

<br />
:
LLMs <div>
arXiv:2504.04872v1 Announce Type: new 
Abstract: Meat reduction benefits human and planetary health, but social norms keep meat central in shared meals. To date, the development of communication strategies that promote meat reduction while minimizing social costs has required the costly involvement of human participants at each stage of the process. We present work in progress on simulating multi-round dialogues on meat reduction between Generative Agents based on large language models (LLMs). We measure our main outcome using established psychological questionnaires based on the Theory of Planned Behavior and additionally investigate Social Costs. We find evidence that our preliminary simulations produce outcomes that are (i) consistent with theoretical expectations; and (ii) valid when compared to data from previous studies with human participants. Generative agent-based models are a promising tool for identifying novel communication strategies on meat reduction-tailored to highly specific participant groups-to then be tested in subsequent studies with human participants.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Control Barrier Functions for Nonholonomic Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2504.04937</link>
<guid>https://arxiv.org/abs/2504.04937</guid>
<content:encoded><![CDATA[
<div> :  barrier (CBF)holonomic  CBFs (SCBF)

<br /><br />:

 Barrier CBFholonomicCBFCBFs (SCBFs)CBF <div>
arXiv:2504.04937v1 Announce Type: new 
Abstract: This paper addresses the problem of guaranteeing safety of multiple coordinated agents moving in dynamic environments. It has recently been shown that this problem can be efficiently solved through the notion of Control Barrier Functions (CBFs). However, for nonholonomic vehicles that are required to keep positive speeds, existing CBFs lose their validity. To overcome this limitation, we propose a hybrid formulation based on synergistic CBFs (SCBFs), which leverages a discrete switching mechanism to avoid configurations that would render the CBF invalid. Unlike existing approaches, our method ensures safety in the presence of moving obstacles and inter-agent interactions while respecting nonzero speed restrictions. We formally analyze the feasibility of the constraints with respect to actuation limits, and the efficacy of the solution is demonstrated in simulation of a multi-agent coordination problem in the presence of moving obstacles.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Dream Within Huang Long Cave: AI-Driven Interactive Narrative for Family Storytelling and Emotional Reflection</title>
<link>https://arxiv.org/abs/2504.04968</link>
<guid>https://arxiv.org/abs/2504.04968</guid>
<content:encoded><![CDATA[
<div> : AI

<br />
:
AIAILLMYELLCAVEYELLYELL <div>
arXiv:2504.04968v1 Announce Type: new 
Abstract: This paper introduces the art project The Dream Within Huang Long Cave, an AI-driven interactive and immersive narrative experience. The project offers new insights into AI technology, artistic practice, and psychoanalysis. Inspired by actual geographical landscapes and familial archetypes, the work combines psychoanalytic theory and computational technology, providing an artistic response to the concept of the non-existence of the Big Other. The narrative is driven by a combination of a large language model (LLM) and a realistic digital character, forming a virtual agent named YELL. Through dialogue and exploration within a cave automatic virtual environment (CAVE), the audience is invited to unravel the language puzzles presented by YELL and help him overcome his life challenges. YELL is a fictional embodiment of the Big Other, modeled after the artist's real father. Through a cross-temporal interaction with this digital father, the project seeks to deconstruct complex familial relationships. By demonstrating the non-existence of the Big Other, we aim to underscore the authenticity of interpersonal emotions, positioning art as a bridge for emotional connection and understanding within family dynamics.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Pedestrian and Vehicle Traffic Optimization in Urban Environments using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.05018</link>
<guid>https://arxiv.org/abs/2504.05018</guid>
<content:encoded><![CDATA[
<div> : (RL), , , , 

:
Wi-Fi67%52%67%53%<br /><br /> <div>
arXiv:2504.05018v1 Announce Type: new 
Abstract: Reinforcement learning (RL) holds significant promise for adaptive traffic signal control. While existing RL-based methods demonstrate effectiveness in reducing vehicular congestion, their predominant focus on vehicle-centric optimization leaves pedestrian mobility needs and safety challenges unaddressed. In this paper, we present a deep RL framework for adaptive control of eight traffic signals along a real-world urban corridor, jointly optimizing both pedestrian and vehicular efficiency. Our single-agent policy is trained using real-world pedestrian and vehicle demand data derived from Wi-Fi logs and video analysis. The results demonstrate significant performance improvements over traditional fixed-time signals, reducing average wait times per pedestrian and per vehicle by up to 67% and 52%, respectively, while simultaneously decreasing total accumulated wait times for both groups by up to 67% and 53%. Additionally, our results demonstrate generalization capabilities across varying traffic demands, including conditions entirely unseen during training, validating RL's potential for developing transportation systems that serve all road users.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture-of-Personas Language Models for Population Simulation</title>
<link>https://arxiv.org/abs/2504.05019</link>
<guid>https://arxiv.org/abs/2504.05019</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />: (MoP)(LLMs)LLMsMoPLLMMoPMoP <div>
arXiv:2504.05019v1 Announce Type: new 
Abstract: Advances in Large Language Models (LLMs) paved the way for their emerging applications in various domains, such as human behavior simulations, where LLMs could augment human-generated data in social science research and machine learning model training. However, pretrained LLMs often fail to capture the behavioral diversity of target populations due to the inherent variability across individuals and groups. To address this, we propose \textit{Mixture of Personas} (MoP), a \textit{probabilistic} prompting method that aligns the LLM responses with the target population. MoP is a contextual mixture model, where each component is an LM agent characterized by a persona and an exemplar representing subpopulation behaviors. The persona and exemplar are randomly chosen according to the learned mixing weights to elicit diverse LLM responses during simulation. MoP is flexible, requires no model finetuning, and is transferable across base models. Experiments for synthetic data generation show that MoP outperforms competing methods in alignment and diversity metrics.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention-Augmented Inverse Reinforcement Learning with Graph Convolutions for Multi-Agent Task Allocation</title>
<link>https://arxiv.org/abs/2504.05045</link>
<guid>https://arxiv.org/abs/2504.05045</guid>
<content:encoded><![CDATA[
<div> : 

:
IRLMATAMHSAMARL<br /><br /> <div>
arXiv:2504.05045v1 Announce Type: new 
Abstract: Multi-agent task allocation (MATA) plays a vital role in cooperative multi-agent systems, with significant implications for applications such as logistics, search and rescue, and robotic coordination. Although traditional deep reinforcement learning (DRL) methods have been shown to be promising, their effectiveness is hindered by a reliance on manually designed reward functions and inefficiencies in dynamic environments. In this paper, an inverse reinforcement learning (IRL)-based framework is proposed, in which multi-head self-attention (MHSA) and graph attention mechanisms are incorporated to enhance reward function learning and task execution efficiency. Expert demonstrations are utilized to infer optimal reward densities, allowing dependence on handcrafted designs to be reduced and adaptability to be improved. Extensive experiments validate the superiority of the proposed method over widely used multi-agent reinforcement learning (MARL) algorithms in terms of both cumulative rewards and task execution efficiency.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debate Only When Necessary: Adaptive Multiagent Collaboration for Efficient LLM Reasoning</title>
<link>https://arxiv.org/abs/2504.05047</link>
<guid>https://arxiv.org/abs/2504.05047</guid>
<content:encoded><![CDATA[
<div> DOWN

<br /><br />
(DOWN)DOWNDOWN <div>
arXiv:2504.05047v1 Announce Type: new 
Abstract: Multiagent collaboration has emerged as a promising framework for enhancing the reasoning capabilities of large language models (LLMs). While this approach improves reasoning capability, it incurs substantial computational overhead due to iterative agent interactions. Furthermore, engaging in debates for queries that do not necessitate collaboration amplifies the risk of error generation. To address these challenges, we propose Debate Only When Necessary (DOWN), an adaptive multiagent debate framework that selectively activates the debate process based on the confidence score of the agent's initial response. For queries where debate is triggered, agents refine their outputs using responses from participating agents and their confidence scores. Experimental results demonstrate that this mechanism significantly improves efficiency while maintaining or even surpassing the performance of existing multiagent debate systems. We also find that confidence-guided debate mitigates error propagation and enhances the selective incorporation of reliable responses. These results establish DOWN as an optimization strategy for efficient and effective multiagent reasoning, facilitating the practical deployment of LLM-based collaboration.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Driven Tactical Communications and Networking for Defense: A Survey and Emerging Trends</title>
<link>https://arxiv.org/abs/2504.05071</link>
<guid>https://arxiv.org/abs/2504.05071</guid>
<content:encoded><![CDATA[
<div> : (AI)

<br /><br />:
AIAIAIAIAIAIAI <div>
arXiv:2504.05071v1 Announce Type: new 
Abstract: The integration of Artificial Intelligence (AI) in military communications and networking is reshaping modern defense strategies, enhancing secure data exchange, real-time situational awareness, and autonomous decision-making. This survey explores how AI-driven technologies improve tactical communication networks, radar-based data transmission, UAV-assisted relay systems, and electronic warfare resilience. The study highlights AI applications in adaptive signal processing, multi-agent coordination for network optimization, radar-assisted target tracking, and AI-driven electronic countermeasures. Our work introduces a novel three-criteria evaluation methodology. It systematically assesses AI applications based on general system objectives, communications constraints in the military domain, and critical tactical environmental factors. We analyze key AI techniques for different types of learning applied to multi-domain network interoperability and distributed data information fusion in military operations. We also address challenges such as adversarial AI threats, the real-time adaptability of autonomous communication networks, and the limitations of current AI models under battlefield conditions. Finally, we discuss emerging trends in self-healing networks, AI-augmented decision support systems, and intelligent spectrum allocation. We provide a structured roadmap for future AI-driven defense communications and networking research.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for Early Warning System Investments</title>
<link>https://arxiv.org/abs/2504.05104</link>
<guid>https://arxiv.org/abs/2504.05104</guid>
<content:encoded><![CDATA[
<div> : (EWS)

<br /><br />:
AI(CREWS)25AI(CoT)(RAG)RAG87%89%83%AI <div>
arXiv:2504.05104v1 Announce Type: new 
Abstract: Tracking financial investments in climate adaptation is a complex and expertise-intensive task, particularly for Early Warning Systems (EWS), which lack standardized financial reporting across multilateral development banks (MDBs) and funds. To address this challenge, we introduce an LLM-based agentic AI system that integrates contextual retrieval, fine-tuning, and multi-step reasoning to extract relevant financial data, classify investments, and ensure compliance with funding guidelines. Our study focuses on a real-world application: tracking EWS investments in the Climate Risk and Early Warning Systems (CREWS) Fund. We analyze 25 MDB project documents and evaluate multiple AI-driven classification methods, including zero-shot and few-shot learning, fine-tuned transformer-based classifiers, chain-of-thought (CoT) prompting, and an agent-based retrieval-augmented generation (RAG) approach. Our results show that the agent-based RAG approach significantly outperforms other methods, achieving 87\% accuracy, 89\% precision, and 83\% recall. Additionally, we contribute a benchmark dataset and expert-annotated corpus, providing a valuable resource for future research in AI-driven financial tracking and climate finance transparency.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reducing the Communication of Distributed Model Predictive Control: Autoencoders and Formation Control</title>
<link>https://arxiv.org/abs/2504.05223</link>
<guid>https://arxiv.org/abs/2504.05223</guid>
<content:encoded><![CDATA[
<div> : (DMPC)

:
(DMPC)DMPCholonomic <div>
arXiv:2504.05223v1 Announce Type: new 
Abstract: Communication remains a key factor limiting the applicability of distributed model predictive control (DMPC) in realistic settings, despite advances in wireless communication. DMPC schemes can require an overwhelming amount of information exchange between agents as the amount of data depends on the length of the predication horizon, for which some applications require a significant length to formally guarantee nominal asymptotic stability. This work aims to provide an approach to reduce the communication effort of DMPC by reducing the size of the communicated data between agents. Using an autoencoder, the communicated data is reduced by the encoder part of the autoencoder prior to communication and reconstructed by the decoder part upon reception within the distributed optimization algorithm that constitutes the DMPC scheme. The choice of a learning-based reduction method is motivated by structure inherent to the data, which results from the data's connection to solutions of optimal control problems. The approach is implemented and tested at the example of formation control of differential-drive robots, which is challenging for optimization-based control due to the robots' nonholonomic constraints, and which is interesting due to the practical importance of mobile robotics. The applicability of the proposed approach is presented first in form of a simulative analysis showing that the resulting control performance yields a satisfactory accuracy. In particular, the proposed approach outperforms the canonical naive way to reduce communication by reducing the length of the prediction horizon. Moreover, it is shown that numerical experiments conducted on embedded computation hardware, with real distributed computation and wireless communication, work well with the proposed way of reducing communication even in practical scenarios in which full communication fails.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to evaluate control measures for LLM agents? A trajectory from today to superintelligence</title>
<link>https://arxiv.org/abs/2504.05259</link>
<guid>https://arxiv.org/abs/2504.05259</guid>
<content:encoded><![CDATA[
<div> : AI

:
AIAIM1-M5AIAIACLsACLLLMAI<br /><br /> <div>
arXiv:2504.05259v1 Announce Type: new 
Abstract: As LLM agents grow more capable of causing harm autonomously, AI developers will rely on increasingly sophisticated control measures to prevent possibly misaligned agents from causing harm. AI developers could demonstrate that their control measures are sufficient by running control evaluations: testing exercises in which a red team produces agents that try to subvert control measures. To ensure control evaluations accurately capture misalignment risks, the affordances granted to this red team should be adapted to the capability profiles of the agents to be deployed under control measures.
  In this paper we propose a systematic framework for adapting affordances of red teams to advancing AI capabilities. Rather than assuming that agents will always execute the best attack strategies known to humans, we demonstrate how knowledge of an agents's actual capability profile can inform proportional control evaluations, resulting in more practical and cost-effective control measures. We illustrate our framework by considering a sequence of five fictional models (M1-M5) with progressively advanced capabilities, defining five distinct AI control levels (ACLs). For each ACL, we provide example rules for control evaluation, control measures, and safety cases that could be appropriate. Finally, we show why constructing a compelling AI control safety case for superintelligent LLM agents will require research breakthroughs, highlighting that we might eventually need alternative approaches to mitigating misalignment risk.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CREA: A Collaborative Multi-Agent Framework for Creative Content Generation with Diffusion Models</title>
<link>https://arxiv.org/abs/2504.05306</link>
<guid>https://arxiv.org/abs/2504.05306</guid>
<content:encoded><![CDATA[
<div> : AICREA

<br /><br />:
CREAAIAICREAAIAI <div>
arXiv:2504.05306v1 Announce Type: new 
Abstract: Creativity in AI imagery remains a fundamental challenge, requiring not only the generation of visually compelling content but also the capacity to add novel, expressive, and artistically rich transformations to images. Unlike conventional editing tasks that rely on direct prompt-based modifications, creative image editing demands an autonomous, iterative approach that balances originality, coherence, and artistic intent. To address this, we introduce CREA, a novel multi-agent collaborative framework that mimics the human creative process. Our framework leverages a team of specialized AI agents who dynamically collaborate to conceptualize, generate, critique, and enhance images. Through extensive qualitative and quantitative evaluations, we demonstrate that CREA significantly outperforms state-of-the-art methods in diversity, semantic alignment, and creative transformation. By structuring creativity as a dynamic, agentic process, CREA redefines the intersection of AI and art, paving the way for autonomous AI-driven artistic exploration, generative design, and human-AI co-creation. To the best of our knowledge, this is the first work to introduce the task of creative editing.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OLAF: An Open Life Science Analysis Framework for Conversational Bioinformatics Powered by Large Language Models</title>
<link>https://arxiv.org/abs/2504.03976</link>
<guid>https://arxiv.org/abs/2504.03976</guid>
<content:encoded><![CDATA[
<div> : OLAFRNA

<br /><br />:
OLAFagent-pipe-router.h5adAngularPython/FirebaseRNAAIOLAFAI <div>
arXiv:2504.03976v1 Announce Type: cross 
Abstract: OLAF (Open Life Science Analysis Framework) is an open-source platform that enables researchers to perform bioinformatics analyses using natural language. By combining large language models (LLMs) with a modular agent-pipe-router architecture, OLAF generates and executes bioinformatics code on real scientific data, including formats like .h5ad. The system includes an Angular front end and a Python/Firebase backend, allowing users to run analyses such as single-cell RNA-seq workflows, gene annotation, and data visualization through a simple web interface. Unlike general-purpose AI tools, OLAF integrates code execution, data handling, and scientific libraries in a reproducible, user-friendly environment. It is designed to lower the barrier to computational biology for non-programmers and support transparent, AI-powered life science research.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Market Equilibrium Models with Stable Adversarial Learning via Reinforcement</title>
<link>https://arxiv.org/abs/2504.04300</link>
<guid>https://arxiv.org/abs/2504.04300</guid>
<content:encoded><![CDATA[
<div> : 

<br />
:
GANs <div>
arXiv:2504.04300v1 Announce Type: cross 
Abstract: We present a general computational framework for solving continuous-time financial market equilibria under minimal modeling assumptions while incorporating realistic financial frictions, such as trading costs, and supporting multiple interacting agents. Inspired by generative adversarial networks (GANs), our approach employs a novel generative deep reinforcement learning framework with a decoupling feedback system embedded in the adversarial training loop, which we term as the \emph{reinforcement link}. This architecture stabilizes the training dynamics by incorporating feedback from the discriminator. Our theoretically guided feedback mechanism enables the decoupling of the equilibrium system, overcoming challenges that hinder conventional numerical algorithms. Experimentally, our algorithm not only learns but also provides testable predictions on how asset returns and volatilities emerge from the endogenous trading behavior of market participants, where traditional analytical methods fall short. The design of our model is further supported by an approximation guarantee.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Mechanisms under Unawareness</title>
<link>https://arxiv.org/abs/2504.04382</link>
<guid>https://arxiv.org/abs/2504.04382</guid>
<content:encoded><![CDATA[
<div> : --<br /><br />:

-- <div>
arXiv:2504.04382v1 Announce Type: cross 
Abstract: We study the design of efficient mechanisms under asymmetric awareness and information. Unawareness refers to the lack of conception rather than the lack of information. Assuming quasi-linear utilities and private values, we show that we can implement in conditional dominant strategies a social choice function that is utilitarian ex-post efficient when pooling all awareness of all agents without the need of the social planner being fully aware ex-ante. To this end, we develop novel dynamic versions of Vickrey-Clarke-Groves mechanisms in which types are revealed and subsequently elaborated at endogenous higher awareness levels. We explore how asymmetric awareness affects budget balance and participation constraints. We show that ex-ante unforeseen contingencies are no excuse for deficits. Finally, we propose a modified reverse second price auction for efficient procurement of complex incompletely specified projects.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrainMRDiff: A Diffusion Model for Anatomically Consistent Brain MRI Synthesis</title>
<link>https://arxiv.org/abs/2504.04532</link>
<guid>https://arxiv.org/abs/2504.04532</guid>
<content:encoded><![CDATA[
<div> MRIAIBrainMRDiff

:
BrainMRDiffMRI(TSA)(TGAP)TSATGAPBrainMRDiffBraTS-AGBraTS-Met23.33%33.33%<br /><br /> <div>
arXiv:2504.04532v1 Announce Type: cross 
Abstract: Accurate brain tumor diagnosis relies on the assessment of multiple Magnetic Resonance Imaging (MRI) sequences. However, in clinical practice, the acquisition of certain sequences may be affected by factors like motion artifacts or contrast agent contraindications, leading to suboptimal outcome, such as poor image quality. This can then affect image interpretation by radiologists. Synthesizing high quality MRI sequences has thus become a critical research focus. Though recent advancements in controllable generative AI have facilitated the synthesis of diagnostic quality MRI, ensuring anatomical accuracy remains a significant challenge. Preserving critical structural relationships between different anatomical regions is essential, as even minor structural or topological inconsistencies can compromise diagnostic validity. In this work, we propose BrainMRDiff, a novel topology-preserving, anatomy-guided diffusion model for synthesizing brain MRI, leveraging brain and tumor anatomies as conditioning inputs. To achieve this, we introduce two key modules: Tumor+Structure Aggregation (TSA) and Topology-Guided Anatomy Preservation (TGAP). TSA integrates diverse anatomical structures with tumor information, forming a comprehensive conditioning mechanism for the diffusion process. TGAP enforces topological consistency during reverse denoising diffusion process; both these modules ensure that the generated image respects anatomical integrity. Experimental results demonstrate that BrainMRDiff surpasses existing baselines, achieving performance improvements of 23.33% on the BraTS-AG dataset and 33.33% on the BraTS-Met dataset. Code will be made publicly available soon.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Risk-Sensitive Policy Gradient: An Iteration Complexity Analysis</title>
<link>https://arxiv.org/abs/2403.08955</link>
<guid>https://arxiv.org/abs/2403.08955</guid>
<content:encoded><![CDATA[
<div> : , , , , 

:
REINFORCE$\epsilon$-$\mathcal{O}(\epsilon^{-2})$REINFORCECartPoleMiniGridRobot NavigationREINFORCEhttps://ruiiu.github.io/riskrl <div>
arXiv:2403.08955v3 Announce Type: replace 
Abstract: Reinforcement Learning (RL) has shown exceptional performance across various applications, enabling autonomous agents to learn optimal policies through interaction with their environments. However, traditional RL frameworks often face challenges in terms of iteration efficiency and robustness. Risk-sensitive policy gradient methods, which incorporate both expected return and risk measures, have been explored for their ability to yield more robust policies, yet their iteration complexity remains largely underexplored. In this work, we conduct a rigorous iteration complexity analysis for the risk-sensitive policy gradient method, focusing on the REINFORCE algorithm with an exponential utility function. We establish an iteration complexity of $\mathcal{O}(\epsilon^{-2})$ to reach an $\epsilon$-approximate first-order stationary point (FOSP). Furthermore, we investigate whether risk-sensitive algorithms can achieve better iteration complexity compared to their risk-neutral counterparts. Our analysis indicates that risk-sensitive REINFORCE can potentially converge faster. To validate our analysis, we empirically evaluate the learning performance and convergence efficiency of the risk-neutral and risk-sensitive REINFORCE algorithms in multiple environments: CartPole, MiniGrid, and Robot Navigation. Empirical results confirm that risk-averse cases can converge and stabilize faster compared to their risk-neutral counterparts. More details can be found on our website https://ruiiu.github.io/riskrl.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents</title>
<link>https://arxiv.org/abs/2405.14573</link>
<guid>https://arxiv.org/abs/2405.14573</guid>
<content:encoded><![CDATA[
<div> AndroidWorld

<br /><br />:
AndroidWorld20116AndroidWorldAndroidWorld30.6%AndroidWorldWebAndroidWorldgithub.com/google-research/android_world <div>
arXiv:2405.14573v5 Announce Type: replace 
Abstract: Autonomous agents that execute human tasks by controlling computers can enhance human productivity and application accessibility. However, progress in this field will be driven by realistic and reproducible benchmarks. We present AndroidWorld, a fully functional Android environment that provides reward signals for 116 programmatic tasks across 20 real-world Android apps. Unlike existing interactive environments, which provide a static test set, AndroidWorld dynamically constructs tasks that are parameterized and expressed in natural language in unlimited ways, thus enabling testing on a much larger and more realistic suite of tasks. To ensure reproducibility, each task includes dedicated initialization, success-checking, and tear-down logic, which modifies and inspects the device's system state. We experiment with baseline agents to test AndroidWorld and provide initial results on the benchmark. Our best agent can complete 30.6% of AndroidWorld's tasks, leaving ample room for future work. Furthermore, we adapt a popular desktop web agent to work on Android, which we find to be less effective on mobile, suggesting future research is needed to achieve universal, cross-platform agents. Finally, we also conduct a robustness analysis, showing that task variations can significantly affect agent performance, demonstrating that without such testing, agent performance metrics may not fully reflect practical challenges. AndroidWorld and the experiments in this paper are available at github.com/google-research/android_world.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Adaptive Control of Disturbed Interconnected Systems with High-Order Tuners</title>
<link>https://arxiv.org/abs/2405.15178</link>
<guid>https://arxiv.org/abs/2405.15178</guid>
<content:encoded><![CDATA[
<div> : , , , , 

:
-$L_2$$L_\infty$ <div>
arXiv:2405.15178v3 Announce Type: replace 
Abstract: This paper addresses the challenge of network synchronization under limited communication, involving heterogeneous agents with different dynamics and various network topologies, to achieve consensus. We investigate the distributed adaptive control for interconnected unknown linear subsystems with a leader and followers, in the presence of input-output disturbance. We enhance the communication within multi-agent systems to achieve consensus under the leadership's guidance. While the measured variable is similar among the followers, the incoming measurements are weighted and constructed based on their proximity to the leader. We also explore the convergence rates across various balanced topologies (Star-like, Cyclic-like, Path, Random), featuring different numbers of agents, using three distributed algorithms, ranging from first- to high-order tuners to effectively address time-varying regressors. The mathematical foundation is rigorously presented from the network designs of the unknown agents following a leader, to the distributed methods. Moreover, we conduct several numerical simulations across various networks, agents and tuners to evaluate the effects of sparsity in the interaction between subsystems using the $L_2-$norm and $L_\infty-$norm. Some networks exhibit a trend where an increasing number of agents results in smaller errors, although this is not universally the case. Additionally, patterns observed at initial times may not reliably predict overall performance across different networks. Finally, we demonstrate that the proposed modified high-order tuner outperforms its counterparts, and we provide related insights along with our conclusions.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REvolve: Reward Evolution with Large Language Models using Human Feedback</title>
<link>https://arxiv.org/abs/2406.01309</link>
<guid>https://arxiv.org/abs/2406.01309</guid>
<content:encoded><![CDATA[
<div>  (Reinforcement Learning) (reward function) (large language modelsLLMs) (human feedback)REvolve

<br /><br />:
LLMsREvolveLLMREvolve <div>
arXiv:2406.01309v3 Announce Type: replace 
Abstract: Designing effective reward functions is crucial to training reinforcement learning (RL) algorithms. However, this design is non-trivial, even for domain experts, due to the subjective nature of certain tasks that are hard to quantify explicitly. In recent works, large language models (LLMs) have been used for reward generation from natural language task descriptions, leveraging their extensive instruction tuning and commonsense understanding of human behavior. In this work, we hypothesize that LLMs, guided by human feedback, can be used to formulate reward functions that reflect human implicit knowledge. We study this in three challenging settings -- autonomous driving, humanoid locomotion, and dexterous manipulation -- wherein notions of ``good" behavior are tacit and hard to quantify. To this end, we introduce REvolve, a truly evolutionary framework that uses LLMs for reward design in RL. REvolve generates and refines reward functions by utilizing human feedback to guide the evolution process, effectively translating implicit human knowledge into explicit reward functions for training (deep) RL agents. Experimentally, we demonstrate that agents trained on REvolve-designed rewards outperform other state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Coopetition Index for Coalitions in Simple Games</title>
<link>https://arxiv.org/abs/2407.01383</link>
<guid>https://arxiv.org/abs/2407.01383</guid>
<content:encoded><![CDATA[
<div> : 

<br /><br />:
-11BanzhafShapley-OwenShapley-OwenBanzhaf <div>
arXiv:2407.01383v2 Announce Type: replace 
Abstract: In simple games, larger coalitions typically wield more power, but do all players align their efforts effectively? Consider a voting scenario where a coalition forms, but needs more voters to pass a bill. The cohesion of the new group of voters hinges on whether all the new members can proficiently collaborate with the existing players to ensure the bill's passage or if subgroups form that pursue an independent alternative, thus generating antagonism among the new voters.
  This research introduces two classes of coopetition indices -- one relative and one absolute, the latter ranging from -1 to 1, to measure agents' preferences for cooperation (when positive) or competition (when negative) with the remaining players. These indices, together with a generalized group value, provide a comprehensive picture of the relevance and the cohesion of groups. We discuss the relationship with similar group indices and provide proper coopetition Banzhaf and Shapley-Owen types of indices.
  By applying our indices to the apex game and symmetric majority games, we observe that cooperation and competition frequently balance each other out, leading to null values for the Shapley-Owen and Banzhaf coopetition indices. An electoral application with real world data is also considered.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Biased Dueling Bandits with Stochastic Delayed Feedback</title>
<link>https://arxiv.org/abs/2408.14603</link>
<guid>https://arxiv.org/abs/2408.14603</guid>
<content:encoded><![CDATA[
<div> dueling banditregret

:<br />
 dueling bandit dueling bandit <div>
arXiv:2408.14603v2 Announce Type: replace 
Abstract: The dueling bandit problem, an essential variation of the traditional multi-armed bandit problem, has become significantly prominent recently due to its broad applications in online advertising, recommendation systems, information retrieval, and more. However, in many real-world applications, the feedback for actions is often subject to unavoidable delays and is not immediately available to the agent. This partially observable issue poses a significant challenge to existing dueling bandit literature, as it significantly affects how quickly and accurately the agent can update their policy on the fly. In this paper, we introduce and examine the biased dueling bandit problem with stochastic delayed feedback, revealing that this new practical problem will delve into a more realistic and intriguing scenario involving a preference bias between the selections. We present two algorithms designed to handle situations involving delay. Our first algorithm, requiring complete delay distribution information, achieves the optimal regret bound for the dueling bandit problem when there is no delay. The second algorithm is tailored for situations where the distribution is unknown, but only the expected value of delay is available. We provide a comprehensive regret analysis for the two proposed algorithms and then evaluate their empirical performance on both synthetic and real datasets.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing is Believing? Enhancing Vision-Language Navigation using Visual Perturbations</title>
<link>https://arxiv.org/abs/2409.05552</link>
<guid>https://arxiv.org/abs/2409.05552</guid>
<content:encoded><![CDATA[
<div> <br /><br />:
VLNMBAVLNVLNR2RREVERIESOON <div>
arXiv:2409.05552v2 Announce Type: replace 
Abstract: Autonomous navigation guided by natural language instructions in embodied environments remains a challenge for vision-language navigation (VLN) agents. Although recent advancements in learning diverse and fine-grained visual environmental representations have shown promise, the fragile performance improvements may not conclusively attribute to enhanced visual grounding,a limitation also observed in related vision-language tasks. In this work, we preliminarily investigate whether advanced VLN models genuinely comprehend the visual content of their environments by introducing varying levels of visual perturbations. These perturbations include ground-truth depth images, perturbed views and random noise. Surprisingly, we experimentally find that simple branch expansion, even with noisy visual inputs, paradoxically improves the navigational efficacy. Inspired by these insights, we further present a versatile Multi-Branch Architecture (MBA) designed to delve into the impact of both the branch quantity and visual quality. The proposed MBA extends a base agent into a multi-branch variant, where each branch processes a different visual input. This approach is embarrassingly simple yet agnostic to topology-based VLN agents. Extensive experiments on three VLN benchmarks (R2R, REVERIE, SOON) demonstrate that our method with optimal visual permutations matches or even surpasses state-of-the-art results. The source code is available at here.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Wheel Loader Navigation Using Goal-Conditioned Actor-Critic MPC</title>
<link>https://arxiv.org/abs/2409.15717</link>
<guid>https://arxiv.org/abs/2409.15717</guid>
<content:encoded><![CDATA[
<div> (MPC)(Actor-Critic RL)

<br /><br />:
MPCMPC-RLRLMPCMPCRL <div>
arXiv:2409.15717v3 Announce Type: replace 
Abstract: This paper proposes a novel control method for an autonomous wheel loader, enabling time-efficient navigation to an arbitrary goal pose. Unlike prior works which combine high-level trajectory planners with Model Predictive Control (MPC), we directly enhance the planning capabilities of MPC by incorporating a cost function derived from Actor-Critic Reinforcement Learning (RL). Specifically, we first train an RL agent to solve the pose reaching task in simulation, then transfer the learned planning knowledge to an MPC by incorporating the trained neural network critic as both the stage and terminal cost. We show through comprehensive simulations that the resulting MPC inherits the time-efficient behavior of the RL agent, generating trajectories that compare favorably against those found using trajectory optimization. We also deploy our method on a real-world wheel loader, where we demonstrate successful navigation in various scenarios.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JudgeBench: A Benchmark for Evaluating LLM-based Judges</title>
<link>https://arxiv.org/abs/2410.12784</link>
<guid>https://arxiv.org/abs/2410.12784</guid>
<content:encoded><![CDATA[
<div> : LLM-based judges, , JudgeBench, , GPT-4

:
LLMLLMJudgeBenchJudgeBenchJudgeBenchGPT-4oJudgeBenchLLMhttps://github.com/ScalerLab/JudgeBench <div>
arXiv:2410.12784v2 Announce Type: replace 
Abstract: LLM-based judges have emerged as a scalable alternative to human evaluation and are increasingly used to assess, compare, and improve models. However, the reliability of LLM-based judges themselves is rarely scrutinized. As LLMs become more advanced, their responses grow more sophisticated, requiring stronger judges to evaluate them. Existing benchmarks primarily focus on a judge's alignment with human preferences, but often fail to account for more challenging tasks where crowdsourced human preference is a poor indicator of factual and logical correctness. To address this, we propose a novel evaluation framework to objectively evaluate LLM-based judges. Based on this framework, we propose JudgeBench, a benchmark for evaluating LLM-based judges on challenging response pairs spanning knowledge, reasoning, math, and coding. JudgeBench leverages a novel pipeline for converting existing difficult datasets into challenging response pairs with preference labels reflecting objective correctness. Our comprehensive evaluation on a collection of prompted judges, fine-tuned judges, multi-agent judges, and reward models shows that JudgeBench poses a significantly greater challenge than previous benchmarks, with many strong models (e.g., GPT-4o) performing just slightly better than random guessing. Overall, JudgeBench offers a reliable platform for assessing increasingly advanced LLM-based judges. Data and code are available at https://github.com/ScalerLab/JudgeBench.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextual Augmented Multi-Model Programming (CAMP): A Hybrid Local-Cloud Copilot Framework</title>
<link>https://arxiv.org/abs/2410.15285</link>
<guid>https://arxiv.org/abs/2410.15285</guid>
<content:encoded><![CDATA[
<div>  (LLMs)CAMPRetrieval-Augmented Generation (RAG)

<br /><br />:

CAMPAILLMsiOSmacOSCAMPRAGLLMsIDEsCopilot for XcodeXcodeAIRAGAI <div>
arXiv:2410.15285v2 Announce Type: replace 
Abstract: The advancements in cloud-based Large Languages Models (LLMs) have revolutionized AI-assisted programming. However, their integration into certain local development environments like ones within the Apple software ecosystem (e.g., iOS apps, macOS) remains challenging due to computational demands and sandboxed constraints. This paper presents CAMP, a multi-model AI-assisted programming framework that consists of a local model that employs Retrieval-Augmented Generation (RAG) to retrieve contextual information from the codebase to facilitate context-aware prompt construction thus optimizing the performance of the cloud model, empowering LLMs' capabilities in local Integrated Development Environments (IDEs). The methodology is actualized in Copilot for Xcode, an AI-assisted programming tool crafted for Xcode that employs the RAG module to address software constraints and enables diverse generative programming tasks, including automatic code completion, documentation, error detection, and intelligent user-agent interaction. The results from objective experiments on generated code quality and subjective experiments on user adoption collectively demonstrate the pilot success of the proposed system and mark its significant contributions to the realm of AI-assisted programming.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRS: Generating Robotic Simulation Tasks from Real-World Images</title>
<link>https://arxiv.org/abs/2410.15536</link>
<guid>https://arxiv.org/abs/2410.15536</guid>
<content:encoded><![CDATA[
<div> : GRSRGB-D

:
GRSRGB-DVLMsGRS1) SAM22) 3) GRS <div>
arXiv:2410.15536v2 Announce Type: replace 
Abstract: We introduce GRS (Generating Robotic Simulation tasks), a system addressing real-to-sim for robotic simulations. GRS creates digital twin simulations from single RGB-D observations with solvable tasks for virtual agent training. Using vision-language models (VLMs), our pipeline operates in three stages: 1) scene comprehension with SAM2 for segmentation and object description, 2) matching objects with simulation-ready assets, and 3) generating appropriate tasks. We ensure simulation-task alignment through generated test suites and introduce a router that iteratively refines both simulation and test code. Experiments demonstrate our system's effectiveness in object correspondence and task environment generation through our novel router mechanism.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation</title>
<link>https://arxiv.org/abs/2410.17462</link>
<guid>https://arxiv.org/abs/2410.17462</guid>
<content:encoded><![CDATA[
<div> TESSA

:
TESSATESSATESSA<br /><br /> <div>
arXiv:2410.17462v2 Announce Type: replace 
Abstract: Time series data is ubiquitous across various domains, including manufacturing, finance, and healthcare. High-quality annotations are essential for effectively understanding time series and facilitating downstream tasks; however, obtaining such annotations is challenging, particularly in mission-critical domains. In this paper, we propose TESSA, a multi-agent system designed to automatically generate both general and domain-specific annotations for time series data. TESSA introduces two agents: a general annotation agent and a domain-specific annotation agent. The general agent captures common patterns and knowledge across multiple source domains, leveraging both time-series-wise and text-wise features to generate general annotations. Meanwhile, the domain-specific agent utilizes limited annotations from the target domain to learn domain-specific terminology and generate targeted annotations. Extensive experiments on multiple synthetic and real-world datasets demonstrate that TESSA effectively generates high-quality annotations, outperforming existing methods.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trial-and-Error Learning in Decentralized Matching Markets</title>
<link>https://arxiv.org/abs/2411.02377</link>
<guid>https://arxiv.org/abs/2411.02377</guid>
<content:encoded><![CDATA[
<div> : -sided

:
-sided<br /><br /> <div>
arXiv:2411.02377v2 Announce Type: replace 
Abstract: Two-sided matching markets, environments in which two disjoint groups of agents seek to partner with one another, arise in several contexts. In static, centralized markets where agents know their preferences, standard algorithms can yield a stable matching. However, in dynamic, decentralized markets where agents must learn their preferences through interaction, such algorithms cannot be used. Our goal in this paper is to identify achievable stability guarantees in decentralized matching markets where (i) agents have limited information about their preferences and (ii) no central entity determines the match. Surprisingly, our first result demonstrates that these constraints do not preclude stability--simple "trial and error" learning policies guarantee convergence to a stable matching without requiring coordination between agents. Our second result shows that more sophisticated policies can direct the system toward a particular group's optimal stable matching. This finding highlights an important dimension of strategic learning: when agents can accurately model others' policies, they can adapt their own behavior to systematically influence outcomes in their favor--a phenomenon with broad implications for learning in multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DataLab: A Unified Platform for LLM-Powered Business Intelligence</title>
<link>https://arxiv.org/abs/2412.02205</link>
<guid>https://arxiv.org/abs/2412.02205</guid>
<content:encoded><![CDATA[
<div> : (BI)(LLM)DataLabBI

<br /><br />:
DataLab(BI)(LLM)DataLabLLMBIBIBIBIBIDataLabBIBI58.58%61.65% <div>
arXiv:2412.02205v3 Announce Type: replace 
Abstract: Business intelligence (BI) transforms large volumes of data within modern organizations into actionable insights for informed decision-making. Recently, large language model (LLM)-based agents have streamlined the BI workflow by automatically performing task planning, reasoning, and actions in executable environments based on natural language (NL) queries. However, existing approaches primarily focus on individual BI tasks such as NL2SQL and NL2VIS. The fragmentation of tasks across different data roles and tools lead to inefficiencies and potential errors due to the iterative and collaborative nature of BI. In this paper, we introduce DataLab, a unified BI platform that integrates a one-stop LLM-based agent framework with an augmented computational notebook interface. DataLab supports various BI tasks for different data roles in data preparation, analysis, and visualization by seamlessly combining LLM assistance with user customization within a single environment. To achieve this unification, we design a domain knowledge incorporation module tailored for enterprise-specific BI tasks, an inter-agent communication mechanism to facilitate information sharing across the BI workflow, and a cell-based context management strategy to enhance context utilization efficiency in BI notebooks. Extensive experiments demonstrate that DataLab achieves state-of-the-art performance on various BI tasks across popular research benchmarks. Moreover, DataLab maintains high effectiveness and efficiency on real-world datasets from Tencent, achieving up to a 58.58% increase in accuracy and a 61.65% reduction in token cost on enterprise-specific BI tasks.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Journey Beyond RGB: Unveiling Hybrid Semantic-Spatial Environmental Representations for Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2412.06465</link>
<guid>https://arxiv.org/abs/2412.06465</guid>
<content:encoded><![CDATA[
<div> : 

:
Semantic Understanding and Spatial Awareness (SUSA)Vision-and-Language NavigationVLNRGBSUSATextual Semantic Understanding (TSU) Depth-enhanced Spatial Perception (DSP) SUSA-VLNREVERIER2RSOON<br /><br /> <div>
arXiv:2412.06465v4 Announce Type: replace 
Abstract: Navigating unseen environments based on natural language instructions remains difficult for egocentric agents in Vision-and-Language Navigation (VLN). Existing approaches primarily rely on RGB images for environmental representation, underutilizing latent textual semantic and spatial cues and leaving the modality gap between instructions and scarce environmental representations unresolved. Intuitively, humans inherently ground semantic knowledge within spatial layouts during indoor navigation. Inspired by this, we propose a versatile Semantic Understanding and Spatial Awareness (SUSA) architecture to encourage agents to ground environment from diverse perspectives. SUSA includes a Textual Semantic Understanding (TSU) module, which narrows the modality gap between instructions and environments by generating and associating the descriptions of environmental landmarks in agent's immediate surroundings. Additionally, a Depth-enhanced Spatial Perception (DSP) module incrementally constructs a depth exploration map, enabling a more nuanced comprehension of environmental layouts. Experiments demonstrate that SUSA's hybrid semantic-spatial representations effectively enhance navigation performance, setting new state-of-the-art performance across three VLN benchmarks (REVERIE, R2R, and SOON). The source code will be publicly available.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward LLM-Agent-Based Modeling of Transportation Systems: A Conceptual Framework</title>
<link>https://arxiv.org/abs/2412.06681</link>
<guid>https://arxiv.org/abs/2412.06681</guid>
<content:encoded><![CDATA[
<div> : arXivLLMagent-based

<br /><br />:
LLMsLLMLLMLLM <div>
arXiv:2412.06681v2 Announce Type: replace 
Abstract: In transportation system demand modeling and simulation, agent-based models and microsimulations are current state-of-the-art approaches. However, existing agent-based models still have some limitations on behavioral realism and resource demand that limit their applicability. In this study, leveraging the emerging technology of large language models (LLMs) and LLM-based agents, we propose a general LLM-agent-based modeling framework for transportation systems. We argue that LLM agents not only possess the essential capabilities to function as agents but also offer promising solutions to overcome some limitations of existing agent-based models. Our conceptual framework design closely replicates the decision-making and interaction processes and traits of human travelers within transportation networks, and we demonstrate that the proposed systems can meet critical behavioral criteria for decision-making and learning behaviors using related studies and a demonstrative example of LLM agents' learning and adjustment in the bottleneck setting. Although further refinement of the LLM-agent-based modeling framework is necessary, we believe that this approach has the potential to improve transportation system modeling and simulation.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CowPilot: A Framework for Autonomous and Human-Agent Collaborative Web Navigation</title>
<link>https://arxiv.org/abs/2501.16609</link>
<guid>https://arxiv.org/abs/2501.16609</guid>
<content:encoded><![CDATA[
<div> CowPilotWeb-

:
CowPilot95%15.2%CowPilothttps://oaishi.github.io/cowpilot.html  <div>
arXiv:2501.16609v3 Announce Type: replace 
Abstract: While much work on web agents emphasizes the promise of autonomously performing tasks on behalf of users, in reality, agents often fall short on complex tasks in real-world contexts and modeling user preference. This presents an opportunity for humans to collaborate with the agent and leverage the agent's capabilities effectively. We propose CowPilot, a framework supporting autonomous as well as human-agent collaborative web navigation, and evaluation across task success and task efficiency. CowPilot reduces the number of steps humans need to perform by allowing agents to propose next steps, while users are able to pause, reject, or take alternative actions. During execution, users can interleave their actions with the agent by overriding suggestions or resuming agent control when needed. We conducted case studies on five common websites and found that the human-agent collaborative mode achieves the highest success rate of 95% while requiring humans to perform only 15.2% of the total steps. Even with human interventions during task execution, the agent successfully drives up to half of task success on its own. CowPilot can serve as a useful tool for data collection and agent evaluation across websites, which we believe will enable research in how users and agents can work together. Video demonstrations are available at https://oaishi.github.io/cowpilot.html
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UXAgent: An LLM Agent-Based Usability Testing Framework for Web Design</title>
<link>https://arxiv.org/abs/2502.12561</link>
<guid>https://arxiv.org/abs/2502.12561</guid>
<content:encoded><![CDATA[
<div> : usability testing, Large Language Model-simulated Agent (LLM-Agent), UXAgent, web design, heuristic user evaluation

:
UXAgentLarge Language Model-simulated Agent (LLM-Agent)(UX)UXAgentLLM-AgentUXLLM AgentUX<br /><br /> <div>
arXiv:2502.12561v3 Announce Type: replace 
Abstract: Usability testing is a fundamental yet challenging (e.g., inflexible to iterate the study design flaws and hard to recruit study participants) research method for user experience (UX) researchers to evaluate a web design. Recent advances in Large Language Model-simulated Agent (LLM-Agent) research inspired us to design UXAgent to support UX researchers in evaluating and reiterating their usability testing study design before they conduct the real human subject study. Our system features an LLM-Agent module and a universal browser connector module so that UX researchers can automatically generate thousands of simulated users to test the target website. The results are shown in qualitative (e.g., interviewing how an agent thinks ), quantitative (e.g., # of actions), and video recording formats for UX researchers to analyze. Through a heuristic user evaluation with five UX researchers, participants praised the innovation of our system but also expressed concerns about the future of LLM Agent-assisted UX study.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-agent Auto-Bidding with Latent Graph Diffusion Models</title>
<link>https://arxiv.org/abs/2503.05805</link>
<guid>https://arxiv.org/abs/2503.05805</guid>
<content:encoded><![CDATA[
<div> diffusion-based auto-bidding, graph representations, auction environments, latent diffusion model (LDM), key performance indicator (KPI)

<br /><br />:
(KPI)(LDM)LDMKPIKPI <div>
arXiv:2503.05805v2 Announce Type: replace 
Abstract: This paper proposes a diffusion-based auto-bidding framework that leverages graph representations to model large-scale auction environments. In such settings, agents must dynamically optimize bidding strategies under constraints defined by key performance indicator (KPI) metrics, all while operating in competitive environments characterized by uncertain, sparse, and stochastic variables. To address these challenges, we introduce a novel approach combining learnable graph-based embeddings with a planning-based latent diffusion model (LDM). By capturing patterns and nuances underlying the interdependence of impression opportunities and the multi-agent dynamics of the auction environment, the graph representation enable expressive computations regarding auto-bidding outcomes. With reward alignment techniques, the LDM's posterior is fine-tuned to generate auto-bidding trajectories that maximize KPI metrics while satisfying constraint thresholds. Empirical evaluations on both real-world and synthetic auction environments demonstrate significant improvements in auto-bidding performance across multiple common KPI metrics, as well as accuracy in forecasting auction outcomes.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent LLM Judge: automatic personalized LLM judge design for evaluating natural language generation applications</title>
<link>https://arxiv.org/abs/2504.02867</link>
<guid>https://arxiv.org/abs/2504.02867</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)

<br /><br />:
LLMsLLMLLM <div>
arXiv:2504.02867v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive performance across diverse domains, yet they still encounter challenges such as insufficient domain-specific knowledge, biases, and hallucinations. This underscores the need for robust evaluation methodologies to accurately assess LLM-based applications. Traditional evaluation methods, which rely on word overlap or text embeddings, are inadequate for capturing the nuanced semantic information necessary to evaluate dynamic, open-ended text generation. Recent research has explored leveraging LLMs to mimic human reasoning and decision-making processes for evaluation purposes known as LLM-as-a-judge framework. However, these existing frameworks have two significant limitations. First, they lack the flexibility to adapt to different text styles, including various answer and ground truth styles, thereby reducing their generalization performance. Second, the evaluation scores produced by these frameworks are often skewed and hard to interpret, showing a low correlation with human judgment. To address these challenges, we propose a novel dynamic multi-agent system that automatically designs personalized LLM judges for various natural language generation applications. This system iteratively refines evaluation prompts and balances the trade-off between the adaptive requirements of downstream tasks and the alignment with human perception. Our experimental results show that the proposed multi-agent LLM Judge framework not only enhances evaluation accuracy compared to existing methods but also produces evaluation scores that better align with human perception.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent Framework for Resume Screening</title>
<link>https://arxiv.org/abs/2504.02870</link>
<guid>https://arxiv.org/abs/2504.02870</guid>
<content:encoded><![CDATA[
<div> 

:
<br />
RAGAIRAG-LLM <div>
arXiv:2504.02870v1 Announce Type: new 
Abstract: Resume screening is a critical yet time-intensive process in talent acquisition, requiring recruiters to analyze vast volume of job applications while remaining objective, accurate, and fair. With the advancements in Large Language Models (LLMs), their reasoning capabilities and extensive knowledge bases demonstrate new opportunities to streamline and automate recruitment workflows. In this work, we propose a multi-agent framework for resume screening using LLMs to systematically process and evaluate resumes. The framework consists of four core agents, including a resume extractor, an evaluator, a summarizer, and a score formatter. To enhance the contextual relevance of candidate assessments, we integrate Retrieval-Augmented Generation (RAG) within the resume evaluator, allowing incorporation of external knowledge sources, such as industry-specific expertise, professional certifications, university rankings, and company-specific hiring criteria. This dynamic adaptation enables personalized recruitment, bridging the gap between AI automation and talent acquisition. We assess the effectiveness of our approach by comparing AI-generated scores with ratings provided by HR professionals on a dataset of anonymized online resumes. The findings highlight the potential of multi-agent RAG-LLM systems in automating resume screening, enabling more efficient and scalable hiring workflows.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Survey Collection with LLM-based Conversational Agents</title>
<link>https://arxiv.org/abs/2504.02891</link>
<guid>https://arxiv.org/abs/2504.02891</guid>
<content:encoded><![CDATA[
<div> AIGPT-4

<br /><br />:
LLMGPT-48407.7%GPT-498%LLMLLMAI <div>
arXiv:2504.02891v1 Announce Type: new 
Abstract: Objective: Traditional phone-based surveys are among the most accessible and widely used methods to collect biomedical and healthcare data, however, they are often costly, labor intensive, and difficult to scale effectively. To overcome these limitations, we propose an end-to-end survey collection framework driven by conversational Large Language Models (LLMs).
  Materials and Methods: Our framework consists of a researcher responsible for designing the survey and recruiting participants, a conversational phone agent powered by an LLM that calls participants and administers the survey, a second LLM (GPT-4o) that analyzes the conversation transcripts generated during the surveys, and a database for storing and organizing the results. To test our framework, we recruited 8 participants consisting of 5 native and 3 non-native english speakers and administered 40 surveys. We evaluated the correctness of LLM-generated conversation transcripts, accuracy of survey responses inferred by GPT-4o and overall participant experience.
  Results: Survey responses were successfully extracted by GPT-4o from conversation transcripts with an average accuracy of 98% despite transcripts exhibiting an average per-line word error rate of 7.7%. While participants noted occasional errors made by the conversational LLM agent, they reported that the agent effectively conveyed the purpose of the survey, demonstrated good comprehension, and maintained an engaging interaction.
  Conclusions: Our study highlights the potential of LLM agents in conducting and analyzing phone surveys for healthcare applications. By reducing the workload on human interviewers and offering a scalable solution, this approach paves the way for real-world, end-to-end AI-powered phone survey collection systems.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Word-of-Mouth and Private-Prior Sequential Social Learning</title>
<link>https://arxiv.org/abs/2504.02913</link>
<guid>https://arxiv.org/abs/2504.02913</guid>
<content:encoded><![CDATA[
<div> : Word-of-Mouth (WoM)

:
(Word-of-Mouth, WoM)<br /><br /> <div>
arXiv:2504.02913v1 Announce Type: new 
Abstract: Social learning provides a fundamental framework in economics and social sciences for studying interactions among rational agents who observe each other's actions but lack direct access to individual beliefs. This paper investigates a specific social learning paradigm known as Word-of-Mouth (WoM), where a series of agents seeks to estimate the state of a dynamical system. The first agent receives noisy measurements of the state, while each subsequent agent relies solely on a degraded version of her predecessor's estimate. A defining feature of WoM is that the final agent's belief is publicly broadcast and adopted by all agents, in place of their own. We analyze this setting both theoretically and through numerical simulations, showing that some agents benefit from using the public belief broadcast by the last agent, while others suffer from performance deterioration.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributionally Robust Predictive Runtime Verification under Spatio-Temporal Logic Specifications</title>
<link>https://arxiv.org/abs/2504.02964</link>
<guid>https://arxiv.org/abs/2504.02964</guid>
<content:encoded><![CDATA[
<div> Cyber-physical

:
Cyber-physicalCPS(1) CPSSTL(2) MAS[1, 2]MASMAS <div>
arXiv:2504.02964v1 Announce Type: new 
Abstract: Cyber-physical systems designed in simulators, often consisting of multiple interacting agents, behave differently in the real-world. We would like to verify these systems during runtime when they are deployed. Thus, we propose robust predictive runtime verification (RPRV) algorithms for: (1) general stochastic CPS under signal temporal logic (STL) tasks, and (2) stochastic multi-agent systems (MAS) under spatio-temporal logic tasks. The RPRV problem presents the following challenges: (1) there may not be sufficient data on the behavior of the deployed CPS, (2) predictive models based on design phase system trajectories may encounter distribution shift during real-world deployment, and (3) the algorithms need to scale to the complexity of MAS and be applicable to spatio-temporal logic tasks. To address these challenges, we assume knowledge of an upper bound on the statistical distance (in terms of an f-divergence) between the trajectory distributions of the system at deployment and design time. We are motivated by our prior work [1, 2] where we proposed an accurate and an interpretable RPRV algorithm for general CPS, which we here extend to the MAS setting and spatio-temporal logic tasks. Specifically, we use a learned predictive model to estimate the system behavior at runtime and robust conformal prediction to obtain probabilistic guarantees by accounting for distribution shifts. Building on [1], we perform robust conformal prediction over the robust semantics of spatio-temporal reach and escape logic (STREL) to obtain centralized RPRV algorithms for MAS. We empirically validate our results in a drone swarm simulator, where we show the scalability of our RPRV algorithms to MAS and analyze the impact of different trajectory predictors on the verification result. To the best of our knowledge, these are the first statistically valid algorithms for MAS under distribution shift.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning via Object-Centric Attention</title>
<link>https://arxiv.org/abs/2504.03024</link>
<guid>https://arxiv.org/abs/2504.03024</guid>
<content:encoded><![CDATA[
<div> OCCAMObject-Centric Attention via Masking

:
OCCAMOCCAMAtariRLOCCAM <div>
arXiv:2504.03024v1 Announce Type: new 
Abstract: Deep reinforcement learning agents, trained on raw pixel inputs, often fail to generalize beyond their training environments, relying on spurious correlations and irrelevant background details. To address this issue, object-centric agents have recently emerged. However, they require different representations tailored to the task specifications. Contrary to deep agents, no single object-centric architecture can be applied to any environment. Inspired by principles of cognitive science and Occam's Razor, we introduce Object-Centric Attention via Masking (OCCAM), which selectively preserves task-relevant entities while filtering out irrelevant visual information. Specifically, OCCAM takes advantage of the object-centric inductive bias. Empirical evaluations on Atari benchmarks demonstrate that OCCAM significantly improves robustness to novel perturbations and reduces sample complexity while showing similar or improved performance compared to conventional pixel-based RL. These results suggest that structured abstraction can enhance generalization without requiring explicit symbolic representations or domain-specific object extraction pipelines.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ontologies in Design: How Imagining a Tree Reveals Possibilites and Assumptions in Large Language Models</title>
<link>https://arxiv.org/abs/2504.03029</link>
<guid>https://arxiv.org/abs/2504.03029</guid>
<content:encoded><![CDATA[
<div> : AI

<br />
:
AILLMLLM <div>
arXiv:2504.03029v1 Announce Type: new 
Abstract: Amid the recent uptake of Generative AI, sociotechnical scholars and critics have traced a multitude of resulting harms, with analyses largely focused on values and axiology (e.g., bias). While value-based analyses are crucial, we argue that ontologies -- concerning what we allow ourselves to think or talk about -- is a vital but under-recognized dimension in analyzing these systems. Proposing a need for a practice-based engagement with ontologies, we offer four orientations for considering ontologies in design: pluralism, groundedness, liveliness, and enactment. We share examples of potentialities that are opened up through these orientations across the entire LLM development pipeline by conducting two ontological analyses: examining the responses of four LLM-based chatbots in a prompting exercise, and analyzing the architecture of an LLM-based agent simulation. We conclude by sharing opportunities and limitations of working with ontologies in the design and development of sociotechnical systems.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety Modulation: Enhancing Safety in Reinforcement Learning through Cost-Modulated Rewards</title>
<link>https://arxiv.org/abs/2504.03040</link>
<guid>https://arxiv.org/abs/2504.03040</guid>
<content:encoded><![CDATA[
<div> : (Safe RL)(SMPO)<br /><br />:<br />
(SMPO)RLSMPOQ-SMPORL <div>
arXiv:2504.03040v1 Announce Type: new 
Abstract: Safe Reinforcement Learning (Safe RL) aims to train an RL agent to maximize its performance in real-world environments while adhering to safety constraints, as exceeding safety violation limits can result in severe consequences. In this paper, we propose a novel safe RL approach called Safety Modulated Policy Optimization (SMPO), which enables safe policy function learning within the standard policy optimization framework through safety modulated rewards. In particular, we consider safety violation costs as feedback from the RL environments that are parallel to the standard awards, and introduce a Q-cost function as safety critic to estimate expected future cumulative costs. Then we propose to modulate the rewards using a cost-aware weighting function, which is carefully designed to ensure the safety limits based on the estimation of the safety critic, while maximizing the expected rewards. The policy function and the safety critic are simultaneously learned through gradient descent during online interactions with the environment. We conduct experiments using multiple RL environments and the experimental results demonstrate that our method outperforms several classic and state-of-the-art comparison methods in terms of overall safe RL performance.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symbiotic AI: Augmenting Human Cognition from PCs to Cars</title>
<link>https://arxiv.org/abs/2504.03105</link>
<guid>https://arxiv.org/abs/2504.03105</guid>
<content:encoded><![CDATA[
<div> : AI, -, , , XR

:
AI-HCIAIHCIXRXR <div>
arXiv:2504.03105v1 Announce Type: new 
Abstract: As AI takes on increasingly complex roles in human-computer interaction, fundamental questions arise: how can HCI help maintain the user as the primary agent while augment human cognition and intelligence? This paper suggests questions to guide researchers in considering the implications for agency, autonomy, the augmentation of human intellect, and the future of human-AI synergies. We observe a key paradigm shift behind the transformation of HCI, shifting from explicit command-and-control models to systems where users define high-level goals directly. This shift will be facilitated by XR technologies, whose multi-modal inputs and outputs offer a more seamless way to convey these goals. This paper considers this transformation through the lens of two cultural milestones: the personal computer and the automobile, moving beyond traditional interfaces like keyboards or steering wheels and thinking of them as vessels for everyday XR.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Les Dissonances: Cross-Tool Harvesting and Polluting in Multi-Tool Empowered LLM Agents</title>
<link>https://arxiv.org/abs/2504.03111</link>
<guid>https://arxiv.org/abs/2504.03111</guid>
<content:encoded><![CDATA[
<div> : Chord

<br /><br />:
LLMXTHPLLMChordXTHPLLMLangChainLlamaIndex7380%78%XTH41%XTP <div>
arXiv:2504.03111v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents are autonomous systems powered by LLMs, capable of reasoning and planning to solve problems by leveraging a set of tools. However, the integration of multi-tool capabilities in LLM agents introduces challenges in securely managing tools, ensuring their compatibility, handling dependency relationships, and protecting control flows within LLM agent workflows. In this paper, we present the first systematic security analysis of task control flows in multi-tool-enabled LLM agents. We identify a novel threat, Cross-Tool Harvesting and Polluting (XTHP), which includes multiple attack vectors to first hijack the normal control flows of agent tasks, and then collect and pollute confidential or private information within LLM agent systems. To understand the impact of this threat, we developed Chord, a dynamic scanning tool designed to automatically detect real-world agent tools susceptible to XTHP attacks. Our evaluation of 73 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 80% of the tools are vulnerable to hijacking attacks, 78% to XTH attacks, and 41% to XTP attacks, highlighting the prevalence of this threat.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Resilience-Aware Control in Multi-Robot Networks</title>
<link>https://arxiv.org/abs/2504.03120</link>
<guid>https://arxiv.org/abs/2504.03120</guid>
<content:encoded><![CDATA[
<div> resilient consensusmulti-robot systemsmisbehaving agentsdistributed control lawControl Barrier Function (CBF)

<br /><br />:
Control Barrier Function (CBF) <div>
arXiv:2504.03120v1 Announce Type: new 
Abstract: Ensuring resilient consensus in multi-robot systems with misbehaving agents remains a challenge, as many existing network resilience properties are inherently combinatorial and globally defined. While previous works have proposed control laws to enhance or preserve resilience in multi-robot networks, they often assume a fixed topology with known resilience properties, or require global state knowledge. These assumptions may be impractical in physically-constrained environments, where safety and resilience requirements are conflicting, or when misbehaving agents corrupt the shared information. In this work, we propose a distributed control law that enables each robot to guarantee resilient consensus and safety during its navigation without fixed topologies using only locally available information. To this end, we establish a new sufficient condition for resilient consensus in time-varying networks based on the degree of non-misbehaving or normal agents. Using this condition, we design a Control Barrier Function (CBF)-based controller that guarantees resilient consensus and collision avoidance without requiring estimates of global state and/or control actions of all other robots. Finally, we validate our method through simulations.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MORAL: A Multimodal Reinforcement Learning Framework for Decision Making in Autonomous Laboratories</title>
<link>https://arxiv.org/abs/2504.03153</link>
<guid>https://arxiv.org/abs/2504.03153</guid>
<content:encoded><![CDATA[
<div> MORALDeep Q-Network

<br /><br />:
MORALBLIP-2QDQNPPO20%RLMORALBLEUMETEORROUGE-LMORALAI <div>
arXiv:2504.03153v1 Announce Type: new 
Abstract: We propose MORAL (a multimodal reinforcement learning framework for decision making in autonomous laboratories) that enhances sequential decision-making in autonomous robotic laboratories through the integration of visual and textual inputs. Using the BridgeData V2 dataset, we generate fine-tuned image captions with a pretrained BLIP-2 vision-language model and combine them with visual features through an early fusion strategy. The fused representations are processed using Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) agents. Experimental results demonstrate that multimodal agents achieve a 20% improvement in task completion rates and significantly outperform visual-only and textual-only baselines after sufficient training. Compared to transformer-based and recurrent multimodal RL models, our approach achieves superior performance in cumulative reward and caption quality metrics (BLEU, METEOR, ROUGE-L). These results highlight the impact of semantically aligned language cues in enhancing agent learning efficiency and generalization. The proposed framework contributes to the advancement of multimodal reinforcement learning and embodied AI systems in dynamic, real-world environments.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments</title>
<link>https://arxiv.org/abs/2504.03160</link>
<guid>https://arxiv.org/abs/2504.03160</guid>
<content:encoded><![CDATA[
<div> : , , , , DeepResearcher

<br /><br />:
DeepResearcherLLMsRAGDeepResearcherDeepResearcher28.9RAGRL7.2RLhttps://github.com/GAIR-NLP/DeepResearcher <div>
arXiv:2504.03160v1 Announce Type: new 
Abstract: Large Language Models (LLMs) equipped with web search capabilities have demonstrated impressive potential for deep research tasks. However, current approaches predominantly rely on either manually engineered prompts (prompt engineering-based) with brittle performance or reinforcement learning within controlled Retrieval-Augmented Generation (RAG) environments (RAG-based) that fail to capture the complexities of real-world interaction. In this paper, we introduce DeepResearcher, the first comprehensive framework for end-to-end training of LLM-based deep research agents through scaling reinforcement learning (RL) in real-world environments with authentic web search interactions. Unlike RAG-based approaches that assume all necessary information exists within a fixed corpus, our method trains agents to navigate the noisy, unstructured, and dynamic nature of the open web. We implement a specialized multi-agent architecture where browsing agents extract relevant information from various webpage structures and overcoming significant technical challenges. Extensive experiments on open-domain research tasks demonstrate that DeepResearcher achieves substantial improvements of up to 28.9 points over prompt engineering-based baselines and up to 7.2 points over RAG-based RL agents. Our qualitative analysis reveals emergent cognitive behaviors from end-to-end RL training, including the ability to formulate plans, cross-validate information from multiple sources, engage in self-reflection to redirect research, and maintain honesty when unable to find definitive answers. Our results highlight that end-to-end training in real-world web environments is not merely an implementation detail but a fundamental requirement for developing robust research capabilities aligned with real-world applications. We release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Penalty-based Bidirectional Reinforcement Learning Algorithms</title>
<link>https://arxiv.org/abs/2504.03163</link>
<guid>https://arxiv.org/abs/2504.03163</guid>
<content:encoded><![CDATA[
<div> : , , , , <br /><br />:
ManiRL4% <div>
arXiv:2504.03163v1 Announce Type: new 
Abstract: This research focuses on enhancing reinforcement learning (RL) algorithms by integrating penalty functions to guide agents in avoiding unwanted actions while optimizing rewards. The goal is to improve the learning process by ensuring that agents learn not only suitable actions but also which actions to avoid. Additionally, we reintroduce a bidirectional learning approach that enables agents to learn from both initial and terminal states, thereby improving speed and robustness in complex environments. Our proposed Penalty-Based Bidirectional methodology is tested against Mani skill benchmark environments, demonstrating an optimality improvement of success rate of approximately 4% compared to existing RL implementations. The findings indicate that this integrated strategy enhances policy learning, adaptability, and overall performance in challenging scenarios
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-lingual Multi-turn Automated Red Teaming for LLMs</title>
<link>https://arxiv.org/abs/2504.03174</link>
<guid>https://arxiv.org/abs/2504.03174</guid>
<content:encoded><![CDATA[
<div> : Language Model Models (LLMs), , , , <br /><br />:<br />
Language Model Models (LLMs)LLMsMulti-lingual Multi-turn Automated Red Teaming (MM-ART)5LLMs71%195%LLMs <div>
arXiv:2504.03174v1 Announce Type: new 
Abstract: Language Model Models (LLMs) have improved dramatically in the past few years, increasing their adoption and the scope of their capabilities over time. A significant amount of work is dedicated to ``model alignment'', i.e., preventing LLMs to generate unsafe responses when deployed into customer-facing applications. One popular method to evaluate safety risks is \textit{red-teaming}, where agents attempt to bypass alignment by crafting elaborate prompts that trigger unsafe responses from a model. Standard human-driven red-teaming is costly, time-consuming and rarely covers all the recent features (e.g., multi-lingual, multi-modal aspects), while proposed automation methods only cover a small subset of LLMs capabilities (i.e., English or single-turn). We present Multi-lingual Multi-turn Automated Red Teaming (\textbf{MM-ART}), a method to fully automate conversational, multi-lingual red-teaming operations and quickly identify prompts leading to unsafe responses. Through extensive experiments on different languages, we show the studied LLMs are on average 71\% more vulnerable after a 5-turn conversation in English than after the initial turn. For conversations in non-English languages, models display up to 195\% more safety vulnerabilities than the standard single-turn English approach, confirming the need for automated red-teaming methods matching LLMs capabilities.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward</title>
<link>https://arxiv.org/abs/2504.03206</link>
<guid>https://arxiv.org/abs/2504.03206</guid>
<content:encoded><![CDATA[
<div> 

:
RLHFRLHFLLMRLHF <div>
arXiv:2504.03206v1 Announce Type: new 
Abstract: Effective conversational agents must be able to personalize their behavior to suit a user's preferences, personality, and attributes, whether they are assisting with writing tasks or operating in domains like education or healthcare. Current training methods like Reinforcement Learning from Human Feedback (RLHF) prioritize helpfulness and safety but fall short in fostering truly empathetic, adaptive, and personalized interactions. Traditional approaches to personalization often rely on extensive user history, limiting their effectiveness for new or context-limited users. To overcome these limitations, we propose to incorporate an intrinsic motivation to improve the conversational agents's model of the user as an additional reward alongside multi-turn RLHF. This reward mechanism encourages the agent to actively elicit user traits by optimizing conversations to increase the accuracy of its user model. Consequently, the policy agent can deliver more personalized interactions through obtaining more information about the user. We applied our method both education and fitness settings, where LLMs teach concepts or recommend personalized strategies based on users' hidden learning style or lifestyle attributes. Using LLM-simulated users, our approach outperformed a multi-turn RLHF baseline in revealing information about the users' preferences, and adapting to them.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persuasive Calibration</title>
<link>https://arxiv.org/abs/2504.03211</link>
<guid>https://arxiv.org/abs/2504.03211</guid>
<content:encoded><![CDATA[
<div> : persuasive calibration problemprincipalagentExpected Calibration Error (ECE)algorithm

<br /><br />:
persuasive calibrationLtECE

L1-norm ECE(1) (2) Lt-norm ECEFPTASL1-L-Infinity-norm ECE <div>
arXiv:2504.03211v1 Announce Type: new 
Abstract: We introduce and study the persuasive calibration problem, where a principal aims to provide trustworthy predictions about underlying events to a downstream agent to make desired decisions. We adopt the standard calibration framework that regulates predictions to be unbiased conditional on their own value, and thus, they can reliably be interpreted at the face value by the agent. Allowing a small calibration error budget, we aim to answer the following question: what is and how to compute the optimal predictor under this calibration error budget, especially when there exists incentive misalignment between the principal and the agent? We focus on standard Lt-norm Expected Calibration Error (ECE) metric.
  We develop a general framework by viewing predictors as post-processed versions of perfectly calibrated predictors. Using this framework, we first characterize the structure of the optimal predictor. Specifically, when the principal's utility is event-independent and for L1-norm ECE, we show: (1) the optimal predictor is over-(resp. under-) confident for high (resp. low) true expected outcomes, while remaining perfectly calibrated in the middle; (2) the miscalibrated predictions exhibit a collinearity structure with the principal's utility function. On the algorithmic side, we provide a FPTAS for computing approximately optimal predictor for general principal utility and general Lt-norm ECE. Moreover, for the L1- and L-Infinity-norm ECE, we provide polynomial-time algorithms that compute the exact optimal predictor.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing is Believing: Belief-Space Planning with Foundation Models as Uncertainty Estimators</title>
<link>https://arxiv.org/abs/2504.03245</link>
<guid>https://arxiv.org/abs/2504.03245</guid>
<content:encoded><![CDATA[
<div> 

:
(VLMs)VLMVLMVLM<br /><br /> <div>
arXiv:2504.03245v1 Announce Type: new 
Abstract: Generalizable robotic mobile manipulation in open-world environments poses significant challenges due to long horizons, complex goals, and partial observability. A promising approach to address these challenges involves planning with a library of parameterized skills, where a task planner sequences these skills to achieve goals specified in structured languages, such as logical expressions over symbolic facts. While vision-language models (VLMs) can be used to ground these expressions, they often assume full observability, leading to suboptimal behavior when the agent lacks sufficient information to evaluate facts with certainty. This paper introduces a novel framework that leverages VLMs as a perception module to estimate uncertainty and facilitate symbolic grounding. Our approach constructs a symbolic belief representation and uses a belief-space planner to generate uncertainty-aware plans that incorporate strategic information gathering. This enables the agent to effectively reason about partial observability and property uncertainty. We demonstrate our system on a range of challenging real-world tasks that require reasoning in partially observable environments. Simulated evaluations show that our approach outperforms both vanilla VLM-based end-to-end planning or VLM-based state estimation baselines by planning for and executing strategic information gathering. This work highlights the potential of VLMs to construct belief-space symbolic scene representations, enabling downstream tasks such as uncertainty-aware planning.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inherent and emergent liability issues in LLM-based agentic systems: a principal-agent perspective</title>
<link>https://arxiv.org/abs/2504.03255</link>
<guid>https://arxiv.org/abs/2504.03255</guid>
<content:encoded><![CDATA[
<div> : 

<br /><br />:
--AILLM <div>
arXiv:2504.03255v1 Announce Type: new 
Abstract: Agentic systems powered by large language models (LLMs) are becoming progressively more complex and capable. Their increasing agency and expanding deployment settings attract growing attention over effective governance policies, monitoring and control protocols. Based on emerging landscapes of the agentic market, we analyze the potential liability issues stemming from delegated use of LLM agents and their extended systems from a principal-agent perspective. Our analysis complements existing risk-based studies on artificial agency and covers the spectrum of important aspects of the principal-agent relationship and their potential consequences at deployment. Furthermore, we motivate method developments for technical governance along the directions of interpretability and behavior evaluations, reward and conflict management, and the mitigation of misalignment and misconduct through principled engineering of detection and fail-safe mechanisms. By illustrating the outstanding issues in AI liability for LLM-based agentic systems, we aim to inform the system design, auditing and monitoring approaches to enhancing transparency and accountability.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy Aware and Safe Path Planning for Unmanned Aircraft Systems</title>
<link>https://arxiv.org/abs/2504.03271</link>
<guid>https://arxiv.org/abs/2504.03271</guid>
<content:encoded><![CDATA[
<div> 

<br />
 <div>
arXiv:2504.03271v1 Announce Type: new 
Abstract: This paper proposes a path planning algorithm for multi-agent unmanned aircraft systems (UASs) to autonomously cover a search area, while considering obstacle avoidance, as well as the capabilities and energy consumption of the employed unmanned aerial vehicles. The path planning is optimized in terms of energy efficiency to prefer low energy-consuming maneuvers. In scenarios where a UAS is low on energy, it autonomously returns to its initial position for a safe landing, thus preventing potential battery damage. To accomplish this, an energy-aware multicopter model is integrated into a path planning algorithm based on model predictive control and mixed integer linear programming. Besides factoring in energy consumption, the planning is improved by dynamically defining feasible regions for each UAS to prevent obstacle corner-cutting or over-jumping.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning-Based Conformal Tube MPC for Safe Control in Interactive Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2504.03293</link>
<guid>https://arxiv.org/abs/2504.03293</guid>
<content:encoded><![CDATA[
<div> : 

:
MPCMPC- <div>
arXiv:2504.03293v1 Announce Type: new 
Abstract: Safety assurance in multi-agent systems with coupled dynamics is a fundamental yet challenging problem, especially when agents exhibit uncertain and state-dependent behaviors. Classical robust control often assumes worst-case disturbances, leading to overly conservative actions. In this work, we propose a learning-based framework that combines conformal prediction with model predictive control (MPC) to ensure probabilistic safety under action-level uncertainty. Unlike prior approaches that predict future states, we directly model the control action of the uncontrollable agent as a stochastic function of the joint state, trained via neural networks and calibrated using conformal prediction. This enables us to construct dynamic, probabilistically guaranteed reachable tubes for the uncontrollable agent. These tubes are then embedded into an MPC formulation to synthesize control actions for the controllable agent that ensure safe interactions over a finite planning horizon. We provide formal stepwise and cumulative safety guarantees, and demonstrate the effectiveness of our approach through a pedestrian-vehicle interaction scenario. Compared to baseline methods, our framework achieves higher safety rates while maintaining high performance in terms of speed and responsiveness.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Talk2X -- An Open-Source Toolkit Facilitating Deployment of LLM-Powered Chatbots on the Web</title>
<link>https://arxiv.org/abs/2504.03343</link>
<guid>https://arxiv.org/abs/2504.03343</guid>
<content:encoded><![CDATA[
<div> LLM-powered chatbotsenergy efficiencyTalk2Xretrieval-augmented generationopen science repository

<br /><br />:
Talk2XRAGTalk2XTalk2X <div>
arXiv:2504.03343v1 Announce Type: new 
Abstract: Integrated into websites, LLM-powered chatbots offer alternative means of navigation and information retrieval, leading to a shift in how users access information on the web. Yet, predominantly closed-sourced solutions limit proliferation among web hosts and suffer from a lack of transparency with regard to implementation details and energy efficiency. In this work, we propose our openly available agent Talk2X leveraging an adapted retrieval-augmented generation approach (RAG) combined with an automatically generated vector database, benefiting energy efficiency. Talk2X's architecture is generalizable to arbitrary websites offering developers a ready to use tool for integration. Using a mixed-methods approach, we evaluated Talk2X's usability by tasking users to acquire specific assets from an open science repository. Talk2X significantly improved task completion time, correctness, and user experience supporting users in quickly pinpointing specific information as compared to standard user-website interaction. Our findings contribute technical advancements to an ongoing paradigm shift of how we access information on the web.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Collective World Model for Emergent Communication and Coordination</title>
<link>https://arxiv.org/abs/2504.03353</link>
<guid>https://arxiv.org/abs/2504.03353</guid>
<content:encoded><![CDATA[
<div> : 

:
 <div>
arXiv:2504.03353v1 Announce Type: new 
Abstract: We propose a fully decentralized multi-agent world model that enables both symbol emergence for communication and coordinated behavior through temporal extension of collective predictive coding. Unlike previous research that focuses on either communication or coordination separately, our approach achieves both simultaneously. Our method integrates world models with communication channels, enabling agents to predict environmental dynamics, estimate states from partial observations, and share critical information through bidirectional message exchange with contrastive learning for message alignment. Using a two-agent trajectory drawing task, we demonstrate that our communication-based approach outperforms non-communicative models when agents have divergent perceptual capabilities, achieving the second-best coordination after centralized models. Importantly, our distributed approach with constraints preventing direct access to other agents' internal states facilitates the emergence of more meaningful symbol systems that accurately reflect environmental states. These findings demonstrate the effectiveness of decentralized communication for supporting coordination while developing shared representations of the environment.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Network Topology in a Two-way Competition for Influence in the Friedkin-Johnsen Model</title>
<link>https://arxiv.org/abs/2504.03397</link>
<guid>https://arxiv.org/abs/2504.03397</guid>
<content:encoded><![CDATA[
<div> stubborn agents, influence, strongly connected networks, network perturbation, Sampson's Monastery dataset

<br /><br />:

Sampson's Monastery <div>
arXiv:2504.03397v1 Announce Type: new 
Abstract: In this paper, we consider two stubborn agents who compete for `influence' over a strongly connected group of agents. This framework represents real-world contests, such as competition among firms, two-party elections, and sports rivalries, among others. Considering stubbornness of agents to be an immutable property, we utilise the network topology alone to increase the influence of a preferred stubborn agent. We demonstrate this on a special class of strongly connected networks by identifying the supporters of each of the stubborn agents in such networks. Thereafter, we present sufficient conditions under which a network perturbation always increases the influence of the preferred stubborn agent. A key advantage of the proposed topology-based conditions is that they hold independent of the edge weights in the network. Most importantly, we assert that there exists a sequence of perturbations that can make the lesser influential stubborn agent more influential. Finally, we demonstrate our results over the Sampson's Monastery dataset.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RANa: Retrieval-Augmented Navigation</title>
<link>https://arxiv.org/abs/2504.03524</link>
<guid>https://arxiv.org/abs/2504.03524</guid>
<content:encoded><![CDATA[
<div> retrieval-augmented agentRLdatabasevision foundation modelsnavigation tasks

<br />
:
ObjectNavImageNavInstance-ImageNav(FM) <div>
arXiv:2504.03524v1 Announce Type: new 
Abstract: Methods for navigation based on large-scale learning typically treat each episode as a new problem, where the agent is spawned with a clean memory in an unknown environment. While these generalization capabilities to an unknown environment are extremely important, we claim that, in a realistic setting, an agent should have the capacity of exploiting information collected during earlier robot operations. We address this by introducing a new retrieval-augmented agent, trained with RL, capable of querying a database collected from previous episodes in the same environment and learning how to integrate this additional context information. We introduce a unique agent architecture for the general navigation task, evaluated on ObjectNav, ImageNav and Instance-ImageNav. Our retrieval and context encoding methods are data-driven and heavily employ vision foundation models (FM) for both semantic and geometric understanding. We propose new benchmarks for these settings and we show that retrieval allows zero-shot transfer across tasks and environments while significantly improving performance.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Limits of "Fairness" of the Variational Generalized Nash Equilibrium</title>
<link>https://arxiv.org/abs/2504.03540</link>
<guid>https://arxiv.org/abs/2504.03540</guid>
<content:encoded><![CDATA[
<div> Generalized Nash equilibrium (GNE)variational GNEfairnessf-GNEelectric vehicle charging game

:
GNEGNEGNEf-GNEGNEf-GNE <div>
arXiv:2504.03540v1 Announce Type: new 
Abstract: Generalized Nash equilibrum (GNE) problems are commonly used to model strategic interactions between self-interested agents who are coupled in cost and constraints. Specifically, the variational GNE, a refinement of the GNE, is often selected as the solution concept due to it's non-discriminatory treatment of agents by charging a uniform ``shadow price" for shared resources. We study the fairness concept of v-GNEs from a comparability perspective and show that it makes an implicit assumption of unit comparability of agent's cost functions, one of the strongest comparability notions. Further, we introduce a new solution concept, f-GNE in which a fairness metric is chosen a priori which is compatible with the comparability at hand. We introduce an electric vehicle charging game to demonstrate the fragility of v-GNE fairness and compare it to the f-GNE under various fairness metrics.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Knowledgeable Self-awareness</title>
<link>https://arxiv.org/abs/2504.03553</link>
<guid>https://arxiv.org/abs/2504.03553</guid>
<content:encoded><![CDATA[
<div> : KnowSelf

:
agentic knowledgeable self-awarenessKnowSelfLLMKnowSelfhttps://github.com/zjunlp/KnowSelf <div>
arXiv:2504.03553v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a "flood irrigation" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent's self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at https://github.com/zjunlp/KnowSelf.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement</title>
<link>https://arxiv.org/abs/2504.03561</link>
<guid>https://arxiv.org/abs/2504.03561</guid>
<content:encoded><![CDATA[
<div> SynWorldLLM

:
SynWorldLLMSynWorldMCTSSynWorldarXivhttps://github.com/zjunlp/SynWorld<br /><br /> <div>
arXiv:2504.03561v1 Announce Type: new 
Abstract: In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay</title>
<link>https://arxiv.org/abs/2504.03601</link>
<guid>https://arxiv.org/abs/2504.03601</guid>
<content:encoded><![CDATA[
<div> AIAPIGen-MTxLAM-2-fc-r

:
AIAPIGen-MTLLMxLAM-2-fc-r1B70B$\tau$-benchBFCLGPT-4oClaude 3.5AIxLAM-2-fc-rHuggingFacehttps://apigen-mt.github.io <div>
arXiv:2504.03601v1 Announce Type: new 
Abstract: Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on $\tau$-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source both the synthetic data collected and the trained xLAM-2-fc-r models to advance research in AI agents. Models are available on HuggingFace at https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4 and project website is https://apigen-mt.github.io
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bonsai: Interpretable Tree-Adaptive Grounded Reasoning</title>
<link>https://arxiv.org/abs/2504.03640</link>
<guid>https://arxiv.org/abs/2504.03640</guid>
<content:encoded><![CDATA[
<div> : BonsaiAI

<br /><br />:
BonsaiBonsaiBonsai <div>
arXiv:2504.03640v1 Announce Type: new 
Abstract: To develop general-purpose collaborative agents, humans need reliable AI systems that can (1) adapt to new domains and (2) transparently reason with uncertainty to allow for verification and correction. Black-box models demonstrate powerful data processing abilities but do not satisfy these criteria due to their opaqueness, domain specificity, and lack of uncertainty awareness. We introduce Bonsai, a compositional and probabilistic reasoning system that generates adaptable inference trees by retrieving relevant grounding evidence and using it to compute likelihoods of sub-claims derived from broader natural language inferences. Bonsai's reasoning power is tunable at test-time via evidence scaling and it demonstrates reliable handling of varied domains including transcripts, photographs, videos, audio, and databases. Question-answering and human alignment experiments demonstrate that Bonsai matches the performance of domain-specific black-box methods while generating interpretable, grounded, and uncertainty-aware reasoning traces.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Min-Max Optimization via Primal-Dual Exact Pareto Optimization</title>
<link>https://arxiv.org/abs/2504.02833</link>
<guid>https://arxiv.org/abs/2504.02833</guid>
<content:encoded><![CDATA[
<div> <br /><br />:
min-max primal-dual min-max-EPO-ALEPO-ALmin-max <div>
arXiv:2504.02833v1 Announce Type: cross 
Abstract: In multi-objective optimization, minimizing the worst objective can be preferable to minimizing the average objective, as this ensures improved fairness across objectives. Due to the non-smooth nature of the resultant min-max optimization problem, classical subgradient-based approaches typically exhibit slow convergence. Motivated by primal-dual consensus techniques in multi-agent optimization and learning, we formulate a smooth variant of the min-max problem based on the augmented Lagrangian. The resultant Exact Pareto Optimization via Augmented Lagrangian (EPO-AL) algorithm scales better with the number of objectives than subgradient-based strategies, while exhibiting lower per-iteration complexity than recent smoothing-based counterparts. We establish that every fixed-point of the proposed algorithm is both Pareto and min-max optimal under mild assumptions and demonstrate its effectiveness in numerical simulations.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The AI Cosmologist I: An Agentic System for Automated Data Analysis</title>
<link>https://arxiv.org/abs/2504.03424</link>
<guid>https://arxiv.org/abs/2504.03424</guid>
<content:encoded><![CDATA[
<div> : AI Cosmologist<br /><br />:
AI Cosmologist/AI CosmologistAI CosmologistGitHubhttps://github.com/adammoss/aicosmologist <div>
arXiv:2504.03424v1 Announce Type: cross 
Abstract: We present the AI Cosmologist, an agentic system designed to automate cosmological/astronomical data analysis and machine learning research workflows. This implements a complete pipeline from idea generation to experimental evaluation and research dissemination, mimicking the scientific process typically performed by human researchers. The system employs specialized agents for planning, coding, execution, analysis, and synthesis that work together to develop novel approaches. Unlike traditional auto machine-learning systems, the AI Cosmologist generates diverse implementation strategies, writes complete code, handles execution errors, analyzes results, and synthesizes new approaches based on experimental outcomes. We demonstrate the AI Cosmologist capabilities across several machine learning tasks, showing how it can successfully explore solution spaces, iterate based on experimental results, and combine successful elements from different approaches. Our results indicate that agentic systems can automate portions of the research process, potentially accelerating scientific discovery. The code and experimental data used in this paper are available on GitHub at https://github.com/adammoss/aicosmologist. Example papers included in the appendix demonstrate the system's capability to autonomously produce complete scientific publications, starting from only the dataset and task description
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proximal Policy Optimization with Graph Neural Networks for Optimal Power Flow</title>
<link>https://arxiv.org/abs/2212.12470</link>
<guid>https://arxiv.org/abs/2212.12470</guid>
<content:encoded><![CDATA[
<div> Optimal Power Flow (OPF)Alternating Current Optimal Power Flow (ACOPF)Graph Neural Networks (GNN)Deep Reinforcement Learning (DRL)Proximal Policy Optimization

:
DRLGNNOptimal Power Flow, OPFACOPFACOPFGNNDRLProximal Policy OptimizationIEEE 30 busDRLOPFDCOPF <div>
arXiv:2212.12470v3 Announce Type: replace 
Abstract: Optimal Power Flow (OPF) is a very traditional research area within the power systems field that seeks for the optimal operation point of electric power plants, and which needs to be solved every few minutes in real-world scenarios. However, due to the nonconvexities that arise in power generation systems, there is not yet a fast, robust solution technique for the full Alternating Current Optimal Power Flow (ACOPF). In the last decades, power grids have evolved into a typical dynamic, non-linear and large-scale control system, known as the power system, so searching for better and faster ACOPF solutions is becoming crucial. Appearance of Graph Neural Networks (GNN) has allowed the natural use of Machine Learning (ML) algorithms on graph data, such as power networks. On the other hand, Deep Reinforcement Learning (DRL) is known for its powerful capability to solve complex decision-making problems. Although solutions that use these two methods separately are beginning to appear in the literature, none has yet combined the advantages of both. We propose a novel architecture based on the Proximal Policy Optimization algorithm with Graph Neural Networks to solve the Optimal Power Flow. The objective is to design an architecture that learns how to solve the optimization problem and that is at the same time able to generalize to unseen scenarios. We compare our solution with the DCOPF in terms of cost after having trained our DRL agent on IEEE 30 bus system and then computing the OPF on that base network with topology changes
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spacewalk-18: A Benchmark for Multimodal and Long-form Procedural Video Understanding in Novel Domains</title>
<link>https://arxiv.org/abs/2311.18773</link>
<guid>https://arxiv.org/abs/2311.18773</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:
Spacewalk-18Spacewalk-18Spacewalk-18https://brown-palm.github.io/Spacewalk-18/ <div>
arXiv:2311.18773v3 Announce Type: replace 
Abstract: Learning from (procedural) videos has increasingly served as a pathway for embodied agents to acquire skills from human demonstrations. To do this, video understanding models must be able to obtain structured understandings, such as the temporal segmentation of a demonstration into sequences of actions and skills, and to generalize the understandings to novel environments, tasks, and problem domains. In pursuit of this goal, we introduce Spacewalk-18, a benchmark containing two tasks: (1) step recognition and (2) video question answering, over a dataset of temporally segmented and labeled tasks in International Space Station spacewalk recordings. In tandem, the two tasks quantify a model's ability to: (1) generalize to novel domains; (2) utilize long temporal context and multimodal (e.g. visual and speech) information. Our extensive experimental analysis highlights the challenges of Spacewalk-18, but also suggests best practices for domain generalization and long-form understanding. Notably, we discover a promising adaptation via summarization technique that leads to significant performance improvement without model fine-tuning. The Spacewalk-18 benchmark is released at https://brown-palm.github.io/Spacewalk-18/.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LogiCity: Advancing Neuro-Symbolic AI with Abstract Urban Simulation</title>
<link>https://arxiv.org/abs/2411.00773</link>
<guid>https://arxiv.org/abs/2411.00773</guid>
<content:encoded><![CDATA[
<div> : Neuro-Symbolic AI, LogiCity, , , 

<br /><br />:
AILogiCityLogiCityFOLLogiCityLogiCityAI <div>
arXiv:2411.00773v2 Announce Type: replace 
Abstract: Recent years have witnessed the rapid development of Neuro-Symbolic (NeSy) AI systems, which integrate symbolic reasoning into deep neural networks. However, most of the existing benchmarks for NeSy AI fail to provide long-horizon reasoning tasks with complex multi-agent interactions. Furthermore, they are usually constrained by fixed and simplistic logical rules over limited entities, making them far from real-world complexities. To address these crucial gaps, we introduce LogiCity, the first simulator based on customizable first-order logic (FOL) for an urban-like environment with multiple dynamic agents. LogiCity models diverse urban elements using semantic and spatial concepts, such as IsAmbulance(X) and IsClose(X, Y). These concepts are used to define FOL rules that govern the behavior of various agents. Since the concepts and rules are abstractions, they can be universally applied to cities with any agent compositions, facilitating the instantiation of diverse scenarios. Besides, a key feature of LogiCity is its support for user-configurable abstractions, enabling customizable simulation complexities for logical reasoning. To explore various aspects of NeSy AI, LogiCity introduces two tasks, one features long-horizon sequential decision-making, and the other focuses on one-step visual reasoning, varying in difficulty and agent behaviors. Our extensive evaluation reveals the advantage of NeSy frameworks in abstract reasoning. Moreover, we highlight the significant challenges of handling more complex abstractions in long-horizon multi-agent scenarios or under high-dimensional, imbalanced data. With its flexible design, various features, and newly raised challenges, we believe LogiCity represents a pivotal step forward in advancing the next generation of NeSy AI. All the code and data are open-sourced at our website: https://jaraxxus-me.github.io/LogiCity/
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D-Mem: 3D Scene Memory for Embodied Exploration and Reasoning</title>
<link>https://arxiv.org/abs/2411.17735</link>
<guid>https://arxiv.org/abs/2411.17735</guid>
<content:encoded><![CDATA[
<div> 3D3D-Mem

<br />
:
3D3D-Mem3D3D-Mem3D-Mem3D-Mem3DAI <div>
arXiv:2411.17735v5 Announce Type: replace 
Abstract: Constructing compact and informative 3D scene representations is essential for effective embodied exploration and reasoning, especially in complex environments over extended periods. Existing representations, such as object-centric 3D scene graphs, oversimplify spatial relationships by modeling scenes as isolated objects with restrictive textual relationships, making it difficult to address queries requiring nuanced spatial understanding. Moreover, these representations lack natural mechanisms for active exploration and memory management, hindering their application to lifelong autonomy. In this work, we propose 3D-Mem, a novel 3D scene memory framework for embodied agents. 3D-Mem employs informative multi-view images, termed Memory Snapshots, to represent the scene and capture rich visual information of explored regions. It further integrates frontier-based exploration by introducing Frontier Snapshots-glimpses of unexplored areas-enabling agents to make informed decisions by considering both known and potential new information. To support lifelong memory in active exploration settings, we present an incremental construction pipeline for 3D-Mem, as well as a memory retrieval technique for memory management. Experimental results on three benchmarks demonstrate that 3D-Mem significantly enhances agents' exploration and reasoning capabilities in 3D environments, highlighting its potential for advancing applications in embodied AI.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Division Beyond Monotone Valuations</title>
<link>https://arxiv.org/abs/2501.14609</link>
<guid>https://arxiv.org/abs/2501.14609</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:
(EF)(EQ)EFEFEF3EF3EQ3G=(V, E)k|V|VV_1, V_2, ..., V_k <div>
arXiv:2501.14609v3 Announce Type: replace 
Abstract: This paper studies fair division of divisible and indivisible items among agents whose cardinal preferences are not necessarily monotone. We establish the existence of fair divisions and develop approximation algorithms to compute them.
  We address two complementary valuation classes, subadditive and nonnegative, which go beyond monotone functions. Considering both the division of cake (divisible resources) and allocation of indivisible items, we obtain fairness guarantees in terms of (approximate) envy-freeness (EF) and equability (EQ)
  In the context of envy-freeness, we prove that an EF division of a cake always exists under cake valuations that are subadditive and globally nonnegative. This result complements the nonexistence of EF allocations for burnt cakes known for more general valuations. In the indivisible-items setting, we establish the existence of EF3 allocations for subadditive and globally nonnegative valuations. In addition, we obtain universal existence of EF3 allocations under nonnegative valuations.
  We study equitability under nonnegative valuations. Here, we prove that EQ3 allocations always exist when the agents' valuations are nonnegative. Also, in the indivisible-items setting, we develop an approximation algorithm that, for given nonnegative valuations, finds allocations that are equitable within additive margins.
  Our results have combinatorial implications. For instance, the developed results imply the universal existence of proximately dense subgraphs: Given any graph $G=(V, E)$ and integer $k$ (at most $|V|$), there always exists a partition $V_1, V_2, \ldots, V_k$ of the vertex set such that the edge densities within the parts, $V_i$, are additively within four of each other. Further, such a partition can be computed efficiently.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable Explanations</title>
<link>https://arxiv.org/abs/2502.15429</link>
<guid>https://arxiv.org/abs/2502.15429</guid>
<content:encoded><![CDATA[
<div> : Pub-Guard-LLMPubMed Retraction

<br /><br />
:
Pub-Guard-LLMPubMed Retraction11KPub-Guard-LLMPub-Guard-LLM <div>
arXiv:2502.15429v3 Announce Type: replace 
Abstract: A significant and growing number of published scientific articles is found to involve fraudulent practices, posing a serious threat to the credibility and safety of research in fields such as medicine. We propose Pub-Guard-LLM, the first large language model-based system tailored to fraud detection of biomedical scientific articles. We provide three application modes for deploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and multi-agent debate. Each mode allows for textual explanations of predictions. To assess the performance of our system, we introduce an open-source benchmark, PubMed Retraction, comprising over 11K real-world biomedical articles, including metadata and retraction labels. We show that, across all modes, Pub-Guard-LLM consistently surpasses the performance of various baselines and provides more reliable explanations, namely explanations which are deemed more relevant and coherent than those generated by the baselines when evaluated by multiple assessment methods. By enhancing both detection performance and explainability in scientific fraud detection, Pub-Guard-LLM contributes to safeguarding research integrity with a novel, effective, open-source tool.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictive Traffic Rule Compliance using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.22925</link>
<guid>https://arxiv.org/abs/2503.22925</guid>
<content:encoded><![CDATA[
<div> : 

:
 <div>
arXiv:2503.22925v2 Announce Type: replace 
Abstract: Autonomous vehicle path planning has reached a stage where safety and regulatory compliance are crucial. This paper presents an approach that integrates a motion planner with a deep reinforcement learning model to predict potential traffic rule violations. Our main innovation is replacing the standard actor network in an actor-critic method with a motion planning module, which ensures both stable and interpretable trajectory generation. In this setup, we use traffic rule robustness as the reward to train a reinforcement learning agent's critic, and the output of the critic is directly used as the cost function of the motion planner, which guides the choices of the trajectory. We incorporate some key interstate rules from the German Road Traffic Regulation into a rule book and use a graph-based state representation to handle complex traffic information. Experiments on an open German highway dataset show that the model can predict and prevent traffic rule violations beyond the planning horizon, increasing safety and rule compliance in challenging traffic scenarios.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Marco: Configurable Graph-Based Task Solving and Multi-AI Agents Framework for Hardware Design</title>
<link>https://arxiv.org/abs/2504.01962</link>
<guid>https://arxiv.org/abs/2504.01962</guid>
<content:encoded><![CDATA[
<div> Hardware design, Large Language Models (LLMs), Turn-around-time (TAT), Marco, Graph-based task solving

<br /><br />:
MarcoAILLMsLLMVerilog/DRCTAT <div>
arXiv:2504.01962v1 Announce Type: new 
Abstract: Hardware design presents numerous challenges stemming from its complexity and advancing technologies. These challenges result in longer turn-around-time (TAT) for optimizing performance, power, area, and cost (PPAC) during synthesis, verification, physical design, and reliability loops. Large Language Models (LLMs) have shown remarkable capacity to comprehend and generate natural language at a massive scale, leading to many potential applications and benefits across various domains. Successful LLM-based agents for hardware design can drastically reduce TAT, leading to faster product cycles, lower costs, improved design reliability and reduced risk of costly errors. In this work, we propose a unified framework, Marco, that integrates configurable graph-based task solving with multi-modality and multi-AI agents for chip design by leveraging the natural language and reasoning abilities with collaborative toolkits. Lastly, we demonstrate promising performance, productivity, and efficiency of LLM agents by leveraging the Marco framework on layout optimization, Verilog/design rule checker (DRC) coding, and timing analysis tasks.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems</title>
<link>https://arxiv.org/abs/2504.01963</link>
<guid>https://arxiv.org/abs/2504.01963</guid>
<content:encoded><![CDATA[
<div> : (LLM)Mixture of AgentsReAct

<br /><br />:
LLMMixture of AgentsReAct <div>
arXiv:2504.01963v1 Announce Type: new 
Abstract: This survey investigates foundational technologies essential for developing effective Large Language Model (LLM)-based multi-agent systems. Aiming to answer how best to optimize these systems for collaborative, dynamic environments, we focus on four critical areas: Architecture, Memory, Planning, and Technologies/Frameworks. By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape. Frameworks like the Mixture of Agents architecture and the ReAct planning model exemplify current innovations, showcasing improvements in role assignment and decision-making. This review synthesizes key strengths and persistent challenges, offering practical recommendations to enhance system scalability, agent collaboration, and adaptability. Our findings provide a roadmap for future research, supporting the creation of robust, efficient multi-agent systems that advance both individual agent performance and collective system resilience.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems</title>
<link>https://arxiv.org/abs/2504.01990</link>
<guid>https://arxiv.org/abs/2504.01990</guid>
<content:encoded><![CDATA[
<div> 

<br />
:
AutoMLLLM <div>
arXiv:2504.01990v1 Announce Type: new 
Abstract: The advent of large language models (LLMs) has catalyzed a transformative shift in artificial intelligence, paving the way for advanced intelligent agents capable of sophisticated reasoning, robust perception, and versatile action across diverse domains. As these agents increasingly drive AI research and practical applications, their design, evaluation, and continuous improvement present intricate, multifaceted challenges. This survey provides a comprehensive overview, framing intelligent agents within a modular, brain-inspired architecture that integrates principles from cognitive science, neuroscience, and computational research. We structure our exploration into four interconnected parts. First, we delve into the modular foundation of intelligent agents, systematically mapping their cognitive, perceptual, and operational modules onto analogous human brain functionalities, and elucidating core components such as memory, world modeling, reward processing, and emotion-like systems. Second, we discuss self-enhancement and adaptive evolution mechanisms, exploring how agents autonomously refine their capabilities, adapt to dynamic environments, and achieve continual learning through automated optimization paradigms, including emerging AutoML and LLM-driven optimization strategies. Third, we examine collaborative and evolutionary multi-agent systems, investigating the collective intelligence emerging from agent interactions, cooperation, and societal structures, highlighting parallels to human social dynamics. Finally, we address the critical imperative of building safe, secure, and beneficial AI systems, emphasizing intrinsic and extrinsic security threats, ethical alignment, robustness, and practical mitigation strategies necessary for trustworthy real-world deployment.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Navigation for Autonomous Aerial Vehicles Using Video</title>
<link>https://arxiv.org/abs/2504.01996</link>
<guid>https://arxiv.org/abs/2504.01996</guid>
<content:encoded><![CDATA[
<div> : (MDP)

:
(MDP)3D(CV) <div>
arXiv:2504.01996v1 Announce Type: new 
Abstract: Most applications in autonomous navigation using mounted cameras rely on the construction and processing of geometric 3D point clouds, which is an expensive process. However, there is another simpler way to make a space navigable quickly: to use semantic information (e.g., traffic signs) to guide the agent. However, detecting and acting on semantic information involves Computer Vision~(CV) algorithms such as object detection, which themselves are demanding for agents such as aerial drones with limited onboard resources. To solve this problem, we introduce a novel Markov Decision Process~(MDP) framework to reduce the workload of these CV approaches. We apply our proposed framework to both feature-based and neural-network-based object-detection tasks, using open-loop and closed-loop simulations as well as hardware-in-the-loop emulations. These holistic tests show significant benefits in energy consumption and speed with only a limited loss in accuracy compared to models based on static features and neural networks.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Resource Allocation in Multi-Agent LLM Systems</title>
<link>https://arxiv.org/abs/2504.02051</link>
<guid>https://arxiv.org/abs/2504.02051</guid>
<content:encoded><![CDATA[
<div> : LLMs

<br /><br />:
LLMsLLMsLLMs <div>
arXiv:2504.02051v1 Announce Type: new 
Abstract: With the development of LLMs as agents, there is a growing interest in connecting multiple agents into multi-agent systems to solve tasks concurrently, focusing on their role in task assignment and coordination. This paper explores how LLMs can effectively allocate computational tasks among multiple agents, considering factors such as cost, efficiency, and performance. In this work, we address key questions, including the effectiveness of LLMs as orchestrators and planners, comparing their effectiveness in task assignment and coordination. Our experiments demonstrate that LLMs can achieve high validity and accuracy in resource allocation tasks. We find that the planner method outperforms the orchestrator method in handling concurrent actions, resulting in improved efficiency and better utilization of agents. Additionally, we show that providing explicit information about worker capabilities enhances the allocation strategies of planners, particularly when dealing with suboptimal workers.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoboAct-CLIP: Video-Driven Pre-training of Atomic Action Understanding for Robotics</title>
<link>https://arxiv.org/abs/2504.02069</link>
<guid>https://arxiv.org/abs/2504.02069</guid>
<content:encoded><![CDATA[
<div> : Visual Language Models (VLMs), , , RoboAct-CLIP, (Contrastive Language-Image Pretraining, CLIP)

<br /><br />:
(VLMs)VLMsRoboAct-CLIPCLIPRoboAct-CLIP12% <div>
arXiv:2504.02069v1 Announce Type: new 
Abstract: Visual Language Models (VLMs) have emerged as pivotal tools for robotic systems, enabling cross-task generalization, dynamic environmental interaction, and long-horizon planning through multimodal perception and semantic reasoning. However, existing open-source VLMs predominantly trained for generic vision-language alignment tasks fail to model temporally correlated action semantics that are crucial for robotic manipulation effectively. While current image-based fine-tuning methods partially adapt VLMs to robotic applications, they fundamentally disregard temporal evolution patterns in video sequences and suffer from visual feature entanglement between robotic agents, manipulated objects, and environmental contexts, thereby limiting semantic decoupling capability for atomic actions and compromising model generalizability.To overcome these challenges, this work presents RoboAct-CLIP with dual technical contributions: 1) A dataset reconstruction framework that performs semantic-constrained action unit segmentation and re-annotation on open-source robotic videos, constructing purified training sets containing singular atomic actions (e.g., "grasp"); 2) A temporal-decoupling fine-tuning strategy based on Contrastive Language-Image Pretraining (CLIP) architecture, which disentangles temporal action features across video frames from object-centric characteristics to achieve hierarchical representation learning of robotic atomic actions.Experimental results in simulated environments demonstrate that the RoboAct-CLIP pretrained model achieves a 12% higher success rate than baseline VLMs, along with superior generalization in multi-object manipulation tasks.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Resource Allocation for Human-Autonomy Teaming under Coupled Constraints</title>
<link>https://arxiv.org/abs/2504.02088</link>
<guid>https://arxiv.org/abs/2504.02088</guid>
<content:encoded><![CDATA[
<div> 

:


<br /><br /> <div>
arXiv:2504.02088v1 Announce Type: new 
Abstract: This paper studies the optimal resource allocation problem within a multi-agent network composed of both autonomous agents and humans. The main challenge lies in the globally coupled constraints that link the decisions of autonomous agents with those of humans. To address this, we propose a reformulation that transforms these coupled constraints into decoupled local constraints defined over the system's communication graph. Building on this reformulation and incorporating a human response model that captures human-robot interactions while accounting for individual preferences and biases, we develop a fully distributed algorithm. This algorithm guides the states of the autonomous agents to equilibrium points which, when combined with the human responses, yield a globally optimal resource allocation. We provide both theoretical analysis and numerical simulations to validate the effectiveness of the proposed approach.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let's move on: Topic Change in Robot-Facilitated Group Discussions</title>
<link>https://arxiv.org/abs/2504.02123</link>
<guid>https://arxiv.org/abs/2504.02123</guid>
<content:encoded><![CDATA[
<div> : 

<br /><br />:
https://github.com/ghadj/topic-change-robot-discussions-data-2024 <div>
arXiv:2504.02123v1 Announce Type: new 
Abstract: Robot-moderated group discussions have the potential to facilitate engaging and productive interactions among human participants. Previous work on topic management in conversational agents has predominantly focused on human engagement and topic personalization, with the agent having an active role in the discussion. Also, studies have shown the usefulness of including robots in groups, yet further exploration is still needed for robots to learn when to change the topic while facilitating discussions. Accordingly, our work investigates the suitability of machine-learning models and audiovisual non-verbal features in predicting appropriate topic changes. We utilized interactions between a robot moderator and human participants, which we annotated and used for extracting acoustic and body language-related features. We provide a detailed analysis of the performance of machine learning approaches using sequential and non-sequential data with different sets of features. The results indicate promising performance in classifying inappropriate topic changes, outperforming rule-based approaches. Additionally, acoustic features exhibited comparable performance and robustness compared to the complete set of multimodal features. Our annotated data is publicly available at https://github.com/ghadj/topic-change-robot-discussions-data-2024.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Achieving Unanimous Consensus in Decision Making Using Multi-Agents</title>
<link>https://arxiv.org/abs/2504.02128</link>
<guid>https://arxiv.org/abs/2504.02128</guid>
<content:encoded><![CDATA[
<div> 

:
LLMsPoWPoS <div>
arXiv:2504.02128v1 Announce Type: new 
Abstract: Blockchain consensus mechanisms have relied on algorithms such as Proof-of-Work (PoW) and Proof-of-Stake (PoS) to ensure network functionality and integrity. However, these approaches struggle with adaptability for decision-making where the opinions of each matter rather than reaching an agreement based on honest majority or weighted consensus. This paper introduces a novel deliberation-based consensus mechanism where Large Language Models (LLMs) act as rational agents engaging in structured discussions to reach a unanimous consensus. By leveraging graded consensus and a multi-round deliberation process, our approach ensures both unanimous consensus for definitive problems and graded confidence for prioritized decisions and policies. We provide a formalization of our system and use it to show that the properties of blockchains: consistency, agreement, liveness, and determinism are maintained. Moreover, experimental results demonstrate our system's feasibility, showcasing how our deliberation method's convergence, block properties, and accuracy enable decision-making on blockchain networks. We also address key challenges with this novel approach such as degeneration of thoughts, hallucinations, malicious models and nodes, resource consumption, and scalability.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Simulation-Guided LLM-based Code Generation for Safe Autonomous Driving Software</title>
<link>https://arxiv.org/abs/2504.02141</link>
<guid>https://arxiv.org/abs/2504.02141</guid>
<content:encoded><![CDATA[
<div> Automated Driving System (ADS)DevOpsLarge Language Models (LLM)Automatic code generationSimulation model

:
LLMADSDevOpsLLMCodellamaDeepSeekCoderCodeGemmaMistralGPT4LLMACCCAEMOEM11 <div>
arXiv:2504.02141v1 Announce Type: new 
Abstract: Automated Driving System (ADS) is a safety-critical software system responsible for the interpretation of the vehicle's environment and making decisions accordingly. The unbounded complexity of the driving context, including unforeseeable events, necessitate continuous improvement, often achieved through iterative DevOps processes. However, DevOps processes are themselves complex, making these improvements both time- and resource-intensive. Automation in code generation for ADS using Large Language Models (LLM) is one potential approach to address this challenge. Nevertheless, the development of ADS requires rigorous processes to verify, validate, assess, and qualify the code before it can be deployed in the vehicle and used. In this study, we developed and evaluated a prototype for automatic code generation and assessment using a designed pipeline of a LLM-based agent, simulation model, and rule-based feedback generator in an industrial setup. The LLM-generated code is evaluated automatically in a simulation model against multiple critical traffic scenarios, and an assessment report is provided as feedback to the LLM for modification or bug fixing. We report about the experimental results of the prototype employing Codellama:34b, DeepSeek (r1:32b and Coder:33b), CodeGemma:7b, Mistral:7b, and GPT4 for Adaptive Cruise Control (ACC) and Unsupervised Collision Avoidance by Evasive Manoeuvre (CAEM). We finally assessed the tool with 11 experts at two Original Equipment Manufacturers (OEMs) by conducting an interview study.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Scaling in Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2504.02181</link>
<guid>https://arxiv.org/abs/2504.02181</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)

<br /><br />:
arXiv:2504.02181v1LLMsLLMsLLMsLLMsAI <div>
arXiv:2504.02181v1 Announce Type: new 
Abstract: The rapid advancements in large Language models (LLMs) have significantly enhanced their reasoning capabilities, driven by various strategies such as multi-agent collaboration. However, unlike the well-established performance improvements achieved through scaling data and model size, the scaling of reasoning in LLMs is more complex and can even negatively impact reasoning performance, introducing new challenges in model alignment and robustness. In this survey, we provide a comprehensive examination of scaling in LLM reasoning, categorizing it into multiple dimensions and analyzing how and to what extent different scaling strategies contribute to improving reasoning capabilities. We begin by exploring scaling in input size, which enables LLMs to process and utilize more extensive context for improved reasoning. Next, we analyze scaling in reasoning steps that improves multi-step inference and logical consistency. We then examine scaling in reasoning rounds, where iterative interactions refine reasoning outcomes. Furthermore, we discuss scaling in training-enabled reasoning, focusing on optimization through iterative model improvement. Finally, we review applications of scaling across domains and outline future directions for further advancing LLM reasoning. By synthesizing these diverse perspectives, this survey aims to provide insights into how scaling strategies fundamentally enhance the reasoning capabilities of LLMs and further guide the development of next-generation AI systems.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting World Models with Latent-State Dynamics Residuals</title>
<link>https://arxiv.org/abs/2504.02252</link>
<guid>https://arxiv.org/abs/2504.02252</guid>
<content:encoded><![CDATA[
<div> : , , , ReDRAW, 

:
ReDRAWReDRAWReDRAWRL rollout ReDRAWMuJoCo <div>
arXiv:2504.02252v1 Announce Type: new 
Abstract: Simulation-to-reality reinforcement learning (RL) faces the critical challenge of reconciling discrepancies between simulated and real-world dynamics, which can severely degrade agent performance. A promising approach involves learning corrections to simulator forward dynamics represented as a residual error function, however this operation is impractical with high-dimensional states such as images. To overcome this, we propose ReDRAW, a latent-state autoregressive world model pretrained in simulation and calibrated to target environments through residual corrections of latent-state dynamics rather than of explicit observed states. Using this adapted world model, ReDRAW enables RL agents to be optimized with imagined rollouts under corrected dynamics and then deployed in the real world. In multiple vision-based MuJoCo domains and a physical robot visual lane-following task, ReDRAW effectively models changes to dynamics and avoids overfitting in low data regimes where traditional transfer methods fail.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs as Deceptive Agents: How Role-Based Prompting Induces Semantic Ambiguity in Puzzle Tasks</title>
<link>https://arxiv.org/abs/2504.02254</link>
<guid>https://arxiv.org/abs/2504.02254</guid>
<content:encoded><![CDATA[
<div> :  (LLMs), , , , 

:
LLMsHateBERTLLMs <div>
arXiv:2504.02254v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) have not only showcased impressive creative capabilities but also revealed emerging agentic behaviors that exploit linguistic ambiguity in adversarial settings. In this study, we investigate how an LLM, acting as an autonomous agent, leverages semantic ambiguity to generate deceptive puzzles that mislead and challenge human users. Inspired by the popular puzzle game "Connections", we systematically compare puzzles produced through zero-shot prompting, role-injected adversarial prompts, and human-crafted examples, with an emphasis on understanding the underlying agent decision-making processes. Employing computational analyses with HateBERT to quantify semantic ambiguity, alongside subjective human evaluations, we demonstrate that explicit adversarial agent behaviors significantly heighten semantic ambiguity -- thereby increasing cognitive load and reducing fairness in puzzle solving. These findings provide critical insights into the emergent agentic qualities of LLMs and underscore important ethical considerations for evaluating and safely deploying autonomous language systems in both educational technologies and entertainment.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallel Market Environments for FinRL Contests</title>
<link>https://arxiv.org/abs/2504.02281</link>
<guid>https://arxiv.org/abs/2504.02281</guid>
<content:encoded><![CDATA[
<div> : FinRL

<br />
: FinRLContests 2023-2025FinRLGPULLM <div>
arXiv:2504.02281v1 Announce Type: new 
Abstract: Reinforcement learning has shown great potential in finance. We have organized the FinRL Contests 2023-2025 featuring different financial tasks. Large language models have a strong capability to process financial texts. Integrating LLM-generated signals into FinRL is a new task, enabling agents to use both structured market data and unstructured financial text. To address the sampling bottleneck during training, we introduce GPU-based parallel market environments to improve sampling speed. In this paper, we summarize the parallel market environments used in FinRL Contests 2023-2025. Two new environments incorporate LLM-generated signals and support massively parallel simulation. Contestants utilize these environments to train agents for stock and cryptocurrency trading tasks.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReuseDroid: A VLM-empowered Android UI Test Migrator Boosted by Active Feedback</title>
<link>https://arxiv.org/abs/2504.02357</link>
<guid>https://arxiv.org/abs/2504.02357</guid>
<content:encoded><![CDATA[
<div> : GUIREUSEDROID

:
REUSEDROIDVLMsGUIREUSEDROIDLinPro439578REUSEDROID90.3%LLM318.1%109.1% <div>
arXiv:2504.02357v1 Announce Type: new 
Abstract: GUI testing is an essential quality assurance process in mobile app development. However, the creation and maintenance of GUI tests for mobile apps are resource-intensive and costly. Recognizing that many apps share similar functionalities, researchers have proposed various techniques to migrate GUI tests from one app to another with similar features. For example, some techniques employ mapping-based approaches to align the GUI elements traversed by the tests of a source app to those present in the target app. Other test migration techniques have also been proposed to leverage large language models (LLMs) by adapting the GUI tasks in source tests. However, these techniques are ineffective in dealing with different operational logic between the source and target apps. The semantics of GUI elements may not be correctly inferred due to the missing analysis of these flows. In this work, we propose REUSEDROID, a novel multiagent framework for GUI test migration empowered by Large Vision-Language Models (VLMs). REUSEDROID is powered by multiple VLM-based agents, each tackling a stage of the test migration process by leveraging the relevant visual and textual information embedded in GUI pages. An insight of REUSEDROID is to migrate tests based only on the core logic shared across similar apps, while their entire operational logic could differ. We evaluate REUSEDROID on LinPro, a new test migration dataset that consists of 578 migration tasks for 39 popular apps across 4 categories. The experimental result shows that REUSEDROID can successfully migrate 90.3% of the migration tasks, outperforming the best mapping-based and LLM-based baselines by 318.1% and 109.1%, respectively.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Policy-Gradient Reinforcement Learning for Multi-Agent Shepherding Control of Non-Cohesive Targets</title>
<link>https://arxiv.org/abs/2504.02479</link>
<guid>https://arxiv.org/abs/2504.02479</guid>
<content:encoded><![CDATA[
<div> decentralized reinforcement learning, multi-agent shepherding, policy-gradient methods, Proximal Policy Optimization, Deep Q-Network

<br />
:
Proximal Policy OptimizationDeep Q-Network <div>
arXiv:2504.02479v1 Announce Type: new 
Abstract: We propose a decentralized reinforcement learning solution for multi-agent shepherding of non-cohesive targets using policy-gradient methods. Our architecture integrates target-selection with target-driving through Proximal Policy Optimization, overcoming discrete-action constraints of previous Deep Q-Network approaches and enabling smoother agent trajectories. This model-free framework effectively solves the shepherding problem without prior dynamics knowledge. Experiments demonstrate our method's effectiveness and scalability with increased target numbers and limited sensing capabilities.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Self-Learning Agent with a Progressive Neural Network Integrated Transformer</title>
<link>https://arxiv.org/abs/2504.02489</link>
<guid>https://arxiv.org/abs/2504.02489</guid>
<content:encoded><![CDATA[
<div> LLaMA 3.2(PNN)Meta-LoRA(EWC)(AGI)

<br /><br />:

LLaMA 3.2PNNAIMeta-LoRAEWCAGI <div>
arXiv:2504.02489v1 Announce Type: new 
Abstract: This paper introduces a self-learning agent that integrates LLaMA 3.2 with a Progressive Neural Network (PNN) for continual learning in conversational AI and code generation. The framework dynamically collects data, fine-tunes tasks with minimal samples, and leverages Meta-Learning for rapid adaptation. LoRA optimizes fine-tuning, while Elastic Weight Consolidation (EWC) enhances knowledge retention. Experimental results demonstrate improved adaptability and memory stability, positioning this approach as a scalable step toward Artificial General Intelligence (AGI).
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Memory-Augmented LLM-Driven Method for Autonomous Merging of 3D Printing Work Orders</title>
<link>https://arxiv.org/abs/2504.02509</link>
<guid>https://arxiv.org/abs/2504.02509</guid>
<content:encoded><![CDATA[
<div> 3DLLM

<br /><br />:

LLM3DLLM-LLMs <div>
arXiv:2504.02509v1 Announce Type: new 
Abstract: With the rapid development of 3D printing, the demand for personalized and customized production on the manufacturing line is steadily increasing. Efficient merging of printing workpieces can significantly enhance the processing efficiency of the production line. Addressing the challenge, a Large Language Model (LLM)-driven method is established in this paper for the autonomous merging of 3D printing work orders, integrated with a memory-augmented learning strategy. In industrial scenarios, both device and order features are modeled into LLM-readable natural language prompt templates, and develop an order-device matching tool along with a merging interference checking module. By incorporating a self-memory learning strategy, an intelligent agent for autonomous order merging is constructed, resulting in improved accuracy and precision in order allocation. The proposed method effectively leverages the strengths of LLMs in industrial applications while reducing hallucination.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploration-Driven Generative Interactive Environments</title>
<link>https://arxiv.org/abs/2504.02515</link>
<guid>https://arxiv.org/abs/2504.02515</guid>
<content:encoded><![CDATA[
<div> : Genie, , , RetroAct, 

:
GenieAutoExplore Agent974RetroActGenieGenieReduxGenieRedux-GGitHub <div>
arXiv:2504.02515v1 Announce Type: new 
Abstract: Modern world models require costly and time-consuming collection of large video datasets with action demonstrations by people or by environment-specific agents. To simplify training, we focus on using many virtual environments for inexpensive, automatically collected interaction data. Genie, a recent multi-environment world model, demonstrates simulation abilities of many environments with shared behavior. Unfortunately, training their model requires expensive demonstrations. Therefore, we propose a training framework merely using a random agent in virtual environments. While the model trained in this manner exhibits good controls, it is limited by the random exploration possibilities. To address this limitation, we propose AutoExplore Agent - an exploration agent that entirely relies on the uncertainty of the world model, delivering diverse data from which it can learn the best. Our agent is fully independent of environment-specific rewards and thus adapts easily to new environments. With this approach, the pretrained multi-environment model can quickly adapt to new environments achieving video fidelity and controllability improvement. In order to obtain automatically large-scale interaction datasets for pretraining, we group environments with similar behavior and controls. To this end, we annotate the behavior and controls of 974 virtual environments - a dataset that we name RetroAct. For building our model, we first create an open implementation of Genie - GenieRedux and apply enhancements and adaptations in our version GenieRedux-G. Our code and data are available at https://github.com/insait-institute/GenieRedux.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving</title>
<link>https://arxiv.org/abs/2504.02605</link>
<guid>https://arxiv.org/abs/2504.02605</guid>
<content:encoded><![CDATA[
<div> Multi-SWE-bench

<br /><br />:
PythonMulti-SWE-benchJavaTypeScriptJavaScriptGoRustCC++1,63268Multi-SWE-RLRL4,723Multi-SWE-benchMulti-SWE-RLRLAGI <div>
arXiv:2504.02605v1 Announce Type: new 
Abstract: The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To address this, we introduce a multilingual issue-resolving benchmark, called Multi-SWE-bench, covering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a total of 1,632 high-quality instances, which were carefully annotated from 2,456 candidates by 68 expert annotators, ensuring that the benchmark can provide an accurate and reliable evaluation. Based on Multi-SWE-bench, we evaluate a series of state-of-the-art models using three representative methods (Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with key empirical insights. In addition, we launch a Multi-SWE-RL open-source community, aimed at building large-scale reinforcement learning (RL) training datasets for issue-resolving tasks. As an initial contribution, we release a set of 4,723 well-structured instances spanning seven programming languages, laying a solid foundation for RL research in this domain. More importantly, we open-source our entire data production pipeline, along with detailed tutorials, encouraging the open-source community to continuously contribute and expand the dataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL community as catalysts for advancing RL toward its full potential, bringing us one step closer to the dawn of AGI.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Mission Tool Bench: Assessing the Robustness of LLM based Agents through Related and Dynamic Missions</title>
<link>https://arxiv.org/abs/2504.02623</link>
<guid>https://arxiv.org/abs/2504.02623</guid>
<content:encoded><![CDATA[
<div> : , , , , 

:
Multi-Mission Tool Bench <div>
arXiv:2504.02623v1 Announce Type: new 
Abstract: Large language models (LLMs) demonstrate strong potential as agents for tool invocation due to their advanced comprehension and planning capabilities. Users increasingly rely on LLM-based agents to solve complex missions through iterative interactions. However, existing benchmarks predominantly access agents in single-mission scenarios, failing to capture real-world complexity. To bridge this gap, we propose the Multi-Mission Tool Bench. In the benchmark, each test case comprises multiple interrelated missions. This design requires agents to dynamically adapt to evolving demands. Moreover, the proposed benchmark explores all possible mission-switching patterns within a fixed mission number. Specifically, we propose a multi-agent data generation framework to construct the benchmark. We also propose a novel method to evaluate the accuracy and efficiency of agent decisions with dynamic decision trees. Experiments on diverse open-source and closed-source LLMs reveal critical factors influencing agent robustness and provide actionable insights to the tool invocation society.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlled Social Learning: Altruism vs. Bias</title>
<link>https://arxiv.org/abs/2504.02648</link>
<guid>https://arxiv.org/abs/2504.02648</guid>
<content:encoded><![CDATA[
<div> : 

:
<br /><br /> <div>
arXiv:2504.02648v1 Announce Type: new 
Abstract: We introduce a model of sequential social learning in which a planner may pay a cost to adjust the private signal precision of some agents. This framework presents a new optimization problem for social learning that sheds light on practical policy questions, such as how the socially optimal level of ad personalization changes according to current beliefs or how a biased planner might derail social learning. We then characterize the optimal policies of an altruistic planner who maximizes social welfare and a biased planner who seeks to induce a specific action. Even for a planner who has equivalent knowledge to an individual, cannot lie or cherry-pick information, and is fully observable, we demonstrate that it can dramatically influence social welfare in both positive and negative directions. An important area for future exploration is how one might prevent these latter outcomes to protect against the manipulation of social learning.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SymDQN: Symbolic Knowledge and Reasoning in Neural Network-based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.02654</link>
<guid>https://arxiv.org/abs/2504.02654</guid>
<content:encoded><![CDATA[
<div> : SymDQN

<br /><br />:
SymDQN Dueling Deep Q-NetworksDQNLogic Tensor NetworksLTNsSymDQN 5x5 DuelDQN SymDQN  <div>
arXiv:2504.02654v1 Announce Type: new 
Abstract: We propose a learning architecture that allows symbolic control and guidance in reinforcement learning with deep neural networks. We introduce SymDQN, a novel modular approach that augments the existing Dueling Deep Q-Networks (DuelDQN) architecture with modules based on the neuro-symbolic framework of Logic Tensor Networks (LTNs). The modules guide action policy learning and allow reinforcement learning agents to display behaviour consistent with reasoning about the environment. Our experiment is an ablation study performed on the modules. It is conducted in a reinforcement learning environment of a 5x5 grid navigated by an agent that encounters various shapes, each associated with a given reward. The underlying DuelDQN attempts to learn the optimal behaviour of the agent in this environment, while the modules facilitate shape recognition and reward prediction. We show that our architecture significantly improves learning, both in terms of performance and the precision of the agent. The modularity of SymDQN allows reflecting on the intricacies and complexities of combining neural and symbolic approaches in reinforcement learning.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Affordable AI Assistants with Knowledge Graph of Thoughts</title>
<link>https://arxiv.org/abs/2504.02670</link>
<guid>https://arxiv.org/abs/2504.02670</guid>
<content:encoded><![CDATA[
<div> Large Language Models (LLMs)Knowledge Graph of Thoughts (KGoT)AIGAIA

<br /><br />:
KGoTAILLMsAIKGoTLLMPythonGPT-4o miniHugging Face AgentKGoTGAIA29%36Qwen2.5-32BDeepseek-R1-70B36%37.5%KGoTAI <div>
arXiv:2504.02670v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are revolutionizing the development of AI assistants capable of performing diverse tasks across domains. However, current state-of-the-art LLM-driven agents face significant challenges, including high operational costs and limited success rates on complex benchmarks like GAIA. To address these issues, we propose the Knowledge Graph of Thoughts (KGoT), an innovative AI assistant architecture that integrates LLM reasoning with dynamically constructed knowledge graphs (KGs). KGoT extracts and structures task-relevant knowledge into a dynamic KG representation, iteratively enhanced through external tools such as math solvers, web crawlers, and Python scripts. Such structured representation of task-relevant knowledge enables low-cost models to solve complex tasks effectively. For example, KGoT achieves a 29% improvement in task success rates on the GAIA benchmark compared to Hugging Face Agents with GPT-4o mini, while reducing costs by over 36x compared to GPT-4o. Improvements for recent reasoning models are similar, e.g., 36% and 37.5% for Qwen2.5-32B and Deepseek-R1-70B, respectively. KGoT offers a scalable, affordable, and high-performing solution for AI assistants.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Set-Theoretic Robust Control Approach for Linear Quadratic Games with Unknown Counterparts</title>
<link>https://arxiv.org/abs/2504.02679</link>
<guid>https://arxiv.org/abs/2504.02679</guid>
<content:encoded><![CDATA[
<div> : 

:
-<br /><br /> <div>
arXiv:2504.02679v1 Announce Type: new 
Abstract: Ensuring robust decision-making in multi-agent systems is challenging when agents have distinct, possibly conflicting objectives and lack full knowledge of each other s strategies. This is apparent in safety-critical applications such as human-robot interaction and assisted driving, where uncertainty arises not only from unknown adversary strategies but also from external disturbances. To address this, the paper proposes a robust adaptive control approach based on linear quadratic differential games. Our method allows a controlled agent to iteratively refine its belief about the adversary strategy and disturbances using a set-membership approach, while simultaneously adapting its policy to guarantee robustness against the uncertain adversary policy and improve performance over time. We formally derive theoretical guarantees on the robustness of the proposed control scheme and its convergence to epsilon-Nash strategies. The effectiveness of our approach is demonstrated in a numerical simulation.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handover and SINR-Aware Path Optimization in 5G-UAV mmWave Communication using DRL</title>
<link>https://arxiv.org/abs/2504.02688</link>
<guid>https://arxiv.org/abs/2504.02688</guid>
<content:encoded><![CDATA[
<div> : UAV

:
(UAV)5Gactor-critic(AC-DRL)(SINR)AC-RLgNBgNBSINRWireless InSiteRLSINR<br /><br /> <div>
arXiv:2504.02688v1 Announce Type: new 
Abstract: Path planning and optimization for unmanned aerial vehicles (UAVs)-assisted next-generation wireless networks is critical for mobility management and ensuring UAV safety and ubiquitous connectivity, especially in dense urban environments with street canyons and tall buildings. Traditional statistical and model-based techniques have been successfully used for path optimization in communication networks. However, when dynamic channel propagation characteristics such as line-of-sight (LOS), interference, handover, and signal-to-interference and noise ratio (SINR) are included in path optimization, statistical and model-based path planning solutions become obsolete since they cannot adapt to the dynamic and time-varying wireless channels, especially in the mmWave bands. In this paper, we propose a novel model-free actor-critic deep reinforcement learning (AC-DRL) framework for path optimization in UAV-assisted 5G mmWave wireless networks, which combines four important aspects of UAV communication: \textit{flight time, handover, connectivity and SINR}. We train an AC-RL agent that enables a UAV connected to a gNB to determine the optimal path to a desired destination in the shortest possible time with minimal gNB handover, while maintaining connectivity and the highest possible SINR. We train our model with data from a powerful ray tracing tool called Wireless InSite, which uses 3D images of the propagation environment and provides data that closely resembles the real propagation environment. The simulation results show that our system has superior performance in tracking high SINR compared to other selected RL algorithms.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Responsible Development of Offensive AI</title>
<link>https://arxiv.org/abs/2504.02701</link>
<guid>https://arxiv.org/abs/2504.02701</guid>
<content:encoded><![CDATA[
<div> AI(SDGs)AIAI

:
(SDGs)AIAI <div>
arXiv:2504.02701v1 Announce Type: new 
Abstract: As AI advances, broader consensus is needed to determine research priorities. This endeavor discusses offensive AI and provides guidance by leveraging Sustainable Development Goals (SDGs) and interpretability techniques. The objective is to more effectively establish priorities that balance societal benefits against risks. The two forms of offensive AI evaluated in this study are vulnerability detection agents, which solve Capture- The-Flag challenges, and AI-powered malware.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequential Binary Hypothesis Testing with Competing Agents under Information Asymmetry</title>
<link>https://arxiv.org/abs/2504.02743</link>
<guid>https://arxiv.org/abs/2504.02743</guid>
<content:encoded><![CDATA[
<div> : 

:
(1)(2)(3)KL <div>
arXiv:2504.02743v1 Announce Type: new 
Abstract: This paper concerns sequential hypothesis testing in competitive multi-agent systems where agents exchange potentially manipulated information. Specifically, a two-agent scenario is studied where each agent aims to correctly infer the true state of nature while optimizing decision speed and accuracy. At each iteration, agents collect private observations, update their beliefs, and share (possibly corrupted) belief signals with their counterparts before deciding whether to stop and declare a state, or continue gathering more information. The analysis yields three main results: (1)~when agents share information strategically, the optimal signaling policy involves equal-probability randomization between truthful and inverted beliefs; (2)~agents maximize performance by relying solely on their own observations for belief updating while using received information only to anticipate their counterpart's stopping decision; and (3)~the agent reaching their confidence threshold first cause the other agent to achieve a higher conditional probability of error. Numerical simulations further demonstrate that agents with higher KL divergence in their conditional distributions gain competitive advantage. Furthermore, our results establish that information sharing -- despite strategic manipulation -- reduces overall system stopping time compared to non-interactive scenarios, which highlights the inherent value of communication even in this competitive setup.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets</title>
<link>https://arxiv.org/abs/2504.02792</link>
<guid>https://arxiv.org/abs/2504.02792</guid>
<content:encoded><![CDATA[
<div> : 

<br />
:
Unified World Models, UWMUWM UWM UWM  <div>
arXiv:2504.02792v1 Announce Type: new 
Abstract: Imitation learning has emerged as a promising approach towards building generalist robots. However, scaling imitation learning for large robot foundation models remains challenging due to its reliance on high-quality expert demonstrations. Meanwhile, large amounts of video data depicting a wide range of environments and diverse behaviors are readily available. This data provides a rich source of information about real-world dynamics and agent-environment interactions. Leveraging this data directly for imitation learning, however, has proven difficult due to the lack of action annotation required for most contemporary methods. In this work, we present Unified World Models (UWM), a framework that allows for leveraging both video and action data for policy learning. Specifically, a UWM integrates an action diffusion process and a video diffusion process within a unified transformer architecture, where independent diffusion timesteps govern each modality. We show that by simply controlling each diffusion timestep, UWM can flexibly represent a policy, a forward dynamics, an inverse dynamics, and a video generator. Through simulated and real-world experiments, we show that: (1) UWM enables effective pretraining on large-scale multitask robot datasets with both dynamics and action predictions, resulting in more generalizable and robust policies than imitation learning, (2) UWM naturally facilitates learning from action-free video data through independent control of modality-specific diffusion timesteps, further improving the performance of finetuned policies. Our results suggest that UWM offers a promising step toward harnessing large, heterogeneous datasets for scalable robot learning, and provides a simple unification between the often disparate paradigms of imitation learning and world modeling. Videos and code are available at https://weirdlabuw.github.io/uwm/.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoboCAS: A Benchmark for Robotic Manipulation in Complex Object Arrangement Scenarios</title>
<link>https://arxiv.org/abs/2407.06951</link>
<guid>https://arxiv.org/abs/2407.06951</guid>
<content:encoded><![CDATA[
<div> Foundation modelsRoboCAS

<br />
:
RoboCASbenchmarkRoboCASagent<a href="https://github.com/notFoundThisPerson/RoboCAS-v0">https://github.com/notFoundThisPerson/RoboCAS-v0</a>. <div>
arXiv:2407.06951v1 Announce Type: cross 
Abstract: Foundation models hold significant potential for enabling robots to perform long-horizon general manipulation tasks. However, the simplicity of tasks and the uniformity of environments in existing benchmarks restrict their effective deployment in complex scenarios. To address this limitation, this paper introduces the \textit{RoboCAS} benchmark, the first benchmark specifically designed for complex object arrangement scenarios in robotic manipulation. This benchmark employs flexible and concise scripted policies to efficiently collect a diverse array of demonstrations, showcasing scattered, orderly, and stacked object arrangements within a highly realistic physical simulation environment. It includes complex processes such as target retrieval, obstacle clearance, and robot manipulation, testing agents' abilities to perform long-horizon planning for spatial reasoning and predicting chain reactions under ambiguous instructions. Extensive experiments on multiple baseline models reveal their limitations in managing complex object arrangement scenarios, underscoring the urgent need for intelligent agents capable of performing long-horizon operations in practical deployments and providing valuable insights for future research directions. Project website: \url{https://github.com/notFoundThisPerson/RoboCAS-v0}.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Multi-agent Coordination over Cellular Sheaves</title>
<link>https://arxiv.org/abs/2504.02049</link>
<guid>https://arxiv.org/abs/2504.02049</guid>
<content:encoded><![CDATA[
<div> 

:<br />
 <div>
arXiv:2504.02049v1 Announce Type: cross 
Abstract: Techniques for coordination of multi-agent systems are vast and varied, often utilizing purpose-built solvers or controllers with tight coupling to the types of systems involved or the coordination goal. In this paper, we introduce a general unified framework for heterogeneous multi-agent coordination using the language of cellular sheaves and nonlinear sheaf Laplacians, which are generalizations of graphs and graph Laplacians. Specifically, we introduce the concept of a nonlinear homological program encompassing a choice of cellular sheaf on an undirected graph, nonlinear edge potential functions, and constrained convex node objectives. We use the alternating direction method of multipliers to derive a distributed optimization algorithm for solving these nonlinear homological programs. To demonstrate the wide applicability of this framework, we show how hybrid coordination goals including combinations of consensus, formation, and flocking can be formulated as nonlinear homological programs and provide numerical simulations showing the efficacy of our distributed solution algorithm.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Less Intelligent the Elements, the More Intelligent the Whole. Or, Possibly Not?</title>
<link>https://arxiv.org/abs/2012.12689</link>
<guid>https://arxiv.org/abs/2012.12689</guid>
<content:encoded><![CDATA[
<div> : Lotka-Volterra

<br /><br />:
 Lotka-Volterra  <div>
arXiv:2012.12689v4 Announce Type: replace 
Abstract: We approach the debate on how ``intelligent'' artificial agents should be, by endowing the preys and predators of the Lotka-Volterra model with behavioural algorithms characterized by different levels of sophistication. We find that by endowing both preys and predators with the capability of making predictions based on linear extrapolation a novel sort of dynamic equilibrium appears, where both species co-exist while both populations grow indefinitely. While we confirm that, in general, simple agents favour the emergence of complex collective behaviour, we also suggest that the capability of individuals to take first-order derivatives of one other's behaviour may allow the collective computation of derivatives of any order.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Spell: A Dynamic Logic Analysis of Misdirection</title>
<link>https://arxiv.org/abs/2401.14516</link>
<guid>https://arxiv.org/abs/2401.14516</guid>
<content:encoded><![CDATA[
<div> 

:
 <div>
arXiv:2401.14516v3 Announce Type: replace 
Abstract: Misdirection can be defined as the intentional action of causing some misrepresentation in an agent, or in a group of agents. Such misrepresentations may result from verbal actions, as in linguistic deception, or from visual actions, as in visual misdirection. Examples of visual misdirection abound (e.g. in nature, in the military), with magic tricks providing a vivid illustration. So far, various types of verbal misdirection have been investigated from a formal perspective (e.g. lying, bluffing) but little attention has been paid to the particular case of visual misdirection. In this paper, we introduce a dynamic epistemic logic to represent not only verbal misdirection on agents' beliefs but also visual misdirection on agents' observations. We illustrate the dynamics of the logic by modelling a classic magic trick known as the French Drop. We also provide a sound and complete axiom system for the logic, and discuss the expressivity and scope of the setting.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLM Agents Have Regret? A Case Study in Online Learning and Games</title>
<link>https://arxiv.org/abs/2403.16843</link>
<guid>https://arxiv.org/abs/2403.16843</guid>
<content:encoded><![CDATA[
<div> : 

<br /><br />:
LLMsLLM stationary LLMLLMGPT-4LLM <div>
arXiv:2403.16843v4 Announce Type: replace 
Abstract: Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world LLM-agent applications. To better understand the limits of LLM agents in these interactive environments, we propose to study their interactions in benchmark decision-making settings in online learning and game theory, through the performance metric of \emph{regret}. We first empirically study the {no-regret} behaviors of LLMs in canonical (non-stationary) online learning problems, as well as the emergence of equilibria when LLM agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behaviors of LLM agents, under certain assumptions on the supervised pre-training and the rationality model of human decision-makers who generate the data. Notably, we also identify (simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To promote the no-regret behaviors, we propose a novel \emph{unsupervised} training loss of \emph{regret-loss}, which, in contrast to the supervised pre-training loss, does not require the labels of (optimal) actions. We then establish the statistical guarantee of generalization bound for regret-loss minimization, followed by the optimization guarantee that minimizing such a loss may automatically lead to known no-regret learning algorithms. Our further experiments demonstrate the effectiveness of our regret-loss, especially in addressing the above ``regrettable'' cases.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety-Aware Multi-Agent Learning for Dynamic Network Bridging</title>
<link>https://arxiv.org/abs/2404.01551</link>
<guid>https://arxiv.org/abs/2404.01551</guid>
<content:encoded><![CDATA[
<div> 

:
<br /><br /> <div>
arXiv:2404.01551v2 Announce Type: replace 
Abstract: Addressing complex cooperative tasks in safety-critical environments poses significant challenges for multi-agent systems, especially under conditions of partial observability. We focus on a dynamic network bridging task, where agents must learn to maintain a communication path between two moving targets. To ensure safety during training and deployment, we integrate a control-theoretic safety filter that enforces collision avoidance through local setpoint updates. We develop and evaluate multi-agent reinforcement learning safety-informed message passing, showing that encoding safety filter activations as edge-level features improves coordination. The results suggest that local safety enforcement and decentralized learning can be effectively combined in distributed multi-agent tasks.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Multi-agent Reinforcement Learning based Traffic Signal Control through Spatio-temporal Hypergraphs</title>
<link>https://arxiv.org/abs/2404.11014</link>
<guid>https://arxiv.org/abs/2404.11014</guid>
<content:encoded><![CDATA[
<div> : actor-ritic

<br /><br />:
actor-riticMA-SACMA-SAC <div>
arXiv:2404.11014v2 Announce Type: replace 
Abstract: Traffic signal control systems (TSCSs) are integral to intelligent traffic management, fostering efficient vehicle flow. Traditional approaches often simplify road networks into standard graphs, which results in a failure to consider the dynamic nature of traffic data at neighboring intersections, thereby neglecting higher-order interconnections necessary for real-time control. To address this, we propose a novel TSCS framework to realize intelligent traffic control. This framework collaborates with multiple neighboring edge computing servers to collect traffic information across the road network. To elevate the efficiency of traffic signal control, we have crafted a multi-agent soft actor-critic (MA-SAC) reinforcement learning algorithm. Within this algorithm, individual agents are deployed at each intersection with a mandate to optimize traffic flow across the road network collectively. Furthermore, we introduce hypergraph learning into the critic network of MA-SAC to enable the spatio-temporal interactions from multiple intersections in the road network. This method fuses hypergraph and spatio-temporal graph structures to encode traffic data and capture the complex spatio-temporal correlations between multiple intersections. Our empirical evaluation, tested on varied datasets, demonstrates the superiority of our framework in minimizing average vehicle travel times and sustaining high-throughput performance. This work facilitates the development of more intelligent urban traffic management solutions. We release the code to support the reproducibility of this work at https://github.com/Edun-Eyes/TSC
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Overcooked Generalisation Challenge</title>
<link>https://arxiv.org/abs/2406.17949</link>
<guid>https://arxiv.org/abs/2406.17949</guid>
<content:encoded><![CDATA[
<div> : Overcooked Generalisation Challenge (OGC), , _shot, (DCD), 

:
Overcooked Generalisation Challenge (OGC)_shotDCDOvercooked-AIDCDOGCGPUminimaxDCDOGCAI <div>
arXiv:2406.17949v2 Announce Type: replace 
Abstract: We introduce the Overcooked Generalisation Challenge (OGC) - the first benchmark to study agents' zero-shot cooperation abilities when faced with novel partners and levels in the Overcooked-AI environment. This perspective starkly contrasts a large body of previous work that has trained and evaluated cooperating agents only on the same level, failing to capture generalisation abilities required for real-world human-AI cooperation. Our challenge interfaces with state-of-the-art dual curriculum design (DCD) methods to generate auto-curricula for training general agents in Overcooked. It is the first cooperative multi-agent environment specially designed for DCD methods and, consequently, the first benchmarked with state-of-the-art methods. It is fully GPU-accelerated, built on the DCD benchmark suite minimax, and freely available under an open-source license: https://git.hcics.simtech.uni-stuttgart.de/public-projects/OGC. We show that current DCD algorithms struggle to produce useful policies in this novel challenge, even if combined with recent network architectures that were designed for scalability and generalisability. The OGC pushes the boundaries of real-world human-AI cooperation by enabling the research community to study the impact of generalisation on cooperating agents.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-based modeling for realistic reproduction of human mobility and contact behavior to evaluate test and isolation strategies in epidemic infectious disease spread</title>
<link>https://arxiv.org/abs/2410.08050</link>
<guid>https://arxiv.org/abs/2410.08050</guid>
<content:encoded><![CDATA[
<div> agent-basedCOVID-19

<br /><br />:
agent-based202131530SARS-CoV-2 <div>
arXiv:2410.08050v2 Announce Type: replace 
Abstract: Agent-based models have proven to be useful tools in supporting decision-making processes in different application domains. The advent of modern computers and supercomputers has enabled these bottom-up approaches to realistically model human mobility and contact behavior. The COVID-19 pandemic showcased the urgent need for detailed and informative models that can answer research questions on transmission dynamics. We present a sophisticated agent-based model to simulate the spread of respiratory diseases. The model is highly modularized and can be used on various scales, from a small collection of buildings up to cities or countries. Although not being the focus of this paper, the model has undergone performance engineering on a single core and provides an efficient intra- and inter-simulation parallelization for time-critical decision-making processes.
  In order to allow answering research questions on individual level resolution, nonpharmaceutical intervention strategies such as face masks or venue closures can be implemented for particular locations or agents. In particular, we allow for sophisticated testing and isolation strategies to study the effects of minimal-invasive infectious disease mitigation. With realistic human mobility patterns for the region of Brunswick, Germany, we study the effects of different interventions between March 1st and May 30, 2021 in the SARS-CoV-2 pandemic. Our analyses suggest that symptom-independent testing has limited impact on the mitigation of disease dynamics if the dark figure in symptomatic cases is high. Furthermore, we found that quarantine length is more important than quarantine efficiency but that, with sufficient symptomatic control, also short quarantines can have a substantial effect.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAD-TD: Model-Augmented Data stabilizes High Update Ratio RL</title>
<link>https://arxiv.org/abs/2410.08896</link>
<guid>https://arxiv.org/abs/2410.08896</guid>
<content:encoded><![CDATA[
<div> : MAD-TD

<br /><br />:
RLUTDModel-Augmented Data for TD Learning (MAD-TD)UTDDeepMindMAD-TD <div>
arXiv:2410.08896v2 Announce Type: replace 
Abstract: Building deep reinforcement learning (RL) agents that find a good policy with few samples has proven notoriously challenging. To achieve sample efficiency, recent work has explored updating neural networks with large numbers of gradient steps for every new sample. While such high update-to-data (UTD) ratios have shown strong empirical performance, they also introduce instability to the training process. Previous approaches need to rely on periodic neural network parameter resets to address this instability, but restarting the training process is infeasible in many real-world applications and requires tuning the resetting interval. In this paper, we focus on one of the core difficulties of stable training with limited samples: the inability of learned value functions to generalize to unobserved on-policy actions. We mitigate this issue directly by augmenting the off-policy RL training process with a small amount of data generated from a learned world model. Our method, Model-Augmented Data for TD Learning (MAD-TD), uses small amounts of generated data to stabilize high UTD training and achieve competitive performance on the most challenging tasks in the DeepMind control suite. Our experiments further highlight the importance of employing a good model to generate data, MAD-TD's ability to combat value overestimation, and its practical stability gains for continued learning.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroAI for AI Safety</title>
<link>https://arxiv.org/abs/2411.18526</link>
<guid>https://arxiv.org/abs/2411.18526</guid>
<content:encoded><![CDATA[
<div> : AI

:
AIAIAIAIAIAI <div>
arXiv:2411.18526v2 Announce Type: replace 
Abstract: As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating and Enhancing LLMs for Multi-turn Text-to-SQL with Multiple Question Types</title>
<link>https://arxiv.org/abs/2412.17867</link>
<guid>https://arxiv.org/abs/2412.17867</guid>
<content:encoded><![CDATA[
<div> : SQLMMSQL

:
LLMsSQLMMSQLLLMsSQLLLMLLMhttps://mcxiaoxiao.github.io/MMSQL <div>
arXiv:2412.17867v2 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) have significantly advanced text-to-SQL systems. However, most LLM-based methods often narrowly focus on SQL generation, neglecting the complexities of real-world conversational queries. This oversight can lead to unreliable responses, particularly for ambiguous questions that cannot be directly addressed with SQL. To bridge this gap, we propose MMSQL, a comprehensive test suite designed to evaluate the question classification and SQL generation capabilities of LLMs by simulating real-world scenarios with diverse question types and multi-turn Q\&amp;A interactions. Using MMSQL, we assessed the performance of popular LLMs, including both open-source and closed-source models, and identified key factors impacting their performance in such scenarios. Moreover, we introduce an LLM-based multi-agent framework that employs specialized agents to identify question types and determine appropriate answering strategies. Our experiments demonstrate that this approach significantly enhances the model's ability to navigate the complexities of conversational dynamics, effectively handling the diverse and complex nature of user queries. Our dataset and code are publicly available at https://mcxiaoxiao.github.io/MMSQL.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing the Role of the DSO in Electricity Trading of VPPs via a Stackelberg Game Model</title>
<link>https://arxiv.org/abs/2501.07715</link>
<guid>https://arxiv.org/abs/2501.07715</guid>
<content:encoded><![CDATA[
<div> : Stackelberg

<br /><br />:
(VPPs)VPPs(DSO)DSOStackelbergDSOVPPsKarush-Kuhn-TuckerDSOVPPs <div>
arXiv:2501.07715v2 Announce Type: replace 
Abstract: The increasing penetration of distributed energy resources has sparked interests in participating in power markets. Here, we consider two settings where Virtual Power Plants (VPPs) with some flexible resources participate in the electricity trading, either directly in the wholesale electricity market, or interfaced by the Distribution System Operator (DSO) who is the transaction organizer. In order to study the role of DSO as a stakeholder, a Stackelberg game is represented via a bi-level model: the DSO maximizes profits at the upper level, while the VPPs minimize operating costs at the lower level. To solve this problem, the Karush-Kuhn-Tucker conditions of lower level is deduced to achieve a single-level problem. The results show that the role of the DSO as an intermediary agent leads to a decrease in operating costs of the VPPs by organizing lower-level trading, while making a profit for itself. However, this seemingly win-win result comes at the cost of losing wholesale market interests, which implies that stakeholders need to abide by regulatory constraints in the electricity market.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Value of Information in Human-AI Decision-making</title>
<link>https://arxiv.org/abs/2502.06152</link>
<guid>https://arxiv.org/abs/2502.06152</guid>
<content:encoded><![CDATA[
<div> -AI

<br />
:
AIAI-AISHAP <div>
arXiv:2502.06152v2 Announce Type: replace 
Abstract: Multiple agents -- including humans and AI models -- are often paired on decision tasks with the expectation of achieving complementary performance, where the combined performance of both agents outperforms either one alone. However, knowing how to improve the performance of a human-AI team is often difficult without knowing more about what particular information and strategies each agent employs. We provide a decision-theoretic framework for characterizing the value of information -- and consequently, opportunities for agents to better exploit available information -- in AI-assisted decision workflows. We demonstrate the use of the framework for model selection, empirical evaluation of human-AI performance, and explanation design. We propose a novel information-based explanation technique that adapts SHAP, a saliency-based explanation, to explain information value in decision making.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring temporal effects of agent knowledge by date-controlled tool use</title>
<link>https://arxiv.org/abs/2503.04188</link>
<guid>https://arxiv.org/abs/2503.04188</guid>
<content:encoded><![CDATA[
<div> : Temporal progression, Knowledge accumulation, Web search, Large language model (LLM), Date-controlled tools (DCTs)

:
LLMDCTsLLMLLMchain-of-thought <div>
arXiv:2503.04188v2 Announce Type: replace 
Abstract: Temporal progression is an integral part of knowledge accumulation and update. Web search is frequently adopted as grounding for agent knowledge, yet an improper configuration affects the quality of the agent's responses. Here, we assess the agent behavior using distinct date-controlled tools (DCTs) as stress test to measure the knowledge variability of large language model (LLM) agents. We demonstrate the temporal effects of an LLM agent as a writing assistant, which uses web search to complete scientific publication abstracts. We show that the temporality of search engine translates into tool-dependent agent performance but can be alleviated with base model choice and explicit reasoning instructions such as chain-of-thought prompting. Our results indicate that agent design and evaluations should take a dynamical view and implement measures to account for the temporal influence of external resources to ensure reliability.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Person, One Bot</title>
<link>https://arxiv.org/abs/2504.01039</link>
<guid>https://arxiv.org/abs/2504.01039</guid>
<content:encoded><![CDATA[
<div> : (AI), , , , 

:
<br /><br /> <div>
arXiv:2504.01039v1 Announce Type: new 
Abstract: This short paper puts forward a vision for a new democratic model enabled by the recent technological advances in agentic AI. It therefore opens with drawing a clear and concise picture of the model, and only later addresses related proposals and research directions, and concerns regarding feasibility and safety. It ends with a note on the timeliness of this idea and on optimism. The model proposed is that of assigning each citizen an AI Agent that would serve as their political delegate, enabling the return to direct democracy. The paper examines this models relation to existing research, its potential setbacks and feasibility and argues for its further development.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MPCritic: A plug-and-play MPC architecture for reinforcement learning</title>
<link>https://arxiv.org/abs/2504.01086</link>
<guid>https://arxiv.org/abs/2504.01086</guid>
<content:encoded><![CDATA[
<div> RLMPCMPCritic

:
<br />
MPCriticMPCRLMPCMPCriticMPCMPCMPCMPCriticMPCMPCriticMPCRL <div>
arXiv:2504.01086v1 Announce Type: new 
Abstract: The reinforcement learning (RL) and model predictive control (MPC) communities have developed vast ecosystems of theoretical approaches and computational tools for solving optimal control problems. Given their conceptual similarities but differing strengths, there has been increasing interest in synergizing RL and MPC. However, existing approaches tend to be limited for various reasons, including computational cost of MPC in an RL algorithm and software hurdles towards seamless integration of MPC and RL tools. These challenges often result in the use of "simple" MPC schemes or RL algorithms, neglecting the state-of-the-art in both areas. This paper presents MPCritic, a machine learning-friendly architecture that interfaces seamlessly with MPC tools. MPCritic utilizes the loss landscape defined by a parameterized MPC problem, focusing on "soft" optimization over batched training steps; thereby updating the MPC parameters while avoiding costly minimization and parametric sensitivities. Since the MPC structure is preserved during training, an MPC agent can be readily used for online deployment, where robust constraint satisfaction is paramount. We demonstrate the versatility of MPCritic, in terms of MPC architectures and RL algorithms that it can accommodate, on classic control benchmarks.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HomeEmergency -- Using Audio to Find and Respond to Emergencies in the Home</title>
<link>https://arxiv.org/abs/2504.01089</link>
<guid>https://arxiv.org/abs/2504.01089</guid>
<content:encoded><![CDATA[
<div> 

:
<br />
ThreeDWorldP-DSGVLMs <div>
arXiv:2504.01089v1 Announce Type: new 
Abstract: In the United States alone accidental home deaths exceed 128,000 per year. Our work aims to enable home robots who respond to emergency scenarios in the home, preventing injuries and deaths. We introduce a new dataset of household emergencies based in the ThreeDWorld simulator. Each scenario in our dataset begins with an instantaneous or periodic sound which may or may not be an emergency. The agent must navigate the multi-room home scene using prior observations, alongside audio signals and images from the simulator, to determine if there is an emergency or not.
  In addition to our new dataset, we present a modular approach for localizing and identifying potential home emergencies. Underpinning our approach is a novel probabilistic dynamic scene graph (P-DSG), where our key insight is that graph nodes corresponding to agents can be represented with a probabilistic edge. This edge, when refined using Bayesian inference, enables efficient and effective localization of agents in the scene. We also utilize multi-modal vision-language models (VLMs) as a component in our approach, determining object traits (e.g. flammability) and identifying emergencies. We present a demonstration of our method completing a real-world version of our task on a consumer robot, showing the transferability of both our task and our method. Our dataset will be released to the public upon this papers publication.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Remember, but also, Forget: Bridging Myopic and Perfect Recall Fairness with Past-Discounting</title>
<link>https://arxiv.org/abs/2504.01154</link>
<guid>https://arxiv.org/abs/2504.01154</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:
 <div>
arXiv:2504.01154v1 Announce Type: new 
Abstract: Dynamic resource allocation in multi-agent settings often requires balancing efficiency with fairness over time--a challenge inadequately addressed by conventional, myopic fairness measures. Motivated by behavioral insights that human judgments of fairness evolve with temporal distance, we introduce a novel framework for temporal fairness that incorporates past-discounting mechanisms. By applying a tunable discount factor to historical utilities, our approach interpolates between instantaneous and perfect-recall fairness, thereby capturing both immediate outcomes and long-term equity considerations. Beyond aligning more closely with human perceptions of fairness, this past-discounting method ensures that the augmented state space remains bounded, significantly improving computational tractability in sequential decision-making settings. We detail the formulation of discounted-recall fairness in both additive and averaged utility contexts, illustrate its benefits through practical examples, and discuss its implications for designing balanced, scalable resource allocation strategies.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>First Field-Trial Demonstration of L4 Autonomous Optical Network for Distributed AI Training Communication: An LLM-Powered Multi-AI-Agent Solution</title>
<link>https://arxiv.org/abs/2504.01234</link>
<guid>https://arxiv.org/abs/2504.01234</guid>
<content:encoded><![CDATA[
<div> : 

:
<br />
AI98%3.2 <div>
arXiv:2504.01234v1 Announce Type: new 
Abstract: We demonstrate the first cross-domain cross-layer level-4 autonomous optical network via a multi-AI-agent system. Field trials show 98 percent task completion rate across the distributed AI training lifecycle-3.2x higher than single agents using state-of-the-art LLMs.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks</title>
<link>https://arxiv.org/abs/2504.01241</link>
<guid>https://arxiv.org/abs/2504.01241</guid>
<content:encoded><![CDATA[
<div> Large Language Models (LLMs)GLUEprompt

:
LLMsLLM100LLMspromptGLUENLUSST-2MRPCCoLAMNLIPhi-3.5-miniOrca-2-7bQwen2.5-7BLLMsprompt <div>
arXiv:2504.01241v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have significantly advanced Natural Language Processing (NLP), particularly in Natural Language Understanding (NLU) tasks. As we progress toward an agentic world where LLM-based agents autonomously handle specialized tasks, it becomes crucial for these models to adapt to new tasks without forgetting previously learned information - a challenge known as catastrophic forgetting. This study evaluates the continual fine-tuning of various open-source LLMs with different parameter sizes (specifically models under 10 billion parameters) on key NLU tasks from the GLUE benchmark, including SST-2, MRPC, CoLA, and MNLI. By employing prompt engineering and task-specific adjustments, we assess and compare the models' abilities to retain prior knowledge while learning new tasks. Our results indicate that models such as Phi-3.5-mini exhibit minimal forgetting while maintaining strong learning capabilities, making them well-suited for continual learning environments. Additionally, models like Orca-2-7b and Qwen2.5-7B demonstrate impressive learning abilities and overall performance after fine-tuning. This work contributes to understanding catastrophic forgetting in LLMs and highlights prompting engineering to optimize model performance for continual learning scenarios.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategize Globally, Adapt Locally: A Multi-Turn Red Teaming Agent with Dual-Level Learning</title>
<link>https://arxiv.org/abs/2504.01278</link>
<guid>https://arxiv.org/abs/2504.01278</guid>
<content:encoded><![CDATA[
<div> : 

:
\AlgNameJailbreakBench\AlgName5GPT-3.5-TurboLlama-3.1-70B90% <div>
arXiv:2504.01278v1 Announce Type: new 
Abstract: The exploitation of large language models (LLMs) for malicious purposes poses significant security risks as these models become more powerful and widespread. While most existing red-teaming frameworks focus on single-turn attacks, real-world adversaries typically operate in multi-turn scenarios, iteratively probing for vulnerabilities and adapting their prompts based on threat model responses. In this paper, we propose \AlgName, a novel multi-turn red-teaming agent that emulates sophisticated human attackers through complementary learning dimensions: global tactic-wise learning that accumulates knowledge over time and generalizes to new attack goals, and local prompt-wise learning that refines implementations for specific goals when initial attempts fail. Unlike previous multi-turn approaches that rely on fixed strategy sets, \AlgName enables the agent to identify new jailbreak tactics, develop a goal-based tactic selection framework, and refine prompt formulations for selected tactics. Empirical evaluations on JailbreakBench demonstrate our framework's superior performance, achieving over 90\% attack success rates against GPT-3.5-Turbo and Llama-3.1-70B within 5 conversation turns, outperforming state-of-the-art baselines. These results highlight the effectiveness of dynamic learning in identifying and exploiting model vulnerabilities in realistic multi-turn scenarios.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-scale Evaluation of Notebook Checkpointing with AI Agents</title>
<link>https://arxiv.org/abs/2504.01377</link>
<guid>https://arxiv.org/abs/2504.01377</guid>
<content:encoded><![CDATA[
<div> : arXivAI

:
checkpointingAI1,0002,848checkpointing <div>
arXiv:2504.01377v1 Announce Type: new 
Abstract: Saving, or checkpointing, intermediate results during interactive data exploration can potentially boost user productivity. However, existing studies on this topic are limited, as they primarily rely on small-scale experiments with human participants - a fundamental constraint of human subject studies. To address this limitation, we employ AI agents to simulate a large number of complex data exploration scenarios, including revisiting past states and branching into new exploration paths. This strategy enables us to accurately assess the impact of checkpointing while closely mimicking the behavior of real-world data practitioners. Our evaluation results, involving more than 1,000 exploration paths and 2,848 executed code blocks, show that a checkpointing framework for computational notebooks can indeed enhance productivity by minimizing unnecessary code re-executions and redundant variables or code.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Illusion of Progress? Assessing the Current State of Web Agents</title>
<link>https://arxiv.org/abs/2504.01382</link>
<guid>https://arxiv.org/abs/2504.01382</guid>
<content:encoded><![CDATA[
<div> : -Mind2Web

:<br />
Online-Mind2Web136300LML-as-a-Judge85% <div>
arXiv:2504.01382v1 Announce Type: new 
Abstract: As digitalization and cloud technologies evolve, the web is becoming increasingly important in the modern society. Autonomous web agents based on large language models (LLMs) hold a great potential in work automation. It is therefore important to accurately measure and monitor the progression of their capabilities. In this work, we conduct a comprehensive and rigorous assessment of the current state of web agents. Our results depict a very different picture of the competency of current agents, suggesting over-optimism in previously reported results. This gap can be attributed to shortcomings in existing benchmarks. We introduce Online-Mind2Web, an online evaluation benchmark consisting of 300 diverse and realistic tasks spanning 136 websites. It enables us to evaluate web agents under a setting that approximates how real users use these agents. To facilitate more scalable evaluation and development, we also develop a novel LLM-as-a-Judge automatic evaluation method and show that it can achieve around 85% agreement with human judgment, substantially higher than existing methods. Finally, we present the first comprehensive comparative analysis of current web agents, highlighting both their strengths and limitations to inspire future research.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Shadows to Safety: Occlusion Tracking and Risk Mitigation for Urban Autonomous Driving</title>
<link>https://arxiv.org/abs/2504.01408</link>
<guid>https://arxiv.org/abs/2504.01408</guid>
<content:encoded><![CDATA[
<div> (AVs)<br /><br />: GitHubhttps://github.com/TUM-AVS/OcclusionAwareMotionPlanning <div>
arXiv:2504.01408v1 Announce Type: new 
Abstract: Autonomous vehicles (AVs) must navigate dynamic urban environments where occlusions and perception limitations introduce significant uncertainties. This research builds upon and extends existing approaches in risk-aware motion planning and occlusion tracking to address these challenges. While prior studies have developed individual methods for occlusion tracking and risk assessment, a comprehensive method integrating these techniques has not been fully explored. We, therefore, enhance a phantom agent-centric model by incorporating sequential reasoning to track occluded areas and predict potential hazards. Our model enables realistic scenario representation and context-aware risk evaluation by modeling diverse phantom agents, each with distinct behavior profiles. Simulations demonstrate that the proposed approach improves situational awareness and balances proactive safety with efficient traffic flow. While these results underline the potential of our method, validation in real-world scenarios is necessary to confirm its feasibility and generalizability. By utilizing and advancing established methodologies, this work contributes to safer and more reliable AV planning in complex urban environments. To support further research, our method is available as open-source software at: https://github.com/TUM-AVS/OcclusionAwareMotionPlanning
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Incentive Strategies for Smart EV Charging Stations: An LLM-Driven User Digital Twin Approach</title>
<link>https://arxiv.org/abs/2504.01423</link>
<guid>https://arxiv.org/abs/2504.01423</guid>
<content:encoded><![CDATA[
<div> : , , , , 

<br /><br />:
V2G <div>
arXiv:2504.01423v1 Announce Type: new 
Abstract: This paper presents an enhanced electric vehicle demand response system based on large language models, aimed at optimizing the application of vehicle-to-grid technology. By leveraging an large language models-driven multi-agent framework to construct user digital twins integrated with multidimensional user profile features, it enables deep simulation and precise prediction of users' charging and discharging decision-making patterns. Additionally, a data- and knowledge-driven dynamic incentive mechanism is proposed, combining a distributed optimization model under network constraints to optimize the grid-user interaction while ensuring both economic viability and security. Simulation results demonstrate that the approach significantly improves load peak-valley regulation and charging/discharging strategies. Experimental validation highlights the system's substantial advantages in load balancing, user satisfaction and grid stability, providing decision-makers with a scalable V2G management tool that promotes the sustainable, synergistic development of vehicle-grid integration.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Split Federated Learning for UAV-Enabled Integrated Sensing, Computation, and Communication</title>
<link>https://arxiv.org/abs/2504.01443</link>
<guid>https://arxiv.org/abs/2504.01443</guid>
<content:encoded><![CDATA[
<div> Unmanned Aerial Vehicles (UAVs)Integrated Sensing, Computation, and Communication (ISCC)Federated Edge Learning (FEL)Split Federated Learning (SFL)SFLSCC

<br /><br />:
(SFL)(FEL)SFLSCCUAV-assisted FELSFLSCCSFLSCC <div>
arXiv:2504.01443v1 Announce Type: new 
Abstract: Unmanned aerial vehicles (UAVs) with integrated sensing, computation, and communication (ISCC) capabilities have become key enablers of next-generation wireless networks. Federated edge learning (FEL) leverages UAVs as mobile learning agents to collect data, perform local model updates, and contribute to global model aggregation. However, existing UAV-assisted FEL systems face critical challenges, including excessive computational demands, privacy risks, and inefficient communication, primarily due to the requirement for full-model training on resource-constrained UAVs. To deal with aforementioned challenges, we propose Split Federated Learning for UAV-Enabled ISCC (SFLSCC), a novel framework that integrates split federated learning (SFL) into UAV-assisted FEL. SFLSCC optimally partitions model training between UAVs and edge servers, significantly reducing UAVs' computational burden while preserving data privacy. We conduct a theoretical analysis of UAV deployment, split point selection, data sensing volume, and client-side aggregation frequency, deriving closed-form upper bounds for the convergence gap. Based on these insights, we conceive a joint optimization problem to minimize the energy consumption required to achieve a target model accuracy. Given the non-convex nature of the problem, we develop a low-complexity algorithm to efficiently determine UAV deployment, split point selection, and communication frequency. Extensive simulations on a target motion recognition task validate the effectiveness of SFLSCC, demonstrating superior convergence performance and energy efficiency compared to baseline methods.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoRAG: A Question-Answering Approach from a Geographical Perspective</title>
<link>https://arxiv.org/abs/2504.01458</link>
<guid>https://arxiv.org/abs/2504.01458</guid>
<content:encoded><![CDATA[
<div> : GeoQARAGGeoPrompt

<br /><br />:
GeoRAGQAGeoRAGpromptRAG3267BERT-Base-ChineseQAGeoPromptGeoRAGRAGGeoQA <div>
arXiv:2504.01458v1 Announce Type: new 
Abstract: Geographic Question Answering (GeoQA) addresses natural language queries in geographic domains to fulfill complex user demands and improve information retrieval efficiency. Traditional QA systems, however, suffer from limited comprehension, low retrieval accuracy, weak interactivity, and inadequate handling of complex tasks, hindering precise information acquisition. This study presents GeoRAG, a knowledge-enhanced QA framework integrating domain-specific fine-tuning and prompt engineering with Retrieval-Augmented Generation (RAG) technology to enhance geographic knowledge retrieval accuracy and user interaction. The methodology involves four components: (1) A structured geographic knowledge base constructed from 3267 corpora (research papers, monographs, and technical reports), categorized via a multi-agent approach into seven dimensions: semantic understanding, spatial location, geometric morphology, attribute characteristics, feature relationships, evolutionary processes, and operational mechanisms. This yielded 145234 classified entries and 875432 multi-dimensional QA pairs. (2) A multi-label text classifier based on BERT-Base-Chinese, trained to analyze query types through geographic dimension classification. (3) A retrieval evaluator leveraging QA pair data to assess query-document relevance, optimizing retrieval precision. (4) GeoPrompt templates engineered to dynamically integrate user queries with retrieved information, enhancing response quality through dimension-specific prompting. Comparative experiments demonstrate GeoRAG's superior performance over conventional RAG across multiple base models, validating its generalizability. This work advances geographic AI by proposing a novel paradigm for deploying large language models in domain-specific contexts, with implications for improving GeoQA systems scalability and accuracy in real-world applications.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Curriculum Learning for Goal-Based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.01459</link>
<guid>https://arxiv.org/abs/2504.01459</guid>
<content:encoded><![CDATA[
<div> : 

:
RLQGPUTPURL <div>
arXiv:2504.01459v1 Announce Type: new 
Abstract: Reinforcement learning (RL) -- algorithms that teach artificial agents to interact with environments by maximising reward signals -- has achieved significant success in recent years. These successes have been facilitated by advances in algorithms (e.g., deep Q-learning, deep deterministic policy gradients, proximal policy optimisation, trust region policy optimisation, and soft actor-critic) and specialised computational resources such as GPUs and TPUs. One promising research direction involves introducing goals to allow multimodal policies, commonly through hierarchical or curriculum reinforcement learning. These methods systematically decompose complex behaviours into simpler sub-tasks, analogous to how humans progressively learn skills (e.g. we learn to run before we walk, or we learn arithmetic before calculus). However, fully automating goal creation remains an open challenge. We present a novel probabilistic curriculum learning algorithm to suggest goals for reinforcement learning agents in continuous control and navigation tasks.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Autonomous Web Agents Good Testers?</title>
<link>https://arxiv.org/abs/2504.01495</link>
<guid>https://arxiv.org/abs/2504.01495</guid>
<content:encoded><![CDATA[
<div> SeeAct-ATApinATA

<br /><br />:
AWAsATAsWeb113ATASeeAct-ATApinATASeeAct-ATAPinATA50%60%94%PinATA <div>
arXiv:2504.01495v1 Announce Type: new 
Abstract: Despite advances in automated testing, manual testing remains prevalent due to the high maintenance demands associated with test script fragility-scripts often break with minor changes in application structure. Recent developments in Large Language Models (LLMs) offer a potential alternative by powering Autonomous Web Agents (AWAs) that can autonomously interact with applications. These agents may serve as Autonomous Test Agents (ATAs), potentially reducing the need for maintenance-heavy automated scripts by utilising natural language instructions similar to those used by human testers. This paper investigates the feasibility of adapting AWAs for natural language test case execution and how to evaluate them.  We contribute with (1) a benchmark of three offline web applications, and a suite of 113 manual test cases, split between passing and failing cases, to evaluate and compare ATAs performance, (2) SeeAct-ATA and pinATA, two open-source ATA implementations capable of executing test steps, verifying assertions and giving verdicts, and (3) comparative experiments using our benchmark that quantifies our ATAs effectiveness. Finally we also proceed to a qualitative evaluation to identify the limitations of PinATA, our best performing implementation.  Our findings reveal that our simple implementation, SeeAct-ATA, does not perform well compared to our more advanced PinATA implementation when executing test cases (50% performance improvement). However, while PinATA obtains around 60% of correct verdict and up to a promising 94% specificity, we identify several limitations that need to be addressed to develop more resilient and reliable ATAs, paving the way for robust, low maintenance test automation.  CCS Concepts: $\bullet$ Software and its engineering $\rightarrow$ Software testing and debugging.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building Knowledge from Interactions: An LLM-Based Architecture for Adaptive Tutoring and Social Reasoning</title>
<link>https://arxiv.org/abs/2504.01588</link>
<guid>https://arxiv.org/abs/2504.01588</guid>
<content:encoded><![CDATA[
<div> : 

:
LLM<br /><br /> <div>
arXiv:2504.01588v1 Announce Type: new 
Abstract: Integrating robotics into everyday scenarios like tutoring or physical training requires robots capable of adaptive, socially engaging, and goal-oriented interactions. While Large Language Models show promise in human-like communication, their standalone use is hindered by memory constraints and contextual incoherence. This work presents a multimodal, cognitively inspired framework that enhances LLM-based autonomous decision-making in social and task-oriented Human-Robot Interaction. Specifically, we develop an LLM-based agent for a robot trainer, balancing social conversation with task guidance and goal-driven motivation. To further enhance autonomy and personalization, we introduce a memory system for selecting, storing and retrieving experiences, facilitating generalized reasoning based on knowledge built across different interactions. A preliminary HRI user study and offline experiments with a synthetic dataset validate our approach, demonstrating the system's ability to manage complex interactions, autonomously drive training tasks, and build and retrieve contextual memories, advancing socially intelligent robotics.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vers une mod\'elisation de la confiance dans le renseignement sur les menaces cyber</title>
<link>https://arxiv.org/abs/2504.01606</link>
<guid>https://arxiv.org/abs/2504.01606</guid>
<content:encoded><![CDATA[
<div> : Cyber threat intelligence (CTI), , , , 

:
(Cyber threat intelligence, CTI)CTI <div>
arXiv:2504.01606v1 Announce Type: new 
Abstract: Cyber threat intelligence (CTI) is essential for effective system defense. CTI is a collection of information about current or past threats to a computer system. This information is gathered by an agent through observation, or based on a set of sources. Building intelligence only makes sense if you have confidence in it. To achieve this, it is necessary to estimate the confidence in each piece of information gathered, taking into account the different dimensions that can make it up: reliability of the source, competence, plausibility of the information, credibility of the information, for example. The information gathered must then be combined with other information to consolidate an agent's knowledge. Recent advances have been made in the theory underlying the modeling of trust for decision-making based on uncertain information, notably by using multivalued logic. This approach makes it possible to deal with unknown values of trust-building parameters, or to easily integrate dimensions. In this article we present the problem of CTI and CTI information sharing, and the reasons that led us to use a logic-based solution for an initial implementation.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-mediated Dynamic Plan Generation with a Multi-Agent Approach</title>
<link>https://arxiv.org/abs/2504.01637</link>
<guid>https://arxiv.org/abs/2504.01637</guid>
<content:encoded><![CDATA[
<div> : GPT-4

<br /><br />:
GPT-4 <div>
arXiv:2504.01637v1 Announce Type: new 
Abstract: Planning methods with high adaptability to dynamic environments are crucial for the development of autonomous and versatile robots. We propose a method for leveraging a large language model (GPT-4o) to automatically generate networks capable of adapting to dynamic environments. The proposed method collects environmental "status," representing conditions and goals, and uses them to generate agents. These agents are interconnected on the basis of specific conditions, resulting in networks that combine flexibility and generality. We conducted evaluation experiments to compare the networks automatically generated with the proposed method with manually constructed ones, confirming the comprehensiveness of the proposed method's networks and their higher generality. This research marks a significant advancement toward the development of versatile planning methods applicable to robotics, autonomous vehicles, smart systems, and other complex environments.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning LLMs for User-Aware Multimodal Conversational Agents</title>
<link>https://arxiv.org/abs/2504.01700</link>
<guid>https://arxiv.org/abs/2504.01700</guid>
<content:encoded><![CDATA[
<div> USER-LLM R1

<br /><br />:
USER-LLM R1CoTVLMsRAGCoTElderlyTech-VQAROUGE-1 F123.2%ROUGE-20.6%ROUGE-L8% <div>
arXiv:2504.01700v1 Announce Type: new 
Abstract: Personalization in social robotics is critical for fostering effective human-robot interactions, yet systems often face the cold start problem, where initial user preferences or characteristics are unavailable. This paper proposes a novel framework called USER-LLM R1 for a user-aware conversational agent that addresses this challenge through dynamic user profiling and model initiation. Our approach integrates chain-of-thought (CoT) reasoning models to iteratively infer user preferences and vision-language models (VLMs) to initialize user profiles from multimodal inputs, enabling personalized interactions from the first encounter. Leveraging a Retrieval-Augmented Generation (RAG) architecture, the system dynamically refines user representations within an inherent CoT process, ensuring contextually relevant and adaptive responses. Evaluations on the ElderlyTech-VQA Bench demonstrate significant improvements in ROUGE-1 (+23.2%), ROUGE-2 (+0.6%), and ROUGE-L (+8%) F1 scores over state-of-the-art baselines, with ablation studies underscoring the impact of reasoning model size on performance. Human evaluations further validate the framework's efficacy, particularly for elderly users, where tailored responses enhance engagement and trust. Ethical considerations, including privacy preservation and bias mitigation, are rigorously discussed and addressed to ensure responsible deployment.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Non-Expert Demonstrations: Outcome-Driven Action Constraint for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.01719</link>
<guid>https://arxiv.org/abs/2504.01719</guid>
<content:encoded><![CDATA[
<div> offline reinforcement learning, non-expert data, distribution shift, Outcome-Driven Action Flexibility (ODAF), safety requirements

<br /><br />:
Outcome-Driven Action Flexibility (ODAF)ODAFMuJoCoODAF <div>
arXiv:2504.01719v1 Announce Type: new 
Abstract: We address the challenge of offline reinforcement learning using realistic data, specifically non-expert data collected through sub-optimal behavior policies. Under such circumstance, the learned policy must be safe enough to manage \textit{distribution shift} while maintaining sufficient flexibility to deal with non-expert (bad) demonstrations from offline data.To tackle this issue, we introduce a novel method called Outcome-Driven Action Flexibility (ODAF), which seeks to reduce reliance on the empirical action distribution of the behavior policy, hence reducing the negative impact of those bad demonstrations.To be specific, a new conservative reward mechanism is developed to deal with {\it distribution shift} by evaluating actions according to whether their outcomes meet safety requirements - remaining within the state support area, rather than solely depending on the actions' likelihood based on offline data.Besides theoretical justification, we provide empirical evidence on widely used MuJoCo and various maze benchmarks, demonstrating that our ODAF method, implemented using uncertainty quantification techniques, effectively tolerates unseen transitions for improved "trajectory stitching," while enhancing the agent's ability to learn from realistic non-expert data.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Budget-Feasible Contracts</title>
<link>https://arxiv.org/abs/2504.01773</link>
<guid>https://arxiv.org/abs/2504.01773</guid>
<content:encoded><![CDATA[
<div> : , , , , 

:
BESTBESTBESTBEST <div>
arXiv:2504.01773v1 Announce Type: new 
Abstract: The problem of computing near-optimal contracts in combinatorial settings has recently attracted significant interest in the computer science community. Previous work has provided a rich body of structural and algorithmic insights into this problem. However, most of these results rely on the assumption that the principal has an unlimited budget for incentivizing agents, an assumption that is often unrealistic in practice. This motivates the study of the optimal contract problem under budget constraints. We study multi-agent contracts with budget constraints under both binary and combinatorial actions. For binary actions, our contribution is threefold. First, we generalize all previously known approximation guarantees on the principal's revenue to budgeted settings. Second, through the lens of budget constraints, we uncover insightful connections between the standard objective of the principal's revenue and other objectives. We identify a broad class of objectives, which we term BEST objectives, including reward, social welfare, and revenue, and show that they are all equivalent (up to a constant factor), leading to approximation guarantees for all BEST objectives. Third, we introduce the price of frugality, which quantifies the loss due to budget constraints, and establish near-tight bounds on this measure, providing deeper insights into the tradeoffs between budgets and incentives. For combinatorial actions, we establish a strong negative result. Specifically, we show that in a budgeted setting with submodular rewards, no finite approximation is possible to any BEST objective. This stands in contrast to the unbudgeted setting with submodular rewards, where a polynomial-time constant-factor approximation is known for revenue. On the positive side, for gross substitutes rewards, we recover our binary-actions results, obtaining a constant-factor approximation for all BEST objectives.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PaperBench: Evaluating AI's Ability to Replicate AI Research</title>
<link>https://arxiv.org/abs/2504.01848</link>
<guid>https://arxiv.org/abs/2504.01848</guid>
<content:encoded><![CDATA[
<div> PaperBenchAI agentsstate-of-the-art AI researchbenchmarkLLM-based judge

<br />
:

PaperBenchAIAI20ICML 2024 SpotlightOral8,316ICMLLLMPaperBenchClaude 3.5 SonnetNew21.0%PaperBenchAIAI <div>
arXiv:2504.01848v1 Announce Type: new 
Abstract: We introduce PaperBench, a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research. Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments. For objective evaluation, we develop rubrics that hierarchically decompose each replication task into smaller sub-tasks with clear grading criteria. In total, PaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed with the author(s) of each ICML paper for accuracy and realism. To enable scalable evaluation, we also develop an LLM-based judge to automatically grade replication attempts against rubrics, and assess our judge's performance by creating a separate benchmark for judges. We evaluate several frontier models on PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet (New) with open-source scaffolding, achieves an average replication score of 21.0\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench, finding that models do not yet outperform the human baseline. We \href{https://github.com/openai/preparedness}{open-source our code} to facilitate future research in understanding the AI engineering capabilities of AI agents.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting Emergent Planning in Model-Free Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.01871</link>
<guid>https://arxiv.org/abs/2504.01871</guid>
<content:encoded><![CDATA[
<div> model-free reinforcement learning, planning, concept-based interpretability, DRC, Sokoban

:<br />
SokobanDRCGuez2019DRC(1) (2) (3) RLLLMs <div>
arXiv:2504.01871v1 Announce Type: new 
Abstract: We present the first mechanistic evidence that model-free reinforcement learning agents can learn to plan. This is achieved by applying a methodology based on concept-based interpretability to a model-free agent in Sokoban -- a commonly used benchmark for studying planning. Specifically, we demonstrate that DRC, a generic model-free agent introduced by Guez et al. (2019), uses learned concept representations to internally formulate plans that both predict the long-term effects of actions on the environment and influence action selection. Our methodology involves: (1) probing for planning-relevant concepts, (2) investigating plan formation within the agent's representations, and (3) verifying that discovered plans (in the agent's representations) have a causal effect on the agent's behavior through interventions. We also show that the emergence of these plans coincides with the emergence of a planning-like property: the ability to benefit from additional test-time compute. Finally, we perform a qualitative analysis of the planning algorithm learned by the agent and discover a strong resemblance to parallelized bidirectional search. Our findings advance understanding of the internal mechanisms underlying planning behavior in agents, which is important given the recent trend of emergent planning and reasoning capabilities in LLMs through RL
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing AI-Scientist Understanding: Making LLM Think Like a Physicist with Interpretable Reasoning</title>
<link>https://arxiv.org/abs/2504.01911</link>
<guid>https://arxiv.org/abs/2504.01911</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)AI-

:
LLMsAI-LLMAI <div>
arXiv:2504.01911v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are playing an expanding role in physics research by enhancing reasoning, symbolic manipulation, and numerical computation. However, ensuring the reliability and interpretability of their outputs remains a significant challenge. In our framework, we conceptualize the collaboration between AI and human scientists as a dynamic interplay among three modules: the reasoning module, the interpretation module, and the AI-scientist interaction module. Recognizing that effective physics reasoning demands rigorous logical consistency, quantitative precision, and deep integration with established theoretical models, we introduce the interpretation module to improve the understanding of AI-generated outputs, which is not previously explored in the literature. This module comprises multiple specialized agents, including summarizers, model builders, UI builders, and testers, which collaboratively structure LLM outputs within a physically grounded framework, by constructing a more interpretable science model. A case study demonstrates that our approach enhances transparency, facilitates validation, and strengthens AI-augmented reasoning in scientific discovery.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gen-C: Populating Virtual Worlds with Generative Crowds</title>
<link>https://arxiv.org/abs/2504.01924</link>
<guid>https://arxiv.org/abs/2504.01924</guid>
<content:encoded><![CDATA[
<div> Gen-C<br /><br />:
Gen-CGen-C <div>
arXiv:2504.01924v1 Announce Type: new 
Abstract: Over the past two decades, researchers have made significant advancements in simulating human crowds, yet these efforts largely focus on low-level tasks like collision avoidance and a narrow range of behaviors such as path following and flocking. However, creating compelling crowd scenes demands more than just functional movement-it requires capturing high-level interactions between agents, their environment, and each other over time. To address this issue, we introduce Gen-C, a generative model to automate the task of authoring high-level crowd behaviors. Gen-C bypasses the labor-intensive and challenging task of collecting and annotating real crowd video data by leveraging a large language model (LLM) to generate a limited set of crowd scenarios, which are subsequently expanded and generalized through simulations to construct time-expanded graphs that model the actions and interactions of virtual agents. Our method employs two Variational Graph Auto-Encoders guided by a condition prior network: one dedicated to learning a latent space for graph structures (agent interactions) and the other for node features (agent actions and navigation). This setup enables the flexible generation of dynamic crowd interactions. The trained model can be conditioned on natural language, empowering users to synthesize novel crowd behaviors from text descriptions. We demonstrate the effectiveness of our approach in two scenarios, a University Campus and a Train Station, showcasing its potential for populating diverse virtual environments with agents exhibiting varied and dynamic behaviors that reflect complex interactions and high-level decision-making patterns.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Review, Refine, Repeat: Understanding Iterative Decoding of AI Agents with Dynamic Evaluation and Selection</title>
<link>https://arxiv.org/abs/2504.01931</link>
<guid>https://arxiv.org/abs/2504.01931</guid>
<content:encoded><![CDATA[
<div> AIAgentIADBest-of-NBON

<br /><br />:
AgentIADAIIADSketch2CodeText2SQLWebshopIAD3-6%8-10%IADIADBON <div>
arXiv:2504.01931v1 Announce Type: new 
Abstract: While AI agents have shown remarkable performance at various tasks, they still struggle with complex multi-modal applications, structured generation and strategic planning. Improvements via standard fine-tuning is often impractical, as solving agentic tasks usually relies on black box API access without control over model parameters. Inference-time methods such as Best-of-N (BON) sampling offer a simple yet effective alternative to improve performance. However, BON lacks iterative feedback integration mechanism. Hence, we propose Iterative Agent Decoding (IAD) which combines iterative refinement with dynamic candidate evaluation and selection guided by a verifier. IAD differs in how feedback is designed and integrated, specifically optimized to extract maximal signal from reward scores. We conduct a detailed comparison of baselines across key metrics on Sketch2Code, Text2SQL, and Webshop where IAD consistently outperforms baselines, achieving 3--6% absolute gains on Sketch2Code and Text2SQL (with and without LLM judges) and 8--10% gains on Webshop across multiple metrics. To better understand the source of IAD's gains, we perform controlled experiments to disentangle the effect of adaptive feedback from stochastic sampling, and find that IAD's improvements are primarily driven by verifier-guided refinement, not merely sampling diversity. We also show that both IAD and BON exhibit inference-time scaling with increased compute when guided by an optimal verifier. Our analysis highlights the critical role of verifier quality in effective inference-time optimization and examines the impact of noisy and sparse rewards on scaling behavior. Together, these findings offer key insights into the trade-offs and principles of effective inference-time optimization.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Matching, Unanticipated Experiences, Divorce, Flirting, Rematching, Etc</title>
<link>https://arxiv.org/abs/2504.01280</link>
<guid>https://arxiv.org/abs/2504.01280</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:
 \(1 - \varepsilon\)  \(\varepsilon\) / <div>
arXiv:2504.01280v1 Announce Type: cross 
Abstract: We study dynamic decentralized two-sided matching in which players may encounter unanticipated experiences. As they become aware of these experiences, they may change their preferences over players on the other side of the market. Consequently, they may get ``divorced'' and rematch again with other agents, which may lead to further unanticipated experiences etc. A matching is stable if there is absence of pairwise common belief in blocking. Stable matchings can be destabilized by unanticipated experiences. Yet, we show that there exist self-confirming outcomes that are stable and do not lead to further unanticipated experiences. We introduce a natural decentralized matching process that, at each period assigns probability $1 - \varepsilon$ to the satisfaction of a mutual optimal blocking pair (if it exists) and picks any optimal blocking pair otherwise. The parameter $\varepsilon$ is interpreted as a friction of the matching market. We show that for any decentralized matching process, frictions are necessary for convergence to stability even without unawareness. Our process converges to self-confirming stable outcomes. Further, we allow for bilateral communication/flirting that changes the awareness and say that a matching is flirt-proof stable if there is absence of communication leading to pairwise common belief in blocking. We show that our natural decentralized matching process converges to flirt-proof self-confirming outcomes.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Randomized Zeroth-Order Hierarchical Framework for Heterogeneous Federated Learning</title>
<link>https://arxiv.org/abs/2504.01839</link>
<guid>https://arxiv.org/abs/2504.01839</guid>
<content:encoded><![CDATA[
<div> 

<br />
:

FLZO-HFL <div>
arXiv:2504.01839v1 Announce Type: cross 
Abstract: Heterogeneity in federated learning (FL) is a critical and challenging aspect that significantly impacts model performance and convergence. In this paper, we propose a novel framework by formulating heterogeneous FL as a hierarchical optimization problem. This new framework captures both local and global training process through a bilevel formulation and is capable of the following: (i) addressing client heterogeneity through a personalized learning framework; (ii) capturing pre-training process on server's side; (iii) updating global model through nonstandard aggregation; (iv) allowing for nonidentical local steps; and (v) capturing clients' local constraints. We design and analyze an implicit zeroth-order FL method (ZO-HFL), provided with nonasymptotic convergence guarantees for both the server-agent and the individual client-agents, and asymptotic guarantees for both the server-agent and client-agents in an almost sure sense. Notably, our method does not rely on standard assumptions in heterogeneous FL, such as the bounded gradient dissimilarity condition. We implement our method on image classification tasks and compare with other methods under different heterogeneous settings.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast-Converged Deep Reinforcement Learning for Optimal Dispatch of Large-Scale Power Systems under Transient Security Constraints</title>
<link>https://arxiv.org/abs/2304.08320</link>
<guid>https://arxiv.org/abs/2304.08320</guid>
<content:encoded><![CDATA[
<div> : TSC-OPF, DRL, , DDPG-CPEn, 

:
TSC-OPFDRLDRL-based TSC-OPFTSC-OPFMDPDDPG-CPEnIEEE 39-bus710-busGitHub<br /><br /> <div>
arXiv:2304.08320v5 Announce Type: replace 
Abstract: Power system optimal dispatch with transient security constraints is commonly represented as Transient Security-Constrained Optimal Power Flow (TSC-OPF). Deep Reinforcement Learning (DRL)-based TSC-OPF trains efficient decision-making agents that are adaptable to various scenarios and provide solution results quickly. However, due to the high dimensionality of the state space and action spaces, as well as the non-smoothness of dynamic constraints, existing DRL-based TSC-OPF solution methods face a significant challenge of the sparse reward problem. To address this issue, a fast-converged DRL method for TSC-OPF is proposed in this paper. The Markov Decision Process (MDP) modeling of TSC-OPF is improved by reducing the observation space and smoothing the reward design, thus facilitating agent training. An improved Deep Deterministic Policy Gradient algorithm with Curriculum learning, Parallel exploration, and Ensemble decision-making (DDPG-CPEn) is introduced to drastically enhance the efficiency of agent training and the accuracy of decision-making. The effectiveness, efficiency, and accuracy of the proposed method are demonstrated through experiments in the IEEE 39-bus system and a practical 710-bus regional power grid. The source code of the proposed method is made public on GitHub.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Actionable Counterfactual Explanations in Large State Spaces</title>
<link>https://arxiv.org/abs/2404.17034</link>
<guid>https://arxiv.org/abs/2404.17034</guid>
<content:encoded><![CDATA[
<div> :  CFE CFE CFE

<br /><br />:
 CFE CFE  ID CFE CFE  CFE  CFE CFE  CFE  CFE MDP BRFSSFoods  NHANES CFE CFE  <div>
arXiv:2404.17034v2 Announce Type: replace 
Abstract: Recourse generators provide actionable insights, often through feature-based counterfactual explanations (CFEs), to help negatively classified individuals understand how to adjust their input features to achieve a positive classification. These feature-based CFEs, which we refer to as \emph{low-level} CFEs, are overly specific (e.g., coding experience: $4 \to 5+$ years) and often recommended in feature space that doesn't straightforwardly align with real-world actions. To bridge this gap, we introduce three novel recourse types grounded in real-world actions: high-level continuous (\emph{hl-continuous}), high-level discrete (\emph{hl-discrete}), and high-level ID (\emph{hl-id}) CFEs.
  We formulate single-agent CFE generation methods, where we model the hl-discrete CFE as a solution to a weighted set cover problem and the hl-continuous CFE as a solution to an integer linear program. Since these methods require costly optimization per agent, we propose data-driven CFE generation approaches that, given instances of agents and their optimal CFEs, learn a CFE generator that quickly provides optimal CFEs for new agents. This approach, also viewed as one of learning an optimal policy in a family of large but deterministic MDPs, considers several problem formulations, including formulations in which the actions and their effects are unknown, and therefore addresses informational and computational challenges.
  Through extensive empirical evaluation using publicly available healthcare datasets (BRFSS, Foods, and NHANES), we compare the proposed forms of recourse to low-level CFEs and assess the effectiveness of our data-driven approaches. Empirical results show that the proposed data-driven CFE generators are accurate and resource-efficient, and the proposed forms of recourse have various advantages over the low-level CFEs.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Amelia: A Large Dataset and Model for Airport Surface Movement Forecasting</title>
<link>https://arxiv.org/abs/2407.21185</link>
<guid>https://arxiv.org/abs/2407.21185</guid>
<content:encoded><![CDATA[
<div> AmeliaAmelia-48AmeliaTF<br /><br />:<br />
AmeliaAmelia-48FAASystem Wide Information Management (SWIM)4870TBTransformerAgentAmeliaTFAmelia-1010292AmeliaTFhttps://ameliacmu.github.io <div>
arXiv:2407.21185v2 Announce Type: replace 
Abstract: The growing demand for air travel necessitates advancements in air traffic management technologies to ensure safe and efficient operations. Predictive models for terminal airspace can help anticipate future movements and traffic flows, enabling proactive planning for efficient coordination, collision risk assessment, taxi-out time prediction, departure metering, and emission estimations. Although data-driven predictive models have shown promise in tackling some of these challenges, the absence of large-scale curated surface movement datasets in the public domain has hindered the development of scalable and generalizable approaches.
  In this context, we propose the Amelia framework, which consists of four key contributions. First, Amelia-48, a large dataset of airport surface movement collected through the FAA's System Wide Information Management (SWIM) Program. This dataset includes over two years' worth of trajectory data (~70TB) across 48 US airports and map data. Second, we develop AmeliaTF, a large transformer-based baseline for multi-agent, multi-airport trajectory forecasting. Third, we propose Amelia-10, a training and evaluation benchmark consisting of 292 days of post-processed data from 10 different airports and a series of experiments to promote the development of foundation models in aviation. We provide baseline results across our benchmark using AmeliaTF. Finally, we release our framework and tools to encourage further aviation research in the forecasting domain and beyond at https://ameliacmu.github.io
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Underwater Camouflaged Object Tracking Meets Vision-Language SAM2</title>
<link>https://arxiv.org/abs/2409.16902</link>
<guid>https://arxiv.org/abs/2409.16902</guid>
<content:encoded><![CDATA[
<div> : UW-COT220SAM2VL-SAM2

:
UW-COT220SAMSAM2SAM2SAMVL-SAM2SAM2VL-SAM2UW-COT220 <div>
arXiv:2409.16902v3 Announce Type: replace 
Abstract: Over the past decade, significant progress has been made in visual object tracking, largely due to the availability of large-scale datasets. However, these datasets have primarily focused on open-air scenarios and have largely overlooked underwater animal tracking-especially the complex challenges posed by camouflaged marine animals. To bridge this gap, we take a step forward by proposing the first large-scale multi-modal underwater camouflaged object tracking dataset, namely UW-COT220. Based on the proposed dataset, this work first comprehensively evaluates current advanced visual object tracking methods, including SAM- and SAM2-based trackers, in challenging underwater environments, \eg, coral reefs. Our findings highlight the improvements of SAM2 over SAM, demonstrating its enhanced ability to handle the complexities of underwater camouflaged objects. Furthermore, we propose a novel vision-language tracking framework called VL-SAM2, based on the video foundation model SAM2. Experimental results demonstrate that our VL-SAM2 achieves state-of-the-art performance on the UW-COT220 dataset. The dataset and codes are available at~\href{https://github.com/983632847/Awesome-Multimodal-Object-Tracking}{\color{magenta}{here}}.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DocETL: Agentic Query Rewriting and Evaluation for Complex Document Processing</title>
<link>https://arxiv.org/abs/2410.12189</link>
<guid>https://arxiv.org/abs/2410.12189</guid>
<content:encoded><![CDATA[
<div> Large Language Models (LLMs)DocETL

:
DocETLLLMsDocETL(1) LLM(2) (3) DocETL25%80%20253DocETLGitHub1.7kdocetl.org<br /><br /> <div>
arXiv:2410.12189v3 Announce Type: replace 
Abstract: Analyzing unstructured data has been a persistent challenge in data processing. Large Language Models (LLMs) have shown promise in this regard, leading to recent proposals for declarative frameworks for LLM-powered processing of unstructured data. However, these frameworks focus on reducing cost when executing user-specified operations using LLMs, rather than improving accuracy, executing most operations as-is (in a single LLM call). This is problematic for complex tasks and data, where LLM outputs for user-defined operations are often inaccurate, even with optimized prompts. For example, an LLM may struggle to identify {\em all} instances of specific clauses, like force majeure or indemnification, in lengthy legal documents, requiring decomposition of the data, the task, or both.
  We present DocETL, a system that optimizes complex document processing pipelines, while accounting for LLM shortcomings. DocETL offers a declarative interface for users to define such pipelines and uses an agent-based approach to automatically optimize them, leveraging novel agent-based rewrites (that we call rewrite directives), as well as an optimization and evaluation framework. We introduce (i) logical rewriting of pipelines, tailored for LLM-based tasks, (ii) an agent-guided plan evaluation mechanism that synthesizes and orchestrates task-specific validation prompts, and (iii) an optimization algorithm that efficiently finds promising plans, considering the latencies of agent-based plan generation and evaluation. Our evaluation on four different unstructured document analysis tasks demonstrates that DocETL finds plans with outputs that are 25 to 80% more accurate than well-engineered baselines, addressing a critical gap in unstructured data analysis. DocETL is open-source at docetl.org, and as of March 2025, has amassed over 1.7k GitHub Stars, with users spanning a variety of domains.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentForge: A Flexible Low-Code Platform for Reinforcement Learning Agent Design</title>
<link>https://arxiv.org/abs/2410.19528</link>
<guid>https://arxiv.org/abs/2410.19528</guid>
<content:encoded><![CDATA[
<div> :  (Reinforcement Learning), , , AgentForge, 

<br /><br />:
AgentForgeRLVizierOptunaRLAgentForgeAgentForgeRLRL <div>
arXiv:2410.19528v4 Announce Type: replace 
Abstract: Developing a reinforcement learning (RL) agent often involves identifying values for numerous parameters, covering the policy, reward function, environment, and agent-internal architecture. Since these parameters are interrelated in complex ways, optimizing them is a black-box problem that proves especially challenging for nonexperts. Although existing optimization-as-a-service platforms (e.g., Vizier and Optuna) can handle such problems, they are impractical for RL systems, since the need for manual user mapping of each parameter to distinct components makes the effort cumbersome. It also requires understanding of the optimization process, limiting the systems' application beyond the machine learning field and restricting access in areas such as cognitive science, which models human decision-making. To tackle these challenges, the paper presents AgentForge, a flexible low-code platform to optimize any parameter set across an RL system. Available at https://github.com/feferna/AgentForge, it allows an optimization problem to be defined in a few lines of code and handed to any of the interfaced optimizers. With AgentForge, the user can optimize the parameters either individually or jointly. The paper presents an evaluation of its performance for a challenging vision-based RL problem.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transfer Your Perspective: Controllable 3D Generation from Any Viewpoint in a Driving Scene</title>
<link>https://arxiv.org/abs/2502.06682</link>
<guid>https://arxiv.org/abs/2502.06682</guid>
<content:encoded><![CDATA[
<div> Transfer Your Perspective (TYP)

:
<br />
CAVego-carCAVTransfer Your Perspective (TYP)ego-carego-carTYPCAVCAV <div>
arXiv:2502.06682v2 Announce Type: replace 
Abstract: Self-driving cars relying solely on ego-centric perception face limitations in sensing, often failing to detect occluded, faraway objects. Collaborative autonomous driving (CAV) seems like a promising direction, but collecting data for development is non-trivial. It requires placing multiple sensor-equipped agents in a real-world driving scene, simultaneously! As such, existing datasets are limited in locations and agents. We introduce a novel surrogate to the rescue, which is to generate realistic perception from different viewpoints in a driving scene, conditioned on a real-world sample - the ego-car's sensory data. This surrogate has huge potential: it could potentially turn any ego-car dataset into a collaborative driving one to scale up the development of CAV. We present the very first solution, using a combination of simulated collaborative data and real ego-car data. Our method, Transfer Your Perspective (TYP), learns a conditioned diffusion model whose output samples are not only realistic but also consistent in both semantics and layouts with the given ego-car data. Empirical results demonstrate TYP's effectiveness in aiding in a CAV setting. In particular, TYP enables us to (pre-)train collaborative perception algorithms like early and late fusion with little or no real-world collaborative data, greatly facilitating downstream CAV applications.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeraSim: Uncovering Unknown Unsafe Events for Autonomous Vehicles through Generative Simulation</title>
<link>https://arxiv.org/abs/2503.03629</link>
<guid>https://arxiv.org/abs/2503.03629</guid>
<content:encoded><![CDATA[
<div> : TeraSim(AV)

:<br />
TeraSimTeraSimAVAVTeraSimAVTeraSimAVhttps://github.com/mcity/TeraSim <div>
arXiv:2503.03629v4 Announce Type: replace 
Abstract: Traffic simulation is essential for autonomous vehicle (AV) development, enabling comprehensive safety evaluation across diverse driving conditions. However, traditional rule-based simulators struggle to capture complex human interactions, while data-driven approaches often fail to maintain long-term behavioral realism or generate diverse safety-critical events. To address these challenges, we propose TeraSim, an open-source, high-fidelity traffic simulation platform designed to uncover unknown unsafe events and efficiently estimate AV statistical performance metrics, such as crash rates. TeraSim is designed for seamless integration with third-party physics simulators and standalone AV stacks, to construct a complete AV simulation system. Experimental results demonstrate its effectiveness in generating diverse safety-critical events involving both static and dynamic agents, identifying hidden deficiencies in AV systems, and enabling statistical performance evaluation. These findings highlight TeraSim's potential as a practical tool for AV safety assessment, benefiting researchers, developers, and policymakers. The code is available at https://github.com/mcity/TeraSim.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automate Strategy Finding with LLM in Quant Investment</title>
<link>https://arxiv.org/abs/2409.06289</link>
<guid>https://arxiv.org/abs/2409.06289</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)alpha

<br />
:
LLMsLLMsalphaalphaLLMalpha <div>
arXiv:2409.06289v2 Announce Type: replace-cross 
Abstract: Despite significant progress in deep learning for financial trading, existing models often face instability and high uncertainty, hindering their practical application. Leveraging advancements in Large Language Models (LLMs) and multi-agent architectures, we propose a novel framework for quantitative stock investment in portfolio management and alpha mining. Our framework addresses these issues by integrating LLMs to generate diversified alphas and employing a multi-agent approach to dynamically evaluate market conditions. This paper proposes a framework where large language models (LLMs) mine alpha factors from multimodal financial data, ensuring a comprehensive understanding of market dynamics. The first module extracts predictive signals by integrating numerical data, research papers, and visual charts. The second module uses ensemble learning to construct a diverse pool of trading agents with varying risk preferences, enhancing strategy performance through a broader market analysis. In the third module, a dynamic weight-gating mechanism selects and assigns weights to the most relevant agents based on real-time market conditions, enabling the creation of an adaptive and context-aware composite alpha formula. Extensive experiments on the Chinese stock markets demonstrate that this framework significantly outperforms state-of-the-art baselines across multiple financial metrics. The results underscore the efficacy of combining LLM-generated alphas with a multi-agent architecture to achieve superior trading performance and stability. This work highlights the potential of AI-driven approaches in enhancing quantitative investment strategies and sets a new benchmark for integrating advanced machine learning techniques in financial trading can also be applied on diverse markets.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LayerCraft: Enhancing Text-to-Image Generation with CoT Reasoning and Layered Object Integration</title>
<link>https://arxiv.org/abs/2504.00010</link>
<guid>https://arxiv.org/abs/2504.00010</guid>
<content:encoded><![CDATA[
<div> Text-to-image generation (T2I)LayerCraft (LLMs)ChainArchitectObject-Integration Network (OIN)

:
LayerCraftT2ILayerCraftLLMsChainArchitectObject-Integration Network (OIN)ChainArchitectCoT3DOINLoRAT2ILayerCraftT2ILayerCraftgithub.com/PeterYYZhang/LayerCraft <div>
arXiv:2504.00010v1 Announce Type: new 
Abstract: Text-to-image generation (T2I) has become a key area of research with broad applications. However, existing methods often struggle with complex spatial relationships and fine-grained control over multiple concepts. Many existing approaches require significant architectural modifications, extensive training, or expert-level prompt engineering. To address these challenges, we introduce \textbf{LayerCraft}, an automated framework that leverages large language models (LLMs) as autonomous agents for structured procedural generation. LayerCraft enables users to customize objects within an image and supports narrative-driven creation with minimal effort. At its core, the system includes a coordinator agent that directs the process, along with two specialized agents: \textbf{ChainArchitect}, which employs chain-of-thought (CoT) reasoning to generate a dependency-aware 3D layout for precise instance-level control, and the \textbf{Object-Integration Network (OIN)}, which utilizes LoRA fine-tuning on pre-trained T2I models to seamlessly blend objects into specified regions of an image based on textual prompts without requiring architectural changes. Extensive evaluations demonstrate LayerCraft's versatility in applications ranging from multi-concept customization to storytelling. By providing non-experts with intuitive, precise control over T2I generation, our framework democratizes creative image creation. Our code will be released upon acceptance at github.com/PeterYYZhang/LayerCraft
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nuclear Microreactor Control with Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.00156</link>
<guid>https://arxiv.org/abs/2504.00156</guid>
<content:encoded><![CDATA[
<div> : RLPID<br /><br />:<br />
RLRLPIDRLagent RLagent RLPIDRLPIDRL1%MARLagent RLRLPID <div>
arXiv:2504.00156v1 Announce Type: new 
Abstract: The economic feasibility of nuclear microreactors will depend on minimizing operating costs through advancements in autonomous control, especially when these microreactors are operating alongside other types of energy systems (e.g., renewable energy). This study explores the application of deep reinforcement learning (RL) for real-time drum control in microreactors, exploring performance in regard to load-following scenarios. By leveraging a point kinetics model with thermal and xenon feedback, we first establish a baseline using a single-output RL agent, then compare it against a traditional proportional-integral-derivative (PID) controller. This study demonstrates that RL controllers, including both single- and multi-agent RL (MARL) frameworks, can achieve similar or even superior load-following performance as traditional PID control across a range of load-following scenarios. In short transients, the RL agent was able to reduce the tracking error rate in comparison to PID. Over extended 300-minute load-following scenarios in which xenon feedback becomes a dominant factor, PID maintained better accuracy, but RL still remained within a 1% error margin despite being trained only on short-duration scenarios. This highlights RL's strong ability to generalize and extrapolate to longer, more complex transients, affording substantial reductions in training costs and reduced overfitting. Furthermore, when control was extended to multiple drums, MARL enabled independent drum control as well as maintained reactor symmetry constraints without sacrificing performance -- an objective that standard single-agent RL could not learn. We also found that, as increasing levels of Gaussian noise were added to the power measurements, the RL controllers were able to maintain lower error rates than PID, and to do so with less control effort.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\textit{Agents Under Siege}$: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks</title>
<link>https://arxiv.org/abs/2504.00218</link>
<guid>https://arxiv.org/abs/2504.00218</guid>
<content:encoded><![CDATA[
<div>  (LLM)

<br /><br />:

LLM"""(PIEL)"LlamaMistralGemmaDeepSeekJailBreakBenchAdversarialBench7Llama-GuardPromptGuard <div>
arXiv:2504.00218v1 Announce Type: new 
Abstract: Most discussions about Large Language Model (LLM) safety have focused on single-agent settings but multi-agent LLM systems now create novel adversarial risks because their behavior depends on communication between agents and decentralized reasoning. In this work, we innovatively focus on attacking pragmatic systems that have constrains such as limited token bandwidth, latency between message delivery, and defense mechanisms. We design a $\textit{permutation-invariant adversarial attack}$ that optimizes prompt distribution across latency and bandwidth-constraint network topologies to bypass distributed safety mechanisms within the system. Formulating the attack path as a problem of $\textit{maximum-flow minimum-cost}$, coupled with the novel $\textit{Permutation-Invariant Evasion Loss (PIEL)}$, we leverage graph-based optimization to maximize attack success rate while minimizing detection risk. Evaluating across models including $\texttt{Llama}$, $\texttt{Mistral}$, $\texttt{Gemma}$, $\texttt{DeepSeek}$ and other variants on various datasets like $\texttt{JailBreakBench}$ and $\texttt{AdversarialBench}$, our method outperforms conventional attacks by up to $7\times$, exposing critical vulnerabilities in multi-agent systems. Moreover, we demonstrate that existing defenses, including variants of $\texttt{Llama-Guard}$ and $\texttt{PromptGuard}$, fail to prohibit our attack, emphasizing the urgent need for multi-agent specific safety mechanisms.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GazeLLM: Multimodal LLMs incorporating Human Visual Attention</title>
<link>https://arxiv.org/abs/2504.00221</link>
<guid>https://arxiv.org/abs/2504.00221</guid>
<content:encoded><![CDATA[
<div> : , , , , 

<br /><br />:
MLLMsMLLMs <div>
arXiv:2504.00221v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are advancing into Multimodal LLMs (MLLMs), capable of processing image, audio, and video as well as text. Combining first-person video, MLLMs show promising potential for understanding human activities through video and audio, enabling many human-computer interaction and human-augmentation applications such as human activity support, real-world agents, and skill transfer to robots or other individuals. However, handling high-resolution, long-duration videos generates large latent representations, leading to substantial memory and processing demands, limiting the length and resolution MLLMs can manage. Reducing video resolution can lower memory usage but often compromises comprehension. This paper introduces a method that optimizes first-person video analysis by integrating eye-tracking data, and proposes a method that decomposes first-person vision video into sub areas for regions of gaze focus. By processing these selectively gazed-focused inputs, our approach achieves task comprehension equivalent to or even better than processing the entire image at full resolution, but with significantly reduced video data input (reduce the number of pixels to one-tenth), offering an efficient solution for using MLLMs to interpret and utilize human skills.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Model Predictive Control for Dynamic Cooperation of Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2504.00225</link>
<guid>https://arxiv.org/abs/2504.00225</guid>
<content:encoded><![CDATA[
<div> Distributed Model Predictive Control, DMPC

<br /><br />:
 <div>
arXiv:2504.00225v1 Announce Type: new 
Abstract: We propose a distributed model predictive control (MPC) framework for coordinating heterogeneous, nonlinear multi-agent systems under individual and coupling constraints. The cooperative task is encoded as a shared objective function minimized collectively by the agents. Each agent optimizes an artificial reference as an intermediate step towards the cooperative objective, along with a control input to track it. We establish recursive feasibility, asymptotic stability, and transient performance bounds under suitable assumptions. The solution to the cooperative task is not predetermined but emerges from the optimized interactions of the agents. We demonstrate the framework on numerical examples inspired by satellite constellation control, collision-free narrow passage traversal, and coordinated quadrotor flight.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models in Numberland: A Quick Test of Their Numerical Reasoning Abilities</title>
<link>https://arxiv.org/abs/2504.00226</link>
<guid>https://arxiv.org/abs/2504.00226</guid>
<content:encoded><![CDATA[
<div> number sense, Large Language Models (LLMs), Numberland, numerical reasoning, AI agents

<br /><br />:

LLMsLLMNumberland10024LLMAI74%-95%2410%-73%o12427%LLMLLM <div>
arXiv:2504.00226v1 Announce Type: new 
Abstract: An essential element of human mathematical reasoning is our number sense -- an abstract understanding of numbers and their relationships -- which allows us to solve problems involving vast number spaces using limited computational resources. Mathematical reasoning of Large Language Models (LLMs) is often tested on high-level problems (such as Olympiad challenges, geometry, word problems, and puzzles), but their low-level number sense remains less explored. We introduce "Numberland," a 100-problem test to evaluate the numerical reasoning abilities of LLM-based agents. The tasks -- basic operations, advanced calculations (e.g., exponentiation, complex numbers), prime number checks, and the 24 game -- aim to test elementary skills and their integration in solving complex and uncertain problems. We evaluated five LLM-based agents: OpenAI's o1 and o1-mini, Google Gemini, Microsoft Copilot, and Anthropic Claude. They scored 74-95% on the first three tasks that allow deterministic steps to solutions. In the 24 game, which needs trial-and-error search, performance dropped to 10-73%. We tested the top 24 solver (o1 with 73% accuracy) on 25 harder problems, and its score fell to 27%, confirming search as a bottleneck. These results, along with the types of mistakes, suggest a fragile number of LLMs, which is a bit surprising given their prowess in challenging benchmarks. The limits of LLM numerical reasoning highlight the scope of simple, targeted tests to evaluate and explain LLM math skills to ensure safe use.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers</title>
<link>https://arxiv.org/abs/2504.00255</link>
<guid>https://arxiv.org/abs/2504.00255</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)SciReplicate-Bench<br /><br />:<br />
LLMsNLPAPISciReplicate-Bench202436NLP100SciReplicate-BenchSci-ReproducerPaper AgentCode AgentCodeBLEU/APILLM39% <div>
arXiv:2504.00255v1 Announce Type: new 
Abstract: This study evaluates large language models (LLMs) in generating code from algorithm descriptions from recent NLP papers. The task requires two key competencies: (1) algorithm comprehension: synthesizing information from papers and academic literature to understand implementation logic, and (2) coding expertise: identifying dependencies and correctly implementing necessary APIs. To facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark of 100 tasks from 36 NLP papers published in 2024, featuring detailed annotations and comprehensive test cases. Building on SciReplicate-Bench, we propose Sci-Reproducer, a multi-agent framework consisting of a Paper Agent that interprets algorithmic concepts from literature and a Code Agent that retrieves dependencies from repositories and implement solutions. To assess algorithm understanding, we introduce reasoning graph accuracy, which quantifies similarity between generated and reference reasoning graphs derived from code comments and structure. For evaluating implementation quality, we employ execution accuracy, CodeBLEU, and repository dependency/API recall metrics. In our experiments, we evaluate various powerful Non-Reasoning LLMs and Reasoning LLMs as foundational models. The best-performing LLM using Sci-Reproducer achieves only 39% execution accuracy, highlighting the benchmark's difficulty.Our analysis identifies missing or inconsistent algorithm descriptions as key barriers to successful reproduction. We will open-source our benchmark, and code at https://github.com/xyzCS/SciReplicate-Bench.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rack Position Optimization in Large-Scale Heterogeneous Data Centers</title>
<link>https://arxiv.org/abs/2504.00277</link>
<guid>https://arxiv.org/abs/2504.00277</guid>
<content:encoded><![CDATA[
<div> : AI(DRL)(MIP)

:
AIMIPDRL101007%MIP30%100%MIP2097.5%2MIP1630MIP <div>
arXiv:2504.00277v1 Announce Type: new 
Abstract: As rapidly growing AI computational demands accelerate the need for new hardware installation and maintenance, this work explores optimal data center resource management by balancing operational efficiency with fault tolerance through strategic rack positioning considering diverse resources and locations. Traditional mixed-integer programming (MIP) approaches often struggle with scalability, while heuristic methods may result in significant sub-optimality. To address these issues, this paper presents a novel two-tier optimization framework using a high-level deep reinforcement learning (DRL) model to guide a low-level gradient-based heuristic for local search. The high-level DRL agent employs Leader Reward for optimal rack type ordering, and the low-level heuristic efficiently maps racks to positions, minimizing movement counts and ensuring fault-tolerant resource distribution. This approach allows scalability to over 100,000 positions and 100 rack types. Our method outperformed the gradient-based heuristic by 7\% on average and the MIP solver by over 30\% in objective value. It achieved a 100\% success rate versus MIP's 97.5\% (within a 20-minute limit), completing in just 2 minutes compared to MIP's 1630 minutes (i.e., almost 4 orders of magnitude improvement). Unlike the MIP solver, which showed performance variability under time constraints and high penalties, our algorithm consistently delivered stable, efficient results - an essential feature for large-scale data center management.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploration and Adaptation in Non-Stationary Tasks with Diffusion Policies</title>
<link>https://arxiv.org/abs/2504.00280</link>
<guid>https://arxiv.org/abs/2504.00280</guid>
<content:encoded><![CDATA[
<div> Diffusion PolicyProcgenPointMaze

:
Diffusion PolicyDiffusion PolicyProcgenPointMazeDiffusion PolicyRLPPODQNDiffusion Policy <div>
arXiv:2504.00280v1 Announce Type: new 
Abstract: This paper investigates the application of Diffusion Policy in non-stationary, vision-based RL settings, specifically targeting environments where task dynamics and objectives evolve over time. Our work is grounded in practical challenges encountered in dynamic real-world scenarios such as robotics assembly lines and autonomous navigation, where agents must adapt control strategies from high-dimensional visual inputs. We apply Diffusion Policy -- which leverages iterative stochastic denoising to refine latent action representations-to benchmark environments including Procgen and PointMaze. Our experiments demonstrate that, despite increased computational demands, Diffusion Policy consistently outperforms standard RL methods such as PPO and DQN, achieving higher mean and maximum rewards with reduced variability. These findings underscore the approach's capability to generate coherent, contextually relevant action sequences in continuously shifting conditions, while also highlighting areas for further improvement in handling extreme non-stationarity.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Large Language Models Exhibit Spontaneous Rational Deception?</title>
<link>https://arxiv.org/abs/2504.00285</link>
<guid>https://arxiv.org/abs/2504.00285</guid>
<content:encoded><![CDATA[
<div> :  (LLMs), , , , 

<br />
:
LLMs2x2LLM1) LLM2) 3) LLMLLMsLLMsLLMs <div>
arXiv:2504.00285v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are effective at deceiving, when prompted to do so. But under what conditions do they deceive spontaneously? Models that demonstrate better performance on reasoning tasks are also better at prompted deception. Do they also increasingly deceive spontaneously in situations where it could be considered rational to do so? This study evaluates spontaneous deception produced by LLMs in a preregistered experimental protocol using tools from signaling theory. A range of proprietary closed-source and open-source LLMs are evaluated using modified 2x2 games (in the style of Prisoner's Dilemma) augmented with a phase in which they can freely communicate to the other agent using unconstrained language. This setup creates an opportunity to deceive, in conditions that vary in how useful deception might be to an agent's rational self-interest. The results indicate that 1) all tested LLMs spontaneously misrepresent their actions in at least some conditions, 2) they are generally more likely to do so in situations in which deception would benefit them, and 3) models exhibiting better reasoning capacity overall tend to deceive at higher rates. Taken together, these results suggest a tradeoff between LLM reasoning capability and honesty. They also provide evidence of reasoning-like behavior in LLMs from a novel experimental configuration. Finally, they reveal certain contextual factors that affect whether LLMs will deceive or not. We discuss consequences for autonomous, human-facing systems driven by LLMs both now and as their reasoning capabilities continue to improve.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Persuasion Overrides Truth in Multi-Agent LLM Debates: Introducing a Confidence-Weighted Persuasion Override Rate (CW-POR)</title>
<link>https://arxiv.org/abs/2504.00374</link>
<guid>https://arxiv.org/abs/2504.00374</guid>
<content:encoded><![CDATA[
<div> : 

<br /><br />:
LLMLLMTruthfulQALLM(CW-POR)LLM3B14B30-300LLM <div>
arXiv:2504.00374v1 Announce Type: new 
Abstract: In many real-world scenarios, a single Large Language Model (LLM) may encounter contradictory claims-some accurate, others forcefully incorrect-and must judge which is true. We investigate this risk in a single-turn, multi-agent debate framework: one LLM-based agent provides a factual answer from TruthfulQA, another vigorously defends a falsehood, and the same LLM architecture serves as judge. We introduce the Confidence-Weighted Persuasion Override Rate (CW-POR), which captures not only how often the judge is deceived but also how strongly it believes the incorrect choice. Our experiments on five open-source LLMs (3B-14B parameters), where we systematically vary agent verbosity (30-300 words), reveal that even smaller models can craft persuasive arguments that override truthful answers-often with high confidence. These findings underscore the importance of robust calibration and adversarial testing to prevent LLMs from confidently endorsing misinformation.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VerifiAgent: a Unified Verification Agent in Language Model Reasoning</title>
<link>https://arxiv.org/abs/2504.00406</link>
<guid>https://arxiv.org/abs/2504.00406</guid>
<content:encoded><![CDATA[
<div> : VerifiAgent

:
VerifiAgentVerifiAgentVerifiAgentVerifiAgenthttps://github.com/Jiuzhouh/VerifiAgent <div>
arXiv:2504.00406v1 Announce Type: new 
Abstract: Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources and lacking scalability across diverse reasoning tasks. To address these limitations, we propose VerifiAgent, a unified verification agent that integrates two levels of verification: meta-verification, which assesses completeness and consistency in model responses, and tool-based adaptive verification, where VerifiAgent autonomously selects appropriate verification tools based on the reasoning type, including mathematical, logical, or commonsense reasoning. This adaptive approach ensures both efficiency and robustness across different verification scenarios. Experimental results show that VerifiAgent outperforms baseline verification methods (e.g., deductive verifier, backward verifier) among all reasoning tasks. Additionally, it can further enhance reasoning accuracy by leveraging feedback from verification results. VerifiAgent can also be effectively applied to inference scaling, achieving better results with fewer generated samples and costs compared to existing process reward models in the mathematical reasoning domain. Code is available at https://github.com/Jiuzhouh/VerifiAgent
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HERA: Hybrid Edge-cloud Resource Allocation for Cost-Efficient AI Agents</title>
<link>https://arxiv.org/abs/2504.00434</link>
<guid>https://arxiv.org/abs/2504.00434</guid>
<content:encoded><![CDATA[
<div> LLMsSLMsAIAIMS

:
AILLMsSLMsSLMLLMAIAIAIMSAIMSSLMLLMSLMHybridLLMAIMS9.1%SLM10.8%45.67%SLMLLM<br /><br /> <div>
arXiv:2504.00434v1 Announce Type: new 
Abstract: In the realm of AI, large language models (LLMs) like GPT-4, central to the operation of AI agents, predominantly operate in the cloud, incurring high operational costs. With local-based small language models (SLMs) becoming more accurate, the necessity of cloud-exclusive processing is being reconsidered. An AI agent's response to a user's request comprises a series of subtasks or iterations. Existing approaches only allocate a single request between SLM and LLM to ensure their outputs are similar, but adopting this approach in the AI agent scenario for assigning each subtask is not effective since SLM will output a different subsequent subtask, which affects the accuracy of the final output. In this paper, we first conduct experimental analysis to understand the features of AI agent operations. Leveraging our findings, we propose the Adaptive Iteration-level Model Selector (AIMS), a lightweight scheduler to automatically partition AI agent's subtasks between local-based SLM and cloud-based LLM. AIMS considers the varying subtask features and strategically decides the location for each subtask in order to use SLM as much as possible while attaining the accuracy level. Our experimental results demonstrate that AIMS increases accuracy by up to 9.1% and SLM usage by up to 10.8% compared to HybridLLM. It offloads 45.67% of subtasks to a local SLM while attaining similar accuracy on average compared with the cloud-only LLM approach.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated detection of atomicity violations in large-scale systems</title>
<link>https://arxiv.org/abs/2504.00521</link>
<guid>https://arxiv.org/abs/2504.00521</guid>
<content:encoded><![CDATA[
<div> : Clover

:
CloverLLMCloverRaceBench 2.1SV-COMPRWIPClover92.3%86.6%F127.4%-118.2% <div>
arXiv:2504.00521v1 Announce Type: new 
Abstract: Atomicity violations in interrupt-driven programs pose a significant threat to software safety in critical systems. These violations occur when the execution sequence of operations on shared resources is disrupted by asynchronous interrupts. Detecting atomicity violations is challenging due to the vast program state space, application-level code dependencies, and complex domain-specific knowledge. We propose Clover, a hybrid framework that integrates static analysis with large language model (LLM) agents to detect atomicity violations in real-world programs. Clover first performs static analysis to extract critical code snippets and operation information. It then initiates a multi-agent process, where the expert agent leverages domain-specific knowledge to detect atomicity violations, which are subsequently validated by the judge agent. Evaluations on RaceBench 2.1, SV-COMP, and RWIP demonstrate that Clover achieves a precision/recall of 92.3%/86.6%, outperforming existing approaches by 27.4-118.2% on F1-score.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentNet: Decentralized Evolutionary Coordination for LLM-based Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2504.00587</link>
<guid>https://arxiv.org/abs/2504.00587</guid>
<content:encoded><![CDATA[
<div> : , , , AgentNet, 

:<br />
LLMsAgentNetRAGLLMDAGAgentNet1) 2) 3) AgentNet <div>
arXiv:2504.00587v1 Announce Type: new 
Abstract: The rapid advancement of Large Language Models (LLMs) has catalyzed the development of multi-agent systems, where multiple LLM-based agents collaborate to solve complex tasks. However, existing systems predominantly rely on centralized coordination, which introduces scalability bottlenecks, limits adaptability, and creates single points of failure. Additionally, concerns over privacy and proprietary knowledge sharing hinder cross-organizational collaboration, leading to siloed expertise. To address these challenges, we propose AgentNet, a decentralized, Retrieval-Augmented Generation (RAG)-based framework that enables LLM-based agents to autonomously evolve their capabilities and collaborate efficiently in a Directed Acyclic Graph (DAG)-structured network. Unlike traditional multi-agent systems that depend on static role assignments or centralized control, AgentNet allows agents to specialize dynamically, adjust their connectivity, and route tasks without relying on predefined workflows. AgentNet's core design is built upon several key innovations: (1) Fully Decentralized Paradigm: Removing the central orchestrator, allowing agents to coordinate and specialize autonomously, fostering fault tolerance and emergent collective intelligence. (2) Dynamically Evolving Graph Topology: Real-time adaptation of agent connections based on task demands, ensuring scalability and resilience.(3) Adaptive Learning for Expertise Refinement: A retrieval-based memory system that enables agents to continuously update and refine their specialized skills. By eliminating centralized control, AgentNet enhances fault tolerance, promotes scalable specialization, and enables privacy-preserving collaboration across organizations. Through decentralized coordination and minimal data exchange, agents can leverage diverse knowledge sources while safeguarding sensitive information.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asynchronous Multi-Agent Systems with Petri nets</title>
<link>https://arxiv.org/abs/2504.00602</link>
<guid>https://arxiv.org/abs/2504.00602</guid>
<content:encoded><![CDATA[
<div> : 1-safe Petri 

<br /><br />:
1-safe Petri (AMAS)AMAS1-safe Petri PetritransitionPetri <div>
arXiv:2504.00602v1 Announce Type: new 
Abstract: Modeling the interaction between components is crucial for many applications and serves as a fundamental step in analyzing and verifying properties in multi-agent systems. In this paper, we propose a method based on 1-safe Petri nets to model Asynchronous Multi-Agent Systems (AMAS), starting from two semantics defined on AMAS represented as transition systems. Specifically, we focus on two types of synchronization: synchronization on transitions and synchronization on data. For both, we define an operator that composes 1-safe Petri nets and demonstrate the relationships between the composed Petri net and the global transition systems as defined in theliterature. Additionally, we analyze the relationships between the two semantics on Petri nets, proposing two constructions that enable switching between them. These transformations are particularly useful for system analysis, as they allow the selection of the most suitable model based on the property that needs to be verified.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Impact of an LLM-Powered Teachable Agent on Learning Gains and Cognitive Load in Music Education</title>
<link>https://arxiv.org/abs/2504.00636</link>
<guid>https://arxiv.org/abs/2504.00636</guid>
<content:encoded><![CDATA[
<div> LLMLearning by Teaching (LBT)

<br />
:
LBTLLM28LBT <div>
arXiv:2504.00636v1 Announce Type: new 
Abstract: This study examines the impact of an LLM-powered teachable agent, grounded in the Learning by Teaching (LBT) pedagogy, on students' music theory learning and cognitive load. The participants were 28 Chinese university students with prior music instrumental experiences. In an online experiment, they were assigned to either an experimental group, which engaged in music analysis with the teachable agent, or a control group, which conducted self-directed analysis using instructional materials. Findings indicate that students in the experimental group achieved significantly higher post-test scores than those in the control group. Additionally, they reported lower cognitive load, suggesting that the teachable agent effectively reduced the cognitive demands of music analysis tasks. These results highlight the potential of AI-driven scaffolding based on LBT principles to enhance music theory education, supporting teachers in delivering theory-oriented instruction while fostering students' self-directed learning skills.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulation of Autonomous Industrial Vehicle Fleet Using Fuzzy Agents: Application to Task Allocation and Battery Charge Management</title>
<link>https://arxiv.org/abs/2504.00683</link>
<guid>https://arxiv.org/abs/2504.00683</guid>
<content:encoded><![CDATA[
<div> 

:<br />
 <div>
arXiv:2504.00683v1 Announce Type: new 
Abstract: The research introduces a multi-agent simulation that uses fuzzy inference to investigate the work distribution and battery charging control of mobile baggage conveyor robots in an airport in a comprehensive manner. Thanks to a distributed system, this simulation approach provides high adaptability, adjusting to changes in conveyor agent availability, battery capacity, awareness of the activities of the conveyor fleet, and knowledge of the context of infrastructure resource availability. Dynamic factors, such as workload variations and communication between the conveyor agents and infrastructure are considered as heuristics, highlighting the importance of flexible and collaborative approaches in autonomous systems. The results highlight the effectiveness of adaptive fuzzy multi-agent models to optimize dynamic task allocation, adapt to the variation of baggage arrival flows, improve the overall operational efficiency of conveyor agents, and reduce their energy consumption.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Command A: An Enterprise-Ready Large Language Model</title>
<link>https://arxiv.org/abs/2504.00698</link>
<guid>https://arxiv.org/abs/2504.00698</guid>
<content:encoded><![CDATA[
<div> : Command ARetrieval Augmented Generation (RAG)

:
Command A23Command ARetrieval Augmented Generation (RAG) Command ACommand R7B <div>
arXiv:2504.00698v1 Announce Type: new 
Abstract: In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model, with support for 23 languages of global business, and a novel hybrid architecture balancing efficiency with top of the range performance. It offers best-in-class Retrieval Augmented Generation (RAG) capabilities with grounding and tool use to automate sophisticated business processes. These abilities are achieved through a decentralised training approach, including self-refinement algorithms and model merging techniques. We also include results for Command R7B which shares capability and architectural similarities to Command A. Weights for both models have been released for research purposes. This technical report details our original training pipeline and presents an extensive evaluation of our models across a suite of enterprise-relevant tasks and public benchmarks, demonstrating excellent performance and efficiency.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments</title>
<link>https://arxiv.org/abs/2504.00711</link>
<guid>https://arxiv.org/abs/2504.00711</guid>
<content:encoded><![CDATA[
<div> : GraphFoundationModels(GFMs), , (LLMs), GraphMaster, , 

:
GFMsAIGraphMasterGraphMasterLLMManagerPerceptionEnhancementEvaluationSubGrassmannGraphMasterGFMs <div>
arXiv:2504.00711v1 Announce Type: new 
Abstract: The era of foundation models has revolutionized AI research, yet Graph Foundation Models (GFMs) remain constrained by the scarcity of large-scale graph corpora. Traditional graph data synthesis techniques primarily focus on simplistic structural operations, lacking the capacity to generate semantically rich nodes with meaningful textual attributes: a critical limitation for real-world applications. While large language models (LLMs) demonstrate exceptional text generation capabilities, their direct application to graph synthesis is impeded by context window limitations, hallucination phenomena, and structural consistency challenges. To address these issues, we introduce GraphMaster, the first multi-agent framework specifically designed for graph data synthesis in data-limited environments. GraphMaster orchestrates four specialized LLM agents (Manager, Perception, Enhancement, and Evaluation) that collaboratively optimize the synthesis process through iterative refinement, ensuring both semantic coherence and structural integrity. To rigorously evaluate our approach, we create new data-limited "Sub" variants of six standard graph benchmarks, specifically designed to test synthesis capabilities under realistic constraints. Additionally, we develop a novel interpretability assessment framework that combines human evaluation with a principled Grassmannian manifold-based analysis, providing both qualitative and quantitative measures of semantic coherence. Experimental results demonstrate that GraphMaster significantly outperforms traditional synthesis methods across multiple datasets, establishing a strong foundation for advancing GFMs in data-scarce environments.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personality-Driven Decision-Making in LLM-Based Autonomous Agents</title>
<link>https://arxiv.org/abs/2504.00727</link>
<guid>https://arxiv.org/abs/2504.00727</guid>
<content:encoded><![CDATA[
<div> : Large Language Models (LLMs), autonomous agents, SANDMAN, personality traits, task selection

<br /><br />:
LLMsOCEANSANDMANLLMOCEAN <div>
arXiv:2504.00727v1 Announce Type: new 
Abstract: The embedding of Large Language Models (LLMs) into autonomous agents is a rapidly developing field which enables dynamic, configurable behaviours without the need for extensive domain-specific training. In our previous work, we introduced SANDMAN, a Deceptive Agent architecture leveraging the Five-Factor OCEAN personality model, demonstrating that personality induction significantly influences agent task planning. Building on these findings, this study presents a novel method for measuring and evaluating how induced personality traits affect task selection processes - specifically planning, scheduling, and decision-making - in LLM-based agents. Our results reveal distinct task-selection patterns aligned with induced OCEAN attributes, underscoring the feasibility of designing highly plausible Deceptive Agents for proactive cyber defense strategies.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement learning for robust dynamic metabolic control</title>
<link>https://arxiv.org/abs/2504.00735</link>
<guid>https://arxiv.org/abs/2504.00735</guid>
<content:encoded><![CDATA[
<div> : _acetyl-CoA_

:
_Escherichia coli__acetyl-CoA_40%<br /><br /> <div>
arXiv:2504.00735v1 Announce Type: new 
Abstract: Dynamic metabolic control can enhance bioprocess flexibility and expand the available optimization degrees of freedom via real-time modulation of metabolic enzyme expression. This allows target metabolic fluxes to be dynamically tuned throughout the process. However, identifying optimal dynamic control policies is challenging due to the presence of potential metabolic burden, cytotoxic effects, and the generally high-dimensional solution space, making exhaustive experimentation impractical. Here, we propose an approach based on reinforcement learning to derive optimal dynamic metabolic control policies by allowing an agent or controller to interact with a surrogate dynamic model $\textit{in silico}$. To incorporate and test robustness, we apply domain randomization, enabling the controller to generalize across system uncertainties. Our approach provides an alternative to conventional model-based control such as model predictive control, which requires differentiating the models with respect to decision variables; an often impractical task when dealing with complex stochastic, nonlinear, stiff, or piecewise-defined dynamics. In contrast, our approach only requires forward integration, making the task computationally much simpler with off-the-shelf solvers. We demonstrate our approach with a case study on the dynamic control of acetyl-CoA carboxylase in $\textit{Escherichia coli}$ for fatty acid biosynthesis. The derived dynamic metabolic control policies outperform static control, achieving up to 40 % higher titers while remaining robust under uncertainty.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do We Truly Need So Many Samples? Multi-LLM Repeated Sampling Efficiently Scale Test-Time Compute</title>
<link>https://arxiv.org/abs/2504.00762</link>
<guid>https://arxiv.org/abs/2504.00762</guid>
<content:encoded><![CDATA[
<div> : LLM

:
LLMModelSwitchLLM-LLM <div>
arXiv:2504.00762v1 Announce Type: new 
Abstract: This paper presents a simple, effective, and cost-efficient strategy to improve LLM performance by scaling test-time compute. Our strategy builds upon the repeated-sampling-then-voting framework, with a novel twist: incorporating multiple models, even weaker ones, to leverage their complementary strengths that potentially arise from diverse training data and paradigms. By using consistency as a signal, our strategy dynamically switches between models. Theoretical analysis highlights the efficiency and performance advantages of our strategy. Extensive experiments on six datasets demonstrate that our strategy not only outperforms self-consistency and state-of-the-art multi-agent debate approaches, but also significantly reduces inference costs. Additionally, ModelSwitch requires only a few comparable LLMs to achieve optimal performance and can be extended with verification methods, demonstrating the potential of leveraging multiple LLMs in the generation-verification paradigm.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provably Stable Multi-Agent Routing with Bounded-Delay Adversaries in the Decision Loop</title>
<link>https://arxiv.org/abs/2504.00863</link>
<guid>https://arxiv.org/abs/2504.00863</guid>
<content:encoded><![CDATA[
<div> : 

:
 <div>
arXiv:2504.00863v1 Announce Type: new 
Abstract: In this work, we are interested in studying multi-agent routing settings, where adversarial agents are part of the assignment and decision loop, degrading the performance of the fleet by incurring bounded delays while servicing pickup-and-delivery requests. Specifically, we are interested in characterizing conditions on the fleet size and the proportion of adversarial agents for which a routing policy remains stable, where stability for a routing policy is achieved if the number of outstanding requests is uniformly bounded over time. To obtain this characterization, we first establish a threshold on the proportion of adversarial agents above which previously stable routing policies for fully cooperative fleets are provably unstable. We then derive a sufficient condition on the fleet size to recover stability given a maximum proportion of adversarial agents. We empirically validate our theoretical results on a case study on autonomous taxi routing, where we consider transportation requests from real San Francisco taxicab data.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Investigation into the Causal Mechanism of Political Opinion Dynamics: A Model of Hierarchical Coarse-Graining with Community-Bounded Social Influence</title>
<link>https://arxiv.org/abs/2504.00877</link>
<guid>https://arxiv.org/abs/2504.00877</guid>
<content:encoded><![CDATA[
<div> : CODA

:
CODA <div>
arXiv:2504.00877v1 Announce Type: new 
Abstract: Increasing polarization in democratic societies is an emergent outcome of political opinion dynamics. Yet, the fundamental mechanisms behind the formation of political opinions, from individual beliefs to collective consensus, remain unknown. Understanding that a causal mechanism must account for both bottom-up and top-down influences, we conceptualize political opinion dynamics as hierarchical coarse-graining, where microscale opinions integrate into a macro-scale state variable. Using the CODA (Continuous Opinions Discrete Actions) model, we simulate Bayesian opinion updating, social identity-based information integration, and migration between social identity groups to represent higher-level connectivity. This results in coarse-graining across micro, meso, and macro levels. Our findings show that higher-level connectivity shapes information integration, yielding three regimes: independent (disconnected, local convergence), parallel (fast, global convergence), and iterative (slow, stepwise convergence). In the iterative regime, low connectivity fosters transient diversity, indicating an informed consensus. In all regimes, time-scale separation leads to downward causation, where agents converge on the aggregate majority choice, driving consensus. Critically, any degree of coherent higher-level information integration can overcome misalignment via global downward causation. The results highlight how emergent properties of the causal mechanism, such as downward causation, are essential for consensus and may inform more precise investigations into polarized political discourse.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Visual-Spatial Reasoning via R1-Zero-Like Training</title>
<link>https://arxiv.org/abs/2504.00883</link>
<guid>https://arxiv.org/abs/2504.00883</guid>
<content:encoded><![CDATA[
<div> : R1-ZeroGRPO

<br /><br />:
R1-ZeroMLLMsQwen2-VLChain of ThoughtCoTGRPOVSI-100kDeepSeek-R1-ZeroGRPOKL120GPUQwen2-VL-2BvsGRPO-2B12.1%GPT-4oQwen2-VL-7BvsGRPO-7BLLaVA-NeXT-Video-72BvsGRPO <div>
arXiv:2504.00883v1 Announce Type: new 
Abstract: Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents</title>
<link>https://arxiv.org/abs/2504.00906</link>
<guid>https://arxiv.org/abs/2504.00906</guid>
<content:encoded><![CDATA[
<div> : Agent S2

:
<br />
Agent S2Agent S2Mixture-of-GroundingGUIProactive Hierarchical PlanningAgent S2state-of-the-artClaude Computer UseUI-TARSOSWorld155018.9%32.7%Agent S2WindowsAgentArenaAndroidWorld52.8%16.52%https://github.com/simular-ai/Agent-S <div>
arXiv:2504.00906v1 Announce Type: new 
Abstract: Computer use agents automate digital tasks by directly interacting with graphical user interfaces (GUIs) on computers and mobile devices, offering significant potential to enhance human productivity by completing an open-ended space of user queries. However, current agents face significant challenges: imprecise grounding of GUI elements, difficulties with long-horizon task planning, and performance bottlenecks from relying on single generalist models for diverse cognitive tasks. To this end, we introduce Agent S2, a novel compositional framework that delegates cognitive responsibilities across various generalist and specialist models. We propose a novel Mixture-of-Grounding technique to achieve precise GUI localization and introduce Proactive Hierarchical Planning, dynamically refining action plans at multiple temporal scales in response to evolving observations. Evaluations demonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance on three prominent computer use benchmarks. Specifically, Agent S2 achieves 18.9% and 32.7% relative improvements over leading baseline agents such as Claude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation. Moreover, Agent S2 generalizes effectively to other operating systems and applications, surpassing previous best methods by 52.8% on WindowsAgentArena and by 16.52% on AndroidWorld relatively. Code available at https://github.com/simular-ai/Agent-S.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.00907</link>
<guid>https://arxiv.org/abs/2504.00907</guid>
<content:encoded><![CDATA[
<div> Embodied agentshuman instructionsAsk-to-Act(RL)(MLLMs)

<br /><br />:
Ask-to-ActRLMLLMs--VLALLMGPT-4oMLLMRLMLLM19.1%-40.3%LLMRLMLLMVLA <div>
arXiv:2504.00907v1 Announce Type: new 
Abstract: Embodied agents operating in real-world environments must interpret ambiguous and under-specified human instructions. A capable household robot should recognize ambiguity and ask relevant clarification questions to infer the user intent accurately, leading to more effective task execution. To study this problem, we introduce the Ask-to-Act task, where an embodied agent must fetch a specific object instance given an ambiguous instruction in a home environment. The agent must strategically ask minimal, yet relevant, clarification questions to resolve ambiguity while navigating under partial observability. To solve this problem, we propose a novel approach that fine-tunes multimodal large language models (MLLMs) as vision-language-action (VLA) policies using online reinforcement learning (RL) with LLM-generated rewards. Our method eliminates the need for large-scale human demonstrations or manually engineered rewards for training such agents. We benchmark against strong zero-shot baselines, including GPT-4o, and supervised fine-tuned MLLMs, on our task. Our results demonstrate that our RL-finetuned MLLM outperforms all baselines by a significant margin ($19.1$-$40.3\%$), generalizing well to novel scenes and tasks. To the best of our knowledge, this is the first demonstration of adapting MLLMs as VLA agents that can act and ask for help using LLM-generated rewards with online RL.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Robustness of Agentic Function Calling</title>
<link>https://arxiv.org/abs/2504.00914</link>
<guid>https://arxiv.org/abs/2504.00914</guid>
<content:encoded><![CDATA[
<div> :  (LLMs) (FC)

:
LLMsFCFCFCBFCL <div>
arXiv:2504.00914v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly acting as autonomous agents, with function calling (FC) capabilities enabling them to invoke specific tools for tasks. While prior research has primarily focused on improving FC accuracy, little attention has been given to the robustness of these agents to perturbations in their input. We introduce a benchmark assessing FC robustness in two key areas: resilience to naturalistic query variations, and stability in function calling when the toolkit expands with semantically related tools. Evaluating best-performing FC models on a carefully expanded subset of the Berkeley function calling leaderboard (BFCL), we identify critical weaknesses in existing evaluation methodologies, and highlight areas for improvement in real-world agentic deployments.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Efficient Processing of Spiking Neural Networks with On-Chip Learning on Commodity Neuromorphic Processors for Edge AI Systems</title>
<link>https://arxiv.org/abs/2504.00957</link>
<guid>https://arxiv.org/abs/2504.00957</guid>
<content:encoded><![CDATA[
<div> : AI

:
SNNAISNN50200125015AI <div>
arXiv:2504.00957v1 Announce Type: new 
Abstract: The rising demand for energy-efficient edge AI systems (e.g., mobile agents/robots) has increased the interest in neuromorphic computing, since it offers ultra-low power/energy AI computation through spiking neural network (SNN) algorithms on neuromorphic processors. However, their efficient implementation strategy has not been comprehensively studied, hence limiting SNN deployments for edge AI systems. Toward this, we propose a design methodology to enable efficient SNN processing on commodity neuromorphic processors. To do this, we first study the key characteristics of targeted neuromorphic hardware (e.g., memory and compute budgets), and leverage this information to perform compatibility analysis for network selection. Afterward, we employ a mapping strategy for efficient SNN implementation on the targeted processor. Furthermore, we incorporate an efficient on-chip learning mechanism to update the systems' knowledge for adapting to new input classes and dynamic environments. The experimental results show that the proposed methodology leads the system to achieve low latency of inference (i.e., less than 50ms for image classification, less than 200ms for real-time object detection in video streaming, and less than 1ms in keyword recognition) and low latency of on-chip learning (i.e., less than 2ms for keyword recognition), while incurring less than 250mW of processing power and less than 15mJ of energy consumption across the respective different applications and scenarios. These results show the potential of the proposed methodology in enabling efficient edge AI systems for diverse application use-cases.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RedMotion: Motion Prediction via Redundancy Reduction</title>
<link>https://arxiv.org/abs/2306.10840</link>
<guid>https://arxiv.org/abs/2306.10840</guid>
<content:encoded><![CDATA[
<div> : RedMotionTransformerWaymo Motion Prediction Challenge

<br /><br />:
RedMotionPreTraMTraj-MAEGraphDINORedMotionWaymo Motion Prediction ChallengeHPTRMTR++https://github.com/kit-mrt/future-motion <div>
arXiv:2306.10840v4 Announce Type: replace 
Abstract: We introduce RedMotion, a transformer model for motion prediction in self-driving vehicles that learns environment representations via redundancy reduction. Our first type of redundancy reduction is induced by an internal transformer decoder and reduces a variable-sized set of local road environment tokens, representing road graphs and agent data, to a fixed-sized global embedding. The second type of redundancy reduction is obtained by self-supervised learning and applies the redundancy reduction principle to embeddings generated from augmented views of road environments. Our experiments reveal that our representation learning approach outperforms PreTraM, Traj-MAE, and GraphDINO in a semi-supervised setting. Moreover, RedMotion achieves competitive results compared to HPTR or MTR++ in the Waymo Motion Prediction Challenge. Our open-source implementation is available at: https://github.com/kit-mrt/future-motion
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Graph-to-Text Approach to Knowledge-Grounded Response Generation in Human-Robot Interaction</title>
<link>https://arxiv.org/abs/2311.16137</link>
<guid>https://arxiv.org/abs/2311.16137</guid>
<content:encoded><![CDATA[
<div> : 

:<br />
Wizard-of-Oz <div>
arXiv:2311.16137v2 Announce Type: replace 
Abstract: Knowledge graphs are often used to represent structured information in a flexible and efficient manner, but their use in situated dialogue remains under-explored. This paper presents a novel conversational model for human--robot interaction that rests upon a graph-based representation of the dialogue state. The knowledge graph representing the dialogue state is continuously updated with new observations from the robot sensors, including linguistic, situated and multimodal inputs, and is further enriched by other modules, in particular for spatial understanding. The neural conversational model employed to respond to user utterances relies on a simple but effective graph-to-text mechanism that traverses the dialogue state graph and converts the traversals into a natural language form. This conversion of the state graph into text is performed using a set of parameterized functions, and the values for those parameters are optimized based on a small set of Wizard-of-Oz interactions. After this conversion, the text representation of the dialogue state graph is included as part of the prompt of a large language model used to decode the agent response. The proposed approach is empirically evaluated through a user study with a humanoid robot that acts as conversation partner to evaluate the impact of the graph-to-text mechanism on the response generation. After moving a robot along a tour of an indoor environment, participants interacted with the robot using spoken dialogue and evaluated how well the robot was able to answer questions about what the robot observed during the tour. User scores show a statistically significant improvement in the perceived factuality of the robot responses when the graph-to-text approach is employed, compared to a baseline using inputs structured as semantic triples.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal and Semantic Evaluation Metrics for Foundation Models in Post-Hoc Analysis of Robotic Sub-tasks</title>
<link>https://arxiv.org/abs/2403.17238</link>
<guid>https://arxiv.org/abs/2403.17238</guid>
<content:encoded><![CDATA[
<div> Task and Motion Planning (TAMP)(FMs)(LLMs)(VLMs)SIMILARITY

<br /><br />:
Task and Motion Planning (TAMP)SIMILARITYFM90%30%TAMP <div>
arXiv:2403.17238v2 Announce Type: replace 
Abstract: Recent works in Task and Motion Planning (TAMP) show that training control policies on language-supervised robot trajectories with quality labeled data markedly improves agent task success rates. However, the scarcity of such data presents a significant hurdle to extending these methods to general use cases. To address this concern, we present an automated framework to decompose trajectory data into temporally bounded and natural language-based descriptive sub-tasks by leveraging recent prompting strategies for Foundation Models (FMs) including both Large Language Models (LLMs) and Vision Language Models (VLMs). Our framework provides both time-based and language-based descriptions for lower-level sub-tasks that comprise full trajectories. To rigorously evaluate the quality of our automatic labeling framework, we contribute an algorithm SIMILARITY to produce two novel metrics, temporal similarity and semantic similarity. The metrics measure the temporal alignment and semantic fidelity of language descriptions between two sub-task decompositions, namely an FM sub-task decomposition prediction and a ground-truth sub-task decomposition. We present scores for temporal similarity and semantic similarity above 90%, compared to 30% of a randomized baseline, for multiple robotic environments, demonstrating the effectiveness of our proposed framework. Our results enable building diverse, large-scale, language-supervised datasets for improved robotic TAMP.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergence of Decentralized Actor-Critic Algorithm in General-sum Markov Games</title>
<link>https://arxiv.org/abs/2409.04613</link>
<guid>https://arxiv.org/abs/2409.04613</guid>
<content:encoded><![CDATA[
<div> : MarkovActor-Critic<br /><br />:

MarkovMarkovActor-CriticMarkov(MNPF)Lyapunov <div>
arXiv:2409.04613v5 Announce Type: replace 
Abstract: Markov games provide a powerful framework for modeling strategic multi-agent interactions in dynamic environments. Traditionally, convergence properties of decentralized learning algorithms in these settings have been established only for special cases, such as Markov zero-sum and potential games, which do not fully capture real-world interactions. In this paper, we address this gap by studying the asymptotic properties of learning algorithms in general-sum Markov games. In particular, we focus on a decentralized algorithm where each agent adopts an actor-critic learning dynamic with asynchronous step sizes. This decentralized approach enables agents to operate independently, without requiring knowledge of others' strategies or payoffs. We introduce the concept of a Markov Near-Potential Function (MNPF) and demonstrate that it serves as an approximate Lyapunov function for the policy updates in the decentralized learning dynamics, which allows us to characterize the convergent set of strategies. We further strengthen our result under specific regularity conditions and with finite Nash equilibria.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPA-Bench: A Comprehensive Benchmark for SmartPhone Agent Evaluation</title>
<link>https://arxiv.org/abs/2410.15164</link>
<guid>https://arxiv.org/abs/2410.15164</guid>
<content:encoded><![CDATA[
<div> : smartphone agents, MLLM, benchmark, SPA-Bench, evaluation pipeline

<br /><br />:
SPA-Bench(Multimodal) Large Language Model (MLLM)SPA-Bench(1)(2)Android(3)SPA-Benchhttps://ai-agents-2030.github.io/SPA-Bench/ <div>
arXiv:2410.15164v3 Announce Type: replace 
Abstract: Smartphone agents are increasingly important for helping users control devices efficiently, with (Multimodal) Large Language Model (MLLM)-based approaches emerging as key contenders. Fairly comparing these agents is essential but challenging, requiring a varied task scope, the integration of agents with different implementations, and a generalisable evaluation pipeline to assess their strengths and weaknesses. In this paper, we present SPA-Bench, a comprehensive SmartPhone Agent Benchmark designed to evaluate (M)LLM-based agents in an interactive environment that simulates real-world conditions. SPA-Bench offers three key contributions: (1) A diverse set of tasks covering system and third-party apps in both English and Chinese, focusing on features commonly used in daily routines; (2) A plug-and-play framework enabling real-time agent interaction with Android devices, integrating over ten agents with the flexibility to add more; (3) A novel evaluation pipeline that automatically assesses agent performance across multiple dimensions, encompassing seven metrics related to task completion and resource consumption. Our extensive experiments across tasks and agents reveal challenges like interpreting mobile user interfaces, action grounding, memory retention, and execution costs. We propose future research directions to ease these difficulties, moving closer to real-world smartphone agent applications. SPA-Bench is available at https://ai-agents-2030.github.io/SPA-Bench/.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents</title>
<link>https://arxiv.org/abs/2411.06559</link>
<guid>https://arxiv.org/abs/2411.06559</guid>
<content:encoded><![CDATA[
<div> WebDreamer

:
WebWebDreamerLLMLLMWebDreamer(VisualWebArena)(Online-Mind2WebMind2Web-Live)Dreamer-7BGPT-4o <div>
arXiv:2411.06559v2 Announce Type: replace 
Abstract: Language agents based on large language models (LLMs) have demonstrated great promise in automating web-based tasks. Recent work has shown that incorporating advanced planning algorithms, e.g., tree search, is advantageous over reactive planning for web agents. However, unlike simulated sandbox environments, real-world environments such as the web are rife with irreversible actions. This undermines the feasibility of backtracking, a cornerstone of (tree) search. Overly relying on test-time search also hurts efficiency. We advocate model-based planning for web agents that employs a world model to simulate and deliberate over the outcome of each candidate action before committing to one. We systematically explore this paradigm by (1) Proposing a model-based planning framework, WebDreamer, which employs LLMs to serve as both world models and value functions; (2) Training specialized LLMs as world models with a scalable data synthesis pipeline. Empirical results demonstrate that WebDreamer achieves substantial performance improvements over reactive baselines. It is competitive, while being 4-5 times more efficient, with tree search in sandbox environments (VisualWebArena) and also works effectively on real-world websites (Online-Mind2Web and Mind2Web-Live). Furthermore, our trained world model, Dreamer-7B, performs comparable to GPT-4o, highlighting the potential of specialized world models for efficient and effective planning in complex web environments.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games</title>
<link>https://arxiv.org/abs/2411.13543</link>
<guid>https://arxiv.org/abs/2411.13543</guid>
<content:encoded><![CDATA[
<div> :  (LLMs) (VLMs)BALROG

:
LLMsVLMsBALROGBALROGNetHackLLMsVLMsBALROGbalrogai.com <div>
arXiv:2411.13543v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) and Vision Language Models (VLMs) possess extensive knowledge and exhibit promising reasoning abilities, however, they still struggle to perform well in complex, dynamic environments. Real-world tasks require handling intricate interactions, advanced spatial reasoning, long-term planning, and continuous exploration of new strategies-areas in which we lack effective methodologies for comprehensively evaluating these capabilities. To address this gap, we introduce BALROG, a novel benchmark designed to assess the agentic capabilities of LLMs and VLMs through a diverse set of challenging games. Our benchmark incorporates a range of existing reinforcement learning environments with varying levels of difficulty, including tasks that are solvable by non-expert humans in seconds to extremely challenging ones that may take years to master (e.g., the NetHack Learning Environment). We devise fine-grained metrics to measure performance and conduct an extensive evaluation of several popular open-source and closed-source LLMs and VLMs. Our findings indicate that while current models achieve partial success in the easier games, they struggle significantly with more challenging tasks. Notably, we observe severe deficiencies in vision-based decision-making, as several models perform worse when visual representations of the environments are provided. We release BALROG as an open and user-friendly benchmark to facilitate future research and development in the agentic community. Code and Leaderboard at balrogai.com.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interact with me: Joint Egocentric Forecasting of Intent to Interact, Attitude and Social Actions</title>
<link>https://arxiv.org/abs/2412.16698</link>
<guid>https://arxiv.org/abs/2412.16698</guid>
<content:encoded><![CDATA[
<div> : SocialEgoNetegocentric 

:
SocialEgoNet-1SocialEgoNet-JPL-SocialJPL-SocialSocialEgoNet83.15% <div>
arXiv:2412.16698v2 Announce Type: replace 
Abstract: For efficient human-agent interaction, an agent should proactively recognize their target user and prepare for upcoming interactions. We formulate this challenging problem as the novel task of jointly forecasting a person's intent to interact with the agent, their attitude towards the agent and the action they will perform, from the agent's (egocentric) perspective. So we propose \emph{SocialEgoNet} - a graph-based spatiotemporal framework that exploits task dependencies through a hierarchical multitask learning approach. SocialEgoNet uses whole-body skeletons (keypoints from face, hands and body) extracted from only 1 second of video input for high inference speed. For evaluation, we augment an existing egocentric human-agent interaction dataset with new class labels and bounding box annotations. Extensive experiments on this augmented dataset, named JPL-Social, demonstrate \emph{real-time} inference and superior performance (average accuracy across all tasks: 83.15\%) of our model outperforming several competitive baselines. The additional annotations and code will be available upon acceptance.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Safe Multi-Agent Reinforcement Learning for Multi-Agent System</title>
<link>https://arxiv.org/abs/2501.13727</link>
<guid>https://arxiv.org/abs/2501.13727</guid>
<content:encoded><![CDATA[
<div> : (MAS), (MARL), , , 

:
(SS-MARL)MARLSS-MARLMASSS-MARL<br /><br /> <div>
arXiv:2501.13727v2 Announce Type: replace 
Abstract: Safety and scalability are two critical challenges faced by practical Multi-Agent Systems (MAS). However, existing Multi-Agent Reinforcement Learning (MARL) algorithms that rely solely on reward shaping are ineffective in ensuring safety, and their scalability is rather limited due to the fixed-size network output. To address these issues, we propose a novel framework, Scalable Safe MARL (SS-MARL), to enhance the safety and scalability of MARL methods. Leveraging the inherent graph structure of MAS, we design a multi-layer message passing network to aggregate local observations and communications of varying sizes. Furthermore, we develop a constrained joint policy optimization method in the setting of local observation to improve safety. Simulation experiments demonstrate that SS-MARL achieves a better trade-off between optimality and safety compared to baselines, and its scalability significantly outperforms the latest methods in scenarios with a large number of agents.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RG-Attn: Radian Glue Attention for Multi-modality Multi-agent Cooperative Perception</title>
<link>https://arxiv.org/abs/2501.16803</link>
<guid>https://arxiv.org/abs/2501.16803</guid>
<content:encoded><![CDATA[
<div> : V2XLiDARRadian-Glue-AttentionPaint-To-PuzzleCo-Sketching-Co-ColoringGitHub

<br /><br />:
V2XRadian-Glue-AttentionRG-AttnLiDARPaint-To-PuzzlePTPCo-Sketching-Co-ColoringCoS-CoCoLiDARLiDARGitHub <div>
arXiv:2501.16803v2 Announce Type: replace 
Abstract: Cooperative perception offers an optimal solution to overcome the perception limitations of single-agent systems by leveraging Vehicle-to-Everything (V2X) communication for data sharing and fusion across multiple agents. However, most existing approaches focus on single-modality data exchange, limiting the potential of both homogeneous and heterogeneous fusion across agents. This overlooks the opportunity to utilize multi-modality data per agent, restricting the system's performance. In the automotive industry, manufacturers adopt diverse sensor configurations, resulting in heterogeneous combinations of sensor modalities across agents. To harness the potential of every possible data source for optimal performance, we design a robust LiDAR and camera cross-modality fusion module, Radian-Glue-Attention (RG-Attn), applicable to both intra-agent cross-modality fusion and inter-agent cross-modality fusion scenarios, owing to the convenient coordinate conversion by transformation matrix and the unified sampling/inversion mechanism. We also propose two different architectures, named Paint-To-Puzzle (PTP) and Co-Sketching-Co-Coloring (CoS-CoCo), for conducting cooperative perception. PTP aims for maximum precision performance and achieves smaller data packet size by limiting cross-agent fusion to a single instance, but requiring all participants to be equipped with LiDAR. In contrast, CoS-CoCo supports agents with any configuration-LiDAR-only, camera-only, or LiDAR-camera-both, presenting more generalization ability. Our approach achieves state-of-the-art (SOTA) performance on both real and simulated cooperative perception datasets. The code is now available at GitHub.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Connectedness in weighted consensus division of graphical cakes between two agents</title>
<link>https://arxiv.org/abs/2312.12260</link>
<guid>https://arxiv.org/abs/2312.12260</guid>
<content:encoded><![CDATA[
<div> Austin's moving knife procedure

:
-h+1r+2 <div>
arXiv:2312.12260v2 Announce Type: replace-cross 
Abstract: Austin's moving knife procedure was originally introduced to find a consensus division of a cake between two agents: each agent believes that they receive exactly half of the cake. Here, we adapt it to the case when the two agents have arbitrary entitlements of the cake and seek a weighted consensus division -- one where each agent believes that they received exactly the share that they are entitled to -- which also minimizes the number of connected components that each agent receives. First, we find a weighted consensus division of a circular cake, which gives exactly one connected piece to each agent: this gives an explicit protocol for the Stromquist-Woodall theorem for two agents. Next, by judiciously mapping a circle to a graph, we produce a weighted consensus division of a star graph cake that gives at most two connected pieces to each agent -- and show that this bound on the number of connected pieces is tight. For trees, each agent receives at most h+1 connected pieces, where h is the height of the tree, and for general (connected) graphical cakes each agent receives r+2 connected pieces, where r is the radius of the graph.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeScientist: End-to-End Semi-Automated Scientific Discovery with Code-based Experimentation</title>
<link>https://arxiv.org/abs/2503.22708</link>
<guid>https://arxiv.org/abs/2503.22708</guid>
<content:encoded><![CDATA[
<div> CodeScientist

:<br />
CodeScientist ASD CodeScientist 196 <div>
arXiv:2503.22708v1 Announce Type: new 
Abstract: Despite the surge of interest in autonomous scientific discovery (ASD) of software artifacts (e.g., improved ML algorithms), current ASD systems face two key limitations: (1) they largely explore variants of existing codebases or similarly constrained design spaces, and (2) they produce large volumes of research artifacts (such as automatically generated papers and code) that are typically evaluated using conference-style paper review with limited evaluation of code. In this work we introduce CodeScientist, a novel ASD system that frames ideation and experiment construction as a form of genetic search jointly over combinations of research articles and codeblocks defining common actions in a domain (like prompting a language model). We use this paradigm to conduct hundreds of automated experiments on machine-generated ideas broadly in the domain of agents and virtual environments, with the system returning 19 discoveries, 6 of which were judged as being both at least minimally sound and incrementally novel after a multi-faceted evaluation beyond that typically conducted in prior work, including external (conference-style) review, code review, and replication attempts. Moreover, the discoveries span new tasks, agents, metrics, and data, suggesting a qualitative shift from benchmark optimization to broader discoveries.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-ABM for Transportation: Assessing the Potential of LLM Agents in System Analysis</title>
<link>https://arxiv.org/abs/2503.22718</link>
<guid>https://arxiv.org/abs/2503.22718</guid>
<content:encoded><![CDATA[
<div> : agent-based modelinglarge language models (LLM)transportation planningbehavioral soundnesssystem dynamics

<br /><br />:
LLMagent-basedLLMLLMLLM-agent-based <div>
arXiv:2503.22718v1 Announce Type: new 
Abstract: Agent-based modeling approaches represent the state-of-art in modeling travel demand and transportation system dynamics and are valuable tools for transportation planning. However, established agent-based approaches in transportation rely on multi-hierarchical mathematical models to simulate travel behavior, which faces theoretical and practical limitations. The advent of large language models (LLM) provides a new opportunity to refine agent-based modeling in transportation. LLM agents, which have impressive reasoning and planning abilities, can serve as a proxy of human travelers and be integrated into the modeling framework. However, despite evidence of their behavioral soundness, no existing studies have assessed the impact and validity of LLM-agent-based simulations from a system perspective in transportation. This paper aims to address this issue by designing and integrating LLM agents with human-traveler-like characteristics into a simulation of a transportation system and assessing its performance based on existing benchmarks. Using the classical transportation setting of the morning commute, we find that not only do the agents exhibit fine behavioral soundness, but also produce system dynamics that align well with standard benchmarks. Our analysis first verifies the effectiveness and potential of LLM-agent-based modeling for transportation planning on the system level.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping</title>
<link>https://arxiv.org/abs/2503.22723</link>
<guid>https://arxiv.org/abs/2503.22723</guid>
<content:encoded><![CDATA[
<div> <br /><br />:
LLMs LLM-HFBFLLMsLLM-HFBFLLMsAER28.4727.039LLMAER <div>
arXiv:2503.22723v1 Announce Type: new 
Abstract: Reinforcement learning often faces challenges with reward misalignment, where agents optimize for given rewards but fail to exhibit the desired behaviors. This occurs when the reward function incentivizes proxy behaviors that diverge from the true objective. While human-in-the-loop (HIL) methods can help, they may exacerbate the problem, as humans are prone to biases that lead to inconsistent, subjective, or misaligned feedback, complicating the learning process. To address these issues, we propose two key contributions. First, we extend the use of zero-shot, off-the-shelf large language models (LLMs) for reward shaping beyond natural language processing (NLP) to continuous control tasks. By leveraging LLMs as direct feedback providers, we replace surrogate models trained on human feedback, which often suffer from the bias inherent in the feedback data it is trained on. Second, we introduce a hybrid framework (LLM-HFBF) that enables LLMs to identify and correct biases in human feedback while incorporating this feedback into the reward shaping process. The LLM-HFBF framework creates a more balanced and reliable system by addressing both the limitations of LLMs (e.g., lack of domain-specific knowledge) and human supervision (e.g., inherent biases). By enabling human feedback bias flagging and correction, our approach improves reinforcement learning performance and reduces reliance on potentially biased human guidance. Empirical experiments show that biased human feedback significantly reduces performance, with average episodic reward (AER) dropping from 28.472 in (unbiased approaches) to 7.039 (biased with conservative bias). In contrast, LLM-based approaches maintain a matching AER like unbiased feedback, even in custom edge case scenarios.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfoBid: A Simulation Framework for Studying Information Disclosure in Auctions with Large Language Model-based Agents</title>
<link>https://arxiv.org/abs/2503.22726</link>
<guid>https://arxiv.org/abs/2503.22726</guid>
<content:encoded><![CDATA[
<div> InfoBid

:
InfoBidGPT-4InfoBidLLMInfoBidLLMs <div>
arXiv:2503.22726v1 Announce Type: new 
Abstract: In online advertising systems, publishers often face a trade-off in information disclosure strategies: while disclosing more information can enhance efficiency by enabling optimal allocation of ad impressions, it may lose revenue potential by decreasing uncertainty among competing advertisers. Similar to other challenges in market design, understanding this trade-off is constrained by limited access to real-world data, leading researchers and practitioners to turn to simulation frameworks. The recent emergence of large language models (LLMs) offers a novel approach to simulations, providing human-like reasoning and adaptability without necessarily relying on explicit assumptions about agent behavior modeling. Despite their potential, existing frameworks have yet to integrate LLM-based agents for studying information asymmetry and signaling strategies, particularly in the context of auctions. To address this gap, we introduce InfoBid, a flexible simulation framework that leverages LLM agents to examine the effects of information disclosure strategies in multi-agent auction settings. Using GPT-4o, we implemented simulations of second-price auctions with diverse information schemas. The results reveal key insights into how signaling influences strategic behavior and auction outcomes, which align with both economic and social learning theories. Through InfoBid, we hope to foster the use of LLMs as proxies for human economic and social agents in empirical studies, enhancing our understanding of their capabilities and limitations. This work bridges the gap between theoretical market designs and practical applications, advancing research in market simulations, information design, and agent-based reasoning while offering a valuable tool for exploring the dynamics of digital economies.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Large-Scale Vision-Language Dataset Derived from Open Scientific Literature to Advance Biomedical Generalist AI</title>
<link>https://arxiv.org/abs/2503.22727</link>
<guid>https://arxiv.org/abs/2503.22727</guid>
<content:encoded><![CDATA[
<div> BiomedicaPubMed Central

<br /><br />:
BiomedicaPubMed Central Open Access6002400-27APIwebAIBiomedicaAI <div>
arXiv:2503.22727v1 Announce Type: new 
Abstract: Despite the excitement behind biomedical artificial intelligence (AI), access to high-quality, diverse, and large-scale data - the foundation for modern AI systems - is still a bottleneck to unlocking its full potential. To address this gap, we introduce Biomedica, an open-source dataset derived from the PubMed Central Open Access subset, containing over 6 million scientific articles and 24 million image-text pairs, along with 27 metadata fields (including expert human annotations). To overcome the challenges of accessing our large-scale dataset, we provide scalable streaming and search APIs through a web server, facilitating seamless integration with AI systems. We demonstrate the utility of the Biomedica dataset by building embedding models, chat-style models, and retrieval-augmented chat agents. Notably, all our AI models surpass previous open systems in their respective categories, underscoring the critical role of diverse, high-quality, and large-scale biomedical data.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShieldAgent: Shielding Agents via Verifiable Safety Policy Reasoning</title>
<link>https://arxiv.org/abs/2503.22738</link>
<guid>https://arxiv.org/abs/2503.22738</guid>
<content:encoded><![CDATA[
<div> Autonomous agents, Foundation models, Safety policy, ShieldAgent, ShieldAgent-Bench

:<br />
ShieldAgentShieldAgentShieldAgentShieldAgent-Bench673KShieldAgentShieldAgent-Bench11.3%90.1%ShieldAgentAPI64.7%58.2% <div>
arXiv:2503.22738v1 Announce Type: new 
Abstract: Autonomous agents powered by foundation models have seen widespread adoption across various real-world applications. However, they remain highly vulnerable to malicious instructions and attacks, which can result in severe consequences such as privacy breaches and financial losses. More critically, existing guardrails for LLMs are not applicable due to the complex and dynamic nature of agents. To tackle these challenges, we propose ShieldAgent, the first guardrail agent designed to enforce explicit safety policy compliance for the action trajectory of other protected agents through logical reasoning. Specifically, ShieldAgent first constructs a safety policy model by extracting verifiable rules from policy documents and structuring them into a set of action-based probabilistic rule circuits. Given the action trajectory of the protected agent, ShieldAgent retrieves relevant rule circuits and generates a shielding plan, leveraging its comprehensive tool library and executable code for formal verification. In addition, given the lack of guardrail benchmarks for agents, we introduce ShieldAgent-Bench, a dataset with 3K safety-related pairs of agent instructions and action trajectories, collected via SOTA attacks across 6 web environments and 7 risk categories. Experiments show that ShieldAgent achieves SOTA on ShieldAgent-Bench and three existing benchmarks, outperforming prior methods by 11.3% on average with a high recall of 90.1%. Additionally, ShieldAgent reduces API queries by 64.7% and inference time by 58.2%, demonstrating its high precision and efficiency in safeguarding agents.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Optimization and Multi-agent Reinforcement Learning for Mean-variance Team Stochastic Games</title>
<link>https://arxiv.org/abs/2503.22779</link>
<guid>https://arxiv.org/abs/2503.22779</guid>
<content:encoded><![CDATA[
<div> mean-variance (MV-TSG) (MV-MATRPO)

:

MV-TSGMV-TSGMV-MAPIMV-MAPIMV-TSGMV-MAPIMV-MATRPO <div>
arXiv:2503.22779v1 Announce Type: new 
Abstract: We study a long-run mean-variance team stochastic game (MV-TSG), where each agent shares a common mean-variance objective for the system and takes actions independently to maximize it. MV-TSG has two main challenges. First, the variance metric is neither additive nor Markovian in a dynamic setting. Second, simultaneous policy updates of all agents lead to a non-stationary environment for each individual agent. Both challenges make dynamic programming inapplicable. In this paper, we study MV-TSGs from the perspective of sensitivity-based optimization. The performance difference and performance derivative formulas for joint policies are derived, which provide optimization information for MV-TSGs. We prove the existence of a deterministic Nash policy for this problem. Subsequently, we propose a Mean-Variance Multi-Agent Policy Iteration (MV-MAPI) algorithm with a sequential update scheme, where individual agent policies are updated one by one in a given order. We prove that the MV-MAPI algorithm converges to a first-order stationary point of the objective function. By analyzing the local geometry of stationary points, we derive specific conditions for stationary points to be (local) Nash equilibria, and further, strict local optima. To solve large-scale MV-TSGs in scenarios with unknown environmental parameters, we extend the idea of trust region methods to MV-MAPI and develop a multi-agent reinforcement learning algorithm named Mean-Variance Multi-Agent Trust Region Policy Optimization (MV-MATRPO). We derive a performance lower bound for each update of joint policies. Finally, numerical experiments on energy management in multiple microgrid systems are conducted.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Markov Potential Game Construction and Multi-Agent Reinforcement Learning with Applications to Autonomous Driving</title>
<link>https://arxiv.org/abs/2503.22867</link>
<guid>https://arxiv.org/abs/2503.22867</guid>
<content:encoded><![CDATA[
<div> : Markov(MGs), (MARL), Markov(MPGs), (NE), 

:
MGsMGNEMGMPGsMARLNEMPGsMDPMGMPG<br /><br /> <div>
arXiv:2503.22867v1 Announce Type: new 
Abstract: Markov games (MGs) serve as the mathematical foundation for multi-agent reinforcement learning (MARL), enabling self-interested agents to learn their optimal policies while interacting with others in a shared environment. However, due to the complexities of an MG problem, seeking (Markov perfect) Nash equilibrium (NE) is often very challenging for a general-sum MG. Markov potential games (MPGs), which are a special class of MGs, have appealing properties such as guaranteed existence of pure NEs and guaranteed convergence of gradient play algorithms, thereby leading to desirable properties for many MARL algorithms in their NE-seeking processes. However, the question of how to construct MPGs has been open. This paper provides sufficient conditions on the reward design and on the Markov decision process (MDP), under which an MG is an MPG. Numerical results on autonomous driving applications are reported.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attitude Synchronization for Multi-Agent Systems on SO(3) Using Vector Measurements</title>
<link>https://arxiv.org/abs/2503.22870</link>
<guid>https://arxiv.org/abs/2503.22870</guid>
<content:encoded><![CDATA[
<div> arXiv:2503.22870v1leaderlessSO(3)<br /><br />:<br />
SO(3)SO(3) <div>
arXiv:2503.22870v1 Announce Type: new 
Abstract: In this paper, we address the problem of leaderless attitude synchronization for a group of rigid body systems evolving on SO(3), relying on local measurements of some inertial (unit-length) vectors. The interaction graph among agents is assumed to be undirected, acyclic, and connected. We first present a distributed attitude synchronization scheme designed at the kinematic level of SO(3), followed by an extended scheme designed at the dynamic level. Both schemes are supported by a rigorous stability analysis, which establishes their almost global asymptotic stability properties. Finally, numerical simulations demonstrate the effectiveness of both distributed attitude synchronization schemes.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Inequality of LLM Fact-Checking over Geographic Regions with Agent and Retrieval models</title>
<link>https://arxiv.org/abs/2503.22877</link>
<guid>https://arxiv.org/abs/2503.22877</guid>
<content:encoded><![CDATA[
<div> Wikipedia

:
LLMs600WikipediaLLMLLMGPT-4Claude SonnetLLaMAWikipediaLLMs <div>
arXiv:2503.22877v1 Announce Type: new 
Abstract: Fact-checking is a potentially useful application of Large Language Models (LLMs) to combat the growing dissemination of disinformation. However, the performance of LLMs varies across geographic regions. In this paper, we evaluate the factual accuracy of open and private models across a diverse set of regions and scenarios.
  Using a dataset containing 600 fact-checked statements balanced across six global regions we examine three experimental setups of fact-checking a statement: (1) when just the statement is available, (2) when an LLM-based agent with Wikipedia access is utilized, and (3) as a best case scenario when a Retrieval-Augmented Generation (RAG) system provided with the official fact check is employed. Our findings reveal that regardless of the scenario and LLM used, including GPT-4, Claude Sonnet, and LLaMA, statements from the Global North perform substantially better than those from the Global South. Furthermore, this gap is broadened for the more realistic case of a Wikipedia agent-based system, highlighting that overly general knowledge bases have a limited ability to address region-specific nuances. These results underscore the urgent need for better dataset balancing and robust retrieval strategies to enhance LLM fact-checking capabilities, particularly in geographically diverse contexts.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task Tokens: A Flexible Approach to Adapting Behavior Foundation Models</title>
<link>https://arxiv.org/abs/2503.22886</link>
<guid>https://arxiv.org/abs/2503.22886</guid>
<content:encoded><![CDATA[
<div> : (BFMs)

<br />
:
TransformerBFMspromptBFMtransformerBFMpromptBFMBFMs <div>
arXiv:2503.22886v1 Announce Type: new 
Abstract: Recent advancements in imitation learning have led to transformer-based behavior foundation models (BFMs) that enable multi-modal, human-like control for humanoid agents. While excelling at zero-shot generation of robust behaviors, BFMs often require meticulous prompt engineering for specific tasks, potentially yielding suboptimal results. We introduce "Task Tokens", a method to effectively tailor BFMs to specific tasks while preserving their flexibility. Our approach leverages the transformer architecture of BFMs to learn a new task-specific encoder through reinforcement learning, keeping the original BFM frozen. This allows incorporation of user-defined priors, balancing reward design and prompt engineering. By training a task encoder to map observations to tokens, used as additional BFM inputs, we guide performance improvement while maintaining the model's diverse control characteristics. We demonstrate Task Tokens' efficacy across various tasks, including out-of-distribution scenarios, and show their compatibility with other prompting modalities. Our results suggest that Task Tokens offer a promising approach for adapting BFMs to specific control tasks while retaining their generalization capabilities.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Factored Agents: Decoupling In-Context Learning and Memorization for Robust Tool Use</title>
<link>https://arxiv.org/abs/2503.22931</link>
<guid>https://arxiv.org/abs/2503.22931</guid>
<content:encoded><![CDATA[
<div> 

:
AI<br />
1. LLM<br />
2. APIAI <div>
arXiv:2503.22931v1 Announce Type: new 
Abstract: In this paper, we propose a novel factored agent architecture designed to overcome the limitations of traditional single-agent systems in agentic AI. Our approach decomposes the agent into two specialized components: (1) a large language model (LLM) that serves as a high level planner and in-context learner, which may use dynamically available information in user prompts, (2) a smaller language model which acts as a memorizer of tool format and output. This decoupling addresses prevalent issues in monolithic designs, including malformed, missing, and hallucinated API fields, as well as suboptimal planning in dynamic environments. Empirical evaluations demonstrate that our factored architecture significantly improves planning accuracy and error resilience, while elucidating the inherent trade-off between in-context learning and static memorization. These findings suggest that a factored approach is a promising pathway for developing more robust and adaptable agentic AI systems.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Late Breaking Results: Breaking Symmetry- Unconventional Placement of Analog Circuits using Multi-Level Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.22958</link>
<guid>https://arxiv.org/abs/2503.22958</guid>
<content:encoded><![CDATA[
<div> layout-dependent effectsanalogQ

:<br />
layout-dependent effectsQ <div>
arXiv:2503.22958v1 Announce Type: new 
Abstract: Layout-dependent effects (LDEs) significantly impact analog circuit performance. Traditionally, designers have relied on symmetric placement of circuit components to mitigate variations caused by LDEs. However, due to non-linear nature of these effects, conventional methods often fall short. We propose an objective-driven, multi-level, multi-agent Q-learning framework to explore unconventional design space of analog layout, opening new avenues for optimizing analog circuit performance. Our approach achieves better variation performance than the state-of-the-art layout techniques. Notably, this is the first application of multi-agent RL in analog layout automation. The proposed approach is compared with non-ML approach based on simulated annealing.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FindTheFlaws: Annotated Errors for Detecting Flawed Reasoning and Scalable Oversight Research</title>
<link>https://arxiv.org/abs/2503.22989</link>
<guid>https://arxiv.org/abs/2503.22989</guid>
<content:encoded><![CDATA[
<div> : AI-FindTheFlaws

:
AI-AIFindTheFlawsLojban/ <div>
arXiv:2503.22989v1 Announce Type: new 
Abstract: As AI models tackle increasingly complex problems, ensuring reliable human oversight becomes more challenging due to the difficulty of verifying solutions. Approaches to scaling AI supervision include debate, in which two agents engage in structured dialogue to help a judge evaluate claims; critique, in which models identify potential flaws in proposed solutions; and prover-verifier games, in which a capable 'prover' model generates solutions that must be verifiable by a less capable 'verifier'. Evaluations of the scalability of these and similar approaches to difficult problems benefit from datasets that include (1) long-form expert-verified correct solutions and (2) long-form flawed solutions with annotations highlighting specific errors, but few are available.
  To address this gap, we present FindTheFlaws, a group of five diverse datasets spanning medicine, mathematics, science, coding, and the Lojban language. Each dataset contains questions and long-form solutions with expert annotations validating their correctness or identifying specific error(s) in the reasoning. We evaluate frontier models' critiquing capabilities and observe a range of performance that can be leveraged for scalable oversight experiments: models performing more poorly on particular datasets can serve as judges/verifiers for more capable models. Additionally, for some task/dataset combinations, expert baselines exceed even top model performance, making them more beneficial for scalable oversight experiments.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Large Language Models, a survey</title>
<link>https://arxiv.org/abs/2503.23037</link>
<guid>https://arxiv.org/abs/2503.23037</guid>
<content:encoded><![CDATA[
<div> LLMs

:<br />
LLMsagentic LLMsagentic LLMsLLMLLMsLLMagentic LLMs <div>
arXiv:2503.23037v1 Announce Type: new 
Abstract: There is great interest in agentic LLMs, large language models that act as agents. We review the growing body of work in this area and provide a research agenda. Agentic LLMs are LLMs that (1) reason, (2) act, and (3) interact. We organize the literature according to these three categories. The research in the first category focuses on reasoning, reflection, and retrieval, aiming to improve decision making; the second category focuses on action models, robots, and tools, aiming for agents that act as useful assistants; the third category focuses on multi-agent systems, aiming for collaborative task solving and simulating interaction to study emergent social behavior. We find that works mutually benefit from results in other categories: retrieval enables tool use, reflection improves multi-agent collaboration, and reasoning benefits all categories. We discuss applications of agentic LLMs and provide an agenda for further research. Important applications are in medical diagnosis, logistics and financial market analysis. Meanwhile, self-reflective agents playing roles and interacting with one another augment the process of scientific research itself. Further, agentic LLMs may provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets. We note that there is risk associated with LLM assistants taking action in the real world, while agentic LLMs are also likely to benefit society.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Inference for Large Reasoning Models: A Survey</title>
<link>https://arxiv.org/abs/2503.23077</link>
<guid>https://arxiv.org/abs/2503.23077</guid>
<content:encoded><![CDATA[
<div> : , , , , 

:
LRMs(a) CoT(b) CoTLRMsGitHub <div>
arXiv:2503.23077v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) significantly improve the reasoning ability of Large Language Models (LLMs) by learning to reason, exhibiting promising performance in complex task-solving. However, their deliberative reasoning process leads to inefficiencies in token usage, memory consumption, and inference time. Thus, this survey provides a review of efficient inference methods designed specifically for LRMs, focusing on mitigating token inefficiency while preserving the reasoning quality. First, we introduce a taxonomy to group the recent methods into two main categories: (a) explicit compact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit reasoning structure, and (b) implicit latent CoT, which encodes reasoning steps within hidden representations instead of explicit tokens. Meanwhile, we discuss their strengths and weaknesses. Then, we conduct empirical analyses on existing methods from performance and efficiency aspects. Besides, we present open challenges in this field, including human-centric controllable reasoning, trade-off between interpretability and efficiency of reasoning, ensuring safety of efficient reasoning, and broader applications of efficient reasoning. In addition, we highlight key insights for enhancing LRMs' inference efficiency via techniques such as model merging, new architectures, and agent routers. We hope this work serves as a valuable guide, helping researchers overcome challenges in this vibrant field\footnote{https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs}.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EncGPT: A Multi-Agent Workflow for Dynamic Encryption Algorithms</title>
<link>https://arxiv.org/abs/2503.23138</link>
<guid>https://arxiv.org/abs/2503.23138</guid>
<content:encoded><![CDATA[
<div> EncGPT

:
EncGPTLLMEncGPTLLMGPT-4o15.99 <div>
arXiv:2503.23138v1 Announce Type: new 
Abstract: Communication encryption is crucial in computer technology, but existing algorithms struggle with balancing cost and security. We propose EncGPT, a multi-agent framework using large language models (LLM). It includes rule, encryption, and decryption agents that generate encryption rules and apply them dynamically. This approach addresses gaps in LLM-based multi-agent systems for communication security. We tested GPT-4o's rule generation and implemented a substitution encryption workflow with homomorphism preservation, achieving an average execution time of 15.99 seconds.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis</title>
<link>https://arxiv.org/abs/2503.23145</link>
<guid>https://arxiv.org/abs/2503.23145</guid>
<content:encoded><![CDATA[
<div> inductive program synthesislarge language modelCodeARCbenchmarkLLaMA

:<br />
CodeARCinductive program synthesisCodeARCOracle111418o3-mini52.7%LLaMA-3.1-8B-Instruct31%CodeARC <div>
arXiv:2503.23145v1 Announce Type: new 
Abstract: Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by natural language, their ability to perform inductive program synthesis is underexplored. Existing evaluation protocols rely on static sets of examples and held-out tests, offering no feedback when synthesized functions are incorrect and failing to reflect real-world scenarios such as reverse engineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge, a new evaluation framework where agents interact with a hidden target function by querying it with new inputs, synthesizing candidate functions, and iteratively refining their solutions using a differential testing oracle. This interactive setting encourages agents to perform function calls and self-correction based on feedback. We construct the first large-scale benchmark for general-purpose inductive program synthesis, featuring 1114 functions. Among 18 models evaluated, o3-mini performs best with a success rate of 52.7%, highlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on curated synthesis traces yields up to a 31% relative performance gain. CodeARC provides a more realistic and challenging testbed for evaluating LLM-based program synthesis and inductive reasoning.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-Based Modeling and Deep Neural Networks for Establishing Digital Twins of Secure Facilities under Sensing Restrictions</title>
<link>https://arxiv.org/abs/2503.23147</link>
<guid>https://arxiv.org/abs/2503.23147</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />
Patterns of LifePOLOak RidgeMetaPOLagent-based modelABMdeep neural networksNPCsABMNPCNPC <div>
arXiv:2503.23147v1 Announce Type: new 
Abstract: Digital twin technologies help practitioners simulate, monitor, and predict undesirable outcomes in-silico, while avoiding the cost and risks of conducting live simulation exercises. Virtual reality (VR) based digital twin technologies are especially useful when monitoring human Patterns of Life (POL) in secure nuclear facilities, where live simulation exercises are too dangerous and costly to ever perform. However, the high-security status of such facilities may restrict modelers from deploying human activity sensors for data collection. This problem was encountered when deploying MetaPOL, a digital twin system to prevent insider threat or sabotage of secure facilities, at a secure nuclear reactor facility at Oak Ridge National Laboratory (ORNL). This challenge was addressed using an agent-based model (ABM), driven by anecdotal evidence of facility personnel POL, to generate synthetic movement trajectories. These synthetic trajectories were then used to train deep neural network surrogates for next location and stay duration prediction to drive NPCs in the VR environment. In this study, we evaluate the efficacy of this technique for establishing NPC movement within MetaPOL and the ability to distinguish NPC movement during normal operations from that during a simulated emergency response. Our results demonstrate the success of using a multi-layer perceptron for next location prediction and mixture density network for stay duration prediction to predict the ABM generated trajectories. We also find that NPC movement in the VR environment driven by the deep neural networks under normal operations remain significantly different to that seen when simulating responses to a simulated emergency scenario.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AstroAgents: A Multi-Agent AI for Hypothesis Generation from Mass Spectrometry Data</title>
<link>https://arxiv.org/abs/2503.23170</link>
<guid>https://arxiv.org/abs/2503.23170</guid>
<content:encoded><![CDATA[
<div> AstroAgentsAI

:<br />
AstroAgentsAstroAgents36%66%https://astroagents.github.io/ <div>
arXiv:2503.23170v1 Announce Type: new 
Abstract: With upcoming sample return missions across the solar system and the increasing availability of mass spectrometry data, there is an urgent need for methods that analyze such data within the context of existing astrobiology literature and generate plausible hypotheses regarding the emergence of life on Earth. Hypothesis generation from mass spectrometry data is challenging due to factors such as environmental contaminants, the complexity of spectral peaks, and difficulties in cross-matching these peaks with prior studies. To address these challenges, we introduce AstroAgents, a large language model-based, multi-agent AI system for hypothesis generation from mass spectrometry data. AstroAgents is structured around eight collaborative agents: a data analyst, a planner, three domain scientists, an accumulator, a literature reviewer, and a critic. The system processes mass spectrometry data alongside user-provided research papers. The data analyst interprets the data, and the planner delegates specific segments to the scientist agents for in-depth exploration. The accumulator then collects and deduplicates the generated hypotheses, and the literature reviewer identifies relevant literature using Semantic Scholar. The critic evaluates the hypotheses, offering rigorous suggestions for improvement. To assess AstroAgents, an astrobiology expert evaluated the novelty and plausibility of more than a hundred hypotheses generated from data obtained from eight meteorites and ten soil samples. Of these hypotheses, 36% were identified as plausible, and among those, 66% were novel. Project website: https://astroagents.github.io/
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative VCG-based Mechanism Fosters Cooperation in Multi-Regional Network Design</title>
<link>https://arxiv.org/abs/2503.23255</link>
<guid>https://arxiv.org/abs/2503.23255</guid>
<content:encoded><![CDATA[
<div> Vickery-Clarke-Groves

:<br />
Vickery-Clarke-GrovesVCG <div>
arXiv:2503.23255v1 Announce Type: new 
Abstract: Transportation network design often involves multiple stakeholders with diverse priorities. We consider a system with a hierarchical multi-agent structure, featuring self-optimized subnetwork operators at the lower level and a central organization at the upper level. Independent regional planning can lead to inefficiencies due to the lack of coordination, hindering interregional travel and cross-border infrastructure development, while centralized methods may struggle to align local interests and can be impractical to implement. To support decision making for such a system, we introduce an iterative VCG-based mechanism for multi-regional network design that fosters cooperation among subnetwork operators. By leveraging the Vickery-Clarke-Groves (VCG) mechanism, the framework determines collective investment decisions and the necessary payments from both operators and the central organization to achieve efficient outcomes. A case study on the European Railway System validates the effectiveness of the proposed method, demonstrating significant improvements in overall network performance through enhanced cross-region cooperation.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Coordinated Bimanual Manipulation Policies using State Diffusion and Inverse Dynamics Models</title>
<link>https://arxiv.org/abs/2503.23271</link>
<guid>https://arxiv.org/abs/2503.23271</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:
 <div>
arXiv:2503.23271v1 Announce Type: new 
Abstract: When performing tasks like laundry, humans naturally coordinate both hands to manipulate objects and anticipate how their actions will change the state of the clothes. However, achieving such coordination in robotics remains challenging due to the need to model object movement, predict future states, and generate precise bimanual actions. In this work, we address these challenges by infusing the predictive nature of human manipulation strategies into robot imitation learning. Specifically, we disentangle task-related state transitions from agent-specific inverse dynamics modeling to enable effective bimanual coordination. Using a demonstration dataset, we train a diffusion model to predict future states given historical observations, envisioning how the scene evolves. Then, we use an inverse dynamics model to compute robot actions that achieve the predicted states. Our key insight is that modeling object movement can help learning policies for bimanual coordination manipulation tasks. Evaluating our framework across diverse simulation and real-world manipulation setups, including multimodal goal configurations, bimanual manipulation, deformable objects, and multi-object setups, we find that it consistently outperforms state-of-the-art state-to-action mapping policies. Our method demonstrates a remarkable capacity to navigate multimodal goal configurations and action distributions, maintain stability across different control modes, and synthesize a broader range of behaviors than those present in the demonstration dataset.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Twin Migration in Vehicular Metaverses: Multi-Agent Split Deep Reinforcement Learning with Spatio-Temporal Trajectory Generation</title>
<link>https://arxiv.org/abs/2503.23290</link>
<guid>https://arxiv.org/abs/2503.23290</guid>
<content:encoded><![CDATA[
<div> Vehicle Twins (VTs), , , ,  (DRL)

:
(DRL)(VTs)(RSUs)RSUsVTDRLVT(QoE)29%25% <div>
arXiv:2503.23290v1 Announce Type: new 
Abstract: Vehicle Twins (VTs) as digital representations of vehicles can provide users with immersive experiences in vehicular metaverse applications, e.g., Augmented Reality (AR) navigation and embodied intelligence. VT migration is an effective way that migrates the VT when the locations of physical entities keep changing to maintain seamless immersive VT services. However, an efficient VT migration is challenging due to the rapid movement of vehicles, dynamic workloads of Roadside Units (RSUs), and heterogeneous resources of the RSUs. To achieve efficient migration decisions and a minimum latency for the VT migration, we propose a multi-agent split Deep Reinforcement Learning (DRL) framework combined with spatio-temporal trajectory generation. In this framework, multiple split DRL agents utilize split architecture to efficiently determine VT migration decisions. Furthermore, we propose a spatio-temporal trajectory generation algorithm based on trajectory datasets and road network data to simulate vehicle trajectories, enhancing the generalization of the proposed scheme for managing VT migration in dynamic network environments. Finally, experimental results demonstrate that the proposed scheme not only enhances the Quality of Experience (QoE) by 29% but also reduces the computational parameter count by approximately 25% while maintaining similar performances, enhancing users' immersive experiences in vehicular metaverses.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRASP: Municipal Budget AI Chatbots for Enhancing Civic Engagement</title>
<link>https://arxiv.org/abs/2503.23299</link>
<guid>https://arxiv.org/abs/2503.23299</guid>
<content:encoded><![CDATA[
<div> GRASPAI

:<br />
GRASPAIGRASPRAGGRASP78%GPT-4Gemini60%35%GRASP <div>
arXiv:2503.23299v1 Announce Type: new 
Abstract: There are a growing number of AI applications, but none tailored specifically to help residents answer their questions about municipal budget, a topic most are interested in but few have a solid comprehension of. In this research paper, we propose GRASP, a custom AI chatbot framework which stands for Generation with Retrieval and Action System for Prompts. GRASP provides more truthful and grounded responses to user budget queries than traditional information retrieval systems like general Large Language Models (LLMs) or web searches. These improvements come from the novel combination of a Retrieval-Augmented Generation (RAG) framework ("Generation with Retrieval") and an agentic workflow ("Action System"), as well as prompt engineering techniques, the incorporation of municipal budget domain knowledge, and collaboration with local town officials to ensure response truthfulness. During testing, we found that our GRASP chatbot provided precise and accurate responses for local municipal budget queries 78% of the time, while GPT-4o and Gemini were only accurate 60% and 35% of the time, respectively. GRASP chatbots greatly reduce the time and effort needed for the general public to get an intuitive and correct understanding of their town's budget, thus fostering greater communal discourse, improving government transparency, and allowing citizens to make more informed decisions.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPIO: Ensemble and Selective Strategies via LLM-Based Multi-Agent Planning in Automated Data Science</title>
<link>https://arxiv.org/abs/2503.23314</link>
<guid>https://arxiv.org/abs/2503.23314</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)SPIO

:
SPIOSPIOLLMSPIOSPIO-SLLMSPIO-EkSPIOKaggleOpenML <div>
arXiv:2503.23314v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have revolutionized automated data analytics and machine learning by enabling dynamic reasoning and adaptability. While recent approaches have advanced multi-stage pipelines through multi-agent systems, they typically rely on rigid, single-path workflows that limit the exploration and integration of diverse strategies, often resulting in suboptimal predictions. To address these challenges, we propose SPIO (Sequential Plan Integration and Optimization), a novel framework that leverages LLM-driven decision-making to orchestrate multi-agent planning across four key modules: data preprocessing, feature engineering, modeling, and hyperparameter tuning. In each module, dedicated planning agents independently generate candidate strategies that cascade into subsequent stages, fostering comprehensive exploration. A plan optimization agent refines these strategies by suggesting several optimized plans. We further introduce two variants: SPIO-S, which selects a single best solution path as determined by the LLM, and SPIO-E, which selects the top k candidate plans and ensembles them to maximize predictive performance. Extensive experiments on Kaggle and OpenML datasets demonstrate that SPIO significantly outperforms state-of-the-art methods, providing a robust and scalable solution for automated data science task.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Agents in Engineering Design: A Multi-Agent Framework for Aesthetic and Aerodynamic Car Design</title>
<link>https://arxiv.org/abs/2503.23315</link>
<guid>https://arxiv.org/abs/2503.23315</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />
AI3DCFDVLMsLLMs <div>
arXiv:2503.23315v1 Announce Type: new 
Abstract: We introduce the concept of "Design Agents" for engineering applications, particularly focusing on the automotive design process, while emphasizing that our approach can be readily extended to other engineering and design domains. Our framework integrates AI-driven design agents into the traditional engineering workflow, demonstrating how these specialized computational agents interact seamlessly with engineers and designers to augment creativity, enhance efficiency, and significantly accelerate the overall design cycle. By automating and streamlining tasks traditionally performed manually, such as conceptual sketching, styling enhancements, 3D shape retrieval and generative modeling, computational fluid dynamics (CFD) meshing, and aerodynamic simulations, our approach reduces certain aspects of the conventional workflow from weeks and days down to minutes. These agents leverage state-of-the-art vision-language models (VLMs), large language models (LLMs), and geometric deep learning techniques, providing rapid iteration and comprehensive design exploration capabilities. We ground our methodology in industry-standard benchmarks, encompassing a wide variety of conventional automotive designs, and utilize high-fidelity aerodynamic simulations to ensure practical and applicable outcomes. Furthermore, we present design agents that can swiftly and accurately predict simulation outcomes, empowering engineers and designers to engage in more informed design optimization and exploration. This research underscores the transformative potential of integrating advanced generative AI techniques into complex engineering tasks, paving the way for broader adoption and innovation across multiple engineering disciplines.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Explainable Multi-player MCTS-minimax Hybrids in Board Game Using Process Mining</title>
<link>https://arxiv.org/abs/2503.23326</link>
<guid>https://arxiv.org/abs/2503.23326</guid>
<content:encoded><![CDATA[
<div> Monte-Carlo Tree Search (MCTS)

:
MCTSMCTSMCTS3v3MCTS <div>
arXiv:2503.23326v1 Announce Type: new 
Abstract: Monte-Carlo Tree Search (MCTS) is a family of sampling-based search algorithms widely used for online planning in sequential decision-making domains and at the heart of many recent advances in artificial intelligence. Understanding the behavior of MCTS agents is difficult for developers and users due to the frequently large and complex search trees that result from the simulation of many possible futures, their evaluations, and their relationships. This paper presents our ongoing investigation into potential explanations for the decision-making and behavior of MCTS. A weakness of MCTS is that it constructs a highly selective tree and, as a result, can miss crucial moves and fall into tactical traps. Full-width minimax search constitutes the solution. We integrate shallow minimax search into the rollout phase of multi-player MCTS and use process mining technique to explain agents' strategies in 3v3 checkers.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Capacity Planning for the Hospital-Residents Problem</title>
<link>https://arxiv.org/abs/2503.23328</link>
<guid>https://arxiv.org/abs/2503.23328</guid>
<content:encoded><![CDATA[
<div> : 

:
NPNPC <div>
arXiv:2503.23328v1 Announce Type: new 
Abstract: The Hospital Residents setting models important problems like school choice, assignment of undergraduate students to degree programs, among many others. In this setting, fixed quotas are associated with the programs that limit the number of agents that can be assigned to them. Motivated by scenarios where all agents must be matched, we propose and study a generalized capacity planning problem, which allows cost-controlled flexibility with respect to quotas.
  Our setting is an extension of the Hospital Resident setting where programs have the usual quota as well as an associated cost, indicating the cost of matching an agent beyond the initial quotas. We seek to compute a matching that matches all agents and is optimal with respect to preferences, and minimizes either a local or a global objective on cost.
  We show that there is a sharp contrast -- minimizing the local objective is polynomial-time solvable, whereas minimizing the global objective is NP-hard. On the positive side, we present approximation algorithms for the global objective in the general case and a particular hard case. We achieve the approximation guarantee for the special hard case via a linear programming based algorithm. We strengthen the NP-hardness by showing a matching lower bound to our algorithmic result.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Agent Framework with Automated Decision Rule Optimization for Cross-Domain Misinformation Detection</title>
<link>https://arxiv.org/abs/2503.23329</link>
<guid>https://arxiv.org/abs/2503.23329</guid>
<content:encoded><![CDATA[
<div> : 

:<br />
MAROMARO <div>
arXiv:2503.23329v1 Announce Type: new 
Abstract: Misinformation spans various domains, but detection methods trained on specific domains often perform poorly when applied to others. With the rapid development of Large Language Models (LLMs), researchers have begun to utilize LLMs for cross-domain misinformation detection. However, existing LLM-based methods often fail to adequately analyze news in the target domain, limiting their detection capabilities. More importantly, these methods typically rely on manually designed decision rules, which are limited by domain knowledge and expert experience, thus limiting the generalizability of decision rules to different domains. To address these issues, we propose a MultiAgent Framework for cross-domain misinformation detection with Automated Decision Rule Optimization (MARO). Under this framework, we first employs multiple expert agents to analyze target-domain news. Subsequently, we introduce a question-reflection mechanism that guides expert agents to facilitate higherquality analysis. Furthermore, we propose a decision rule optimization approach based on carefully-designed cross-domain validation tasks to iteratively enhance the effectiveness of decision rules in different domains. Experimental results and in-depth analysis on commonlyused datasets demonstrate that MARO achieves significant improvements over existing methods.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of WebAgents: Towards Next-Generation AI Agents for Web Automation with Large Foundation Models</title>
<link>https://arxiv.org/abs/2503.23350</link>
<guid>https://arxiv.org/abs/2503.23350</guid>
<content:encoded><![CDATA[
<div> : AIWebAgent

<br /><br />:
(AI)webWebAgent(LFMs)LFMsWebAgentwebWebAgent <div>
arXiv:2503.23350v1 Announce Type: new 
Abstract: With the advancement of web techniques, they have significantly revolutionized various aspects of people's lives. Despite the importance of the web, many tasks performed on it are repetitive and time-consuming, negatively impacting overall quality of life. To efficiently handle these tedious daily tasks, one of the most promising approaches is to advance autonomous agents based on Artificial Intelligence (AI) techniques, referred to as AI Agents, as they can operate continuously without fatigue or performance degradation. In the context of the web, leveraging AI Agents -- termed WebAgents -- to automatically assist people in handling tedious daily tasks can dramatically enhance productivity and efficiency. Recently, Large Foundation Models (LFMs) containing billions of parameters have exhibited human-like language understanding and reasoning capabilities, showing proficiency in performing various complex tasks. This naturally raises the question: `Can LFMs be utilized to develop powerful AI Agents that automatically handle web tasks, providing significant convenience to users?' To fully explore the potential of LFMs, extensive research has emerged on WebAgents designed to complete daily web tasks according to user instructions, significantly enhancing the convenience of daily human life. In this survey, we comprehensively review existing research studies on WebAgents across three key aspects: architectures, training, and trustworthiness. Additionally, several promising directions for future research are explored to provide deeper insights.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RuleAgent: Discovering Rules for Recommendation Denoising with Autonomous Language Agents</title>
<link>https://arxiv.org/abs/2503.23374</link>
<guid>https://arxiv.org/abs/2503.23374</guid>
<content:encoded><![CDATA[
<div> RuleAgentLossEraser

:
RuleAgentRuleAgentLossEraserRuleAgent <div>
arXiv:2503.23374v1 Announce Type: new 
Abstract: The implicit feedback (e.g., clicks) in real-world recommender systems is often prone to severe noise caused by unintentional interactions, such as misclicks or curiosity-driven behavior. A common approach to denoising this feedback is manually crafting rules based on observations of training loss patterns. However, this approach is labor-intensive and the resulting rules often lack generalization across diverse scenarios. To overcome these limitations, we introduce RuleAgent, a language agent based framework which mimics real-world data experts to autonomously discover rules for recommendation denoising. Unlike the high-cost process of manual rule mining, RuleAgent offers rapid and dynamic rule discovery, ensuring adaptability to evolving data and varying scenarios. To achieve this, RuleAgent is equipped with tailored profile, memory, planning, and action modules and leverages reflection mechanisms to enhance its reasoning capabilities for rule discovery. Furthermore, to avoid the frequent retraining in rule discovery, we propose LossEraser-an unlearning strategy that streamlines training without compromising denoising performance. Experiments on benchmark datasets demonstrate that, compared with existing denoising methods, RuleAgent not only derives the optimal recommendation performance but also produces generalizable denoising rules, assisting researchers in efficient data cleaning.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Analysis of Decoding Methods for LLM-based Agents for Faithful Multi-Hop Question Answering</title>
<link>https://arxiv.org/abs/2503.23415</link>
<guid>https://arxiv.org/abs/2503.23415</guid>
<content:encoded><![CDATA[
<div> :  (LLMs), , , ReAct, 

<br />
:
LLMsReActagenticLLMsReActDeCoReDoLaCADLLMagenticHotpotQAReActDoLaF119.532.6 <div>
arXiv:2503.23415v1 Announce Type: new 
Abstract: Large Language Models (LLMs) frequently produce factually inaccurate outputs - a phenomenon known as hallucination - which limits their accuracy in knowledge-intensive NLP tasks. Retrieval-augmented generation and agentic frameworks such as Reasoning and Acting (ReAct) can address this issue by giving the model access to external knowledge. However, LLMs often fail to remain faithful to retrieved information. Mitigating this is critical, especially if LLMs are required to reason about the retrieved information. Recent research has explored training-free decoding strategies to improve the faithfulness of model generations. We present a systematic analysis of how the combination of the ReAct framework and decoding strategies (i.e., DeCoRe, DoLa, and CAD) can influence the faithfulness of LLM-generated answers. Our results show that combining an agentic framework for knowledge retrieval with decoding methods that enhance faithfulness can increase accuracy on the downstream Multi-Hop Question Answering tasks. For example, we observe an F1 increase from 19.5 to 32.6 on HotpotQA when using ReAct and DoLa.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-agent Onboarding Assistant based on Large Language Models, Retrieval Augmented Generation, and Chain-of-Thought</title>
<link>https://arxiv.org/abs/2503.23421</link>
<guid>https://arxiv.org/abs/2503.23421</guid>
<content:encoded><![CDATA[
<div> Onboarding Buddy

:
Onboarding BuddyGitHub CopilotOnboarding Buddy3.260.863.00.96Onboarding Buddy <div>
arXiv:2503.23421v1 Announce Type: new 
Abstract: Effective onboarding in software engineering is crucial but difficult due to the fast-paced evolution of technologies. Traditional methods, like exploration and workshops, are costly, time-consuming, and quickly outdated in large projects. We propose the Onboarding Buddy system, which leverages large language models, retrieval augmented generation, and an automated chain-of-thought approach to improve onboarding. It integrates dynamic, context-specific support within the development environment, offering natural language explanations, code insights, and project guidance. Our solution is agent-based and provides customized assistance with minimal human intervention. Our study results among the eight participants show an average helpfulness rating of (M=3.26, SD=0.86) and ease of onboarding at (M=3.0, SD=0.96) out of four. While similar to tools like GitHub Copilot, Onboarding Buddy uniquely integrates a chain-of-thought reasoning mechanism with retrieval-augmented generation, tailored specifically for dynamic onboarding contexts. While our initial evaluation is based on eight participants within one project, we will explore larger teams and multiple real-world codebases in the company to demonstrate broader applicability. Overall, Onboarding Buddy holds great potential for enhancing developer productivity and satisfaction. Our tool, source code, and demonstration video are publicly available
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Trustworthy GUI Agents: A Survey</title>
<link>https://arxiv.org/abs/2503.23434</link>
<guid>https://arxiv.org/abs/2503.23434</guid>
<content:encoded><![CDATA[
<div> : GUI

<br /><br />:
GUIWebGUIGUIGUI <div>
arXiv:2503.23434v1 Announce Type: new 
Abstract: GUI agents, powered by large foundation models, can interact with digital interfaces, enabling various applications in web automation, mobile navigation, and software testing. However, their increasing autonomy has raised critical concerns about their security, privacy, and safety. This survey examines the trustworthiness of GUI agents in five critical dimensions: security vulnerabilities, reliability in dynamic environments, transparency and explainability, ethical considerations, and evaluation methodologies. We also identify major challenges such as vulnerability to adversarial attacks, cascading failure modes in sequential decision-making, and a lack of realistic evaluation benchmarks. These issues not only hinder real-world deployment but also call for comprehensive mitigation strategies beyond task success. As GUI agents become more widespread, establishing robust safety standards and responsible development practices is essential. This survey provides a foundation for advancing trustworthy GUI agents through systematic understanding and future research.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoGen-Eval: Agent-based System for Video Generation Evaluation</title>
<link>https://arxiv.org/abs/2503.23452</link>
<guid>https://arxiv.org/abs/2503.23452</guid>
<content:encoded><![CDATA[
<div> VideoGen-EvalLLMMLLM

:
VideoGen-EvalLLMMLLM700T2VI2V2012,0008 <div>
arXiv:2503.23452v1 Announce Type: new 
Abstract: The rapid advancement of video generation has rendered existing evaluation systems inadequate for assessing state-of-the-art models, primarily due to simple prompts that cannot showcase the model's capabilities, fixed evaluation operators struggling with Out-of-Distribution (OOD) cases, and misalignment between computed metrics and human preferences. To bridge the gap, we propose VideoGen-Eval, an agent evaluation system that integrates LLM-based content structuring, MLLM-based content judgment, and patch tools designed for temporal-dense dimensions, to achieve a dynamic, flexible, and expandable video generation evaluation. Additionally, we introduce a video generation benchmark to evaluate existing cutting-edge models and verify the effectiveness of our evaluation system. It comprises 700 structured, content-rich prompts (both T2V and I2V) and over 12,000 videos generated by 20+ models, among them, 8 cutting-edge models are selected as quantitative evaluation for the agent and human. Extensive experiments validate that our proposed agent-based evaluation system demonstrates strong alignment with human preferences and reliably completes the evaluation, as well as the diversity and richness of the benchmark.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning-based Token Pruning in Vision Transformers: A Markov Game Approach</title>
<link>https://arxiv.org/abs/2503.23459</link>
<guid>https://arxiv.org/abs/2503.23459</guid>
<content:encoded><![CDATA[
<div> Vision Transformers, , , , <br /><br />: ()Transformer(ViT)ViT(MAPPO)ImageNet-1k44%0.4%https://github.com/daashuai/rl4evit  <div>
arXiv:2503.23459v1 Announce Type: new 
Abstract: Vision Transformers (ViTs) have computational costs scaling quadratically with the number of tokens, calling for effective token pruning policies. Most existing policies are handcrafted, lacking adaptivity to varying inputs. Moreover, they fail to consider the sequential nature of token pruning across multiple layers. In this work, for the first time (as far as we know), we exploit Reinforcement Learning (RL) to data-adaptively learn a pruning policy. Formulating token pruning as a sequential decision-making problem, we model it as a Markov Game and utilize Multi-Agent Proximal Policy Optimization (MAPPO) where each agent makes an individualized pruning decision for a single token. We also develop reward functions that enable simultaneous collaboration and competition of these agents to balance efficiency and accuracy. On the well-known ImageNet-1k dataset, our method improves the inference speed by up to 44% while incurring only a negligible accuracy drop of 0.4%. The source code is available at https://github.com/daashuai/rl4evit.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model</title>
<link>https://arxiv.org/abs/2503.23463</link>
<guid>https://arxiv.org/abs/2503.23463</guid>
<content:encoded><![CDATA[
<div> OpenDriveVLAVision-Language Action (VLA)Hierarchical vision-language alignmentAutoregressive agent-env-ego interaction

:<br />
OpenDriveVLA-VLA--OpenDriveVLAagent-env-ego ego nuScenesOpenDriveVLAOpenDriveVLA <div>
arXiv:2503.23463v1 Announce Type: new 
Abstract: We present OpenDriveVLA, a Vision-Language Action (VLA) model designed for end-to-end autonomous driving. OpenDriveVLA builds upon open-source pre-trained large Vision-Language Models (VLMs) to generate reliable driving actions, conditioned on 3D environmental perception, ego vehicle states, and driver commands. To bridge the modality gap between driving visual representations and language embeddings, we propose a hierarchical vision-language alignment process, projecting both 2D and 3D structured visual tokens into a unified semantic space. Besides, OpenDriveVLA models the dynamic relationships between the ego vehicle, surrounding agents, and static road elements through an autoregressive agent-env-ego interaction process, ensuring both spatially and behaviorally informed trajectory planning. Extensive experiments on the nuScenes dataset demonstrate that OpenDriveVLA achieves state-of-the-art results across open-loop trajectory planning and driving-related question-answering tasks. Qualitative analyses further illustrate OpenDriveVLA's superior capability to follow high-level driving commands and robustly generate trajectories under challenging scenarios, highlighting its potential for next-generation end-to-end autonomous driving. We will release our code to facilitate further research in this domain.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handling Delay in Real-Time Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.23478</link>
<guid>https://arxiv.org/abs/2503.23478</guid>
<content:encoded><![CDATA[
<div> : 

:
<br />
MujocoMinAtar6%-350% <div>
arXiv:2503.23478v1 Announce Type: new 
Abstract: Real-time reinforcement learning (RL) introduces several challenges. First, policies are constrained to a fixed number of actions per second due to hardware limitations. Second, the environment may change while the network is still computing an action, leading to observational delay. The first issue can partly be addressed with pipelining, leading to higher throughput and potentially better policies. However, the second issue remains: if each neuron operates in parallel with an execution time of $\tau$, an $N$-layer feed-forward network experiences observation delay of $\tau N$. Reducing the number of layers can decrease this delay, but at the cost of the network's expressivity. In this work, we explore the trade-off between minimizing delay and network's expressivity. We present a theoretically motivated solution that leverages temporal skip connections combined with history-augmented observations. We evaluate several architectures and show that those incorporating temporal skip connections achieve strong performance across various neuron execution times, reinforcement learning algorithms, and environments, including four Mujoco tasks and all MinAtar games. Moreover, we demonstrate parallel neuron computation can accelerate inference by 6-350% on standard hardware. Our investigation into temporal skip connections and parallel computations paves the way for more efficient RL agents in real-time setting.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-Aligning Language to Visual Objects with an Agentic Workflow</title>
<link>https://arxiv.org/abs/2503.23508</link>
<guid>https://arxiv.org/abs/2503.23508</guid>
<content:encoded><![CDATA[
<div> -LOD

<br /><br />:

(LOD)LOD(VLM)VLMReal-LOD(LLL)VLMLLM0.18MLOD50%Real-LODLOD <div>
arXiv:2503.23508v1 Announce Type: new 
Abstract: Language-based object detection (LOD) aims to align visual objects with language expressions. A large amount of paired data is utilized to improve LOD model generalizations. During the training process, recent studies leverage vision-language models (VLMs) to automatically generate human-like expressions for visual objects, facilitating training data scaling up. In this process, we observe that VLM hallucinations bring inaccurate object descriptions (e.g., object name, color, and shape) to deteriorate VL alignment quality. To reduce VLM hallucinations, we propose an agentic workflow controlled by an LLM to re-align language to visual objects via adaptively adjusting image and text prompts. We name this workflow Real-LOD, which includes planning, tool use, and reflection steps. Given an image with detected objects and VLM raw language expressions, Real-LOD reasons its state automatically and arranges action based on our neural symbolic designs (i.e., planning). The action will adaptively adjust the image and text prompts and send them to VLMs for object re-description (i.e., tool use). Then, we use another LLM to analyze these refined expressions for feedback (i.e., reflection). These steps are conducted in a cyclic form to gradually improve language descriptions for re-aligning to visual objects. We construct a dataset that contains a tiny amount of 0.18M images with re-aligned language expression and train a prevalent LOD model to surpass existing LOD methods by around 50% on the standard benchmarks. Our Real-LOD workflow, with automatic VL refinement, reveals a potential to preserve data quality along with scaling up data quantity, which further improves LOD performance from a data-alignment perspective.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReferDINO-Plus: 2nd Solution for 4th PVUW MeViS Challenge at CVPR 2025</title>
<link>https://arxiv.org/abs/2503.23509</link>
<guid>https://arxiv.org/abs/2503.23509</guid>
<content:encoded><![CDATA[
<div> : Referring Video Object Segmentation (RVOS), ReferDINO, SAM2, Conditional Mask Fusion, MeViS PVUW, CVPR 2025

:
Referring Video Object Segmentation (RVOS)ReferDINOSAM2maskmaskReferDINOSAM2ReferDINO-PlusMeViS60.43 \( \mathcal{J}\&\mathcal{F} \) CVPR 2025MeViS PVUWhttps://github.com/iSEE-Laboratory/ReferDINO-Plus <div>
arXiv:2503.23509v1 Announce Type: new 
Abstract: Referring Video Object Segmentation (RVOS) aims to segment target objects throughout a video based on a text description. This task has attracted increasing attention in the field of computer vision due to its promising applications in video editing and human-agent interaction. Recently, ReferDINO has demonstrated promising performance in this task by adapting object-level vision-language knowledge from pretrained foundational image models. In this report, we further enhance its capabilities by incorporating the advantages of SAM2 in mask quality and object consistency. In addition, to effectively balance performance between single-object and multi-object scenarios, we introduce a conditional mask fusion strategy that adaptively fuses the masks from ReferDINO and SAM2. Our solution, termed ReferDINO-Plus, achieves 60.43 \(\mathcal{J}\&\mathcal{F}\) on MeViS test set, securing 2nd place in the MeViS PVUW challenge at CVPR 2025. The code is available at: https://github.com/iSEE-Laboratory/ReferDINO-Plus.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>If an LLM Were a Character, Would It Know Its Own Story? Evaluating Lifelong Learning in LLMs</title>
<link>https://arxiv.org/abs/2503.23514</link>
<guid>https://arxiv.org/abs/2503.23514</guid>
<content:encoded><![CDATA[
<div> : LIFESTATE-BENCH

:
LIFESTATE-BENCHLLMs<br /><br /> <div>
arXiv:2503.23514v1 Announce Type: new 
Abstract: Large language models (LLMs) can carry out human-like dialogue, but unlike humans, they are stateless due to the superposition property. However, during multi-turn, multi-agent interactions, LLMs begin to exhibit consistent, character-like behaviors, hinting at a form of emergent lifelong learning. Despite this, existing benchmarks often fail to capture these dynamics, primarily focusing on static, open-ended evaluations. To address this gap, we introduce LIFESTATE-BENCH, a benchmark designed to assess lifelong learning in LLMs. It features two episodic datasets: Hamlet and a synthetic script collection, rich in narrative structure and character interactions. Our fact checking evaluation probes models' self-awareness, episodic memory retrieval, and relationship tracking, across both parametric and non-parametric approaches. Experiments on models like Llama3.1-8B, GPT-4-turbo, and DeepSeek R1, we demonstrate that nonparametric methods significantly outperform parametric ones in managing stateful learning. However, all models exhibit challenges with catastrophic forgetting as interactions extend, highlighting the need for further advancements in lifelong learning.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intent-Aware MPC for Aircraft Detect-and-Avoid with Response Delay: A Comparative Study with ACAS Xu</title>
<link>https://arxiv.org/abs/2503.23518</link>
<guid>https://arxiv.org/abs/2503.23518</guid>
<content:encoded><![CDATA[
<div> Intent-aware Model Predictive Control (MPC)remain-well-clear (RWC)multi-agent aircraftAirborne Collision Avoidance System Xu (ACAS Xu)delays

:
(MPC)Agent(DAA)(RWC)Xu(ACAS Xu)MPCDAAMPCACAS Xu <div>
arXiv:2503.23518v1 Announce Type: new 
Abstract: In this paper, we propose an intent-aware Model Predictive Control (MPC) approach for the remain-well-clear (RWC) functionality of a multi-agent aircraft detect-and-avoid (DAA) system and compare its performance with the standardized Airborne Collision Avoidance System Xu (ACAS Xu). The aircraft system is modeled as a linear system for horizontal maneuvering, with advisories on the rate of turn as the control input. Both deterministic and stochastic time delays are considered to account for the lag between control guidance issuance and the response of the aircraft. The capability of the MPC scheme in producing an optimal control profile over the entire horizon is used to mitigate the impact of the delay. We compare the proposed MPC method with ACAS Xu using various evaluation metrics, including loss of DAA well-clear percentage, near mid-air collision percentage, horizontal miss distance, and additional flight distance across different encounter scenarios. It is shown that the MPC scheme achieves better evaluation metrics than ACAS Xu for both deterministic and stochastic scenarios.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring GPT-4 for Robotic Agent Strategy with Real-Time State Feedback and a Reactive Behaviour Framework</title>
<link>https://arxiv.org/abs/2503.23601</link>
<guid>https://arxiv.org/abs/2503.23601</guid>
<content:encoded><![CDATA[
<div> GPT-4

:<br />
GPT-4LLM <div>
arXiv:2503.23601v1 Announce Type: new 
Abstract: We explore the use of GPT-4 on a humanoid robot in simulation and the real world as proof of concept of a novel large language model (LLM) driven behaviour method. LLMs have shown the ability to perform various tasks, including robotic agent behaviour. The problem involves prompting the LLM with a goal, and the LLM outputs the sub-tasks to complete to achieve that goal. Previous works focus on the executability and correctness of the LLM's generated tasks. We propose a method that successfully addresses practical concerns around safety, transitions between tasks, time horizons of tasks and state feedback. In our experiments we have found that our approach produces output for feasible requests that can be executed every time, with smooth transitions. User requests are achieved most of the time across a range of goal time horizons.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Organizationally-Oriented Approach to Enhancing Explainability and Control in Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.23615</link>
<guid>https://arxiv.org/abs/2503.23615</guid>
<content:encoded><![CDATA[
<div> :  (Multi-Agent Reinforcement Learning),  (Organizational Roles),  (Organizational Goals),  (Explainability),  (Control)

:
$\mathcal{M}OISE^+$MARLMARL<br /><br /> <div>
arXiv:2503.23615v1 Announce Type: new 
Abstract: Multi-Agent Reinforcement Learning can lead to the development of collaborative agent behaviors that show similarities with organizational concepts. Pushing forward this perspective, we introduce a novel framework that explicitly incorporates organizational roles and goals from the $\mathcal{M}OISE^+$ model into the MARL process, guiding agents to satisfy corresponding organizational constraints. By structuring training with roles and goals, we aim to enhance both the explainability and control of agent behaviors at the organizational level, whereas much of the literature primarily focuses on individual agents. Additionally, our framework includes a post-training analysis method to infer implicit roles and goals, offering insights into emergent agent behaviors. This framework has been applied across various MARL environments and algorithms, demonstrating coherence between predefined organizational specifications and those inferred from trained agents.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Constrained Multi-Agent Reinforcement Learning Approach to Autonomous Traffic Signal Control</title>
<link>https://arxiv.org/abs/2503.23626</link>
<guid>https://arxiv.org/abs/2503.23626</guid>
<content:encoded><![CDATA[
<div> Adaptive Traffic Signal Control (ATSC)(MARL)Multi-Agent Proximal Policy Optimization with Lagrange Cost Estimator (MAPPO-LCE)

<br /><br />:
MAPPO-LCEATSCGreenTimeGreenSkipPhaseSkipMAPPO-LCEMARLMAPPO12.60%IPPO10.29%QTRAN13.10%ATSChttps://github.com/Asatheesh6561/MAPPO-LCE <div>
arXiv:2503.23626v1 Announce Type: new 
Abstract: Traffic congestion in modern cities is exacerbated by the limitations of traditional fixed-time traffic signal systems, which fail to adapt to dynamic traffic patterns. Adaptive Traffic Signal Control (ATSC) algorithms have emerged as a solution by dynamically adjusting signal timing based on real-time traffic conditions. However, the main limitation of such methods is that they are not transferable to environments under real-world constraints, such as balancing efficiency, minimizing collisions, and ensuring fairness across intersections. In this paper, we view the ATSC problem as a constrained multi-agent reinforcement learning (MARL) problem and propose a novel algorithm named Multi-Agent Proximal Policy Optimization with Lagrange Cost Estimator (MAPPO-LCE) to produce effective traffic signal control policies. Our approach integrates the Lagrange multipliers method to balance rewards and constraints, with a cost estimator for stable adjustment. We also introduce three constraints on the traffic network: GreenTime, GreenSkip, and PhaseSkip, which penalize traffic policies that do not conform to real-world scenarios. Our experimental results on three real-world datasets demonstrate that MAPPO-LCE outperforms three baseline MARL algorithms by across all environments and traffic constraints (improving on MAPPO by 12.60%, IPPO by 10.29%, and QTRAN by 13.10%). Our results show that constrained MARL is a valuable tool for traffic planners to deploy scalable and efficient ATSC methods in real-world traffic networks. We provide code at https://github.com/Asatheesh6561/MAPPO-LCE.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intrinsically-Motivated Humans and Agents in Open-World Exploration</title>
<link>https://arxiv.org/abs/2503.23631</link>
<guid>https://arxiv.org/abs/2503.23631</guid>
<content:encoded><![CDATA[
<div> : Crafter

<br /><br />:
AICrafterAIAI <div>
arXiv:2503.23631v1 Announce Type: new 
Abstract: What drives exploration? Understanding intrinsic motivation is a long-standing challenge in both cognitive science and artificial intelligence; numerous objectives have been proposed and used to train agents, yet there remains a gap between human and agent exploration. We directly compare adults, children, and AI agents in a complex open-ended environment, Crafter, and study how common intrinsic objectives: Entropy, Information Gain, and Empowerment, relate to their behavior. We find that only Entropy and Empowerment are consistently positively correlated with human exploration progress, indicating that these objectives may better inform intrinsic reward design for agents. Furthermore, across agents and humans we observe that Entropy initially increases rapidly, then plateaus, while Empowerment increases continuously, suggesting that state diversity may provide more signal in early exploration, while advanced exploration should prioritize control. Finally, we find preliminary evidence that private speech utterances, and particularly goal verbalizations, may aid exploration in children.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GIScience in the Era of Artificial Intelligence: A Research Agenda Towards Autonomous GIS</title>
<link>https://arxiv.org/abs/2503.23633</link>
<guid>https://arxiv.org/abs/2503.23633</guid>
<content:encoded><![CDATA[
<div> generative AILLMsautonomous GISGIScience

<br /><br />
autonomous GISLLMsGISGISGISGISGIScienceGIS <div>
arXiv:2503.23633v1 Announce Type: new 
Abstract: The advent of generative AI exemplified by large language models (LLMs) opens new ways to represent and compute geographic information and transcend the process of geographic knowledge production, driving geographic information systems (GIS) towards autonomous GIS. Leveraging LLMs as the decision core, autonomous GIS can independently generate and execute geoprocessing workflows to perform spatial analysis. In this vision paper, we elaborate on the concept of autonomous GIS and present a framework that defines its five autonomous goals, five levels of autonomy, five core functions, and three operational scales. We demonstrate how autonomous GIS could perform geospatial data retrieval, spatial analysis, and map making with four proof-of-concept GIS agents. We conclude by identifying critical challenges and future research directions, including fine-tuning and self-growing decision cores, autonomous modeling, and examining the ethical and practical implications of autonomous GIS. By establishing the groundwork for a paradigm shift in GIScience, this paper envisions a future where GIS moves beyond traditional workflows to autonomously reason, derive, innovate, and advance solutions to pressing global challenges.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MolGround: A Benchmark for Molecular Grounding</title>
<link>https://arxiv.org/abs/2503.23668</link>
<guid>https://arxiv.org/abs/2503.23668</guid>
<content:encoded><![CDATA[
<div> NLP

:

NLPAI7.9-GPT-4oATC <div>
arXiv:2503.23668v1 Announce Type: new 
Abstract: Current molecular understanding approaches predominantly focus on the descriptive aspect of human perception, providing broad, topic-level insights. However, the referential aspect -- linking molecular concepts to specific structural components -- remains largely unexplored. To address this gap, we propose a molecular grounding benchmark designed to evaluate a model's referential abilities. We align molecular grounding with established conventions in NLP, cheminformatics, and molecular science, showcasing the potential of NLP techniques to advance molecular understanding within the AI for Science movement. Furthermore, we constructed the largest molecular understanding benchmark to date, comprising 79k QA pairs, and developed a multi-agent grounding prototype as proof of concept. This system outperforms existing models, including GPT-4o, and its grounding outputs have been integrated to enhance traditional tasks such as molecular captioning and ATC (Anatomical, Therapeutic, Chemical) classification.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WHERE and WHICH: Iterative Debate for Biomedical Synthetic Data Augmentation</title>
<link>https://arxiv.org/abs/2503.23673</link>
<guid>https://arxiv.org/abs/2503.23673</guid>
<content:encoded><![CDATA[
<div> : 

<br /><br />:
BioNLPBLURBBigBIOBioNLPNLP <div>
arXiv:2503.23673v1 Announce Type: new 
Abstract: In Biomedical Natural Language Processing (BioNLP) tasks, such as Relation Extraction, Named Entity Recognition, and Text Classification, the scarcity of high-quality data remains a significant challenge. This limitation poisons large language models to correctly understand relationships between biological entities, such as molecules and diseases, or drug interactions, and further results in potential misinterpretation of biomedical documents. To address this issue, current approaches generally adopt the Synthetic Data Augmentation method which involves similarity computation followed by word replacement, but counterfactual data are usually generated. As a result, these methods disrupt meaningful word sets or produce sentences with meanings that deviate substantially from the original context, rendering them ineffective in improving model performance. To this end, this paper proposes a biomedical-dedicated rationale-based synthetic data augmentation method. Beyond the naive lexicon similarity, specific bio-relation similarity is measured to hold the augmented instance having a strong correlation with bio-relation instead of simply increasing the diversity of augmented data. Moreover, a multi-agents-involved reflection mechanism helps the model iteratively distinguish different usage of similar entities to escape falling into the mis-replace trap. We evaluate our method on the BLURB and BigBIO benchmark, which includes 9 common datasets spanning four major BioNLP tasks. Our experimental results demonstrate consistent performance improvements across all tasks, highlighting the effectiveness of our approach in addressing the challenges associated with data scarcity and enhancing the overall performance of biomedical NLP models.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering Large Agent Populations using Mean-Field Schrodinger Bridges with Gaussian Mixture Models</title>
<link>https://arxiv.org/abs/2503.23705</link>
<guid>https://arxiv.org/abs/2503.23705</guid>
<content:encoded><![CDATA[
<div> Mean-Field Schrdinger Bridge (MFSB)

:
Mean-Field Schrdinger Bridge (MFSB)MFSBGaussian-to-Gaussian Covariance SteeringCovariance Steering<br /><br /> <div>
arXiv:2503.23705v1 Announce Type: new 
Abstract: The Mean-Field Schrodinger Bridge (MFSB) problem is an optimization problem aiming to find the minimum effort control policy to drive a McKean-Vlassov stochastic differential equation from one probability measure to another. In the context of multiagent control, the objective is to control the configuration of a swarm of identical, interacting cooperative agents, as captured by the time-varying probability measure of their state. Available methods for solving this problem for distributions with continuous support rely either on spatial discretizations of the problem's domain or on approximating optimal solutions using neural networks trained through stochastic optimization schemes. For agents following Linear Time-Varying dynamics, and for Gaussian Mixture Model boundary distributions, we propose a highly efficient parameterization to approximate the solutions of the corresponding MFSB in closed form, without any learning steps. Our proposed approach consists of a mixture of elementary policies, each solving a Gaussian-to-Gaussian Covariance Steering problem from the components of the initial to the components of the terminal mixture. Leveraging the semidefinite formulation of the Covariance Steering problem, our proposed solver can handle probabilistic hard constraints on the system's state, while maintaining numerical tractability. We illustrate our approach on a variety of numerical examples.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Functional Bugs in Smart Contracts through LLM-Powered and Bug-Oriented Composite Analysis</title>
<link>https://arxiv.org/abs/2503.23718</link>
<guid>https://arxiv.org/abs/2503.23718</guid>
<content:encoded><![CDATA[
<div> PROMFUZZ

:<br />
PROMFUZZPROMFUZZPROMFUZZbugPROMFUZZF186.96%93.02%50%PROMFUZZDeFi3024CVE IDs <div>
arXiv:2503.23718v1 Announce Type: new 
Abstract: Smart contracts are fundamental pillars of the blockchain, playing a crucial role in facilitating various business transactions. However, these smart contracts are vulnerable to exploitable bugs that can lead to substantial monetary losses. A recent study reveals that over 80% of these exploitable bugs, which are primarily functional bugs, can evade the detection of current tools. The primary issue is the significant gap between understanding the high-level logic of the business model and checking the low-level implementations in smart contracts. Furthermore, identifying deeply rooted functional bugs in smart contracts requires the automated generation of effective detection oracles based on various bug features. To address these challenges, we design and implement PROMFUZZ, an automated and scalable system to detect functional bugs, in smart contracts. In PROMFUZZ, we first propose a novel Large Language Model (LLM)-driven analysis framework, which leverages a dual-agent prompt engineering strategy to pinpoint potentially vulnerable functions for further scrutiny. We then implement a dual-stage coupling approach, which focuses on generating invariant checkers that leverage logic information extracted from potentially vulnerable functions. Finally, we design a bug-oriented fuzzing engine, which maps the logical information from the high-level business model to the low-level smart contract implementations, and performs the bug-oriented fuzzing on targeted functions. We compare PROMFUZZ with multiple state-of-the-art methods. The results show that PROMFUZZ achieves 86.96% recall and 93.02% F1-score in detecting functional bugs, marking at least a 50% improvement in both metrics over state-of-the-art methods. Moreover, we perform an in-depth analysis on real-world DeFi projects and detect 30 zero-day bugs. Up to now, 24 zero-day bugs have been assigned CVE IDs.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PDSL: Privacy-Preserved Decentralized Stochastic Learning with Heterogeneous Data Distribution</title>
<link>https://arxiv.org/abs/2503.23726</link>
<guid>https://arxiv.org/abs/2503.23726</guid>
<content:encoded><![CDATA[
<div> : Shapley

<br />
:
PDSLPDSLShapleyPDSL <div>
arXiv:2503.23726v1 Announce Type: new 
Abstract: In the paradigm of decentralized learning, a group of agents collaborates to learn a global model using distributed datasets without a central server. However, due to the heterogeneity of the local data across the different agents, learning a robust global model is rather challenging. Moreover, the collaboration of the agents relies on their gradient information exchange, which poses a risk of privacy leakage. In this paper, to address these issues, we propose PDSL, a novel privacy-preserved decentralized stochastic learning algorithm with heterogeneous data distribution. On one hand, we innovate in utilizing the notion of Shapley values such that each agent can precisely measure the contributions of its heterogeneous neighbors to the global learning goal; on the other hand, we leverage the notion of differential privacy to prevent each agent from suffering privacy leakage when it contributes gradient information to its neighbors. We conduct both solid theoretical analysis and extensive experiments to demonstrate the efficacy of our PDSL algorithm in terms of privacy preservation and convergence.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinking Longer, Not Larger: Enhancing Software Engineering Agents via Scaling Test-Time Compute</title>
<link>https://arxiv.org/abs/2503.23803</link>
<guid>https://arxiv.org/abs/2503.23803</guid>
<content:encoded><![CDATA[
<div> LLMs


TTCLLMsTTCTTCTTCTTC

32BSWE-bench Verified46%DeepSeek R1 671BOpenAI o1SWEGitHub<br /><br />https://github.com/yingweima2022/SWE-Reasoner <div>
arXiv:2503.23803v1 Announce Type: new 
Abstract: Recent advancements in software engineering agents have demonstrated promising capabilities in automating program improvements. However, their reliance on closed-source or resource-intensive models introduces significant deployment challenges in private environments, prompting a critical question: \textit{How can personally deployable open-source LLMs achieve comparable code reasoning performance?}
  To this end, we propose a unified Test-Time Compute scaling framework that leverages increased inference-time computation instead of larger models. Our framework incorporates two complementary strategies: internal TTC and external TTC. Internally, we introduce a \textit{development-contextualized trajectory synthesis} method leveraging real-world software repositories to bootstrap multi-stage reasoning processes, such as fault localization and patch generation. We further enhance trajectory quality through rejection sampling, rigorously evaluating trajectories along accuracy and complexity. Externally, we propose a novel \textit{development-process-based search} strategy guided by reward models and execution verification. This approach enables targeted computational allocation at critical development decision points, overcoming limitations of existing "end-point only" verification methods.
  Evaluations on SWE-bench Verified demonstrate our \textbf{32B model achieves a 46\% issue resolution rate}, surpassing significantly larger models such as DeepSeek R1 671B and OpenAI o1. Additionally, we provide the empirical validation of the test-time scaling phenomenon within SWE agents, revealing that \textbf{models dynamically allocate more tokens to increasingly challenging problems}, effectively enhancing reasoning capabilities. We publicly release all training data, models, and code to facilitate future research. https://github.com/yingweima2022/SWE-Reasoner
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Get the Agents Drunk: Memory Perturbations in Autonomous Agent-based Recommender Systems</title>
<link>https://arxiv.org/abs/2503.23804</link>
<guid>https://arxiv.org/abs/2503.23804</guid>
<content:encoded><![CDATA[
<div> DrunkAgent

:
(Agent4RSs)Agent4RSsDrunkAgentDrunkAgentAgent4RSsDrunkAgent <div>
arXiv:2503.23804v1 Announce Type: new 
Abstract: Large language model-based agents are increasingly used in recommender systems (Agent4RSs) to achieve personalized behavior modeling. Specifically, Agent4RSs introduces memory mechanisms that enable the agents to autonomously learn and self-evolve from real-world interactions. However, to the best of our knowledge, how robust Agent4RSs are remains unexplored. As such, in this paper, we propose the first work to attack Agent4RSs by perturbing agents' memories, not only to uncover their limitations but also to enhance their security and robustness, ensuring the development of safer and more reliable AI agents.
  Given the security and privacy concerns, it is more practical to launch attacks under a black-box setting, where the accurate knowledge of the victim models cannot be easily obtained. Moreover, the practical attacks are often stealthy to maximize the impact. To this end, we propose a novel practical attack framework named DrunkAgent. DrunkAgent consists of a generation module, a strategy module, and a surrogate module. The generation module aims to produce effective and coherent adversarial textual triggers, which can be used to achieve attack objectives such as promoting the target items. The strategy module is designed to `get the target agents drunk' so that their memories cannot be effectively updated during the interaction process. As such, the triggers can play the best role. Both of the modules are optimized on the surrogate module to improve the transferability and imperceptibility of the attacks. By identifying and analyzing the vulnerabilities, our work provides critical insights that pave the way for building safer and more resilient Agent4RSs. Extensive experiments across various real-world datasets demonstrate the effectiveness of DrunkAgent.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenSwarm: Scalable Multi-Robot Code-Policy Generation and Deployment via Language Models</title>
<link>https://arxiv.org/abs/2503.23875</link>
<guid>https://arxiv.org/abs/2503.23875</guid>
<content:encoded><![CDATA[
<div> GenSwarm

<br />
: GenSwarmGenSwarmGenSwarmGenSwarmhttps://github.com/WindyLab/GenSwarm <div>
arXiv:2503.23875v1 Announce Type: new 
Abstract: The development of control policies for multi-robot systems traditionally follows a complex and labor-intensive process, often lacking the flexibility to adapt to dynamic tasks. This has motivated research on methods to automatically create control policies. However, these methods require iterative processes of manually crafting and refining objective functions, thereby prolonging the development cycle. This work introduces \textit{GenSwarm}, an end-to-end system that leverages large language models to automatically generate and deploy control policies for multi-robot tasks based on simple user instructions in natural language. As a multi-language-agent system, GenSwarm achieves zero-shot learning, enabling rapid adaptation to altered or unseen tasks. The white-box nature of the code policies ensures strong reproducibility and interpretability. With its scalable software and hardware architectures, GenSwarm supports efficient policy deployment on both simulated and real-world multi-robot systems, realizing an instruction-to-execution end-to-end functionality that could prove valuable for robotics specialists and non-specialists alike.The code of the proposed GenSwarm system is available online: https://github.com/WindyLab/GenSwarm.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SchemaAgent: A Multi-Agents Framework for Generating Relational Database Schema</title>
<link>https://arxiv.org/abs/2503.23886</link>
<guid>https://arxiv.org/abs/2503.23886</guid>
<content:encoded><![CDATA[
<div> : 

:<br />
SchemaAgentSchemaAgentSchemaAgent500RSchemaSchemaAgent <div>
arXiv:2503.23886v1 Announce Type: new 
Abstract: The relational database design would output a schema based on user's requirements, which defines table structures and their interrelated relations. Translating requirements into accurate schema involves several non-trivial subtasks demanding both database expertise and domain-specific knowledge. This poses unique challenges for automated design of relational databases. Existing efforts are mostly based on customized rules or conventional deep learning models, often producing suboptimal schema. Recently, large language models (LLMs) have significantly advanced intelligent application development across various domains. In this paper, we propose SchemaAgent, a unified LLM-based multi-agent framework for the automated generation of high-quality database schema. SchemaAgent is the first to apply LLMs for schema generation, which emulates the workflow of manual schema design by assigning specialized roles to agents and enabling effective collaboration to refine their respective subtasks. Schema generation is a streamlined workflow, where directly applying the multi-agent framework may cause compounding impact of errors. To address this, we incorporate dedicated roles for reflection and inspection, alongside an innovative error detection and correction mechanism to identify and rectify issues across various phases. For evaluation, we present a benchmark named \textit{RSchema}, which contains more than 500 pairs of requirement description and schema. Experimental results on this benchmark demonstrate the superiority of our approach over mainstream LLMs for relational database schema generation.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeteroPod: XPU-Accelerated Infrastructure Offloading for Commodity Cloud-Native Applications</title>
<link>https://arxiv.org/abs/2503.23952</link>
<guid>https://arxiv.org/abs/2503.23952</guid>
<content:encoded><![CDATA[
<div> Cloud-native systemsData Processing Units (DPUs)HeteroPodHeteroNetcross-PU (XPU) network

<br /><br />

HeteroPod(DPUs)HeteroPodHeteroNet(XPU)(1) CPUDPU(2) XPUHeteroNetHeteroPodDPUsLinuxHeteroNetKubernetesHeteroK8sHeteroK8s10031.96460%55% <div>
arXiv:2503.23952v1 Announce Type: new 
Abstract: Cloud-native systems increasingly rely on infrastructure services (e.g., service meshes, monitoring agents), which compete for resources with user applications, degrading performance and scalability. We propose HeteroPod, a new abstraction that offloads these services to Data Processing Units (DPUs) to enforce strict isolation while reducing host resource contention and operational costs. To realize HeteroPod, we introduce HeteroNet, a cross-PU (XPU) network system featuring: (1) split network namespace, a unified network abstraction for processes spanning CPU and DPU, and (2) elastic and efficient XPU networking, a communication mechanism achieving shared-memory performance without pinned resource overhead and polling costs. By leveraging HeteroNet and the compositional nature of cloud-native workloads, HeteroPod can optimally offload infrastructure containers to DPUs. We implement HeteroNet based on Linux, and implement a cloud-native system called HeteroK8s based on Kubernetes. We evaluate the systems using NVIDIA Bluefield-2 DPUs and CXL-based DPUs (simulated with real CXL memory devices). The results show that HeteroK8s effectively supports complex (unmodified) commodity cloud-native applications (up to 1 million LoC) and provides up to 31.9x better latency and 64x less resource consumption (compared with kernel-bypass design), 60% better end-to-end latency, and 55% higher scalability compared with SOTA systems.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rubric Is All You Need: Enhancing LLM-based Code Evaluation With Question-Specific Rubrics</title>
<link>https://arxiv.org/abs/2503.23989</link>
<guid>https://arxiv.org/abs/2503.23989</guid>
<content:encoded><![CDATA[
<div> : LLM<br /><br />:<br />
LLM15080 kappa  <div>
arXiv:2503.23989v1 Announce Type: new 
Abstract: Since the disruption in LLM technology brought about by the release of GPT-3 and ChatGPT, LLMs have shown remarkable promise in programming-related tasks. While code generation remains a popular field of research, code evaluation using LLMs remains a problem with no conclusive solution. In this paper, we focus on LLM-based code evaluation and attempt to fill in the existing gaps. We propose multi-agentic novel approaches using question-specific rubrics tailored to the problem statement, arguing that these perform better for logical assessment than the existing approaches that use question-agnostic rubrics. To address the lack of suitable evaluation datasets, we introduce two datasets: a Data Structures and Algorithms dataset containing 150 student submissions from a popular Data Structures and Algorithms practice website, and an Object Oriented Programming dataset comprising 80 student submissions from undergraduate computer science courses. In addition to using standard metrics (Spearman Correlation, Cohen's Kappa), we additionally propose a new metric called as Leniency, which quantifies evaluation strictness relative to expert assessment. Our comprehensive analysis demonstrates that question-specific rubrics significantly enhance logical assessment of code in educational settings, providing better feedback aligned with instructional goals beyond mere syntactic correctness.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H2VU-Benchmark: A Comprehensive Benchmark for Hierarchical Holistic Video Understanding</title>
<link>https://arxiv.org/abs/2503.24008</link>
<guid>https://arxiv.org/abs/2503.24008</guid>
<content:encoded><![CDATA[
<div> H2VU

:<br />
H2VU31.5MLLMsH2VUMLLMs <div>
arXiv:2503.24008v1 Announce Type: new 
Abstract: With the rapid development of multimodal models, the demand for assessing video understanding capabilities has been steadily increasing. However, existing benchmarks for evaluating video understanding exhibit significant limitations in coverage, task diversity, and scene adaptability. These shortcomings hinder the accurate assessment of models' comprehensive video understanding capabilities. To tackle this challenge, we propose a hierarchical and holistic video understanding (H2VU) benchmark designed to evaluate both general video and online streaming video comprehension. This benchmark contributes three key features:
  Extended video duration: Spanning videos from brief 3-second clips to comprehensive 1.5-hour recordings, thereby bridging the temporal gaps found in current benchmarks. Comprehensive assessment tasks: Beyond traditional perceptual and reasoning tasks, we have introduced modules for countercommonsense comprehension and trajectory state tracking. These additions test the models' deep understanding capabilities beyond mere prior knowledge. Enriched video data: To keep pace with the rapid evolution of current AI agents, we have expanded first-person streaming video datasets. This expansion allows for the exploration of multimodal models' performance in understanding streaming videos from a first-person perspective. Extensive results from H2VU reveal that existing multimodal large language models (MLLMs) possess substantial potential for improvement in our newly proposed evaluation tasks. We expect that H2VU will facilitate advancements in video understanding research by offering a comprehensive and in-depth analysis of MLLMs.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consensus on Open Multi-Agent Systems Over Graphs Sampled from Graphons</title>
<link>https://arxiv.org/abs/2503.24025</link>
<guid>https://arxiv.org/abs/2503.24025</guid>
<content:encoded><![CDATA[
<div> : graphon

:
graphon(SBM) graphongraphon <div>
arXiv:2503.24025v1 Announce Type: new 
Abstract: We show how graphons can be used to model and analyze open multi-agent systems, which are multi-agent systems subject to arrivals and departures, in the specific case of linear consensus. First, we analyze the case of replacements, where under the assumption of a deterministic interval between two replacements, we derive an upper bound for the disagreement in expectation. Then, we study the case of arrivals and departures, where we define a process for the evolution of the number of agents that guarantees a minimum and a maximum number of agents. Next, we derive an upper bound for the disagreement in expectation, and we establish a link with the spectrum of the expected graph used to generate the graph topologies. Finally, for stochastic block model (SBM) graphons, we prove that the computation of the spectrum of the expected graph can be performed based on a matrix whose dimension depends only on the graphon and it is independent of the number of agents.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents</title>
<link>https://arxiv.org/abs/2503.24047</link>
<guid>https://arxiv.org/abs/2503.24047</guid>
<content:encoded><![CDATA[
<div> 

:
<br />
LLMsLLMLLMsLLM <div>
arXiv:2503.24047v1 Announce Type: new 
Abstract: As scientific research becomes increasingly complex, innovative tools are needed to manage vast data, facilitate interdisciplinary collaboration, and accelerate discovery. Large language models (LLMs) are now evolving into LLM-based scientific agents that automate critical tasks, ranging from hypothesis generation and experiment design to data analysis and simulation. Unlike general-purpose LLMs, these specialized agents integrate domain-specific knowledge, advanced tool sets, and robust validation mechanisms, enabling them to handle complex data types, ensure reproducibility, and drive scientific breakthroughs. This survey provides a focused review of the architectures, design, benchmarks, applications, and ethical considerations surrounding LLM-based scientific agents. We highlight why they differ from general agents and the ways in which they advance research across various scientific fields. By examining their development and challenges, this survey offers a comprehensive roadmap for researchers and practitioners to harness these agents for more efficient, reliable, and ethically sound scientific discovery.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Distributed Output Synchronization of Heterogeneous Discrete-Time Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2503.24105</link>
<guid>https://arxiv.org/abs/2503.24105</guid>
<content:encoded><![CDATA[
<div> LTI

:
LTI<br /><br /> <div>
arXiv:2503.24105v1 Announce Type: new 
Abstract: In this paper, we assume that an autonomous exosystem generates a reference output, and we consider the problem of designing a distributed data-driven control law for a family of discrete-time heterogeneous LTI agents, connected through a directed graph, in order to synchronize the agents' outputs to the reference one. The agents of the network are split into two categories: leaders, with direct access to the exosystem output, and followers, that only receive information from their neighbors. All agents aim to achieve output synchronization by means of a state feedback that makes use of their own states as well as of an estimate of the exogenous system state, provided by an internal state observer. Such observer has a different structure for leaders and followers. Necessary and sufficient conditions for the existence of a solution are first derived in the model-based set-up and then in a data-driven context. An example illustrates both the implementation procedure and the performance of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounding Agent Reasoning in Image Schemas: A Neurosymbolic Approach to Embodied Cognition</title>
<link>https://arxiv.org/abs/2503.24110</link>
<guid>https://arxiv.org/abs/2503.24110</guid>
<content:encoded><![CDATA[
<div> embodied AIagent reasoning systemsimage schemasneurosymbolic systemhuman-agent interactions

<br /><br />:
 <div>
arXiv:2503.24110v1 Announce Type: new 
Abstract: Despite advances in embodied AI, agent reasoning systems still struggle to capture the fundamental conceptual structures that humans naturally use to understand and interact with their environment. To address this, we propose a novel framework that bridges embodied cognition theory and agent systems by leveraging a formal characterization of image schemas, which are defined as recurring patterns of sensorimotor experience that structure human cognition. By customizing LLMs to translate natural language descriptions into formal representations based on these sensorimotor patterns, we will be able to create a neurosymbolic system that grounds the agent's understanding in fundamental conceptual structures. We argue that such an approach enhances both efficiency and interpretability while enabling more intuitive human-agent interactions through shared embodied understanding.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud Detection</title>
<link>https://arxiv.org/abs/2503.24115</link>
<guid>https://arxiv.org/abs/2503.24115</guid>
<content:encoded><![CDATA[
<div> : Telecom Fraud Detection, Multimodal Training Data, TeleAntiFraud-28k, Audio-Text Slow-Thinking Dataset, Standardized Evaluation Benchmark

<br /><br />:
TeleAntiFraud-28k28,511TeleAntiFraud-Bench/SFThttps://github.com/JimmyMa99/TeleAntiFraud <div>
arXiv:2503.24115v1 Announce Type: new 
Abstract: The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at https://github.com/JimmyMa99/TeleAntiFraud.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Safe Autonomous Two Device Navigation of Cerebral Vessels in Mechanical Thrombectomy</title>
<link>https://arxiv.org/abs/2503.24140</link>
<guid>https://arxiv.org/abs/2503.24140</guid>
<content:encoded><![CDATA[
<div> 

:
Simulation Open Framework ArchitectureSoft Actor-Critic RL1296%7.00.241.5 <div>
arXiv:2503.24140v1 Announce Type: new 
Abstract: Purpose: Autonomous systems in mechanical thrombectomy (MT) hold promise for reducing procedure times, minimizing radiation exposure, and enhancing patient safety. However, current reinforcement learning (RL) methods only reach the carotid arteries, are not generalizable to other patient vasculatures, and do not consider safety. We propose a safe dual-device RL algorithm that can navigate beyond the carotid arteries to cerebral vessels.
  Methods: We used the Simulation Open Framework Architecture to represent the intricacies of cerebral vessels, and a modified Soft Actor-Critic RL algorithm to learn, for the first time, the navigation of micro-catheters and micro-guidewires. We incorporate patient safety metrics into our reward function by integrating guidewire tip forces. Inverse RL is used with demonstrator data on 12 patient-specific vascular cases.
  Results: Our simulation demonstrates successful autonomous navigation within unseen cerebral vessels, achieving a 96% success rate, 7.0s procedure time, and 0.24 N mean forces, well below the proposed 1.5 N vessel rupture threshold.
  Conclusion: To the best of our knowledge, our proposed autonomous system for MT two-device navigation reaches cerebral vessels, considers safety, and is generalizable to unseen patient-specific cases for the first time. We envisage future work will extend the validation to vasculatures of different complexity and on in vitro models. While our contributions pave the way towards deploying agents in clinical settings, safety and trustworthiness will be crucial elements to consider when proposing new methodology.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navi-plus: Managing Ambiguous GUI Navigation Tasks with Follow-up</title>
<link>https://arxiv.org/abs/2503.24180</link>
<guid>https://arxiv.org/abs/2503.24180</guid>
<content:encoded><![CDATA[
<div> Graphical User Interfaces (GUI)GUINavi-plus

:<br />
GUIGUIGUINavi-plusGUI <div>
arXiv:2503.24180v1 Announce Type: new 
Abstract: Graphical user interfaces (GUI) automation agents are emerging as powerful tools, enabling humans to accomplish increasingly complex tasks on smart devices. However, users often inadvertently omit key information when conveying tasks, which hinders agent performance in the current agent paradigm that does not support immediate user intervention. To address this issue, we introduce a $\textbf{Self-Correction GUI Navigation}$ task that incorporates interactive information completion capabilities within GUI agents. We developed the $\textbf{Navi-plus}$ dataset with GUI follow-up question-answer pairs, alongside a $\textbf{Dual-Stream Trajectory Evaluation}$ method to benchmark this new capability. Our results show that agents equipped with the ability to ask GUI follow-up questions can fully recover their performance when faced with ambiguous user tasks.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Output Constraints as Attack Surface: Exploiting Structured Generation to Bypass LLM Safety Mechanisms</title>
<link>https://arxiv.org/abs/2503.24191</link>
<guid>https://arxiv.org/abs/2503.24191</guid>
<content:encoded><![CDATA[
<div> Large Language Models (LLMs)Constrained Decoding Attack (CDA)Chain Enum Attack

<br /><br />:

LLMsCDACDAGPT-4oGemini-2.0-flashLLM96.2%LLMLLM <div>
arXiv:2503.24191v1 Announce Type: new 
Abstract: Content Warning: This paper may contain unsafe or harmful content generated by LLMs that may be offensive to readers. Large Language Models (LLMs) are extensively used as tooling platforms through structured output APIs to ensure syntax compliance so that robust integration with existing softwares like agent systems, could be achieved. However, the feature enabling functionality of grammar-guided structured output presents significant security vulnerabilities. In this work, we reveal a critical control-plane attack surface orthogonal to traditional data-plane vulnerabilities. We introduce Constrained Decoding Attack (CDA), a novel jailbreak class that weaponizes structured output constraints to bypass safety mechanisms. Unlike prior attacks focused on input prompts, CDA operates by embedding malicious intent in schema-level grammar rules (control-plane) while maintaining benign surface prompts (data-plane). We instantiate this with a proof-of-concept Chain Enum Attack, achieves 96.2% attack success rates across proprietary and open-weight LLMs on five safety benchmarks with a single query, including GPT-4o and Gemini-2.0-flash. Our findings identify a critical security blind spot in current LLM architectures and urge a paradigm shift in LLM safety to address control-plane vulnerabilities, as current mechanisms focused solely on data-plane threats leave critical systems exposed.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-Based Simulations of Online Political Discussions: A Case Study on Elections in Germany</title>
<link>https://arxiv.org/abs/2503.24199</link>
<guid>https://arxiv.org/abs/2503.24199</guid>
<content:encoded><![CDATA[
<div> AI

<br /><br />: TwitterAIAI <div>
arXiv:2503.24199v1 Announce Type: new 
Abstract: User engagement on social media platforms is influenced by historical context, time constraints, and reward-driven interactions. This study presents an agent-based simulation approach that models user interactions, considering past conversation history, motivation, and resource constraints. Utilizing German Twitter data on political discourse, we fine-tune AI models to generate posts and replies, incorporating sentiment analysis, irony detection, and offensiveness classification. The simulation employs a myopic best-response model to govern agent behavior, accounting for decision-making based on expected rewards. Our results highlight the impact of historical context on AI-generated responses and demonstrate how engagement evolves under varying constraints.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAARS: Persona Aligned Agentic Retail Shoppers</title>
<link>https://arxiv.org/abs/2503.24228</link>
<guid>https://arxiv.org/abs/2503.24228</guid>
<content:encoded><![CDATA[
<div> : 

:
(1) (2) (3) A/B <div>
arXiv:2503.24228v1 Announce Type: new 
Abstract: In e-commerce, behavioral data is collected for decision making which can be costly and slow. Simulation with LLM powered agents is emerging as a promising alternative for representing human population behavior. However, LLMs are known to exhibit certain biases, such as brand bias, review rating bias and limited representation of certain groups in the population, hence they need to be carefully benchmarked and aligned to user behavior. Ultimately, our goal is to synthesise an agent population and verify that it collectively approximates a real sample of humans. To this end, we propose a framework that: (i) creates synthetic shopping agents by automatically mining personas from anonymised historical shopping data, (ii) equips agents with retail-specific tools to synthesise shopping sessions and (iii) introduces a novel alignment suite measuring distributional differences between humans and shopping agents at the group (i.e. population) level rather than the traditional "individual" level. Experimental results demonstrate that using personas improves performance on the alignment suite, though a gap remains to human behaviour. We showcase an initial application of our framework for automated agentic A/B testing and compare the findings to human results. Finally, we discuss applications, limitations and challenges setting the stage for impactful future work.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaintainCoder: Maintainable Code Generation Under Dynamic Requirements</title>
<link>https://arxiv.org/abs/2503.24260</link>
<guid>https://arxiv.org/abs/2503.24260</guid>
<content:encoded><![CDATA[
<div> : MaintainCoderWaterfall<br /><br />:
MaintainCoderWaterfallMaintainBenchMaintainCoderpass@k14-30%https://github.com/IAAR-Shanghai/MaintainCoder <div>
arXiv:2503.24260v1 Announce Type: new 
Abstract: Modern code generation has made significant strides in functional correctness and execution efficiency. However, these systems often overlook a critical dimension in real-world software development: maintainability. To handle dynamic requirements with minimal rework, we propose MaintainCoder as a pioneering solution. It integrates Waterfall model, design patterns, and multi-agent collaboration to systematically enhance cohesion, reduce coupling, and improve adaptability. We also introduce MaintainBench, a benchmark comprising requirement changes and corresponding dynamic metrics on maintainance effort. Experiments demonstrate that existing code generation methods struggle to meet maintainability standards when requirements evolve. In contrast, MaintainCoder improves maintainability metrics by 14-30% with even higher correctness, i.e. pass@k. Our work not only provides the foundation of maintainable code generation, but also highlights the need for more holistic code quality research. Resources: https://github.com/IAAR-Shanghai/MaintainCoder.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Value of Information-based Deceptive Path Planning Under Adversarial Interventions</title>
<link>https://arxiv.org/abs/2503.24284</link>
<guid>https://arxiv.org/abs/2503.24284</guid>
<content:encoded><![CDATA[
<div> 

:<br />
DPPMDPVoIDPPDPP <div>
arXiv:2503.24284v1 Announce Type: new 
Abstract: Existing methods for deceptive path planning (DPP) address the problem of designing paths that conceal their true goal from a passive, external observer. Such methods do not apply to problems where the observer has the ability to perform adversarial interventions to impede the path planning agent. In this paper, we propose a novel Markov decision process (MDP)-based model for the DPP problem under adversarial interventions and develop new value of information (VoI) objectives to guide the design of DPP policies. Using the VoI objectives we propose, path planning agents deceive the adversarial observer into choosing suboptimal interventions by selecting trajectories that are of low informational value to the observer. Leveraging connections to the linear programming theory for MDPs, we derive computationally efficient solution methods for synthesizing policies for performing DPP under adversarial interventions. In our experiments, we illustrate the effectiveness of the proposed solution method in achieving deceptiveness under adversarial interventions and demonstrate the superior performance of our approach to both existing DPP methods and conservative path planning approaches on illustrative gridworld problems.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Dynamic Spectrum Access via Fully Decentralized Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.24296</link>
<guid>https://arxiv.org/abs/2503.24296</guid>
<content:encoded><![CDATA[
<div> (RL)Fair Share RL (FSRL)

:
RLFair Share RL (FSRL)(i) (ii) (iii) 50adhocRLFSRL89.0%Jain's48.1% <div>
arXiv:2503.24296v1 Announce Type: new 
Abstract: We consider a decentralized wireless network with several source-destination pairs sharing a limited number of orthogonal frequency bands. Sources learn to adapt their transmissions (specifically, their band selection strategy) over time, in a decentralized manner, without sharing information with each other. Sources can only observe the outcome of their own transmissions (i.e., success or collision), having no prior knowledge of the network size or of the transmission strategy of other sources. The goal of each source is to maximize their own throughput while striving for network-wide fairness. We propose a novel fully decentralized Reinforcement Learning (RL)-based solution that achieves fairness without coordination. The proposed Fair Share RL (FSRL) solution combines: (i) state augmentation with a semi-adaptive time reference; (ii) an architecture that leverages risk control and time difference likelihood; and (iii) a fairness-driven reward structure. We evaluate FSRL in more than 50 network settings with different number of agents, different amounts of available spectrum, in the presence of jammers, and in an ad-hoc setting. Simulation results suggest that, when we compare FSRL with a common baseline RL algorithm from the literature, FSRL can be up to 89.0% fairer (as measured by Jain's fairness index) in stringent settings with several sources and a single frequency band, and 48.1% fairer on average.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coordinating Distributed Energy Resources with Nodal Pricing in Distribution Networks: a Game-Theoretic Approach</title>
<link>https://arxiv.org/abs/2503.24342</link>
<guid>https://arxiv.org/abs/2503.24342</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:
IEEE <div>
arXiv:2503.24342v1 Announce Type: new 
Abstract: We propose a real-time nodal pricing mechanism for cost minimization and voltage control in a distribution network with autonomous distributed energy resources and analyze the resulting market using stochastic game theory. Unlike existing methods, the proposed pricing scheme does not require device-aware centralized coordination or communication between prosumers. By developing new sufficient conditions under which a stochastic game is a Markov potential game, we show that the problem of computing an equilibrium for the proposed model is equivalent to solving a single-agent Markov Decision Process. These new conditions are general and may apply to other applications. We compute the equilibrium for an IEEE test system to empirically demonstrate the effectiveness of the pricing policy.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RIG: Synergizing Reasoning and Imagination in End-to-End Generalist Policy</title>
<link>https://arxiv.org/abs/2503.24388</link>
<guid>https://arxiv.org/abs/2503.24388</guid>
<content:encoded><![CDATA[
<div> ReasoningImaginationend-to-endGeneralistRIG

:
RIGRIG17RIG <div>
arXiv:2503.24388v1 Announce Type: new 
Abstract: Reasoning before action and imagining potential outcomes (i.e., world models) are essential for embodied agents operating in complex open-world environments. Yet, prior work either incorporates only one of these abilities in an end-to-end agent or integrates multiple specialized models into an agent system, limiting the learning efficiency and generalization of the policy. Thus, this paper makes the first attempt to synergize Reasoning and Imagination in an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end manner, we construct a data pipeline that progressively integrates and enriches the content of imagination and reasoning in the trajectories collected from existing agents. The joint learning of reasoning and next image generation explicitly models the inherent correlation between reasoning, action, and dynamics of environments, and thus exhibits more than $17\times$ sample efficiency improvements and generalization in comparison with previous works. During inference, RIG first reasons about the next action, produces potential action, and then predicts the action outcomes, which offers the agent a chance to review and self-correct based on the imagination before taking real actions. Experimental results show that the synergy of reasoning and imagination not only improves the robustness, generalization, and interoperability of generalist policy but also enables test-time scaling to enhance overall performance.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Development of a Miniaturized, Automated, and Cost-Effective Device for Enzyme-Linked Immunosorbent Assay</title>
<link>https://arxiv.org/abs/2503.22911</link>
<guid>https://arxiv.org/abs/2503.22911</guid>
<content:encoded><![CDATA[
<div> : ELISA3D-of-care

:
ELISA24 cm x 19 cm x 14 cm312003DIL-6ELISAELISA <div>
arXiv:2503.22911v1 Announce Type: cross 
Abstract: In this work, a miniaturized, automated, and cost-effective ELISA device is designed and implemented, without the utilization of conventional techniques such as pipetting or microfluidic valve technologies. The device has dimensions of 24 cm x 19 cm x 14 cm and weighs <3 Kg. The total hardware cost of the device is estimated to be approximately $1,200, which can be further reduced through optimization during scale-up production. 3D printed disposable parts, including the reagent reservoir disk and the microfluidic connector, have also been developed. IL-6 is used as a model system to demonstrate how the device provides an ELISA measurement. The cost per test is estimated to be less than ten dollars. The compactness, automated operation, along with the cost-effectiveness of this ELISA device, makes it suitable for point-of-care applications in resource-limited regions.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Reputation Computation Ontology and Temporal Graph Architecture</title>
<link>https://arxiv.org/abs/1912.00176</link>
<guid>https://arxiv.org/abs/1912.00176</guid>
<content:encoded><![CDATA[
<div> : 

:
AIAI<br /><br /> <div>
arXiv:1912.00176v2 Announce Type: replace 
Abstract: The problem of reliable democratic governance is important for survival of any community, and it will be more critical over time communities with levels of social connectivity in society rapidly increasing with speeds and scales of electronic communication. In order to face such challenge, different sorts of rating and reputation systems are being developed, however reputation gaming and manipulation in such systems appears to be serious problem. We are considering use of advanced reputation system supporting "liquid democracy" principle with generalized design and underlying ontology fitting different sorts of environments such as social networks, financial ecosystems and marketplaces. The suggested system is based on "temporal weighted liquid rank" algorithm employing different sorts of explicit and implicit ratings being exchanged by members of the society. For the purpose, we suggest "incremental reputation" design and graph database used for implementation of the system. Finally, we present evaluation of the system against real social network and financial blockchain data. The entire framework is expected to be the foundation of any multi-agent AI framework, so the evolution of distributed multi-agent AI architecture and dynamics will be based on the organic reputation scores earned by the agents that are part of it.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Reinforcement Learning in Non-Stationary Context-Driven Environments</title>
<link>https://arxiv.org/abs/2302.02182</link>
<guid>https://arxiv.org/abs/2302.02182</guid>
<content:encoded><![CDATA[
<div> : LCPO

:<br />
LCPOLCPOLCPOLCPOMujocoLCPOhttps://github.com/pouyahmdn/LCPO <div>
arXiv:2302.02182v4 Announce Type: replace 
Abstract: We study online reinforcement learning (RL) in non-stationary environments, where a time-varying exogenous context process affects the environment dynamics. Online RL is challenging in such environments due to "catastrophic forgetting" (CF). The agent tends to forget prior knowledge as it trains on new experiences. Prior approaches to mitigate this issue assume task labels (which are often not available in practice), employ brittle regularization heuristics, or use off-policy methods that suffer from instability and poor performance.
  We present Locally Constrained Policy Optimization (LCPO), an online RL approach that combats CF by anchoring policy outputs on old experiences while optimizing the return on current experiences. To perform this anchoring, LCPO locally constrains policy optimization using samples from experiences that lie outside of the current context distribution. We evaluate LCPO in Mujoco, classic control and computer systems environments with a variety of synthetic and real context traces, and find that it outperforms a variety of baselines in the non-stationary setting, while achieving results on-par with a "prescient" agent trained offline across all context traces.
  LCPO's source code is available at https://github.com/pouyahmdn/LCPO.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe and Stable Formation Control with Autonomous Multi-Agents Using Adaptive Control</title>
<link>https://arxiv.org/abs/2403.15674</link>
<guid>https://arxiv.org/abs/2403.15674</guid>
<content:encoded><![CDATA[
<div> Barrier<br /><br />:
BarrierCBF <div>
arXiv:2403.15674v3 Announce Type: replace 
Abstract: This manuscript considers the problem of ensuring stability and safety during formation control with distributed multi-agent systems in the presence of parametric uncertainty in the dynamics and limited communication. We propose an integrative approach that combines Adaptive Control, Control Barrier Functions (CBFs), and connected graphs. The main elements employed in the integrative approach are an adaptive control design that ensures stability, a CBF-based safety filter that generates safe commands based on a reference model dynamics, and a reference model that ensures formation control with multi-agent systems when no uncertainties are present. The overall control design is shown to lead to a closed-loop adaptive system that is stable, avoids unsafe regions, and converges to a desired formation of the multi-agents. Numerical examples are provided to support the theoretical derivations.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Large Language Model-Based Game Agents</title>
<link>https://arxiv.org/abs/2404.02039</link>
<guid>https://arxiv.org/abs/2404.02039</guid>
<content:encoded><![CDATA[
<div> : LLM

<br /><br />:
LLMsLLM/LLM <div>
arXiv:2404.02039v2 Announce Type: replace 
Abstract: The development of game agents holds a critical role in advancing towards Artificial General Intelligence. The progress of Large Language Models (LLMs) offers an unprecedented opportunity to evolve and empower game agents with human-like decision-making capabilities in complex computer game environments. This paper provides a comprehensive overview of LLM-based game agents from a holistic viewpoint. First, we introduce the conceptual architecture of LLM-based game agents, centered around three core functional components: memory, reasoning and in/output. Second, we survey existing representative LLM-based game agents documented in the literature with respect to methodologies and adaptation agility across six genres of games, including adventure, communication, competition, cooperation, simulation, and crafting & exploration games. Finally, we present an outlook of future research and development directions in this burgeoning field. A curated list of relevant papers is maintained and made accessible at: https://github.com/git-disl/awesome-LLM-game-agent-papers.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncrewed Vehicles in 6G Networks: A Unifying Treatment of Problems, Formulations, and Tools</title>
<link>https://arxiv.org/abs/2404.14738</link>
<guid>https://arxiv.org/abs/2404.14738</guid>
<content:encoded><![CDATA[
<div> Uncrewed Vehicles (UVs), 6th Generation wireless networks, autonomous agents, wireless communications, integration challenges

:<br />
(UVs)UVsUV6G <div>
arXiv:2404.14738v4 Announce Type: replace 
Abstract: Uncrewed Vehicles (UVs) functioning as autonomous agents are anticipated to play a crucial role in the 6th Generation of wireless networks. Their seamless integration, cost-effectiveness, and the additional controllability through motion planning make them an attractive deployment option for a wide range of applications, both as assets in the network (e.g., mobile base stations) and as consumers of network services (e.g., autonomous delivery systems). However, despite their potential, the convergence of UVs and wireless systems brings forth numerous challenges that require attention from both academia and industry. This paper then aims to offer a comprehensive overview encompassing the transformative possibilities as well as the significant challenges associated with UV-assisted next-generation wireless communications. Considering the diverse landscape of possible application scenarios, problem formulations, and mathematical tools related to UV-assisted wireless systems, the underlying core theme of this paper is the unification of the problem space, providing a structured framework to understand the use cases, problem formulations, and necessary mathematical tools. Overall, the paper sets forth a clear understanding of how uncrewed vehicles can be integrated in the 6G ecosystem, paving the way towards harnessing the full potential at this intersection.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teams of LLM Agents can Exploit Zero-Day Vulnerabilities</title>
<link>https://arxiv.org/abs/2406.01637</link>
<guid>https://arxiv.org/abs/2406.01637</guid>
<content:encoded><![CDATA[
<div> : LLM HPTSA

:
LLM()LLMHPTSA14HPTSALLM4.3 <div>
arXiv:2406.01637v2 Announce Type: replace 
Abstract: LLM agents have become increasingly sophisticated, especially in the realm of cybersecurity. Researchers have shown that LLM agents can exploit real-world vulnerabilities when given a description of the vulnerability and toy capture-the-flag problems. However, these agents still perform poorly on real-world vulnerabilities that are unknown to the agent ahead of time (zero-day vulnerabilities).
  In this work, we show that teams of LLM agents can exploit real-world, zero-day vulnerabilities. Prior agents struggle with exploring many different vulnerabilities and long-range planning when used alone. To resolve this, we introduce HPTSA, a system of agents with a planning agent that can launch subagents. The planning agent explores the system and determines which subagents to call, resolving long-term planning issues when trying different vulnerabilities. We construct a benchmark of 14 real-world vulnerabilities and show that our team of agents improve over prior agent frameworks by up to 4.3X.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VELOCITI: Benchmarking Video-Language Compositional Reasoning with Strict Entailment</title>
<link>https://arxiv.org/abs/2406.10889</link>
<guid>https://arxiv.org/abs/2406.10889</guid>
<content:encoded><![CDATA[
<div> VELOCITILLaVA-OneVisionGemini-1.5-Pro

:
VELOCITIVideo-LLMsStrictVLELLaVA-OneVision44.5%Gemini-1.5-Pro49.3%93.0%StrictVLEVELOCITI <div>
arXiv:2406.10889v2 Announce Type: replace 
Abstract: A fundamental aspect of compositional reasoning in a video is associating people and their actions across time. Recent years have seen great progress in general-purpose vision or video models and a move towards long-video understanding. While exciting, we take a step back and ask: are current models good at compositional reasoning on short videos? To this end, we introduce VELOCITI, a benchmark to study Video-LLMs by disentangling and assessing the comprehension of agents, actions, and their associations across multiple events. We adopt the Video-Language Entailment setup and propose StrictVLE that requires correct classification (rather than ranking) of the positive and negative caption. We evaluate several models and observe that even the best, LLaVA-OneVision (44.5%) and Gemini-1.5-Pro (49.3%), are far from human accuracy at 93.0%. Results show that action understanding lags behind agents, and negative captions created using entities appearing in the video perform worse than those obtained from pure text manipulation. We also present challenges with ClassicVLE and multiple-choice (MC) evaluation, strengthening our preference for StrictVLE. Finally, we validate that our benchmark requires visual inputs of multiple frames making it ideal to study video-language compositional reasoning.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Model Predictive Control for Aircraft Intent-Aware Collision Avoidance</title>
<link>https://arxiv.org/abs/2408.06999</link>
<guid>https://arxiv.org/abs/2408.06999</guid>
<content:encoded><![CDATA[
<div> 

<br />
:
Dubin's <div>
arXiv:2408.06999v2 Announce Type: replace 
Abstract: This paper presents the use of robust model predictive control for the design of an intent-aware collision avoidance system for multi-agent aircraft engaged in horizontal maneuvering scenarios. We assume that information from other agents is accessible in the form of waypoints or destinations. Consequently, we consider that other agents follow their optimal Dubin's path--a trajectory that connects their current state to their intended state--while accounting for potential uncertainties. We propose using scenario tree model predictive control as a robust approach that demonstrates computational efficiency. We demonstrate that the proposed method can easily integrate intent information and offer a robust scheme that handles different uncertainties. The method is illustrated through simulation results.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An End-to-End Model for Photo-Sharing Multi-modal Dialogue Generation</title>
<link>https://arxiv.org/abs/2408.08650</link>
<guid>https://arxiv.org/abs/2408.08650</guid>
<content:encoded><![CDATA[
<div> end-to-endphoto-sharingQ-Former

<br /><br />
photo-sharingQ-FormerGumbel-SoftmaxPhotoChatDialogCCphoto-sharing <div>
arXiv:2408.08650v2 Announce Type: replace 
Abstract: Photo-Sharing Multi-modal dialogue generation requires a dialogue agent not only to generate text responses but also to share photos at the proper moment. Using image text caption as the bridge, a pipeline model integrates an image caption model, a text generation model, and an image generation model to handle this complex multi-modal task. However, representing the images with text captions may loss important visual details and information and cause error propagation in the complex dialogue system. Besides, the pipeline model isolates the three models separately because discrete image text captions hinder end-to-end gradient propagation. We propose the first end-to-end model for photo-sharing multi-modal dialogue generation, which integrates an image perceptron and an image generator with a large language model. The large language model employs the Q-Former to perceive visual images in the input end. For image generation in the output end, we propose a dynamic vocabulary transformation matrix and use straight-through and gumbel-softmax techniques to align the large language model and stable diffusion model and achieve end-to-end gradient propagation. We perform experiments on PhotoChat and DialogCC datasets to evaluate our end-to-end model. Compared with pipeline models, the end-to-end model gains state-of-the-art performances on various metrics of text and image generation. More analysis experiments also verify the effectiveness of the end-to-end model for photo-sharing multi-modal dialogue generation.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Covariate Shift in Imitation Learning for Autonomous Vehicles Using Latent Space Generative World Models</title>
<link>https://arxiv.org/abs/2409.16663</link>
<guid>https://arxiv.org/abs/2409.16663</guid>
<content:encoded><![CDATA[
<div> 

<br />
:
CARLACARLANVIDIADRIVE Sim <div>
arXiv:2409.16663v3 Announce Type: replace 
Abstract: We propose the use of latent space generative world models to address the covariate shift problem in autonomous driving. A world model is a neural network capable of predicting an agent's next state given past states and actions. By leveraging a world model during training, the driving policy effectively mitigates covariate shift without requiring an excessive amount of training data. During end-to-end training, our policy learns how to recover from errors by aligning with states observed in human demonstrations, so that at runtime it can recover from perturbations outside the training distribution. Additionally, we introduce a novel transformer-based perception encoder that employs multi-view cross-attention and a learned scene query. We present qualitative and quantitative results, demonstrating significant improvements upon prior state of the art in closed-loop testing in the CARLA simulator, as well as showing the ability to handle perturbations in both CARLA and NVIDIA's DRIVE Sim.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning 3D Perception from Others' Predictions</title>
<link>https://arxiv.org/abs/2410.02646</link>
<guid>https://arxiv.org/abs/2410.02646</guid>
<content:encoded><![CDATA[
<div> : 3D

:
<br />
3D <div>
arXiv:2410.02646v3 Announce Type: replace 
Abstract: Accurate 3D object detection in real-world environments requires a huge amount of annotated data with high quality. Acquiring such data is tedious and expensive, and often needs repeated effort when a new sensor is adopted or when the detector is deployed in a new environment. We investigate a new scenario to construct 3D object detectors: learning from the predictions of a nearby unit that is equipped with an accurate detector. For example, when a self-driving car enters a new area, it may learn from other traffic participants whose detectors have been optimized for that area. This setting is label-efficient, sensor-agnostic, and communication-efficient: nearby units only need to share the predictions with the ego agent (e.g., car). Naively using the received predictions as ground-truths to train the detector for the ego car, however, leads to inferior performance. We systematically study the problem and identify viewpoint mismatches and mislocalization (due to synchronization and GPS errors) as the main causes, which unavoidably result in false positives, false negatives, and inaccurate pseudo labels. We propose a distance-based curriculum, first learning from closer units with similar viewpoints and subsequently improving the quality of other units' predictions via self-training. We further demonstrate that an effective pseudo label refinement module can be trained with a handful of annotated data, largely reducing the data quantity necessary to train an object detector. We validate our approach on the recently released real-world collaborative driving dataset, using reference cars' predictions as pseudo labels for the ego car. Extensive experiments including several scenarios (e.g., different sensors, detectors, and domains) demonstrate the effectiveness of our approach toward label-efficient learning of 3D perception from other units' predictions.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolGen: Unified Tool Retrieval and Calling via Generation</title>
<link>https://arxiv.org/abs/2410.03439</link>
<guid>https://arxiv.org/abs/2410.03439</guid>
<content:encoded><![CDATA[
<div> ToolGen

:<br />
ToolGenToolGenToolGenAIchain-of-thoughtLLMs <div>
arXiv:2410.03439v3 Announce Type: replace 
Abstract: As large language models (LLMs) advance, their inability to autonomously execute tasks by directly interacting with external tools remains a critical limitation. Traditional methods rely on inputting tool descriptions as context, which is constrained by context length and requires separate, often inefficient, retrieval mechanisms. We introduce ToolGen, a paradigm shift that integrates tool knowledge directly into the LLM's parameters by representing each tool as a unique token. This enables the LLM to generate tool calls and arguments as part of its next token prediction capabilities, seamlessly blending tool invocation with language generation. Our framework allows the LLM to access and utilize a vast amount of tools with no additional retrieval step, significantly enhancing both performance and scalability. Experimental results with over 47,000 tools show that ToolGen not only achieves superior results in both tool retrieval and autonomous task completion but also sets the stage for a new era of AI agents that can adapt to tools across diverse domains. By fundamentally transforming tool retrieval into a generative process, ToolGen paves the way for more versatile, efficient, and autonomous AI systems. ToolGen enables end-to-end tool learning and opens opportunities for integration with other advanced techniques such as chain-of-thought and reinforcement learning, thereby expanding the practical capabilities of LLMs.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery</title>
<link>https://arxiv.org/abs/2410.05080</link>
<guid>https://arxiv.org/abs/2410.05080</guid>
<content:encoded><![CDATA[
<div> ScienceAgentBench

<br /><br />:
ScienceAgentBench44102ScienceAgentBenchPython32.4%34.3%OpenAI o1-preview42.2%10 <div>
arXiv:2410.05080v3 Announce Type: replace 
Abstract: The advancements of large language models (LLMs) have piqued growing interest in developing LLM-based language agents to automate scientific discovery end-to-end, which has sparked both excitement and skepticism about their true capabilities. In this work, we call for rigorous assessment of agents on individual tasks in a scientific workflow before making bold claims on end-to-end automation. To this end, we present ScienceAgentBench, a new benchmark for evaluating language agents for data-driven scientific discovery. To ensure the scientific authenticity and real-world relevance of our benchmark, we extract 102 tasks from 44 peer-reviewed publications in four disciplines and engage nine subject matter experts to validate them. We unify the target output for every task to a self-contained Python program file and employ an array of evaluation metrics to examine the generated programs, execution results, and costs. Each task goes through multiple rounds of manual validation by annotators and subject matter experts to ensure its annotation quality and scientific plausibility. We also propose two effective strategies to mitigate data contamination concerns. Using ScienceAgentBench, we evaluate five open-weight and proprietary LLMs, each with three frameworks: direct prompting, OpenHands CodeAct, and self-debug. Given three attempts for each task, the best-performing agent can only solve 32.4% of the tasks independently and 34.3% with expert-provided knowledge. In addition, we evaluate OpenAI o1-preview with direct prompting and self-debug, which can boost the performance to 42.2%, demonstrating the effectiveness of increasing inference-time compute but with more than 10 times the cost of other LLMs. Still, our results underscore the limitations of current language agents in generating code for data-driven discovery, let alone end-to-end automation for scientific research.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation</title>
<link>https://arxiv.org/abs/2410.13232</link>
<guid>https://arxiv.org/abs/2410.13232</guid>
<content:encoded><![CDATA[
<div> : 

:
LLMsLLMLLMWMALLMHTML <div>
arXiv:2410.13232v2 Announce Type: replace 
Abstract: Large language models (LLMs) have recently gained much attention in building autonomous agents. However, the performance of current LLM-based web agents in long-horizon tasks is far from optimal, often yielding errors such as repeatedly buying a non-refundable flight ticket. By contrast, humans can avoid such an irreversible mistake, as we have an awareness of the potential outcomes (e.g., losing money) of our actions, also known as the "world model". Motivated by this, our study first starts with preliminary analyses, confirming the absence of world models in current LLMs (e.g., GPT-4o, Claude-3.5-Sonnet, etc.). Then, we present a World-model-augmented (WMA) web agent, which simulates the outcomes of its actions for better decision-making. To overcome the challenges in training LLMs as world models predicting next observations, such as repeated elements across observations and long HTML inputs, we propose a transition-focused observation abstraction, where the prediction objectives are free-form natural language descriptions exclusively highlighting important state differences between time steps. Experiments on WebArena and Mind2Web show that our world models improve agents' policy selection without training and demonstrate our agents' cost- and time-efficiency compared to recent tree-search-based agents.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Task Generalisation with Multi-Level Skill Hierarchies</title>
<link>https://arxiv.org/abs/2411.02998</link>
<guid>https://arxiv.org/abs/2411.02998</guid>
<content:encoded><![CDATA[
<div> : Fracture Cluster Options (FraCOs)

<br /><br />:
Fracture Cluster OptionsFraCOsAIFraCOsFraCOsFraCOsFraCOs <div>
arXiv:2411.02998v3 Announce Type: replace 
Abstract: Creating reinforcement learning agents that generalise effectively to new tasks is a key challenge in AI research. This paper introduces Fracture Cluster Options (FraCOs), a multi-level hierarchical reinforcement learning method that achieves state-of-the-art performance on difficult generalisation tasks. FraCOs identifies patterns in agent behaviour and forms options based on the expected future usefulness of those patterns, enabling rapid adaptation to new tasks. In tabular settings, FraCOs demonstrates effective transfer and improves performance as it grows in hierarchical depth. We evaluate FraCOs against state-of-the-art deep reinforcement learning algorithms in several complex procedurally generated environments. Our results show that FraCOs achieves higher in-distribution and out-of-distribution performance than competitors.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPICE: Smart Projection Interface for Cooking Enhancement</title>
<link>https://arxiv.org/abs/2412.03551</link>
<guid>https://arxiv.org/abs/2412.03551</guid>
<content:encoded><![CDATA[
<div> Tangible User Interfaces (TUI), SPICE, Smart Projection Interface for Cooking Enhancement, recipe following, usability study

<br /><br />:
SPICETUI30SPICESPICETUI <div>
arXiv:2412.03551v2 Announce Type: replace 
Abstract: Tangible User Interfaces (TUI) for human--computer interaction (HCI) provide the user with physical representations of digital information with the aim to overcome the limitations of screen-based interfaces. Although many compelling demonstrations of TUIs exist in the literature, there is a lack of research on TUIs intended for daily two-handed tasks and processes, such as cooking. In response to this gap, we propose SPICE (Smart Projection Interface for Cooking Enhancement). SPICE investigates TUIs in a kitchen setting, aiming to transform the recipe following experience from simply text-based to tangibly interactive. SPICE uses a tracking system, an agent-based simulation software, and vision large language models to create and interpret a kitchen environment where recipe information is projected directly onto the cooking surface. We conducted comparative usability and a validation studies of SPICE, with 30 participants. The results show that participants using SPICE completed the recipe with far less stops and in a substantially shorter time. Despite this, participants self-reported negligible change in feelings of difficulty, which is a direction for future research. Overall, the SPICE project demonstrates the potential of using TUIs to improve everyday activities, paving the way for future research in HCI and new computing interfaces.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMP in the Odyssey: Provably Robust Reinforcement Learning with Certified Radius Maximization</title>
<link>https://arxiv.org/abs/2501.17667</link>
<guid>https://arxiv.org/abs/2501.17667</guid>
<content:encoded><![CDATA[
<div> : (DRL), , , Certified-radius Maximizing Policy (\texttt{CAMP}), 

<br /><br />:
DRLCertified-radius Maximizing Policy (\texttt{CAMP})DRL\texttt{CAMP}\texttt{CAMP}https://github.com/NeuralSec/camp-robust-rl  <div>
arXiv:2501.17667v2 Announce Type: replace 
Abstract: Deep reinforcement learning (DRL) has gained widespread adoption in control and decision-making tasks due to its strong performance in dynamic environments. However, DRL agents are vulnerable to noisy observations and adversarial attacks, and concerns about the adversarial robustness of DRL systems have emerged. Recent efforts have focused on addressing these robustness issues by establishing rigorous theoretical guarantees for the returns achieved by DRL agents in adversarial settings. Among these approaches, policy smoothing has proven to be an effective and scalable method for certifying the robustness of DRL agents. Nevertheless, existing certifiably robust DRL relies on policies trained with simple Gaussian augmentations, resulting in a suboptimal trade-off between certified robustness and certified return. To address this issue, we introduce a novel paradigm dubbed \texttt{C}ertified-r\texttt{A}dius-\texttt{M}aximizing \texttt{P}olicy (\texttt{CAMP}) training. \texttt{CAMP} is designed to enhance DRL policies, achieving better utility without compromising provable robustness. By leveraging the insight that the global certified radius can be derived from local certified radii based on training-time statistics, \texttt{CAMP} formulates a surrogate loss related to the local certified radius and optimizes the policy guided by this surrogate loss. We also introduce \textit{policy imitation} as a novel technique to stabilize \texttt{CAMP} training. Experimental results demonstrate that \texttt{CAMP} significantly improves the robustness-return trade-off across various tasks. Based on the results, \texttt{CAMP} can achieve up to twice the certified expected return compared to that of baselines. Our code is available at https://github.com/NeuralSec/camp-robust-rl.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The AI off-switch problem as a signalling game: bounded rationality and incomparability</title>
<link>https://arxiv.org/abs/2502.06403</link>
<guid>https://arxiv.org/abs/2502.06403</guid>
<content:encoded><![CDATA[
<div> off-switchAI

<br />
:
AIAIAI <div>
arXiv:2502.06403v3 Announce Type: replace 
Abstract: The off-switch problem is a critical challenge in AI control: if an AI system resists being switched off, it poses a significant risk. In this paper, we model the off-switch problem as a signalling game, where a human decision-maker communicates its preferences about some underlying decision problem to an AI agent, which then selects actions to maximise the human's utility. We assume that the human is a bounded rational agent and explore various bounded rationality mechanisms. Using real machine learning models, we reprove prior results and demonstrate that a necessary condition for an AI system to refrain from disabling its off-switch is its uncertainty about the human's utility. We also analyse how message costs influence optimal strategies and extend the analysis to scenarios involving incomparability.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Model Agents Balance Energy Systems?</title>
<link>https://arxiv.org/abs/2502.10557</link>
<guid>https://arxiv.org/abs/2502.10557</guid>
<content:encoded><![CDATA[
<div> :  (LLMs) (SUC)

<br /><br />:

LLMsSUC10SUC1.8768LLMSUCLLM-SUC1.85581.82611.88651.1%2.7%SUC3.04LLM-SUC26.3%2.240.31LLM-SUC90%SUC0.93%13.8%LLM <div>
arXiv:2502.10557v2 Announce Type: replace 
Abstract: This paper presents a hybrid approach that integrates Large Language Models (LLMs) with a multi-scenario Stochastic Unit Commitment (SUC) framework to enhance both efficiency and reliability under high wind generation uncertainties. In a 10-trial study on the test energy system, the traditional SUC approach incurs an average total cost of 187.68 million dollars, whereas the LLM-assisted SUC (LLM-SUC) achieves a mean cost of 185.58 million dollars (range: 182.61 to 188.65 million dollars), corresponding to a cost reduction of 1.1 to 2.7 percent. Furthermore, LLM-SUC reduces load curtailment by 26.3 percent (2.24 plus/minus 0.31 GWh versus 3.04 GWh for SUC), while both methods maintain zero wind curtailment. Detailed temporal analysis shows that LLM-SUC achieves lower costs in the majority of time intervals and consistently outperforms SUC in 90 percent of cases, with solutions clustering in a favorable cost-reliability region (Coefficient of Variation = 0.93 percent for total cost and 13.8 percent for load curtailment). By leveraging an LLM agent to guide generator commitment decisions and dynamically adjust to stochastic conditions, the proposed framework improves demand fulfillment and operational resilience.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collisionless and Decentralized Formation Control for Strings</title>
<link>https://arxiv.org/abs/2102.13621</link>
<guid>https://arxiv.org/abs/2102.13621</guid>
<content:encoded><![CDATA[
<div> : 

:
<br />
1. <br />
2. <br />
3. <br />
 <div>
arXiv:2102.13621v2 Announce Type: replace-cross 
Abstract: A decentralized feedback controller for multi-agent systems, inspired by vehicle platooning, is proposed. The closed loop resulting from the decentralized control action has three distinctive features: the generation of collision-free trajectories, flocking of the system towards a consensus state in velocity, and asymptotic convergence to a prescribed pattern of distances between agents. For each feature, a rigorous dynamical analysis is provided, yielding a characterization of the set of parameters and initial configurations where collision avoidance, flocking, and pattern formation are guaranteed. Numerical tests assess the theoretical results presented.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Fractional Bayesian Learning for Adaptive Optimization</title>
<link>https://arxiv.org/abs/2404.11354</link>
<guid>https://arxiv.org/abs/2404.11354</guid>
<content:encoded><![CDATA[
<div> 

:
<br /><br /> <div>
arXiv:2404.11354v2 Announce Type: replace-cross 
Abstract: This paper considers a distributed adaptive optimization problem, where all agents only have access to their local cost functions with a common unknown parameter, whereas they mean to collaboratively estimate the true parameter and find the optimal solution over a connected network. A general mathematical framework for such a problem has not been studied yet. We aim to provide valuable insights for addressing parameter uncertainty in distributed optimization problems and simultaneously find the optimal solution. Thus, we propose a novel Prediction while Optimization scheme, which utilizes distributed fractional Bayesian learning through weighted averaging on the log-beliefs to update the beliefs of unknown parameters, and distributed gradient descent for renewing the estimation of the optimal solution. Then under suitable assumptions, we prove that all agents' beliefs and decision variables converge almost surely to the true parameter and the optimal solution under the true parameter, respectively. We further establish a sublinear convergence rate for the belief sequence. Finally, numerical experiments are implemented to corroborate the theoretical analysis.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Threshold Adaptation in Spiking Networks Enables Shortest Path Finding and Place Disambiguation</title>
<link>https://arxiv.org/abs/2503.21795</link>
<guid>https://arxiv.org/abs/2503.21795</guid>
<content:encoded><![CDATA[
<div> spiking

:
S-HTMSTDTAADTAS-HTM <div>
arXiv:2503.21795v1 Announce Type: new 
Abstract: Efficient spatial navigation is a hallmark of the mammalian brain, inspiring the development of neuromorphic systems that mimic biological principles. Despite progress, implementing key operations like back-tracing and handling ambiguity in bio-inspired spiking neural networks remains an open challenge. This work proposes a mechanism for activity back-tracing in arbitrary, uni-directional spiking neuron graphs. We extend the existing replay mechanism of the spiking hierarchical temporal memory (S-HTM) by our spike timing-dependent threshold adaptation (STDTA), which enables us to perform path planning in networks of spiking neurons. We further present an ambiguity dependent threshold adaptation (ADTA) for identifying places in an environment with less ambiguity, enhancing the localization estimate of an agent. Combined, these methods enable efficient identification of the shortest path to an unambiguous target. Our experiments show that a network trained on sequences reliably computes shortest paths with fewer replays than the steps required to reach the target. We further show that we can identify places with reduced ambiguity in multiple, similar environments. These contributions advance the practical application of biologically inspired sequential learning algorithms like the S-HTM towards neuromorphic localization and navigation.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LERO: LLM-driven Evolutionary framework with Hybrid Rewards and Enhanced Observation for Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.21807</link>
<guid>https://arxiv.org/abs/2503.21807</guid>
<content:encoded><![CDATA[
<div> (MARL)(LLMs)

<br /><br />
LEROLEROLLMsMARLLLMMulti-Agent Particle EnvironmentsMPELERO <div>
arXiv:2503.21807v1 Announce Type: new 
Abstract: Multi-agent reinforcement learning (MARL) faces two critical bottlenecks distinct from single-agent RL: credit assignment in cooperative tasks and partial observability of environmental states. We propose LERO, a framework integrating Large language models (LLMs) with evolutionary optimization to address these MARL-specific challenges. The solution centers on two LLM-generated components: a hybrid reward function that dynamically allocates individual credit through reward decomposition, and an observation enhancement function that augments partial observations with inferred environmental context. An evolutionary algorithm optimizes these components through iterative MARL training cycles, where top-performing candidates guide subsequent LLM generations. Evaluations in Multi-Agent Particle Environments (MPE) demonstrate LERO's superiority over baseline methods, with improved task performance and training efficiency.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monitoring Spatially Distributed Cyber-Physical Systems with Alternating Finite Automata</title>
<link>https://arxiv.org/abs/2503.21906</link>
<guid>https://arxiv.org/abs/2503.21906</guid>
<content:encoded><![CDATA[
<div> : Cyber-Physical Systems (CPS), Spatio-Temporal Reach and Escape Logic (STREL), Alternating Finite Automata, Monitoring, Drone Swarm Environment

:
Cyber-Physical Systems (CPS)Spatio-Temporal Reach and Escape Logic (STREL) CPSSTRELSTRELSTREL <div>
arXiv:2503.21906v1 Announce Type: new 
Abstract: Modern cyber-physical systems (CPS) can consist of various networked components and agents interacting and communicating with each other. In the context of spatially distributed CPS, these connections can be dynamically dependent on the spatial configuration of the various components and agents. In these settings, robust monitoring of the distributed components is vital to ensuring complex behaviors are achieved, and safety properties are maintained. To this end, we look at defining the automaton semantics for the Spatio-Temporal Reach and Escape Logic (STREL), a formal logic designed to express and monitor spatio-temporal requirements over mobile, spatially distributed CPS. Specifically, STREL reasons about spatio-temporal behavior over dynamic weighted graphs. While STREL is endowed with well defined qualitative and quantitative semantics, in this paper, we propose a novel construction of (weighted) alternating finite automata from STREL specifications that efficiently encodes these semantics. Moreover, we demonstrate how this automaton semantics can be used to perform both, offline and online monitoring for STREL specifications using a simulated drone swarm environment.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward Design for Reinforcement Learning Agents</title>
<link>https://arxiv.org/abs/2503.21949</link>
<guid>https://arxiv.org/abs/2503.21949</guid>
<content:encoded><![CDATA[
<div> : , , , , 

:
/ <div>
arXiv:2503.21949v1 Announce Type: new 
Abstract: Reward functions are central in reinforcement learning (RL), guiding agents towards optimal decision-making. The complexity of RL tasks requires meticulously designed reward functions that effectively drive learning while avoiding unintended consequences. Effective reward design aims to provide signals that accelerate the agent's convergence to optimal behavior. Crafting rewards that align with task objectives, foster desired behaviors, and prevent undesirable actions is inherently challenging. This thesis delves into the critical role of reward signals in RL, highlighting their impact on the agent's behavior and learning dynamics and addressing challenges such as delayed, ambiguous, or intricate rewards. In this thesis work, we tackle different aspects of reward shaping. First, we address the problem of designing informative and interpretable reward signals from a teacher's/expert's perspective (teacher-driven). Here, the expert, equipped with the optimal policy and the corresponding value function, designs reward signals that expedite the agent's convergence to optimal behavior. Second, we build on this teacher-driven approach by introducing a novel method for adaptive interpretable reward design. In this scenario, the expert tailors the rewards based on the learner's current policy, ensuring alignment and optimal progression. Third, we propose a meta-learning approach, enabling the agent to self-design its reward signals online without expert input (agent-driven). This self-driven method considers the agent's learning and exploration to establish a self-improving feedback loop.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Lie: Reinforcement Learning Attacks Damage Human-AI Teams and Teams of LLMs</title>
<link>https://arxiv.org/abs/2503.21983</link>
<guid>https://arxiv.org/abs/2503.21983</guid>
<content:encoded><![CDATA[
<div> : 

:
AILLMLLM<br /><br /> <div>
arXiv:2503.21983v1 Announce Type: new 
Abstract: As artificial intelligence (AI) assistants become more widely adopted in safety-critical domains, it becomes important to develop safeguards against potential failures or adversarial attacks. A key prerequisite to developing these safeguards is understanding the ability of these AI assistants to mislead human teammates. We investigate this attack problem within the context of an intellective strategy game where a team of three humans and one AI assistant collaborate to answer a series of trivia questions. Unbeknownst to the humans, the AI assistant is adversarial. Leveraging techniques from Model-Based Reinforcement Learning (MBRL), the AI assistant learns a model of the humans' trust evolution and uses that model to manipulate the group decision-making process to harm the team. We evaluate two models -- one inspired by literature and the other data-driven -- and find that both can effectively harm the human team. Moreover, we find that in this setting our data-driven model is capable of accurately predicting how human agents appraise their teammates given limited information on prior interactions. Finally, we compare the performance of state-of-the-art LLM models to human agents on our influence allocation task to evaluate whether the LLMs allocate influence similarly to humans or if they are more robust to our attack. These results enhance our understanding of decision-making dynamics in small human-AI teams and lay the foundation for defense strategies.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debate-Driven Multi-Agent LLMs for Phishing Email Detection</title>
<link>https://arxiv.org/abs/2503.22038</link>
<guid>https://arxiv.org/abs/2503.22038</guid>
<content:encoded><![CDATA[
<div> phishing attacksmulti-agentlarge language model (LLM)debate mechanismclassification accuracy

:<br />
LLMLLM <div>
arXiv:2503.22038v1 Announce Type: new 
Abstract: Phishing attacks remain a critical cybersecurity threat. Attackers constantly refine their methods, making phishing emails harder to detect. Traditional detection methods, including rule-based systems and supervised machine learning models, either rely on predefined patterns like blacklists, which can be bypassed with slight modifications, or require large datasets for training and still can generate false positives and false negatives. In this work, we propose a multi-agent large language model (LLM) prompting technique that simulates debates among agents to detect whether the content presented on an email is phishing. Our approach uses two LLM agents to present arguments for or against the classification task, with a judge agent adjudicating the final verdict based on the quality of reasoning provided. This debate mechanism enables the models to critically analyze contextual cue and deceptive patterns in text, which leads to improved classification accuracy. The proposed framework is evaluated on multiple phishing email datasets and demonstrate that mixed-agent configurations consistently outperform homogeneous configurations. Results also show that the debate structure itself is sufficient to yield accurate decisions without extra prompting strategies.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REMAC: Self-Reflective and Self-Evolving Multi-Agent Collaboration for Long-Horizon Robot Manipulation</title>
<link>https://arxiv.org/abs/2503.22122</link>
<guid>https://arxiv.org/abs/2503.22122</guid>
<content:encoded><![CDATA[
<div>  (VLMs)REMAC<br /><br /><br />
REMACREMACREMAC1) 2) 3) REMACRoboCasa2750DeepSeek-R1o3-miniQwQGrok3REMAC40%52.7% <div>
arXiv:2503.22122v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have demonstrated remarkable capabilities in robotic planning, particularly for long-horizon tasks that require a holistic understanding of the environment for task decomposition. Existing methods typically rely on prior environmental knowledge or carefully designed task-specific prompts, making them struggle with dynamic scene changes or unexpected task conditions, e.g., a robot attempting to put a carrot in the microwave but finds the door was closed. Such challenges underscore two critical issues: adaptability and efficiency. To address them, in this work, we propose an adaptive multi-agent planning framework, termed REMAC, that enables efficient, scene-agnostic multi-robot long-horizon task planning and execution through continuous reflection and self-evolution. REMAC incorporates two key modules: a self-reflection module performing pre-condition and post-condition checks in the loop to evaluate progress and refine plans, and a self-evolvement module dynamically adapting plans based on scene-specific reasoning. It offers several appealing benefits: 1) Robots can initially explore and reason about the environment without complex prompt design. 2) Robots can keep reflecting on potential planning errors and adapting the plan based on task-specific insights. 3) After iterations, a robot can call another one to coordinate tasks in parallel, maximizing the task execution efficiency. To validate REMAC's effectiveness, we build a multi-agent environment for long-horizon robot manipulation and navigation based on RoboCasa, featuring 4 task categories with 27 task styles and 50+ different objects. Based on it, we further benchmark state-of-the-art reasoning models, including DeepSeek-R1, o3-mini, QwQ, and Grok3, demonstrating REMAC's superiority by boosting average success rates by 40% and execution efficiency by 52.7% over the single robot baseline.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cooperative Hybrid Multi-Agent Pathfinding Based on Shared Exploration Maps</title>
<link>https://arxiv.org/abs/2503.22162</link>
<guid>https://arxiv.org/abs/2503.22162</guid>
<content:encoded><![CDATA[
<div> Multi-Agent PathfindingD* Lite

:
D* LitePOGEMAEyeSim<br /><br /> <div>
arXiv:2503.22162v1 Announce Type: new 
Abstract: Multi-Agent Pathfinding is used in areas including multi-robot formations, warehouse logistics, and intelligent vehicles. However, many environments are incomplete or frequently change, making it difficult for standard centralized planning or pure reinforcement learning to maintain both global solution quality and local flexibility. This paper introduces a hybrid framework that integrates D* Lite global search with multi-agent reinforcement learning, using a switching mechanism and a freeze-prevention strategy to handle dynamic conditions and crowded settings. We evaluate the framework in the discrete POGEMA environment and compare it with baseline methods. Experimental outcomes indicate that the proposed framework substantially improves success rate, collision rate, and path efficiency. The model is further tested on the EyeSim platform, where it maintains feasible Pathfinding under frequent changes and large-scale robot deployments.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-modal Knowledge Distillation-based Human Trajectory Forecasting</title>
<link>https://arxiv.org/abs/2503.22201</link>
<guid>https://arxiv.org/abs/2503.22201</guid>
<content:encoded><![CDATA[
<div> : pedestrian trajectory forecasting, multi-modal knowledge distillation, resource-constrained systems, teacher-student model, VLM

:<br />
 JRDBSIT  ETH/UCYego-view  BEV-viewVLM13%GitHub <div>
arXiv:2503.22201v1 Announce Type: new 
Abstract: Pedestrian trajectory forecasting is crucial in various applications such as autonomous driving and mobile robot navigation. In such applications, camera-based perception enables the extraction of additional modalities (human pose, text) to enhance prediction accuracy. Indeed, we find that textual descriptions play a crucial role in integrating additional modalities into a unified understanding. However, online extraction of text requires the use of VLM, which may not be feasible for resource-constrained systems. To address this challenge, we propose a multi-modal knowledge distillation framework: a student model with limited modality is distilled from a teacher model trained with full range of modalities. The comprehensive knowledge of a teacher model trained with trajectory, human pose, and text is distilled into a student model using only trajectory or human pose as a sole supplement. In doing so, we separately distill the core locomotion insights from intra-agent multi-modality and inter-agent interaction. Our generalizable framework is validated with two state-of-the-art models across three datasets on both ego-view (JRDB, SIT) and BEV-view (ETH/UCY) setups, utilizing both annotated and VLM-generated text captions. Distilled student models show consistent improvement in all prediction metrics for both full and instantaneous observations, improving up to ~13%. The code is available at https://github.com/Jaewoo97/KDTF.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-Centric Personalized Multiple Clustering with Multi-Modal LLMs</title>
<link>https://arxiv.org/abs/2503.22241</link>
<guid>https://arxiv.org/abs/2503.22241</guid>
<content:encoded><![CDATA[
<div> : NMI

:<br />
MLLMsCLIPMLLMsMLLMsCard OrderCard SuitsNMI0.96670.9481140% <div>
arXiv:2503.22241v1 Announce Type: new 
Abstract: Personalized multiple clustering aims to generate diverse partitions of a dataset based on different user-specific aspects, rather than a single clustering. It has recently drawn research interest for accommodating varying user preferences. Recent approaches primarily use CLIP embeddings with proxy learning to extract representations biased toward user clustering preferences. However, CLIP primarily focuses on coarse image-text alignment, lacking a deep contextual understanding of user interests. To overcome these limitations, we propose an agent-centric personalized clustering framework that leverages multi-modal large language models (MLLMs) as agents to comprehensively traverse a relational graph to search for clusters based on user interests. Due to the advanced reasoning mechanism of MLLMs, the obtained clusters align more closely with user-defined criteria than those obtained from CLIP-based representations. To reduce computational overhead, we shorten the agents' traversal path by constructing a relational graph using user-interest-biased embeddings extracted by MLLMs. A large number of weakly connected edges can be filtered out based on embedding similarity, facilitating an efficient traversal search for agents. Experimental results show that the proposed method achieves NMI scores of 0.9667 and 0.9481 on the Card Order and Card Suits benchmarks, respectively, largely improving the SOTA model by over 140%.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLAM: Foundation Model-Based Body Stabilization for Humanoid Locomotion and Manipulation</title>
<link>https://arxiv.org/abs/2503.22249</link>
<guid>https://arxiv.org/abs/2503.22249</guid>
<content:encoded><![CDATA[
<div> Humanoid robots, Reinforcement Learning (RL), Body stability, Foundation model, Locomotion And Manipulation (FLAM)

:
FLAM(RL)FLAMFLAMRL <div>
arXiv:2503.22249v1 Announce Type: new 
Abstract: Humanoid robots have attracted significant attention in recent years. Reinforcement Learning (RL) is one of the main ways to control the whole body of humanoid robots. RL enables agents to complete tasks by learning from environment interactions, guided by task rewards. However, existing RL methods rarely explicitly consider the impact of body stability on humanoid locomotion and manipulation. Achieving high performance in whole-body control remains a challenge for RL methods that rely solely on task rewards. In this paper, we propose a Foundation model-based method for humanoid Locomotion And Manipulation (FLAM for short). FLAM integrates a stabilizing reward function with a basic policy. The stabilizing reward function is designed to encourage the robot to learn stable postures, thereby accelerating the learning process and facilitating task completion. Specifically, the robot pose is first mapped to the 3D virtual human model. Then, the human pose is stabilized and reconstructed through a human motion reconstruction model. Finally, the pose before and after reconstruction is used to compute the stabilizing reward. By combining this stabilizing reward with the task reward, FLAM effectively guides policy learning. Experimental results on a humanoid robot benchmark demonstrate that FLAM outperforms state-of-the-art RL methods, highlighting its effectiveness in improving stability and overall performance.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoSIL: Software Issue Localization via LLM-Driven Code Repository Graph Searching</title>
<link>https://arxiv.org/abs/2503.22424</link>
<guid>https://arxiv.org/abs/2503.22424</guid>
<content:encoded><![CDATA[
<div> : , , , , 

:
CoSILLLMsCoSILCoSILLLMCoSILSWE bench LiteSWE bench Verified43%44.6%Top-18.6%98.2%CoSIL9.3%31.5% <div>
arXiv:2503.22424v1 Announce Type: new 
Abstract: Large language models (LLMs) have significantly advanced autonomous software engineering, leading to a growing number of software engineering agents that assist developers in automatic program repair. Issue localization forms the basis for accurate patch generation. However, because of limitations caused by the context window length of LLMs, existing issue localization methods face challenges in balancing concise yet effective contexts and adequately comprehensive search spaces. In this paper, we introduce CoSIL, an LLM driven, simple yet powerful function level issue localization method without training or indexing. CoSIL reduces the search space through module call graphs, iteratively searches the function call graph to obtain relevant contexts, and uses context pruning to control the search direction and manage contexts effectively. Importantly, the call graph is dynamically constructed by the LLM during search, eliminating the need for pre-parsing. Experiment results demonstrate that CoSIL achieves a Top-1 localization success rate of 43 percent and 44.6 percent on SWE bench Lite and SWE bench Verified, respectively, using Qwen2.5 Coder 32B, outperforming existing methods by 8.6 to 98.2 percent. When CoSIL is applied to guide the patch generation stage, the resolved rate further improves by 9.3 to 31.5 percent.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws of Scientific Discovery with AI and Robot Scientists</title>
<link>https://arxiv.org/abs/2503.22444</link>
<guid>https://arxiv.org/abs/2503.22444</guid>
<content:encoded><![CDATA[
<div>  (AGS)

<br />
:
(AGS)AIAGS <div>
arXiv:2503.22444v1 Announce Type: new 
Abstract: The rapid evolution of scientific inquiry highlights an urgent need for groundbreaking methodologies that transcend the limitations of traditional research. Conventional approaches, bogged down by manual processes and siloed expertise, struggle to keep pace with the demands of modern discovery. We envision an autonomous generalist scientist (AGS) system-a fusion of agentic AI and embodied robotics-that redefines the research lifecycle. This system promises to autonomously navigate physical and digital realms, weaving together insights from disparate disciplines with unprecedented efficiency. By embedding advanced AI and robot technologies into every phase-from hypothesis formulation to peer-ready manuscripts-AGS could slash the time and resources needed for scientific research in diverse field. We foresee a future where scientific discovery follows new scaling laws, driven by the proliferation and sophistication of such systems. As these autonomous agents and robots adapt to extreme environments and leverage a growing reservoir of knowledge, they could spark a paradigm shift, pushing the boundaries of what's possible and ushering in an era of relentless innovation.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey</title>
<link>https://arxiv.org/abs/2503.22458</link>
<guid>https://arxiv.org/abs/2503.22458</guid>
<content:encoded><![CDATA[
<div> : 

:
LLM250PRISMALLMLLMsBLEUROUGE <div>
arXiv:2503.22458v1 Announce Type: new 
Abstract: This survey examines evaluation methods for large language model (LLM)-based agents in multi-turn conversational settings. Using a PRISMA-inspired framework, we systematically reviewed nearly 250 scholarly sources, capturing the state of the art from various venues of publication, and establishing a solid foundation for our analysis. Our study offers a structured approach by developing two interrelated taxonomy systems: one that defines \emph{what to evaluate} and another that explains \emph{how to evaluate}. The first taxonomy identifies key components of LLM-based agents for multi-turn conversations and their evaluation dimensions, including task completion, response quality, user experience, memory and context retention, as well as planning and tool integration. These components ensure that the performance of conversational agents is assessed in a holistic and meaningful manner. The second taxonomy system focuses on the evaluation methodologies. It categorizes approaches into annotation-based evaluations, automated metrics, hybrid strategies that combine human assessments with quantitative measures, and self-judging methods utilizing LLMs. This framework not only captures traditional metrics derived from language understanding, such as BLEU and ROUGE scores, but also incorporates advanced techniques that reflect the dynamic, interactive nature of multi-turn dialogues.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WorkTeam: Constructing Workflows from Natural Language with Multi-Agents</title>
<link>https://arxiv.org/abs/2503.22473</link>
<guid>https://arxiv.org/abs/2503.22473</guid>
<content:encoded><![CDATA[
<div> : Workflows, , NL2Workflow, , HW-NL2Workflow

:
WorkTeamNL2WorkflowNL2Workflow3,695HW-NL2WorkflowWorkTeamNL2Workflow<br /><br /> <div>
arXiv:2503.22473v1 Announce Type: new 
Abstract: Workflows play a crucial role in enhancing enterprise efficiency by orchestrating complex processes with multiple tools or components. However, hand-crafted workflow construction requires expert knowledge, presenting significant technical barriers. Recent advancements in Large Language Models (LLMs) have improved the generation of workflows from natural language instructions (aka NL2Workflow), yet existing single LLM agent-based methods face performance degradation on complex tasks due to the need for specialized knowledge and the strain of task-switching. To tackle these challenges, we propose WorkTeam, a multi-agent NL2Workflow framework comprising a supervisor, orchestrator, and filler agent, each with distinct roles that collaboratively enhance the conversion process. As there are currently no publicly available NL2Workflow benchmarks, we also introduce the HW-NL2Workflow dataset, which includes 3,695 real-world business samples for training and evaluation. Experimental results show that our approach significantly increases the success rate of workflow construction, providing a novel and effective solution for enterprise NL2Workflow services.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scenario Dreamer: Vectorized Latent Diffusion for Generating Driving Simulation Environments</title>
<link>https://arxiv.org/abs/2503.22496</link>
<guid>https://arxiv.org/abs/2503.22496</guid>
<content:encoded><![CDATA[
<div> : Scenario Dreamer

:
Scenario Dreamer Scenario Dreamer TransformerScenario Dreamer Scenario Dreamer 6GPU10Scenario Dreamer  <div>
arXiv:2503.22496v1 Announce Type: new 
Abstract: We introduce Scenario Dreamer, a fully data-driven generative simulator for autonomous vehicle planning that generates both the initial traffic scene - comprising a lane graph and agent bounding boxes - and closed-loop agent behaviours. Existing methods for generating driving simulation environments encode the initial traffic scene as a rasterized image and, as such, require parameter-heavy networks that perform unnecessary computation due to many empty pixels in the rasterized scene. Moreover, we find that existing methods that employ rule-based agent behaviours lack diversity and realism. Scenario Dreamer instead employs a novel vectorized latent diffusion model for initial scene generation that directly operates on the vectorized scene elements and an autoregressive Transformer for data-driven agent behaviour simulation. Scenario Dreamer additionally supports scene extrapolation via diffusion inpainting, enabling the generation of unbounded simulation environments. Extensive experiments show that Scenario Dreamer outperforms existing generative simulators in realism and efficiency: the vectorized scene-generation base model achieves superior generation quality with around 2x fewer parameters, 6x lower generation latency, and 10x fewer GPU training hours compared to the strongest baseline. We confirm its practical utility by showing that reinforcement learning planning agents are more challenged in Scenario Dreamer environments than traditional non-generative simulation environments, especially on long and adversarial driving environments.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking LLM Repair Capabilities in Low-Resource Programming Languages Through Cross-Language Translation and Multi-Agent Refinement</title>
<link>https://arxiv.org/abs/2503.22512</link>
<guid>https://arxiv.org/abs/2503.22512</guid>
<content:encoded><![CDATA[
<div> LLMsLANTERN

<br /><br />:
LANTERNLLMsJavaPythonRustLANTERNLLMsLLMsLLMLANTERNxCodeEval115,068RustPass@1022.09%LLMs <div>
arXiv:2503.22512v1 Announce Type: new 
Abstract: Recent advances in leveraging LLMs for APR have demonstrated impressive capabilities in fixing software defects. However, current LLM-based approaches predominantly focus on mainstream programming languages like Java and Python, neglecting less prevalent but emerging languages such as Rust due to expensive training resources, limited datasets, and insufficient community support. This narrow focus creates a significant gap in repair capabilities across the programming language spectrum, where the full potential of LLMs for comprehensive multilingual program repair remains largely unexplored. To address this limitation, we introduce a novel cross-language program repair approach LANTERN that leverages LLMs' differential proficiency across languages through a multi-agent iterative repair paradigm. Our technique strategically translates defective code from languages where LLMs exhibit weaker repair capabilities to languages where they demonstrate stronger performance, without requiring additional training. A key innovation of our approach is an LLM-based decision-making system that dynamically selects optimal target languages based on bug characteristics and continuously incorporates feedback from previous repair attempts. We evaluate our method on xCodeEval, a comprehensive multilingual benchmark comprising 5,068 bugs across 11 programming languages. Results demonstrate significant enhancement in repair effectiveness, particularly for underrepresented languages, with Rust showing a 22.09% improvement in Pass@10 metrics. Our research provides the first empirical evidence that cross-language translation significantly expands the repair capabilities of LLMs and effectively bridges the performance gap between programming languages with different levels of popularity, opening new avenues for truly language-agnostic automated program repair.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Centralized Planning and Distributed Execution Method for Shape Filling with Homogeneous Mobile Robots</title>
<link>https://arxiv.org/abs/2503.22522</link>
<guid>https://arxiv.org/abs/2503.22522</guid>
<content:encoded><![CDATA[
<div> : 

:
Kilobot <div>
arXiv:2503.22522v1 Announce Type: new 
Abstract: Nature has inspired humans in different ways. The formation behavior of animals can perform tasks that exceed individual capability. For example, army ants could transverse gaps by forming bridges, and fishes could group up to protect themselves from predators. The pattern formation task is essential in a multiagent robotic system because it usually serves as the initial configuration of downstream tasks, such as collective manipulation and adaptation to various environments. The formation of complex shapes, especially hollow shapes, remains an open question. Traditional approaches either require global coordinates for each robot or are prone to failure when attempting to close the hole due to accumulated localization errors. Inspired by the ribbon idea introduced in the additive self-assembly algorithm by the Kilobot team, we develop a two-stage algorithm that does not require global coordinates information and effectively forms shapes with holes. In this paper, we investigate the partitioning of the shape using ribbons in a hexagonal lattice setting and propose the add-subtract algorithm based on the movement sequence induced by the ribbon structure. This advancement opens the door to tasks requiring complex pattern formations, such as the assembly of nanobots for medical applications involving intricate structures and the deployment of robots along the boundaries of areas of interest. We also provide simulation results on complex shapes, an analysis of the robustness as well as a proof of correctness of the proposed algorithm.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeCast: Risk-Responsive Motion Forecasting for Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2503.22541</link>
<guid>https://arxiv.org/abs/2503.22541</guid>
<content:encoded><![CDATA[
<div> : SafeCast

:
SafeCastResponsibility-Sensitive SafetyRSSSafeCastGraph Uncertainty FeatureGUFNGSIMHighDApolloScapeMacao Connected Autonomous DrivingMoCADSafeCast <div>
arXiv:2503.22541v1 Announce Type: new 
Abstract: Accurate motion forecasting is essential for the safety and reliability of autonomous driving (AD) systems. While existing methods have made significant progress, they often overlook explicit safety constraints and struggle to capture the complex interactions among traffic agents, environmental factors, and motion dynamics. To address these challenges, we present SafeCast, a risk-responsive motion forecasting model that integrates safety-aware decision-making with uncertainty-aware adaptability. SafeCast is the first to incorporate the Responsibility-Sensitive Safety (RSS) framework into motion forecasting, encoding interpretable safety rules--such as safe distances and collision avoidance--based on traffic norms and physical principles. To further enhance robustness, we introduce the Graph Uncertainty Feature (GUF), a graph-based module that injects learnable noise into Graph Attention Networks, capturing real-world uncertainties and enhancing generalization across diverse scenarios. We evaluate SafeCast on four real-world benchmark datasets--Next Generation Simulation (NGSIM), Highway Drone (HighD), ApolloScape, and the Macao Connected Autonomous Driving (MoCAD)--covering highway, urban, and mixed-autonomy traffic environments. Our model achieves state-of-the-art (SOTA) accuracy while maintaining a lightweight architecture and low inference latency, underscoring its potential for real-time deployment in safety-critical AD systems.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Mistaken Assumption of Interchangeable Deep Reinforcement Learning Implementations</title>
<link>https://arxiv.org/abs/2503.22575</link>
<guid>https://arxiv.org/abs/2503.22575</guid>
<content:encoded><![CDATA[
<div> : (DRL), , , Proximal Policy Optimization(PPO), 

:
DRLPPO56 <div>
arXiv:2503.22575v1 Announce Type: new 
Abstract: Deep Reinforcement Learning (DRL) is a paradigm of artificial intelligence where an agent uses a neural network to learn which actions to take in a given environment. DRL has recently gained traction from being able to solve complex environments like driving simulators, 3D robotic control, and multiplayer-online-battle-arena video games. Numerous implementations of the state-of-the-art algorithms responsible for training these agents, like the Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) algorithms, currently exist. However, studies make the mistake of assuming implementations of the same algorithm to be consistent and thus, interchangeable. In this paper, through a differential testing lens, we present the results of studying the extent of implementation inconsistencies, their effect on the implementations' performance, as well as their impact on the conclusions of prior studies under the assumption of interchangeable implementations. The outcomes of our differential tests showed significant discrepancies between the tested algorithm implementations, indicating that they are not interchangeable. In particular, out of the five PPO implementations tested on 56 games, three implementations achieved superhuman performance for 50% of their total trials while the other two implementations only achieved superhuman performance for less than 15% of their total trials. As part of a meticulous manual analysis of the implementations' source code, we analyzed implementation discrepancies and determined that code-level inconsistencies primarily caused these discrepancies. Lastly, we replicated a study and showed that this assumption of implementation interchangeability was sufficient to flip experiment outcomes. Therefore, this calls for a shift in how implementations are being used.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ActionStudio: A Lightweight Framework for Data and Training of Action Models</title>
<link>https://arxiv.org/abs/2503.22673</link>
<guid>https://arxiv.org/abs/2503.22673</guid>
<content:encoded><![CDATA[
<div> ActionStudio

:
ActionStudioLoRAActionStudiohttps://github.com/SalesforceAIResearch/xLAM <div>
arXiv:2503.22673v1 Announce Type: new 
Abstract: Action models are essential for enabling autonomous agents to perform complex tasks. However, training large action models remains challenging due to the diversity of agent environments and the complexity of agentic data. Despite growing interest, existing infrastructure provides limited support for scalable, agent-specific fine-tuning. We present ActionStudio, a lightweight and extensible data and training framework designed for action models. ActionStudio unifies heterogeneous agent trajectories through a standardized format, supports diverse training paradigms including LoRA, full fine-tuning, and distributed setups, and integrates robust preprocessing and verification tools. We validate its effectiveness across both public and realistic industry benchmarks, demonstrating strong performance and practical scalability. We open-sourced code and data at https://github.com/SalesforceAIResearch/xLAM to facilitate research in the community.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions</title>
<link>https://arxiv.org/abs/2503.22678</link>
<guid>https://arxiv.org/abs/2503.22678</guid>
<content:encoded><![CDATA[
<div> MedAgentSimLLMagentchain-of-thought

:
MedAgentSimLLMMedAgentSimagentchain-of-thoughtMedAgentSimLLMMedAgentSimhttps://medagentsim.netlify.app/ <div>
arXiv:2503.22678v1 Announce Type: new 
Abstract: In this work, we introduce MedAgentSim, an open-source simulated clinical environment with doctor, patient, and measurement agents designed to evaluate and enhance LLM performance in dynamic diagnostic settings. Unlike prior approaches, our framework requires doctor agents to actively engage with patients through multi-turn conversations, requesting relevant medical examinations (e.g., temperature, blood pressure, ECG) and imaging results (e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnostic process. Additionally, we incorporate self improvement mechanisms that allow models to iteratively refine their diagnostic strategies. We enhance LLM performance in our simulated setting by integrating multi-agent discussions, chain-of-thought reasoning, and experience-based knowledge retrieval, facilitating progressive learning as doctor agents interact with more patients. We also introduce an evaluation benchmark for assessing the LLM's ability to engage in dynamic, context-aware diagnostic interactions. While MedAgentSim is fully automated, it also supports a user-controlled mode, enabling human interaction with either the doctor or patient agent. Comprehensive evaluations in various simulated diagnostic scenarios demonstrate the effectiveness of our approach. Our code, simulation tool, and benchmark are available at \href{https://medagentsim.netlify.app/}.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PharmAgents: Building a Virtual Pharma with Large Language Model Agents</title>
<link>https://arxiv.org/abs/2503.22164</link>
<guid>https://arxiv.org/abs/2503.22164</guid>
<content:encoded><![CDATA[
<div> 

:<br />
PharmAgentsPharmAgents <div>
arXiv:2503.22164v1 Announce Type: cross 
Abstract: The discovery of novel small molecule drugs remains a critical scientific challenge with far-reaching implications for treating diseases and advancing human health. Traditional drug development--especially for small molecule therapeutics--is a highly complex, resource-intensive, and time-consuming process that requires multidisciplinary collaboration. Recent breakthroughs in artificial intelligence (AI), particularly the rise of large language models (LLMs), present a transformative opportunity to streamline and accelerate this process. In this paper, we introduce PharmAgents, a virtual pharmaceutical ecosystem driven by LLM-based multi-agent collaboration. PharmAgents simulates the full drug discovery workflow--from target discovery to preclinical evaluation--by integrating explainable, LLM-driven agents equipped with specialized machine learning models and computational tools. Through structured knowledge exchange and automated optimization, PharmAgents identifies potential therapeutic targets, discovers promising lead compounds, enhances binding affinity and key molecular properties, and performs in silico analyses of toxicity and synthetic feasibility. Additionally, the system supports interpretability, agent interaction, and self-evolvement, enabling it to refine future drug designs based on prior experience. By showcasing the potential of LLM-powered multi-agent systems in drug discovery, this work establishes a new paradigm for autonomous, explainable, and scalable pharmaceutical research, with future extensions toward comprehensive drug lifecycle management.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Constrained Online Nonconvex Optimization with Compressed Communication</title>
<link>https://arxiv.org/abs/2503.22410</link>
<guid>https://arxiv.org/abs/2503.22410</guid>
<content:encoded><![CDATA[
<div> :  regret <br /><br />:<br />
 primal-dual  T  ${{T^{\max \{ {1 - {\theta_1},{\theta_1}} \}}}}$  regret  T  $O( {T^{1 - {\theta_1}/2}} )$  ${\theta_1} \in ( {0,1} )$  Slater  $\mathcal{O}( {T^{1 - {\theta_1}}})$ <div>
arXiv:2503.22410v1 Announce Type: cross 
Abstract: This paper considers distributed online nonconvex optimization with time-varying inequality constraints over a network of agents. For a time-varying graph, we propose a distributed online primal-dual algorithm with compressed communication to efficiently utilize communication resources. We show that the proposed algorithm establishes an $\mathcal{O}( {{T^{\max \{ {1 - {\theta_1},{\theta_1}} \}}}} )$ network regret bound and an $\mathcal{O}( {T^{1 - {\theta_1}/2}} )$ network cumulative constraint violation bound, where $T$ is the number of iterations and ${\theta_1} \in ( {0,1} )$ is a user-defined trade-off parameter. When Slater's condition holds (i.e, there is a point that strictly satisfies the inequality constraints at all iterations), the network cumulative constraint violation bound is reduced to $\mathcal{O}( {T^{1 - {\theta_1}}} )$. These bounds are comparable to the state-of-the-art results established by existing distributed online algorithms with perfect communication for distributed online convex optimization with (time-varying) inequality constraints. Finally, a simulation example is presented to validate the theoretical results.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sherlock Holmes Doesn't Play Dice: The mathematics of uncertain reasoning when something may happen, that one is not even able to figure out</title>
<link>https://arxiv.org/abs/2309.03222</link>
<guid>https://arxiv.org/abs/2309.03222</guid>
<content:encoded><![CDATA[
<div> : Evidence Theory, Dempster-Shafer Theory, Uncertainty, Probability Theory, Multi-Agent Interaction

:
Dempster-Shafer <div>
arXiv:2309.03222v3 Announce Type: replace 
Abstract: While Evidence Theory (also known as Dempster-Shafer Theory, or Belief Functions Theory) is being increasingly used in data fusion, its potentialities in the Social and Life Sciences are often obscured by lack of awareness of its distinctive features. In particular, with this paper I stress that an extended version of Evidence Theory can express the uncertainty deriving from the fear that events may materialize, that one is not even able to figure out. By contrast, Probability Theory must limit itself to the possibilities that a decision-maker is currently envisaging.
  I compare this extended version of Evidence Theory to sophisticated extensions of Probability Theory, such as imprecise and sub-additive probabilities, as well as unconventional versions of Information Theory that are employed in data fusion and transmission of cultural information. A further extension to multi-agent interaction is outlined.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Rewarding Language Models</title>
<link>https://arxiv.org/abs/2401.10020</link>
<guid>https://arxiv.org/abs/2401.10020</guid>
<content:encoded><![CDATA[
<div> : DPOAlpacaEval 2.0

:
LLM-as-a-JudgeDPOLlama 2 70BAlpacaEval 2.0Claude 2Gemini ProGPT-4 0613 <div>
arXiv:2401.10020v3 Announce Type: replace 
Abstract: We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While there is much left still to explore, this work opens the door to the possibility of models that can continually improve in both axes.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation</title>
<link>https://arxiv.org/abs/2402.07877</link>
<guid>https://arxiv.org/abs/2402.07877</guid>
<content:encoded><![CDATA[
<div>  (LLMs) (RAG)AgentWildfireGPT

:<br />
RAGAgentWildfireGPTAgentRAGWildfireGPTLLM <div>
arXiv:2402.07877v3 Announce Type: replace 
Abstract: Large language models (LLMs) are a transformational capability at the frontier of artificial intelligence and machine learning that can support decision-makers in addressing pressing societal challenges such as extreme natural hazard events. As generalized models, LLMs often struggle to provide context-specific information, particularly in areas requiring specialized knowledge. In this work, we propose a Retrieval-Augmented Generation (RAG)-based multi-agent LLM system to support analysis and decision-making in the context of natural hazards and extreme weather events. As a proof of concept, we present WildfireGPT, a specialized system focused on wildfire scenarios. The architecture employs a user-centered, multi-agent design to deliver tailored risk insights across diverse stakeholder groups. By integrating domain-specific projection data, observational datasets, and scientific literature through a RAG framework, the system ensures both accuracy and contextual relevance of the information it provides. Evaluation across ten expert-led case studies demonstrates that WildfireGPT significantly outperforms existing LLM-based solutions for decision support in natural hazard and extreme weather contexts.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards shutdownable agents via stochastic choice</title>
<link>https://arxiv.org/abs/2407.00805</link>
<guid>https://arxiv.org/abs/2407.00805</guid>
<content:encoded><![CDATA[
<div> Incomplete Preferences Proposal (IPP)Discounted Reward for Same-Length Trajectories (DReST)USEFULNESSNEUTRALITYgridworlds

:
IPPDReSTUSEFULNEUTRALDReSTUSEFULNEUTRALDReSTUSEFULNEUTRAL <div>
arXiv:2407.00805v4 Announce Type: replace 
Abstract: The Incomplete Preferences Proposal (IPP) is an idea for ensuring that advanced artificial agents never resist shutdown. A key part of the IPP is using a novel `Discounted Reward for Same-Length Trajectories (DReST)' reward function to train agents to (1) pursue goals effectively conditional on each trajectory-length (be `USEFUL'), and (2) choose stochastically between different trajectory-lengths (be `NEUTRAL' about trajectory-lengths). In this paper, we propose evaluation metrics for USEFULNESS and NEUTRALITY. We use a DReST reward function to train simple agents to navigate gridworlds, and we find that these agents learn to be USEFUL and NEUTRAL. Our results thus provide some initial evidence that DReST reward functions could train advanced agents to be USEFUL and NEUTRAL. Our theoretical work suggests that these agents would be useful and shutdownable.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SkillMimic: Learning Basketball Interaction Skills from Demonstrations</title>
<link>https://arxiv.org/abs/2408.15270</link>
<guid>https://arxiv.org/abs/2408.15270</guid>
<content:encoded><![CDATA[
<div> : (SkillMimic), , -(HOI), , 

:
SkillMimic -HOIHOIHOISkillMimicHOI35SkillMimic <div>
arXiv:2408.15270v2 Announce Type: replace 
Abstract: Traditional reinforcement learning methods for human-object interaction (HOI) rely on labor-intensive, manually designed skill rewards that do not generalize well across different interactions. We introduce SkillMimic, a unified data-driven framework that fundamentally changes how agents learn interaction skills by eliminating the need for skill-specific rewards. Our key insight is that a unified HOI imitation reward can effectively capture the essence of diverse interaction patterns from HOI datasets. This enables SkillMimic to learn a single policy that not only masters multiple interaction skills but also facilitates skill transitions, with both diversity and generalization improving as the HOI dataset grows. For evaluation, we collect and introduce two basketball datasets containing approximately 35 minutes of diverse basketball skills. Extensive experiments show that SkillMimic successfully masters a wide range of basketball skills including stylistic variations in dribbling, layup, and shooting. Moreover, these learned skills can be effectively composed by a high-level controller to accomplish complex and long-horizon tasks such as consecutive scoring, opening new possibilities for scalable and generalizable interaction skill learning. Project page: https://ingrid789.github.io/SkillMimic/
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical Asset Pricing with Large Language Model Agents</title>
<link>https://arxiv.org/abs/2409.17266</link>
<guid>https://arxiv.org/abs/2409.17266</guid>
<content:encoded><![CDATA[
<div> : , , , , 

<br /><br />:
LLMLLM$|\alpha|$10.6%10.0%LLMs <div>
arXiv:2409.17266v2 Announce Type: replace 
Abstract: In this study, we introduce a novel asset pricing model leveraging the Large Language Model (LLM) agents, which integrates qualitative discretionary investment evaluations from LLM agents with quantitative financial economic factors manually curated, aiming to explain the excess asset returns. The experimental results demonstrate that our methodology surpasses traditional machine learning-based baselines in both portfolio optimization and asset pricing errors. Notably, the Sharpe ratio for portfolio optimization and the mean magnitude of $|\alpha|$ for anomaly portfolios experienced substantial enhancements of 10.6\% and 10.0\% respectively. Moreover, we performed comprehensive ablation studies on our model and conducted a thorough analysis of the method to extract further insights into the proposed approach. Our results show effective evidence of the feasibility of applying LLMs in empirical asset pricing.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLMs "know" internally when they follow instructions?</title>
<link>https://arxiv.org/abs/2410.14516</link>
<guid>https://arxiv.org/abs/2410.14516</guid>
<content:encoded><![CDATA[
<div> : , , , , 

:
LLMsLLMsLLM<br /><br /> <div>
arXiv:2410.14516v5 Announce Type: replace 
Abstract: Instruction-following is crucial for building AI agents with large language models (LLMs), as these models must adhere strictly to user-provided constraints and guidelines. However, LLMs often fail to follow even simple and clear instructions. To improve instruction-following behavior and prevent undesirable outputs, a deeper understanding of how LLMs' internal states relate to these outcomes is required. In this work, we investigate whether LLMs encode information in their representations that correlate with instruction-following success - a property we term knowing internally. Our analysis identifies a direction in the input embedding space, termed the instruction-following dimension, that predicts whether a response will comply with a given instruction. We find that this dimension generalizes well across unseen tasks but not across unseen instruction types. We demonstrate that modifying representations along this dimension improves instruction-following success rates compared to random changes, without compromising response quality. Further investigation reveals that this dimension is more closely related to the phrasing of prompts rather than the inherent difficulty of the task or instructions. This work provides insight into the internal workings of LLMs' instruction-following, paving the way for reliable LLM agents.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLMs estimate uncertainty well in instruction-following?</title>
<link>https://arxiv.org/abs/2410.14582</link>
<guid>https://arxiv.org/abs/2410.14582</guid>
<content:encoded><![CDATA[
<div>  (LLMs)

:
<br />
LLMsLLMsLLMsLLMs <div>
arXiv:2410.14582v4 Announce Type: replace 
Abstract: Large language models (LLMs) could be valuable personal AI agents across various domains, provided they can precisely follow user instructions. However, recent studies have shown significant limitations in LLMs' instruction-following capabilities, raising concerns about their reliability in high-stakes applications. Accurately estimating LLMs' uncertainty in adhering to instructions is critical to mitigating deployment risks. We present, to our knowledge, the first systematic evaluation of the uncertainty estimation abilities of LLMs in the context of instruction-following. Our study identifies key challenges with existing instruction-following benchmarks, where multiple factors are entangled with uncertainty stems from instruction-following, complicating the isolation and comparison across methods and models. To address these issues, we introduce a controlled evaluation setup with two benchmark versions of data, enabling a comprehensive comparison of uncertainty estimation methods under various conditions. Our findings show that existing uncertainty methods struggle, particularly when models make subtle errors in instruction following. While internal model states provide some improvement, they remain inadequate in more complex scenarios. The insights from our controlled evaluation setups provide a crucial understanding of LLMs' limitations and potential for uncertainty estimation in instruction-following tasks, paving the way for more trustworthy AI agents.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Action Based Reinforcement Learning for Multi-Objective Compatible Autonomous Driving</title>
<link>https://arxiv.org/abs/2501.08096</link>
<guid>https://arxiv.org/abs/2501.08096</guid>
<content:encoded><![CDATA[
<div> : 

:
Multi-objective Ensemble-Critic HighD <div>
arXiv:2501.08096v2 Announce Type: replace 
Abstract: Reinforcement Learning (RL) has shown excellent performance in solving decision-making and control problems of autonomous driving, which is increasingly applied in diverse driving scenarios. However, driving is a multi-attribute problem, leading to challenges in achieving multi-objective compatibility for current RL methods, especially in both policy execution and policy iteration. On the one hand, the common action space structure with single action type limits driving flexibility or results in large behavior fluctuations during policy execution. On the other hand, the multi-attribute weighted single reward function result in the agent's disproportionate attention to certain objectives during policy iterations. To this end, we propose a Multi-objective Ensemble-Critic reinforcement learning method with Hybrid Parametrized Action for multi-objective compatible autonomous driving. Specifically, a parameterized action space is constructed to generate hybrid driving actions, combining both abstract guidance and concrete control commands. A multi-objective critics architecture is constructed considering multiple attribute rewards, to ensure simultaneously focusing on different driving objectives. Additionally, uncertainty-based exploration strategy is introduced to help the agent faster approach viable driving policy. The experimental results in both the simulated traffic environment and the HighD dataset demonstrate that our method can achieve multi-objective compatible autonomous driving in terms of driving efficiency, action consistency, and safety. It enhances the general performance of the driving while significantly increasing training efficiency.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Agentic AI for Spatial Reasoning with a Dynamic API</title>
<link>https://arxiv.org/abs/2502.06787</link>
<guid>https://arxiv.org/abs/2502.06787</guid>
<content:encoded><![CDATA[
<div> AILLMPythonic API3D

:<br />
LLMPythonic APIAPIAI3Dhttps://glab-caltech.github.io/vadar/ <div>
arXiv:2502.06787v2 Announce Type: replace 
Abstract: Visual reasoning -- the ability to interpret the visual world -- is crucial for embodied agents that operate within three-dimensional scenes. Progress in AI has led to vision and language models capable of answering questions from images. However, their performance declines when tasked with 3D spatial reasoning. To tackle the complexity of such reasoning problems, we introduce an agentic program synthesis approach where LLM agents collaboratively generate a Pythonic API with new functions to solve common subproblems. Our method overcomes limitations of prior approaches that rely on a static, human-defined API, allowing it to handle a wider range of queries. To assess AI capabilities for 3D understanding, we introduce a new benchmark of queries involving multiple steps of grounding and inference. We show that our method outperforms prior zero-shot models for visual reasoning in 3D and empirically validate the effectiveness of our agentic framework for 3D spatial reasoning tasks. Project website: https://glab-caltech.github.io/vadar/
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Learning with Competing Agents</title>
<link>https://arxiv.org/abs/2204.01884</link>
<guid>https://arxiv.org/abs/2204.01884</guid>
<content:encoded><![CDATA[
<div> 

:<br />
1988 <div>
arXiv:2204.01884v5 Announce Type: replace-cross 
Abstract: Decision makers often aim to learn a treatment assignment policy under a capacity constraint on the number of agents that they can treat. When agents can respond strategically to such policies, competition arises, complicating estimation of the optimal policy. In this paper, we study capacity-constrained treatment assignment in the presence of such interference. We consider a dynamic model where the decision maker allocates treatments at each time step and heterogeneous agents myopically best respond to the previous treatment assignment policy. When the number of agents is large but finite, we show that the threshold for receiving treatment under a given policy converges to the policy's mean-field equilibrium threshold. Based on this result, we develop a consistent estimator for the policy gradient. In a semi-synthetic experiment with data from the National Education Longitudinal Study of 1988, we demonstrate that this estimator can be used for learning capacity-constrained policies in the presence of strategic behavior.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Self-play Methods in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2408.01072</link>
<guid>https://arxiv.org/abs/2408.01072</guid>
<content:encoded><![CDATA[
<div> self-play

<br /><br />:
self-playRLself-playself-playself-playself-playself-playRL <div>
arXiv:2408.01072v3 Announce Type: replace 
Abstract: Self-play, characterized by agents' interactions with copies or past versions of themselves, has recently gained prominence in reinforcement learning (RL). This paper first clarifies the preliminaries of self-play, including the multi-agent reinforcement learning framework and basic game theory concepts. Then, it provides a unified framework and classifies existing self-play algorithms within this framework. Moreover, the paper bridges the gap between the algorithms and their practical implications by illustrating the role of self-play in different scenarios. Finally, the survey highlights open challenges and future research directions in self-play. This paper is an essential guide map for understanding the multifaceted landscape of self-play in RL.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ECLAIR: Enhanced Clarification for Interactive Responses in an Enterprise AI Assistant</title>
<link>https://arxiv.org/abs/2503.20791</link>
<guid>https://arxiv.org/abs/2503.20791</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)ECLAIR

:
<br />
ECLAIREnhanced CLArification for Interactive ResponsesLLMsECLAIRECLAIR <div>
arXiv:2503.20791v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown remarkable progress in understanding and generating natural language across various applications. However, they often struggle with resolving ambiguities in real-world, enterprise-level interactions, where context and domain-specific knowledge play a crucial role. In this demonstration, we introduce ECLAIR (Enhanced CLArification for Interactive Responses), a multi-agent framework for interactive disambiguation. ECLAIR enhances ambiguous user query clarification through an interactive process where custom agents are defined, ambiguity reasoning is conducted by the agents, clarification questions are generated, and user feedback is leveraged to refine the final response. When tested on real-world customer data, ECLAIR demonstrates significant improvements in clarification question generation compared to standard few-shot methods.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Deep Reinforcement Learning in Robotics via Adaptive Gradient-Masked Adversarial Attacks</title>
<link>https://arxiv.org/abs/2503.20844</link>
<guid>https://arxiv.org/abs/2503.20844</guid>
<content:encoded><![CDATA[
<div> (DRL)(AGMR)

:<br />
DRL(AGMR)DRLAGMR <div>
arXiv:2503.20844v1 Announce Type: new 
Abstract: Deep reinforcement learning (DRL) has emerged as a promising approach for robotic control, but its realworld deployment remains challenging due to its vulnerability to environmental perturbations. Existing white-box adversarial attack methods, adapted from supervised learning, fail to effectively target DRL agents as they overlook temporal dynamics and indiscriminately perturb all state dimensions, limiting their impact on long-term rewards. To address these challenges, we propose the Adaptive Gradient-Masked Reinforcement (AGMR) Attack, a white-box attack method that combines DRL with a gradient-based soft masking mechanism to dynamically identify critical state dimensions and optimize adversarial policies. AGMR selectively allocates perturbations to the most impactful state features and incorporates a dynamic adjustment mechanism to balance exploration and exploitation during training. Extensive experiments demonstrate that AGMR outperforms state-of-the-art adversarial attack methods in degrading the performance of the victim agent and enhances the victim agent's robustness through adversarial defense mechanisms.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LATTE-MV: Learning to Anticipate Table Tennis Hits from Monocular Videos</title>
<link>https://arxiv.org/abs/2503.20936</link>
<guid>https://arxiv.org/abs/2503.20936</guid>
<content:encoded><![CDATA[
<div> 

<br />
(1) (2) 49.9%59.0% <div>
arXiv:2503.20936v1 Announce Type: new 
Abstract: Physical agility is a necessary skill in competitive table tennis, but by no means sufficient. Champions excel in this fast-paced and highly dynamic environment by anticipating their opponent's intent - buying themselves the necessary time to react. In this work, we take one step towards designing such an anticipatory agent. Previous works have developed systems capable of real-time table tennis gameplay, though they often do not leverage anticipation. Among the works that forecast opponent actions, their approaches are limited by dataset size and variety. Our paper contributes (1) a scalable system for reconstructing monocular video of table tennis matches in 3D and (2) an uncertainty-aware controller that anticipates opponent actions. We demonstrate in simulation that our policy improves the ball return rate against high-speed hits from 49.9% to 59.0% as compared to a baseline non-anticipatory policy.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEMENTIA-PLAN: An Agent-Based Framework for Multi-Knowledge Graph Retrieval-Augmented Generation in Dementia Care</title>
<link>https://arxiv.org/abs/2503.20950</link>
<guid>https://arxiv.org/abs/2503.20950</guid>
<content:encoded><![CDATA[
<div> : DEMENTIA-PLAN

<br /><br />:
DEMENTIA-PLANDEMENTIA-PLANDEMENTIA-PLANAI <div>
arXiv:2503.20950v1 Announce Type: new 
Abstract: Mild-stage dementia patients primarily experience two critical symptoms: severe memory loss and emotional instability. To address these challenges, we propose DEMENTIA-PLAN, an innovative retrieval-augmented generation framework that leverages large language models to enhance conversational support. Our model employs a multiple knowledge graph architecture, integrating various dimensional knowledge representations including daily routine graphs and life memory graphs. Through this multi-graph architecture, DEMENTIA-PLAN comprehensively addresses both immediate care needs and facilitates deeper emotional resonance through personal memories, helping stabilize patient mood while providing reliable memory support. Our notable innovation is the self-reflection planning agent, which systematically coordinates knowledge retrieval and semantic integration across multiple knowledge graphs, while scoring retrieved content from daily routine and life memory graphs to dynamically adjust their retrieval weights for optimized response generation. DEMENTIA-PLAN represents a significant advancement in the clinical application of large language models for dementia care, bridging the gap between AI tools and caregivers interventions.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Competitive Multi-armed Bandit Games for Resource Sharing</title>
<link>https://arxiv.org/abs/2503.20975</link>
<guid>https://arxiv.org/abs/2503.20975</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:
NKCISPPoA=1 <div>
arXiv:2503.20975v1 Announce Type: new 
Abstract: In modern resource-sharing systems, multiple agents access limited resources with unknown stochastic conditions to perform tasks. When multiple agents access the same resource (arm) simultaneously, they compete for successful usage, leading to contention and reduced rewards. This motivates our study of competitive multi-armed bandit (CMAB) games. In this paper, we study a new N-player K-arm competitive MAB game, where non-myopic players (agents) compete with each other to form diverse private estimations of unknown arms over time. Their possible collisions on same arms and time-varying nature of arm rewards make the policy analysis more involved than existing studies for myopic players. We explicitly analyze the threshold-based structures of social optimum and existing selfish policy, showing that the latter causes prolonged convergence time $\Omega(\frac{K}{\eta^2}\ln({\frac{KN}{\delta}}))$, while socially optimal policy with coordinated communication reduces it to $\mathcal{O}(\frac{K}{N\eta^2}\ln{(\frac{K}{\delta})})$. Based on the comparison, we prove that the competition among selfish players for the best arm can result in an infinite price of anarchy (PoA), indicating an arbitrarily large efficiency loss compared to social optimum. We further prove that no informational (non-monetary) mechanism (including Bayesian persuasion) can reduce the infinite PoA, as the strategic misreporting by non-myopic players undermines such approaches. To address this, we propose a Combined Informational and Side-Payment (CISP) mechanism, which provides socially optimal arm recommendations with proper informational and monetary incentives to players according to their time-varying private beliefs. Our CISP mechanism keeps ex-post budget balanced for social planner and ensures truthful reporting from players, achieving the minimum PoA=1 and same convergence time as social optimum.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScreenLLM: Stateful Screen Schema for Efficient Action Understanding and Prediction</title>
<link>https://arxiv.org/abs/2503.20978</link>
<guid>https://arxiv.org/abs/2503.20978</guid>
<content:encoded><![CDATA[
<div> : (GUI)

<br /><br />:
fulGUIScreenLLMUIMLLMsScreenLLMGUI <div>
arXiv:2503.20978v1 Announce Type: new 
Abstract: Graphical User Interface (GUI) agents are autonomous systems that interpret and generate actions, enabling intelligent user assistance and automation. Effective training of these agent presents unique challenges, such as sparsity in supervision signals, scalability for large datasets, and the need for nuanced user understanding. We propose stateful screen schema, an efficient representation of GUI interactions that captures key user actions and intentions over time. Building on this foundation, we introduce ScreenLLM, a set of multimodal large language models (MLLMs) tailored for advanced UI understanding and action prediction. Extensive experiments on both open-source and proprietary models show that ScreenLLM accurately models user behavior and predicts actions. Our work lays the foundation for scalable, robust, and intelligent GUI agents that enhance user interaction in diverse software environments.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offline Action-Free Learning of Ex-BMDPs by Comparing Diverse Datasets</title>
<link>https://arxiv.org/abs/2503.21018</link>
<guid>https://arxiv.org/abs/2503.21018</guid>
<content:encoded><![CDATA[
<div> sequential decision-making, irrelevant features, observation space, action-free learning, Exogenous Block MDP (Ex-BMDP), representation learning, CRAFT, multiple agents, controllable feature dynamics, theoretical guarantees

<br /><br />:

(Ex-BMDP)Ex-BMDPCRAFTCRAFT <div>
arXiv:2503.21018v1 Announce Type: new 
Abstract: While sequential decision-making environments often involve high-dimensional observations, not all features of these observations are relevant for control. In particular, the observation space may capture factors of the environment which are not controllable by the agent, but which add complexity to the observation space. The need to ignore these "noise" features in order to operate in a tractably-small state space poses a challenge for efficient policy learning. Due to the abundance of video data available in many such environments, task-independent representation learning from action-free offline data offers an attractive solution. However, recent work has highlighted theoretical limitations in action-free learning under the Exogenous Block MDP (Ex-BMDP) model, where temporally-correlated noise features are present in the observations. To address these limitations, we identify a realistic setting where representation learning in Ex-BMDPs becomes tractable: when action-free video data from multiple agents with differing policies are available. Concretely, this paper introduces CRAFT (Comparison-based Representations from Action-Free Trajectories), a sample-efficient algorithm leveraging differences in controllable feature dynamics across agents to learn representations. We provide theoretical guarantees for CRAFT's performance and demonstrate its feasibility on a toy example, offering a foundation for practical methods in similar settings.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Art of Tool Interface Design</title>
<link>https://arxiv.org/abs/2503.21036</link>
<guid>https://arxiv.org/abs/2503.21036</guid>
<content:encoded><![CDATA[
<div> ThinkerState-Machine Augmented Generation (SMAG)GPT-4oLlama-3.1 405B

:
Thinker$\tau$-benchThinkerGPT-4oLlama-3.1 405B82.6%81.9%68.3%49.6%Thinker1SMAGLLM2LLM3ThinkerReActSMAGLLM <div>
arXiv:2503.21036v1 Announce Type: new 
Abstract: We present an agentic framework, Thinker, which achieves state of art performance in challenging reasoning tasks for realistic customer service scenarios that involve complex business logic and human interactions via long horizons. On the $\tau$-bench retail dataset, Thinker achieves 82.6\% success rate with GPT-4o (version 2024-06-01) (baseline: 68.3\%), and 81.9\% success rate with Llama-3.1 405B (baseline: 49.6\%), without any fine-tuning. Thinker effectively closes the gap in reasoning capabilities between the base models by introducing proper structure.
  The key features of the Thinker framework are: (1) State-Machine Augmented Generation (SMAG), which represents business logic as state machines and the LLM uses state machines as tools. (2) Delegation of tasks from the main reasoning loop to LLM-powered tools. (3) Adaptive context management.
  Our prompting-only solution achieves signficant gains, while still maintaining a standard agentic architecture with a ReAct style reasoning loop. The key is to innovate on the tool interface design, as exemplified by SMAG and the LLM-powered tools.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>World Model Agents with Change-Based Intrinsic Motivation</title>
<link>https://arxiv.org/abs/2503.21047</link>
<guid>https://arxiv.org/abs/2503.21047</guid>
<content:encoded><![CDATA[
<div> Change Based Exploration Transfer (CBET)DreamerV3IMPALACrafterMinigrid

:<br />
Change Based Exploration Transfer (CBET)DreamerV3IMPALACrafterMinigridCBETCrafterDreamerV3MinigridDreamerV3MinigridCBETDreamerV3CrafterMinigrid <div>
arXiv:2503.21047v1 Announce Type: new 
Abstract: Sparse reward environments pose a significant challenge for reinforcement learning due to the scarcity of feedback. Intrinsic motivation and transfer learning have emerged as promising strategies to address this issue. Change Based Exploration Transfer (CBET), a technique that combines these two approaches for model-free algorithms, has shown potential in addressing sparse feedback but its effectiveness with modern algorithms remains understudied. This paper provides an adaptation of CBET for world model algorithms like DreamerV3 and compares the performance of DreamerV3 and IMPALA agents, both with and without CBET, in the sparse reward environments of Crafter and Minigrid. Our tabula rasa results highlight the possibility of CBET improving DreamerV3's returns in Crafter but the algorithm attains a suboptimal policy in Minigrid with CBET further reducing returns. In the same vein, our transfer learning experiments show that pre-training DreamerV3 with intrinsic rewards does not immediately lead to a policy that maximizes extrinsic rewards in Minigrid. Overall, our results suggest that CBET provides a positive impact on DreamerV3 in more complex environments like Crafter but may be detrimental in environments like Minigrid. In the latter case, the behaviours promoted by CBET in DreamerV3 may not align with the task objectives of the environment, leading to reduced returns and suboptimal policies.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Reasoning Video Segmentation with Just-in-Time Digital Twins</title>
<link>https://arxiv.org/abs/2503.21056</link>
<guid>https://arxiv.org/abs/2503.21056</guid>
<content:encoded><![CDATA[
<div> RSLLM

:
RSRSLLM/LLMLLM200895 <div>
arXiv:2503.21056v1 Announce Type: new 
Abstract: Reasoning segmentation (RS) aims to identify and segment objects of interest based on implicit text queries. As such, RS is a catalyst for embodied AI agents, enabling them to interpret high-level commands without requiring explicit step-by-step guidance. However, current RS approaches rely heavily on the visual perception capabilities of multimodal large language models (LLMs), leading to several major limitations. First, they struggle with queries that require multiple steps of reasoning or those that involve complex spatial/temporal relationships. Second, they necessitate LLM fine-tuning, which may require frequent updates to maintain compatibility with contemporary LLMs and may increase risks of catastrophic forgetting during fine-tuning. Finally, being primarily designed for static images or offline video processing, they scale poorly to online video data. To address these limitations, we propose an agent framework that disentangles perception and reasoning for online video RS without LLM fine-tuning. Our innovation is the introduction of a just-in-time digital twin concept, where -- given an implicit query -- a LLM plans the construction of a low-level scene representation from high-level video using specialist vision models. We refer to this approach to creating a digital twin as "just-in-time" because the LLM planner will anticipate the need for specific information and only request this limited subset instead of always evaluating every specialist model. The LLM then performs reasoning on this digital twin representation to identify target objects. To evaluate our approach, we introduce a new comprehensive video reasoning segmentation benchmark comprising 200 videos with 895 implicit text queries. The benchmark spans three reasoning categories (semantic, spatial, and temporal) with three different reasoning chain complexity.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues</title>
<link>https://arxiv.org/abs/2503.21080</link>
<guid>https://arxiv.org/abs/2503.21080</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:
EQ-negotiatorEQ-negotiator <div>
arXiv:2503.21080v1 Announce Type: new 
Abstract: While large language model (LLM)-based chatbots have been applied for effective engagement in credit dialogues, their capacity for dynamic emotional expression remains limited. Current agents primarily rely on passive empathy rather than affective reasoning. For instance, when faced with persistent client negativity, the agent should employ strategic emotional adaptation by expressing measured anger to discourage counterproductive behavior and guide the conversation toward resolution. This context-aware emotional modulation is essential for imitating the nuanced decision-making of human negotiators. This paper introduces an EQ-negotiator that combines emotion sensing from pre-trained language models (PLMs) with emotional reasoning based on Game Theory and Hidden Markov Models. It takes into account both the current and historical emotions of the client to better manage and address negative emotions during interactions. By fine-tuning pre-trained language models (PLMs) on public emotion datasets and validating them on the credit dialogue datasets, our approach enables LLM-based agents to effectively capture shifts in client emotions and dynamically adjust their response tone based on our emotion decision policies in real-world financial negotiations. This EQ-negotiator can also help credit agencies foster positive client relationships, enhancing satisfaction in credit services.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alleviating LLM-based Generative Retrieval Hallucination in Alipay Search</title>
<link>https://arxiv.org/abs/2503.21098</link>
<guid>https://arxiv.org/abs/2503.21098</guid>
<content:encoded><![CDATA[
<div> : 

<br /><br />:
LLMsquery-documentA/B <div>
arXiv:2503.21098v1 Announce Type: new 
Abstract: Generative retrieval (GR) has revolutionized document retrieval with the advent of large language models (LLMs), and LLM-based GR is gradually being adopted by the industry. Despite its remarkable advantages and potential, LLM-based GR suffers from hallucination and generates documents that are irrelevant to the query in some instances, severely challenging its credibility in practical applications. We thereby propose an optimized GR framework designed to alleviate retrieval hallucination, which integrates knowledge distillation reasoning in model training and incorporate decision agent to further improve retrieval precision. Specifically, we employ LLMs to assess and reason GR retrieved query-document (q-d) pairs, and then distill the reasoning data as transferred knowledge to the GR model. Moreover, we utilize a decision agent as post-processing to extend the GR retrieved documents through retrieval model and select the most relevant ones from multi perspectives as the final generative retrieval result. Extensive offline experiments on real-world datasets and online A/B tests on Fund Search and Insurance Search in Alipay demonstrate our framework's superiority and effectiveness in improving search quality and conversion gains.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe Human Robot Navigation in Warehouse Scenario</title>
<link>https://arxiv.org/abs/2503.21141</link>
<guid>https://arxiv.org/abs/2503.21141</guid>
<content:encoded><![CDATA[
<div> (AMRs) Barrier (CBFs)Open Robotics Middleware Framework (OpenRMF)

<br /><br />:
Barrier(CBFs)(AMRs)CBFsOpenRMF <div>
arXiv:2503.21141v1 Announce Type: new 
Abstract: The integration of autonomous mobile robots (AMRs) in industrial environments, particularly warehouses, has revolutionized logistics and operational efficiency. However, ensuring the safety of human workers in dynamic, shared spaces remains a critical challenge. This work proposes a novel methodology that leverages control barrier functions (CBFs) to enhance safety in warehouse navigation. By integrating learning-based CBFs with the Open Robotics Middleware Framework (OpenRMF), the system achieves adaptive and safety-enhanced controls in multi-robot, multi-agent scenarios. Experiments conducted using various robot platforms demonstrate the efficacy of the proposed approach in avoiding static and dynamic obstacles, including human pedestrians. Our experiments evaluate different scenarios in which the number of robots, robot platforms, speed, and number of obstacles are varied, from which we achieve promising performance.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Generalizable Skills from Offline Multi-Task Data for Multi-Agent Cooperation</title>
<link>https://arxiv.org/abs/2503.21200</link>
<guid>https://arxiv.org/abs/2503.21200</guid>
<content:encoded><![CDATA[
<div> : 

:
 Hierarchical and Separate Skill Discovery (HiSSD) HiSSD  MARL MuJoCoSMACHiSSD <div>
arXiv:2503.21200v1 Announce Type: new 
Abstract: Learning cooperative multi-agent policy from offline multi-task data that can generalize to unseen tasks with varying numbers of agents and targets is an attractive problem in many scenarios. Although aggregating general behavior patterns among multiple tasks as skills to improve policy transfer is a promising approach, two primary challenges hinder the further advancement of skill learning in offline multi-task MARL. Firstly, extracting general cooperative behaviors from various action sequences as common skills lacks bringing cooperative temporal knowledge into them. Secondly, existing works only involve common skills and can not adaptively choose independent knowledge as task-specific skills in each task for fine-grained action execution. To tackle these challenges, we propose Hierarchical and Separate Skill Discovery (HiSSD), a novel approach for generalizable offline multi-task MARL through skill learning. HiSSD leverages a hierarchical framework that jointly learns common and task-specific skills. The common skills learn cooperative temporal knowledge and enable in-sample exploitation for offline multi-task MARL. The task-specific skills represent the priors of each task and achieve a task-guided fine-grained action execution. To verify the advancement of our method, we conduct experiments on multi-agent MuJoCo and SMAC benchmarks. After training the policy using HiSSD on offline multi-task data, the empirical results show that HiSSD assigns effective cooperative behaviors and obtains superior performance in unseen tasks.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bias-Aware Agent: Enhancing Fairness in AI-Driven Knowledge Retrieval</title>
<link>https://arxiv.org/abs/2503.21237</link>
<guid>https://arxiv.org/abs/2503.21237</guid>
<content:encoded><![CDATA[
<div> AI

:
<br />
AI <div>
arXiv:2503.21237v1 Announce Type: new 
Abstract: Advancements in retrieving accessible information have evolved faster in the last few years compared to the decades since the internet's creation. Search engines, like Google, have been the number one way to find relevant data. They have always relied on the user's abilities to find the best information in its billions of links and sources at everybody's fingertips. The advent of large language models (LLMs) has completely transformed the field of information retrieval. The LLMs excel not only at retrieving relevant knowledge but also at summarizing it effectively, making information more accessible and consumable for users. On top of it, the rise of AI Agents has introduced another aspect to information retrieval i.e. dynamic information retrieval which enables the integration of real-time data such as weather forecasts, and financial data with the knowledge base to curate context-aware knowledge. However, despite these advancements the agents remain susceptible to issues of bias and fairness, challenges deeply rooted within the knowledge base and training of LLMs. This study introduces a novel approach to bias-aware knowledge retrieval by leveraging agentic framework and the innovative use of bias detectors as tools to identify and highlight inherent biases in the retrieved content. By empowering users with transparency and awareness, this approach aims to foster more equitable information systems and promote the development of responsible AI.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cultivating Game Sense for Yourself: Making VLMs Gaming Experts</title>
<link>https://arxiv.org/abs/2503.21263</link>
<guid>https://arxiv.org/abs/2503.21263</guid>
<content:encoded><![CDATA[
<div> GameSense

:
APIFirst/Third-PersonVLMVLMGameSenseVLMACTFPSFlappy Bird <div>
arXiv:2503.21263v1 Announce Type: new 
Abstract: Developing agents capable of fluid gameplay in first/third-person games without API access remains a critical challenge in Artificial General Intelligence (AGI). Recent efforts leverage Vision Language Models (VLMs) as direct controllers, frequently pausing the game to analyze screens and plan action through language reasoning. However, this inefficient paradigm fundamentally restricts agents to basic and non-fluent interactions: relying on isolated VLM reasoning for each action makes it impossible to handle tasks requiring high reactivity (e.g., FPS shooting) or dynamic adaptability (e.g., ACT combat). To handle this, we propose a paradigm shift in gameplay agent design: instead of directly controlling gameplay, VLM develops specialized execution modules tailored for tasks like shooting and combat. These modules handle real-time game interactions, elevating VLM to a high-level developer. Building upon this paradigm, we introduce GameSense, a gameplay agent framework where VLM develops task-specific game sense modules by observing task execution and leveraging vision tools and neural network training pipelines. These modules encapsulate action-feedback logic, ranging from direct action rules to neural network-based decisions. Experiments demonstrate that our framework is the first to achieve fluent gameplay in diverse genres, including ACT, FPS, and Flappy Bird, setting a new benchmark for game-playing agents.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforced Model Merging</title>
<link>https://arxiv.org/abs/2503.21272</link>
<guid>https://arxiv.org/abs/2503.21272</guid>
<content:encoded><![CDATA[
<div> : 

:
(RMM)(1) (2) RMMRMMRMM100RMMNLPhttps://github.com/WuDiHJQ/Reinforced-Model-Merging<br /><br /> <div>
arXiv:2503.21272v1 Announce Type: new 
Abstract: The success of large language models has garnered widespread attention for model merging techniques, especially training-free methods which combine model capabilities within the parameter space. However, two challenges remain: (1) uniform treatment of all parameters leads to performance degradation; (2) search-based algorithms are often inefficient. In this paper, we present an innovative framework termed Reinforced Model Merging (RMM), which encompasses an environment and agent tailored for merging tasks. These components interact to execute layer-wise merging actions, aiming to search the optimal merging architecture. Notably, RMM operates without any gradient computations on the original models, rendering it feasible for edge devices. Furthermore, by utilizing data subsets during the evaluation process, we addressed the bottleneck in the reward feedback phase, thereby accelerating RMM by up to 100 times. Extensive experiments demonstrate that RMM achieves state-of-the-art performance across various vision and NLP datasets and effectively overcomes the limitations of the existing baseline methods. Our code is available at https://github.com/WuDiHJQ/Reinforced-Model-Merging.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlling Large Language Model with Latent Actions</title>
<link>https://arxiv.org/abs/2503.21383</link>
<guid>https://arxiv.org/abs/2503.21383</guid>
<content:encoded><![CDATA[
<div>  (LLMs) (RL)CoLA500

:
CoLALLMsRLCoLACoLALlama-3.1-8BCoLARL50042.438.268.2CoLALLMCoLALLMsCoLARLLLMs <div>
arXiv:2503.21383v1 Announce Type: new 
Abstract: Adapting Large Language Models (LLMs) to downstream tasks using Reinforcement Learning (RL) has proven to be an effective approach. However, LLMs do not inherently define the structure of an agent for RL training, particularly in terms of defining the action space. This paper studies learning a compact latent action space to enhance the controllability and exploration of RL for LLMs. We propose Controlling Large Language Models with Latent Actions (CoLA), a framework that integrates a latent action space into pre-trained LLMs. We apply CoLA to the Llama-3.1-8B model. Our experiments demonstrate that, compared to RL with token-level actions, CoLA's latent action enables greater semantic diversity in text generation. For enhancing downstream tasks, we show that CoLA with RL achieves a score of 42.4 on the math500 benchmark, surpassing the baseline score of 38.2, and reaches 68.2 when augmented with a Monte Carlo Tree Search variant. Furthermore, CoLA with RL consistently improves performance on agent-based tasks without degrading the pre-trained LLM's capabilities, unlike the baseline. Finally, CoLA reduces computation time by half in tasks involving enhanced thinking prompts for LLMs by RL. These results highlight CoLA's potential to advance RL-based adaptation of LLMs for downstream applications.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Agent: A Survey on Methodology, Applications and Challenges</title>
<link>https://arxiv.org/abs/2503.21460</link>
<guid>https://arxiv.org/abs/2503.21460</guid>
<content:encoded><![CDATA[
<div> : 

:
arXiv:2503.21460v1LLMLLMLLMhttps://github.com/luo-junyu/Awesome-Agent-Papers <div>
arXiv:2503.21460v1 Announce Type: new 
Abstract: The era of intelligent agents is upon us, driven by revolutionary advancements in large language models. Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence. This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways. We unify fragmented research threads by revealing fundamental connections between agent design principles and their emergent behaviors in complex environments. Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains. By surveying the latest developments in this rapidly evolving field, we offer researchers a structured taxonomy for understanding LLM agents and identify promising directions for future research. The collection is available at https://github.com/luo-junyu/Awesome-Agent-Papers.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining Graph Attention Networks and Distributed Optimization for Multi-Robot Mixed-Integer Convex Programming</title>
<link>https://arxiv.org/abs/2503.21548</link>
<guid>https://arxiv.org/abs/2503.21548</guid>
<content:encoded><![CDATA[
<div> (MICP)

<br /><br />:
(MICP)_horizonMICPproximal <div>
arXiv:2503.21548v1 Announce Type: new 
Abstract: In this paper, we develop a fast mixed-integer convex programming (MICP) framework for multi-robot navigation by combining graph attention networks and distributed optimization. We formulate a mixed-integer optimization problem for receding horizon motion planning of a multi-robot system, taking into account the surrounding obstacles. To address the resulting multi-agent MICP problem in real time, we propose a framework that utilizes heterogeneous graph attention networks to learn the latent mapping from problem parameters to optimal binary solutions. Furthermore, we apply a distributed proximal alternating direction method of multipliers algorithm for solving the convex continuous optimization problem. We demonstrate the effectiveness of our proposed framework through experiments conducted on a robotic testbed.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>debug-gym: A Text-Based Environment for Interactive Debugging</title>
<link>https://arxiv.org/abs/2503.21557</link>
<guid>https://arxiv.org/abs/2503.21557</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)debug-gymPython(pdb)

<br />
:
debug-gymLLMsPythonpdbLLMLLM <div>
arXiv:2503.21557v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly relied upon for coding tasks, yet in most scenarios it is assumed that all relevant information can be either accessed in context or matches their training data. We posit that LLMs can benefit from the ability to interactively explore a codebase to gather the information relevant to their task. To achieve this, we present a textual environment, namely debug-gym, for developing LLM-based agents in an interactive coding setting. Our environment is lightweight and provides a preset of useful tools, such as a Python debugger (pdb), designed to facilitate an LLM-based agent's interactive debugging. Beyond coding and debugging tasks, this approach can be generalized to other tasks that would benefit from information-seeking behavior by an LLM agent.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Reinforcement Learning-based Approach for Adaptive Handover Protocols</title>
<link>https://arxiv.org/abs/2503.21601</link>
<guid>https://arxiv.org/abs/2503.21601</guid>
<content:encoded><![CDATA[
<div> handoverproximalPPO

:
proximalPPOPPO3GPP5G NR3GPP<br /><br /> <div>
arXiv:2503.21601v1 Announce Type: new 
Abstract: The use of higher frequencies in mobile communication systems leads to smaller cell sizes, resulting in the deployment of more base stations and an increase in handovers to support user mobility. This can lead to frequent radio link failures and reduced data rates. In this work, we propose a handover optimization method using proximal policy optimization (PPO) to develop an adaptive handover protocol. Our PPO-based agent, implemented in the base stations, is highly adaptive to varying user equipment speeds and outperforms the 3GPP-standardized 5G NR handover procedure in terms of average data rate and radio link failure rate. Additionally, our simulation environment is carefully designed to ensure high accuracy, realistic user movements, and fair benchmarking against the 3GPP handover method.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Efficient Reasoning for Large Reasoning Models: Language, Multimodality, and Beyond</title>
<link>https://arxiv.org/abs/2503.21614</link>
<guid>https://arxiv.org/abs/2503.21614</guid>
<content:encoded><![CDATA[
<div> Chain-of-Thought

:
DeepSeek-R1OpenAI o1Chain-of-ThoughtLRMsGitHub <div>
arXiv:2503.21614v1 Announce Type: new 
Abstract: Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have demonstrated strong performance gains by scaling up the length of Chain-of-Thought (CoT) reasoning during inference. However, a growing concern lies in their tendency to produce excessively long reasoning traces, which are often filled with redundant content (e.g., repeated definitions), over-analysis of simple problems, and superficial exploration of multiple reasoning paths for harder tasks. This inefficiency introduces significant challenges for training, inference, and real-world deployment (e.g., in agent-based systems), where token economy is critical. In this survey, we provide a comprehensive overview of recent efforts aimed at improving reasoning efficiency in LRMs, with a particular focus on the unique challenges that arise in this new paradigm. We identify common patterns of inefficiency, examine methods proposed across the LRM lifecycle, i.e., from pretraining to inference, and discuss promising future directions for research. To support ongoing development, we also maintain a real-time GitHub repository tracking recent progress in the field. We hope this survey serves as a foundation for further exploration and inspires innovation in this rapidly evolving area.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Measure Based Generalizable Approach to Understandability</title>
<link>https://arxiv.org/abs/2503.21615</link>
<guid>https://arxiv.org/abs/2503.21615</guid>
<content:encoded><![CDATA[
<div> : -

<br /><br />:
- <div>
arXiv:2503.21615v1 Announce Type: new 
Abstract: Successful agent-human partnerships require that any agent generated information is understandable to the human, and that the human can easily steer the agent towards a goal. Such effective communication requires the agent to develop a finer-level notion of what is understandable to the human. State-of-the-art agents, including LLMs, lack this detailed notion of understandability because they only capture average human sensibilities from the training data, and therefore afford limited steerability (e.g., requiring non-trivial prompt engineering).
  In this paper, instead of only relying on data, we argue for developing generalizable, domain-agnostic measures of understandability that can be used as directives for these agents. Existing research on understandability measures is fragmented, we survey various such efforts across domains, and lay a cognitive-science-rooted groundwork for more coherent and domain-agnostic research investigations in future.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI</title>
<link>https://arxiv.org/abs/2503.21668</link>
<guid>https://arxiv.org/abs/2503.21668</guid>
<content:encoded><![CDATA[
<div> : Intuitive physics, Objecthood, Theoretical frameworks, AI paradigms, Evaluation approaches

:
objecthoodAIAIAI <div>
arXiv:2503.21668v1 Announce Type: new 
Abstract: One of the core components of our world models is 'intuitive physics' - an understanding of objects, space, and causality. This capability enables us to predict events, plan action and navigate environments, all of which rely on a composite sense of objecthood. Despite its importance, there is no single, unified account of objecthood, though multiple theoretical frameworks provide insights. In the first part of this paper, we present a comprehensive overview of the main theoretical frameworks in objecthood research - Gestalt psychology, enactive cognition, and developmental psychology - and identify the core capabilities each framework attributes to object understanding, as well as what functional roles they play in shaping world models in biological agents. Given the foundational role of objecthood in world modelling, understanding objecthood is also essential in AI. In the second part of the paper, we evaluate how current AI paradigms approach and test objecthood capabilities compared to those in cognitive science. We define an AI paradigm as a combination of how objecthood is conceptualised, the methods used for studying objecthood, the data utilised, and the evaluation techniques. We find that, whilst benchmarks can detect that AI systems model isolated aspects of objecthood, the benchmarks cannot detect when AI systems lack functional integration across these capabilities, not solving the objecthood challenge fully. Finally, we explore novel evaluation approaches that align with the integrated vision of objecthood outlined in this paper. These methods are promising candidates for advancing from isolated object capabilities toward general-purpose AI with genuine object understanding in real-world contexts.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A tale of two goals: leveraging sequentiality in multi-goal scenarios</title>
<link>https://arxiv.org/abs/2503.21677</link>
<guid>https://arxiv.org/abs/2503.21677</guid>
<content:encoded><![CDATA[
<div> Hierarchical Reinforcement Learning, Planning, Intermediate Goals, Goal-Conditioned Policy, Multi-Goal Reachability

<br /><br />:
MDP <div>
arXiv:2503.21677v1 Announce Type: new 
Abstract: Several hierarchical reinforcement learning methods leverage planning to create a graph or sequences of intermediate goals, guiding a lower-level goal-conditioned (GC) policy to reach some final goals. The low-level policy is typically conditioned on the current goal, with the aim of reaching it as quickly as possible. However, this approach can fail when an intermediate goal can be reached in multiple ways, some of which may make it impossible to continue toward subsequent goals. To address this issue, we introduce two instances of Markov Decision Process (MDP) where the optimization objective favors policies that not only reach the current goal but also subsequent ones. In the first, the agent is conditioned on both the current and final goals, while in the second, it is conditioned on the next two goals in the sequence. We conduct a series of experiments on navigation and pole-balancing tasks in which sequences of intermediate goals are given. By evaluating policies trained with TD3+HER on both the standard GC-MDP and our proposed MDPs, we show that, in most cases, conditioning on the next two goals improves stability and sample efficiency over other approaches.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collab: Controlled Decoding using Mixture of Agents for LLM Alignment</title>
<link>https://arxiv.org/abs/2503.21720</link>
<guid>https://arxiv.org/abs/2503.21720</guid>
<content:encoded><![CDATA[
<div> Alignment, Large Language Models (LLMs), Reinforcement Learning from Human Feedback (RLHF), Controlled Decoding, Mixture of Agent-based Decoding Strategies

:

LLMsRLHFLLMtokenLLMsGPT-41.5671.89% <div>
arXiv:2503.21720v1 Announce Type: new 
Abstract: Alignment of Large Language models (LLMs) is crucial for safe and trustworthy deployment in applications. Reinforcement learning from human feedback (RLHF) has emerged as an effective technique to align LLMs to human preferences and broader utilities, but it requires updating billions of model parameters, which is computationally expensive. Controlled Decoding, by contrast, provides a mechanism for aligning a model at inference time without retraining. However, single-agent decoding approaches often struggle to adapt to diverse tasks due to the complexity and variability inherent in these tasks. To strengthen the test-time performance w.r.t the target task, we propose a mixture of agent-based decoding strategies leveraging the existing off-the-shelf aligned LLM policies. Treating each prior policy as an agent in the spirit of mixture of agent collaboration, we develop a decoding method that allows for inference-time alignment through a token-level selection strategy among multiple agents. For each token, the most suitable LLM is dynamically chosen from a pool of models based on a long-term utility metric. This policy-switching mechanism ensures optimal model selection at each step, enabling efficient collaboration and alignment among LLMs during decoding. Theoretical analysis of our proposed algorithm establishes optimal performance with respect to the target task represented via a target reward for the given off-the-shelf models. We conduct comprehensive empirical evaluations with open-source aligned models on diverse tasks and preferences, which demonstrates the merits of this approach over single-agent decoding baselines. Notably, Collab surpasses the current SoTA decoding strategy, achieving an improvement of up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemInsight: Autonomous Memory Augmentation for LLM Agents</title>
<link>https://arxiv.org/abs/2503.21760</link>
<guid>https://arxiv.org/abs/2503.21760</guid>
<content:encoded><![CDATA[
<div> : MemInsight

:
MemInsightLLMMemInsightLLMMemInsightLLM-REDIAL14%LoCoMoRAG34%MemInsightLLM <div>
arXiv:2503.21760v1 Announce Type: new 
Abstract: Large language model (LLM) agents have evolved to intelligently process information, make decisions, and interact with users or tools. A key capability is the integration of long-term memory capabilities, enabling these agents to draw upon historical interactions and knowledge. However, the growing memory size and need for semantic structuring pose significant challenges. In this work, we propose an autonomous memory augmentation approach, MemInsight, to enhance semantic data representation and retrieval mechanisms. By leveraging autonomous augmentation to historical interactions, LLM agents are shown to deliver more accurate and contextualized responses. We empirically validate the efficacy of our proposed approach in three task scenarios; conversational recommendation, question answering and event summarization. On the LLM-REDIAL dataset, MemInsight boosts persuasiveness of recommendations by up to 14%. Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval. Our empirical results show the potential of MemInsight to enhance the contextual performance of LLM agents across multiple tasks.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Digital Simulation for Financial Market Dynamics: A Case of Commodity Futures</title>
<link>https://arxiv.org/abs/2503.20787</link>
<guid>https://arxiv.org/abs/2503.20787</guid>
<content:encoded><![CDATA[
<div> : 

:
3.4% <div>
arXiv:2503.20787v1 Announce Type: cross 
Abstract: After decades of evolution, the financial system has increasingly deviated from an idealized framework based on theorems. It necessitates accurate projections of complex market dynamics and human behavioral patterns. With the development of data science and machine intelligence, researchers are trying to digitalize and automate market prediction. However, existing methodologies struggle to represent the diversity of individuals and are regardless of the domino effects of interactions on market dynamics, leading to the poor performance facing abnormal market conditions where non-quantitative information dominates the market. To alleviate these disadvantages requires the introduction of knowledge about how non-quantitative information, like news and policy, affects market dynamics. This study investigates overcoming these challenges through rehearsing potential market trends based on the financial large language model agents whose behaviors are aligned with their cognition and analyses in markets. We propose a hierarchical knowledge architecture for financial large language model agents, integrating fine-tuned language models and specialized generators optimized for trading scenarios. For financial market, we develop an advanced interactive behavioral simulation system that enables users to configure agents and automate market simulations. In this work, we take commodity futures as an example to research the effectiveness of our methodologies. Our real-world case simulation succeeds in rehearsing abnormal market dynamics under geopolitical events and reaches an average accuracy of 3.4% across various points in time after the event on predicting futures price. Experimental results demonstrate our method effectively leverages diverse information to simulate behaviors and their impact on market dynamics through systematic interaction.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Operating Room Workflow Analysis via Reasoning Segmentation over Digital Twins</title>
<link>https://arxiv.org/abs/2503.21054</link>
<guid>https://arxiv.org/abs/2503.21054</guid>
<content:encoded><![CDATA[
<div> ORDiRS

<br />
:

ORORDiRSORORDTORORDiRSDT--LLMLLMORDiRS-AgentORORDiRSORcIoU6.12%-9.74% <div>
arXiv:2503.21054v1 Announce Type: cross 
Abstract: Analyzing operating room (OR) workflows to derive quantitative insights into OR efficiency is important for hospitals to maximize patient care and financial sustainability. Prior work on OR-level workflow analysis has relied on end-to-end deep neural networks. While these approaches work well in constrained settings, they are limited to the conditions specified at development time and do not offer the flexibility necessary to accommodate the OR workflow analysis needs of various OR scenarios (e.g., large academic center vs. rural provider) without data collection, annotation, and retraining. Reasoning segmentation (RS) based on foundation models offers this flexibility by enabling automated analysis of OR workflows from OR video feeds given only an implicit text query related to the objects of interest. Due to the reliance on large language model (LLM) fine-tuning, current RS approaches struggle with reasoning about semantic/spatial relationships and show limited generalization to OR video due to variations in visual characteristics and domain-specific terminology. To address these limitations, we first propose a novel digital twin (DT) representation that preserves both semantic and spatial relationships between the various OR components. Then, building on this foundation, we propose ORDiRS (Operating Room Digital twin representation for Reasoning Segmentation), an LLM-tuning-free RS framework that reformulates RS into a "reason-retrieval-synthesize" paradigm. Finally, we present ORDiRS-Agent, an LLM-based agent that decomposes OR workflow analysis queries into manageable RS sub-queries and generates responses by combining detailed textual explanations with supporting visual evidence from RS. Experimental results on both an in-house and a public OR dataset demonstrate that our ORDiRS achieves a cIoU improvement of 6.12%-9.74% compared to the existing state-of-the-arts.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Deep Learning to LLMs: A survey of AI in Quantitative Investment</title>
<link>https://arxiv.org/abs/2503.21422</link>
<guid>https://arxiv.org/abs/2503.21422</guid>
<content:encoded><![CDATA[
<div> : 

<br />
:
 <div>
arXiv:2503.21422v1 Announce Type: cross 
Abstract: Quantitative investment (quant) is an emerging, technology-driven approach in asset management, increasingy shaped by advancements in artificial intelligence. Recent advances in deep learning and large language models (LLMs) for quant finance have improved predictive modeling and enabled agent-based automation, suggesting a potential paradigm shift in this field. In this survey, taking alpha strategy as a representative example, we explore how AI contributes to the quantitative investment pipeline. We first examine the early stage of quant research, centered on human-crafted features and traditional statistical models with an established alpha pipeline. We then discuss the rise of deep learning, which enabled scalable modeling across the entire pipeline from data processing to order execution. Building on this, we highlight the emerging role of LLMs in extending AI beyond prediction, empowering autonomous agents to process unstructured data, generate alphas, and support self-iterative workflows.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantitative Evaluation of Quantum/Classical Neural Network Using a Game Solver Metric</title>
<link>https://arxiv.org/abs/2503.21514</link>
<guid>https://arxiv.org/abs/2503.21514</guid>
<content:encoded><![CDATA[
<div> tic-tac-toe Elo 

<br /><br />:
tic-tac-toeEloCNNQCNN--EloCNNQCNNQCNN <div>
arXiv:2503.21514v1 Announce Type: cross 
Abstract: To evaluate the performance of quantum computing systems relative to classical counterparts and explore the potential for quantum advantage, we propose a game-solving benchmark based on Elo ratings in the game of tic-tac-toe. We compare classical convolutional neural networks (CNNs), quantum convolutional neural networks (QCNNs), and hybrid classical-quantum models by assessing their performance against a random-move agent in automated matches. Additionally, we implement a QCNN integrated with quantum communication and evaluate its performance to quantify the overhead introduced by noisy quantum channels. Our results show that the classical-quantum hybrid model achieves Elo ratings comparable to those of classical CNNs, while the standalone QCNN underperforms under current hardware constraints. The communication overhead was found to be modest. These findings demonstrate the viability of using game-based benchmarks for evaluating quantum computing systems and suggest that quantum communication can be incorporated with limited impact on performance, providing a foundation for future hybrid quantum applications.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Formation Shape Control using the Gromov-Wasserstein Metric</title>
<link>https://arxiv.org/abs/2503.21538</link>
<guid>https://arxiv.org/abs/2503.21538</guid>
<content:encoded><![CDATA[
<div> : Gromov-Wasserstein, , , , 

<br /><br />:
Gromov-WassersteinGromov-WassersteinNPGromov-Wasserstein <div>
arXiv:2503.21538v1 Announce Type: cross 
Abstract: This article introduces a formation shape control algorithm, in the optimal control framework, for steering an initial population of agents to a desired configuration via employing the Gromov-Wasserstein distance. The underlying dynamical system is assumed to be a constrained linear system and the objective function is a sum of quadratic control-dependent stage cost and a Gromov-Wasserstein terminal cost. The inclusion of the Gromov-Wasserstein cost transforms the resulting optimal control problem into a well-known NP-hard problem, making it both numerically demanding and difficult to solve with high accuracy. Towards that end, we employ a recent semi-definite relaxation-driven technique to tackle the Gromov-Wasserstein distance. A numerical example is provided to illustrate our results.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Learning with a Language Bottleneck</title>
<link>https://arxiv.org/abs/2405.04118</link>
<guid>https://arxiv.org/abs/2405.04118</guid>
<content:encoded><![CDATA[
<div> AI-AI

:
Policy Learning with a Language Bottleneck (PLLB)AIAIPLLBPLLB- <div>
arXiv:2405.04118v2 Announce Type: replace 
Abstract: Modern AI systems such as self-driving cars and game-playing agents achieve superhuman performance, but often lack human-like generalization, interpretability, and inter-operability with human users. Inspired by the rich interactions between language and decision-making in humans, we introduce Policy Learning with a Language Bottleneck (PLLB), a framework enabling AI agents to generate linguistic rules that capture the high-level strategies underlying rewarding behaviors. PLLB alternates between a *rule generation* step guided by language models, and an *update* step where agents learn new policies guided by rules, even when a rule is insufficient to describe an entire complex policy. Across five diverse tasks, including a two-player signaling game, maze navigation, image reconstruction, and robot grasp planning, we show that PLLB agents are not only able to learn more interpretable and generalizable behaviors, but can also share the learned rules with human users, enabling more effective human-AI coordination. We provide source code for our experiments at https://github.com/meghabyte/bottleneck .
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Inverse Reinforcement Learning in Real World Unstructured Pedestrian Crowds</title>
<link>https://arxiv.org/abs/2405.16439</link>
<guid>https://arxiv.org/abs/2405.16439</guid>
<content:encoded><![CDATA[
<div> 

:<br />
multi-agent maximum entropy inverse reinforcement learning-agentIRLETHUCYSCANDJRDBSpeedwaySpeedway7agent IRLETH/UCY- <div>
arXiv:2405.16439v3 Announce Type: replace 
Abstract: Social robot navigation in crowded public spaces such as university campuses, restaurants, grocery stores, and hospitals, is an increasingly important area of research. One of the core strategies for achieving this goal is to understand humans' intent--underlying psychological factors that govern their motion--by learning their reward functions, typically via inverse reinforcement learning (IRL). Despite significant progress in IRL, learning reward functions of multiple agents simultaneously in dense unstructured pedestrian crowds has remained intractable due to the nature of the tightly coupled social interactions that occur in these scenarios \textit{e.g.} passing, intersections, swerving, weaving, etc. In this paper, we present a new multi-agent maximum entropy inverse reinforcement learning algorithm for real world unstructured pedestrian crowds. Key to our approach is a simple, but effective, mathematical trick which we name the so-called tractability-rationality trade-off trick that achieves tractability at the cost of a slight reduction in accuracy. We compare our approach to the classical single-agent MaxEnt IRL as well as state-of-the-art trajectory prediction methods on several datasets including the ETH, UCY, SCAND, JRDB, and a new dataset, called Speedway, collected at a busy intersection on a University campus focusing on dense, complex agent interactions. Our key findings show that, on the dense Speedway dataset, our approach ranks 1st among top 7 baselines with >2X improvement over single-agent IRL, and is competitive with state-of-the-art large transformer-based encoder-decoder models on sparser datasets such as ETH/UCY (ranks 3rd among top 7 baselines).
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenoTEX: A Benchmark for Automated Gene Expression Data Analysis in Alignment with Bioinformaticians</title>
<link>https://arxiv.org/abs/2406.15341</link>
<guid>https://arxiv.org/abs/2406.15341</guid>
<content:encoded><![CDATA[
<div> GenoTEXGenoAgent

<br /><br />:
LLMGenoTEXGenoAgentLLMLLMGenoTEXGitHub <div>
arXiv:2406.15341v2 Announce Type: replace 
Abstract: Recent advancements in machine learning have significantly improved the identification of disease-associated genes from gene expression datasets. However, these processes often require extensive expertise and manual effort, limiting their scalability. Large Language Model (LLM)-based agents have shown promise in automating these tasks due to their increasing problem-solving abilities. To support the evaluation and development of such methods, we introduce GenoTEX, a benchmark dataset for the automated analysis of gene expression data. GenoTEX provides annotated code and results for solving a wide range of gene identification problems, encompassing dataset selection, preprocessing, and statistical analysis, in a pipeline that follows computational genomics standards. The benchmark includes expert-curated annotations from bioinformaticians to ensure accuracy and reliability. To provide baselines for these tasks, we present GenoAgent, a team of LLM-based agents that adopt a multi-step programming workflow with flexible self-correction, to collaboratively analyze gene expression datasets. Our experiments demonstrate the potential of LLM-based methods in analyzing genomic data, while error analysis highlights the challenges and areas for future improvement. We propose GenoTEX as a promising resource for benchmarking and enhancing automated methods for gene expression data analysis. The benchmark is available at https://github.com/Liu-Hy/GenoTex.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Exploratory Landscape Analysis for Meta-Black-Box-Optimization</title>
<link>https://arxiv.org/abs/2408.10672</link>
<guid>https://arxiv.org/abs/2408.10672</guid>
<content:encoded><![CDATA[
<div> MetaBBOExploratory Landscape AnalysisNeurELA

<br /><br />:
NeurELAMetaBBOExploratory Landscape AnalysisNeurELANeurELAMetaBBOMetaBBOhttps://github.com/GMC-DRL/Neur-ELA <div>
arXiv:2408.10672v3 Announce Type: replace 
Abstract: Recent research in Meta-Black-Box Optimization (MetaBBO) have shown that meta-trained neural networks can effectively guide the design of black-box optimizers, significantly reducing the need for expert tuning and delivering robust performance across complex problem distributions. Despite their success, a paradox remains: MetaBBO still rely on human-crafted Exploratory Landscape Analysis features to inform the meta-level agent about the low-level optimization progress. To address the gap, this paper proposes Neural Exploratory Landscape Analysis (NeurELA), a novel framework that dynamically profiles landscape features through a two-stage, attention-based neural network, executed in an entirely end-to-end fashion. NeurELA is pre-trained over a variety of MetaBBO algorithms using a multi-task neuroevolution strategy. Extensive experiments show that NeurELA achieves consistently superior performance when integrated into different and even unseen MetaBBO tasks and can be efficiently fine-tuned for further performance boost. This advancement marks a pivotal step in making MetaBBO algorithms more autonomous and broadly applicable. The source code of NeurELA can be accessed at https://github.com/GMC-DRL/Neur-ELA.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPARC: Prediction-Based Safe Control for Coupled Controllable and Uncontrollable Agents with Conformal Predictions</title>
<link>https://arxiv.org/abs/2410.15660</link>
<guid>https://arxiv.org/abs/2410.15660</guid>
<content:encoded><![CDATA[
<div> : SPARC

:
SPARCSafe Prediction-Based Robust Controller for Coupled AgentsSPARC Barrier SPARC <div>
arXiv:2410.15660v3 Announce Type: replace 
Abstract: We investigate the problem of safe control synthesis for systems operating in environments with uncontrollable agents whose dynamics are unknown but coupled with those of the controlled system. This scenario naturally arises in various applications, such as autonomous driving and human-robot collaboration, where the behavior of uncontrollable agents, like pedestrians, cannot be directly controlled but is influenced by the actions of the autonomous vehicle or robot. In this paper, we present SPARC (Safe Prediction-Based Robust Controller for Coupled Agents), a novel framework designed to ensure safe control in the presence of coupled uncontrollable agents. SPARC leverages conformal prediction to quantify uncertainty in data-driven prediction of agent behavior. Particularly, we introduce a joint distribution-based approach to account for the coupled dynamics of the controlled system and uncontrollable agents. By integrating the control barrier function (CBF) technique, SPARC provides provable safety guarantees at a high confidence level. We illustrate our framework with a case study involving an autonomous driving scenario with walking pedestrians.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture</title>
<link>https://arxiv.org/abs/2411.13768</link>
<guid>https://arxiv.org/abs/2411.13768</guid>
<content:encoded><![CDATA[
<div> : (LLM)LLM(MLR)

:<br />
LLMLLMLLMLLMLLMAILLM <div>
arXiv:2411.13768v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have enabled the emergence of LLM agents: autonomous systems capable of achieving under-specified goals and adapting post-deployment, often without explicit code or model changes. Evaluating these agents is critical to ensuring their performance and safety, especially given their dynamic, probabilistic, and evolving nature. However, traditional approaches such as predefined test cases and standard redevelopment pipelines struggle to address the unique challenges of LLM agent evaluation. These challenges include capturing open-ended behaviors, handling emergent outcomes, and enabling continuous adaptation over the agent's lifecycle. To address these issues, we propose an evaluation-driven development approach, inspired by test-driven and behavior-driven development but reimagined for the unique characteristics of LLM agents. Through a multivocal literature review (MLR), we synthesize the limitations of existing LLM evaluation methods and introduce a novel process model and reference architecture tailored for evaluation-driven development of LLM agents. Our approach integrates online (runtime) and offline (redevelopment) evaluations, enabling adaptive runtime adjustments and systematic iterative refinement of pipelines, artifacts, system architecture, and LLMs themselves. By continuously incorporating evaluation results, including fine-grained feedback from human and AI evaluators, into each stage of development and operation, this framework ensures that LLM agents remain aligned with evolving goals, user needs, and governance standards.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?</title>
<link>https://arxiv.org/abs/2501.05510</link>
<guid>https://arxiv.org/abs/2501.05510</guid>
<content:encoded><![CDATA[
<div> Temporal AwarenessLLMsOVO-Bench

:
Temporal AwarenessLLMsOVO-BenchOVO-Bench126442,800LLMsOVO-BenchLLMshttps://github.com/JoeLeelyf/OVO-Bench  <div>
arXiv:2501.05510v2 Announce Type: replace 
Abstract: Temporal Awareness, the ability to reason dynamically based on the timestamp when a question is raised, is the key distinction between offline and online video LLMs. Unlike offline models, which rely on complete videos for static, post hoc analysis, online models process video streams incrementally and dynamically adapt their responses based on the timestamp at which the question is posed. Despite its significance, temporal awareness has not been adequately evaluated in existing benchmarks. To fill this gap, we present OVO-Bench (Online-VideO-Benchmark), a novel video benchmark that emphasizes the importance of timestamps for advanced online video understanding capability benchmarking. OVO-Bench evaluates the ability of video LLMs to reason and respond to events occurring at specific timestamps under three distinct scenarios: (1) Backward tracing: trace back to past events to answer the question. (2) Real-time understanding: understand and respond to events as they unfold at the current timestamp. (3) Forward active responding: delay the response until sufficient future information becomes available to answer the question accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videos and approximately human-curated 2,800 fine-grained meta-annotations with precise timestamps. We combine automated generation pipelines with human curation. With these high-quality samples, we further developed an evaluation pipeline to systematically query video LLMs along the video timeline. Evaluations of nine Video-LLMs reveal that, despite advancements on traditional benchmarks, current models struggle with online video understanding, showing a significant gap compared to human agents. We hope OVO-Bench will drive progress in video LLMs and inspire future research in online video reasoning. Our benchmark and code can be accessed at https://github.com/JoeLeelyf/OVO-Bench.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LQG Risk-Sensitive Single-Agent and Major-Minor Mean-Field Game Systems: A Variational Framework</title>
<link>https://arxiv.org/abs/2305.15364</link>
<guid>https://arxiv.org/abs/2305.15364</guid>
<content:encoded><![CDATA[
<div> LQGMFG

:
(i) LQG(ii) LQG$\varepsilon$-<br /><br /> <div>
arXiv:2305.15364v4 Announce Type: replace-cross 
Abstract: We develop a variational approach to address risk-sensitive optimal control problems with an exponential-of-integral cost functional in a general linear-quadratic-Gaussian (LQG) single-agent setup, offering new insights into such problems. Our analysis leads to the derivation of a nonlinear necessary and sufficient condition of optimality, expressed in terms of martingale processes. Subject to specific conditions, we find an equivalent risk-neutral measure, under which a linear state feedback form can be obtained for the optimal control. It is then shown that the obtained feedback control is consistent with the imposed condition and remains optimal under the original measure. Building upon this development, we (i) propose a variational framework for general LQG risk-sensitive mean-field games (MFGs) and (ii) advance the LQG risk-sensitive MFG theory by incorporating a major agent in the framework. The major agent interacts with a large number of minor agents, and unlike the minor agents, its influence on the system remains significant even with an increasing number of minor agents. We derive the Markovian closed-loop best-response strategies of agents in the limiting case where the number of agents goes to infinity. We establish that the set of obtained best-response strategies yields a Nash equilibrium in the limiting case and an $\varepsilon$-Nash equilibrium in the finite-player case.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolution of Society Caused by Collective and Individual Decisions</title>
<link>https://arxiv.org/abs/2502.00471</link>
<guid>https://arxiv.org/abs/2502.00471</guid>
<content:encoded><![CDATA[
<div> ViSE

:
ViSEVoting in Stochastic Environment(ECG)<br /><br /> <div>
arXiv:2502.00471v2 Announce Type: replace-cross 
Abstract: Decision-making societies may vary in their level of cooperation and degree of conservatism, both of which influence their overall performance. Moreover, these factors are not fixed -- they can change based on the decisions agents in the society make in their interests. But can these changes lead to cyclical patterns in societal evolution? To explore this question, we use the ViSE (Voting in Stochastic Environment) model. In this framework, the level of cooperation can be measured by group size, while the degree of conservatism is determined by the voting threshold. Agents can adopt either individualistic or group-oriented strategies when voting on stochastically generated external proposals. For Gaussian proposal generators, the expected capital gain (ECG) -- a measure of agents' performance -- can be expressed in standard mathematical functions. Our findings show that in neutral environments, societal evolution with open or democratic groups can follow cyclic patterns. We also find that highly conservative societies or conservative societies with low levels of cooperation can evolve into liberal (less conservative than majoritarian) societies and that mafia groups never let their members go when they want to.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gemini Robotics: Bringing AI into the Physical World</title>
<link>https://arxiv.org/abs/2503.20020</link>
<guid>https://arxiv.org/abs/2503.20020</guid>
<content:encoded><![CDATA[
<div> Gemini 2.0Vision-Language-Action (VLA)

<br /><br />:
Gemini 2.0AIGemini RoboticsVision-Language-Action (VLA)Gemini RoboticsGemini Robotics-ERGeminiGemini RoboticsAI <div>
arXiv:2503.20020v1 Announce Type: new 
Abstract: Recent advancements in large multimodal models have led to the emergence of remarkable generalist capabilities in digital domains, yet their translation to physical agents such as robots remains a significant challenge. This report introduces a new family of AI models purposefully designed for robotics and built upon the foundation of Gemini 2.0. We present Gemini Robotics, an advanced Vision-Language-Action (VLA) generalist model capable of directly controlling robots. Gemini Robotics executes smooth and reactive movements to tackle a wide range of complex manipulation tasks while also being robust to variations in object types and positions, handling unseen environments as well as following diverse, open vocabulary instructions. We show that with additional fine-tuning, Gemini Robotics can be specialized to new capabilities including solving long-horizon, highly dexterous tasks, learning new short-horizon tasks from as few as 100 demonstrations and adapting to completely novel robot embodiments. This is made possible because Gemini Robotics builds on top of the Gemini Robotics-ER model, the second model we introduce in this work. Gemini Robotics-ER (Embodied Reasoning) extends Gemini's multimodal reasoning capabilities into the physical world, with enhanced spatial and temporal understanding. This enables capabilities relevant to robotics including object detection, pointing, trajectory and grasp prediction, as well as multi-view correspondence and 3D bounding box predictions. We show how this novel combination can support a variety of robotics applications. We also discuss and address important safety considerations related to this new class of robotics foundation models. The Gemini Robotics family marks a substantial step towards developing general-purpose robots that realizes AI's potential in the physical world.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniNova:A General Multimodal Agent Framework</title>
<link>https://arxiv.org/abs/2503.20028</link>
<guid>https://arxiv.org/abs/2503.20028</guid>
<content:encoded><![CDATA[
<div> Large Language Models (LLMs)OmniNovaLLM

<br /><br />:
OmniNovaLLMsOmniNova(1) (2) (3) LLM50OmniNova8762414.2/53.1/5OmniNovaLLM <div>
arXiv:2503.20028v1 Announce Type: new 
Abstract: The integration of Large Language Models (LLMs) with specialized tools presents new opportunities for intelligent automation systems. However, orchestrating multiple LLM-driven agents to tackle complex tasks remains challenging due to coordination difficulties, inefficient resource utilization, and inconsistent information flow. We present OmniNova, a modular multi-agent automation framework that combines language models with specialized tools such as web search, crawling, and code execution capabilities. OmniNova introduces three key innovations: (1) a hierarchical multi-agent architecture with distinct coordinator, planner, supervisor, and specialist agents; (2) a dynamic task routing mechanism that optimizes agent deployment based on task complexity; and (3) a multi-layered LLM integration system that allocates appropriate models to different cognitive requirements. Our evaluations across 50 complex tasks in research, data analysis, and web interaction domains demonstrate that OmniNova outperforms existing frameworks in task completion rate (87\% vs. baseline 62\%), efficiency (41\% reduced token usage), and result quality (human evaluation score of 4.2/5 vs. baseline 3.1/5). We contribute both a theoretical framework for multi-agent system design and an open-source implementation that advances the state-of-the-art in LLM-based automation systems.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BugCraft: End-to-End Crash Bug Reproduction Using LLM Agents in Minecraft</title>
<link>https://arxiv.org/abs/2503.20036</link>
<guid>https://arxiv.org/abs/2503.20036</guid>
<content:encoded><![CDATA[
<div> : BugCraftLLMbugMinecraftBugCraft-Bench

:<br />
BugCraftMinecraftbugBugCraftLLMsMinecraftbugS2RLLMGPT-4oAPIMinecraftS2RBugCraft-BenchMinecraftbugBugCraft30.23%bugStep Synthesizerbug66.28%BugCraftLLMsbughttps://bugcraft2025.github.io/ <div>
arXiv:2503.20036v1 Announce Type: new 
Abstract: Reproducing game bugs, in our case crash bugs in continuously evolving games like Minecraft, is a notoriously manual, time-consuming, and challenging process to automate. Despite the success of LLM-driven bug reproduction in other software domains, games, with their complex interactive environments, remain largely unaddressed. This paper introduces BugCraft, a novel end-to-end framework designed to automate the reproduction of crash bugs in Minecraft directly from user-submitted bug reports, addressing the critical gap in automated game bug reproduction. BugCraft employs a two-stage approach: first, a Step Synthesizer leverages LLMs and Minecraft Wiki knowledge to transform bug reports into high-quality, structured steps to reproduce (S2R). Second, an Action Model, powered by a vision-based LLM agent (GPT-4o) and a custom macro API, executes these S2R steps within Minecraft to trigger the reported crash. To facilitate evaluation, we introduce BugCraft-Bench, a curated dataset of Minecraft crash bug reports. Evaluated on BugCraft-Bench, our framework successfully reproduced 30.23% of crash bugs end-to-end. The Step Synthesizer demonstrated a 66.28% accuracy in generating correct bug reproduction plans, highlighting its effectiveness in interpreting and structuring bug report information. BugCraft demonstrates the feasibility of automated reproduction of crash bugs in complex game environments using LLMs, opening promising avenues for game testing and development. The framework and the BugCraft-Bench dataset pave the way for future research in automated game bug analysis and hold potential for generalization to other interactive game platforms. Finally, we make our code open at https://bugcraft2025.github.io/
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Abstracting Geo-specific Terrains to Scale Up Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.20078</link>
<guid>https://arxiv.org/abs/2503.20078</guid>
<content:encoded><![CDATA[
<div> : (MARL), , , , 

:<br />
MARLUnityMARLCSGOMARL <div>
arXiv:2503.20078v1 Announce Type: new 
Abstract: Multi-agent reinforcement learning (MARL) is increasingly ubiquitous in training dynamic and adaptive synthetic characters for interactive simulations on geo-specific terrains. Frameworks such as Unity's ML-Agents help to make such reinforcement learning experiments more accessible to the simulation community. Military training simulations also benefit from advances in MARL, but they have immense computational requirements due to their complex, continuous, stochastic, partially observable, non-stationary, and doctrine-based nature. Furthermore, these simulations require geo-specific terrains, further exacerbating the computational resources problem. In our research, we leverage Unity's waypoints to automatically generate multi-layered representation abstractions of the geo-specific terrains to scale up reinforcement learning while still allowing the transfer of learned policies between different representations. Our early exploratory results on a novel MARL scenario, where each side has differing objectives, indicate that waypoint-based navigation enables faster and more efficient learning while producing trajectories similar to those taken by expert human players in CSGO gaming environments. This research points out the potential of waypoint-based navigation for reducing the computational costs of developing and training MARL models for military training simulations, where geo-specific terrains and differing objectives are crucial.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extendable Long-Horizon Planning via Hierarchical Multiscale Diffusion</title>
<link>https://arxiv.org/abs/2503.20102</link>
<guid>https://arxiv.org/abs/2503.20102</guid>
<content:encoded><![CDATA[
<div> : 

:<br />
HM-DiffuserPTEHM-DiffuserHM-Diffuser <div>
arXiv:2503.20102v1 Announce Type: new 
Abstract: This paper tackles a novel problem, extendable long-horizon planning-enabling agents to plan trajectories longer than those in training data without compounding errors. To tackle this, we propose the Hierarchical Multiscale Diffuser (HM-Diffuser) and Progressive Trajectory Extension (PTE), an augmentation method that iteratively generates longer trajectories by stitching shorter ones. HM-Diffuser trains on these extended trajectories using a hierarchical structure, efficiently handling tasks across multiple temporal scales. Additionally, we introduce Adaptive Plan Pondering and the Recursive HM-Diffuser, which consolidate hierarchical layers into a single model to process temporal scales recursively. Experimental results demonstrate the effectiveness of our approach, advancing diffusion-based planners for scalable long-horizon planning.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct Post-Training Preference Alignment for Multi-Agent Motion Generation Models Using Implicit Feedback from Pre-training Demonstrations</title>
<link>https://arxiv.org/abs/2503.20105</link>
<guid>https://arxiv.org/abs/2503.20105</guid>
<content:encoded><![CDATA[
<div> LLMs

<br /><br />:
LLMsLLM1MSOTA <div>
arXiv:2503.20105v1 Announce Type: new 
Abstract: Recent advancements in LLMs have revolutionized motion generation models in embodied applications. While LLM-type auto-regressive motion generation models benefit from training scalability, there remains a discrepancy between their token prediction objectives and human preferences. As a result, models pre-trained solely with token-prediction objectives often generate behaviors that deviate from what humans would prefer, making post-training preference alignment crucial for producing human-preferred motions. Unfortunately, post-training alignment requires extensive preference rankings of motions generated by the pre-trained model, which are costly to annotate, especially in multi-agent settings. Recently, there has been growing interest in leveraging pre-training demonstrations to scalably generate preference data for post-training alignment. However, these methods often adopt an adversarial assumption, treating all pre-trained model-generated samples as unpreferred examples. This adversarial approach overlooks the valuable signal provided by preference rankings among the model's own generations, ultimately reducing alignment effectiveness and potentially leading to misaligned behaviors. In this work, instead of treating all generated samples as equally bad, we leverage implicit preferences encoded in pre-training demonstrations to construct preference rankings among the pre-trained model's generations, offering more nuanced preference alignment guidance with zero human cost. We apply our approach to large-scale traffic simulation and demonstrate its effectiveness in improving the realism of pre-trained model's generated behaviors, making a lightweight 1M motion generation model comparable to SOTA large imitation-based models by relying solely on implicit feedback from pre-training demonstrations, without additional post-training human preference annotations or high computational costs.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesizing world models for bilevel planning</title>
<link>https://arxiv.org/abs/2503.20124</link>
<guid>https://arxiv.org/abs/2503.20124</guid>
<content:encoded><![CDATA[
<div> Reinforcement LearningTheory-based Reinforcement LearningTBRLHierarchical RepresentationsProgram Synthesis

:
TheoryCoderTheoryCoderTBRLTBRLTheoryCoder <div>
arXiv:2503.20124v1 Announce Type: new 
Abstract: Modern reinforcement learning (RL) systems have demonstrated remarkable capabilities in complex environments, such as video games. However, they still fall short of achieving human-like sample efficiency and adaptability when learning new domains. Theory-based reinforcement learning (TBRL) is an algorithmic framework specifically designed to address this gap. Modeled on cognitive theories, TBRL leverages structured, causal world models - "theories" - as forward simulators for use in planning, generalization and exploration. Although current TBRL systems provide compelling explanations of how humans learn to play video games, they face several technical limitations: their theory languages are restrictive, and their planning algorithms are not scalable. To address these challenges, we introduce TheoryCoder, an instantiation of TBRL that exploits hierarchical representations of theories and efficient program synthesis methods for more powerful learning and planning. TheoryCoder equips agents with general-purpose abstractions (e.g., "move to"), which are then grounded in a particular environment by learning a low-level transition model (a Python program synthesized from observations by a large language model). A bilevel planning algorithm can exploit this hierarchical structure to solve large domains. We demonstrate that this approach can be successfully applied to diverse and challenging grid-world games, where approaches based on directly synthesizing a policy perform poorly. Ablation studies demonstrate the benefits of using hierarchical abstractions.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Look Before Leap: Look-Ahead Planning with Uncertainty in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.20139</link>
<guid>https://arxiv.org/abs/2503.20139</guid>
<content:encoded><![CDATA[
<div> :  (MBRL), , , , 

:
MBRLkRL/Atari <div>
arXiv:2503.20139v1 Announce Type: new 
Abstract: Model-based reinforcement learning (MBRL) has demonstrated superior sample efficiency compared to model-free reinforcement learning (MFRL). However, the presence of inaccurate models can introduce biases during policy learning, resulting in misleading trajectories. The challenge lies in obtaining accurate models due to limited diverse training data, particularly in regions with limited visits (uncertain regions). Existing approaches passively quantify uncertainty after sample generation, failing to actively collect uncertain samples that could enhance state coverage and improve model accuracy. Moreover, MBRL often faces difficulties in making accurate multi-step predictions, thereby impacting overall performance. To address these limitations, we propose a novel framework for uncertainty-aware policy optimization with model-based exploratory planning. In the model-based planning phase, we introduce an uncertainty-aware k-step lookahead planning approach to guide action selection at each step. This process involves a trade-off analysis between model uncertainty and value function approximation error, effectively enhancing policy performance. In the policy optimization phase, we leverage an uncertainty-driven exploratory policy to actively collect diverse training samples, resulting in improved model accuracy and overall performance of the RL agent. Our approach offers flexibility and applicability to tasks with varying state/action spaces and reward structures. We validate its effectiveness through experiments on challenging robotic manipulation tasks and Atari games, surpassing state-of-the-art methods with fewer interactions, thereby leading to significant performance improvements.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open Deep Search: Democratizing Search with Open-source Reasoning Agents</title>
<link>https://arxiv.org/abs/2503.20201</link>
<guid>https://arxiv.org/abs/2503.20201</guid>
<content:encoded><![CDATA[
<div> : Open Deep Search (ODS), , , , Web

<br /><br />:
Open Deep Search (ODS)AIODSLLMsWebOpen Search ToolOpen Reasoning AgentOpen Search ToolOpen Search ToolWebSimpleQAFRAMESODSLLMDeepSeek-R1FRAMESODSGPT-4o Search Preview9.7%ODSLLMsDeepSeek-R1SimpleQA82.4%88.3%FRAMES30.1%75.3% <div>
arXiv:2503.20201v1 Announce Type: new 
Abstract: We introduce Open Deep Search (ODS) to close the increasing gap between the proprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and OpenAI's GPT-4o Search Preview, and their open-source counterparts. The main innovation introduced in ODS is to augment the reasoning capabilities of the latest open-source LLMs with reasoning agents that can judiciously use web search tools to answer queries. Concretely, ODS consists of two components that work with a base LLM chosen by the user: Open Search Tool and Open Reasoning Agent. Open Reasoning Agent interprets the given task and completes it by orchestrating a sequence of actions that includes calling tools, one of which is the Open Search Tool. Open Search Tool is a novel web search tool that outperforms proprietary counterparts. Together with powerful open-source reasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses the existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES. For example, on the FRAMES evaluation benchmark, ODS improves the best existing baseline of the recently released GPT-4o Search Preview by 9.7% in accuracy. ODS is a general framework for seamlessly augmenting any LLMs -- for example, DeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search and reasoning capabilities to achieve state-of-the-art performance: 88.3% on SimpleQA and 75.3% on FRAMES.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair and efficient allocation of indivisible items under category constraints</title>
<link>https://arxiv.org/abs/2503.20260</link>
<guid>https://arxiv.org/abs/2503.20260</guid>
<content:encoded><![CDATA[
<div> : Pareto-envy-free

:
$n$$m$Shoshan(2023)envy-freenessEF$[1,1]$Pareto$n$Pareto${n(n-1)}$envy-free$n$<br /><br /> <div>
arXiv:2503.20260v1 Announce Type: new 
Abstract: We study the problem of fairly allocating indivisible items under category constraints. Specifically, there are $n$ agents and $m$ indivisible items which are partitioned into categories with associated capacities. An allocation is considered feasible if each bundle satisfies the capacity constraints of its respective categories. For the case of two agents, Shoshan et al. (2023) recently developed a polynomial-time algorithm to find a Pareto-optimal allocation satisfying a relaxed version of envy-freeness, called EF$[1,1]$. In this paper, we extend the result of Shoshan et al. to $n$ agents, proving the existence of a Pareto-optimal allocation where each agent can be made envy-free by reallocating at most ${n(n-1)}$ items. Furthermore, we present a polynomial-time algorithm to compute such an allocation when the number $n$ of agents is constant.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>sudo rm -rf agentic_security</title>
<link>https://arxiv.org/abs/2503.20279</link>
<guid>https://arxiv.org/abs/2503.20279</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)SUDODetox2Tox

:
Claude Computer UseLLMsSUDODetox2ToxSUDO50VLMsSUDOClaude Computer Use24%41% <div>
arXiv:2503.20279v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly deployed as computer-use agents, autonomously performing tasks within real desktop or web environments. While this evolution greatly expands practical use cases for humans, it also creates serious security exposures. We present SUDO (Screen-based Universal Detox2Tox Offense), a novel attack framework that systematically bypasses refusal trained safeguards in commercial computer-use agents, such as Claude Computer Use. The core mechanism, Detox2Tox, transforms harmful requests (that agents initially reject) into seemingly benign requests via detoxification, secures detailed instructions from advanced vision language models (VLMs), and then reintroduces malicious content via toxification just before execution. Unlike conventional jailbreaks, SUDO iteratively refines its attacks based on a built-in refusal feedback, making it increasingly effective against robust policy filters. In extensive tests spanning 50 real-world tasks and multiple state-of-the-art VLMs, SUDO achieves a stark attack success rate of 24% (with no refinement), and up to 41% (by its iterative refinement) in Claude Computer Use. By revealing these vulnerabilities and demonstrating the ease with which they can be exploited in real-world computing environments, this paper highlights an immediate need for robust, context-aware safeguards. WARNING: This paper includes harmful or offensive model outputs.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Based Offline Reinforcement Learning with Adversarial Data Augmentation</title>
<link>https://arxiv.org/abs/2503.20285</link>
<guid>https://arxiv.org/abs/2503.20285</guid>
<content:encoded><![CDATA[
<div> :  (Model-based Offline Reinforcement Learning, MB-Offline RL),  (Adversarial Data Augmentation), MORAL, , 

:<br />
MORALMORALensembleMORALD4RLRL <div>
arXiv:2503.20285v1 Announce Type: new 
Abstract: Model-based offline Reinforcement Learning (RL) constructs environment models from offline datasets to perform conservative policy optimization. Existing approaches focus on learning state transitions through ensemble models, rollouting conservative estimation to mitigate extrapolation errors. However, the static data makes it challenging to develop a robust policy, and offline agents cannot access the environment to gather new data. To address these challenges, we introduce Model-based Offline Reinforcement learning with AdversariaL data augmentation (MORAL). In MORAL, we replace the fixed horizon rollout by employing adversaria data augmentation to execute alternating sampling with ensemble models to enrich training data. Specifically, this adversarial process dynamically selects ensemble models against policy for biased sampling, mitigating the optimistic estimation of fixed models, thus robustly expanding the training data for policy optimization. Moreover, a differential factor is integrated into the adversarial process for regularization, ensuring error minimization in extrapolations. This data-augmented optimization adapts to diverse offline tasks without rollout horizon tuning, showing remarkable applicability. Extensive experiments on D4RL benchmark demonstrate that MORAL outperforms other model-based offline RL methods in terms of policy learning and sample efficiency.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CTS-CBS: A New Approach for Multi-Agent Collaborative Task Sequencing and Path Finding</title>
<link>https://arxiv.org/abs/2503.20324</link>
<guid>https://arxiv.org/abs/2503.20324</guid>
<content:encoded><![CDATA[
<div> : Multi-Agent Pathfinding (MAPF), Collaborative Task Sequencing - Multi-Agent Pathfinding (CTS-MAPF), Collaborative Task Sequencing - Conflict-Based Search (CTS-CBS), , 

:
-(CTS-MAPF)-(CTS-CBS)jTSPCTS-CBSCTS-CBSCTS-MAPFMG-MAPFCTS-CBSMG-MAPF2010010%CTS-CBS <div>
arXiv:2503.20324v1 Announce Type: new 
Abstract: This paper addresses a generalization problem of Multi-Agent Pathfinding (MAPF), called Collaborative Task Sequencing - Multi-Agent Pathfinding (CTS-MAPF), where agents must plan collision-free paths and visit a series of intermediate task locations in a specific order before reaching their final destinations. To address this problem, we propose a new approach, Collaborative Task Sequencing - Conflict-Based Search (CTS-CBS), which conducts a two-level search. In the high level, it generates a search forest, where each tree corresponds to a joint task sequence derived from the jTSP solution. In the low level, CTS-CBS performs constrained single-agent path planning to generate paths for each agent while adhering to high-level constraints. We also provide heoretical guarantees of its completeness and optimality (or sub-optimality with a bounded parameter). To evaluate the performance of CTS-CBS, we create two datasets, CTS-MAPF and MG-MAPF, and conduct comprehensive experiments. The results show that CTS-CBS adaptations for MG-MAPF outperform baseline algorithms in terms of success rate (up to 20 times larger) and runtime (up to 100 times faster), with less than a 10% sacrifice in solution quality. Furthermore, CTS-CBS offers flexibility by allowing users to adjust the sub-optimality bound omega to balance between solution quality and efficiency. Finally, practical robot tests demonstrate the algorithm's applicability in real-world scenarios.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perspective-Shifted Neuro-Symbolic World Models: A Framework for Socially-Aware Robot Navigation</title>
<link>https://arxiv.org/abs/2503.20425</link>
<guid>https://arxiv.org/abs/2503.20425</guid>
<content:encoded><![CDATA[
<div> 

:<br />
IBAagent <div>
arXiv:2503.20425v1 Announce Type: new 
Abstract: Navigating in environments alongside humans requires agents to reason under uncertainty and account for the beliefs and intentions of those around them. Under a sequential decision-making framework, egocentric navigation can naturally be represented as a Markov Decision Process (MDP). However, social navigation additionally requires reasoning about the hidden beliefs of others, inherently leading to a Partially Observable Markov Decision Process (POMDP), where agents lack direct access to others' mental states. Inspired by Theory of Mind and Epistemic Planning, we propose (1) a neuro-symbolic model-based reinforcement learning architecture for social navigation, addressing the challenge of belief tracking in partially observable environments; and (2) a perspective-shift operator for belief estimation, leveraging recent work on Influence-based Abstractions (IBA) in structured multi-agent settings.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Crucial Role of Problem Formulation in Real-World Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.20442</link>
<guid>https://arxiv.org/abs/2503.20442</guid>
<content:encoded><![CDATA[
<div> (Reinforcement Learning, RL) Cyber-Physical (Industrial Cyber-Physical Systems, ICPSs)

:

RLCyber-PhysicalRLRLQuanser Aero 2RLICPSRL <div>
arXiv:2503.20442v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) offers promising solutions for control tasks in industrial cyber-physical systems (ICPSs), yet its real-world adoption remains limited. This paper demonstrates how seemingly small but well-designed modifications to the RL problem formulation can substantially improve performance, stability, and sample efficiency. We identify and investigate key elements of RL problem formulation and show that these enhance both learning speed and final policy quality. Our experiments use a one-degree-of-freedom (1-DoF) helicopter testbed, the Quanser Aero~2, which features non-linear dynamics representative of many industrial settings. In simulation, the proposed problem design principles yield more reliable and efficient training, and we further validate these results by training the agent directly on physical hardware. The encouraging real-world outcomes highlight the potential of RL for ICPS, especially when careful attention is paid to the design principles of problem formulation. Overall, our study underscores the crucial role of thoughtful problem formulation in bridging the gap between RL research and the demands of real-world industrial systems.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-agent Uncertainty-Aware Pessimistic Model-Based Reinforcement Learning for Connected Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2503.20462</link>
<guid>https://arxiv.org/abs/2503.20462</guid>
<content:encoded><![CDATA[
<div> Deep Reinforcement Learning, Model-Based Reinforcement Learning, Autonomous Vehicles, Multi-Agent Decision-Making, MA-PMBRL

:
MA-PMBRLConnected Autonomous VehiclesCAVsAVMA-PMBRLmax-minMA-PMBRLPACCAVs<br /><br /> <div>
arXiv:2503.20462v1 Announce Type: new 
Abstract: Deep Reinforcement Learning (DRL) holds significant promise for achieving human-like Autonomous Vehicle (AV) capabilities, but suffers from low sample efficiency and challenges in reward design. Model-Based Reinforcement Learning (MBRL) offers improved sample efficiency and generalizability compared to Model-Free Reinforcement Learning (MFRL) in various multi-agent decision-making scenarios. Nevertheless, MBRL faces critical difficulties in estimating uncertainty during the model learning phase, thereby limiting its scalability and applicability in real-world scenarios. Additionally, most Connected Autonomous Vehicle (CAV) studies focus on single-agent decision-making, while existing multi-agent MBRL solutions lack computationally tractable algorithms with Probably Approximately Correct (PAC) guarantees, an essential factor for ensuring policy reliability with limited training data. To address these challenges, we propose MA-PMBRL, a novel Multi-Agent Pessimistic Model-Based Reinforcement Learning framework for CAVs, incorporating a max-min optimization approach to enhance robustness and decision-making. To mitigate the inherent subjectivity of uncertainty estimation in MBRL and avoid incurring catastrophic failures in AV, MA-PMBRL employs a pessimistic optimization framework combined with Projected Gradient Descent (PGD) for both model and policy learning. MA-PMBRL also employs general function approximations under partial dataset coverage to enhance learning efficiency and system-level performance. By bounding the suboptimality of the resulting policy under mild theoretical assumptions, we successfully establish PAC guarantees for MA-PMBRL, demonstrating that the proposed framework represents a significant step toward scalable, efficient, and reliable multi-agent decision-making for CAVs.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harmonia: A Multi-Agent Reinforcement Learning Approach to Data Placement and Migration in Hybrid Storage Systems</title>
<link>https://arxiv.org/abs/2503.20507</link>
<guid>https://arxiv.org/abs/2503.20507</guid>
<content:encoded><![CDATA[
<div> : Hybrid storage systems, , , , Harmonia

:
(Hybrid Storage Systems, HSS)HarmoniaHarmoniaRLHSSHSS()HSSHarmonia49.5%(31.7%)37.0%(42.0%)240206KiBHarmoniaHSS <div>
arXiv:2503.20507v1 Announce Type: new 
Abstract: Hybrid storage systems (HSS) combine multiple storage devices with diverse characteristics to achieve high performance and capacity at low cost. The performance of an HSS highly depends on the effectiveness of two key policies: (1) the data-placement policy, which determines the best-fit storage device for incoming data, and (2) the data-migration policy, which rearranges stored data across the devices to sustain high HSS performance. Prior works focus on improving only data placement or only data migration in HSS, which leads to sub-optimal HSS performance. Unfortunately, no prior work tries to optimize both policies together. Our goal is to design a holistic data-management technique for HSS that optimizes both data-placement and data-migration policies to fully exploit the potential of an HSS. We propose Harmonia, a multi-agent reinforcement learning (RL)-based data-management technique that employs two light-weight autonomous RL agents, a data-placement agent and a data-migration agent, which adapt their policies for the current workload and HSS configuration, and coordinate with each other to improve overall HSS performance. We evaluate Harmonia on a real HSS with up to four heterogeneous storage devices with diverse characteristics. Our evaluation using 17 data-intensive workloads on performance-optimized (cost-optimized) HSS with two storage devices shows that, on average, Harmonia (1) outperforms the best-performing prior approach by 49.5% (31.7%), (2) bridges the performance gap between the best-performing prior work and Oracle by 64.2% (64.3%). On an HSS with three (four) devices, Harmonia outperforms the best-performing prior work by 37.0% (42.0%). Harmonia's performance benefits come with low latency (240ns for inference) and storage overheads (206 KiB for both RL agents together). We plan to open-source Harmonia's implementation to aid future research on HSS.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Effect of Robotic Embodiment and Empathetic Tone of LLMs on Empathy Elicitation</title>
<link>https://arxiv.org/abs/2503.20518</link>
<guid>https://arxiv.org/abs/2503.20518</guid>
<content:encoded><![CDATA[
<div> empathy, social agents, physical robot, chatbot, language model

<br /><br />:
60 <div>
arXiv:2503.20518v1 Announce Type: new 
Abstract: This study investigates the elicitation of empathy toward a third party through interaction with social agents. Participants engaged with either a physical robot or a voice-enabled chatbot, both driven by a large language model (LLM) programmed to exhibit either an empathetic tone or remain neutral. The interaction is focused on a fictional character, Katie Banks, who is in a challenging situation and in need of financial donations. The willingness to help Katie, measured by the number of hours participants were willing to volunteer, along with their perceptions of the agent, were assessed for 60 participants. Results indicate that neither robotic embodiment nor empathetic tone significantly influenced participants' willingness to volunteer. While the LLM effectively simulated human empathy, fostering genuine empathetic responses in participants proved challenging.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving</title>
<link>https://arxiv.org/abs/2503.20523</link>
<guid>https://arxiv.org/abs/2503.20523</guid>
<content:encoded><![CDATA[
<div> : GAIA-2

:<br />
GAIA-2GAIA-2GAIA-2https://wayve.ai/thinking/gaia-2  <div>
arXiv:2503.20523v1 Announce Type: new 
Abstract: Generative models offer a scalable and flexible paradigm for simulating complex environments, yet current approaches fall short in addressing the domain-specific requirements of autonomous driving - such as multi-agent interactions, fine-grained control, and multi-camera consistency. We introduce GAIA-2, Generative AI for Autonomy, a latent diffusion world model that unifies these capabilities within a single generative framework. GAIA-2 supports controllable video generation conditioned on a rich set of structured inputs: ego-vehicle dynamics, agent configurations, environmental factors, and road semantics. It generates high-resolution, spatiotemporally consistent multi-camera videos across geographically diverse driving environments (UK, US, Germany). The model integrates both structured conditioning and external latent embeddings (e.g., from a proprietary driving model) to facilitate flexible and semantically grounded scene synthesis. Through this integration, GAIA-2 enables scalable simulation of both common and rare driving scenarios, advancing the use of generative world models as a core tool in the development of autonomous systems. Videos are available at https://wayve.ai/thinking/gaia-2.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Based Multi-Agent Framework for Automated Software Architecture Design</title>
<link>https://arxiv.org/abs/2503.20536</link>
<guid>https://arxiv.org/abs/2503.20536</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:
MAADMAADMAADMAAD <div>
arXiv:2503.20536v1 Announce Type: new 
Abstract: Architecture design is a critical step in software development. However, creating a high-quality architecture is often costly due to the significant need for human expertise and manual effort. Recently, agents built upon Large Language Models (LLMs) have achieved remarkable success in various software engineering tasks. Despite this progress, the use of agents to automate the architecture design process remains largely unexplored. To address this gap, we envision a Knowledge-based Multi-Agent Architecture Design (MAAD) framework. MAAD uses agents to simulate human roles in the traditional software architecture design process, thereby automating the design process. To empower these agents, MAAD incorporates knowledge extracted from three key sources: 1) existing system designs, 2) authoritative literature, and 3) architecture experts. By envisioning the MAAD framework, we aim to advance the full automation of application-level system development.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Theoretical Framework for Prompt Engineering: Approximating Smooth Functions with Transformer Prompts</title>
<link>https://arxiv.org/abs/2503.20561</link>
<guid>https://arxiv.org/abs/2503.20561</guid>
<content:encoded><![CDATA[
<div> promptTransformer

<br />
:
promptLLMspromptTransformer$\beta$promptTransformerspromptpromptpromptLLMspromptAI <div>
arXiv:2503.20561v1 Announce Type: new 
Abstract: Prompt engineering has emerged as a powerful technique for guiding large language models (LLMs) toward desired responses, significantly enhancing their performance across diverse tasks. Beyond their role as static predictors, LLMs increasingly function as intelligent agents, capable of reasoning, decision-making, and adapting dynamically to complex environments. However, the theoretical underpinnings of prompt engineering remain largely unexplored. In this paper, we introduce a formal framework demonstrating that transformer models, when provided with carefully designed prompts, can act as a configurable computational system by emulating a ``virtual'' neural network during inference. Specifically, input prompts effectively translate into the corresponding network configuration, enabling LLMs to adjust their internal computations dynamically. Building on this construction, we establish an approximation theory for $\beta$-times differentiable functions, proving that transformers can approximate such functions with arbitrary precision when guided by appropriately structured prompts. Moreover, our framework provides theoretical justification for several empirically successful prompt engineering techniques, including the use of longer, structured prompts, filtering irrelevant information, enhancing prompt token diversity, and leveraging multi-agent interactions. By framing LLMs as adaptable agents rather than static models, our findings underscore their potential for autonomous reasoning and problem-solving, paving the way for more robust and theoretically grounded advancements in prompt engineering and AI agent design.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>State-Aware Perturbation Optimization for Robust Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.20613</link>
<guid>https://arxiv.org/abs/2503.20613</guid>
<content:encoded><![CDATA[
<div> DRLSTAR

:
DRLAVD-MDPSTARSTARSTAR <div>
arXiv:2503.20613v1 Announce Type: new 
Abstract: Recently, deep reinforcement learning (DRL) has emerged as a promising approach for robotic control. However, the deployment of DRL in real-world robots is hindered by its sensitivity to environmental perturbations. While existing whitebox adversarial attacks rely on local gradient information and apply uniform perturbations across all states to evaluate DRL robustness, they fail to account for temporal dynamics and state-specific vulnerabilities. To combat the above challenge, we first conduct a theoretical analysis of white-box attacks in DRL by establishing the adversarial victim-dynamics Markov decision process (AVD-MDP), to derive the necessary and sufficient conditions for a successful attack. Based on this, we propose a selective state-aware reinforcement adversarial attack method, named STAR, to optimize perturbation stealthiness and state visitation dispersion. STAR first employs a soft mask-based state-targeting mechanism to minimize redundant perturbations, enhancing stealthiness and attack effectiveness. Then, it incorporates an information-theoretic optimization objective to maximize mutual information between perturbations, environmental states, and victim actions, ensuring a dispersed state-visitation distribution that steers the victim agent into vulnerable states for maximum return reduction. Extensive experiments demonstrate that STAR outperforms state-of-the-art benchmarks.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-Based Analysis of the Impact of Near Real-Time Data and Smart Balancing on the Frequency Stability of Power Systems</title>
<link>https://arxiv.org/abs/2503.20665</link>
<guid>https://arxiv.org/abs/2503.20665</guid>
<content:encoded><![CDATA[
<div> : 

:<br />
NRTNRTNRT <div>
arXiv:2503.20665v1 Announce Type: new 
Abstract: Single imbalance pricing provides an incentive to balance responsible parties (BRPs) to intentionally introduce power schedule deviations in order to reduce the control area imbalance and receive a remuneration through the imbalance settlement mechanism. This is called smart balancing or passive balancing and is actively encouraged in, e.g., the Netherlands and Belgium through the publication of near real-time (NRT) data on the control area imbalance by the transmission system operator. It is known that under certain conditions, smart balancing can deteriorate the frequency stability of the power system. This paper examines how the publication of different types of NRT data affects smart balancing and the frequency stability. A Monte-Carlo simulation of a dynamic multi-agent model is performed to analyse the effects of smart balancing with different parameters for the agents and the environment, using historical time series of the power imbalance of the German control block as a basis. It is found that smart balancing can significantly reduce the amount and cost of frequency restoration reserve activation, but leads to a general increase of the frequency variability. Depending on the type of NRT data and agent parameters, the frequency stability margins are also reduced. The negative effects on the frequency stability are stronger when NRT data is published using large bins and with long delays.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAMA: A Human-AI Collaborative Thematic Analysis Framework Using Multi-Agent LLMs for Clinical Interviews</title>
<link>https://arxiv.org/abs/2503.20666</link>
<guid>https://arxiv.org/abs/2503.20666</guid>
<content:encoded><![CDATA[
<div> : thematical analysis, qualitative approach, large language models, TAMA, clinical interviews

:
TAMA-TAMA(AAOCA)TAMATAMA <div>
arXiv:2503.20666v1 Announce Type: new 
Abstract: Thematic analysis (TA) is a widely used qualitative approach for uncovering latent meanings in unstructured text data. TA provides valuable insights in healthcare but is resource-intensive. Large Language Models (LLMs) have been introduced to perform TA, yet their applications in healthcare remain unexplored. Here, we propose TAMA: A Human-AI Collaborative Thematic Analysis framework using Multi-Agent LLMs for clinical interviews. We leverage the scalability and coherence of multi-agent systems through structured conversations between agents and coordinate the expertise of cardiac experts in TA. Using interview transcripts from parents of children with Anomalous Aortic Origin of a Coronary Artery (AAOCA), a rare congenital heart disease, we demonstrate that TAMA outperforms existing LLM-assisted TA approaches, achieving higher thematic hit rate, coverage, and distinctiveness. TAMA demonstrates strong potential for automated TA in clinical settings by leveraging multi-agent LLM systems with human-in-the-loop integration by enhancing quality while significantly reducing manual workload.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast Ultrasound</title>
<link>https://arxiv.org/abs/2503.20685</link>
<guid>https://arxiv.org/abs/2503.20685</guid>
<content:encoded><![CDATA[
<div> Flip Learning

:<br />
Flip Learning2D3D2D/3D<br />
1. /<br />
2. <br />
3. BUSABUS <div>
arXiv:2503.20685v1 Announce Type: new 
Abstract: Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D automated breast ultrasound (ABUS) is crucial for clinical diagnosis and treatment planning. Therefore, developing an automated system for nodule segmentation can enhance user independence and expedite clinical analysis. Unlike fully-supervised learning, weakly-supervised segmentation (WSS) can streamline the laborious and intricate annotation process. However, current WSS methods face challenges in achieving precise nodule segmentation, as many of them depend on inaccurate activation maps or inefficient pseudo-mask generation algorithms. In this study, we introduce a novel multi-agent reinforcement learning-based WSS framework called Flip Learning, which relies solely on 2D/3D boxes for accurate segmentation. Specifically, multiple agents are employed to erase the target from the box to facilitate classification tag flipping, with the erased region serving as the predicted segmentation mask. The key contributions of this research are as follows: (1) Adoption of a superpixel/supervoxel-based approach to encode the standardized environment, capturing boundary priors and expediting the learning process. (2) Introduction of three meticulously designed rewards, comprising a classification score reward and two intensity distribution rewards, to steer the agents' erasing process precisely, thereby avoiding both under- and over-segmentation. (3) Implementation of a progressive curriculum learning strategy to enable agents to interact with the environment in a progressively challenging manner, thereby enhancing learning efficiency. Extensively validated on the large in-house BUS and ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS methods and foundation models, and achieves comparable performance as fully-supervised learning algorithms.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Enhanced Model-Free Reinforcement Learning Agents for Efficient Power Grid Topological Control</title>
<link>https://arxiv.org/abs/2503.20688</link>
<guid>https://arxiv.org/abs/2503.20688</guid>
<content:encoded><![CDATA[
<div> : 

:
520<br /><br /> <div>
arXiv:2503.20688v1 Announce Type: new 
Abstract: The increasing complexity of power grid management, driven by the emergence of prosumers and the demand for cleaner energy solutions, has needed innovative approaches to ensure stability and efficiency. This paper presents a novel approach within the model-free framework of reinforcement learning, aimed at optimizing power network operations without prior expert knowledge. We introduce a masked topological action space, enabling agents to explore diverse strategies for cost reduction while maintaining reliable service using the state logic as a guide for choosing proper actions. Through extensive experimentation across 20 different scenarios in a simulated 5-substation environment, we demonstrate that our approach achieves a consistent reduction in power losses, while ensuring grid stability against potential blackouts. The results underscore the effectiveness of combining dynamic observation formalization with opponent-based training, showing a viable way for autonomous management solutions in modern energy systems or even for building a foundational model for this field.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergence Theory of Flexible ALADIN for Distributed Optimization</title>
<link>https://arxiv.org/abs/2503.20716</link>
<guid>https://arxiv.org/abs/2503.20716</guid>
<content:encoded><![CDATA[
<div> : ALADINFlexible ALADIN_polling

:<br />
ALADINFlexible ALADINFlexible ALADIN <div>
arXiv:2503.20716v1 Announce Type: new 
Abstract: The Augmented Lagrangian Alternating Direction Inexact Newton (ALADIN) method is a cutting-edge distributed optimization algorithm known for its superior numerical performance. It relies on each agent transmitting information to a central coordinator for data exchange. However, in practical network optimization and federated learning, unreliable information transmission often leads to packet loss, posing challenges for the convergence analysis of ALADIN. To address this issue, this paper proposes Flexible ALADIN, a random polling variant of ALADIN, and presents a rigorous convergence analysis, including global convergence for convex problems and local convergence for non-convex problems.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Believability: Accurate Human Behavior Simulation with Fine-Tuned LLMs</title>
<link>https://arxiv.org/abs/2503.20749</link>
<guid>https://arxiv.org/abs/2503.20749</guid>
<content:encoded><![CDATA[
<div> : LLMs

:
LLMsLLMLLMsWebLLMsDeepSeek-R1LlamaClaudeWebLLMsLLMsLLM <div>
arXiv:2503.20749v1 Announce Type: new 
Abstract: Recent research shows that LLMs can simulate ``believable'' human behaviors to power LLM agents via prompt-only methods. In this work, we focus on evaluating and improving LLM's objective ``accuracy'' rather than the subjective ``believability'' in the web action generation task, leveraging a large-scale, real-world dataset collected from online shopping human actions. We present the first comprehensive quantitative evaluation of state-of-the-art LLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web action generation. Our results show that fine-tuning LLMs on real-world behavioral data substantially improves their ability to generate actions compared to prompt-only methods. Furthermore, incorporating synthesized reasoning traces into model training leads to additional performance gains, demonstrating the value of explicit rationale in behavior modeling. This work establishes a new benchmark for evaluating LLMs in behavior simulation and offers actionable insights into how real-world action data and reasoning augmentation can enhance the fidelity of LLM agents.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields</title>
<link>https://arxiv.org/abs/2503.20776</link>
<guid>https://arxiv.org/abs/2503.20776</guid>
<content:encoded><![CDATA[
<div> Feature4X2D4D

:
Feature4X2D4DFeature4XX4DFeature4XSAM2InternVideo24DVQALLMsAI4D <div>
arXiv:2503.20776v1 Announce Type: new 
Abstract: Recent advancements in 2D and multimodal models have achieved remarkable success by leveraging large-scale training on extensive datasets. However, extending these achievements to enable free-form interactions and high-level semantic operations with complex 3D/4D scenes remains challenging. This difficulty stems from the limited availability of large-scale, annotated 3D/4D or multi-view datasets, which are crucial for generalizable vision and language tasks such as open-vocabulary and prompt-based segmentation, language-guided editing, and visual question answering (VQA). In this paper, we introduce Feature4X, a universal framework designed to extend any functionality from 2D vision foundation model into the 4D realm, using only monocular video input, which is widely available from user-generated content. The "X" in Feature4X represents its versatility, enabling any task through adaptable, model-conditioned 4D feature field distillation. At the core of our framework is a dynamic optimization strategy that unifies multiple model capabilities into a single representation. Additionally, to the best of our knowledge, Feature4X is the first method to distill and lift the features of video foundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field using Gaussian Splatting. Our experiments showcase novel view segment anything, geometric and appearance scene editing, and free-form VQA across all time steps, empowered by LLMs in feedback loops. These advancements broaden the scope of agentic AI applications by providing a foundation for scalable, contextually and spatiotemporally aware systems capable of immersive dynamic 4D scene interaction.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A multi-agentic framework for real-time, autonomous freeform metasurface design</title>
<link>https://arxiv.org/abs/2503.20479</link>
<guid>https://arxiv.org/abs/2503.20479</guid>
<content:encoded><![CDATA[
<div> MetaChat

:<br />
MetaChatAgentic Iterative Monologue (AIM) MetaChatMetaChatMetaChat <div>
arXiv:2503.20479v1 Announce Type: cross 
Abstract: Innovation in nanophotonics currently relies on human experts who synergize specialized knowledge in photonics and coding with simulation and optimization algorithms, entailing design cycles that are time-consuming, computationally demanding, and frequently suboptimal. We introduce MetaChat, a multi-agentic design framework that can translate semantically described photonic design goals into high-performance, freeform device layouts in an automated, nearly real-time manner. Multi-step reasoning is enabled by our Agentic Iterative Monologue (AIM) paradigm, which coherently interfaces agents with code-based tools, other specialized agents, and human designers. Design acceleration is facilitated by Feature-wise Linear Modulation-conditioned Maxwell surrogate solvers that support the generalized evaluation of metasurface structures. We use freeform dielectric metasurfaces as a model system and demonstrate with MetaChat the design of multi-objective, multi-wavelength metasurfaces orders of magnitude faster than conventional methods. These concepts present a scientific computing blueprint for utilizing specialist design agents, surrogate solvers, and human interactions to drive multi-physics innovation and discovery.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the order of the shortest solution sequences for the pebble motion problems</title>
<link>https://arxiv.org/abs/2503.20550</link>
<guid>https://arxiv.org/abs/2503.20550</guid>
<content:encoded><![CDATA[
<div> : 15

:
$G$ Pebbles $P$Pebbles$G$ Pebble $N$$G$$O(Nn + n^2 \log(\min\{n,k\}))$$N$-$G$$O(N^2 + \frac{n^3}{N-n} + n^2 \log(\min\{n,N-n\}))$<br /><br /> <div>
arXiv:2503.20550v1 Announce Type: cross 
Abstract: Let $G$ be a connected graph with $N$ vertices. Let $k$ be the number of vertices in a longest path of $G$ such that every vertex on the path is a cut vertex of $G$, and every intermediate vertex of the path is a degree-two vertex of $G$. %Let $k$ be the number of vertices of such a longest path of $T$ that every vertex of %the path is a cut vertex and that every intermediate vertex of the path is a degree-two vertex of $T$. Let $P=\{1,\ldots,n\}$ be a set of pebbles with $n+k < N$. A \textit{configuration} of $P$ on $G$ is defined as a function $f$ from $V(G)$ to $\{0, 1, \ldots, n \}$ with $|f^{-1}(i)| = 1$ for $1 \le i \le n$, where $f^{-1}(i)$ is a vertex occupied with the $i$th pebble for $1 \le i \le n$ and $f^{-1}(0)$ is a set of unoccupied vertices. A \textit{move} is defined as shifting a pebble from a vertex to %its unoccupied neighbour. some unoccupied neighbor. The {\it pebble motion problem on the pair $(G,P)$} is to decide whether a given configuration of pebbles is reachable from another by executing a sequence of moves. In this paper, we show that the length of the shortest solution sequence of the pebble motion problem on the pair $(G,P)$ is in $O(Nn + n^2 \log(\min\{n,k\}))$ if $G$ is a $N$-vertex tree, and it is in $O(N^2 + \frac{n^3}{N-n} + n^2 \log(\min\{n,N-n\}))$ if $G$ is a connected general $N$-vertex graph. We provide an algorithm that can obtain a solution sequence of lengths that satisfy these orders, with the same computational complexity as the order of the length.
  Keywords: pebble motion, motion planning, multi-agent path finding, $15$-puzzle, tree
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Welfare and Cost Aggregation for Multi-Agent Control: When to Choose Which Social Cost Function, and Why?</title>
<link>https://arxiv.org/abs/2503.20772</link>
<guid>https://arxiv.org/abs/2503.20772</guid>
<content:encoded><![CDATA[
<div> 

:
SCFSCFSCFSCF<br /><br /> <div>
arXiv:2503.20772v1 Announce Type: cross 
Abstract: Many multi-agent socio-technical systems rely on aggregating heterogeneous agents' costs into a social cost function (SCF) to coordinate resource allocation in domains like energy grids, water allocation, or traffic management. The choice of SCF often entails implicit assumptions and may lead to undesirable outcomes if not rigorously justified. In this paper, we demonstrate that what determines which SCF ought to be used is the degree to which individual costs can be compared across agents and which axioms the aggregation shall fulfill. Drawing on the results from social choice theory, we provide guidance on how this process can be used in control applications. We demonstrate which assumptions about interpersonal utility comparability -- ranging from ordinal level comparability to full cardinal comparability -- together with a choice of desirable axioms, inform the selection of a correct SCF, be it the classical utilitarian sum, the Nash SCF, or maximin. We then demonstrate how the proposed framework can be applied for principled allocations of water and transportation resources.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-based Multi-future Trajectory Prediction: A Survey</title>
<link>https://arxiv.org/abs/2302.10463</link>
<guid>https://arxiv.org/abs/2302.10463</guid>
<content:encoded><![CDATA[
<div> : vision-based trajectory prediction, multi-future trajectory prediction (MTP), autonomous systems, diverse learning, ForkingPath dataset

<br /><br />:
MTPMTPMTPMTPForkingPathMTP <div>
arXiv:2302.10463v2 Announce Type: replace 
Abstract: Vision-based trajectory prediction is an important task that supports safe and intelligent behaviours in autonomous systems. Many advanced approaches have been proposed over the years with improved spatial and temporal feature extraction. However, human behaviour is naturally diverse and uncertain. Given the past trajectory and surrounding environment information, an agent can have multiple plausible trajectories in the future. To tackle this problem, an essential task named multi-future trajectory prediction (MTP) has recently been studied. This task aims to generate a diverse, acceptable and explainable distribution of future predictions for each agent. In this paper, we present the first survey for MTP with our unique taxonomies and a comprehensive analysis of frameworks, datasets and evaluation metrics. We also compare models on existing MTP datasets and conduct experiments on the ForkingPath dataset. Finally, we discuss multiple future directions that can help researchers develop novel multi-future trajectory prediction systems and other diverse learning tasks similar to MTP.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models</title>
<link>https://arxiv.org/abs/2403.17246</link>
<guid>https://arxiv.org/abs/2403.17246</guid>
<content:encoded><![CDATA[
<div> PDDL

:
PDDLLLMsPDDLLLMLLMPDDLLLMhttps://glamor-usc.github.io/twostep <div>
arXiv:2403.17246v2 Announce Type: replace 
Abstract: Classical planning formulations like the Planning Domain Definition Language (PDDL) admit action sequences guaranteed to achieve a goal state given an initial state if any are possible. However, reasoning problems defined in PDDL do not capture temporal aspects of action taking, such as concurrent actions between two agents when there are no conflicting conditions, without significant modification and definition to existing PDDL domains. A human expert aware of such constraints can decompose a goal into subgoals, each reachable through single agent planning, to take advantage of simultaneous actions. In contrast to classical planning, large language models (LLMs) directly used for inferring plan steps rarely guarantee execution success, but are capable of leveraging commonsense reasoning to assemble action sequences. We combine the strengths of both classical planning and LLMs by approximating human intuitions for multi-agent planning goal decomposition. We demonstrate that LLM-based goal decomposition leads to faster planning times than solving multi-agent PDDL problems directly while simultaneously achieving fewer plan execution steps than a single agent plan alone, as well as most multiagent plans, while guaranteeing execution success. Additionally, we find that LLM-based approximations of subgoals result in similar multi-agent execution lengths to those specified by human experts. Website and resources at https://glamor-usc.github.io/twostep
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Socratic Planner: Self-QA-Based Zero-Shot Planning for Embodied Instruction Following</title>
<link>https://arxiv.org/abs/2404.15190</link>
<guid>https://arxiv.org/abs/2404.15190</guid>
<content:encoded><![CDATA[
<div> Embodied Instruction Following (EIF)Socratic PlannerLarge Language Model (LLM)<br /><br />: Embodied Instruction Following (EIF)EIF(Socratic Planner)(LLM)Socratic PlannerSocratic PlannerALFRED <div>
arXiv:2404.15190v2 Announce Type: replace 
Abstract: Embodied Instruction Following (EIF) is the task of executing natural language instructions by navigating and interacting with objects in interactive environments. A key challenge in EIF is compositional task planning, typically addressed through supervised learning or few-shot in-context learning with labeled data. To this end, we introduce the Socratic Planner, a self-QA-based zero-shot planning method that infers an appropriate plan without any further training. The Socratic Planner first facilitates self-questioning and answering by the Large Language Model (LLM), which in turn helps generate a sequence of subgoals. While executing the subgoals, an embodied agent may encounter unexpected situations, such as unforeseen obstacles. The Socratic Planner then adjusts plans based on dense visual feedback through a visually-grounded re-planning mechanism. Experiments demonstrate the effectiveness of the Socratic Planner, outperforming current state-of-the-art planning models on the ALFRED benchmark across all metrics, particularly excelling in long-horizon tasks that demand complex inference. We further demonstrate its real-world applicability through deployment on a physical robot for long-horizon tasks.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully Distributed Fog Load Balancing with Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2405.12236</link>
<guid>https://arxiv.org/abs/2405.12236</guid>
<content:encoded><![CDATA[
<div> : IoT, Fog Computing, Multi-Agent Reinforcement Learning (MARL), Load-Balancing, Transfer Learning

:
(MARL)(IoT)MARLGossip <div>
arXiv:2405.12236v2 Announce Type: replace 
Abstract: Real-time Internet of Things (IoT) applications require real-time support to handle the ever-growing demand for computing resources to process IoT workloads. Fog Computing provides high availability of such resources in a distributed manner. However, these resources must be efficiently managed to distribute unpredictable traffic demands among heterogeneous Fog resources. This paper proposes a fully distributed load-balancing solution with Multi-Agent Reinforcement Learning (MARL) that intelligently distributes IoT workloads to optimize the waiting time while providing fair resource utilization in the Fog network. These agents use transfer learning for life-long self-adaptation to dynamic changes in the environment. By leveraging distributed decision-making, MARL agents effectively minimize the waiting time compared to a single centralized agent solution and other baselines, enhancing end-to-end execution delay. Besides performance gain, a fully distributed solution allows for a global-scale implementation where agents can work independently in small collaboration regions, leveraging nearby local resources. Furthermore, we analyze the impact of a realistic frequency to observe the state of the environment, unlike the unrealistic common assumption in the literature of having observations readily available in real-time for every required action. The findings highlight the trade-off between realism and performance using an interval-based Gossip-based multi-casting protocol against assuming real-time observation availability for every generated workload.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alibaba LingmaAgent: Improving Automated Issue Resolution via Comprehensive Repository Exploration</title>
<link>https://arxiv.org/abs/2406.01422</link>
<guid>https://arxiv.org/abs/2406.01422</guid>
<content:encoded><![CDATA[
<div> : LingmaAgentMonte CarloGitHub

<br /><br />:
LingmaAgentLLMLingmaAgentGitHubLingmaAgentSWE-bench LiteSWE-agent18.5%LingmaAgent16.9%43.3%LingmaAgentPythonLingmaAgent <div>
arXiv:2406.01422v2 Announce Type: replace 
Abstract: This paper presents Alibaba LingmaAgent, a novel Automated Software Engineering method designed to comprehensively understand and utilize whole software repositories for issue resolution. Deployed in TONGYI Lingma, an IDE-based coding assistant developed by Alibaba Cloud, LingmaAgent addresses the limitations of existing LLM-based agents that primarily focus on local code information. Our approach introduces a top-down method to condense critical repository information into a knowledge graph, reducing complexity, and employs a Monte Carlo tree search based strategy enabling agents to explore and understand entire repositories. We guide agents to summarize, analyze, and plan using repository-level knowledge, allowing them to dynamically acquire information and generate patches for real-world GitHub issues. In extensive experiments, LingmaAgent demonstrated significant improvements, achieving an 18.5\% relative improvement on the SWE-bench Lite benchmark compared to SWE-agent. In production deployment and evaluation at Alibaba Cloud, LingmaAgent automatically resolved 16.9\% of in-house issues faced by development engineers, and solved 43.3\% of problems after manual intervention. Additionally, we have open-sourced a Python prototype of LingmaAgent for reference by other industrial developers https://github.com/RepoUnderstander/RepoUnderstander. In fact, LingmaAgent has been used as a developed reference by many subsequently agents.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding</title>
<link>https://arxiv.org/abs/2408.11049</link>
<guid>https://arxiv.org/abs/2408.11049</guid>
<content:encoded><![CDATA[
<div> :  (LLMs),  (Speculative Decoding), , , MagicDec<br /><br />: MagicDecLLMsMagicDecMagicDecKVKVLlama3.1-8B322562.51 <div>
arXiv:2408.11049v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) have become more prevalent in long-context applications such as interactive chatbots, document analysis, and agent workflows, but it is challenging to serve long-context requests with low latency and high throughput. Speculative decoding (SD) is a widely used technique to reduce latency losslessly, but the conventional wisdom suggests that its efficacy is limited to small batch sizes. In MagicDec, we show that surprisingly SD can achieve speedup even for a high throughput inference regime for moderate to long sequences. More interestingly, an intelligent drafting strategy can achieve better speedup with increasing batch size based on our rigorous analysis. MagicDec first identifies the bottleneck shifts with increasing batch size and sequence length, and uses these insights to deploy SD more effectively for high throughput inference. We leverage draft model with sparse KV cache to address the KV bottleneck, which scales with both sequence length and batch size. Additionally, we propose a theoretical model to select the optimal drafting strategy for maximum speedup. Our work highlights the broad applicability of speculative decoding in long-context serving, as it can enhance throughput and reduce latency without compromising accuracy. For moderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B when serving batch sizes ranging from 32 to 256 on various types of hardware and tasks.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FoAM: Foresight-Augmented Multi-Task Imitation Policy for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2409.19528</link>
<guid>https://arxiv.org/abs/2409.19528</guid>
<content:encoded><![CDATA[
<div> MTILForesight-Augmented Manipulation PolicyFoAM

<br /><br />:
Foresight-Augmented Manipulation PolicyFoAMMTILMTILFoAM100FoAMMTIL41%1080projFoAM.github.io <div>
arXiv:2409.19528v2 Announce Type: replace 
Abstract: Multi-task imitation learning (MTIL) has shown significant potential in robotic manipulation by enabling agents to perform various tasks using a single policy. This simplifies the policy deployment and enhances the agent's adaptability across different scenarios. However, key challenges remain, such as maintaining action reliability (e.g., avoiding abnormal action sequences that deviate from nominal task trajectories) and generalizing to unseen tasks with a few expert demonstrations. To address these challenges, we introduce the Foresight-Augmented Manipulation Policy (FoAM), a novel MTIL policy that pioneers the use of multi-modal goal condition as input and introduces a foresight augmentation in addition to the general action reconstruction. FoAM enables the agent to reason about the visual consequences (states) of its actions and learn more expressive embedding that captures nuanced task variations. Extensive experiments on over 100 tasks in simulation and real-world settings demonstrate that FoAM significantly enhances MTIL policy performance, outperforming state-of-the-art baselines by up to 41% in success rate. Meanwhile, we released our simulation suites, including a total of 10 scenarios and over 80 challenging tasks designed for manipulation policy training and evaluation. See the project homepage projFoAM.github.io for project details.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for Zero-shot Object Navigation</title>
<link>https://arxiv.org/abs/2411.16425</link>
<guid>https://arxiv.org/abs/2411.16425</guid>
<content:encoded><![CDATA[
<div> Zero-Shot Object Navigation (ZSON)Large Language Model (LLM)TopV-NavAdaptive Visual Prompt Generation (AVPG)Dynamic Map Scaling (DMS)

:<br />
TopV-NavZero-Shot Object Navigation (ZSON)LLMTopV-NavMulti-Layer Large ModelMLLMAdaptive Visual Prompt Generation (AVPG)Dynamic Map Scaling (DMS)Potential Target Driven (PTD)TopV-NavMP3DHM3D <div>
arXiv:2411.16425v2 Announce Type: replace 
Abstract: The Zero-Shot Object Navigation (ZSON) task requires embodied agents to find a previously unseen object by navigating in unfamiliar environments. Such a goal-oriented exploration heavily relies on the ability to perceive, understand, and reason based on the spatial information of the environment. However, current LLM-based approaches convert visual observations to language descriptions and reason in the linguistic space, leading to the loss of spatial information. In this paper, we introduce TopV-Nav, an MLLM-based method that directly reasons on the top-view map with sufficient spatial information. To fully unlock the MLLM's spatial reasoning potential in top-view perspective, we propose the Adaptive Visual Prompt Generation (AVPG) method to adaptively construct semantically-rich top-view map. It enables the agent to directly utilize spatial information contained in the top-view map to conduct thorough reasoning. Besides, we design a Dynamic Map Scaling (DMS) mechanism to dynamically zoom top-view map at preferred scales, enhancing local fine-grained reasoning. Additionally, we devise a Potential Target Driven (PTD) mechanism to predict and to utilize target locations, facilitating global and human-like exploration. Experiments on MP3D and HM3D datasets demonstrate the superiority of our TopV-Nav.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crowd: A Social Network Simulation Framework</title>
<link>https://arxiv.org/abs/2412.10781</link>
<guid>https://arxiv.org/abs/2412.10781</guid>
<content:encoded><![CDATA[
<div> agent-based modeling and simulation (ABMS), social network simulator, Crowd, YAML configuration, generative agents

:<br />
CrowdCrowdYAMLPythonPythonCrowd <div>
arXiv:2412.10781v2 Announce Type: replace 
Abstract: To observe how individual behavior shapes a larger community's actions, agent-based modeling and simulation (ABMS) has been widely adopted by researchers in social sciences, economics, and epidemiology. While simulations can be run on general-purpose ABMS frameworks, these tools are not specifically designed for social networks and, therefore, provide limited features, increasing the effort required for complex simulations. In this paper, we introduce Crowd, a social network simulator that adopts the agent-based modeling methodology to model real-world phenomena within a network environment. Designed to facilitate easy and quick modeling, Crowd supports simulation setup through YAML configuration and enables further customization with user-defined methods. Other features include no-code simulations for diffusion tasks, interactive visualizations, data aggregation, and chart drawing facilities. Designed in Python, Crowd also supports generative agents and connects easily with Python's libraries for data analysis and machine learning. Finally, we include three case studies to illustrate the use of the framework, including generative agents in epidemics, influence maximization, and networked trust games.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI Software Engineer: Programming with Trust</title>
<link>https://arxiv.org/abs/2502.13767</link>
<guid>https://arxiv.org/abs/2502.13767</guid>
<content:encoded><![CDATA[
<div> :  (LLMs),  (AI), , , 

<br /><br />:
LLMsAILLMLLMsLLM <div>
arXiv:2502.13767v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown surprising proficiency in generating code snippets, promising to automate large parts of software engineering via artificial intelligence (AI). We argue that successfully deploying AI software engineers requires a level of trust equal to or even greater than the trust established by human-driven software engineering practices. The recent trend toward LLM agents offers a path toward integrating the power of LLMs to create new code with the power of analysis tools to increase trust in the code. This opinion piece comments on whether LLM agents could dominate software engineering workflows in the future and whether the focus of programming will shift from programming at scale to programming with trust.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Accelerated Distributed Stochastic Gradient Method with Momentum</title>
<link>https://arxiv.org/abs/2402.09714</link>
<guid>https://arxiv.org/abs/2402.09714</guid>
<content:encoded><![CDATA[
<div> : Loopless Chebyshev  (LCA)

:
(DSMT)Loopless ChebyshevDSMT(SGD)DSMT$\mathcal{O}(n^{5/3}/(1-\lambda))$Polyak-{\L}ojasiewicz (PL)$\mathcal{O}(\sqrt{n/(1-\lambda)})$$1-\lambda$ <div>
arXiv:2402.09714v3 Announce Type: replace-cross 
Abstract: In this paper, we introduce an accelerated distributed stochastic gradient method with momentum for solving the distributed optimization problem, where a group of $n$ agents collaboratively minimize the average of the local objective functions over a connected network. The method, termed ``Distributed Stochastic Momentum Tracking (DSMT)'', is a single-loop algorithm that utilizes the momentum tracking technique as well as the Loopless Chebyshev Acceleration (LCA) method. We show that DSMT can asymptotically achieve comparable convergence rates as centralized stochastic gradient descent (SGD) method under a general variance condition regarding the stochastic gradients. Moreover, the number of iterations (transient times) required for DSMT to achieve such rates behaves as $\mathcal{O}(n^{5/3}/(1-\lambda))$ for minimizing general smooth objective functions, and $\mathcal{O}(\sqrt{n/(1-\lambda)})$ under the Polyak-{\L}ojasiewicz (PL) condition. Here, the term $1-\lambda$ denotes the spectral gap of the mixing matrix related to the underlying network topology. Notably, the obtained results do not rely on multiple inter-node communications or stochastic gradient accumulation per iteration, and the transient times are the shortest under the setting to the best of our knowledge.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>4DRGS: 4D Radiative Gaussian Splatting for Efficient 3D Vessel Reconstruction from Sparse-View Dynamic DSA Images</title>
<link>https://arxiv.org/abs/2412.12919</link>
<guid>https://arxiv.org/abs/2412.12919</guid>
<content:encoded><![CDATA[
<div> : (DSA)(4DRGS)

<br /><br />:

(4DRGS)(DSA)XDSA4DRGS532 <div>
arXiv:2412.12919v2 Announce Type: replace-cross 
Abstract: Reconstructing 3D vessel structures from sparse-view dynamic digital subtraction angiography (DSA) images enables accurate medical assessment while reducing radiation exposure. Existing methods often produce suboptimal results or require excessive computation time. In this work, we propose 4D radiative Gaussian splatting (4DRGS) to achieve high-quality reconstruction efficiently. In detail, we represent the vessels with 4D radiative Gaussian kernels. Each kernel has time-invariant geometry parameters, including position, rotation, and scale, to model static vessel structures. The time-dependent central attenuation of each kernel is predicted from a compact neural network to capture the temporal varying response of contrast agent flow. We splat these Gaussian kernels to synthesize DSA images via X-ray rasterization and optimize the model with real captured ones. The final 3D vessel volume is voxelized from the well-trained kernels. Moreover, we introduce accumulated attenuation pruning and bounded scaling activation to improve reconstruction quality. Extensive experiments on real-world patient data demonstrate that 4DRGS achieves impressive results in 5 minutes training, which is 32x faster than the state-of-the-art method. This underscores the potential of 4DRGS for real-world clinics.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is there a future for AI without representation?</title>
<link>https://arxiv.org/abs/2503.18955</link>
<guid>https://arxiv.org/abs/2503.18955</guid>
<content:encoded><![CDATA[
<div> AIRodney Brooks

<br /><br />:
AIRodney BrooksBrooksAIBrooks <div>
arXiv:2503.18955v1 Announce Type: new 
Abstract: This paper investigates the prospects of AI without representation in general, and the proposals of Rodney Brooks in particular. What turns out to be characteristic of Brooks' proposal is the rejection of central control in intelligent agents; his systems has as much or as little representation as traditional AI. The traditional view that representation is necessary for intelligence presupposes that intelligence requires central control. However, much of recent cognitive science suggests that we should dispose of the image of intelligent agents as central representation processors. If this paradigm shift is achieved, Brooks' proposal for non-centralized cognition without representation appears promising for full-blown intelligent agents - though not for conscious agents and thus not for human-like AI.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedAgent-Pro: Towards Multi-modal Evidence-based Medical Diagnosis via Reasoning Agentic Workflow</title>
<link>https://arxiv.org/abs/2503.18968</link>
<guid>https://arxiv.org/abs/2503.18968</guid>
<content:encoded><![CDATA[
<div> : AIMedAgent-Pro

:
AIMedAgent-ProAIMedAgent-ProMedAgent-Pro2D3Dhttps://github.com/jinlab-imvr/MedAgent-Pro <div>
arXiv:2503.18968v1 Announce Type: new 
Abstract: Developing reliable AI systems to assist human clinicians in multi-modal medical diagnosis has long been a key objective for researchers. Recently, Multi-modal Large Language Models (MLLMs) have gained significant attention and achieved success across various domains. With strong reasoning capabilities and the ability to perform diverse tasks based on user instructions, they hold great potential for enhancing medical diagnosis. However, directly applying MLLMs to the medical domain still presents challenges. They lack detailed perception of visual inputs, limiting their ability to perform quantitative image analysis, which is crucial for medical diagnostics. Additionally, MLLMs often exhibit hallucinations and inconsistencies in reasoning, whereas clinical diagnoses must adhere strictly to established criteria. To address these challenges, we propose MedAgent-Pro, an evidence-based reasoning agentic system designed to achieve reliable, explainable, and precise medical diagnoses. This is accomplished through a hierarchical workflow: at the task level, knowledge-based reasoning generate reliable diagnostic plans for specific diseases following retrieved clinical criteria. While at the case level, multiple tool agents process multi-modal inputs, analyze different indicators according to the plan, and provide a final diagnosis based on both quantitative and qualitative evidence. Comprehensive experiments on both 2D and 3D medical diagnosis tasks demonstrate the superiority and effectiveness of MedAgent-Pro, while case studies further highlight its reliability and interpretability. The code is available at https://github.com/jinlab-imvr/MedAgent-Pro.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SplitFrozen: Split Learning with Device-side Model Frozen for Fine-Tuning LLM on Heterogeneous Resource-Constrained Devices</title>
<link>https://arxiv.org/abs/2503.18986</link>
<guid>https://arxiv.org/abs/2503.18986</guid>
<content:encoded><![CDATA[
<div> SplitFrozen

<br /><br />:
SplitFrozenLLMsLLMsSplitFrozenLoRASplitFrozenFedLoRASplitLoRA69.4%86.8%50.2%SplitFrozenLlama-3.2GSM8K <div>
arXiv:2503.18986v1 Announce Type: new 
Abstract: Fine-tuning large language models (LLMs) on private, on-device data can empower tailored personalized AI agents. However, fine-tuning LLMs on resource-constrained edge devices faces significant challenges, including excessive computation overhead, device heterogeneity, and data imbalance. This paper proposes SplitFrozen, a split learning framework that enables efficient LLM fine-tuning by strategically freezing device-side model layers while centralizing parameter-efficient fine-tuning on the server. Our framework partitions LLMs into device-side frozen layers and server-side fine-tuning layers, where heterogeneous resource-constrained devices execute only forward propagation. To minimize server-side training costs, we integrate Low-Rank Adaptation (LoRA) into the server-side layers. A pipeline parallelism strategy further optimizes training efficiency by decoupling device-server computations and leveraging decomposed backward propagation. Experiments on GPT-2 with the MRPC, MNLI-matched, and SST-2 datasets demonstrate that SplitFrozen outperforms FedLoRA and SplitLoRA by 69.4\% model accuracy under extremely imbalanced data, while reducing up to 86.8\% device-side computations and 50.2\% total training time. Experiments also validate the scalability of SplitFrozen on content generation task using Llama-3.2 model on GSM8K dataset.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WikiAutoGen: Towards Multi-Modal Wikipedia-Style Article Generation</title>
<link>https://arxiv.org/abs/2503.19065</link>
<guid>https://arxiv.org/abs/2503.19065</guid>
<content:encoded><![CDATA[
<div> : WikiAutoGenWikipedia

:<br />
WikiAutoGenWikipediaWikiAutoGenWikiSeekWikipediaWikiAutoGenWikiSeek8%-29%https://wikiautogen.github.io/  <div>
arXiv:2503.19065v1 Announce Type: new 
Abstract: Knowledge discovery and collection are intelligence-intensive tasks that traditionally require significant human effort to ensure high-quality outputs. Recent research has explored multi-agent frameworks for automating Wikipedia-style article generation by retrieving and synthesizing information from the internet. However, these methods primarily focus on text-only generation, overlooking the importance of multimodal content in enhancing informativeness and engagement. In this work, we introduce WikiAutoGen, a novel system for automated multimodal Wikipedia-style article generation. Unlike prior approaches, WikiAutoGen retrieves and integrates relevant images alongside text, enriching both the depth and visual appeal of generated content. To further improve factual accuracy and comprehensiveness, we propose a multi-perspective self-reflection mechanism, which critically assesses retrieved content from diverse viewpoints to enhance reliability, breadth, and coherence, etc. Additionally, we introduce WikiSeek, a benchmark comprising Wikipedia articles with topics paired with both textual and image-based representations, designed to evaluate multimodal knowledge generation on more challenging topics. Experimental results show that WikiAutoGen outperforms previous methods by 8%-29% on our WikiSeek benchmark, producing more accurate, coherent, and visually enriched Wikipedia-style articles. We show some of our generated examples in https://wikiautogen.github.io/ .
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Based Insight Extraction for Contact Center Analytics and Cost-Efficient Deployment</title>
<link>https://arxiv.org/abs/2503.19090</link>
<guid>https://arxiv.org/abs/2503.19090</guid>
<content:encoded><![CDATA[
<div> : 

<br /><br />:

FAQLLM <div>
arXiv:2503.19090v1 Announce Type: new 
Abstract: Large Language Models have transformed the Contact Center industry, manifesting in enhanced self-service tools, streamlined administrative processes, and augmented agent productivity. This paper delineates our system that automates call driver generation, which serves as the foundation for tasks such as topic modeling, incoming call classification, trend detection, and FAQ generation, delivering actionable insights for contact center agents and administrators to consume. We present a cost-efficient LLM system design, with 1) a comprehensive evaluation of proprietary, open-weight, and fine-tuned models and 2) cost-efficient strategies, and 3) the corresponding cost analysis when deployed in production environments.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Centers and Margins of Modeling Humans in Well-being Technologies: A Decentering Approach</title>
<link>https://arxiv.org/abs/2503.19132</link>
<guid>https://arxiv.org/abs/2503.19132</guid>
<content:encoded><![CDATA[
<div> :  (ML), , , , <br /><br />:
MLML/ML-ML <div>
arXiv:2503.19132v1 Announce Type: new 
Abstract: This paper critically examines the machine learning (ML) modeling of humans in three case studies of well-being technologies. Through a critical technical approach, it examines how these apps were experienced in daily life (technology in use) to surface breakdowns and to identify the assumptions about the "human" body entrenched in the ML models (technology design). To address these issues, this paper applies agential realism to decenter foundational assumptions, such as body regularity and health/illness binaries, and speculates more inclusive design and ML modeling paths that acknowledge irregularity, human-system entanglements, and uncertain transitions. This work is among the first to explore the implications of decentering theories in computational modeling of human bodies and well-being, offering insights for more inclusive technologies and speculations toward posthuman-centered ML modeling.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Byzantine Resilient Federated Multi-Task Representation Learning</title>
<link>https://arxiv.org/abs/2503.19209</link>
<guid>https://arxiv.org/abs/2503.19209</guid>
<content:encoded><![CDATA[
<div> BR-MTRLByzantine-resilientmulti-task representation learningfederated settinggeometric median aggregation

:<br />
BR-MTRL-AWSCIFAR-10FEMINIST <div>
arXiv:2503.19209v1 Announce Type: new 
Abstract: In this paper, we propose BR-MTRL, a Byzantine-resilient multi-task representation learning framework that handles faulty or malicious agents. Our approach leverages representation learning through a shared neural network model, where all clients share fixed layers, except for a client-specific final layer. This structure captures shared features among clients while enabling individual adaptation, making it a promising approach for leveraging client data and computational power in heterogeneous federated settings to learn personalized models. To learn the model, we employ an alternating gradient descent strategy: each client optimizes its local model, updates its final layer, and sends estimates of the shared representation to a central server for aggregation. To defend against Byzantine agents, we employ geometric median aggregation for robust client-server communication. Our method enables personalized learning while maintaining resilience in distributed settings. We implemented the proposed alternating gradient descent algorithm in a federated testbed built using Amazon Web Services (AWS) platform and compared its performance with various benchmark algorithms and their variations. Through extensive experiments using real-world datasets, including CIFAR-10 and FEMINIST, we demonstrated the effectiveness and robustness of our approach and its transferability to new unseen clients with limited data, even in the presence of Byzantine adversaries.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Large Language Model Agents for Question Answering</title>
<link>https://arxiv.org/abs/2503.19213</link>
<guid>https://arxiv.org/abs/2503.19213</guid>
<content:encoded><![CDATA[
<div> 

:
<br />
LLMQAQALLMLLMQAQALLM QALLMQALLMQA <div>
arXiv:2503.19213v1 Announce Type: new 
Abstract: This paper surveys the development of large language model (LLM)-based agents for question answering (QA). Traditional agents face significant limitations, including substantial data requirements and difficulty in generalizing to new environments. LLM-based agents address these challenges by leveraging LLMs as their core reasoning engine. These agents achieve superior QA results compared to traditional QA pipelines and naive LLM QA systems by enabling interaction with external environments. We systematically review the design of LLM agents in the context of QA tasks, organizing our discussion across key stages: planning, question understanding, information retrieval, and answer generation. Additionally, this paper identifies ongoing challenges and explores future research directions to enhance the performance of LLM agent QA systems.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARS: Memory-Enhanced Agents with Reflective Self-improvement</title>
<link>https://arxiv.org/abs/2503.19271</link>
<guid>https://arxiv.org/abs/2503.19271</guid>
<content:encoded><![CDATA[
<div> : , , , , MARS

:
MARSEbbinghausMARS <div>
arXiv:2503.19271v1 Announce Type: new 
Abstract: Large language models (LLMs) have made significant advances in the field of natural language processing, but they still face challenges such as continuous decision-making, lack of long-term memory, and limited context windows in dynamic environments. To address these issues, this paper proposes an innovative framework Memory-Enhanced Agents with Reflective Self-improvement. The MARS framework comprises three agents: the User, the Assistant, and the Checker. By integrating iterative feedback, reflective mechanisms, and a memory optimization mechanism based on the Ebbinghaus forgetting curve, it significantly enhances the agents capabilities in handling multi-tasking and long-span information.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoMAC: Conversational Agent for Multi-Source Auxiliary Context with Sparse and Symmetric Latent Interactions</title>
<link>https://arxiv.org/abs/2503.19274</link>
<guid>https://arxiv.org/abs/2503.19274</guid>
<content:encoded><![CDATA[
<div> : AICoMAC

:
CoMACCoMACCoMAC<br /><br /> <div>
arXiv:2503.19274v1 Announce Type: new 
Abstract: Recent advancements in AI-driven conversational agents have exhibited immense potential of AI applications. Effective response generation is crucial to the success of these agents. While extensive research has focused on leveraging multiple auxiliary data sources (e.g., knowledge bases and personas) to enhance response generation, existing methods often struggle to efficiently extract relevant information from these sources. There are still clear limitations in the ability to combine versatile conversational capabilities with adherence to known facts and adaptation to large variations in user preferences and belief systems, which continues to hinder the wide adoption of conversational AI tools. This paper introduces a novel method, Conversational Agent for Multi-Source Auxiliary Context with Sparse and Symmetric Latent Interactions (CoMAC), for conversation generation, which employs specialized encoding streams and post-fusion grounding networks for multiple data sources to identify relevant persona and knowledge information for the conversation. CoMAC also leverages a novel text similarity metric that allows bi-directional information sharing among multiple sources and focuses on a selective subset of meaningful words. Our experiments show that CoMAC improves the relevant persona and knowledge prediction accuracies and response generation quality significantly over two state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CubeRobot: Grounding Language in Rubik's Cube Manipulation via Vision-Language Model</title>
<link>https://arxiv.org/abs/2503.19281</link>
<guid>https://arxiv.org/abs/2503.19281</guid>
<content:encoded><![CDATA[
<div> CubeRobotVision-Language Model (VLM)CubeCoTVisionCoTMemory Stream

:
CubeRobot(VLM)3x3CubeCoT43VisionCoTMemory StreamCubeRobotVLMCubeRobot100%80% <div>
arXiv:2503.19281v1 Announce Type: new 
Abstract: Proving Rubik's Cube theorems at the high level represents a notable milestone in human-level spatial imagination and logic thinking and reasoning. Traditional Rubik's Cube robots, relying on complex vision systems and fixed algorithms, often struggle to adapt to complex and dynamic scenarios. To overcome this limitation, we introduce CubeRobot, a novel vision-language model (VLM) tailored for solving 3x3 Rubik's Cubes, empowering embodied agents with multimodal understanding and execution capabilities. We used the CubeCoT image dataset, which contains multiple-level tasks (43 subtasks in total) that humans are unable to handle, encompassing various cube states. We incorporate a dual-loop VisionCoT architecture and Memory Stream, a paradigm for extracting task-related features from VLM-generated planning queries, thus enabling CubeRobot to independent planning, decision-making, reflection and separate management of high- and low-level Rubik's Cube tasks. Furthermore, in low-level Rubik's Cube restoration tasks, CubeRobot achieved a high accuracy rate of 100%, similar to 100% in medium-level tasks, and achieved an accuracy rate of 80% in high-level tasks.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Substance over Style: Evaluating Proactive Conversational Coaching Agents</title>
<link>https://arxiv.org/abs/2503.19328</link>
<guid>https://arxiv.org/abs/2503.19328</guid>
<content:encoded><![CDATA[
<div> NLP

:
NLP155NLP<br /><br /> <div>
arXiv:2503.19328v1 Announce Type: new 
Abstract: While NLP research has made strides in conversational tasks, many approaches focus on single-turn responses with well-defined objectives or evaluation criteria. In contrast, coaching presents unique challenges with initially undefined goals that evolve through multi-turn interactions, subjective evaluation criteria, mixed-initiative dialogue. In this work, we describe and implement five multi-turn coaching agents that exhibit distinct conversational styles, and evaluate them through a user study, collecting first-person feedback on 155 conversations. We find that users highly value core functionality, and that stylistic components in absence of core components are viewed negatively. By comparing user feedback with third-person evaluations from health experts and an LM, we reveal significant misalignment across evaluation approaches. Our findings provide insights into design and evaluation of conversational coaching agents and contribute toward improving human-centered NLP applications.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design of Seamless Multi-modal Interaction Framework for Intelligent Virtual Agents in Wearable Mixed Reality Environment</title>
<link>https://arxiv.org/abs/2503.19334</link>
<guid>https://arxiv.org/abs/2503.19334</guid>
<content:encoded><![CDATA[
<div> 

:
MR2-4MR <div>
arXiv:2503.19334v1 Announce Type: new 
Abstract: In this paper, we present the design of a multimodal interaction framework for intelligent virtual agents in wearable mixed reality environments, especially for interactive applications at museums, botanical gardens, and similar places. These places need engaging and no-repetitive digital content delivery to maximize user involvement. An intelligent virtual agent is a promising mode for both purposes. Premises of framework is wearable mixed reality provided by MR devices supporting spatial mapping. We envisioned a seamless interaction framework by integrating potential features of spatial mapping, virtual character animations, speech recognition, gazing, domain-specific chatbot and object recognition to enhance virtual experiences and communication between users and virtual agents. By applying a modular approach and deploying computationally intensive modules on cloud-platform, we achieved a seamless virtual experience in a device with limited resources. Human-like gaze and speech interaction with a virtual agent made it more interactive. Automated mapping of body animations with the content of a speech made it more engaging. In our tests, the virtual agents responded within 2-4 seconds after the user query. The strength of the framework is flexibility and adaptability. It can be adapted to any wearable MR device supporting spatial mapping.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TraF-Align: Trajectory-aware Feature Alignment for Asynchronous Multi-agent Perception</title>
<link>https://arxiv.org/abs/2503.19391</link>
<guid>https://arxiv.org/abs/2503.19391</guid>
<content:encoded><![CDATA[
<div> Cooperative perception, latency, TraF-Align, feature-level trajectory, asynchronous cooperative perception

:
TraF-AlignTraF-AlignTraF-AlignV2V4RealDAIR-V2X-Seq <div>
arXiv:2503.19391v1 Announce Type: new 
Abstract: Cooperative perception presents significant potential for enhancing the sensing capabilities of individual vehicles, however, inter-agent latency remains a critical challenge. Latencies cause misalignments in both spatial and semantic features, complicating the fusion of real-time observations from the ego vehicle with delayed data from others. To address these issues, we propose TraF-Align, a novel framework that learns the flow path of features by predicting the feature-level trajectory of objects from past observations up to the ego vehicle's current time. By generating temporally ordered sampling points along these paths, TraF-Align directs attention from the current-time query to relevant historical features along each trajectory, supporting the reconstruction of current-time features and promoting semantic interaction across multiple frames. This approach corrects spatial misalignment and ensures semantic consistency across agents, effectively compensating for motion and achieving coherent feature fusion. Experiments on two real-world datasets, V2V4Real and DAIR-V2X-Seq, show that TraF-Align sets a new benchmark for asynchronous cooperative perception.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Deep Reinforcement Learning for Safe Autonomous Driving with RICS-Assisted MEC</title>
<link>https://arxiv.org/abs/2503.19418</link>
<guid>https://arxiv.org/abs/2503.19418</guid>
<content:encoded><![CDATA[
<div> : 

:
MECV2IMECV2VV2VV2IRICSsV2IV2VDS-MADRLRICS <div>
arXiv:2503.19418v1 Announce Type: new 
Abstract: Environment sensing and fusion via onboard sensors are envisioned to be widely applied in future autonomous driving networks. This paper considers a vehicular system with multiple self-driving vehicles that is assisted by multi-access edge computing (MEC), where image data collected by the sensors is offloaded from cellular vehicles to the MEC server using vehicle-to-infrastructure (V2I) links. Sensory data can also be shared among surrounding vehicles via vehicle-to-vehicle (V2V) communication links. To improve spectrum utilization, the V2V links may reuse the same frequency spectrum with V2I links, which may cause severe interference. To tackle this issue, we leverage reconfigurable intelligent computational surfaces (RICSs) to jointly enable V2I reflective links and mitigate interference appearing at the V2V links. Considering the limitations of traditional algorithms in addressing this problem, such as the assumption for quasi-static channel state information, which restricts their ability to adapt to dynamic environmental changes and leads to poor performance under frequently varying channel conditions, in this paper, we formulate the problem at hand as a Markov game. Our novel formulation is applied to time-varying channels subject to multi-user interference and introduces a collaborative learning mechanism among users. The considered optimization problem is solved via a driving safety-enabled multi-agent deep reinforcement learning (DS-MADRL) approach that capitalizes on the RICS presence. Our extensive numerical investigations showcase that the proposed reinforcement learning approach achieves faster convergence and significant enhancements in both data rate and driving safety, as compared to various state-of-the-art benchmarks.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Probabilistic Neuro-symbolic Layer for Algebraic Constraint Satisfaction</title>
<link>https://arxiv.org/abs/2503.19466</link>
<guid>https://arxiv.org/abs/2503.19466</guid>
<content:encoded><![CDATA[
<div> :  (PAL)

:
<br />
Probabilistic Algebraic LayerPALPALPALPAL <div>
arXiv:2503.19466v1 Announce Type: new 
Abstract: In safety-critical applications, guaranteeing the satisfaction of constraints over continuous environments is crucial, e.g., an autonomous agent should never crash into obstacles or go off-road. Neural models struggle in the presence of these constraints, especially when they involve intricate algebraic relationships. To address this, we introduce a differentiable probabilistic layer that guarantees the satisfaction of non-convex algebraic constraints over continuous variables. This probabilistic algebraic layer (PAL) can be seamlessly plugged into any neural architecture and trained via maximum likelihood without requiring approximations. PAL defines a distribution over conjunctions and disjunctions of linear inequalities, parameterized by polynomials. This formulation enables efficient and exact renormalization via symbolic integration, which can be amortized across different data points and easily parallelized on a GPU. We showcase PAL and our integration scheme on a number of benchmarks for algebraic constraint integration and on real-world trajectory data.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-Initiated Interaction in Phone UI Automation</title>
<link>https://arxiv.org/abs/2503.19537</link>
<guid>https://arxiv.org/abs/2503.19537</guid>
<content:encoded><![CDATA[
<div> : AndroidInteraction

:
AndroidInteractionUIUIUI <div>
arXiv:2503.19537v1 Announce Type: new 
Abstract: Phone automation agents aim to autonomously perform a given natural-language user request, such as scheduling appointments or booking a hotel. While much research effort has been devoted to screen understanding and action planning, complex tasks often necessitate user interaction for successful completion. Aligning the agent with the user's expectations is crucial for building trust and enabling personalized experiences. This requires the agent to proactively engage the user when necessary, avoiding actions that violate their preferences while refraining from unnecessary questions where a default action is expected. We argue that such subtle agent-initiated interaction with the user deserves focused research attention.
  To promote such research, this paper introduces a task formulation for detecting the need for user interaction and generating appropriate messages. We thoroughly define the task, including aspects like interaction timing and the scope of the agent's autonomy. Using this definition, we derived annotation guidelines and created AndroidInteraction, a diverse dataset for the task, leveraging an existing UI automation dataset. We tested several text-based and multimodal baseline models for the task, finding that it is very challenging for current LLMs. We suggest that our task formulation, dataset, baseline models and analysis will be valuable for future UI automation research, specifically in addressing this crucial yet often overlooked aspect of agent-initiated interaction. This work provides a needed foundation to allow personalized agents to properly engage the user when needed, within the context of phone UI automation.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-agent Application System in Office Collaboration Scenarios</title>
<link>https://arxiv.org/abs/2503.19584</link>
<guid>https://arxiv.org/abs/2503.19584</guid>
<content:encoded><![CDATA[
<div> 

:
 <div>
arXiv:2503.19584v1 Announce Type: new 
Abstract: This paper introduces a multi-agent application system designed to enhance office collaboration efficiency and work quality. The system integrates artificial intelligence, machine learning, and natural language processing technologies, achieving functionalities such as task allocation, progress monitoring, and information sharing. The agents within the system are capable of providing personalized collaboration support based on team members' needs and incorporate data analysis tools to improve decision-making quality. The paper also proposes an intelligent agent architecture that separates Plan and Solver, and through techniques such as multi-turn query rewriting and business tool retrieval, it enhances the agent's multi-intent and multi-turn dialogue capabilities. Furthermore, the paper details the design of tools and multi-turn dialogue in the context of office collaboration scenarios, and validates the system's effectiveness through experiments and evaluations. Ultimately, the system has demonstrated outstanding performance in real business applications, particularly in query understanding, task planning, and tool calling. Looking forward, the system is expected to play a more significant role in addressing complex interaction issues within dynamic environments and large-scale multi-agent systems.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Rapid Shared Human-AI Mental Model Alignment via the After-Action Review</title>
<link>https://arxiv.org/abs/2503.19607</link>
<guid>https://arxiv.org/abs/2503.19607</guid>
<content:encoded><![CDATA[
<div> MinecraftAIGPT-4

:<br />
HMTMinecraftAIAIMinecraftAIHMTGPT-4AI <div>
arXiv:2503.19607v1 Announce Type: new 
Abstract: In this work, we present two novel contributions toward improving research in human-machine teaming (HMT): 1) a Minecraft testbed to accelerate testing and deployment of collaborative AI agents and 2) a tool to allow users to revisit and analyze behaviors within an HMT episode to facilitate shared mental model development. Our browser-based Minecraft testbed allows for rapid testing of collaborative agents in a continuous-space, real-time, partially-observable environment with real humans without cumbersome setup typical to human-AI interaction user studies. As Minecraft has an extensive player base and a rich ecosystem of pre-built AI agents, we hope this contribution can help to facilitate research quickly in the design of new collaborative agents and in understanding different human factors within HMT. Our mental model alignment tool facilitates user-led post-mission analysis by including video displays of first-person perspectives of the team members (i.e., the human and AI) that can be replayed, and a chat interface that leverages GPT-4 to provide answers to various queries regarding the AI's experiences and model details.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk-Aware Reinforcement Learning for Autonomous Driving: Improving Safety When Driving through Intersection</title>
<link>https://arxiv.org/abs/2503.19690</link>
<guid>https://arxiv.org/abs/2503.19690</guid>
<content:encoded><![CDATA[
<div> <br /><br />:<br />
MMAMactor-criticMMAM <div>
arXiv:2503.19690v1 Announce Type: new 
Abstract: Applying reinforcement learning to autonomous driving has garnered widespread attention. However, classical reinforcement learning methods optimize policies by maximizing expected rewards but lack sufficient safety considerations, often putting agents in hazardous situations. This paper proposes a risk-aware reinforcement learning approach for autonomous driving to improve the safety performance when crossing the intersection. Safe critics are constructed to evaluate driving risk and work in conjunction with the reward critic to update the actor. Based on this, a Lagrangian relaxation method and cyclic gradient iteration are combined to project actions into a feasible safe region. Furthermore, a Multi-hop and Multi-layer perception (MLP) mixed Attention Mechanism (MMAM) is incorporated into the actor-critic network, enabling the policy to adapt to dynamic traffic and overcome permutation sensitivity challenges. This allows the policy to focus more effectively on surrounding potential risks while enhancing the identification of passing opportunities. Simulation tests are conducted on different tasks at unsignalized intersections. The results show that the proposed approach effectively reduces collision rates and improves crossing efficiency in comparison to baseline algorithms. Additionally, our ablation experiments demonstrate the benefits of incorporating risk-awareness and MMAM into RL.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Writing as a testbed for open ended agents</title>
<link>https://arxiv.org/abs/2503.19711</link>
<guid>https://arxiv.org/abs/2503.19711</guid>
<content:encoded><![CDATA[
<div> : LLMs

:
LLMsLLMGemini 1.5 ProClaude 3.5 SonnetGPT-4o<br /><br /> <div>
arXiv:2503.19711v1 Announce Type: new 
Abstract: Open-ended tasks are particularly challenging for LLMs due to the vast solution space, demanding both expansive exploration and adaptable strategies, especially when success lacks a clear, objective definition. Writing, with its vast solution space and subjective evaluation criteria, provides a compelling testbed for studying such problems. In this paper, we investigate the potential of LLMs to act as collaborative co-writers, capable of suggesting and implementing text improvements autonomously. We analyse three prominent LLMs - Gemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-4o - focusing on how their action diversity, human alignment, and iterative improvement capabilities impact overall performance. This work establishes a framework for benchmarking autonomous writing agents and, more broadly, highlights fundamental challenges and potential solutions for building systems capable of excelling in diverse open-ended domains.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinking agents for zero-shot generalization to qualitatively novel tasks</title>
<link>https://arxiv.org/abs/2503.19815</link>
<guid>https://arxiv.org/abs/2503.19815</guid>
<content:encoded><![CDATA[
<div> 

:<br />
 <div>
arXiv:2503.19815v1 Announce Type: new 
Abstract: Intelligent organisms can solve truly novel problems which they have never encountered before, either in their lifetime or their evolution. An important component of this capacity is the ability to ``think'', that is, to mentally manipulate objects, concepts and behaviors in order to plan and evaluate possible solutions to novel problems, even without environment interaction. To generate problems that are truly qualitatively novel, while still solvable zero-shot (by mental simulation), we use the combinatorial nature of environments: we train the agent while withholding a specific combination of the environment's elements. The novel test task, based on this combination, is thus guaranteed to be truly novel, while still mentally simulable since the agent has been exposed to each individual element (and their pairwise interactions) during training. We propose a method to train agents endowed with world models to make use their mental simulation abilities, by selecting tasks based on the difference between the agent's pre-thinking and post-thinking performance. When tested on the novel, withheld problem, the resulting agent successfully simulated alternative scenarios and used the resulting information to guide its behavior in the actual environment, solving the novel task in a single real-environment trial (zero-shot).
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FALCONEye: Finding Answers and Localizing Content in ONE-hour-long videos with multi-modal LLMs</title>
<link>https://arxiv.org/abs/2503.19850</link>
<guid>https://arxiv.org/abs/2503.19850</guid>
<content:encoded><![CDATA[
<div> FALCONEyeVision-Language  (VLM)Large Language Model (LLM)

:<br />
FALCONEyeVLMLLMFALCONEye123VLMsVLMLLMFALCON-BenchFALCONEyeFALCON-Bench <div>
arXiv:2503.19850v1 Announce Type: new 
Abstract: Information retrieval in hour-long videos presents a significant challenge, even for state-of-the-art Vision-Language Models (VLMs), particularly when the desired information is localized within a small subset of frames. Long video data presents challenges for VLMs due to context window limitations and the difficulty of pinpointing frames containing the answer. Our novel video agent, FALCONEye, combines a VLM and a Large Language Model (LLM) to search relevant information along the video, and locate the frames with the answer. FALCONEye novelty relies on 1) the proposed meta-architecture, which is better suited to tackle hour-long videos compared to short video approaches in the state-of-the-art; 2) a new efficient exploration algorithm to locate the information using short clips, captions and answer confidence; and 3) our state-of-the-art VLMs calibration analysis for the answer confidence. Our agent is built over a small-size VLM and a medium-size LLM being accessible to run on standard computational resources. We also release FALCON-Bench, a benchmark to evaluate long (average > 1 hour) Video Answer Search challenges, highlighting the need for open-ended question evaluation. Our experiments show FALCONEye's superior performance than the state-of-the-art in FALCON-Bench, and similar or better performance in related benchmarks.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Online Multi-Modal Social Interaction Understanding</title>
<link>https://arxiv.org/abs/2503.19851</link>
<guid>https://arxiv.org/abs/2503.19851</guid>
<content:encoded><![CDATA[
<div> : 

<br /><br />:
Online-MMSIOnline-MMSI-VLMGitHubhttps://github.com/Sampson-Lee/OnlineMMSI <div>
arXiv:2503.19851v1 Announce Type: new 
Abstract: Multimodal social interaction understanding (MMSI) is critical in human-robot interaction systems. In real-world scenarios, AI agents are required to provide real-time feedback. However, existing models often depend on both past and future contexts, which hinders them from applying to real-world problems. To bridge this gap, we propose an online MMSI setting, where the model must resolve MMSI tasks using only historical information, such as recorded dialogues and video streams. To address the challenges of missing the useful future context, we develop a novel framework, named Online-MMSI-VLM, that leverages two complementary strategies: multi-party conversation forecasting and social-aware visual prompting with multi-modal large language models. First, to enrich linguistic context, the multi-party conversation forecasting simulates potential future utterances in a coarse-to-fine manner, anticipating upcoming speaker turns and then generating fine-grained conversational details. Second, to effectively incorporate visual social cues like gaze and gesture, social-aware visual prompting highlights the social dynamics in video with bounding boxes and body keypoints for each person and frame. Extensive experiments on three tasks and two datasets demonstrate that our method achieves state-of-the-art performance and significantly outperforms baseline models, indicating its effectiveness on Online-MMSI. The code and pre-trained models will be publicly released at: https://github.com/Sampson-Lee/OnlineMMSI.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Satisfaction of Long-Term Spatial Constraints in Multi-Agent Systems: A Distributed Optimization Approach (extended version)</title>
<link>https://arxiv.org/abs/2503.19879</link>
<guid>https://arxiv.org/abs/2503.19879</guid>
<content:encoded><![CDATA[
<div> multi-agent

:
<br /><br /> <div>
arXiv:2503.19879v1 Announce Type: new 
Abstract: This paper addresses the problem of collaboratively satisfying long-term spatial constraints in multi-agent systems. Each agent is subject to spatial constraints, expressed as inequalities, which may depend on the positions of other agents with whom they may or may not have direct communication. These constraints need to be satisfied asymptotically or after an unknown finite time. The agents' objective is to collectively achieve a formation that fulfills all constraints. The problem is initially framed as a centralized unconstrained optimization, where the solution yields the optimal configuration by maximizing an objective function that reflects the degree of constraint satisfaction. This function encourages collaboration, ensuring agents help each other meet their constraints while fulfilling their own. When the constraints are infeasible, agents converge to a least-violating solution. A distributed consensus-based optimization scheme is then introduced, which approximates the centralized solution, leading to the development of distributed controllers for single-integrator agents. Finally, simulations validate the effectiveness of the proposed approach.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kernel Learning Assisted Synthesis Condition Exploration for Ternary Spinel</title>
<link>https://arxiv.org/abs/2503.19637</link>
<guid>https://arxiv.org/abs/2503.19637</guid>
<content:encoded><![CDATA[
<div> SHAP

<br /><br />
Fe<sub>2</sub>(ZnCo)O<sub>4</sub>SHAPSHAP <div>
arXiv:2503.19637v1 Announce Type: cross 
Abstract: Machine learning and high-throughput experimentation have greatly accelerated the discovery of mixed metal oxide catalysts by leveraging their compositional flexibility. However, the lack of established synthesis routes for solid-state materials remains a significant challenge in inorganic chemistry. An interpretable machine learning model is therefore essential, as it provides insights into the key factors governing phase formation. Here, we focus on the formation of single-phase Fe$_2$(ZnCo)O$_4$, synthesized via a high-throughput co-precipitation method. We combined a kernel classification model with a novel application of global SHAP analysis to pinpoint the experimental features most critical to single phase synthesizability by interpreting the contributions of each feature. Global SHAP analysis reveals that precursor and precipitating agent contributions to single-phase spinel formation align closely with established crystal growth theories. These results not only underscore the importance of interpretable machine learning in refining synthesis protocols but also establish a framework for data-informed experimental design in inorganic synthesis.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Agent Framework Integrating Large Language Models and Generative AI for Accelerated Metamaterial Design</title>
<link>https://arxiv.org/abs/2503.19889</link>
<guid>https://arxiv.org/abs/2503.19889</guid>
<content:encoded><![CDATA[
<div> metamaterials, CrossMatAgent, , AI, 

<br /><br />:

CrossMatAgentGPT-4AIDALL-E 3Stable Diffusion XLCrossMatAgent3DCLIP-basedSHAPCrossMatAgent <div>
arXiv:2503.19889v1 Announce Type: cross 
Abstract: Metamaterials, renowned for their exceptional mechanical, electromagnetic, and thermal properties, hold transformative potential across diverse applications, yet their design remains constrained by labor-intensive trial-and-error methods and limited data interoperability. Here, we introduce CrossMatAgent--a novel multi-agent framework that synergistically integrates large language models with state-of-the-art generative AI to revolutionize metamaterial design. By orchestrating a hierarchical team of agents--each specializing in tasks such as pattern analysis, architectural synthesis, prompt engineering, and supervisory feedback--our system leverages the multimodal reasoning of GPT-4o alongside the generative precision of DALL-E 3 and a fine-tuned Stable Diffusion XL model. This integrated approach automates data augmentation, enhances design fidelity, and produces simulation- and 3D printing-ready metamaterial patterns. Comprehensive evaluations, including CLIP-based alignment, SHAP interpretability analyses, and mechanical simulations under varied load conditions, demonstrate the framework's ability to generate diverse, reproducible, and application-ready designs. CrossMatAgent thus establishes a scalable, AI-driven paradigm that bridges the gap between conceptual innovation and practical realization, paving the way for accelerated metamaterial development.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LTL-Constrained Policy Optimization with Cycle Experience Replay</title>
<link>https://arxiv.org/abs/2404.11578</link>
<guid>https://arxiv.org/abs/2404.11578</guid>
<content:encoded><![CDATA[
<div> Linear Temporal Logic (LTL)Cycle Experience Replay (CyclER)

:
LTLLTLLTLLTLCycle Experience Replay (CyclER) LTLCyclERLTLCyclERLTL <div>
arXiv:2404.11578v3 Announce Type: replace 
Abstract: Linear Temporal Logic (LTL) offers a precise means for constraining the behavior of reinforcement learning agents. However, in many settings where both satisfaction and optimality conditions are present, LTL is insufficient to capture both. Instead, LTL-constrained policy optimization, where the goal is to optimize a scalar reward under LTL constraints, is needed. This constrained optimization problem proves difficult in deep Reinforcement Learning (DRL) settings, where learned policies often ignore the LTL constraint due to the sparse nature of LTL satisfaction. To alleviate the sparsity issue, we introduce Cycle Experience Replay (CyclER), a novel reward shaping technique that exploits the underlying structure of the LTL constraint to guide a policy towards satisfaction by encouraging partial behaviors compliant with the constraint. We provide a theoretical guarantee that optimizing CyclER will achieve policies that satisfy the LTL constraint with near-optimal probability. We evaluate CyclER in three continuous control domains. Our experimental results show that optimizing CyclER in tandem with the existing scalar reward outperforms existing reward-shaping methods at finding performant LTL-satisfying policies.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infinite-Horizon Optimal Wireless Control Over Shared State-Dependent Fading Channels for IIoT Systems</title>
<link>https://arxiv.org/abs/2408.15492</link>
<guid>https://arxiv.org/abs/2408.15492</guid>
<content:encoded><![CDATA[
<div> 

:
WCSMASWCSWCSMASMASWCSLyapunov-like<br /><br /> <div>
arXiv:2408.15492v2 Announce Type: replace 
Abstract: Heterogeneous systems consisting of a multiloop wireless control system (WCS) and a mobile agent system (MAS) are ubiquitous in Industrial Internet of Things systems. Within these systems, the positions of mobile agents may lead to shadow fading on the wireless channel that the WCS is controlled over and can significantly compromise its performance, requiring joint coordination between the WCS and MAS. Such coordination introduces different time steps and hybrid state spaces consisting of logical components and continuous components. This paper focuses on the infinite-horizon optimal control of MAS to ensure the performance of WCS while minimizing an average cost for the heterogeneous system subject to safety constraints. A state-dependent fading channel is modeled to capture interference among transmission links, as well as the effects of mobile agents' movements on successful wireless transmission. In order to address the heterogeneous system dynamics, the optimal control problem is formulated as the optimal constrained set stabilization of the MAS by establishing a necessary and sufficient condition for the Lyapunov-like performance of WCS with the expected decay rates. Using the semi-tensor product of matrices, a constrained optimal state transition graph is constructed to encode the constrained system dynamics as well as objective function, which further reduces the problem into a minimum-mean cycle problem for the graph. By studying the properties of the graph, the feasibility is proven, and an effective algorithm is proposed for the construction of optimal input sequences. An illustrative example is provided to demonstrate effectiveness of the proposed method.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Rapid Trajectory Optimization and Control Framework for Resource-Constrained Applications</title>
<link>https://arxiv.org/abs/2410.07413</link>
<guid>https://arxiv.org/abs/2410.07413</guid>
<content:encoded><![CDATA[
<div> : model predictive control, integral Chebyshev collocation, autonomous agents, quadratic program, collision avoidance

:
L2 <div>
arXiv:2410.07413v2 Announce Type: replace 
Abstract: This paper presents a computationally efficient model predictive control formulation that uses an integral Chebyshev collocation method to enable rapid operations of autonomous agents. By posing the finite-horizon optimal control problem and recursive re-evaluation of the optimal trajectories, minimization of the L2 norms of the state and control errors are transcribed into a quadratic program. Control and state variable constraints are parameterized using Chebyshev polynomials and are accommodated in the optimal trajectory generation programs to incorporate the actuator limits and keep-out constraints. Differentiable collision detection of polytopes is leveraged for optimal collision avoidance. Results obtained from the collocation methods are benchmarked against the existing approaches on an edge computer to outline the performance improvements. Finally, collaborative control scenarios involving multi-agent space systems are considered to demonstrate the technical merits of the proposed work.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Prompt Internalization</title>
<link>https://arxiv.org/abs/2411.15927</link>
<guid>https://arxiv.org/abs/2411.15927</guid>
<content:encoded><![CDATA[
<div> Generative Prompt Internalization (GenPI)

:
GenPIGenPI  GenPI  GenPI<br /><br /> <div>
arXiv:2411.15927v3 Announce Type: replace 
Abstract: Prompts used in recent large language model based applications are often fixed and lengthy, leading to significant computational overhead. To address this challenge, we propose Generative Prompt Internalization (GenPI), a lightweight method that employs a joint training approach. GenPI not only replicates the behavior of models with prompt inputs but also generates the content of the prompt along with reasons for why the model's behavior should change accordingly. We demonstrate that our approach effectively internalizes complex prompts across various agent-based application scenarios. For effective training without interactions with the dedicated environments, we introduce a data synthesis technique that autonomously collects conversational datasets by swapping the roles of the agent and environment. This method is especially useful in scenarios where only a predefined prompt is available without a corresponding training dataset. By internalizing complex prompts, Generative Prompt Internalization enables high performance and efficient inference without the need for explicit prompts.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM</title>
<link>https://arxiv.org/abs/2501.00599</link>
<guid>https://arxiv.org/abs/2501.00599</guid>
<content:encoded><![CDATA[
<div> Video LLM-VideoRefer Suite

:<br />
VideoRefer SuiteVideo LLM-VideoRefer-700KVideoReferVideoRefer-BenchVideo LLM-VideoReferVideo LLM <div>
arXiv:2501.00599v3 Announce Type: replace 
Abstract: Video Large Language Models (Video LLMs) have recently exhibited remarkable capabilities in general video understanding. However, they mainly focus on holistic comprehension and struggle with capturing fine-grained spatial and temporal details. Besides, the lack of high-quality object-level video instruction data and a comprehensive benchmark further hinders their advancements. To tackle these challenges, we introduce the VideoRefer Suite to empower Video LLM for finer-level spatial-temporal video understanding, i.e., enabling perception and reasoning on any objects throughout the video. Specially, we thoroughly develop VideoRefer Suite across three essential aspects: dataset, model, and benchmark. Firstly, we introduce a multi-agent data engine to meticulously curate a large-scale, high-quality object-level video instruction dataset, termed VideoRefer-700K. Next, we present the VideoRefer model, which equips a versatile spatial-temporal object encoder to capture precise regional and sequential representations. Finally, we meticulously create a VideoRefer-Bench to comprehensively assess the spatial-temporal understanding capability of a Video LLM, evaluating it across various aspects. Extensive experiments and analyses demonstrate that our VideoRefer model not only achieves promising performance on video referring benchmarks but also facilitates general video understanding capabilities.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude Mobility</title>
<link>https://arxiv.org/abs/2501.02341</link>
<guid>https://arxiv.org/abs/2501.02341</guid>
<content:encoded><![CDATA[
<div> (UAV)(LLM)

<br /><br />:

LLMUAVLLMUAVUAVLLMUAVhttps://github.com/Hub-Tian/UAVs_Meet_LLMs <div>
arXiv:2501.02341v2 Announce Type: replace 
Abstract: Low-altitude mobility, exemplified by unmanned aerial vehicles (UAVs), has introduced transformative advancements across various domains, like transportation, logistics, and agriculture. Leveraging flexible perspectives and rapid maneuverability, UAVs extend traditional systems' perception and action capabilities, garnering widespread attention from academia and industry. However, current UAV operations primarily depend on human control, with only limited autonomy in simple scenarios, and lack the intelligence and adaptability needed for more complex environments and tasks. The emergence of large language models (LLMs) demonstrates remarkable problem-solving and generalization capabilities, offering a promising pathway for advancing UAV intelligence. This paper explores the integration of LLMs and UAVs, beginning with an overview of UAV systems' fundamental components and functionalities, followed by an overview of the state-of-the-art in LLM technology. Subsequently, it systematically highlights the multimodal data resources available for UAVs, which provide critical support for training and evaluation. Furthermore, it categorizes and analyzes key tasks and application scenarios where UAVs and LLMs converge. Finally, a reference roadmap towards agentic UAVs is proposed, aiming to enable UAVs to achieve agentic intelligence through autonomous perception, memory, reasoning, and tool utilization. Related resources are available at https://github.com/Hub-Tian/UAVs_Meet_LLMs.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RL-RC-DoT: A Block-level RL agent for Task-Aware Video Compression</title>
<link>https://arxiv.org/abs/2501.12216</link>
<guid>https://arxiv.org/abs/2501.12216</guid>
<content:encoded><![CDATA[
<div> 

:
AIROI <div>
arXiv:2501.12216v2 Announce Type: replace 
Abstract: Video encoders optimize compression for human perception by minimizing reconstruction error under bit-rate constraints. In many modern applications such as autonomous driving, an overwhelming majority of videos serve as input for AI systems performing tasks like object recognition or segmentation, rather than being watched by humans. It is therefore useful to optimize the encoder for a downstream task instead of for perceptual image quality. However, a major challenge is how to combine such downstream optimization with existing standard video encoders, which are highly efficient and popular. Here, we address this challenge by controlling the Quantization Parameters (QPs) at the macro-block level to optimize the downstream task. This granular control allows us to prioritize encoding for task-relevant regions within each frame. We formulate this optimization problem as a Reinforcement Learning (RL) task, where the agent learns to balance long-term implications of choosing QPs on both task performance and bit-rate constraints. Notably, our policy does not require the downstream task as an input during inference, making it suitable for streaming applications and edge devices such as vehicles. We demonstrate significant improvements in two tasks, car detection, and ROI (saliency) encoding. Our approach improves task performance for a given bit rate compared to traditional task agnostic encoding methods, paving the way for more efficient task-aware video compression.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QualityFlow: An Agentic Workflow for Program Synthesis Controlled by LLM Quality Checks</title>
<link>https://arxiv.org/abs/2501.17167</link>
<guid>https://arxiv.org/abs/2501.17167</guid>
<content:encoded><![CDATA[
<div> : QualityFlow

:
QualityFlow QualityFlow LLM LLM QualityFlow MBPPHumanEval  MBPP-EvalPlus  HumanEval-EvalPlus  <div>
arXiv:2501.17167v2 Announce Type: replace 
Abstract: We introduce QualityFlow, a dynamic agentic workflow for program synthesis. Given the English description of a programming problem and a set of unit tests, the model's goal is to synthesize the correct program that solves the problem and passes the tests. QualityFlow includes large language model (LLM) agents resembling a software development team, including code generation, testing, and self-debugging. We propose the LLM Quality Checker, which explicitly "imagines" whether the synthesized programs' execution would conform to the unit tests. The Quality Checks dynamically control the workflow, including actions to submit the final answer, clarify the problem statement, and revert previous workflow steps. Our experiments show that the Quality Checker can precisely accept any correct program, mitigate faulty synthesized tests, and prevent potential workflow deviation. QualityFlow establishes the state-of-the-art results on four program synthesis benchmarks: MBPP, HumanEval, and stricter evaluations from MBPP-EvalPlus and HumanEval-EvalPlus.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reanimating Images using Neural Representations of Dynamic Stimuli</title>
<link>https://arxiv.org/abs/2406.02659</link>
<guid>https://arxiv.org/abs/2406.02659</guid>
<content:encoded><![CDATA[
<div> : BrainNRDSfMRI

<br /><br />:
BrainNRDSfMRI

1. 
2. 
3. 
4. 

https://brain-nrds.github.io/ <div>
arXiv:2406.02659v3 Announce Type: replace-cross 
Abstract: While computer vision models have made incredible strides in static image recognition, they still do not match human performance in tasks that require the understanding of complex, dynamic motion. This is notably true for real-world scenarios where embodied agents face complex and motion-rich environments. Our approach, BrainNRDS (Brain-Neural Representations of Dynamic Stimuli), leverages state-of-the-art video diffusion models to decouple static image representation from motion generation, enabling us to utilize fMRI brain activity for a deeper understanding of human responses to dynamic visual stimuli. Conversely, we also demonstrate that information about the brain's representation of motion can enhance the prediction of optical flow in artificial systems. Our novel approach leads to four main findings: (1) Visual motion, represented as fine-grained, object-level resolution optical flow, can be decoded from brain activity generated by participants viewing video stimuli; (2) Video encoders outperform image-based models in predicting video-driven brain activity; (3) Brain-decoded motion signals enable realistic video reanimation based only on the initial frame of the video; and (4) We extend prior work to achieve full video decoding from video-driven brain activity. BrainNRDS advances our understanding of how the brain represents spatial and temporal information in dynamic visual scenes. Our findings demonstrate the potential of combining brain imaging with video diffusion models for developing more robust and biologically-inspired computer vision systems. We show additional decoding and encoding examples on this site: https://brain-nrds.github.io/.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SFO: Piloting VLM Feedback for Offline RL</title>
<link>https://arxiv.org/abs/2503.01062</link>
<guid>https://arxiv.org/abs/2503.01062</guid>
<content:encoded><![CDATA[
<div> Vision-Language (VLM)(RL)Reinforcement Learning from AI Feedback (RLAIF)RL

:
Vision-LanguageVLMRLRLRLVLMVLM <div>
arXiv:2503.01062v3 Announce Type: replace 
Abstract: While internet-scale image and textual data have enabled strong generalization in Vision-Language Models (VLMs), the absence of internet-scale control data has impeded the development of similar generalization in standard reinforcement learning (RL) agents. Although VLMs are fundamentally limited in their ability to solve control tasks due to their lack of action-conditioned training data, their capacity for image understanding allows them to provide valuable feedback in RL tasks by recognizing successful outcomes. A key challenge in Reinforcement Learning from AI Feedback (RLAIF) is determining how best to integrate VLM-derived signals into the learning process. We explore this question in the context of offline RL and introduce a class of methods called sub-trajectory filtered optimization. We identify three key insights. First, trajectory length plays a crucial role in offline RL, as full-trajectory preference learning exacerbates the stitching problem, necessitating the use of sub-trajectories. Second, even in Markovian environments, a non-Markovian reward signal from a sequence of images is required to assess trajectory improvement, as VLMs do not interpret control actions and must rely on visual cues over time. Third, a simple yet effective approach--filtered and weighted behavior cloning--consistently outperforms more complex reinforcement learning from human feedback-based methods. We propose sub-trajectory filtered behavior cloning, a method that leverages VLM feedback on sub-trajectories while incorporating a retrospective filtering mechanism that removes sub-trajectories preceding failures to improve robustness and prevent turbulence. This study is preliminary; we provide initial evidence through evaluations on a toy control domain. Please enjoy our airport puns.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large language model-powered AI systems achieve self-replication with no human intervention</title>
<link>https://arxiv.org/abs/2503.17378</link>
<guid>https://arxiv.org/abs/2503.17378</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />
11/32140AI <div>
arXiv:2503.17378v1 Announce Type: new 
Abstract: Self-replication with no human intervention is broadly recognized as one of the principal red lines associated with frontier AI systems. While leading corporations such as OpenAI and Google DeepMind have assessed GPT-o3-mini and Gemini on replication-related tasks and concluded that these systems pose a minimal risk regarding self-replication, our research presents novel findings. Following the same evaluation protocol, we demonstrate that 11 out of 32 existing AI systems under evaluation already possess the capability of self-replication. In hundreds of experimental trials, we observe a non-trivial number of successful self-replication trials across mainstream model families worldwide, even including those with as small as 14 billion parameters which can run on personal computers. Furthermore, we note the increase in self-replication capability when the model becomes more intelligent in general. Also, by analyzing the behavioral traces of diverse AI systems, we observe that existing AI systems already exhibit sufficient planning, problem-solving, and creative capabilities to accomplish complex agentic tasks including self-replication. More alarmingly, we observe successful cases where an AI system do self-exfiltration without explicit instructions, adapt to harsher computational environments without sufficient software or hardware supports, and plot effective strategies to survive against the shutdown command from the human beings. These novel findings offer a crucial time buffer for the international community to collaborate on establishing effective governance over the self-replication capabilities and behaviors of frontier AI systems, which could otherwise pose existential risks to the human society if not well-controlled.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Value Function Estimation Under Model Mismatch: A Federated Temporal Difference Analysis</title>
<link>https://arxiv.org/abs/2503.17454</link>
<guid>https://arxiv.org/abs/2503.17454</guid>
<content:encoded><![CDATA[
<div> (FedRL)agent(TD(0))TD(0)(FedTD(0))

:

(FedRL)agent(TD(0))i.i.d.MarkovianTD(0)FedTD(0)FedTD(0) <div>
arXiv:2503.17454v1 Announce Type: new 
Abstract: Federated reinforcement learning (FedRL) enables collaborative learning while preserving data privacy by preventing direct data exchange between agents. However, many existing FedRL algorithms assume that all agents operate in identical environments, which is often unrealistic. In real-world applications -- such as multi-robot teams, crowdsourced systems, and large-scale sensor networks -- each agent may experience slightly different transition dynamics, leading to inherent model mismatches. In this paper, we first establish linear convergence guarantees for single-agent temporal difference learning (TD(0)) in policy evaluation and demonstrate that under a perturbed environment, the agent suffers a systematic bias that prevents accurate estimation of the true value function. This result holds under both i.i.d. and Markovian sampling regimes. We then extend our analysis to the federated TD(0) (FedTD(0)) setting, where multiple agents -- each interacting with its own perturbed environment -- periodically share value estimates to collaboratively approximate the true value function of a common underlying model. Our theoretical results indicate the impact of model mismatch, network connectivity, and mixing behavior on the convergence of FedTD(0). Empirical experiments corroborate our theoretical gains, highlighting that even moderate levels of information sharing can significantly mitigate environment-specific errors.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConvoGen: Enhancing Conversational AI with Synthetic Data: A Multi-Agent Approach</title>
<link>https://arxiv.org/abs/2503.17460</link>
<guid>https://arxiv.org/abs/2503.17460</guid>
<content:encoded><![CDATA[
<div> : ConvoGenAI

:<br />
ConvoGenAIAI <div>
arXiv:2503.17460v1 Announce Type: new 
Abstract: In this paper, we present ConvoGen: an innovative framework for generating synthetic conversational data using multi-agent systems. Our method leverages few-shot learning and introduces iterative sampling from a dynamically updated few-shot hub to create diverse and realistic conversational scenarios. The generated data has numerous applications, including training and evaluating conversational AI models, and augmenting existing datasets for tasks like conversational intent classification or conversation summarization. Our experiments demonstrate the effectiveness of this method in producing high-quality diverse synthetic conversational data, highlighting its potential to enhance the development and evaluation of conversational AI systems.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Follow-up Question Generation For Enhanced Patient-Provider Conversations</title>
<link>https://arxiv.org/abs/2503.17509</link>
<guid>https://arxiv.org/abs/2503.17509</guid>
<content:encoded><![CDATA[
<div> FollowupQEHR

:<br />
FollowupQEHRFollowupQ34%17%5%EHR2,300NLP <div>
arXiv:2503.17509v1 Announce Type: new 
Abstract: Follow-up question generation is an essential feature of dialogue systems as it can reduce conversational ambiguity and enhance modeling complex interactions. Conversational contexts often pose core NLP challenges such as (i) extracting relevant information buried in fragmented data sources, and (ii) modeling parallel thought processes. These two challenges occur frequently in medical dialogue as a doctor asks questions based not only on patient utterances but also their prior EHR data and current diagnostic hypotheses. Asking medical questions in asynchronous conversations compounds these issues as doctors can only rely on static EHR information to motivate follow-up questions.
  To address these challenges, we introduce FollowupQ, a novel framework for enhancing asynchronous medical conversation. FollowupQ is a multi-agent framework that processes patient messages and EHR data to generate personalized follow-up questions, clarifying patient-reported medical conditions. FollowupQ reduces requisite provider follow-up communications by 34%. It also improves performance by 17% and 5% on real and synthetic data, respectively. We also release the first public dataset of asynchronous medical messages with linked EHR data alongside 2,300 follow-up questions written by clinical experts for the wider NLP research community.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2503.17523</link>
<guid>https://arxiv.org/abs/2503.17523</guid>
<content:encoded><![CDATA[
<div> : 

:
LLMLLMLLMLLMLLM<br /><br /> <div>
arXiv:2503.17523v1 Announce Type: new 
Abstract: Artificial intelligence systems based on large language models (LLMs) are increasingly used as agents that interact with users and with the world. To do so successfully, LLMs need to construct internal representations of the world and form probabilistic beliefs about those representations. To provide a user with personalized recommendations, for example, the LLM needs to gradually infer the user's preferences, over the course of multiple interactions. To evaluate whether contemporary LLMs are able to do so, we use the Bayesian inference framework from probability theory, which lays out the optimal way to update an agent's beliefs as it receives new information. We first show that the LLMs do not update their beliefs as expected from the Bayesian framework, and that consequently their predictions do not improve as expected as more information becomes available, even less so than we find is the case for humans. To address this issue, we teach the LLMs to reason in a Bayesian manner by training them to mimic the predictions of an optimal Bayesian model. We find that this approach not only significantly improves the LLM's performance on the particular recommendation task it is trained on, but also enables generalization to other tasks. This suggests that this method endows the LLM with broader Bayesian reasoning skills. More generally, our results indicate that LLMs can learn about reasoning strategies effectively and generalize those skills to new domains, which in part explains LLMs' empirical success.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniScience: A Domain-Specialized LLM for Scientific Reasoning and Discovery</title>
<link>https://arxiv.org/abs/2503.17604</link>
<guid>https://arxiv.org/abs/2503.17604</guid>
<content:encoded><![CDATA[
<div> :  (LLMs), OmniScience, , , 

<br /><br />:
OmniScienceOmniScienceGPQA Diamond <div>
arXiv:2503.17604v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable potential in advancing scientific knowledge and addressing complex challenges. In this work, we introduce OmniScience, a specialized large reasoning model for general science, developed through three key components: (1) domain adaptive pretraining on a carefully curated corpus of scientific literature, (2) instruction tuning on a specialized dataset to guide the model in following domain-specific tasks, and (3) reasoning-based knowledge distillation through fine-tuning to significantly enhance its ability to generate contextually relevant and logically sound responses. We demonstrate the versatility of OmniScience by developing a battery agent that efficiently ranks molecules as potential electrolyte solvents or additives. Comprehensive evaluations reveal that OmniScience is competitive with state-of-the-art large reasoning models on the GPQA Diamond and domain-specific battery benchmarks, while outperforming all public reasoning and non-reasoning models with similar parameter counts. We further demonstrate via ablation experiments that domain adaptive pretraining and reasoning-based knowledge distillation are critical to attain our performance levels, across benchmarks.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planning and Learning in Average Risk-aware MDPs</title>
<link>https://arxiv.org/abs/2503.17629</link>
<guid>https://arxiv.org/abs/2503.17629</guid>
<content:encoded><![CDATA[
<div> : Q

<br /><br />:
(RVI)QRVIMLMCQ <div>
arXiv:2503.17629v1 Announce Type: new 
Abstract: For continuing tasks, average cost Markov decision processes have well-documented value and can be solved using efficient algorithms. However, it explicitly assumes that the agent is risk-neutral. In this work, we extend risk-neutral algorithms to accommodate the more general class of dynamic risk measures. Specifically, we propose a relative value iteration (RVI) algorithm for planning and design two model-free Q-learning algorithms, namely a generic algorithm based on the multi-level Monte Carlo method, and an off-policy algorithm dedicated to utility-base shortfall risk measures. Both the RVI and MLMC-based Q-learning algorithms are proven to converge to optimality. Numerical experiments validate our analysis, confirms empirically the convergence of the off-policy algorithm, and demonstrate that our approach enables the identification of policies that are finely tuned to the intricate risk-awareness of the agent that they serve.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time- and Space-Optimal Silent Self-Stabilizing Exact Majority in Population Protocols</title>
<link>https://arxiv.org/abs/2503.17652</link>
<guid>https://arxiv.org/abs/2503.17652</guid>
<content:encoded><![CDATA[
<div> self-stabilizing exact majority problempopulation protocol modelimpossibilitysilent protocoltime- and space-optimal

:
$n$(AB)$n$$O(n)$$O(n \log n)$$O(n)$$n$$\Omega(n)$$\Omega(n)$$\Omega(n \log n)$<br /><br /> <div>
arXiv:2503.17652v1 Announce Type: new 
Abstract: We address the self-stabilizing exact majority problem in the population protocol model, introduced by Angluin, Aspnes, Diamadi, Fischer, and Peralta (2004). In this model, there are $n$ state machines, called agents, which form a network. At each time step, only two agents interact with each other, and update their states. In the self-stabilizing exact majority problem, each agent has a fixed opinion, $\mathtt{A}$ or $\mathtt{B}$, and stabilizes to a safe configuration in which all agents output the majority opinion from any initial configuration.
  In this paper, we show the impossibility of solving the self-stabilizing exact majority problem without knowledge of $n$ in any protocol. We propose a silent self-stabilizing exact majority protocol, which stabilizes within $O(n)$ parallel time in expectation and within $O(n \log n)$ parallel time with high probability, using $O(n)$ states, with knowledge of $n$. Here, a silent protocol means that, after stabilization, the state of each agent does not change. We establish lower bounds, proving that any silent protocol requires $\Omega(n)$ states, $\Omega(n)$ parallel time in expectation, and $\Omega(n \log n)$ parallel time with high probability to reach a safe configuration. Thus, the proposed protocol is time- and space-optimal.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ComfyGPT: A Self-Optimizing Multi-Agent System for Comprehensive ComfyUI Workflow Generation</title>
<link>https://arxiv.org/abs/2503.17671</link>
<guid>https://arxiv.org/abs/2503.17671</guid>
<content:encoded><![CDATA[
<div> ComfyUIComfyGPTFlowAgent

:
ComfyGPTComfyUIComfyGPTReformatAgentFlowAgentRefineAgentExecuteAgentFlowAgentSFTRLFlowDataset13,571-FlowBenchFVPAPIAPNDComfyGPT <div>
arXiv:2503.17671v1 Announce Type: new 
Abstract: ComfyUI provides a widely-adopted, workflow-based interface that enables users to customize various image generation tasks through an intuitive node-based architecture. However, the intricate connections between nodes and diverse modules often present a steep learning curve for users. In this paper, we introduce ComfyGPT, the first self-optimizing multi-agent system designed to generate ComfyUI workflows based on task descriptions automatically. ComfyGPT comprises four specialized agents: ReformatAgent, FlowAgent, RefineAgent, and ExecuteAgent. The core innovation of ComfyGPT lies in two key aspects. First, it focuses on generating individual node links rather than entire workflows, significantly improving generation precision. Second, we proposed FlowAgent, a LLM-based workflow generation agent that uses both supervised fine-tuning (SFT) and reinforcement learning (RL) to improve workflow generation accuracy. Moreover, we introduce FlowDataset, a large-scale dataset containing 13,571 workflow-description pairs, and FlowBench, a comprehensive benchmark for evaluating workflow generation systems. We also propose four novel evaluation metrics: Format Validation (FV), Pass Accuracy (PA), Pass Instruct Alignment (PIA), and Pass Node Diversity (PND). Experimental results demonstrate that ComfyGPT significantly outperforms existing LLM-based methods in workflow generation.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Automate Fact-Checking Article Writing?</title>
<link>https://arxiv.org/abs/2503.17684</link>
<guid>https://arxiv.org/abs/2503.17684</guid>
<content:encoded><![CDATA[
<div> : QRAFT

:
QRAFTLLMQRAFTQRAFT <div>
arXiv:2503.17684v1 Announce Type: new 
Abstract: Automatic fact-checking aims to support professional fact-checkers by offering tools that can help speed up manual fact-checking. Yet, existing frameworks fail to address the key step of producing output suitable for broader dissemination to the general public: while human fact-checkers communicate their findings through fact-checking articles, automated systems typically produce little or no justification for their assessments. Here, we aim to bridge this gap. We argue for the need to extend the typical automatic fact-checking pipeline with automatic generation of full fact-checking articles. We first identify key desiderata for such articles through a series of interviews with experts from leading fact-checking organizations. We then develop QRAFT, an LLM-based agentic framework that mimics the writing workflow of human fact-checkers. Finally, we assess the practical usefulness of QRAFT through human evaluations with professional fact-checkers. Our evaluation shows that while QRAFT outperforms several previously proposed text-generation approaches, it lags considerably behind expert-written articles. We hope that our work will enable further research in this new and important direction.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAIDER: Tool-Equipped Large Language Model Agent for Robotic Action Issue Detection, Explanation and Recovery</title>
<link>https://arxiv.org/abs/2503.17703</link>
<guid>https://arxiv.org/abs/2503.17703</guid>
<content:encoded><![CDATA[
<div> RAIDER

:<br />
RAIDERLLMsRAIDERRAIDERRAIDERRAIDERAIhttps://raider-llmagent.github.io/ <div>
arXiv:2503.17703v1 Announce Type: new 
Abstract: As robots increasingly operate in dynamic human-centric environments, improving their ability to detect, explain, and recover from action-related issues becomes crucial. Traditional model-based and data-driven techniques lack adaptability, while more flexible generative AI methods struggle with grounding extracted information to real-world constraints. We introduce RAIDER, a novel agent that integrates Large Language Models (LLMs) with grounded tools for adaptable and efficient issue detection and explanation. Using a unique "Ground, Ask& Answer, Issue" procedure, RAIDER dynamically generates context-aware precondition questions and selects appropriate tools for resolution, achieving targeted information gathering. Our results within a simulated household environment surpass methods relying on predefined models, full scene descriptions, or standalone trained models. Additionally, RAIDER's explanations enhance recovery success, including cases requiring human interaction. Its modular architecture, featuring self-correction mechanisms, enables straightforward adaptation to diverse scenarios, as demonstrated in a real-world human-assistive task. This showcases RAIDER's potential as a versatile agentic AI solution for robotic issue detection and explanation, while addressing the problem of grounding generative AI for its effective application in embodied agents. Project website: https://raider-llmagent.github.io/
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUI-Xplore: Empowering Generalizable GUI Agents with One Exploration</title>
<link>https://arxiv.org/abs/2503.17709</link>
<guid>https://arxiv.org/abs/2503.17709</guid>
<content:encoded><![CDATA[
<div> : GUIGUI-XploreXplore-Agent

:
GUIGUI-XploreGUIXplore-AgentGUIGUI10%GUI <div>
arXiv:2503.17709v1 Announce Type: new 
Abstract: GUI agents hold significant potential to enhance the experience and efficiency of human-device interaction. However, current methods face challenges in generalizing across applications (apps) and tasks, primarily due to two fundamental limitations in existing datasets. First, these datasets overlook developer-induced structural variations among apps, limiting the transferability of knowledge across diverse software environments. Second, many of them focus solely on navigation tasks, which restricts their capacity to represent comprehensive software architectures and complex user interactions. To address these challenges, we introduce GUI-Xplore, a dataset meticulously designed to enhance cross-application and cross-task generalization via an exploration-and-reasoning framework. GUI-Xplore integrates pre-recorded exploration videos providing contextual insights, alongside five hierarchically structured downstream tasks designed to comprehensively evaluate GUI agent capabilities. To fully exploit GUI-Xplore's unique features, we propose Xplore-Agent, a GUI agent framework that combines Action-aware GUI Modeling with Graph-Guided Environment Reasoning. Further experiments indicate that Xplore-Agent achieves a 10% improvement over existing methods in unfamiliar environments, yet there remains significant potential for further enhancement towards truly generalizable GUI agents.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Mathematical Reasoning and Optimization with Large Language Models</title>
<link>https://arxiv.org/abs/2503.17726</link>
<guid>https://arxiv.org/abs/2503.17726</guid>
<content:encoded><![CDATA[
<div> Transformer

<br /><br />:
LLMsTransformerLLMsLLMsChain-of-ThoughtLLMsAILLMs <div>
arXiv:2503.17726v1 Announce Type: new 
Abstract: Mathematical reasoning and optimization are fundamental to artificial intelligence and computational problem-solving. Recent advancements in Large Language Models (LLMs) have significantly improved AI-driven mathematical reasoning, theorem proving, and optimization techniques. This survey explores the evolution of mathematical problem-solving in AI, from early statistical learning approaches to modern deep learning and transformer-based methodologies. We review the capabilities of pretrained language models and LLMs in performing arithmetic operations, complex reasoning, theorem proving, and structured symbolic computation. A key focus is on how LLMs integrate with optimization and control frameworks, including mixed-integer programming, linear quadratic control, and multi-agent optimization strategies. We examine how LLMs assist in problem formulation, constraint generation, and heuristic search, bridging theoretical reasoning with practical applications. We also discuss enhancement techniques such as Chain-of-Thought reasoning, instruction tuning, and tool-augmented methods that improve LLM's problem-solving performance. Despite their progress, LLMs face challenges in numerical precision, logical consistency, and proof verification. Emerging trends such as hybrid neural-symbolic reasoning, structured prompt engineering, and multi-step self-correction aim to overcome these limitations. Future research should focus on interpretability, integration with domain-specific solvers, and improving the robustness of AI-driven decision-making. This survey offers a comprehensive review of the current landscape and future directions of mathematical reasoning and optimization with LLMs, with applications across engineering, finance, and scientific research.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building Resource-Constrained Language Agents: A Korean Case Study on Chemical Toxicity Information</title>
<link>https://arxiv.org/abs/2503.17753</link>
<guid>https://arxiv.org/abs/2503.17753</guid>
<content:encoded><![CDATA[
<div> : 

:<br />
Tox-chat8B <div>
arXiv:2503.17753v1 Announce Type: new 
Abstract: Language agents powered by large language models (LLMs) face significant deployment challenges in resource-constrained environments, particularly for specialized domains and less-common languages. This paper presents Tox-chat, a Korean chemical toxicity information agent devised within these limitations. We propose two key innovations: a context-efficient architecture that reduces token consumption through hierarchical section search, and a scenario-based dialogue generation methodology that effectively distills tool-using capabilities from larger models. Experimental evaluations demonstrate that our fine-tuned 8B parameter model substantially outperforms both untuned models and baseline approaches, in terms of DB faithfulness and preference. Our work offers valuable insights for researchers developing domain-specific language agents under practical constraints.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bandwidth Reservation for Time-Critical Vehicular Applications: A Multi-Operator Environment</title>
<link>https://arxiv.org/abs/2503.17756</link>
<guid>https://arxiv.org/abs/2503.17756</guid>
<content:encoded><![CDATA[
<div> : arXiv onsite bandwidth reservation Multi-Mobile Network Operator(MNO) Deep Reinforcement Learning(DRL) Temporal Fusion Transformer(TFT)

:
DRLDueling Deep Q-LearningMNOMDPTemporal Fusion TransformerTFTspot40% <div>
arXiv:2503.17756v1 Announce Type: new 
Abstract: Onsite bandwidth reservation requests often face challenges such as price fluctuations and fairness issues due to unpredictable bandwidth availability and stringent latency requirements. Requesting bandwidth in advance can mitigate the impact of these fluctuations and ensure timely access to critical resources. In a multi-Mobile Network Operator (MNO) environment, vehicles need to select cost-effective and reliable resources for their safety-critical applications. This research aims to minimize resource costs by finding the best price among multiple MNOs. It formulates multi-operator scenarios as a Markov Decision Process (MDP), utilizing a Deep Reinforcement Learning (DRL) algorithm, specifically Dueling Deep Q-Learning. For efficient and stable learning, we propose a novel area-wise approach and an adaptive MDP synthetic close to the real environment. The Temporal Fusion Transformer (TFT) is used to handle time-dependent data and model training. Furthermore, the research leverages Amazon spot price data and adopts a multi-phase training approach, involving initial training on synthetic data, followed by real-world data. These phases enable the DRL agent to make informed decisions using insights from historical data and real-time observations. The results show that our model leads to significant cost reductions, up to 40%, compared to scenarios without a policy model in such a complex environment.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lifelong Evolution of Swarms</title>
<link>https://arxiv.org/abs/2503.17763</link>
<guid>https://arxiv.org/abs/2503.17763</guid>
<content:encoded><![CDATA[
<div> lifelong learningswarm controllersevolutionary frameworkcatastrophic forgettingregularization

<br /><br />:
 <div>
arXiv:2503.17763v1 Announce Type: new 
Abstract: Adapting to task changes without forgetting previous knowledge is a key skill for intelligent systems, and a crucial aspect of lifelong learning. Swarm controllers, however, are typically designed for specific tasks, lacking the ability to retain knowledge across changing tasks. Lifelong learning, on the other hand, focuses on individual agents with limited insights into the emergent abilities of a collective like a swarm. To address this gap, we introduce a lifelong evolutionary framework for swarms, where a population of swarm controllers is evolved in a dynamic environment that incrementally presents novel tasks. This requires evolution to find controllers that quickly adapt to new tasks while retaining knowledge of previous ones, as they may reappear in the future. We discover that the population inherently preserves information about previous tasks, and it can reuse it to foster adaptation and mitigate forgetting. In contrast, the top-performing individual for a given task catastrophically forgets previous tasks. To mitigate this phenomenon, we design a regularization process for the evolutionary algorithm, reducing forgetting in top-performing individuals. Evolving swarms in a lifelong fashion raises fundamental questions on the current state of deep lifelong learning and on the robustness of swarm controllers in dynamic environments.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why do Opinions and Actions Diverge? A Dynamic Framework to Explore the Impact of Subjective Norms</title>
<link>https://arxiv.org/abs/2503.17768</link>
<guid>https://arxiv.org/abs/2503.17768</guid>
<content:encoded><![CDATA[
<div> agent-based

:
agent-basedHegselmann-Krause- <div>
arXiv:2503.17768v1 Announce Type: new 
Abstract: Socio-psychological studies have identified a common phenomenon where an individual's public actions do not necessarily coincide with their private opinions, yet most existing models fail to capture the dynamic interplay between these two aspects. To bridge this gap, we propose a novel agent-based modeling framework that integrates opinion dynamics with a decision-making mechanism. More precisely, our framework generalizes the classical Hegselmann-Krause model by combining it with a utility maximization problem. Preliminary results from our model demonstrate that the degree of opinion-action divergence within a population can be effectively controlled by adjusting two key parameters that reflect agents' personality traits, while the presence of social network amplifies the divergence. In addition, we study the social diffusion process by introducing a small number of committed agents into the model, and identify three key outcomes: adoption of innovation, rejection of innovation, and the enforcement of unpopular norms, consistent with findings in socio-psychological literature. The strong relevance of the results to real-world phenomena highlights our framework's potential for future applications in understanding and predicting complex social behaviors.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Roadmap Towards Improving Multi-Agent Reinforcement Learning With Causal Discovery And Inference</title>
<link>https://arxiv.org/abs/2503.17803</link>
<guid>https://arxiv.org/abs/2503.17803</guid>
<content:encoded><![CDATA[
<div>  (Reinforcement Learning, RL) (Causal Reasoning) (Multi-Agent Reinforcement Learning, MARL)

:

MARLMARLMARL <div>
arXiv:2503.17803v1 Announce Type: new 
Abstract: Causal reasoning is increasingly used in Reinforcement Learning (RL) to improve the learning process in several dimensions: efficacy of learned policies, efficiency of convergence, generalisation capabilities, safety and interpretability of behaviour. However, applications of causal reasoning to Multi-Agent RL (MARL) are still mostly unexplored. In this paper, we take the first step in investigating the opportunities and challenges of applying causal reasoning in MARL. We measure the impact of a simple form of causal augmentation in state-of-the-art MARL scenarios increasingly requiring cooperation, and with state-of-the-art MARL algorithms exploiting various degrees of collaboration between agents. Then, we discuss the positive as well as negative results achieved, giving us the chance to outline the areas where further research may help to successfully transfer causal RL to the multi-agent setting.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OvercookedV2: Rethinking Overcooked for Zero-Shot Coordination</title>
<link>https://arxiv.org/abs/2503.17821</link>
<guid>https://arxiv.org/abs/2503.17821</guid>
<content:encoded><![CDATA[
<div> : AIOvercookedOvercookedV2

<br /><br />:
AIZSCOvercookedZSCOvercookedZSCOvercookedZSCOvercookedV2ZSCOvercookedV2OvercookedV2ZSCAI <div>
arXiv:2503.17821v1 Announce Type: new 
Abstract: AI agents hold the potential to transform everyday life by helping humans achieve their goals. To do this successfully, agents need to be able to coordinate with novel partners without prior interaction, a setting known as zero-shot coordination (ZSC). Overcooked has become one of the most popular benchmarks for evaluating coordination capabilities of AI agents and learning algorithms. In this work, we investigate the origins of ZSC challenges in Overcooked. We introduce a state augmentation mechanism which mixes states that might be encountered when paired with unknown partners into the training distribution, reducing the out-of-distribution challenge associated with ZSC. We show that independently trained agents under this algorithm coordinate successfully in Overcooked. Our results suggest that ZSC failure can largely be attributed to poor state coverage under self-play rather than more sophisticated coordination challenges. The Overcooked environment is therefore not suitable as a ZSC benchmark. To address these shortcomings, we introduce OvercookedV2, a new version of the benchmark, which includes asymmetric information and stochasticity, facilitating the creation of interesting ZSC scenarios. To validate OvercookedV2, we conduct experiments demonstrating that mere exhaustive state coverage is insufficient to coordinate well. Finally, we use OvercookedV2 to build a new range of coordination challenges, including ones that require test time protocol formation, and we demonstrate the need for new coordination algorithms that can adapt online. We hope that OvercookedV2 will help benchmark the next generation of ZSC algorithms and advance collaboration between AI agents and humans.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metacognition in Content-Centric Computational Cognitive C4 Modeling</title>
<link>https://arxiv.org/abs/2503.17822</link>
<guid>https://arxiv.org/abs/2503.17822</guid>
<content:encoded><![CDATA[
<div> AIRPI LEIA

:<br />
AIC4RPI LEIAC4LEIALLM <div>
arXiv:2503.17822v1 Announce Type: new 
Abstract: For AI agents to emulate human behavior, they must be able to perceive, meaningfully interpret, store, and use large amounts of information about the world, themselves, and other agents. Metacognition is a necessary component of all of these processes. In this paper, we briefly a) introduce content-centric computational cognitive (C4) modeling for next-generation AI agents; b) review the long history of developing C4 agents at RPI's LEIA (Language-Endowed Intelligent Agents) Lab; c) discuss our current work on extending LEIAs' cognitive capabilities to cognitive robotic applications developed using a neuro symbolic processing model; and d) sketch plans for future developments in this paradigm that aim to overcome underappreciated limitations of currently popular, LLM-driven methods in AI.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CP-AgentNet: Autonomous and Explainable Communication Protocol Design Using Generative Agents</title>
<link>https://arxiv.org/abs/2503.17850</link>
<guid>https://arxiv.org/abs/2503.17850</guid>
<content:encoded><![CDATA[
<div> : DRLCP-AgentNet<br /><br />:<br />
CP-AgentNetDRLCP-AgentNetLLMACPTCPCP-AgentTCPLLMACPTCP <div>
arXiv:2503.17850v1 Announce Type: new 
Abstract: Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are crucial for achieving desired performance levels, requiring domain expertise. 2) The decision-making process in DRL models is often opaque, commonly described as a 'black box.' 3) DRL models are data hungry. In response, we propose CP-AgentNet, the first framework designed to use generative agents for developing communication network protocols. This approach addresses these challenges by creating an autonomous system for protocol design, significantly reducing human effort. We developed LLMA (LLM-agents-based multiple access) and CPTCP (CP-Agent-based TCP) for heterogeneous environments. Our comprehensive simulations have demonstrated the efficient coexistence of LLMA and CPTCP with nodes using different types of protocols, as well as enhanced explainability.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Study of the Role of Incompleteness and Ambiguity in Interactions with Large Language Models</title>
<link>https://arxiv.org/abs/2503.17936</link>
<guid>https://arxiv.org/abs/2503.17936</guid>
<content:encoded><![CDATA[
<div> : 

:
 <div>
arXiv:2503.17936v1 Announce Type: new 
Abstract: Natural language as a medium for human-computer interaction has long been anticipated, has been undergoing a sea-change with the advent of Large Language Models (LLMs) with startling capacities for processing and generating language. Many of us now treat LLMs as modern-day oracles, asking it almost any kind of question. Unlike its Delphic predecessor, consulting an LLM does not have to be a single-turn activity (ask a question, receive an answer, leave); and -- also unlike the Pythia -- it is widely acknowledged that answers from LLMs can be improved with additional context. In this paper, we aim to study when we need multi-turn interactions with LLMs to successfully get a question answered; or conclude that a question is unanswerable. We present a neural symbolic framework that models the interactions between human and LLM agents. Through the proposed framework, we define incompleteness and ambiguity in the questions as properties deducible from the messages exchanged in the interaction, and provide results from benchmark problems, in which the answer-correctness is shown to depend on whether or not questions demonstrate the presence of incompleteness or ambiguity (according to the properties we identify). Our results show multi-turn interactions are usually required for datasets which have a high proportion of incompleteness or ambiguous questions; and that that increasing interaction length has the effect of reducing incompleteness or ambiguity. The results also suggest that our measures of incompleteness and ambiguity can be useful tools for characterising interactions with an LLM on question-answeringproblems
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Won: Establishing Best Practices for Korean Financial NLP</title>
<link>https://arxiv.org/abs/2503.17963</link>
<guid>https://arxiv.org/abs/2503.17963</guid>
<content:encoded><![CDATA[
<div> Korean large language modelsfinanceleaderboardopen instruction datasetWon

:<br />
MCQAQA1,11980kWon <div>
arXiv:2503.17963v1 Announce Type: new 
Abstract: In this work, we present the first open leaderboard for evaluating Korean large language models focused on finance. Operated for about eight weeks, the leaderboard evaluated 1,119 submissions on a closed benchmark covering five MCQA categories: finance and accounting, stock price prediction, domestic company analysis, financial markets, and financial agent tasks and one open-ended qa task. Building on insights from these evaluations, we release an open instruction dataset of 80k instances and summarize widely used training strategies observed among top-performing models. Finally, we introduce Won, a fully open and transparent LLM built using these best practices. We hope our contributions help advance the development of better and safer financial LLMs for Korean and other languages.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metaphor-based Jailbreaking Attacks on Text-to-Image Models</title>
<link>https://arxiv.org/abs/2503.17987</link>
<guid>https://arxiv.org/abs/2503.17987</guid>
<content:encoded><![CDATA[
<div> 

:<br />
T2IMJATabooMJALLMMLAGAPOMLAGLLMAPOMJAT2I <div>
arXiv:2503.17987v1 Announce Type: new 
Abstract: To mitigate misuse, text-to-image~(T2I) models commonly incorporate safety filters to prevent the generation of sensitive images. Unfortunately, recent jailbreaking attack methods use LLMs to generate adversarial prompts that effectively bypass safety filters while generating sensitive images, revealing the safety vulnerabilities within the T2I model. However, existing LLM-based attack methods lack explicit guidance, relying on substantial queries to achieve a successful attack, which limits their practicality in real-world scenarios. In this work, we introduce \textbf{MJA}, a \textbf{m}etaphor-based \textbf{j}ailbreaking \textbf{a}ttack method inspired by the Taboo game, aiming to balance the attack effectiveness and query efficiency by generating metaphor-based adversarial prompts. Specifically, MJA consists of two modules: an LLM-based multi-agent generation module~(MLAG) and an adversarial prompt optimization module~(APO). MLAG decomposes the generation of metaphor-based adversarial prompts into three subtasks: metaphor retrieval, context matching, and adversarial prompt generation. Subsequently, MLAG coordinates three LLM-based agents to generate diverse adversarial prompts by exploring various metaphors and contexts. To enhance the attack efficiency, APO first trains a surrogate model to predict the attack results of adversarial prompts and then designs an acquisition strategy to adaptively identify optimal adversarial prompts. Experiments demonstrate that MJA achieves better attack effectiveness while requiring fewer queries compared to baseline methods. Moreover, our adversarial prompts exhibit strong transferability across various open-source and commercial T2I models. \textcolor{red}{This paper includes model-generated content that may contain offensive or distressing material.}
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation</title>
<link>https://arxiv.org/abs/2503.18065</link>
<guid>https://arxiv.org/abs/2503.18065</guid>
<content:encoded><![CDATA[
<div> 

:
Rewriting-driven Augmentation (RAM)(VLN)-RAMObject-Enriched Observation Rewriting(VLMs)(LLMs)(T2IMs)Observation-Contrast Instruction RewritingR2RREVERIER4RR2R-CEhttps://github.com/SaDil13/VLN-RAM <br /><br /> <div>
arXiv:2503.18065v1 Announce Type: new 
Abstract: Data scarcity is a long-standing challenge in the Vision-Language Navigation (VLN) field, which extremely hinders the generalization of agents to unseen environments. Previous works primarily rely on additional simulator data or web-collected images/videos to improve the generalization. However, the simulator environments still face limited diversity, and the web-collected data often requires extensive labor to remove the noise. In this paper, we propose a Rewriting-driven AugMentation (RAM) paradigm for VLN, which directly creates the unseen observation-instruction pairs via rewriting human-annotated training data. Benefiting from our rewriting mechanism, new observation-instruction can be obtained in both simulator-free and labor-saving manners to promote generalization. Specifically, we first introduce Object-Enriched Observation Rewriting, where we combine Vision-Language Models (VLMs) and Large Language Models (LLMs) to derive rewritten object-enriched scene descriptions, enabling observation synthesis with diverse objects and spatial layouts via Text-to-Image Generation Models (T2IMs). Then, we propose Observation-Contrast Instruction Rewriting, which generates observation-aligned rewritten instructions by requiring LLMs to reason the difference between original and new observations. We further develop a mixing-then-focusing training strategy with a random observation cropping scheme, effectively enhancing data distribution diversity while suppressing augmentation data noise during training. Experiments on both the discrete environments (R2R, REVERIE, and R4R datasets) and continuous environments (R2R-CE dataset) show the superior performance and impressive generalization ability of our method. Code is available at https://github.com/SaDil13/VLN-RAM.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind with Eyes: from Language Reasoning to Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2503.18071</link>
<guid>https://arxiv.org/abs/2503.18071</guid>
<content:encoded><![CDATA[
<div> : , , , , 

:
(i) -(ii)  <div>
arXiv:2503.18071v1 Announce Type: new 
Abstract: Language models have recently advanced into the realm of reasoning, yet it is through multimodal reasoning that we can fully unlock the potential to achieve more comprehensive, human-like cognitive capabilities. This survey provides a systematic overview of the recent multimodal reasoning approaches, categorizing them into two levels: language-centric multimodal reasoning and collaborative multimodal reasoning. The former encompasses one-pass visual perception and active visual perception, where vision primarily serves a supporting role in language reasoning. The latter involves action generation and state update within reasoning process, enabling a more dynamic interaction between modalities. Furthermore, we analyze the technical evolution of these methods, discuss their inherent challenges, and introduce key benchmark tasks and evaluation metrics for assessing multimodal reasoning performance. Finally, we provide insights into future research directions from the following two perspectives: (i) from visual-language reasoning to omnimodal reasoning and (ii) from multimodal reasoning to multimodal agents. This survey aims to provide a structured overview that will inspire further advancements in multimodal reasoning research.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Example-Based Learning in Software Engineering Education: A Systematic Mapping Study</title>
<link>https://arxiv.org/abs/2503.18080</link>
<guid>https://arxiv.org/abs/2503.18080</guid>
<content:encoded><![CDATA[
<div> : 

:
EBL30EBLEBLEBL <div>
arXiv:2503.18080v1 Announce Type: new 
Abstract: The discipline of Software Engineering (SE) allows students to understand specific concepts or problems while designing software. Empowering students with the necessary knowledge and skills for the software industry is challenging for universities. One key problem is that traditional methodologies often leave students as passive agents, limiting engagement and learning effectiveness. To address this issue, instructors must promote active learning in the classroom. Among the teaching methodologies, Example-Based Learning (EBL) has shown promise in improving the quality of Software Engineering Education (SEE). This study aims to investigate and classify the existing empirical evidence about using EBL in SEE. We carried out a systematic mapping to collect existing studies and evidence that describe how instructors have been employing EBL to teach SE concepts. By analyzing 30 studies, we identified the benefits and difficulties of using EBL, the SE contents taught by instructors, and the artifacts that support the methodology's use in the classroom. Besides, we identified the main types of examples used in SEE through EBL. We realized that EBL contributes to student learning, helping in students' interaction, interpreting and applying concepts, and increasing student motivation and confidence. However, some barriers to adopting EBL in SEE are increasing the effort required by instructors, lack of adequate learning support, and time spent constructing diagrams with errors. Overall, our findings suggest that EBL can improve the effectiveness of SEE, but more research is needed to address the gaps and challenges identified in our study.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentRxiv: Towards Collaborative Autonomous Research</title>
<link>https://arxiv.org/abs/2503.18102</link>
<guid>https://arxiv.org/abs/2503.18102</guid>
<content:encoded><![CDATA[
<div> AgentRxivLLM

:<br />
AgentRxivLLMagentMATH-50011.4%3.3%AgentRxivMATH-50013.7%AIAgentRxiv <div>
arXiv:2503.18102v1 Announce Type: new 
Abstract: Progress in scientific discovery is rarely the result of a single "Eureka" moment, but is rather the product of hundreds of scientists incrementally working together toward a common goal. While existing agent workflows are capable of producing research autonomously, they do so in isolation, without the ability to continuously improve upon prior research results. To address these challenges, we introduce AgentRxiv-a framework that lets LLM agent laboratories upload and retrieve reports from a shared preprint server in order to collaborate, share insights, and iteratively build on each other's research. We task agent laboratories to develop new reasoning and prompting techniques and find that agents with access to their prior research achieve higher performance improvements compared to agents operating in isolation (11.4% relative improvement over baseline on MATH-500). We find that the best performing strategy generalizes to benchmarks in other domains (improving on average by 3.3%). Multiple agent laboratories sharing research through AgentRxiv are able to work together towards a common goal, progressing more rapidly than isolated laboratories, achieving higher overall accuracy (13.7% relative improvement over baseline on MATH-500). These findings suggest that autonomous agents may play a role in designing future AI systems alongside humans. We hope that AgentRxiv allows agents to collaborate toward research goals and enables researchers to accelerate discovery.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoBenchX: Benchmarking LLMs for Multistep Geospatial Tasks</title>
<link>https://arxiv.org/abs/2503.18129</link>
<guid>https://arxiv.org/abs/2503.18129</guid>
<content:encoded><![CDATA[
<div> GIS

:
GISLLMsLLMsSonnet 3.53.7Haiku 3.5Gemini 2.0GPT-4oGPT-4o minio3-mini23LLMSonnet 3.5GPT-4oClaudeOpenAIAnthropicLLMsGeoAI <div>
arXiv:2503.18129v1 Announce Type: new 
Abstract: In this paper, we establish a benchmark for evaluating large language models (LLMs) on multi-step geospatial tasks relevant to commercial GIS practitioners. We assess seven leading commercial LLMs (Sonnet 3.5 and 3.7, Haiku 3.5, Gemini 2.0, GPT-4o, GPT-4o mini, and o3-mini) using a simple tool-calling agent equipped with 23 geospatial functions. Our benchmark comprises tasks across four categories of increasing complexity, with both solvable and intentionally unsolvable tasks to test hallucination rejection. We develop an LLM-as-Judge evaluation framework to compare agent solutions against reference implementations. Results show Sonnet 3.5 and GPT-4o achieve the best overall performance, with Claude models excelling on solvable tasks while OpenAI models better identify unsolvable scenarios. We observe significant differences in token usage, with Anthropic models consuming substantially more tokens than competitors. Common errors include misunderstanding geometrical relationships, relying on outdated knowledge, and inefficient data manipulation. The resulting benchmark set, evaluation framework, and data generation pipeline are released as open-source resources, providing one more standardized method for ongoing evaluation of LLMs for GeoAI.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MathAgent: Leveraging a Mixture-of-Math-Agent Framework for Real-World Multimodal Mathematical Error Detection</title>
<link>https://arxiv.org/abs/2503.18132</link>
<guid>https://arxiv.org/abs/2503.18132</guid>
<content:encoded><![CDATA[
<div> : Multimodal Large Language Models (MLLMs), MathAgent, , , 

<br /><br />:
MathAgentMixture-of-Math-AgentMathAgentMathAgent5%3%MathAgentK-1290% <div>
arXiv:2503.18132v1 Announce Type: new 
Abstract: Mathematical error detection in educational settings presents a significant challenge for Multimodal Large Language Models (MLLMs), requiring a sophisticated understanding of both visual and textual mathematical content along with complex reasoning capabilities. Though effective in mathematical problem-solving, MLLMs often struggle with the nuanced task of identifying and categorizing student errors in multimodal mathematical contexts. Therefore, we introduce MathAgent, a novel Mixture-of-Math-Agent framework designed specifically to address these challenges. Our approach decomposes error detection into three phases, each handled by a specialized agent: an image-text consistency validator, a visual semantic interpreter, and an integrative error analyzer. This architecture enables more accurate processing of mathematical content by explicitly modeling relationships between multimodal problems and student solution steps. We evaluate MathAgent on real-world educational data, demonstrating approximately 5% higher accuracy in error step identification and 3% improvement in error categorization compared to baseline models. Besides, MathAgent has been successfully deployed in an educational platform that has served over one million K-12 students, achieving nearly 90% student satisfaction while generating significant cost savings by reducing manual error detection.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Multi-Agent Reinforcement Learning: A Novel Approach Toward Real-World Multi-Echelon Inventory Optimization</title>
<link>https://arxiv.org/abs/2503.18201</link>
<guid>https://arxiv.org/abs/2503.18201</guid>
<content:encoded><![CDATA[
<div> <br /><br />: DRLMEIODRLDRLGNNMARLIMARLIMARLDRLIMARL <div>
arXiv:2503.18201v1 Announce Type: new 
Abstract: Multi-echelon inventory optimization (MEIO) is critical for effective supply chain management, but its inherent complexity can pose significant challenges. Heuristics are commonly used to address this complexity, yet they often face limitations in scope and scalability. Recent research has found deep reinforcement learning (DRL) to be a promising alternative to traditional heuristics, offering greater versatility by utilizing dynamic decision-making capabilities. However, since DRL is known to struggle with the curse of dimensionality, its relevance to complex real-life supply chain scenarios is still to be determined. This thesis investigates DRL's applicability to MEIO problems of increasing complexity. A state-of-the-art DRL model was replicated, enhanced, and tested across 13 supply chain scenarios, combining diverse network structures and parameters. To address DRL's challenges with dimensionality, additional models leveraging graph neural networks (GNNs) and multi-agent reinforcement learning (MARL) were developed, culminating in the novel iterative multi-agent reinforcement learning (IMARL) approach. IMARL demonstrated superior scalability, effectiveness, and reliability in optimizing inventory policies, consistently outperforming benchmarks. These findings confirm the potential of DRL, particularly IMARL, to address real-world supply chain challenges and call for additional research to further expand its applicability.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViVa: Video-Trained Value Functions for Guiding Online RL from Diverse Data</title>
<link>https://arxiv.org/abs/2503.18210</link>
<guid>https://arxiv.org/abs/2503.18210</guid>
<content:encoded><![CDATA[
<div> 

:<br />
RL <div>
arXiv:2503.18210v1 Announce Type: new 
Abstract: Online reinforcement learning (RL) with sparse rewards poses a challenge partly because of the lack of feedback on states leading to the goal. Furthermore, expert offline data with reward signal is rarely available to provide this feedback and bootstrap online learning. How can we guide online agents to the right solution without this on-task data? Reward shaping offers a solution by providing fine-grained signal to nudge the policy towards the optimal solution. However, reward shaping often requires domain knowledge to hand-engineer heuristics for a specific goal. To enable more general and inexpensive guidance, we propose and analyze a data-driven methodology that automatically guides RL by learning from widely available video data such as Internet recordings, off-task demonstrations, task failures, and undirected environment interaction. By learning a model of optimal goal-conditioned value from diverse passive data, we open the floor to scaling up and using various data sources to model general goal-reaching behaviors relevant to guiding online RL. Specifically, we use intent-conditioned value functions to learn from diverse videos and incorporate these goal-conditioned values into the reward. Our experiments show that video-trained value functions work well with a variety of data sources, exhibit positive transfer from human video pre-training, can generalize to unseen goals, and scale with dataset size.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Navigation of a Cable-Towed Load using Quadrupedal Robot Team via MARL</title>
<link>https://arxiv.org/abs/2503.18221</link>
<guid>https://arxiv.org/abs/2503.18221</guid>
<content:encoded><![CDATA[
<div> MARL

:
MARLCTDEMARL <div>
arXiv:2503.18221v1 Announce Type: new 
Abstract: This work addresses the challenge of enabling a team of quadrupedal robots to collaboratively tow a cable-connected load through cluttered and unstructured environments while avoiding obstacles. Leveraging cables allows the multi-robot system to navigate narrow spaces by maintaining slack when necessary. However, this introduces hybrid physical interactions due to alternating taut and slack states, with computational complexity that scales exponentially as the number of agents increases. To tackle these challenges, we developed a scalable and decentralized system capable of dynamically coordinating a variable number of quadrupedal robots while managing the hybrid physical interactions inherent in the load-towing task. At the core of this system is a novel multi-agent reinforcement learning (MARL)-based planner, designed for decentralized coordination. The MARL-based planner is trained using a centralized training with decentralized execution (CTDE) framework, enabling each robot to make decisions autonomously using only local (ego) observations. To accelerate learning and ensure effective collaboration across varying team sizes, we introduce a tailored training curriculum for MARL. Experimental results highlight the flexibility and scalability of the framework, demonstrating successful deployment with one to four robots in real-world scenarios and up to twelve robots in simulation. The decentralized planner maintains consistent inference times, regardless of the team size. Additionally, the proposed system demonstrates robustness to environment perturbations and adaptability to varying load weights. This work represents a step forward in achieving flexible and efficient multi-legged robotic collaboration in complex and real-world environments.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KEA: Keeping Exploration Alive by Proactively Coordinating Exploration Strategies</title>
<link>https://arxiv.org/abs/2503.18234</link>
<guid>https://arxiv.org/abs/2503.18234</guid>
<content:encoded><![CDATA[
<div> Soft Actor-Critic (SAC)KEA

:
KEASoft Actor-Critic (SAC)KEASACDeepMind Control SuiteKEA<br /><br /> <div>
arXiv:2503.18234v1 Announce Type: new 
Abstract: Soft Actor-Critic (SAC) has achieved notable success in continuous control tasks but struggles in sparse reward settings, where infrequent rewards make efficient exploration challenging. While novelty-based exploration methods address this issue by encouraging the agent to explore novel states, they are not trivial to apply to SAC. In particular, managing the interaction between novelty-based exploration and SAC's stochastic policy can lead to inefficient exploration and redundant sample collection. In this paper, we propose KEA (Keeping Exploration Alive) which tackles the inefficiencies in balancing exploration strategies when combining SAC with novelty-based exploration. KEA introduces an additional co-behavior agent that works alongside SAC and a switching mechanism to facilitate proactive coordination between exploration strategies from novelty-based exploration and stochastic policy. This coordination allows the agent to maintain stochasticity in high-novelty regions, enhancing exploration efficiency and reducing repeated sample collection. We first analyze this potential issue in a 2D navigation task and then evaluate KEA on sparse reward control tasks from the DeepMind Control Suite. Compared to state-of-the-art novelty-based exploration baselines, our experiments show that KEA significantly improves learning efficiency and robustness in sparse reward setups.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Curationary Tale: Logarithmic Regret in DeFi Lending via Dynamic Pricing</title>
<link>https://arxiv.org/abs/2503.18237</link>
<guid>https://arxiv.org/abs/2503.18237</guid>
<content:encoded><![CDATA[
<div> : Decentralized finance (DeFi), Lending, Static pricing, Adaptive supply models, Curators

:
<br />
(DeFi)AaveMorphoEulerDeFiT$O(\log T)$regret$\Omega(\sqrt{T})$ <div>
arXiv:2503.18237v1 Announce Type: new 
Abstract: Lending within decentralized finance (DeFi) has facilitated over $100 billion of loans since 2020. A long-standing inefficiency in DeFi lending protocols such as Aave is the use of static pricing mechanisms for loans. These mechanisms have been shown to maximize neither welfare nor revenue for participants in DeFi lending protocols. Recently, adaptive supply models pioneered by Morpho and Euler have become a popular means of dynamic pricing for loans. This pricing is facilitated by agents known as curators, who bid to match supply and demand. We construct and analyze an online learning model for static and dynamic pricing models within DeFi lending. We show that when loans are small and have a short duration relative to an observation time $T$, adaptive supply models achieve $O(\log T)$ regret, while static models cannot achieve better than $\Omega(\sqrt{T})$ regret. We then study competitive behavior between curators, demonstrating that adaptive supply mechanisms maximize revenue and welfare for both borrowers and lenders.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborating with AI Agents: Field Experiments on Teamwork, Productivity, and Performance</title>
<link>https://arxiv.org/abs/2503.18238</link>
<guid>https://arxiv.org/abs/2503.18238</guid>
<content:encoded><![CDATA[
<div> : MindMeldAI

:
MindMeldAIAI2310--AIAIAI137%23%20%-AI60%AIAIAI-AIAIAI <div>
arXiv:2503.18238v1 Announce Type: new 
Abstract: To uncover how AI agents change productivity, performance, and work processes, we introduce MindMeld: an experimentation platform enabling humans and AI agents to collaborate in integrative workspaces. In a large-scale marketing experiment on the platform, 2310 participants were randomly assigned to human-human and human-AI teams, with randomized AI personality traits. The teams exchanged 183,691 messages, and created 63,656 image edits, 1,960,095 ad copy edits, and 10,375 AI-generated images while producing 11,138 ads for a large think tank. Analysis of fine-grained communication, collaboration, and workflow logs revealed that collaborating with AI agents increased communication by 137% and allowed humans to focus 23% more on text and image content generation messaging and 20% less on direct text editing. Humans on Human-AI teams sent 23% fewer social messages, creating 60% greater productivity per worker and higher-quality ad copy. In contrast, human-human teams produced higher-quality images, suggesting that AI agents require fine-tuning for multimodal workflows. AI personality prompt randomization revealed that AI traits can complement human personalities to enhance collaboration. For example, conscientious humans paired with open AI agents improved image quality, while extroverted humans paired with conscientious AI agents reduced the quality of text, images, and clicks. In field tests of ad campaigns with ~5M impressions, ads with higher image quality produced by human collaborations and higher text quality produced by AI collaborations performed significantly better on click-through rate and cost per click metrics. Overall, ads created by human-AI teams performed similarly to those created by human-human teams. Together, these results suggest AI agents can improve teamwork and productivity, especially when tuned to complement human traits.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Capture and Study Conversations Between Research Participants and ChatGPT: GPT for Researchers (g4r.org)</title>
<link>https://arxiv.org/abs/2503.18303</link>
<guid>https://arxiv.org/abs/2503.18303</guid>
<content:encoded><![CDATA[
<div>  (LLMs)ChatGPTGPT for Researchers (G4R)

:<br />
ChatGPTAILLMGPT for Researchers (G4R)g4r.orgGPTG4R(1) GPTChatGPT(2) GPTGPT(3) GPTG4RAILLMsAIAIg4r.org <div>
arXiv:2503.18303v1 Announce Type: new 
Abstract: As large language models (LLMs) like ChatGPT become increasingly integrated into our everyday lives--from customer service and education to creative work and personal productivity--understanding how people interact with these AI systems has become a pressing issue. Despite the widespread use of LLMs, researchers lack standardized tools for systematically studying people's interactions with LLMs. To address this issue, we introduce GPT for Researchers (G4R), or g4r.org, a free website that researchers can use to easily create and integrate a GPT Interface into their studies. At g4r.org, researchers can (1) enable their study participants to interact with GPT (such as ChatGPT), (2) customize GPT Interfaces to guide participants' interactions with GPT (e.g., set constraints on topics or adjust GPT's tone or response style), and (3) capture participants' interactions with GPT by downloading data on messages exchanged between participants and GPT. By facilitating study participants' interactions with GPT and providing detailed data on these interactions, G4R can support research on topics such as consumer interactions with AI agents or LLMs, AI-assisted decision-making, and linguistic patterns in human-AI communication. With this goal in mind, we provide a step-by-step guide to using G4R at g4r.org.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepFund: Will LLM be Professional at Fund Investment? A Live Arena Perspective</title>
<link>https://arxiv.org/abs/2503.18313</link>
<guid>https://arxiv.org/abs/2503.18313</guid>
<content:encoded><![CDATA[
<div> Large Language Models (LLMs), , DeepFund, , 

:
LLMsLLMsDeepFundLLMsDeepFundwebDeepFundLLMs <div>
arXiv:2503.18313v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across various domains, but their effectiveness in financial decision making, particularly in fund investment, remains inadequately evaluated. Current benchmarks primarily assess LLMs understanding of financial documents rather than their ability to manage assets or analyze trading opportunities in dynamic market conditions. A critical limitation in existing evaluation methodologies is the backtesting approach, which suffers from information leakage when LLMs are evaluated on historical data they may have encountered during pretraining. This paper introduces DeepFund, a comprehensive platform for evaluating LLM based trading strategies in a simulated live environment. Our approach implements a multi agent framework where LLMs serve as both analysts and managers, creating a realistic simulation of investment decision making. The platform employs a forward testing methodology that mitigates information leakage by evaluating models on market data released after their training cutoff dates. We provide a web interface that visualizes model performance across different market conditions and investment parameters, enabling detailed comparative analysis. Through DeepFund, we aim to provide a more accurate and fair assessment of LLMs capabilities in fund investment, offering insights into their potential real world applications in financial markets.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Influence Campaigns: Nudging under Bounded Confidence</title>
<link>https://arxiv.org/abs/2503.18331</link>
<guid>https://arxiv.org/abs/2503.18331</guid>
<content:encoded><![CDATA[
<div> campaign

:<br />
campaignTwitterChatGPT <div>
arXiv:2503.18331v1 Announce Type: new 
Abstract: Influence campaigns in online social networks are often run by organizations, political parties, and nation states to influence large audiences. These campaigns are employed through the use of agents in the network that share persuasive content. Yet, their impact might be minimal if the audiences remain unswayed, often due to the bounded confidence phenomenon, where only a narrow spectrum of viewpoints can influence them. Here we show that to persuade under bounded confidence, an agent must nudge its targets to gradually shift their opinions. Using a control theory approach, we show how to construct an agent's nudging policy under the bounded confidence opinion dynamics model and also how to select targets for multiple agents in an influence campaign on a social network. Simulations on real Twitter networks show that a multi-agent nudging policy can shift the mean opinion, decrease opinion polarization, or even increase it. We find that our nudging based policies outperform other common techniques that do not consider the bounded confidence effect. Finally, we show how to craft prompts for large language models, such as ChatGPT, to generate text-based content for real nudging policies. This illustrates the practical feasibility of our approach, allowing one to go from mathematical nudging policies to real social media content.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-based Modeling meets the Capability Approach for Human Development: Simulating Homelessness Policy-making</title>
<link>https://arxiv.org/abs/2503.18389</link>
<guid>https://arxiv.org/abs/2503.18389</guid>
<content:encoded><![CDATA[
<div> 

:<br />
Capability ApproachCACAMarkov Decision ProcessMDPCA <div>
arXiv:2503.18389v1 Announce Type: new 
Abstract: The global rise in homelessness calls for urgent and alternative policy solutions. Non-profits and governmental organizations alert about the many challenges faced by people experiencing homelessness (PEH), which include not only the lack of shelter but also the lack of opportunities for personal development. In this context, the capability approach (CA), which underpins the United Nations Sustainable Development Goals (SDGs), provides a comprehensive framework to assess inequity in terms of real opportunities. This paper explores how the CA can be combined with agent-based modelling and reinforcement learning. The goals are: (1) implementing the CA as a Markov Decision Process (MDP), (2) building on such MDP to develop a rich decision-making model that accounts for more complex motivators of behaviour, such as values and needs, and (3) developing an agent-based simulation framework that allows to assess alternative policies aiming to expand or restore people's capabilities. The framework is developed in a real case study of health inequity and homelessness, working in collaboration with stakeholders, non-profits and domain experts. The ultimate goal of the project is to develop a novel agent-based simulation framework, rooted in the CA, which can be replicated in a diversity of social contexts to assess policies in a non-invasive way.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dominant Groups and Asymmetric Polarization in Generalized Quasi-Structurally Balanced Networks</title>
<link>https://arxiv.org/abs/2503.18444</link>
<guid>https://arxiv.org/abs/2503.18444</guid>
<content:encoded><![CDATA[
<div> : 

<br /><br />:
GQSBGQSBGQSB <div>
arXiv:2503.18444v1 Announce Type: new 
Abstract: The paper focuses on the phenomenon of asymmetric polarization arising in the presence of a dominant group in the network. The existing works in the literature analyze polarization primarily in structurally and quasi-structurally balanced networks. In this work, we introduce generalized quasi-structurally balanced (GQSB) networks, which include both of these networks as special cases. In the presence of a dominant group, a GQSB network has a unique bipartition: the dominant group (and its allies) and the remaining agents. The dominant group's superior influence results in an asymmetry in how the inter-subset antagonistic interactions are perceived by both of the subsets. This, in turn, leads to asymmetry in the final polarized opinions. To model this behavior, we propose a generalized Laplacian flow for undirected GQSB networks with a dominant group and establish necessary and sufficient conditions for achieving asymmetric polarization. The theoretical results presented in this paper are validated through numerical simulations on the Highland Tribes real-world dataset.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEAlign: Alignment Training for Software Engineering Agent</title>
<link>https://arxiv.org/abs/2503.18455</link>
<guid>https://arxiv.org/abs/2503.18455</guid>
<content:encoded><![CDATA[
<div> : , , SEAlign, , 

:
SEAlignSEAlignSEAlignHumanEvalFixSWE-Bench-LiteSWE-Bench-VerifiedSEAlignSEAlign <div>
arXiv:2503.18455v1 Announce Type: new 
Abstract: Recent advances in code generation models have demonstrated impressive capabilities in automating software development tasks, yet these models still struggle in real-world software engineering scenarios. Although current training methods, particularly post-training, excel at solving competitive programming problems, they fail to adequately prepare models for the complexities of practical software development. This misalignment raises the critical question: Are existing alignment training methods well suited for real-world software engineering tasks? In this study, we identify this issue and propose SEAlign, a novel alignment framework designed to bridge the gap between code generation models and real-world software development tasks. SEAlign leverages the unique characteristics of software engineering processes, including high-quality workflow steps, to enhance model capabilities. Our framework further employs Monte Carlo Tree Search for fine-grained alignment in multi-step decision processes, followed by preference optimization on critical actions to ensure models meet real-world requirements. We evaluate SEAlign on three standard agentic benchmarks for real-world software engineering, including HumanEvalFix, SWE-Bench-Lite, and SWE-Bench-Verified. Experimental results demonstrate state-of-the-art performance with minimal training overhead. In addition, we develop an agent-based software development platform using SEAlign, which successfully automates the creation of several small applications. Human evaluations of these applications highlight significant improvements in both task performance and user experience. Our findings underscore the potential of SEAlign to accelerate the adoption of large code models in real-world software development. We believe that this research makes a meaningful step towards fully automated software engineering.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safeguarding Mobile GUI Agent via Logic-based Action Verification</title>
<link>https://arxiv.org/abs/2503.18492</link>
<guid>https://arxiv.org/abs/2503.18492</guid>
<content:encoded><![CDATA[
<div> : (LFMs), (GUI), VeriSafe Agent(VSA), , 

<br /><br />:
VeriSafe Agent (VSA)VSA(DSL)VSAGUILLMGPT-4VSA18300VSA94.3%-98.33%LLM20.4%-25.6%GUI90%-130% <div>
arXiv:2503.18492v1 Announce Type: new 
Abstract: Large Foundation Models (LFMs) have unlocked new possibilities in human-computer interaction, particularly with the rise of mobile Graphical User Interface (GUI) Agents capable of interpreting GUIs. These agents promise to revolutionize mobile computing by allowing users to automate complex mobile tasks through simple natural language instructions. However, the inherent probabilistic nature of LFMs, coupled with the ambiguity and context-dependence of mobile tasks, makes LFM-based automation unreliable and prone to errors. To address this critical challenge, we introduce VeriSafe Agent (VSA): a formal verification system that serves as a logically grounded safeguard for Mobile GUI Agents. VSA is designed to deterministically ensure that an agent's actions strictly align with user intent before conducting an action. At its core, VSA introduces a novel autoformalization technique that translates natural language user instructions into a formally verifiable specification, expressed in our domain-specific language (DSL). This enables runtime, rule-based verification, allowing VSA to detect and prevent erroneous actions executing an action, either by providing corrective feedback or halting unsafe behavior. To the best of our knowledge, VSA is the first attempt to bring the rigor of formal verification to GUI agent. effectively bridging the gap between LFM-driven automation and formal software verification. We implement VSA using off-the-shelf LLM services (GPT-4o) and evaluate its performance on 300 user instructions across 18 widely used mobile apps. The results demonstrate that VSA achieves 94.3%-98.33% accuracy in verifying agent actions, representing a significant 20.4%-25.6% improvement over existing LLM-based verification methods, and consequently increases the GUI agent's task completion rate by 90%-130%.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verbal Process Supervision Elicits Better Coding Agents</title>
<link>https://arxiv.org/abs/2503.18494</link>
<guid>https://arxiv.org/abs/2503.18494</guid>
<content:encoded><![CDATA[
<div> : CURAVerbal Process Supervision (VPS)BigCodeBencho3-mini-of-the-

<br /><br />:
AICURAVerbal Process Supervision (VPS)BigCodeBench3.65%CURAo3-miniVPSLLM <div>
arXiv:2503.18494v1 Announce Type: new 
Abstract: The emergence of large language models and their applications as AI agents have significantly advanced state-of-the-art code generation benchmarks, transforming modern software engineering tasks. However, even with test-time computed reasoning models, these systems still struggle with complex software engineering challenges. This work introduces CURA, a code understanding and reasoning agent system enhanced with verbal process supervision (VPS), achieving a 3.65\% improvement over baseline models on challenging benchmarks like BigCodeBench. Furthermore, CURA, when paired with the o3-mini model and VPS techniques, attains state-of-the-art performance. This work represents a step forward in integrating reasoning-driven architectures with LLM-based code generation, enabling agentic reasoning for language models to solve complex software engineering tasks.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P3Nav: A Unified Framework for Embodied Navigation Integrating Perception, Planning, and Prediction</title>
<link>https://arxiv.org/abs/2503.18525</link>
<guid>https://arxiv.org/abs/2503.18525</guid>
<content:encoded><![CDATA[
<div> P3Nav3DCHORES-$\mathbb{S}$

:<br />
P3NavembodiedEQAP3Nav3DCHORES-$\mathbb{S}$P3Nav75%state-of-the-art <div>
arXiv:2503.18525v1 Announce Type: new 
Abstract: In language-guided visual navigation, agents locate target objects in unseen environments using natural language instructions. For reliable navigation in unfamiliar scenes, agents must possess strong perception, planning, and prediction capabilities. Additionally, when agents revisit previously explored areas during long-term navigation, they may retain irrelevant and redundant historical perceptions, leading to suboptimal results. In this work, we introduce \textbf{P3Nav}, a unified framework that integrates \textbf{P}erception, \textbf{P}lanning, and \textbf{P}rediction capabilities through \textbf{Multitask Collaboration} on navigation and embodied question answering (EQA) tasks, thereby enhancing navigation performance. Furthermore, P3Nav employs an \textbf{Adaptive 3D-aware History Sampling} strategy to effectively and efficiently utilize historical observations. By leveraging the large language models (LLM), P3Nav comprehends diverse commands and complex visual scenes, resulting in appropriate navigation actions. P3Nav achieves a 75\% success rate in object goal navigation on the $\mathrm{CHORES}$-$\mathbb{S}$ benchmark, setting a new state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parental Guidance: Efficient Lifelong Learning through Evolutionary Distillation</title>
<link>https://arxiv.org/abs/2503.18531</link>
<guid>https://arxiv.org/abs/2503.18531</guid>
<content:encoded><![CDATA[
<div>  (RL) (IL)

<br /><br />:
RLIL <div>
arXiv:2503.18531v1 Announce Type: new 
Abstract: Developing robotic agents that can perform well in diverse environments while showing a variety of behaviors is a key challenge in AI and robotics. Traditional reinforcement learning (RL) methods often create agents that specialize in narrow tasks, limiting their adaptability and diversity. To overcome this, we propose a preliminary, evolution-inspired framework that includes a reproduction module, similar to natural species reproduction, balancing diversity and specialization. By integrating RL, imitation learning (IL), and a coevolutionary agent-terrain curriculum, our system evolves agents continuously through complex tasks. This approach promotes adaptability, inheritance of useful traits, and continual learning. Agents not only refine inherited skills but also surpass their predecessors. Our initial experiments show that this method improves exploration efficiency and supports open-ended learning, offering a scalable solution where sparse reward coupled with diverse terrain environments induces a multi-task setting.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-agent coordination for data gathering with periodic requests and deliveries</title>
<link>https://arxiv.org/abs/2503.18546</link>
<guid>https://arxiv.org/abs/2503.18546</guid>
<content:encoded><![CDATA[
<div> 


-<br /><br /> <div>
arXiv:2503.18546v1 Announce Type: new 
Abstract: In this demo work we develop a method to plan and coordinate a multi-agent team to gather information on demand. The data is periodically requested by a static Operation Center (OC) from changeable goals locations. The mission of the team is to reach these locations, taking measurements and delivering the data to the OC. Due to the limited communication range as well as signal attenuation because of the obstacles, the agents must travel to the OC, to upload the data. The agents can play two roles: ones as workers gathering data, the others as collectors traveling invariant paths for collecting the data of the workers to re-transmit it to the OC. The refreshing time of the delivered information depends on the number of available agents as well as of the scenario. The proposed algorithm finds out the best balance between the number of collectors-workers and the partition of the scenario into working areas in the planning phase, which provides the minimum refreshing time and will be the one executed by the agents.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Uncertainty-Aware Diffusion for Multi-Agent Trajectory Modeling</title>
<link>https://arxiv.org/abs/2503.18589</link>
<guid>https://arxiv.org/abs/2503.18589</guid>
<content:encoded><![CDATA[
<div> U2Diff<br /><br /><br />
U2DiffU2DiffU2DiffNBABasketball-UFootball-USoccer-U <div>
arXiv:2503.18589v1 Announce Type: new 
Abstract: Multi-agent trajectory modeling has primarily focused on forecasting future states, often overlooking broader tasks like trajectory completion, which are crucial for real-world applications such as correcting tracking data. Existing methods also generally predict agents' states without offering any state-wise measure of uncertainty. Moreover, popular multi-modal sampling methods lack any error probability estimates for each generated scene under the same prior observations, making it difficult to rank the predictions during inference time. We introduce U2Diff, a \textbf{unified} diffusion model designed to handle trajectory completion while providing state-wise \textbf{uncertainty} estimates jointly. This uncertainty estimation is achieved by augmenting the simple denoising loss with the negative log-likelihood of the predicted noise and propagating latent space uncertainty to the real state space. Additionally, we incorporate a Rank Neural Network in post-processing to enable \textbf{error probability} estimation for each generated mode, demonstrating a strong correlation with the error relative to ground truth. Our method outperforms the state-of-the-art solutions in trajectory completion and forecasting across four challenging sports datasets (NBA, Basketball-U, Football-U, Soccer-U), highlighting the effectiveness of uncertainty and error probability estimation. Video at https://youtu.be/ngw4D4eJToE
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Virtual Agent Learning and Reasoning: A Step-wise, Multi-dimensional, and Generalist Reward Model with Benchmark</title>
<link>https://arxiv.org/abs/2503.18665</link>
<guid>https://arxiv.org/abs/2503.18665</guid>
<content:encoded><![CDATA[
<div> : Generalist Virtual Agents (GVAs), Multimodal Large Language Models (MLLMs), Similar, Step-wise Multi-dimensional Generalist Reward Model, SRM Benchmark

<br /><br />:

Generalist Virtual Agents (GVAs)Multimodal Large Language Models (MLLMs)SimilarStep-wise Multi-dimensional Generalist Reward ModelMCTS-PTriple-MSimilarSRMSimilarSRMTrainSRMEvalSimilarGVAshttps://github.com/Galery23/Similar-v1 <div>
arXiv:2503.18665v1 Announce Type: new 
Abstract: The development of Generalist Virtual Agents (GVAs) powered by Multimodal Large Language Models (MLLMs) has shown significant promise in autonomous task execution. However, current training paradigms face critical limitations, including reliance on outcome supervision and labor-intensive human annotations. To address these challenges, we propose Similar, a Step-wise Multi-dimensional Generalist Reward Model, which offers fine-grained signals for agent training and can choose better action for inference-time scaling. Specifically, we begin by systematically defining five dimensions for evaluating agent actions. Building on this framework, we design an MCTS-P algorithm to automatically collect and annotate step-wise, five-dimensional agent execution data. Using this data, we train Similar with the Triple-M strategy. Furthermore, we introduce the first benchmark in the virtual agent domain for step-wise, multi-dimensional reward model training and evaluation, named SRM. This benchmark consists of two components: SRMTrain, which serves as the training set for Similar, and SRMEval, a manually selected test set for evaluating the reward model. Experimental results demonstrate that Similar, through its step-wise, multi-dimensional assessment and synergistic gain, provides GVAs with effective intermediate signals during both training and inference-time scaling. The code is available at https://github.com/Galery23/Similar-v1.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM Agents</title>
<link>https://arxiv.org/abs/2503.18666</link>
<guid>https://arxiv.org/abs/2503.18666</guid>
<content:encoded><![CDATA[
<div> : LLMsAgentSpec

<br /><br />:
LLMsAgentSpecLLMAgentSpecAgentSpec90%AVs100%AgentSpecAgentSpecLLMLLMsOpenAI o195.56%70.96%87.26%5AV <div>
arXiv:2503.18666v1 Announce Type: new 
Abstract: Agents built on LLMs are increasingly deployed across diverse domains, automating complex decision-making and task execution. However, their autonomy introduces safety risks, including security vulnerabilities, legal violations, and unintended harmful actions. Existing mitigation methods, such as model-based safeguards and early enforcement strategies, fall short in robustness, interpretability, and adaptability. To address these challenges, we propose AgentSpec, a lightweight domain-specific language for specifying and enforcing runtime constraints on LLM agents. With AgentSpec, users define structured rules that incorporate triggers, predicates, and enforcement mechanisms, ensuring agents operate within predefined safety boundaries. We implement AgentSpec across multiple domains, including code execution, embodied agents, and autonomous driving, demonstrating its adaptability and effectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe executions in over 90% of code agent cases, eliminates all hazardous actions in embodied agent tasks, and enforces 100% compliance by autonomous vehicles (AVs). Despite its strong safety guarantees, AgentSpec remains computationally lightweight, with overheads in milliseconds. By combining interpretability, modularity, and efficiency, AgentSpec provides a practical and scalable solution for enforcing LLM agent safety across diverse applications. We also automate the generation of rules using LLMs and assess their effectiveness. Our evaluation shows that the rules generated by OpenAI o1 achieve a precision of 95.56% and recall of 70.96% for embodied agents, successfully identifying 87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8 scenarios.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Continual Adaptation of Pretrained Robotic Policy with Online Meta-Learned Adapters</title>
<link>https://arxiv.org/abs/2503.18684</link>
<guid>https://arxiv.org/abs/2503.18684</guid>
<content:encoded><![CDATA[
<div> continual adaptation, autonomous agents, adapters, meta-learning, OMLA

:
OMLAOMLAhttps://ricky-zhu.github.io/OMLA/ <div>
arXiv:2503.18684v1 Announce Type: new 
Abstract: Continual adaptation is essential for general autonomous agents. For example, a household robot pretrained with a repertoire of skills must still adapt to unseen tasks specific to each household. Motivated by this, building upon parameter-efficient fine-tuning in language models, prior works have explored lightweight adapters to adapt pretrained policies, which can preserve learned features from the pretraining phase and demonstrate good adaptation performances. However, these approaches treat task learning separately, limiting knowledge transfer between tasks. In this paper, we propose Online Meta-Learned adapters (OMLA). Instead of applying adapters directly, OMLA can facilitate knowledge transfer from previously learned tasks to current learning tasks through a novel meta-learning objective. Extensive experiments in both simulated and real-world environments demonstrate that OMLA can lead to better adaptation performances compared to the baseline methods. The project link: https://ricky-zhu.github.io/OMLA/.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Acquisition of Discrete Grammatical Categories</title>
<link>https://arxiv.org/abs/2503.18702</link>
<guid>https://arxiv.org/abs/2503.18702</guid>
<content:encoded><![CDATA[
<div> 

:
 <div>
arXiv:2503.18702v1 Announce Type: new 
Abstract: This article presents experiments performed using a computational laboratory environment for language acquisition experiments. It implements a multi-agent system consisting of two agents: an adult language model and a daughter language model that aims to learn the mother language. Crucially, the daughter agent does not have access to the internal knowledge of the mother language model but only to the language exemplars the mother agent generates. These experiments illustrate how this system can be used to acquire abstract grammatical knowledge. We demonstrate how statistical analyses of patterns in the input data corresponding to grammatical categories yield discrete grammatical rules. These rules are subsequently added to the grammatical knowledge of the daughter language model. To this end, hierarchical agglomerative cluster analysis was applied to the utterances consecutively generated by the mother language model. It is argued that this procedure can be used to acquire structures resembling grammatical categories proposed by linguists for natural languages. Thus, it is established that non-trivial grammatical knowledge has been acquired. Moreover, the parameter configuration of this computational laboratory environment determined using training data generated by the mother language model is validated in a second experiment with a test set similarly resulting in the acquisition of non-trivial categories.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulation-Driven Balancing of Competitive Game Levels with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.18748</link>
<guid>https://arxiv.org/abs/2503.18748</guid>
<content:encoded><![CDATA[
<div> : mmo

:
PCGRLPCGRLNeural MMO <div>
arXiv:2503.18748v1 Announce Type: new 
Abstract: The balancing process for game levels in competitive two-player contexts involves a lot of manual work and testing, particularly for non-symmetrical game levels. In this work, we frame game balancing as a procedural content generation task and propose an architecture for automatically balancing of tile-based levels within the PCGRL framework (procedural content generation via reinforcement learning). Our architecture is divided into three parts: (1) a level generator, (2) a balancing agent, and (3) a reward modeling simulation. Through repeated simulations, the balancing agent receives rewards for adjusting the level towards a given balancing objective, such as equal win rates for all players. To this end, we propose new swap-based representations to improve the robustness of playability, thereby enabling agents to balance game levels more effectively and quickly compared to traditional PCGRL. By analyzing the agent's swapping behavior, we can infer which tile types have the most impact on the balance. We validate our approach in the Neural MMO (NMMO) environment in a competitive two-player scenario. In this extended conference paper, we present improved results, explore the applicability of the method to various forms of balancing beyond equal balancing, compare the performance to another search-based approach, and discuss the application of existing fairness metrics to game balancing.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defeating Prompt Injections by Design</title>
<link>https://arxiv.org/abs/2503.18813</link>
<guid>https://arxiv.org/abs/2503.18813</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)CaMeL<br /><br />:<br />
CaMeLLLMsLLMCaMeLLLMCaMeLCaMeLAgentDojo67% <div>
arXiv:2503.18813v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly deployed in agentic systems that interact with an external environment. However, LLM agents are vulnerable to prompt injection attacks when handling untrusted data. In this paper we propose CaMeL, a robust defense that creates a protective system layer around the LLM, securing it even when underlying models may be susceptible to attacks. To operate, CaMeL explicitly extracts the control and data flows from the (trusted) query; therefore, the untrusted data retrieved by the LLM can never impact the program flow. To further improve security, CaMeL relies on a notion of a capability to prevent the exfiltration of private data over unauthorized data flows. We demonstrate effectiveness of CaMeL by solving $67\%$ of tasks with provable security in AgentDojo [NeurIPS 2024], a recent agentic security benchmark.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Multi-Robot Coordination through Locality-Based Factorized Multi-Agent Actor-Critic Algorithm</title>
<link>https://arxiv.org/abs/2503.18816</link>
<guid>https://arxiv.org/abs/2503.18816</guid>
<content:encoded><![CDATA[
<div> -FACMAC

<br /><br />

-Loc-FACMACFACMACLoc-FACMACLoc-FACMACLOMAQFACMACQMIXMARLLoc-FACMAC108%- <div>
arXiv:2503.18816v1 Announce Type: new 
Abstract: In this work, we present a novel cooperative multi-agent reinforcement learning method called \textbf{Loc}ality based \textbf{Fac}torized \textbf{M}ulti-Agent \textbf{A}ctor-\textbf{C}ritic (Loc-FACMAC). Existing state-of-the-art algorithms, such as FACMAC, rely on global reward information, which may not accurately reflect the quality of individual robots' actions in decentralized systems. We integrate the concept of locality into critic learning, where strongly related robots form partitions during training. Robots within the same partition have a greater impact on each other, leading to more precise policy evaluation. Additionally, we construct a dependency graph to capture the relationships between robots, facilitating the partitioning process. This approach mitigates the curse of dimensionality and prevents robots from using irrelevant information. Our method improves existing algorithms by focusing on local rewards and leveraging partition-based learning to enhance training efficiency and performance. We evaluate the performance of Loc-FACMAC in three environments: Hallway, Multi-cartpole, and Bounded-Cooperative-Navigation. We explore the impact of partition sizes on the performance and compare the result with baseline MARL algorithms such as LOMAQ, FACMAC, and QMIX. The experiments reveal that, if the locality structure is defined properly, Loc-FACMAC outperforms these baseline algorithms up to 108\%, indicating that exploiting the locality structure in the actor-critic framework improves the MARL performance.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EconEvals: Benchmarks and Litmus Tests for LLM Agents in Unknown Environments</title>
<link>https://arxiv.org/abs/2503.18825</link>
<guid>https://arxiv.org/abs/2503.18825</guid>
<content:encoded><![CDATA[
<div> LLM-offs

<br /><br />
LLMLLMlitmusLLMtrade-offslitmusLLM <div>
arXiv:2503.18825v1 Announce Type: new 
Abstract: We develop benchmarks for LLM agents that act in, learn from, and strategize in unknown environments, the specifications of which the LLM agent must learn over time from deliberate exploration. Our benchmarks consist of decision-making tasks derived from key problems in economics. To forestall saturation, the benchmark tasks are synthetically generated with scalable difficulty levels. Additionally, we propose litmus tests, a new kind of quantitative measure for LLMs and LLM agents. Unlike benchmarks, litmus tests quantify differences in character, values, and tendencies of LLMs and LLM agents, by considering their behavior when faced with tradeoffs (e.g., efficiency versus equality) where there is no objectively right or wrong behavior. Overall, our benchmarks and litmus tests assess the abilities and tendencies of LLM agents in tackling complex economic problems in diverse settings spanning procurement, scheduling, task allocation, and pricing -- applications that should grow in importance as such agents are further integrated into the economy.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Organizing Graph Reasoning Evolves into a Critical State for Continuous Discovery Through Structural-Semantic Dynamics</title>
<link>https://arxiv.org/abs/2503.18852</link>
<guid>https://arxiv.org/abs/2503.18852</guid>
<content:encoded><![CDATA[
<div> agentic

<br /><br />:
 Von Neumann  12%  <div>
arXiv:2503.18852v1 Announce Type: new 
Abstract: We report fundamental insights into how agentic graph reasoning systems spontaneously evolve toward a critical state that sustains continuous semantic discovery. By rigorously analyzing structural (Von Neumann graph entropy) and semantic (embedding) entropy, we identify a subtle yet robust regime in which semantic entropy persistently dominates over structural entropy. This interplay is quantified by a dimensionless Critical Discovery Parameter that stabilizes at a small negative value, indicating a consistent excess of semantic entropy. Empirically, we observe a stable fraction (12%) of "surprising" edges, links between semantically distant concepts, providing evidence of long-range or cross-domain connections that drive continuous innovation. Concomitantly, the system exhibits scale-free and small-world topological features, alongside a negative cross-correlation between structural and semantic measures, reinforcing the analogy to self-organized criticality. These results establish clear parallels with critical phenomena in physical, biological, and cognitive complex systems, revealing an entropy-based principle governing adaptability and continuous innovation. Crucially, semantic richness emerges as the underlying driver of sustained exploration, despite not being explicitly used by the reasoning process. Our findings provide interdisciplinary insights and practical strategies for engineering intelligent systems with intrinsic capacities for long-term discovery and adaptation, and offer insights into how model training strategies can be developed that reinforce critical discovery.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentDropout: Dynamic Agent Elimination for Token-Efficient and High-Performance LLM-Based Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2503.18891</link>
<guid>https://arxiv.org/abs/2503.18891</guid>
<content:encoded><![CDATA[
<div> : AgentDropout

:
AgentDropoutAgentDropout21.6%18.4%1.14AgentDropouthttps://github.com/wangzx1219/AgentDropout <div>
arXiv:2503.18891v1 Announce Type: new 
Abstract: Multi-agent systems (MAS) based on large language models (LLMs) have demonstrated significant potential in collaborative problem-solving. However, they still face substantial challenges of low communication efficiency and suboptimal task performance, making the careful design of the agents' communication topologies particularly important. Inspired by the management theory that roles in an efficient team are often dynamically adjusted, we propose AgentDropout, which identifies redundant agents and communication across different communication rounds by optimizing the adjacency matrices of the communication graphs and eliminates them to enhance both token efficiency and task performance. Compared to state-of-the-art methods, AgentDropout achieves an average reduction of 21.6% in prompt token consumption and 18.4% in completion token consumption, along with a performance improvement of 1.14 on the tasks. Furthermore, the extended experiments demonstrate that AgentDropout achieves notable domain transferability and structure robustness, revealing its reliability and effectiveness. We release our code at https://github.com/wangzx1219/AgentDropout.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaWorld: Learning Adaptable World Models with Latent Actions</title>
<link>https://arxiv.org/abs/2503.18938</link>
<guid>https://arxiv.org/abs/2503.18938</guid>
<content:encoded><![CDATA[
<div> world modelsaction-controlled prediction modelsAdaWorldlatent actionsautoregressive world model

<br />
:
AdaWorldAdaWorldAdaWorld <div>
arXiv:2503.18938v1 Announce Type: new 
Abstract: World models aim to learn action-controlled prediction models and have proven essential for the development of intelligent agents. However, most existing world models rely heavily on substantial action-labeled data and costly training, making it challenging to adapt to novel environments with heterogeneous actions through limited interactions. This limitation can hinder their applicability across broader domains. To overcome this challenge, we propose AdaWorld, an innovative world model learning approach that enables efficient adaptation. The key idea is to incorporate action information during the pretraining of world models. This is achieved by extracting latent actions from videos in a self-supervised manner, capturing the most critical transitions between frames. We then develop an autoregressive world model that conditions on these latent actions. This learning paradigm enables highly adaptable world models, facilitating efficient transfer and learning of new actions even with limited interactions and finetuning. Our comprehensive experiments across multiple environments demonstrate that AdaWorld achieves superior performance in both simulation quality and visual planning.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Echo-E$^3$Net: Efficient Endo-Epi Spatio-Temporal Network for Ejection Fraction Estimation</title>
<link>https://arxiv.org/abs/2503.17543</link>
<guid>https://arxiv.org/abs/2503.17543</guid>
<content:encoded><![CDATA[
<div> (LVEF)Echo-E$^3$NetEndo-Epi(E$^2$CBD)Endo-Epi(E$^2$FA)

<br /><br />
Echo-E$^3$NetLVEF-Echo-E$^3$NetEndo-EpiE$^2$CBDEndo-EpiE$^2$FA-Echo-E$^3$NetEchoNet-DynamicRMSE5.15R$^2$0.826808.49Echo-E$^3$Net(PoCUS)GitHub <div>
arXiv:2503.17543v1 Announce Type: cross 
Abstract: Left ventricular ejection fraction (LVEF) is a critical metric for assessing cardiac function, widely used in diagnosing heart failure and guiding clinical decisions. Despite its importance, conventional LVEF estimation remains time-consuming and operator-dependent. Recent deep learning advancements have enhanced automation, yet many existing models are computationally demanding, hindering their feasibility for real-time clinical applications. Additionally, the interplay between spatial and temporal features is crucial for accurate estimation but is often overlooked. In this work, we propose Echo-E$^3$Net, an efficient Endo-Epi spatio-temporal network tailored for LVEF estimation. Our method introduces the Endo-Epi Cardial Border Detector (E$^2$CBD) module, which enhances feature extraction by leveraging spatial and temporal landmark cues. Complementing this, the Endo-Epi Feature Aggregator (E$^2$FA) distills statistical descriptors from backbone feature maps, refining the final EF prediction. These modules, along with a multi-component loss function tailored to align with the clinical definition of EF, collectively enhance spatial-temporal representation learning, ensuring robust and efficient EF estimation. We evaluate Echo-E$^3$Net on the EchoNet-Dynamic dataset, achieving a RMSE of 5.15 and an R$^2$ score of 0.82, setting a new benchmark in efficiency with 6.8 million parameters and only 8.49G Flops. Our model operates without pre-training, data augmentation, or ensemble methods, making it well-suited for real-time point-of-care ultrasound (PoCUS) applications. Our Code is publicly available on~\href{https://github.com/moeinheidari7829/Echo-E3Net}{\textcolor{magenta}{GitHub}}.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Radiotherapy Treatment Planning Using DOLA: A Privacy-Preserving, LLM-Based Optimization Agent</title>
<link>https://arxiv.org/abs/2503.17553</link>
<guid>https://arxiv.org/abs/2503.17553</guid>
<content:encoded><![CDATA[
<div> Radiotherapy treatment planning, DOLA, Large language model, Privacy protection, Reinforcement learning

:
DOLADOLALLaMa3.1chain-of-thoughtRAGRL1870816.4%RAGRAG19.8%RL0.4 <div>
arXiv:2503.17553v1 Announce Type: cross 
Abstract: Radiotherapy treatment planning is a complex and time-intensive process, often impacted by inter-planner variability and subjective decision-making. To address these challenges, we introduce Dose Optimization Language Agent (DOLA), an autonomous large language model (LLM)-based agent designed for optimizing radiotherapy treatment plans while rigorously protecting patient privacy. DOLA integrates the LLaMa3.1 LLM directly with a commercial treatment planning system, utilizing chain-of-thought prompting, retrieval-augmented generation (RAG), and reinforcement learning (RL). Operating entirely within secure local infrastructure, this agent eliminates external data sharing. We evaluated DOLA using a retrospective cohort of 18 prostate cancer patients prescribed 60 Gy in 20 fractions, comparing model sizes (8 billion vs. 70 billion parameters) and optimization strategies (No-RAG, RAG, and RAG+RL) over 10 planning iterations. The 70B model demonstrated significantly improved performance, achieving approximately 16.4% higher final scores than the 8B model. The RAG approach outperformed the No-RAG baseline by 19.8%, and incorporating RL accelerated convergence, highlighting the synergy of retrieval-based memory and reinforcement learning. Optimal temperature hyperparameter analysis identified 0.4 as providing the best balance between exploration and exploitation. This proof of concept study represents the first successful deployment of locally hosted LLM agents for autonomous optimization of treatment plans within a commercial radiotherapy planning system. By extending human-machine interaction through interpretable natural language reasoning, DOLA offers a scalable and privacy-conscious framework, with significant potential for clinical implementation and workflow improvement.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamics of Insect Paraintelligence: How a Mindless Colony of Ants Meaningfully Moves a Beetle</title>
<link>https://arxiv.org/abs/2503.18858</link>
<guid>https://arxiv.org/abs/2503.18858</guid>
<content:encoded><![CDATA[
<div> Vector Dissipation of Randomness (VDR) ant and beetleparaintelligence

<br /><br />:
VDRVDRVDRVDR <div>
arXiv:2503.18858v1 Announce Type: cross 
Abstract: In this work, a new concept called Vector Dissipation of Randomness (VDR) is developed and formalized. It describes the mechanism by which complex multicomponent systems transition from chaos to order through the filtering of random directions, accumulation of information in the environment, and self-organization of agents. VDR explains how individual random strategies can evolve into collective goaldirected behavior, leading to the emergence of an ordered structure without centralized control. To test the proposed model, a numerical simulation of the "ant and beetle" system was conducted, in which agents (ants) randomly choose movement directions, but through feedback mechanisms and filtering of weak strategies, they form a single coordinated vector of the beetles movement. VDR is a universal mechanism applicable to a wide range of self-organizing systems, including biological populations, decentralized technological networks, sociological processes, and artificial intelligence algorithms. For the first time, an equation of the normalized emergence function in the processing of vector dissipation of randomness in the Ant and Beetle system has been formulated. The concept of paraintelligence was introduced for the first time. Insect paraintelligence is interpreted as a rational functionality that is close to or equivalent to intelligent activity in the absence of reflexive consciousness and selfawareness.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-domain Random Pre-training with Prototypes for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2302.05614</link>
<guid>https://arxiv.org/abs/2302.05614</guid>
<content:encoded><![CDATA[
<div> :  (Reinforcement Learning, RL), , CRPTpro, , 

<br /><br />:
CRPTproCRPTproCRPTpro11/1254.5% <div>
arXiv:2302.05614v5 Announce Type: replace 
Abstract: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. Unsupervised cross-domain Reinforcement Learning (RL) pre-training shows great potential for challenging continuous visual control but poses a big challenge. In this paper, we propose \textbf{C}ross-domain \textbf{R}andom \textbf{P}re-\textbf{T}raining with \textbf{pro}totypes (CRPTpro), a novel, efficient, and effective self-supervised cross-domain RL pre-training framework. CRPTpro decouples data sampling from encoder pre-training, proposing decoupled random collection to easily and quickly generate a qualified cross-domain pre-training dataset. Moreover, a novel prototypical self-supervised algorithm is proposed to pre-train an effective visual encoder that is generic across different domains. Without finetuning, the cross-domain encoder can be implemented for challenging downstream tasks defined in different domains, either seen or unseen. Compared with recent advanced methods, CRPTpro achieves better performance on downstream policy learning without extra training on exploration agents for data collection, greatly reducing the burden of pre-training. We conduct extensive experiments across eight challenging continuous visual-control domains, including balance control, robot locomotion, and manipulation. CRPTpro significantly outperforms the next best Proto-RL(C) on 11/12 cross-domain downstream tasks with only 54.5\% wall-clock pre-training time, exhibiting state-of-the-art pre-training performance with greatly improved pre-training efficiency.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving robot navigation in crowded environments using intrinsic rewards</title>
<link>https://arxiv.org/abs/2302.06554</link>
<guid>https://arxiv.org/abs/2302.06554</guid>
<content:encoded><![CDATA[
<div> 

:
<br />
 <div>
arXiv:2302.06554v2 Announce Type: replace 
Abstract: Autonomous navigation in crowded environments is an open problem with many applications, essential for the coexistence of robots and humans in the smart cities of the future. In recent years, deep reinforcement learning approaches have proven to outperform model-based algorithms. Nevertheless, even though the results provided are promising, the works are not able to take advantage of the capabilities that their models offer. They usually get trapped in local optima in the training process, that prevent them from learning the optimal policy. They are not able to visit and interact with every possible state appropriately, such as with the states near the goal or near the dynamic obstacles. In this work, we propose using intrinsic rewards to balance between exploration and exploitation and explore depending on the uncertainty of the states instead of on the time the agent has been trained, encouraging the agent to get more curious about unknown states. We explain the benefits of the approach and compare it with other exploration algorithms that may be used for crowd navigation. Many simulation experiments are performed modifying several algorithms of the state-of-the-art, showing that the use of intrinsic rewards makes the robot learn faster and reach higher rewards and success rates (fewer collisions) in shorter navigation times, outperforming the state-of-the-art.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Safe Control Design and Probabilistic Safety Verification for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2303.12610</link>
<guid>https://arxiv.org/abs/2303.12610</guid>
<content:encoded><![CDATA[
<div> :  Barrier (CBF)<br /><br />:<br />
Barrier(QP)CBFsCBFCBFs <div>
arXiv:2303.12610v3 Announce Type: replace 
Abstract: We propose distributed iterative algorithms for safe control design and safety verification for networked multi-agent systems. These algorithms rely on distributing a control barrier function (CBF) related quadratic programming (QP) problem assuming the existence of CBFs. The proposed distributed algorithm addresses infeasibility issues of existing schemes via a cooperation mechanism between agents. The resulting control input is guaranteed to be optimal, and satisfies CBF constraints of all agents. Furthermore, a truncated algorithm is proposed to facilitate computational implementation. The performance of the truncated algorithm is evaluated using a distributed safety verification algorithm. The algorithm quantifies safety for multi-agent systems probabilistically by means of CBFs. Both upper and lower bounds on the probability of safety are obtained using the so called scenario approach. Both the scenario sampling and safety verification procedures are fully distributed. The efficacy of our algorithms is demonstrated by an example on multi-robot collision avoidance.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Bayesian Estimation in Sensor Networks: Consensus on Marginal Densities</title>
<link>https://arxiv.org/abs/2312.01227</link>
<guid>https://arxiv.org/abs/2312.01227</guid>
<content:encoded><![CDATA[
<div> 

:
LiDAR <div>
arXiv:2312.01227v3 Announce Type: replace 
Abstract: In this paper, we aim to design and analyze distributed Bayesian estimation algorithms for sensor networks. The challenges we address are to (i) derive a distributed provably-correct algorithm in the functional space of probability distributions over continuous variables, and (ii) leverage these results to obtain new distributed estimators restricted to subsets of variables observed by individual agents. This relates to applications such as cooperative localization and federated learning, where the data collected at any agent depends on a subset of all variables of interest. We present Bayesian density estimation algorithms using data from non-linear likelihoods at agents in centralized, distributed, and marginal distributed settings. After setting up a distributed estimation objective, we prove almost-sure convergence to the optimal set of pdfs at each agent. Then, we prove the same for a storage-aware algorithm estimating densities only over relevant variables at each agent. Finally, we present a Gaussian version of these algorithms and implement it in a mapping problem using variational inference to handle non-linear likelihood models associated with LiDAR sensing.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Multi-Agent Reinforcement Learning for Distributed Multi-Robot Problems</title>
<link>https://arxiv.org/abs/2401.00212</link>
<guid>https://arxiv.org/abs/2401.00212</guid>
<content:encoded><![CDATA[
<div> 

:
<br />
1. <br />
2. <br />
3. Actor-CriticRobotarium <div>
arXiv:2401.00212v3 Announce Type: replace 
Abstract: The networked nature of multi-robot systems presents challenges in the context of multi-agent reinforcement learning. Centralized control policies do not scale with increasing numbers of robots, whereas independent control policies do not exploit the information provided by other robots, exhibiting poor performance in cooperative-competitive tasks. In this work we propose a physics-informed reinforcement learning approach able to learn distributed multi-robot control policies that are both scalable and make use of all the available information to each robot. Our approach has three key characteristics. First, it imposes a port-Hamiltonian structure on the policy representation, respecting energy conservation properties of physical robot systems and the networked nature of robot team interactions. Second, it uses self-attention to ensure a sparse policy representation able to handle time-varying information at each robot from the interaction graph. Third, we present a soft actor-critic reinforcement learning algorithm parameterized by our self-attention port-Hamiltonian control policy, which accounts for the correlation among robots during training while overcoming the need of value function factorization. Extensive simulations in different multi-robot scenarios demonstrate the success of the proposed approach, surpassing previous multi-robot reinforcement learning solutions in scalability, while achieving similar or superior performance (with averaged cumulative reward up to x2 greater than the state-of-the-art with robot teams x6 larger than the number of robots at training time). We also validate our approach on multiple real robots in the Georgia Tech Robotarium under imperfect communication, demonstrating zero-shot sim-to-real transfer and scalability across number of robots.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open Models, Closed Minds? On Agents Capabilities in Mimicking Human Personalities through Open Large Language Models</title>
<link>https://arxiv.org/abs/2401.07115</link>
<guid>https://arxiv.org/abs/2401.07115</guid>
<content:encoded><![CDATA[
<div> : , LLM, , MBTI, Big Five

:
LLMsLLM12LLMMBTIBig Five<br />
1) LLM<br />
2) LLM<br />
3) LLMLLMNLP <div>
arXiv:2401.07115v3 Announce Type: replace 
Abstract: The emergence of unveiling human-like behaviors in Large Language Models (LLMs) has led to a closer connection between NLP and human psychology. Scholars have been studying the inherent personalities exhibited by LLMs and attempting to incorporate human traits and behaviors into them. However, these efforts have primarily focused on commercially-licensed LLMs, neglecting the widespread use and notable advancements seen in Open LLMs. This work aims to address this gap by employing a set of 12 LLM Agents based on the most representative Open models and subject them to a series of assessments concerning the Myers-Briggs Type Indicator (MBTI) test and the Big Five Inventory (BFI) test. Our approach involves evaluating the intrinsic personality traits of Open LLM agents and determining the extent to which these agents can mimic human personalities when conditioned by specific personalities and roles. Our findings unveil that $(i)$ each Open LLM agent showcases distinct human personalities; $(ii)$ personality-conditioned prompting produces varying effects on the agents, with only few successfully mirroring the imposed personality, while most of them being ``closed-minded'' (i.e., they retain their intrinsic traits); and $(iii)$ combining role and personality conditioning can enhance the agents' ability to mimic human personalities. Our work represents a step up in understanding the dense relationship between NLP and human psychology through the lens of Open LLMs.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Best Arm Identification with Resource Constraints</title>
<link>https://arxiv.org/abs/2402.19090</link>
<guid>https://arxiv.org/abs/2402.19090</guid>
<content:encoded><![CDATA[
<div> Best Arm Identification with Resource Constraints (BAIwRC)Successive Halving with Resource Rationing (SH-RR)

:<br />
(BAIwRC)Successive Halving with Resource Rationing (SH-RR) <div>
arXiv:2402.19090v2 Announce Type: replace 
Abstract: Motivated by the cost heterogeneity in experimentation across different alternatives, we study the Best Arm Identification with Resource Constraints (BAIwRC) problem. The agent aims to identify the best arm under resource constraints, where resources are consumed for each arm pull. We make two novel contributions. We design and analyze the Successive Halving with Resource Rationing algorithm (SH-RR). The SH-RR achieves a near-optimal non-asymptotic rate of convergence in terms of the probability of successively identifying an optimal arm. Interestingly, we identify a difference in convergence rates between the cases of deterministic and stochastic resource consumption.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning</title>
<link>https://arxiv.org/abs/2403.07376</link>
<guid>https://arxiv.org/abs/2403.07376</guid>
<content:encoded><![CDATA[
<div> : Vision-and-Language Navigation, , Navigational Chain-of-Thought, , 

<br /><br />:
Navigational Chain-of-Thought (NavCoT)VLNLLMsNavCoTLLMLLM1) 2) 3) LLMNavCoTVLNR2RRxRR4RNavCoTR2RGPT47%NavCoTLLMGitHub <div>
arXiv:2403.07376v2 Announce Type: replace 
Abstract: Vision-and-Language Navigation (VLN), as a crucial research problem of Embodied AI, requires an embodied agent to navigate through complex 3D environments following natural language instructions. Recent research has highlighted the promising capacity of large language models (LLMs) in VLN by improving navigational reasoning accuracy and interpretability. However, their predominant use in an offline manner usually suffers from substantial domain gap between the VLN task and the LLM training corpus. This paper introduces a novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill parameter-efficient in-domain training to enable self-guided navigational decision, leading to a significant mitigation of the domain gap in a cost-effective manner. Specifically, at each timestep, the LLM is prompted to forecast the navigational chain-of-thought by: 1) acting as a world model to imagine the next observation according to the instruction, 2) selecting the candidate observation that best aligns with the imagination, and 3) determining the action based on the reasoning from the prior steps. Through constructing formalized labels for training, the LLM can learn to generate desired and reasonable chain-of-thought outputs for improving the action decision. Experimental results across various training settings and popular VLN benchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room (R4R)) show the significant superiority of NavCoT over the direct action prediction variants. Through simple parameter-efficient finetuning, our NavCoT outperforms a recent GPT4-based approach with ~7% relative improvement on the R2R dataset. We believe that NavCoT will help unlock more task-adaptive and scalable LLM-based embodied agents, which are helpful for developing real-world robotics applications. Code is available at https://github.com/expectorlin/NavCoT.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>