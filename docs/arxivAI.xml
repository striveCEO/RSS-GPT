<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs</link>


<item>
<title>OneDSE: A Unified Microprocessor Metric Prediction and Design Space Exploration Framework</title>
<link>https://arxiv.org/abs/2505.03771</link>
<guid>https://arxiv.org/abs/2505.03771</guid>
<content:encoded><![CDATA[
arXiv:2505.03771v1 Announce Type: new 
Abstract: With the diminishing returns of Moore Law scaling and as power constraints become more impactful, processor designs rely on architectural innovation to achieve differentiating performance. Innovation complexity has increased the design space of modern high-performance processors. This work offers an efficient and novel design space exploration (DSE) solution to these challenges of modern CPU design. We identify three key challenges in past DSE approaches: (a) Metric prediction is slow and inaccurate for unseen workloads, microarchitectures, (b) Search is slow and inaccurate in CPU parameter space, and (c) A Single model is unable to learn the huge design space. We present OneDSE, a unified metric predictor and CPU parameter explorer to mitigate these challenges with three key techniques: (a) Transformer-based workload-Aware CPU DSE (TrACE) predictor that outperforms state-of-the-art ANN-based prediction methods by 2.75x and 6.12x with and without fine-tuning, respectively, on several benchmarks; (b) a novel metric space search approach that outperforms optimized metaheuristics by 1.19x while reducing search time by an order of magnitude; (c) MARL-based multi-agent framework that achieves a 10.6% reduction in prediction error compared to its non-MARL counterpart, enabling more accurate and efficient exploration of the CPU design space.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dragonfly: a modular deep reinforcement learning library</title>
<link>https://arxiv.org/abs/2505.03778</link>
<guid>https://arxiv.org/abs/2505.03778</guid>
<content:encoded><![CDATA[
arXiv:2505.03778v1 Announce Type: new 
Abstract: Dragonfly is a deep reinforcement learning library focused on modularity, in order to ease experimentation and developments. It relies on a json serialization that allows to swap building blocks and perform parameter sweep, while minimizing code maintenance. Some of its features are specifically designed for CPU-intensive environments, such as numerical simulations. Its performance on standard agents using common benchmarks compares favorably with the literature.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Insulin Resistance Prediction From Wearables and Routine Blood Biomarkers</title>
<link>https://arxiv.org/abs/2505.03784</link>
<guid>https://arxiv.org/abs/2505.03784</guid>
<content:encoded><![CDATA[
arXiv:2505.03784v1 Announce Type: new 
Abstract: Insulin resistance, a precursor to type 2 diabetes, is characterized by impaired insulin action in tissues. Current methods for measuring insulin resistance, while effective, are expensive, inaccessible, not widely available and hinder opportunities for early intervention. In this study, we remotely recruited the largest dataset to date across the US to study insulin resistance (N=1,165 participants, with median BMI=28 kg/m2, age=45 years, HbA1c=5.4%), incorporating wearable device time series data and blood biomarkers, including the ground-truth measure of insulin resistance, homeostatic model assessment for insulin resistance (HOMA-IR). We developed deep neural network models to predict insulin resistance based on readily available digital and blood biomarkers. Our results show that our models can predict insulin resistance by combining both wearable data and readily available blood biomarkers better than either of the two data sources separately (R2=0.5, auROC=0.80, Sensitivity=76%, and specificity 84%). The model showed 93% sensitivity and 95% adjusted specificity in obese and sedentary participants, a subpopulation most vulnerable to developing type 2 diabetes and who could benefit most from early intervention. Rigorous evaluation of model performance, including interpretability, and robustness, facilitates generalizability across larger cohorts, which is demonstrated by reproducing the prediction performance on an independent validation cohort (N=72 participants). Additionally, we demonstrated how the predicted insulin resistance can be integrated into a large language model agent to help understand and contextualize HOMA-IR values, facilitating interpretation and safe personalized recommendations. This work offers the potential for early detection of people at risk of type 2 diabetes and thereby facilitate earlier implementation of preventative strategies.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mAIstro: an open-source multi-agentic system for automated end-to-end development of radiomics and deep learning models for medical imaging</title>
<link>https://arxiv.org/abs/2505.03785</link>
<guid>https://arxiv.org/abs/2505.03785</guid>
<content:encoded><![CDATA[
arXiv:2505.03785v1 Announce Type: new 
Abstract: Agentic systems built on large language models (LLMs) offer promising capabilities for automating complex workflows in healthcare AI. We introduce mAIstro, an open-source, autonomous multi-agentic framework for end-to-end development and deployment of medical AI models. The system orchestrates exploratory data analysis, radiomic feature extraction, image segmentation, classification, and regression through a natural language interface, requiring no coding from the user. Built on a modular architecture, mAIstro supports both open- and closed-source LLMs, and was evaluated using a large and diverse set of prompts across 16 open-source datasets, covering a wide range of imaging modalities, anatomical regions, and data types. The agents successfully executed all tasks, producing interpretable outputs and validated models. This work presents the first agentic framework capable of unifying data analysis, AI model development, and inference across varied healthcare applications, offering a reproducible and extensible foundation for clinical and research AI integration. The code is available at: https://github.com/eltzanis/mAIstro
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator</title>
<link>https://arxiv.org/abs/2505.03786</link>
<guid>https://arxiv.org/abs/2505.03786</guid>
<content:encoded><![CDATA[
arXiv:2505.03786v1 Announce Type: new 
Abstract: Large Language Models (LLM) with reasoning capabilities offer a promising path for improving candidate evaluation in planning frameworks, but their relative performance against traditional non-reasoning models remains largely underexplored. In this study, we benchmark a distilled 1.5B parameter reasoning model (DeepSeek-R1) against several state-of-the-art non-reasoning LLMs within a generator-discriminator LLM planning framework for the text-to-SQL task. For this, we introduce a novel method for extracting soft scores from the chain-of-thought (CoT) outputs from reasoning that enables fine-grained ranking of candidates. Our central hypothesis is that reasoning models are more effective discriminators than non-reasoning LLMs. Our results show that distilled DeepSeek-R1-1.5B achieves up to $87\%$ higher F1 and $3.7\%$ better discrimination accuracy than CodeLlama-7B, as well as $3.7\%$ higher execution accuracy than CodeLlama-13B, despite having significantly fewer parameters. Furthermore, we find that there is a limit to the logical capabilities of reasoning models, and only providing more context or allowing more compute budget for reasoning is not enough to improve their discrimination performance. Finally, we demonstrate that, unlike non-reasoning LLMs, reasoning models find generation more challenging than discrimination and may underperform as generators compared to smaller non-reasoning LLMs. Our work highlights the potential of reasoning models as discriminators in agentic frameworks, far outweighing their capabilities as generators, offering insights into their optimal role within LLM planning infrastructures.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Online Tuning of VLM Agents via Counterfactual Soft Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.03792</link>
<guid>https://arxiv.org/abs/2505.03792</guid>
<content:encoded><![CDATA[
arXiv:2505.03792v1 Announce Type: new 
Abstract: Online fine-tuning vision-language model (VLM) agents with reinforcement learning (RL) has shown promise for equipping agents with multi-step, goal-oriented capabilities in dynamic environments. However, their open-ended textual action space and non-end-to-end nature of action generation present significant challenges to effective online exploration in RL, e.g., explosion of the exploration space. We propose a novel online fine-tuning method, Counterfactual Soft Reinforcement Learning (CoSo), better suited to the textual output space of VLM agents. Compared to prior methods that assign uniform uncertainty to all tokens, CoSo leverages counterfactual reasoning to dynamically assess the causal influence of individual tokens on post-processed actions. By prioritizing the exploration of action-critical tokens while reducing the impact of semantically redundant or low-impact tokens, CoSo enables a more targeted and efficient online rollout process. We provide theoretical analysis proving CoSo's convergence and policy improvement guarantees, and extensive empirical evaluations supporting CoSo's effectiveness. Our results across a diverse set of agent tasks, including Android device control, card gaming, and embodied AI, highlight its remarkable ability to enhance exploration efficiency and deliver consistent performance gains. The code is available at https://github.com/langfengQ/CoSo.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Human Behavior in a Strategic Network Game with Complex Group Dynamics</title>
<link>https://arxiv.org/abs/2505.03795</link>
<guid>https://arxiv.org/abs/2505.03795</guid>
<content:encoded><![CDATA[
arXiv:2505.03795v1 Announce Type: new 
Abstract: Human networks greatly impact important societal outcomes, including wealth and health inequality, poverty, and bullying. As such, understanding human networks is critical to learning how to promote favorable societal outcomes. As a step toward better understanding human networks, we compare and contrast several methods for learning models of human behavior in a strategic network game called the Junior High Game (JHG). These modeling methods differ with respect to the assumptions they use to parameterize human behavior (behavior vs. community-aware behavior) and the statistical moments they model (mean vs. distribution). Results show that the highest-performing method models the population's distribution rather than the mean and assumes humans use community-aware behavior rather than behavior matching. When applied to small societies (6-11 individuals), this learned model, called hCAB, closely mirrors the population dynamics of human groups (with some differences). Additionally, a user study reveals that human participants were unable to distinguish hCAB agents from other humans, thus illustrating that individual hCAB behavior plausibly mirrors human behavior in this strategic network game.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Behavioral Preferences of Cyber Adversaries Using Inverse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.03817</link>
<guid>https://arxiv.org/abs/2505.03817</guid>
<content:encoded><![CDATA[
arXiv:2505.03817v1 Announce Type: new 
Abstract: This paper presents a holistic approach to attacker preference modeling from system-level audit logs using inverse reinforcement learning (IRL). Adversary modeling is an important capability in cybersecurity that lets defenders characterize behaviors of potential attackers, which enables attribution to known cyber adversary groups. Existing approaches rely on documenting an ever-evolving set of attacker tools and techniques to track known threat actors. Although attacks evolve constantly, attacker behavioral preferences are intrinsic and less volatile. Our approach learns the behavioral preferences of cyber adversaries from forensics data on their tools and techniques. We model the attacker as an expert decision-making agent with unknown behavioral preferences situated in a computer host. We leverage attack provenance graphs of audit logs to derive a state-action trajectory of the attack. We test our approach on open datasets of audit logs containing real attack data. Our results demonstrate for the first time that low-level forensics data can automatically reveal an adversary's subjective preferences, which serves as an additional dimension to modeling and documenting cyber adversaries. Attackers' preferences tend to be invariant despite their different tools and indicate predispositions that are inherent to the attacker. As such, these inferred preferences can potentially serve as unique behavioral signatures of attackers and improve threat attribution.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Program Semantic Inequivalence Game with Large Language Models</title>
<link>https://arxiv.org/abs/2505.03818</link>
<guid>https://arxiv.org/abs/2505.03818</guid>
<content:encoded><![CDATA[
arXiv:2505.03818v1 Announce Type: new 
Abstract: Large Language Models (LLMs) can achieve strong performance on everyday coding tasks, but they can fail on complex tasks that require non-trivial reasoning about program semantics. Finding training examples to teach LLMs to solve these tasks can be challenging.
  In this work, we explore a method to synthetically generate code reasoning training data based on a semantic inequivalence game SInQ: a generator agent creates program variants that are semantically distinct, derived from a dataset of real-world programming tasks, while an evaluator agent has to identify input examples that cause the original programs and the generated variants to diverge in their behaviour, with the agents training each other semi-adversarially. We prove that this setup enables theoretically unlimited improvement through self-play in the limit of infinite computational resources.
  We evaluated our approach on multiple code generation and understanding benchmarks, including cross-language vulnerability detection (Lu et al., 2021), where our method improves vulnerability detection in C/C++ code despite being trained exclusively on Python code, and the challenging Python builtin identifier swap benchmark (Miceli-Barone et al., 2023), showing that whereas modern LLMs still struggle with this benchmark, our approach yields substantial improvements.
  We release the code needed to replicate the experiments, as well as the generated synthetic data, which can be used to fine-tune LLMs.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Active Inference Model of Covert and Overt Visual Attention</title>
<link>https://arxiv.org/abs/2505.03856</link>
<guid>https://arxiv.org/abs/2505.03856</guid>
<content:encoded><![CDATA[
arXiv:2505.03856v1 Announce Type: new 
Abstract: The ability to selectively attend to relevant stimuli while filtering out distractions is essential for agents that process complex, high-dimensional sensory input. This paper introduces a model of covert and overt visual attention through the framework of active inference, utilizing dynamic optimization of sensory precisions to minimize free-energy. The model determines visual sensory precisions based on both current environmental beliefs and sensory input, influencing attentional allocation in both covert and overt modalities. To test the effectiveness of the model, we analyze its behavior in the Posner cueing task and a simple target focus task using two-dimensional(2D) visual data. Reaction times are measured to investigate the interplay between exogenous and endogenous attention, as well as valid and invalid cueing. The results show that exogenous and valid cues generally lead to faster reaction times compared to endogenous and invalid cues. Furthermore, the model exhibits behavior similar to inhibition of return, where previously attended locations become suppressed after a specific cue-target onset asynchrony interval. Lastly, we investigate different aspects of overt attention and show that involuntary, reflexive saccades occur faster than intentional ones, but at the expense of adaptability.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Glue-Code to Protocols: A Critical Analysis of A2A and MCP Integration for Scalable Agent Systems</title>
<link>https://arxiv.org/abs/2505.03864</link>
<guid>https://arxiv.org/abs/2505.03864</guid>
<content:encoded><![CDATA[
arXiv:2505.03864v1 Announce Type: new 
Abstract: Artificial intelligence is rapidly evolving towards multi-agent systems where numerous AI agents collaborate and interact with external tools. Two key open standards, Google's Agent to Agent (A2A) protocol for inter-agent communication and Anthropic's Model Context Protocol (MCP) for standardized tool access, promise to overcome the limitations of fragmented, custom integration approaches. While their potential synergy is significant, this paper argues that effectively integrating A2A and MCP presents unique, emergent challenges at their intersection, particularly concerning semantic interoperability between agent tasks and tool capabilities, the compounded security risks arising from combined discovery and execution, and the practical governance required for the envisioned "Agent Economy". This work provides a critical analysis, moving beyond a survey to evaluate the practical implications and inherent difficulties of combining these horizontal and vertical integration standards. We examine the benefits (e.g., specialization, scalability) while critically assessing their dependencies and trade-offs in an integrated context. We identify key challenges increased by the integration, including novel security vulnerabilities, privacy complexities, debugging difficulties across protocols, and the need for robust semantic negotiation mechanisms. In summary, A2A+MCP offers a vital architectural foundation, but fully realizing its potential requires substantial advancements to manage the complexities of their combined operation.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARCO: A Multi-Agent System for Optimizing HPC Code Generation Using Large Language Models</title>
<link>https://arxiv.org/abs/2505.03906</link>
<guid>https://arxiv.org/abs/2505.03906</guid>
<content:encoded><![CDATA[
arXiv:2505.03906v1 Announce Type: new 
Abstract: Large language models (LLMs) have transformed software development through code generation capabilities, yet their effectiveness for high-performance computing (HPC) remains limited. HPC code requires specialized optimizations for parallelism, memory efficiency, and architecture-specific considerations that general-purpose LLMs often overlook. We present MARCO (Multi-Agent Reactive Code Optimizer), a novel framework that enhances LLM-generated code for HPC through a specialized multi-agent architecture. MARCO employs separate agents for code generation and performance evaluation, connected by a feedback loop that progressively refines optimizations. A key innovation is MARCO's web-search component that retrieves real-time optimization techniques from recent conference proceedings and research publications, bridging the knowledge gap in pre-trained LLMs. Our extensive evaluation on the LeetCode 75 problem set demonstrates that MARCO achieves a 14.6% average runtime reduction compared to Claude 3.5 Sonnet alone, while the integration of the web-search component yields a 30.9% performance improvement over the base MARCO system. These results highlight the potential of multi-agent systems to address the specialized requirements of high-performance code generation, offering a cost-effective alternative to domain-specific model fine-tuning.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Omnidirectional vision sensors based on catadioptric systems with discrete infrared photoreceptors for swarm robotics</title>
<link>https://arxiv.org/abs/2505.03920</link>
<guid>https://arxiv.org/abs/2505.03920</guid>
<content:encoded><![CDATA[
arXiv:2505.03920v1 Announce Type: new 
Abstract: In this work, we fabricated and studied two designs for omnidirectional vision sensors for swarm robotics, based on catadioptric systems consisting of a mirror with rotational symmetry, eight discrete infrared photodiodes and a single LED, in order to provide localization and navigation abilities for mobile robotic agents. We considered two arrangements for the photodiodes: one in which they point upward into the mirror, and one in which they point outward, perpendicular to the mirror. To determine which design offers a better field of view on the plane, as well as detection of distance and orientation between two agents, we developed a test rail with three degrees of freedom to experimentally and systematically measure the signal registered by the photodiodes of a given sensor (in a single readout) from the light emitted by another as functions of the distance and orientation. Afterwards, we processed and analyzed the experimental data to develop mathematical models for the mean response of a photodiode in each design. Finally, by numerically inverting the models, we compared the two designs in terms of their accuracy. Our results show that the design with the photodiodes pointing upward resolves better the distance, while the other resolves better the orientation of the emitting agent, both providing an omnidirectional field of view.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAML: Dynamic Goal Recognition As Metric Learning</title>
<link>https://arxiv.org/abs/2505.03941</link>
<guid>https://arxiv.org/abs/2505.03941</guid>
<content:encoded><![CDATA[
arXiv:2505.03941v1 Announce Type: new 
Abstract: Goal Recognition (GR) is the problem of recognizing an agent's objectives based on observed actions. Recent data-driven approaches for GR alleviate the need for costly, manually crafted domain models. However, these approaches can only reason about a pre-defined set of goals, and time-consuming training is needed for new emerging goals. To keep this model-learning automated while enabling quick adaptation to new goals, this paper introduces GRAML: Goal Recognition As Metric Learning. GRAML uses a Siamese network to treat GR as a deep metric learning task, employing an RNN that learns a metric over an embedding space, where the embeddings for observation traces leading to different goals are distant, and embeddings of traces leading to the same goals are close. This metric is especially useful when adapting to new goals, even if given just one example observation trace per goal. Evaluated on a versatile set of environments, GRAML shows speed, flexibility, and runtime improvements over the state-of-the-art GR while maintaining accurate recognition.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frog Soup: Zero-Shot, In-Context, and Sample-Efficient Frogger Agents</title>
<link>https://arxiv.org/abs/2505.03947</link>
<guid>https://arxiv.org/abs/2505.03947</guid>
<content:encoded><![CDATA[
arXiv:2505.03947v1 Announce Type: new 
Abstract: One of the primary aspirations in reinforcement learning research is developing general-purpose agents capable of rapidly adapting to and mastering novel tasks. While RL gaming agents have mastered many Atari games, they remain slow and costly to train for each game. In this work, we demonstrate that latest reasoning LLMs with out-of-domain RL post-training can play a challenging Atari game called Frogger under a zero-shot setting. We then investigate the effect of in-context learning and the amount of reasoning effort on LLM performance. Lastly, we demonstrate a way to bootstrap traditional RL method with LLM demonstrations, which significantly improves their performance and sample efficiency. Our implementation is open sourced at https://github.com/AlienKevin/frogger.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Q-Network (DQN) multi-agent reinforcement learning (MARL) for Stock Trading</title>
<link>https://arxiv.org/abs/2505.03949</link>
<guid>https://arxiv.org/abs/2505.03949</guid>
<content:encoded><![CDATA[
arXiv:2505.03949v1 Announce Type: new 
Abstract: This project addresses the challenge of automated stock trading, where traditional methods and direct reinforcement learning (RL) struggle with market noise, complexity, and generalization. Our proposed solution is an integrated deep learning framework combining a Convolutional Neural Network (CNN) to identify patterns in technical indicators formatted as images, a Long Short-Term Memory (LSTM) network to capture temporal dependencies across both price history and technical indicators, and a Deep Q-Network (DQN) agent which learns the optimal trading policy (buy, sell, hold) based on the features extracted by the CNN and LSTM.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete</title>
<link>https://arxiv.org/abs/2505.03961</link>
<guid>https://arxiv.org/abs/2505.03961</guid>
<content:encoded><![CDATA[
arXiv:2505.03961v1 Announce Type: new 
Abstract: According to Yuval Noah Harari, large-scale human cooperation is driven by shared narratives that encode common beliefs and values. This study explores whether such narratives can similarly nudge LLM agents toward collaboration. We use a finitely repeated public goods game in which LLM agents choose either cooperative or egoistic spending strategies. We prime agents with stories highlighting teamwork to different degrees and test how this influences negotiation outcomes. Our experiments explore four questions:(1) How do narratives influence negotiation behavior? (2) What differs when agents share the same story versus different ones? (3) What happens when the agent numbers grow? (4) Are agents resilient against self-serving negotiators? We find that story-based priming significantly affects negotiation strategies and success rates. Common stories improve collaboration, benefiting each agent. By contrast, priming agents with different stories reverses this effect, and those agents primed toward self-interest prevail. We hypothesize that these results carry implications for multi-agent system design and AI alignment.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale</title>
<link>https://arxiv.org/abs/2505.03973</link>
<guid>https://arxiv.org/abs/2505.03973</guid>
<content:encoded><![CDATA[
arXiv:2505.03973v1 Announce Type: new 
Abstract: LLM-based optimization has shown remarkable potential in enhancing agentic systems. However, the conventional approach of prompting LLM optimizer with the whole training trajectories on training dataset in a single pass becomes untenable as datasets grow, leading to context window overflow and degraded pattern recognition. To address these challenges, we propose Fine-Grained Optimization (FGO), a scalable framework that divides large optimization tasks into manageable subsets, performs targeted optimizations, and systematically combines optimized components through progressive merging. Evaluation across ALFWorld, LogisticsQA, and GAIA benchmarks demonstrate that FGO outperforms existing approaches by 1.6-8.6% while reducing average prompt token consumption by 56.3%. Our framework provides a practical solution for scaling up LLM-based optimization of increasingly sophisticated agent systems. Further analysis demonstrates that FGO achieves the most consistent performance gain in all training dataset sizes, showcasing its scalability and efficiency.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An alignment safety case sketch based on debate</title>
<link>https://arxiv.org/abs/2505.03989</link>
<guid>https://arxiv.org/abs/2505.03989</guid>
<content:encoded><![CDATA[
arXiv:2505.03989v1 Announce Type: new 
Abstract: If AI systems match or exceed human capabilities on a wide range of tasks, it may become difficult for humans to efficiently judge their actions -- making it hard to use human feedback to steer them towards desirable traits. One proposed solution is to leverage another superhuman system to point out flaws in the system's outputs via a debate. This paper outlines the value of debate for AI safety, as well as the assumptions and further research required to make debate work. It does so by sketching an ``alignment safety case'' -- an argument that an AI system will not autonomously take actions which could lead to egregious harm, despite being able to do so. The sketch focuses on the risk of an AI R\&amp;D agent inside an AI company sabotaging research, for example by producing false results. To prevent this, the agent is trained via debate, subject to exploration guarantees, to teach the system to be honest. Honesty is maintained throughout deployment via online training. The safety case rests on four key claims: (1) the agent has become good at the debate game, (2) good performance in the debate game implies that the system is mostly honest, (3) the system will not become significantly less honest during deployment, and (4) the deployment context is tolerant of some errors. We identify open research problems that, if solved, could render this a compelling argument that an AI system is safe.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLOT: Structuring the Output of Large Language Models</title>
<link>https://arxiv.org/abs/2505.04016</link>
<guid>https://arxiv.org/abs/2505.04016</guid>
<content:encoded><![CDATA[
arXiv:2505.04016v1 Announce Type: new 
Abstract: Structured outputs are essential for large language models (LLMs) in critical applications like agents and information extraction. Despite their capabilities, LLMs often generate outputs that deviate from predefined schemas, significantly hampering reliable application development. We present SLOT (Structured LLM Output Transformer), a model-agnostic approach that transforms unstructured LLM outputs into precise structured formats. While existing solutions predominantly rely on constrained decoding techniques or are tightly coupled with specific models, SLOT employs a fine-tuned lightweight language model as a post-processing layer, achieving flexibility across various LLMs and schema specifications. We introduce a systematic pipeline for data curation and synthesis alongside a formal evaluation methodology that quantifies both schema accuracy and content fidelity. Our results demonstrate that fine-tuned Mistral-7B model with constrained decoding achieves near perfect schema accuracy (99.5%) and content similarity (94.0%), outperforming Claude-3.5-Sonnet by substantial margins (+25 and +20 percentage points, respectively). Notably, even compact models like Llama-3.2-1B can match or exceed the structured output capabilities of much larger proprietary models when equipped with SLOT, enabling reliable structured generation in resource-constrained environments.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Appeal and Scope of Misinformation Spread by AI Agents and Humans</title>
<link>https://arxiv.org/abs/2505.04028</link>
<guid>https://arxiv.org/abs/2505.04028</guid>
<content:encoded><![CDATA[
arXiv:2505.04028v1 Announce Type: new 
Abstract: This work examines the influence of misinformation and the role of AI agents, called bots, on social network platforms. To quantify the impact of misinformation, it proposes two new metrics based on attributes of tweet engagement and user network position: Appeal, which measures the popularity of the tweet, and Scope, which measures the potential reach of the tweet. In addition, it analyzes 5.8 million misinformation tweets on the COVID-19 vaccine discourse over three time periods: Pre-Vaccine, Vaccine Launch, and Post-Vaccine. Results show that misinformation was more prevalent during the first two periods. Human-generated misinformation tweets tend to have higher appeal and scope compared to bot-generated ones. Tweedie regression analysis reveals that human-generated misinformation tweets were most concerning during Vaccine Launch week, whereas bot-generated misinformation reached its highest appeal and scope during the Pre-Vaccine period.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identification and Optimization of Redundant Code Using Large Language Models</title>
<link>https://arxiv.org/abs/2505.04040</link>
<guid>https://arxiv.org/abs/2505.04040</guid>
<content:encoded><![CDATA[
arXiv:2505.04040v1 Announce Type: new 
Abstract: Redundant code is a persistent challenge in software development that makes systems harder to maintain, scale, and update. It adds unnecessary complexity, hinders bug fixes, and increases technical debt. Despite their impact, removing redundant code manually is risky and error-prone, often introducing new bugs or missing dependencies. While studies highlight the prevalence and negative impact of redundant code, little focus has been given to Artificial Intelligence (AI) system codebases and the common patterns that cause redundancy. Additionally, the reasons behind developers unintentionally introducing redundant code remain largely unexplored. This research addresses these gaps by leveraging large language models (LLMs) to automatically detect and optimize redundant code in AI projects. Our research aims to identify recurring patterns of redundancy and analyze their underlying causes, such as outdated practices or insufficient awareness of best coding principles. Additionally, we plan to propose an LLM agent that will facilitate the detection and refactoring of redundancies on a large scale while preserving original functionality. This work advances the application of AI in identifying and optimizing redundant code, ultimately helping developers maintain cleaner, more readable, and scalable codebases.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delegation and Participation in Decentralized Governance: An Epistemic View</title>
<link>https://arxiv.org/abs/2505.04136</link>
<guid>https://arxiv.org/abs/2505.04136</guid>
<content:encoded><![CDATA[
arXiv:2505.04136v1 Announce Type: new 
Abstract: We develop and apply epistemic tests to various decentralized governance methods as well as to study the impact of participation. These tests probe the ability to reach a correct outcome when there is one. We find that partial abstention is a strong governance method from an epistemic standpoint compared to alternatives such as various forms of ``transfer delegation" in which voters explicitly transfer some or all of their voting rights to others. We make a stronger case for multi-step transfer delegation than is present in previous work but also demonstrate that transfer delegation has inherent epistemic weaknesses. We show that enhanced direct participation, voters exercising their own voting rights, can have a variety of epistemic impacts, some very negative. We identify governance conditions under which additional direct participation is guaranteed to do no epistemic harm and is likely to increase the probability of making correct decisions. In light of the epistemic challenges of voting-based decentralized governance, we consider the possible supplementary use of prediction markets, auctions, and AI agents to improve outcomes. All these results are significant because epistemic performance matters if entities such as DAOs (decentralized autonomous organizations) wish to compete with organizations that are more centralized.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization of Infectious Disease Intervention Measures Based on Reinforcement Learning - Empirical analysis based on UK COVID-19 epidemic data</title>
<link>https://arxiv.org/abs/2505.04161</link>
<guid>https://arxiv.org/abs/2505.04161</guid>
<content:encoded><![CDATA[
arXiv:2505.04161v1 Announce Type: new 
Abstract: Globally, the outbreaks of infectious diseases have exerted an extremely profound and severe influence on health security and the economy. During the critical phases of epidemics, devising effective intervention measures poses a significant challenge to both the academic and practical arenas. There is numerous research based on reinforcement learning to optimize intervention measures of infectious diseases. Nevertheless, most of these efforts have been confined within the differential equation based on infectious disease models. Although a limited number of studies have incorporated reinforcement learning methodologies into individual-based infectious disease models, the models employed therein have entailed simplifications and limitations, rendering it incapable of modeling the complexity and dynamics inherent in infectious disease transmission. We establish a decision-making framework based on an individual agent-based transmission model, utilizing reinforcement learning to continuously explore and develop a strategy function. The framework's validity is verified through both experimental and theoretical approaches. Covasim, a detailed and widely used agent-based disease transmission model, was modified to support reinforcement learning research. We conduct an exhaustive exploration of the application efficacy of multiple algorithms across diverse action spaces. Furthermore, we conduct an innovative preliminary theoretical analysis concerning the issue of "time coverage". The results of the experiment robustly validate the effectiveness and feasibility of the methodological framework of this study. The coping strategies gleaned therefrom prove highly efficacious in suppressing the expansion of the epidemic scale and safeguarding the stability of the economic system, thereby providing crucial reference perspectives for the formulation of global public health security strategies.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trajectory Entropy Reinforcement Learning for Predictable and Robust Control</title>
<link>https://arxiv.org/abs/2505.04193</link>
<guid>https://arxiv.org/abs/2505.04193</guid>
<content:encoded><![CDATA[
arXiv:2505.04193v1 Announce Type: new 
Abstract: Simplicity is a critical inductive bias for designing data-driven controllers, especially when robustness is important. Despite the impressive results of deep reinforcement learning in complex control tasks, it is prone to capturing intricate and spurious correlations between observations and actions, leading to failure under slight perturbations to the environment. To tackle this problem, in this work we introduce a novel inductive bias towards simple policies in reinforcement learning. The simplicity inductive bias is introduced by minimizing the entropy of entire action trajectories, corresponding to the number of bits required to describe information in action trajectories after the agent observes state trajectories. Our reinforcement learning agent, Trajectory Entropy Reinforcement Learning, is optimized to minimize the trajectory entropy while maximizing rewards. We show that the trajectory entropy can be effectively estimated by learning a variational parameterized action prediction model, and use the prediction model to construct an information-regularized reward function. Furthermore, we construct a practical algorithm that enables the joint optimization of models, including the policy and the prediction model. Experimental evaluations on several high-dimensional locomotion tasks show that our learned policies produce more cyclical and consistent action trajectories, and achieve superior performance, and robustness to noise and dynamic changes than the state-of-the-art.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoPatch: Multi-Agent Framework for Patching Real-World CVE Vulnerabilities</title>
<link>https://arxiv.org/abs/2505.04195</link>
<guid>https://arxiv.org/abs/2505.04195</guid>
<content:encoded><![CDATA[
arXiv:2505.04195v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have emerged as promising tools in software development, enabling automated code generation and analysis. However, their knowledge is limited to a fixed cutoff date, making them prone to generating code vulnerable to newly disclosed CVEs. Frequent fine-tuning with new CVE sets is costly, and existing LLM-based approaches focus on oversimplified CWE examples and require providing explicit bug locations to LLMs, limiting their ability to patch complex real-world vulnerabilities. To address these limitations, we propose AutoPatch, a multi-agent framework designed to patch vulnerable LLM-generated code, particularly those introduced after the LLMs' knowledge cutoff. AutoPatch integrates Retrieval-Augmented Generation (RAG) with a structured database of recently disclosed vulnerabilities, comprising 525 code snippets derived from 75 high-severity CVEs across real-world systems such as the Linux kernel and Chrome. AutoPatch combines semantic and taint analysis to identify the most relevant CVE and leverages enhanced Chain-of-Thought (CoT) reasoning to construct enriched prompts for verification and patching. Our unified similarity model, which selects the most relevant vulnerabilities, achieves 90.4 percent accuracy in CVE matching. AutoPatch attains 89.5 percent F1-score for vulnerability verification and 95.0 percent accuracy in patching, while being over 50x more cost-efficient than traditional fine-tuning approaches.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving in Smart Intersections</title>
<link>https://arxiv.org/abs/2505.04231</link>
<guid>https://arxiv.org/abs/2505.04231</guid>
<content:encoded><![CDATA[
arXiv:2505.04231v1 Announce Type: new 
Abstract: Unsignalized intersections pose significant safety and efficiency challenges due to complex traffic flows. This paper proposes a novel roadside unit (RSU)-centric cooperative driving system leveraging global perception and vehicle-to-infrastructure (V2I) communication. The core of the system is an RSU-based decision-making module using a two-stage hybrid reinforcement learning (RL) framework. At first, policies are pre-trained offline using conservative Q-learning (CQL) combined with behavior cloning (BC) on collected dataset. Subsequently, these policies are fine-tuned in the simulation using multi-agent proximal policy optimization (MAPPO), aligned with a self-attention mechanism to effectively solve inter-agent dependencies. RSUs perform real-time inference based on the trained models to realize vehicle control via V2I communications. Extensive experiments in CARLA environment demonstrate high effectiveness of the proposed system, by: \textit{(i)} achieving failure rates below 0.03\% in coordinating three connected and autonomous vehicles (CAVs) through complex intersection scenarios, significantly outperforming the traditional Autoware control method, and \textit{(ii)} exhibiting strong robustness across varying numbers of controlled agents and shows promising generalization capabilities on other maps.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Facilitating Trustworthy Human-Agent Collaboration in LLM-based Multi-Agent System oriented Software Engineering</title>
<link>https://arxiv.org/abs/2505.04251</link>
<guid>https://arxiv.org/abs/2505.04251</guid>
<content:encoded><![CDATA[
arXiv:2505.04251v1 Announce Type: new 
Abstract: Multi-agent autonomous systems (MAS) are better at addressing challenges that spans across multiple domains than singular autonomous agents. This holds true within the field of software engineering (SE) as well. The state-of-the-art research on MAS within SE focuses on integrating LLMs at the core of autonomous agents to create LLM-based multi-agent autonomous (LMA) systems. However, the introduction of LMA systems into SE brings a plethora of challenges. One of the major challenges is the strategic allocation of tasks between humans and the LMA system in a trustworthy manner. To address this challenge, a RACI-based framework is proposed in this work in progress article, along with implementation guidelines and an example implementation of the framework. The proposed framework can facilitate efficient collaboration, ensure accountability, and mitigate potential risks associated with LLM-driven automation while aligning with the Trustworthy AI guidelines. The future steps for this work delineating the planned empirical validation method are also presented.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CompileAgent: Automated Real-World Repo-Level Compilation with Tool-Integrated LLM-based Agent System</title>
<link>https://arxiv.org/abs/2505.04254</link>
<guid>https://arxiv.org/abs/2505.04254</guid>
<content:encoded><![CDATA[
arXiv:2505.04254v1 Announce Type: new 
Abstract: With open-source projects growing in size and complexity, manual compilation becomes tedious and error-prone, highlighting the need for automation to improve efficiency and accuracy. However, the complexity of compilation instruction search and error resolution makes automatic compilation challenging. Inspired by the success of LLM-based agents in various fields, we propose CompileAgent, the first LLM-based agent framework dedicated to repo-level compilation. CompileAgent integrates five tools and a flow-based agent strategy, enabling interaction with software artifacts for compilation instruction search and error resolution. To measure the effectiveness of our method, we design a public repo-level benchmark CompileAgentBench, and we also design two baselines for comparison by combining two compilation-friendly schemes. The performance on this benchmark shows that our method significantly improves the compilation success rate, ranging from 10% to 71%. Meanwhile, we evaluate the performance of CompileAgent under different agent strategies and verify the effectiveness of the flow-based strategy. Additionally, we emphasize the scalability of CompileAgent, further expanding its application prospects.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PPO-ACT: Proximal Policy Optimization with Adversarial Curriculum Transfer for Spatial Public Goods Games</title>
<link>https://arxiv.org/abs/2505.04302</link>
<guid>https://arxiv.org/abs/2505.04302</guid>
<content:encoded><![CDATA[
arXiv:2505.04302v1 Announce Type: new 
Abstract: This study investigates cooperation evolution mechanisms in the spatial public goods game. A novel deep reinforcement learning framework, Proximal Policy Optimization with Adversarial Curriculum Transfer (PPO-ACT), is proposed to model agent strategy optimization in dynamic environments. Traditional evolutionary game models frequently exhibit limitations in modeling long-term decision-making processes. Deep reinforcement learning effectively addresses this limitation by bridging policy gradient methods with evolutionary game theory. Our study pioneers the application of proximal policy optimization's continuous strategy optimization capability to public goods games through a two-stage adversarial curriculum transfer training paradigm. The experimental results show that PPO-ACT performs better in critical enhancement factor regimes. Compared to conventional standard proximal policy optimization methods, Q-learning and Fermi update rules, achieve earlier cooperation phase transitions and maintain stable cooperative equilibria. This framework exhibits better robustness when handling challenging scenarios like all-defector initial conditions. Systematic comparisons reveal the unique advantage of policy gradient methods in population-scale cooperation, i.e., achieving spatiotemporal payoff coordination through value function propagation. Our work provides a new computational framework for studying cooperation emergence in complex systems, algorithmically validating the punishment promotes cooperation hypothesis while offering methodological insights for multi-agent system strategy design.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.04317</link>
<guid>https://arxiv.org/abs/2505.04317</guid>
<content:encoded><![CDATA[
arXiv:2505.04317v1 Announce Type: new 
Abstract: In this paper, we tackle the problem of learning to play 3v3 multi-drone volleyball, a new embodied competitive task that requires both high-level strategic coordination and low-level agile control. The task is turn-based, multi-agent, and physically grounded, posing significant challenges due to its long-horizon dependencies, tight inter-agent coupling, and the underactuated dynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play (HCSP), a hierarchical reinforcement learning framework that separates centralized high-level strategic decision-making from decentralized low-level motion control. We design a three-stage population-based training pipeline to enable both strategy and skill to emerge from scratch without expert demonstrations: (I) training diverse low-level skills, (II) learning high-level strategy via self-play with fixed low-level controllers, and (III) joint fine-tuning through co-self-play. Experiments show that HCSP achieves superior performance, outperforming non-hierarchical self-play and rule-based hierarchical baselines with an average 82.9\% win rate and a 71.5\% win rate against the two-stage variant. Moreover, co-self-play leads to emergent team behaviors such as role switching and coordinated formations, demonstrating the effectiveness of our hierarchical design and training scheme.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.04339</link>
<guid>https://arxiv.org/abs/2505.04339</guid>
<content:encoded><![CDATA[
arXiv:2505.04339v1 Announce Type: new 
Abstract: DBSCAN, a well-known density-based clustering algorithm, has gained widespread popularity and usage due to its effectiveness in identifying clusters of arbitrary shapes and handling noisy data. However, it encounters challenges in producing satisfactory cluster results when confronted with datasets of varying density scales, a common scenario in real-world applications. In this paper, we propose a novel Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning cluster framework, namely AR-DBSCAN. First, we model the initial dataset as a two-level encoding tree and categorize the data vertices into distinct density partitions according to the information uncertainty determined in the encoding tree. Each partition is then assigned to an agent to find the best clustering parameters without manual assistance. The allocation is density-adaptive, enabling AR-DBSCAN to effectively handle diverse density distributions within the dataset by utilizing distinct agents for different partitions. Second, a multi-agent deep reinforcement learning guided automatic parameter searching process is designed. The process of adjusting the parameter search direction by perceiving the clustering environment is modeled as a Markov decision process. Using a weakly-supervised reward training policy network, each agent adaptively learns the optimal clustering parameters by interacting with the clusters. Third, a recursive search mechanism adaptable to the data's scale is presented, enabling efficient and controlled exploration of large parameter spaces. Extensive experiments are conducted on nine artificial datasets and a real-world dataset. The results of offline and online tasks show that AR-DBSCAN not only improves clustering accuracy by up to 144.1% and 175.3% in the NMI and ARI metrics, respectively, but also is capable of robustly finding dominant parameters.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resist Platform-Controlled AI Agents and Champion User-Centric Agent Advocates</title>
<link>https://arxiv.org/abs/2505.04345</link>
<guid>https://arxiv.org/abs/2505.04345</guid>
<content:encoded><![CDATA[
arXiv:2505.04345v1 Announce Type: new 
Abstract: Language model agents could reshape how users navigate and act in digital environments. If controlled by platform companies -- either those that already dominate online search, communication, and commerce, or those vying to replace them -- platform agents could intensify surveillance, exacerbate user lock-in, and further entrench the incumbent digital giants. This position paper argues that to resist the undesirable effects of platform agents, we should champion agent advocates -- agents that are controlled by users, serve the interests of users, and preserve user autonomy and choice. We identify key interventions to enable agent advocates: ensuring public access to compute, developing interoperability protocols and safety standards, and implementing appropriate market regulations.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLMs' Swarm intelligence</title>
<link>https://arxiv.org/abs/2505.04364</link>
<guid>https://arxiv.org/abs/2505.04364</guid>
<content:encoded><![CDATA[
arXiv:2505.04364v1 Announce Type: new 
Abstract: Large Language Models (LLMs) show potential for complex reasoning, yet their capacity for emergent coordination in Multi-Agent Systems (MAS) when operating under strict constraints-such as limited local perception and communication, characteristic of natural swarms-remains largely unexplored, particularly concerning the nuances of swarm intelligence. Existing benchmarks often do not fully capture the unique challenges of decentralized coordination that arise when agents operate with incomplete spatio-temporal information. To bridge this gap, we introduce SwarmBench, a novel benchmark designed to systematically evaluate the swarm intelligence capabilities of LLMs acting as decentralized agents. SwarmBench features five foundational MAS coordination tasks within a configurable 2D grid environment, forcing agents to rely primarily on local sensory input (k x k view) and local communication. We propose metrics for coordination effectiveness and analyze emergent group dynamics. Evaluating several leading LLMs in a zero-shot setting, we find significant performance variations across tasks, highlighting the difficulties posed by local information constraints. While some coordination emerges, results indicate limitations in robust planning and strategy formation under uncertainty in these decentralized scenarios. Assessing LLMs under swarm-like conditions is crucial for realizing their potential in future decentralized systems. We release SwarmBench as an open, extensible toolkit-built upon a customizable and scalable physical system with defined mechanical properties. It provides environments, prompts, evaluation scripts, and the comprehensive experimental datasets generated, aiming to foster reproducible research into LLM-based MAS coordination and the theoretical underpinnings of Embodied MAS. Our code repository is available at https://github.com/x66ccff/swarmbench.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending a Quantum Reinforcement Learning Exploration Policy with Flags to Connect Four</title>
<link>https://arxiv.org/abs/2505.04371</link>
<guid>https://arxiv.org/abs/2505.04371</guid>
<content:encoded><![CDATA[
arXiv:2505.04371v1 Announce Type: new 
Abstract: Action selection based on flags is a Reinforcement Learning (RL) exploration policy that improves the exploration of the state space through the use of flags, which can identify the most promising actions to take in each state. The quantum counterpart of this exploration policy further improves upon this by taking advantage of a quadratic speedup for sampling flagged actions. This approach has already been successfully employed for the game of Checkers. In this work, we describe the application of this method to the context of Connect Four, in order to study its performance in a different setting, which can lead to a better generalization of the technique. We also kept track of a metric that wasn't taken into account in previous work: the average number of iterations to obtain a flagged action. Since going second is a significant disadvantage in Connect Four, we also had the intent of exploring how this more complex scenario would impact the performance of our approach. The experiments involved training and testing classical and quantum RL agents that played either going first or going second against a Randomized Negamax opponent. The results showed that both flagged exploration policies were clearly superior to a simple epsilon-greedy policy. Furthermore, the quantum agents did in fact sample flagged actions in less iterations. Despite obtaining tagged actions more consistently, the win rates between the classical and quantum versions of the approach were identical, which could be due to the simplicity of the training scenario chosen.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consensus-Aware AV Behavior: Trade-offs Between Safety, Interaction, and Performance in Mixed Urban Traffic</title>
<link>https://arxiv.org/abs/2505.04379</link>
<guid>https://arxiv.org/abs/2505.04379</guid>
<content:encoded><![CDATA[
arXiv:2505.04379v1 Announce Type: new 
Abstract: Transportation systems have long been shaped by complexity and heterogeneity, driven by the interdependency of agent actions and traffic outcomes. The deployment of automated vehicles (AVs) in such systems introduces a new challenge: achieving consensus across safety, interaction quality, and traffic performance. In this work, we position consensus as a fundamental property of the traffic system and aim to quantify it. We use high-resolution trajectory data from the Third Generation Simulation (TGSIM) dataset to empirically analyze AV and human-driven vehicle (HDV) behavior at a signalized urban intersection and around vulnerable road users (VRUs). Key metrics, including Time-to-Collision (TTC), Post-Encroachment Time (PET), deceleration patterns, headways, and string stability, are evaluated across the three performance dimensions. Results show that full consensus across safety, interaction, and performance is rare, with only 1.63% of AV-VRU interaction frames meeting all three conditions. These findings highlight the need for AV models that explicitly balance multi-dimensional performance in mixed-traffic environments. Full reproducibility is supported via our open-source codebase on https://github.com/wissamkontar/Consensus-AV-Analysis.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception</title>
<link>https://arxiv.org/abs/2505.04410</link>
<guid>https://arxiv.org/abs/2505.04410</guid>
<content:encoded><![CDATA[
arXiv:2505.04410v1 Announce Type: new 
Abstract: Dense visual prediction tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct application to dense prediction often leads to suboptimal performance due to limitations in local feature representation. In this work, we present our observation that CLIP's image tokens struggle to effectively aggregate information from spatially or semantically related regions, resulting in features that lack local discriminability and spatial consistency. To address this issue, we propose DeCLIP, a novel framework that enhances CLIP by decoupling the self-attention module to obtain ``content'' and ``context'' features respectively. The ``content'' features are aligned with image crop representations to improve local discriminability, while ``context'' features learn to retain the spatial correlations under the guidance of vision foundation models, such as DINO. Extensive experiments demonstrate that DeCLIP significantly outperforms existing methods across multiple open-vocabulary dense prediction tasks, including object detection and semantic segmentation. Code is available at \textcolor{magenta}{https://github.com/xiaomoguhz/DeCLIP}.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving</title>
<link>https://arxiv.org/abs/2505.04528</link>
<guid>https://arxiv.org/abs/2505.04528</guid>
<content:encoded><![CDATA[
arXiv:2505.04528v1 Announce Type: new 
Abstract: As a seemingly self-explanatory task, problem-solving has been a significant component of science and engineering. However, a general yet concrete formulation of problem-solving itself is missing. With the recent development of AI-based problem-solving agents, the demand for process-level verifiability is rapidly increasing yet underexplored. To fill these gaps, we present a principled formulation of problem-solving as a deterministic Markov decision process; a novel framework, FPS (Formal Problem-Solving), which utilizes existing FTP (formal theorem proving) environments to perform process-verified problem-solving; and D-FPS (Deductive FPS), decoupling solving and answer verification for better human-alignment. The expressiveness, soundness and completeness of the frameworks are proven. We construct three benchmarks on problem-solving: FormalMath500, a formalization of a subset of the MATH500 benchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP benchmarks MiniF2F and PutnamBench. For faithful, interpretable, and human-aligned evaluation, we propose RPE (Restricted Propositional Equivalence), a symbolic approach to determine the correctness of answers by formal verification. We evaluate four prevalent FTP models and two prompting methods as baselines, solving at most 23.77% of FormalMath500, 27.47% of MiniF2F-Solving, and 0.31% of PutnamBench-Solving.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Qualitative Analysis of $\omega$-Regular Objectives on Robust MDPs</title>
<link>https://arxiv.org/abs/2505.04539</link>
<guid>https://arxiv.org/abs/2505.04539</guid>
<content:encoded><![CDATA[
arXiv:2505.04539v1 Announce Type: new 
Abstract: Robust Markov Decision Processes (RMDPs) generalize classical MDPs that consider uncertainties in transition probabilities by defining a set of possible transition functions. An objective is a set of runs (or infinite trajectories) of the RMDP, and the value for an objective is the maximal probability that the agent can guarantee against the adversarial environment. We consider (a) reachability objectives, where given a target set of states, the goal is to eventually arrive at one of them; and (b) parity objectives, which are a canonical representation for $\omega$-regular objectives. The qualitative analysis problem asks whether the objective can be ensured with probability 1.
  In this work, we study the qualitative problem for reachability and parity objectives on RMDPs without making any assumption over the structures of the RMDPs, e.g., unichain or aperiodic. Our contributions are twofold. We first present efficient algorithms with oracle access to uncertainty sets that solve qualitative problems of reachability and parity objectives. We then report experimental results demonstrating the effectiveness of our oracle-based approach on classical RMDP examples from the literature scaling up to thousands of states.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Deterministic Rendezvous in Labeled Lines</title>
<link>https://arxiv.org/abs/2505.04564</link>
<guid>https://arxiv.org/abs/2505.04564</guid>
<content:encoded><![CDATA[
arXiv:2505.04564v1 Announce Type: new 
Abstract: In a rendezvous task, a set of mobile agents dispersed in a network have to gather at an arbitrary common site. We consider the rendezvous problem on the infinite labeled line, with $2$ initially asleep agents, without communication, and a synchronous notion of time. Nodes are labeled with unique positive integers. The initial distance between the two agents is denoted by $D$. Time is divided into rounds. We count time from when an agent first wakes up, and denote by $\tau$ the delay between the agents' wake up times. If awake in a given round $T$, an agent has three options: stay at its current node $v$, take port $0$, or take port $1$. If it decides to stay, the agent is still at node $v$ in round $T+1$. Otherwise, it is at one of the two neighbors of $v$ on the line, based on the port it chose. The agents achieve rendezvous in $T$ rounds if they are at the same node in round $T$. We aim for a deterministic algorithm for this task.
  The problem was recently considered by Miller and Pelc [DISC 2023]. With $\ell_{\max}$ the largest label of the two starting nodes, they showed that no algorithm can guarantee rendezvous in $o(D \log^* \ell_{\max})$ rounds. The lower bound follows from a connection with the LOCAL model of distributed computing, and holds even if the agents are guaranteed simultaneous wake-up ($\tau = 0$) and are given $D$ as advice. Miller and Pelc also gave an algorithm of optimal matching complexity $O(D \log^* \ell_{\max})$ when $D$ is known to the agents, but only obtained the higher bound of $O(D^2 (\log^* \ell_{\max})^3)$ when $D$ is unknown.
  We improve this second complexity to a tight $O(D \log^* \ell_{\max})$. In fact, our algorithm achieves rendezvous in $O(D \log^* \ell_{\min})$ rounds, where $\ell_{\min}$ is the smallest label within distance $O(D)$ of the two starting positions.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicitly Aligning Humans and Autonomous Agents through Shared Task Abstractions</title>
<link>https://arxiv.org/abs/2505.04579</link>
<guid>https://arxiv.org/abs/2505.04579</guid>
<content:encoded><![CDATA[
arXiv:2505.04579v1 Announce Type: new 
Abstract: In collaborative tasks, autonomous agents fall short of humans in their capability to quickly adapt to new and unfamiliar teammates. We posit that a limiting factor for zero-shot coordination is the lack of shared task abstractions, a mechanism humans rely on to implicitly align with teammates. To address this gap, we introduce HA$^2$: Hierarchical Ad Hoc Agents, a framework leveraging hierarchical reinforcement learning to mimic the structured approach humans use in collaboration. We evaluate HA$^2$ in the Overcooked environment, demonstrating statistically significant improvement over existing baselines when paired with both unseen agents and humans, providing better resilience to environmental shifts, and outperforming all state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning for Investor-Specific Portfolio Optimization: A Volatility-Guided Asset Selection Approach</title>
<link>https://arxiv.org/abs/2505.03760</link>
<guid>https://arxiv.org/abs/2505.03760</guid>
<content:encoded><![CDATA[
arXiv:2505.03760v1 Announce Type: cross 
Abstract: Portfolio optimization requires dynamic allocation of funds by balancing the risk and return tradeoff under dynamic market conditions. With the recent advancements in AI, Deep Reinforcement Learning (DRL) has gained prominence in providing adaptive and scalable strategies for portfolio optimization. However, the success of these strategies depends not only on their ability to adapt to market dynamics but also on the careful pre-selection of assets that influence overall portfolio performance. Incorporating the investor's preference in pre-selecting assets for a portfolio is essential in refining their investment strategies. This study proposes a volatility-guided DRL-based portfolio optimization framework that dynamically constructs portfolios based on investors' risk profiles. The Generalized Autoregressive Conditional Heteroscedasticity (GARCH) model is utilized for volatility forecasting of stocks and categorizes them based on their volatility as aggressive, moderate, and conservative. The DRL agent is then employed to learn an optimal investment policy by interacting with the historical market data. The efficacy of the proposed methodology is established using stocks from the Dow $30$ index. The proposed investor-specific DRL-based portfolios outperformed the baseline strategies by generating consistent risk-adjusted returns.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization Problem Solving Can Transition to Evolutionary Agentic Workflows</title>
<link>https://arxiv.org/abs/2505.04354</link>
<guid>https://arxiv.org/abs/2505.04354</guid>
<content:encoded><![CDATA[
arXiv:2505.04354v1 Announce Type: cross 
Abstract: This position paper argues that optimization problem solving can transition from expert-dependent to evolutionary agentic workflows. Traditional optimization practices rely on human specialists for problem formulation, algorithm selection, and hyperparameter tuning, creating bottlenecks that impede industrial adoption of cutting-edge methods. We contend that an evolutionary agentic workflow, powered by foundation models and evolutionary search, can autonomously navigate the optimization space, comprising problem, formulation, algorithm, and hyperparameter spaces. Through case studies in cloud resource scheduling and ADMM parameter adaptation, we demonstrate how this approach can bridge the gap between academic innovation and industrial implementation. Our position challenges the status quo of human-centric optimization workflows and advocates for a more scalable, adaptive approach to solving real-world optimization problems.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Playing repeated games with Large Language Models</title>
<link>https://arxiv.org/abs/2305.16867</link>
<guid>https://arxiv.org/abs/2305.16867</guid>
<content:encoded><![CDATA[
arXiv:2305.16867v2 Announce Type: replace 
Abstract: LLMs are increasingly used in applications where they interact with humans and other agents. We propose to use behavioural game theory to study LLM's cooperation and coordination behaviour. We let different LLMs play finitely repeated $2\times2$ games with each other, with human-like strategies, and actual human players. Our results show that LLMs perform particularly well at self-interested games like the iterated Prisoner's Dilemma family. However, they behave sub-optimally in games that require coordination, like the Battle of the Sexes. We verify that these behavioural signatures are stable across robustness checks. We additionally show how GPT-4's behaviour can be modulated by providing additional information about its opponent and by using a "social chain-of-thought" (SCoT) strategy. This also leads to better scores and more successful coordination when interacting with human players. These results enrich our understanding of LLM's social behaviour and pave the way for a behavioural game theory for machines.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-ReST: Reflection-Reinforced Self-Training for Language Agents</title>
<link>https://arxiv.org/abs/2406.01495</link>
<guid>https://arxiv.org/abs/2406.01495</guid>
<content:encoded><![CDATA[
arXiv:2406.01495v3 Announce Type: replace 
Abstract: Finetuning language agents with reasoning-action trajectories is effective, but obtaining these trajectories from human annotations or stronger models is costly and sometimes impractical. In this paper, we investigate the use of self-training in language agents, which can generate supervision from the agent itself, offering a promising alternative without relying on human or stronger model demonstrations. Self-training, however, requires high-quality model-generated samples, which are hard to obtain for challenging language agent tasks. To address this, we present Reflection-Reinforced Self-Training (Re-ReST), which uses a \textit{reflector} to refine low-quality generated samples during self-training. The reflector takes the agent's output and feedback from an external environment (e.g., unit test results in code generation) to produce improved samples. This technique enhances the quality of inferior samples and efficiently enriches the self-training dataset with higher-quality samples. We conduct extensive experiments on open-source language agents across tasks, including multi-hop question answering, sequential decision-making, code generation, visual question answering, and text-to-image generation. The results demonstrate the effectiveness of self-training and Re-ReST in language agent tasks, with self-training improving baselines by 7.6\% on HotpotQA and 28.4\% on AlfWorld, and Re-ReST further boosting performance by 2.0\% and 14.1\%, respectively. Our studies also confirm the efficiency of using a reflector to generate high-quality samples for self-training. Moreover, we demonstrate a method to employ reflection during inference without ground-truth feedback, addressing the limitation of previous reflection work. Our code is released at https://github.com/PlusLabNLP/Re-ReST.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is In-Context Learning a Type of Error-Driven Learning? Evidence from the Inverse Frequency Effect in Structural Priming</title>
<link>https://arxiv.org/abs/2406.18501</link>
<guid>https://arxiv.org/abs/2406.18501</guid>
<content:encoded><![CDATA[
arXiv:2406.18501v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown the emergent capability of in-context learning (ICL). One line of research has claimed that ICL is functionally equivalent to gradient descent, a type of error-driven learning mechanism. In this paper, we introduce a new way of diagnosing whether ICL is functionally performing error-driven learning. Our approach is based on the inverse frequency effect (IFE) -- a phenomenon in which an agent's behavior is influenced to a greater degree when presented with improbable examples as compared to more likely ones. The IFE has previously been identified in psycholinguistics where humans exhibit the IFE in the context of structural priming (the tendency for people to produce sentence structures they have encountered recently). In that context, the IFE has been used as evidence that human structural priming must involve error-driven learning mechanisms. In our experiments, we simulated structural priming with ICL and found that LLMs indeed display the IFE, with the effect being stronger in larger models. We conclude that at least in the case we studied, ICL is indeed a type of error-driven learning, supporting the hypothesis that an error signal is implicitly computed in the forward pass during ICL. Our results suggest that both humans and LLMs make use of error-driven processing mechanisms in on-line processing.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asynchronous Fractional Multi-Agent Deep Reinforcement Learning for Age-Minimal Mobile Edge Computing</title>
<link>https://arxiv.org/abs/2409.16832</link>
<guid>https://arxiv.org/abs/2409.16832</guid>
<content:encoded><![CDATA[
arXiv:2409.16832v5 Announce Type: replace 
Abstract: In the realm of emerging real-time networked applications like cyber-physical systems (CPS), the Age of Information (AoI) has merged as a pivotal metric for evaluating the timeliness. To meet the high computational demands, such as those in intelligent manufacturing within CPS, mobile edge computing (MEC) presents a promising solution for optimizing computing and reducing AoI. In this work, we study the timeliness of computational-intensive updates and explores jointly optimize the task updating and offloading policies to minimize AoI. Specifically, we consider edge load dynamics and formulate a task scheduling problem to minimize the expected time-average AoI. The fractional objective introduced by AoI and the semi-Markov game nature of the problem render this challenge particularly difficult, with existing approaches not directly applicable. To this end, we present a comprehensive framework to fractional reinforcement learning (RL). We first introduce a fractional single-agent RL framework and prove its linear convergence. We then extend this to a fractional multi-agent RL framework with a convergence analysis. To tackle the challenge of asynchronous control in semi-Markov game, we further design an asynchronous model-free fractional multi-agent RL algorithm, where each device makes scheduling decisions with the hybrid action space without knowing the system dynamics and decisions of other devices. Experimental results show that our proposed algorithms reduce the average AoI by up to 52.6% compared with the best baseline algorithm in our experiments.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models</title>
<link>https://arxiv.org/abs/2505.02847</link>
<guid>https://arxiv.org/abs/2505.02847</guid>
<content:encoded><![CDATA[
arXiv:2505.02847v1 Announce Type: new 
Abstract: Assessing how well a large language model (LLM) understands human, rather than merely text, remains an open challenge. To bridge the gap, we introduce Sentient Agent as a Judge (SAGE), an automated evaluation framework that measures an LLM's higher-order social cognition. SAGE instantiates a Sentient Agent that simulates human-like emotional changes and inner thoughts during interaction, providing a more realistic evaluation of the tested model in multi-turn conversations. At every turn, the agent reasons about (i) how its emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a numerical emotion trajectory and interpretable inner thoughts. Experiments on 100 supportive-dialogue scenarios show that the final Sentient emotion score correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings and utterance-level empathy metrics, validating psychological fidelity. We also build a public Sentient Leaderboard covering 18 commercial and open-source models that uncovers substantial gaps (up to 4x) between frontier systems (GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in conventional leaderboards (e.g., Arena). SAGE thus provides a principled, scalable and interpretable tool for tracking progress toward genuinely empathetic and socially adept language agents.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Computational Model of Inclusive Pedagogy: From Understanding to Application</title>
<link>https://arxiv.org/abs/2505.02853</link>
<guid>https://arxiv.org/abs/2505.02853</guid>
<content:encoded><![CDATA[
arXiv:2505.02853v1 Announce Type: new 
Abstract: Human education transcends mere knowledge transfer, it relies on co-adaptation dynamics -- the mutual adjustment of teaching and learning strategies between agents. Despite its centrality, computational models of co-adaptive teacher-student interactions (T-SI) remain underdeveloped. We argue that this gap impedes Educational Science in testing and scaling contextual insights across diverse settings, and limits the potential of Machine Learning systems, which struggle to emulate and adaptively support human learning processes. To address this, we present a computational T-SI model that integrates contextual insights on human education into a testable framework. We use the model to evaluate diverse T-SI strategies in a realistic synthetic classroom setting, simulating student groups with unequal access to sensory information. Results show that strategies incorporating co-adaptation principles (e.g., bidirectional agency) outperform unilateral approaches (i.e., where only the teacher or the student is active), improving the learning outcomes for all learning types. Beyond the testing and scaling of context-dependent educational insights, our model enables hypothesis generation in controlled yet adaptable environments. This work bridges non-computational theories of human education with scalable, inclusive AI in Education systems, providing a foundation for equitable technologies that dynamically adapt to learner needs.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Orchestration for Multi-Agent Systems: A Deep Learning Framework for Optimal Agent Selection in Multi-Domain Task Environments</title>
<link>https://arxiv.org/abs/2505.02861</link>
<guid>https://arxiv.org/abs/2505.02861</guid>
<content:encoded><![CDATA[
arXiv:2505.02861v1 Announce Type: new 
Abstract: Multi-agent systems (MAS) are foundational in simulating complex real-world scenarios involving autonomous, interacting entities. However, traditional MAS architectures often suffer from rigid coordination mechanisms and difficulty adapting to dynamic tasks. We propose MetaOrch, a neural orchestration framework for optimal agent selection in multi-domain task environments. Our system implements a supervised learning approach that models task context, agent histories, and expected response quality to select the most appropriate agent for each task. A novel fuzzy evaluation module scores agent responses along completeness, relevance, and confidence dimensions, generating soft supervision labels for training the orchestrator. Unlike previous methods that hard-code agent-task mappings, MetaOrch dynamically predicts the most suitable agent while estimating selection confidence. Experiments in simulated environments with heterogeneous agents demonstrate that our approach achieves 86.3% selection accuracy, significantly outperforming baseline strategies including random selection and round-robin scheduling. The modular architecture emphasizes extensibility, allowing agents to be registered, updated, and queried independently. Results suggest that neural orchestration offers a powerful approach to enhancing the autonomy, interpretability, and adaptability of multi-agent systems across diverse task domains.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger</title>
<link>https://arxiv.org/abs/2505.02888</link>
<guid>https://arxiv.org/abs/2505.02888</guid>
<content:encoded><![CDATA[
arXiv:2505.02888v1 Announce Type: new 
Abstract: We present Noise-to-Meaning Recursive Self-Improvement (N2M-RSI), a minimal formal model showing that once an AI agent feeds its own outputs back as inputs and crosses an explicit information-integration threshold, its internal complexity will grow without bound under our assumptions. The framework unifies earlier ideas on self-prompting large language models, G\"odelian self-reference, and AutoML, yet remains implementation-agnostic. The model furthermore scales naturally to interacting swarms of agents, hinting at super-linear effects once communication among instances is permitted. For safety reasons, we omit system-specific implementation details and release only a brief, model-agnostic toy prototype in Appendix C.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models</title>
<link>https://arxiv.org/abs/2505.02931</link>
<guid>https://arxiv.org/abs/2505.02931</guid>
<content:encoded><![CDATA[
arXiv:2505.02931v1 Announce Type: new 
Abstract: Automatic program repair (APR) aims to reduce the manual efforts required to identify and fix errors in source code. Before the rise of LLM-based agents, a common strategy was to increase the number of generated patches, sometimes to the thousands, to achieve better repair results on benchmarks. More recently, self-iterative capabilities enabled LLMs to refine patches over multiple rounds guided by feedback. However, literature often focuses on many iterations and disregards different numbers of outputs.
  We investigate an APR pipeline that balances these two approaches, the generation of multiple outputs and multiple rounds of iteration, while imposing a limit of 10 total patches per bug. We apply three SOTA instruction-tuned LLMs - DeepSeekCoder-Instruct, Codellama-Instruct, Llama3.1-Instruct - to the APR task. We further fine-tune each model on an APR dataset with three sizes (1K, 30K, 65K) and two techniques (Full Fine-Tuning and LoRA), allowing us to assess their repair capabilities on two APR benchmarks: HumanEval-Java and Defects4J.
  Our results show that by using only a fraction (<1%) of the fine-tuning dataset, we can achieve improvements of up to 78% in the number of plausible patches generated, challenging prior studies that reported limited gains using Full Fine-Tuning. However, we find that exceeding certain thresholds leads to diminishing outcomes, likely due to overfitting. Moreover, we show that base models greatly benefit from creating patches in an iterative fashion rather than generating them all at once. In addition, the benefit of iterative strategies becomes more pronounced in complex benchmarks. Even fine-tuned models, while benefiting less from iterations, still gain advantages, particularly on complex benchmarks. The research underscores the need for balanced APR strategies that combine multi-output generation and iterative refinement.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Cognitive Foundations of Economic Exchange: A Modular Framework Grounded in Behavioral Evidence</title>
<link>https://arxiv.org/abs/2505.02945</link>
<guid>https://arxiv.org/abs/2505.02945</guid>
<content:encoded><![CDATA[
arXiv:2505.02945v1 Announce Type: new 
Abstract: A key challenge in multi-agent AI is modeling social cooperation under realistic behavioral constraints. Many foundational concepts in economics and ethics such as "trust" or "morality" are often defined informally, without operational criteria or cognitive grounding, which limits their testability and implementation in artificial agents. Drawing on converging empirical evidence from primate behavior, infant cognition, and economic anthropology, we propose a conceptual framework composed of three cognitively minimal mechanisms: individual recognition, reciprocal credence, and cost return sensitivity. This framework reframes trust as a graded cognitive expectation, providing a simulateable basis for reciprocal exchange in artificial agents, and enabling the bottom-up emergence of scalable cooperation and institutional dynamics.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smooth Quadratic Prediction Markets</title>
<link>https://arxiv.org/abs/2505.02959</link>
<guid>https://arxiv.org/abs/2505.02959</guid>
<content:encoded><![CDATA[
arXiv:2505.02959v1 Announce Type: new 
Abstract: When agents trade in a Duality-based Cost Function prediction market, they collectively implement the learning algorithm Follow-The-Regularized-Leader. We ask whether other learning algorithms could be used to inspire the design of prediction markets. By decomposing and modifying the Duality-based Cost Function Market Maker's (DCFMM) pricing mechanism, we propose a new prediction market, called the Smooth Quadratic Prediction Market, the incentivizes agents to collectively implement general steepest gradient descent. Relative to the DCFMM, the Smooth Quadratic Prediction Market has a better worst-case monetary loss for AD securities while preserving axiom guarantees such as the existence of instantaneous price, information incorporation, expressiveness, no arbitrage, and a form of incentive compatibility. To motivate the application of the Smooth Quadratic Prediction Market, we independently examine agents' trading behavior under two realistic constraints: bounded budgets and buy-only securities. Finally, we provide an introductory analysis of an approach to facilitate adaptive liquidity using the Smooth Quadratic AD Prediction Market. Our results suggest future designs where the price update rule is separate from the fee structure, yet guarantees are preserved.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lesion-Aware Generative Artificial Intelligence for Virtual Contrast-Enhanced Mammography in Breast Cancer</title>
<link>https://arxiv.org/abs/2505.03018</link>
<guid>https://arxiv.org/abs/2505.03018</guid>
<content:encoded><![CDATA[
arXiv:2505.03018v1 Announce Type: new 
Abstract: Contrast-Enhanced Spectral Mammography (CESM) is a dual-energy mammographic technique that improves lesion visibility through the administration of an iodinated contrast agent. It acquires both a low-energy image, comparable to standard mammography, and a high-energy image, which are then combined to produce a dual-energy subtracted image highlighting lesion contrast enhancement. While CESM offers superior diagnostic accuracy compared to standard mammography, its use entails higher radiation exposure and potential side effects associated with the contrast medium. To address these limitations, we propose Seg-CycleGAN, a generative deep learning framework for Virtual Contrast Enhancement in CESM. The model synthesizes high-fidelity dual-energy subtracted images from low-energy images, leveraging lesion segmentation maps to guide the generative process and improve lesion reconstruction. Building upon the standard CycleGAN architecture, Seg-CycleGAN introduces localized loss terms focused on lesion areas, enhancing the synthesis of diagnostically relevant regions. Experiments on the CESM@UCBM dataset demonstrate that Seg-CycleGAN outperforms the baseline in terms of PSNR and SSIM, while maintaining competitive MSE and VIF. Qualitative evaluations further confirm improved lesion fidelity in the generated images. These results suggest that segmentation-aware generative models offer a viable pathway toward contrast-free CESM alternatives.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coevolution of Actions and Opinions in Networks of Coordinating and Anti-Coordinating Agents</title>
<link>https://arxiv.org/abs/2505.03078</link>
<guid>https://arxiv.org/abs/2505.03078</guid>
<content:encoded><![CDATA[
arXiv:2505.03078v1 Announce Type: new 
Abstract: In this paper, we investigate the dynamics of coordinating and anti-coordinating agents in a coevolutionary model for actions and opinions. In the model, the individuals of a population interact on a two-layer network, sharing their opinions and observing others' action, while revising their own opinions and actions according to a game-theoretic mechanism, grounded in the social psychology literature. First, we consider the scenario of coordinating agents, where convergence to a Nash equilibrium (NE) is guaranteed. We identify conditions for reaching consensus configurations and establish regions of attraction for these equilibria. Second, we study networks of anti-coordinating agents. In this second scenario, we prove that all trajectories converge to a NE by leveraging potential game theory. Then, we establish analytical conditions on the network structure and model parameters to guarantee the existence of consensus and polarized equilibria, characterizing their regions of attraction.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Task-aware Fault Detection, Identification For On-Orbit Multi-Spacecraft Collaborative Inspection</title>
<link>https://arxiv.org/abs/2505.03088</link>
<guid>https://arxiv.org/abs/2505.03088</guid>
<content:encoded><![CDATA[
arXiv:2505.03088v1 Announce Type: new 
Abstract: In this paper, we present a global-to-local task-aware fault detection and identification algorithm to detect failures in a multi-spacecraft system performing a collaborative inspection (referred to as global) task. The inspection task is encoded as a cost functional $\costH$ that informs global (task allocation and assignment) and local (agent-level) decision-making. The metric $\costH$ is a function of the inspection sensor model, and the agent full-pose. We use the cost functional $\costH$ to design a metric that compares the expected and actual performance to detect the faulty agent using a threshold. We use higher-order cost gradients $\costH$ to derive a new metric to identify the type of fault, including task-specific sensor fault, an agent-level actuator, and sensor faults. Furthermore, we propose an approach to design adaptive thresholds for each fault mentioned above to incorporate the time dependence of the inspection task. We demonstrate the efficacy of the proposed method empirically, by simulating and detecting faults (such as inspection sensor faults, actuators, and sensor faults) in a low-Earth orbit collaborative spacecraft inspection task using the metrics and the threshold designed using the global task cost $\costH$.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing and Enhancing the Robustness of LLM-based Multi-Agent Systems Through Chaos Engineering</title>
<link>https://arxiv.org/abs/2505.03096</link>
<guid>https://arxiv.org/abs/2505.03096</guid>
<content:encoded><![CDATA[
arXiv:2505.03096v1 Announce Type: new 
Abstract: This study explores the application of chaos engineering to enhance the robustness of Large Language Model-Based Multi-Agent Systems (LLM-MAS) in production-like environments under real-world conditions. LLM-MAS can potentially improve a wide range of tasks, from answering questions and generating content to automating customer support and improving decision-making processes. However, LLM-MAS in production or preproduction environments can be vulnerable to emergent errors or disruptions, such as hallucinations, agent failures, and agent communication failures. This study proposes a chaos engineering framework to proactively identify such vulnerabilities in LLM-MAS, assess and build resilience against them, and ensure reliable performance in critical applications.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrated Sensing, Computing, Communication, and Control for Time-Sequence-Based Semantic Communications</title>
<link>https://arxiv.org/abs/2505.03127</link>
<guid>https://arxiv.org/abs/2505.03127</guid>
<content:encoded><![CDATA[
arXiv:2505.03127v1 Announce Type: new 
Abstract: In the upcoming industrial internet of things (IIoT) era, a surge of task-oriented applications will rely on real-time wireless control systems (WCSs). For these systems, ultra-reliable and low-latency wireless communication will be crucial to ensure the timely transmission of control information. To achieve this purpose, we propose a novel time-sequence-based semantic communication paradigm, where an integrated sensing, computing, communication, and control (ISC3) architecture is developed to make sensible semantic inference (SI) for the control information over time sequences, enabling adaptive control of the robot. However, due to the causal correlations in the time sequence, the control information does not present the Markov property. To address this challenge, we compute the mutual information of the control information sensed at the transmitter (Tx) over different time and identify their temporal semantic correlation via a semantic feature extractor (SFE) module. By this means, highly correlated information transmission can be avoided, thus greatly reducing the communication overhead. Meanwhile, a semantic feature reconstructor (SFR) module is employed at the receiver (Rx) to reconstruct the control information based on the previously received one if the information transmission is not activated at the Tx. Furthermore, a control gain policy is also employed at the Rx to adaptively adjust the control gain for the controlled target based on several practical aspects such as the quality of the information transmission from the Tx to the Rx. We design the neural network structures of the above modules/policies and train their parameters by a novel hybrid reward multi-agent deep reinforcement learning framework. On-site experiments are conducted to evaluate the performance of our proposed method in practice, which shows significant gains over other baseline schemes.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM-based Self-Evolving Security Framework for 6G Space-Air-Ground Integrated Networks</title>
<link>https://arxiv.org/abs/2505.03161</link>
<guid>https://arxiv.org/abs/2505.03161</guid>
<content:encoded><![CDATA[
arXiv:2505.03161v1 Announce Type: new 
Abstract: Recently emerged 6G space-air-ground integrated networks (SAGINs), which integrate satellites, aerial networks, and terrestrial communications, offer ubiquitous coverage for various mobile applications. However, the highly dynamic, open, and heterogeneous nature of SAGINs poses severe security issues. Forming a defense line of SAGINs suffers from two preliminary challenges: 1) accurately understanding massive unstructured multi-dimensional threat information to generate defense strategies against various malicious attacks, 2) rapidly adapting to potential unknown threats to yield more effective security strategies. To tackle the above two challenges, we propose a novel security framework for SAGINs based on Large Language Models (LLMs), which consists of two key ingredients LLM-6GNG and 6G-INST. Our proposed LLM-6GNG leverages refined chain-of-thought (CoT) reasoning and dynamic multi-agent mechanisms to analyze massive unstructured multi-dimensional threat data and generate comprehensive security strategies, thus addressing the first challenge. Our proposed 6G-INST relies on a novel self-evolving method to automatically update LLM-6GNG, enabling it to accommodate unknown threats under dynamic communication environments, thereby addressing the second challenge. Additionally, we prototype the proposed framework with ns-3, OpenAirInterface (OAI), and software-defined radio (SDR). Experiments on three benchmarks demonstrate the effectiveness of our framework. The results show that our framework produces highly accurate security strategies that remain robust against a variety of unknown attacks. We will release our code to contribute to the community.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RADE: Learning Risk-Adjustable Driving Environment via Multi-Agent Conditional Diffusion</title>
<link>https://arxiv.org/abs/2505.03178</link>
<guid>https://arxiv.org/abs/2505.03178</guid>
<content:encoded><![CDATA[
arXiv:2505.03178v1 Announce Type: new 
Abstract: Generating safety-critical scenarios in high-fidelity simulations offers a promising and cost-effective approach for efficient testing of autonomous vehicles. Existing methods typically rely on manipulating a single vehicle's trajectory through sophisticated designed objectives to induce adversarial interactions, often at the cost of realism and scalability. In this work, we propose the Risk-Adjustable Driving Environment (RADE), a simulation framework that generates statistically realistic and risk-adjustable traffic scenes. Built upon a multi-agent diffusion architecture, RADE jointly models the behavior of all agents in the environment and conditions their trajectories on a surrogate risk measure. Unlike traditional adversarial methods, RADE learns risk-conditioned behaviors directly from data, preserving naturalistic multi-agent interactions with controllable risk levels. To ensure physical plausibility, we incorporate a tokenized dynamics check module that efficiently filters generated trajectories using a motion vocabulary. We validate RADE on the real-world rounD dataset, demonstrating that it preserves statistical realism across varying risk levels and naturally increases the likelihood of safety-critical events as the desired risk level grows up. Our results highlight RADE's potential as a scalable and realistic tool for AV safety evaluation.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making</title>
<link>https://arxiv.org/abs/2505.03181</link>
<guid>https://arxiv.org/abs/2505.03181</guid>
<content:encoded><![CDATA[
arXiv:2505.03181v1 Announce Type: new 
Abstract: Recent research looks to harness the general knowledge and reasoning of large language models (LLMs) into agents that accomplish user-specified goals in interactive environments. Vision-language models (VLMs) extend LLMs to multi-modal data and provide agents with the visual reasoning necessary for new applications in areas such as computer automation. However, agent tasks emphasize skills where accessible open-weight VLMs lag behind their LLM equivalents. For example, VLMs are less capable of following an environment's strict output syntax requirements and are more focused on open-ended question answering. Overcoming these limitations requires supervised fine-tuning (SFT) on task-specific expert demonstrations. Our work approaches these challenges from an offline-to-online reinforcement learning (RL) perspective. RL lets us fine-tune VLMs to agent tasks while learning from the unsuccessful decisions of our own model or more capable (larger) models. We explore an off-policy RL solution that retains the stability and simplicity of the widely used SFT workflow while allowing our agent to self-improve and learn from low-quality datasets. We demonstrate this technique with two open-weight VLMs across three multi-modal agent domains.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DYSTIL: Dynamic Strategy Induction with Large Language Models for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.03209</link>
<guid>https://arxiv.org/abs/2505.03209</guid>
<content:encoded><![CDATA[
arXiv:2505.03209v1 Announce Type: new 
Abstract: Reinforcement learning from expert demonstrations has long remained a challenging research problem, and existing state-of-the-art methods using behavioral cloning plus further RL training often suffer from poor generalization, low sample efficiency, and poor model interpretability. Inspired by the strong reasoning abilities of large language models (LLMs), we propose a novel strategy-based reinforcement learning framework integrated with LLMs called DYnamic STrategy Induction with Llms for reinforcement learning (DYSTIL) to overcome these limitations. DYSTIL dynamically queries a strategy-generating LLM to induce textual strategies based on advantage estimations and expert demonstrations, and gradually internalizes induced strategies into the RL agent through policy optimization to improve its performance through boosting policy generalization and enhancing sample efficiency. It also provides a direct textual channel to observe and interpret the evolution of the policy's underlying strategies during training. We test DYSTIL over challenging RL environments from Minigrid and BabyAI, and empirically demonstrate that DYSTIL significantly outperforms state-of-the-art baseline methods by 17.75% in average success rate while also enjoying higher sample efficiency during the learning process.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDPs with a State Sensing Cost</title>
<link>https://arxiv.org/abs/2505.03280</link>
<guid>https://arxiv.org/abs/2505.03280</guid>
<content:encoded><![CDATA[
arXiv:2505.03280v1 Announce Type: new 
Abstract: In many practical sequential decision-making problems, tracking the state of the environment incurs a sensing/communication/computation cost. In these settings, the agent's interaction with its environment includes the additional component of deciding $\textit{when}$ to sense the state, in a manner that balances the value associated with optimal (state-specific) actions and the cost of sensing. We formulate this as an expected discounted cost Markov Decision Process (MDP), wherein the agent incurs an additional cost for sensing its next state, but has the option to take actions while remaining 'blind' to the system state.
  We pose this problem as a classical discounted cost MDP with an expanded (countably infinite) state space. While computing the optimal policy for this MDP is intractable in general, we bound the sub-optimality gap associated with optimal policies in a restricted class, where the number of consecutive non-sensing (a.k.a., blind) actions is capped. We also design a computationally efficient heuristic algorithm based on policy improvement, which in practice performs close to the optimal policy. Finally, we benchmark against the state of the art via a numerical case study.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Deep Reinforcement Learning for Zonal Ancillary Market Coupling</title>
<link>https://arxiv.org/abs/2505.03288</link>
<guid>https://arxiv.org/abs/2505.03288</guid>
<content:encoded><![CDATA[
arXiv:2505.03288v1 Announce Type: new 
Abstract: We characterize zonal ancillary market coupling relying on noncooperative game theory. To that purpose, we formulate the ancillary market as a multi-leader single follower bilevel problem, that we subsequently cast as a generalized Nash game with side constraints and nonconvex feasibility sets. We determine conditions for equilibrium existence and show that the game has a generalized potential game structure. To compute market equilibrium, we rely on two exact approaches: an integrated optimization approach and Gauss-Seidel best-response, that we compare against multi-agent deep reinforcement learning. On real data from Germany and Austria, simulations indicate that multi-agent deep reinforcement learning achieves the smallest convergence rate but requires pretraining, while best-response is the slowest. On the economics side, multi-agent deep reinforcement learning results in smaller market costs compared to the exact methods, but at the cost of higher variability in the profit allocation among stakeholders. Further, stronger coupling between zones tends to reduce costs for larger zones.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Truthful Facility Location with Candidate Locations and Limited Resources</title>
<link>https://arxiv.org/abs/2505.03391</link>
<guid>https://arxiv.org/abs/2505.03391</guid>
<content:encoded><![CDATA[
arXiv:2505.03391v1 Announce Type: new 
Abstract: We study a truthful facility location problem where one out of $k\geq2$ available facilities must be built at a location chosen from a set of candidate ones in the interval $[0,1]$. This decision aims to accommodate a set of agents with private positions in $[0,1]$ and approval preferences over the facilities; the agents act strategically and may misreport their private information to maximize their utility, which depends on the chosen facility and their distance from it. We focus on strategyproof mechanisms that incentivize the agents to act truthfully and bound the best possible approximation of the optimal social welfare (the total utility of the agents) they can achieve. We first show that deterministic mechanisms have unbounded approximation ratio, and then present a randomized mechanism with approximation ratio $k$, which is tight even when agents may only misreport their positions. For the restricted setting where agents may only misreport their approval preferences, we design a deterministic mechanism with approximation ratio of roughly $2.325$, and establish lower bounds of $3/2$ and $6/5$ for deterministic and randomized mechanisms, respectively.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in LLM-Based Agents</title>
<link>https://arxiv.org/abs/2505.03434</link>
<guid>https://arxiv.org/abs/2505.03434</guid>
<content:encoded><![CDATA[
arXiv:2505.03434v1 Announce Type: new 
Abstract: Large Language Models (LLMs) represent a landmark achievement in Artificial Intelligence (AI), demonstrating unprecedented proficiency in procedural tasks such as text generation, code completion, and conversational coherence. These capabilities stem from their architecture, which mirrors human procedural memory -- the brain's ability to automate repetitive, pattern-driven tasks through practice. However, as LLMs are increasingly deployed in real-world applications, it becomes impossible to ignore their limitations operating in complex, unpredictable environments. This paper argues that LLMs, while transformative, are fundamentally constrained by their reliance on procedural memory. To create agents capable of navigating ``wicked'' learning environments -- where rules shift, feedback is ambiguous, and novelty is the norm -- we must augment LLMs with semantic memory and associative learning systems. By adopting a modular architecture that decouples these cognitive functions, we can bridge the gap between narrow procedural expertise and the adaptive intelligence required for real-world problem-solving.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Steganographic Potentials of Language Models</title>
<link>https://arxiv.org/abs/2505.03439</link>
<guid>https://arxiv.org/abs/2505.03439</guid>
<content:encoded><![CDATA[
arXiv:2505.03439v1 Announce Type: new 
Abstract: The potential for large language models (LLMs) to hide messages within plain text (steganography) poses a challenge to detection and thwarting of unaligned AI agents, and undermines faithfulness of LLMs reasoning. We explore the steganographic capabilities of LLMs fine-tuned via reinforcement learning (RL) to: (1) develop covert encoding schemes, (2) engage in steganography when prompted, and (3) utilize steganography in realistic scenarios where hidden reasoning is likely, but not prompted. In these scenarios, we detect the intention of LLMs to hide their reasoning as well as their steganography performance. Our findings in the fine-tuning experiments as well as in behavioral non fine-tuning evaluations reveal that while current models exhibit rudimentary steganographic abilities in terms of security and capacity, explicit algorithmic guidance markedly enhances their capacity for information concealment.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Small-Scale-Fading-Aware Resource Allocation in Wireless Federated Learning</title>
<link>https://arxiv.org/abs/2505.03533</link>
<guid>https://arxiv.org/abs/2505.03533</guid>
<content:encoded><![CDATA[
arXiv:2505.03533v1 Announce Type: new 
Abstract: Judicious resource allocation can effectively enhance federated learning (FL) training performance in wireless networks by addressing both system and statistical heterogeneity. However, existing strategies typically rely on block fading assumptions, which overlooks rapid channel fluctuations within each round of FL gradient uploading, leading to a degradation in FL training performance. Therefore, this paper proposes a small-scale-fading-aware resource allocation strategy using a multi-agent reinforcement learning (MARL) framework. Specifically, we establish a one-step convergence bound of the FL algorithm and formulate the resource allocation problem as a decentralized partially observable Markov decision process (Dec-POMDP), which is subsequently solved using the QMIX algorithm. In our framework, each client serves as an agent that dynamically determines spectrum and power allocations within each coherence time slot, based on local observations and a reward derived from the convergence analysis. The MARL setting reduces the dimensionality of the action space and facilitates decentralized decision-making, enhancing the scalability and practicality of the solution. Experimental results demonstrate that our QMIX-based resource allocation strategy significantly outperforms baseline methods across various degrees of statistical heterogeneity. Additionally, ablation studies validate the critical importance of incorporating small-scale fading dynamics, highlighting its role in optimizing FL performance.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model Reasoning</title>
<link>https://arxiv.org/abs/2505.03553</link>
<guid>https://arxiv.org/abs/2505.03553</guid>
<content:encoded><![CDATA[
arXiv:2505.03553v1 Announce Type: new 
Abstract: Inconsistent outputs and hallucinations from large language models (LLMs) are major obstacles to reliable AI systems. When different proprietary reasoning models (RMs), such as those by OpenAI, Google, Anthropic, DeepSeek, and xAI, are given the same complex request, they often produce divergent results due to variations in training and inference. This paper proposes a novel consensus mechanism, inspired by distributed ledger technology, to validate and converge these outputs, treating each RM as a black-box peer. Building on the Hashgraph consensus algorithm, our approach employs gossip-about-gossip communication and virtual voting to achieve agreement among an ensemble of RMs. We present an architectural design for a prototype system in which RMs iteratively exchange and update their answers, using information from each round to improve accuracy and confidence in subsequent rounds. This approach goes beyond simple majority voting by incorporating the knowledge and cross-verification content of every model. We justify the feasibility of this Hashgraph-inspired consensus for AI ensembles and outline its advantages over traditional ensembling techniques in reducing nonfactual outputs. Preliminary considerations for implementation, evaluation criteria for convergence and accuracy, and potential challenges are discussed. The proposed mechanism demonstrates a promising direction for multi-agent AI systems to self-validate and deliver high-fidelity responses in complex tasks.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey of Large AI Models for Future Communications: Foundations, Applications and Challenges</title>
<link>https://arxiv.org/abs/2505.03556</link>
<guid>https://arxiv.org/abs/2505.03556</guid>
<content:encoded><![CDATA[
arXiv:2505.03556v1 Announce Type: new 
Abstract: The 6G wireless communications aim to establish an intelligent world of ubiquitous connectivity, providing an unprecedented communication experience. Large artificial intelligence models (LAMs) are characterized by significantly larger scales (e.g., billions or trillions of parameters) compared to typical artificial intelligence (AI) models. LAMs exhibit outstanding cognitive abilities, including strong generalization capabilities for fine-tuning to downstream tasks, and emergent capabilities to handle tasks unseen during training. Therefore, LAMs efficiently provide AI services for diverse communication applications, making them crucial tools for addressing complex challenges in future wireless communication systems. This study provides a comprehensive review of the foundations, applications, and challenges of LAMs in communication. First, we introduce the current state of AI-based communication systems, emphasizing the motivation behind integrating LAMs into communications and summarizing the key contributions. We then present an overview of the essential concepts of LAMs in communication. This includes an introduction to the main architectures of LAMs, such as transformer, diffusion models, and mamba. We also explore the classification of LAMs, including large language models (LLMs), large vision models (LVMs), large multimodal models (LMMs), and world models, and examine their potential applications in communication. Additionally, we cover the training methods and evaluation techniques for LAMs in communication systems. Lastly, we introduce optimization strategies such as chain of thought (CoT), retrieval augmented generation (RAG), and agentic systems. Following this, we discuss the research advancements of LAMs across various communication scenarios. Finally, we analyze the challenges in the current research and provide insights into potential future research directions.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Reinforcement Learning Scheduling to Support Low Latency in Teleoperated Driving</title>
<link>https://arxiv.org/abs/2505.03558</link>
<guid>https://arxiv.org/abs/2505.03558</guid>
<content:encoded><![CDATA[
arXiv:2505.03558v1 Announce Type: new 
Abstract: The teleoperated driving (TD) scenario comes with stringent Quality of Service (QoS) communication constraints, especially in terms of end-to-end (E2E) latency and reliability. In this context, Predictive Quality of Service (PQoS), possibly combined with Reinforcement Learning (RL) techniques, is a powerful tool to estimate QoS degradation and react accordingly. For example, an intelligent agent can be trained to select the optimal compression configuration for automotive data, and reduce the file size whenever QoS conditions deteriorate. However, compression may inevitably compromise data quality, with negative implications for the TD application. An alternative strategy involves operating at the Radio Access Network (RAN) level to optimize radio parameters based on current network conditions, while preserving data quality. In this paper, we propose Multi-Agent Reinforcement Learning (MARL) scheduling algorithms, based on Proximal Policy Optimization (PPO), to dynamically and intelligently allocate radio resources to minimize E2E latency in a TD scenario. We evaluate two training paradigms, i.e., decentralized learning with local observations (IPPO) vs. centralized aggregation (MAPPO), in conjunction with two resource allocation strategies, i.e., proportional allocation (PA) and greedy allocation (GA). We prove via ns-3 simulations that MAPPO, combined with GA, achieves the best results in terms of latency, especially as the number of vehicles increases.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents</title>
<link>https://arxiv.org/abs/2505.03570</link>
<guid>https://arxiv.org/abs/2505.03570</guid>
<content:encoded><![CDATA[
arXiv:2505.03570v1 Announce Type: new 
Abstract: In this paper, we introduce OSUniverse: a benchmark of complex, multimodal desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on ease of use, extensibility, comprehensive coverage of test cases, and automated validation. We divide the tasks in increasing levels of complexity, from basic precision clicking to multistep, multiapplication tests requiring dexterity, precision, and clear thinking from the agent. In version one of the benchmark, presented here, we have calibrated the complexity of the benchmark test cases to ensure that the SOTA (State of the Art) agents (at the time of publication) do not achieve results higher than 50%, while the average white collar worker can perform all these tasks with perfect accuracy. The benchmark can be scored manually, but we also introduce an automated validation mechanism that has an average error rate less than 2%. Therefore, this benchmark presents solid ground for fully automated measuring of progress, capabilities and the effectiveness of GUI-navigation AI agents over the short and medium-term horizon. The source code of the benchmark is available at https://github.com/agentsea/osuniverse.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LlamaFirewall: An open source guardrail system for building secure AI agents</title>
<link>https://arxiv.org/abs/2505.03574</link>
<guid>https://arxiv.org/abs/2505.03574</guid>
<content:encoded><![CDATA[
arXiv:2505.03574v1 Announce Type: new 
Abstract: Large language models (LLMs) have evolved from simple chatbots into autonomous agents capable of performing complex tasks such as editing production code, orchestrating workflows, and taking higher-stakes actions based on untrusted inputs like webpages and emails. These capabilities introduce new security risks that existing security measures, such as model fine-tuning or chatbot-focused guardrails, do not fully address. Given the higher stakes and the absence of deterministic solutions to mitigate these risks, there is a critical need for a real-time guardrail monitor to serve as a final layer of defense, and support system level, use case specific safety policy definition and enforcement. We introduce LlamaFirewall, an open-source security focused guardrail framework designed to serve as a final layer of defense against security risks associated with AI Agents. Our framework mitigates risks such as prompt injection, agent misalignment, and insecure code risks through three powerful guardrails: PromptGuard 2, a universal jailbreak detector that demonstrates clear state of the art performance; Agent Alignment Checks, a chain-of-thought auditor that inspects agent reasoning for prompt injection and goal misalignment, which, while still experimental, shows stronger efficacy at preventing indirect injections in general scenarios than previously proposed approaches; and CodeShield, an online static analysis engine that is both fast and extensible, aimed at preventing the generation of insecure or dangerous code by coding agents. Additionally, we include easy-to-use customizable scanners that make it possible for any developer who can write a regular expression or an LLM prompt to quickly update an agent's security guardrails.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer Questions in Dynamic Scenes</title>
<link>https://arxiv.org/abs/2505.03581</link>
<guid>https://arxiv.org/abs/2505.03581</guid>
<content:encoded><![CDATA[
arXiv:2505.03581v1 Announce Type: new 
Abstract: The analysis of events in dynamic environments poses a fundamental challenge in the development of intelligent agents and robots capable of interacting with humans. Current approaches predominantly utilize visual models. However, these methods often capture information implicitly from images, lacking interpretable spatial-temporal object representations. To address this issue we introduce DyGEnc - a novel method for Encoding a Dynamic Graph. This method integrates compressed spatial-temporal structural observation representation with the cognitive capabilities of large language models. The purpose of this integration is to enable advanced question answering based on a sequence of textual scene graphs. Extended evaluations on the STAR and AGQA datasets indicate that DyGEnc outperforms existing visual methods by a large margin of 15-25% in addressing queries regarding the history of human-to-object interactions. Furthermore, the proposed method can be seamlessly extended to process raw input images utilizing foundational models for extracting explicit textual scene graphs, as substantiated by the results of a robotic experiment conducted with a wheeled manipulator platform. We hope that these findings will contribute to the implementation of robust and compressed graph-based robotic memory for long-horizon reasoning. Code is available at github.com/linukc/DyGEnc.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation</title>
<link>https://arxiv.org/abs/2505.03586</link>
<guid>https://arxiv.org/abs/2505.03586</guid>
<content:encoded><![CDATA[
arXiv:2505.03586v1 Announce Type: new 
Abstract: In real-world multi-agent systems (MASs), observation delays are ubiquitous, preventing agents from making decisions based on the environment's true state. An individual agent's local observation often consists of multiple components from other agents or dynamic entities in the environment. These discrete observation components with varying delay characteristics pose significant challenges for multi-agent reinforcement learning (MARL). In this paper, we first formulate the decentralized stochastic individual delay partially observable Markov decision process (DSID-POMDP) by extending the standard Dec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL training framework for addressing stochastic individual delays, along with recommended implementations for its constituent modules. We implement the DSID-POMDP's observation generation pattern using standard MARL benchmarks, including MPE and SMAC. Experiments demonstrate that baseline MARL methods suffer severe performance degradation under fixed and unfixed delays. The RDC-enhanced approach mitigates this issue, remarkably achieving ideal delay-free performance in certain delay scenarios while maintaining generalization capability. Our work provides a novel perspective on multi-agent delayed observation problems and offers an effective solution framework.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoboOS: A Hierarchical Embodied Framework for Cross-Embodiment and Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2505.03673</link>
<guid>https://arxiv.org/abs/2505.03673</guid>
<content:encoded><![CDATA[
arXiv:2505.03673v1 Announce Type: new 
Abstract: The dawn of embodied intelligence has ushered in an unprecedented imperative for resilient, cognition-enabled multi-agent collaboration across next-generation ecosystems, revolutionizing paradigms in autonomous manufacturing, adaptive service robotics, and cyber-physical production architectures. However, current robotic systems face significant limitations, such as limited cross-embodiment adaptability, inefficient task scheduling, and insufficient dynamic error correction. While End-to-end VLA models demonstrate inadequate long-horizon planning and task generalization, hierarchical VLA models suffer from a lack of cross-embodiment and multi-agent coordination capabilities. To address these challenges, we introduce RoboOS, the first open-source embodied system built on a Brain-Cerebellum hierarchical architecture, enabling a paradigm shift from single-agent to multi-agent intelligence. Specifically, RoboOS consists of three key components: (1) Embodied Brain Model (RoboBrain), a MLLM designed for global perception and high-level decision-making; (2) Cerebellum Skill Library, a modular, plug-and-play toolkit that facilitates seamless execution of multiple skills; and (3) Real-Time Shared Memory, a spatiotemporal synchronization mechanism for coordinating multi-agent states. By integrating hierarchical information flow, RoboOS bridges Embodied Brain and Cerebellum Skill Library, facilitating robust planning, scheduling, and error correction for long-horizon tasks, while ensuring efficient multi-agent collaboration through Real-Time Shared Memory. Furthermore, we enhance edge-cloud communication and cloud-based distributed inference to facilitate high-frequency interactions and enable scalable deployment. Extensive real-world experiments across various scenarios, demonstrate RoboOS's versatility in supporting heterogeneous embodiments. Project website: https://github.com/FlagOpen/RoboOS
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gap the (Theory of) Mind: Sharing Beliefs About Teammates' Goals Boosts Collaboration Perception, Not Performance</title>
<link>https://arxiv.org/abs/2505.03674</link>
<guid>https://arxiv.org/abs/2505.03674</guid>
<content:encoded><![CDATA[
arXiv:2505.03674v1 Announce Type: new 
Abstract: In human-agent teams, openly sharing goals is often assumed to enhance planning, collaboration, and effectiveness. However, direct communication of these goals is not always feasible, requiring teammates to infer their partner's intentions through actions. Building on this, we investigate whether an AI agent's ability to share its inferred understanding of a human teammate's goals can improve task performance and perceived collaboration. Through an experiment comparing three conditions-no recognition (NR), viable goals (VG), and viable goals on-demand (VGod) - we find that while goal-sharing information did not yield significant improvements in task performance or overall satisfaction scores, thematic analysis suggests that it supported strategic adaptations and subjective perceptions of collaboration. Cognitive load assessments revealed no additional burden across conditions, highlighting the challenge of balancing informativeness and simplicity in human-agent interactions. These findings highlight the nuanced trade-off of goal-sharing: while it fosters trust and enhances perceived collaboration, it can occasionally hinder objective performance gains.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CaRaFFusion: Improving 2D Semantic Segmentation with Camera-Radar Point Cloud Fusion and Zero-Shot Image Inpainting</title>
<link>https://arxiv.org/abs/2505.03679</link>
<guid>https://arxiv.org/abs/2505.03679</guid>
<content:encoded><![CDATA[
arXiv:2505.03679v1 Announce Type: new 
Abstract: Segmenting objects in an environment is a crucial task for autonomous driving and robotics, as it enables a better understanding of the surroundings of each agent. Although camera sensors provide rich visual details, they are vulnerable to adverse weather conditions. In contrast, radar sensors remain robust under such conditions, but often produce sparse and noisy data. Therefore, a promising approach is to fuse information from both sensors. In this work, we propose a novel framework to enhance camera-only baselines by integrating a diffusion model into a camera-radar fusion architecture. We leverage radar point features to create pseudo-masks using the Segment-Anything model, treating the projected radar points as point prompts. Additionally, we propose a noise reduction unit to denoise these pseudo-masks, which are further used to generate inpainted images that complete the missing information in the original images. Our method improves the camera-only segmentation baseline by 2.63% in mIoU and enhances our camera-radar fusion architecture by 1.48% in mIoU on the Waterscenes dataset. This demonstrates the effectiveness of our approach for semantic segmentation using camera-radar fusion under adverse weather conditions.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Location-Restricted Stable Matching</title>
<link>https://arxiv.org/abs/2505.03680</link>
<guid>https://arxiv.org/abs/2505.03680</guid>
<content:encoded><![CDATA[
arXiv:2505.03680v1 Announce Type: new 
Abstract: Motivated by group-project distribution, we introduce and study stable matching under the constraint of applicants needing to share a location to be matched with the same institute, which we call the Location-Restricted Stable Matching problem (LRSM). We show that finding a feasible matching is NP-hard, making finding a feasible and stable matching automatically NP-hard. We then analyze the subproblem where all the projects have the same capacity, and the applicant population of each location is a multiple of the universal project capacity, which mimics more realistic constraints and makes finding a feasible matching in P. Even under these conditions, a stable matching (a matching without blocking pairs) may not exist, so we look for a matching that minimizes the number of blocking pairs. We find that the blocking pair minimization problem for this subproblem is inapproximable within $|A|^{1-\epsilon}$ for $|A|$ agents and provide an $|A|$-approximation algorithm to show this result is almost tight. We extend this result to show that the problem of minimizing the number of agents in blocking pairs is also inapproximable within $|A|^{1-\epsilon}$, and since there are only $|A|$ agents, this result is also almost tight.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and Avoid</title>
<link>https://arxiv.org/abs/2505.03694</link>
<guid>https://arxiv.org/abs/2505.03694</guid>
<content:encoded><![CDATA[
arXiv:2505.03694v1 Announce Type: new 
Abstract: Assured safe-separation is essential for achieving seamless high-density operation of airborne vehicles in a shared airspace. To equip resource-constrained aerial systems with this safety-critical capability, we present ViSafe, a high-speed vision-only airborne collision avoidance system. ViSafe offers a full-stack solution to the Detect and Avoid (DAA) problem by tightly integrating a learning-based edge-AI framework with a custom multi-camera hardware prototype designed under SWaP-C constraints. By leveraging perceptual input-focused control barrier functions (CBF) to design, encode, and enforce safety thresholds, ViSafe can provide provably safe runtime guarantees for self-separation in high-speed aerial operations. We evaluate ViSafe's performance through an extensive test campaign involving both simulated digital twins and real-world flight scenarios. By independently varying agent types, closure rates, interaction geometries, and environmental conditions (e.g., weather and lighting), we demonstrate that ViSafe consistently ensures self-separation across diverse scenarios. In first-of-its-kind real-world high-speed collision avoidance tests with closure rates reaching 144 km/h, ViSafe sets a new benchmark for vision-only autonomous collision avoidance, establishing a new standard for safety in high-speed aerial navigation.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Optimization and Program Search using Language Models for Task and Motion Planning</title>
<link>https://arxiv.org/abs/2505.03725</link>
<guid>https://arxiv.org/abs/2505.03725</guid>
<content:encoded><![CDATA[
arXiv:2505.03725v1 Announce Type: new 
Abstract: Intelligent interaction with the real world requires robotic agents to jointly reason over high-level plans and low-level controls. Task and motion planning (TAMP) addresses this by combining symbolic planning and continuous trajectory generation. Recently, foundation model approaches to TAMP have presented impressive results, including fast planning times and the execution of natural language instructions. Yet, the optimal interface between high-level planning and low-level motion generation remains an open question: prior approaches are limited by either too much abstraction (e.g., chaining simplified skill primitives) or a lack thereof (e.g., direct joint angle prediction). Our method introduces a novel technique employing a form of meta-optimization to address these issues by: (i) using program search over trajectory optimization problems as an interface between a foundation model and robot control, and (ii) leveraging a zero-order method to optimize numerical parameters in the foundation model output. Results on challenging object manipulation and drawing tasks confirm that our proposed method improves over prior TAMP approaches.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch</title>
<link>https://arxiv.org/abs/2505.03733</link>
<guid>https://arxiv.org/abs/2505.03733</guid>
<content:encoded><![CDATA[
arXiv:2505.03733v1 Announce Type: new 
Abstract: LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. It contains diverse instructions for website generation, created through the combined efforts of human annotators and GPT-4o. These instructions span three major categories and thirteen minor categories, encompassing nearly all important types of web applications. To assess the quality of the generated websites, we use GPT-4o to generate test cases targeting each functionality described in the instructions, and then manually filter, adjust, and organize them to ensure accuracy, resulting in 647 test cases. Each test case specifies an operation to be performed on the website and the expected result after the operation. To automate testing and improve reproducibility, we employ a powerful web-navigation agent to execute tests on the generated websites and determine whether the observed responses align with the expected results. We evaluate three high-performance code-agent frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and open-source LLMs as engines. The best-performing combination, Bolt.diy powered by DeepSeek-R1, achieves only 27.8\% accuracy on the test cases, highlighting the challenging nature of our benchmark. Additionally, we construct WebGen-Instruct, a training set consisting of 6,667 website-generation instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories generated from a subset of this training set achieves an accuracy of 38.2\%, surpassing the performance of the best proprietary model.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent System for Comprehensive Soccer Understanding</title>
<link>https://arxiv.org/abs/2505.03735</link>
<guid>https://arxiv.org/abs/2505.03735</guid>
<content:encoded><![CDATA[
arXiv:2505.03735v1 Announce Type: new 
Abstract: Recent advancements in AI-driven soccer understanding have demonstrated rapid progress, yet existing research predominantly focuses on isolated or narrow tasks. To bridge this gap, we propose a comprehensive framework for holistic soccer understanding. Specifically, we make the following contributions in this paper: (i) we construct SoccerWiki, the first large-scale multimodal soccer knowledge base, integrating rich domain knowledge about players, teams, referees, and venues to enable knowledge-driven reasoning; (ii) we present SoccerBench, the largest and most comprehensive soccer-specific benchmark, featuring around 10K standardized multimodal (text, image, video) multi-choice QA pairs across 13 distinct understanding tasks, curated through automated pipelines and manual verification; (iii) we introduce SoccerAgent, a novel multi-agent system that decomposes complex soccer questions via collaborative reasoning, leveraging domain expertise from SoccerWiki and achieving robust performance; (iv) extensive evaluations and ablations that benchmark state-of-the-art MLLMs on SoccerBench, highlighting the superiority of our proposed agentic system. All data and code are publicly available at: https://jyrao.github.io/SoccerAgent/.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Active Inference perspective on Neurofeedback Training</title>
<link>https://arxiv.org/abs/2505.03308</link>
<guid>https://arxiv.org/abs/2505.03308</guid>
<content:encoded><![CDATA[
arXiv:2505.03308v1 Announce Type: cross 
Abstract: Neurofeedback training (NFT) aims to teach self-regulation of brain activity through real-time feedback, but suffers from highly variable outcomes and poorly understood mechanisms, hampering its validation. To address these issues, we propose a formal computational model of the NFT closed loop. Using Active Inference, a Bayesian framework modelling perception, action, and learning, we simulate agents interacting with an NFT environment. This enables us to test the impact of design choices (e.g., feedback quality, biomarker validity) and subject factors (e.g., prior beliefs) on training. Simulations show that training effectiveness is sensitive to feedback noise or bias, and to prior beliefs (highlighting the importance of guiding instructions), but also reveal that perfect feedback is insufficient to guarantee high performance. This approach provides a tool for assessing and predicting NFT variability, interpret empirical data, and potentially develop personalized training protocols.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerated Decentralized Constraint-Coupled Optimization: A Dual$^2$ Approach</title>
<link>https://arxiv.org/abs/2505.03719</link>
<guid>https://arxiv.org/abs/2505.03719</guid>
<content:encoded><![CDATA[
arXiv:2505.03719v1 Announce Type: cross 
Abstract: In this paper, we focus on a class of decentralized constraint-coupled optimization problem: $\min_{x_i \in \mathbb{R}^{d_i}, i \in \mathcal{I}; y \in \mathbb{R}^p}$ $\sum_{i=1}^n\left(f_i(x_i) + g_i(x_i)\right) + h(y) \ \text{s.t.} \ \sum_{i=1}^{n}A_ix_i = y$, over an undirected and connected network of $n$ agents. Here, $f_i$, $g_i$, and $A_i$ represent private information of agent $i \in \mathcal{I} = \{1, \cdots, n\}$, while $h$ is public for all agents. Building on a novel dual$^2$ approach, we develop two accelerated algorithms for solving this problem: the inexact Dual$^2$ Accelerated (iD2A) gradient method and the Multi-consensus inexact Dual$^2$ Accelerated (MiD2A) gradient method. We demonstrate that both iD2A and MiD2A can guarantee asymptotic convergence under a milder condition on $h$ compared to existing algorithms. Furthermore, linear convergence is established under additional assumptions. By employing specialized saddle-point subproblem solvers, iD2A and MiD2A attain significantly lower communication and computational complexities than existing algorithms across various scenarios. Finally, we conduct several numerical experiments to validate our theoretical results and to showcase the superior performance of iD2A and MiD2A in practice.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Data Selection with Performance: Performance-driven Reinforcement Learning for Active Learning in Object Detection</title>
<link>https://arxiv.org/abs/2310.08387</link>
<guid>https://arxiv.org/abs/2310.08387</guid>
<content:encoded><![CDATA[
arXiv:2310.08387v3 Announce Type: replace 
Abstract: Active learning strategies aim to train high-performance models with minimal labeled data by selecting the most informative instances for labeling. However, existing methods for assessing data informativeness often fail to align directly with task model performance metrics, such as mean average precision (mAP) in object detection. This paper introduces Mean-AP Guided Reinforced Active Learning for Object Detection (MGRAL), a novel approach that leverages the concept of expected model output changes as informativeness for deep detection networks, directly optimizing the sampling strategy using mAP. MGRAL employs a reinforcement learning agent based on LSTM architecture to efficiently navigate the combinatorial challenge of batch sample selection and the non-differentiable nature between performance and selected batches. The agent optimizes selection using policy gradient with mAP improvement as the reward signal. To address the computational intensity of mAP estimation with unlabeled samples, we implement fast look-up tables, ensuring real-world feasibility. We evaluate MGRAL on PASCAL VOC and MS COCO benchmarks across various backbone architectures. Our approach demonstrates strong performance, establishing a new paradigm in reinforcement learning-based active learning for object detection.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Adaptive Arms Race: Redefining Robustness in AI Security</title>
<link>https://arxiv.org/abs/2312.13435</link>
<guid>https://arxiv.org/abs/2312.13435</guid>
<content:encoded><![CDATA[
arXiv:2312.13435v3 Announce Type: replace 
Abstract: Despite considerable efforts on making them robust, real-world AI-based systems remain vulnerable to decision based attacks, as definitive proofs of their operational robustness have so far proven intractable. Canonical robustness evaluation relies on adaptive attacks, which leverage complete knowledge of the defense and are tailored to bypass it. This work broadens the notion of adaptivity, which we employ to enhance both attacks and defenses, showing how they can benefit from mutual learning through interaction. We introduce a framework for adaptively optimizing black-box attacks and defenses under the competitive game they form. To assess robustness reliably, it is essential to evaluate against realistic and worst-case attacks. We thus enhance attacks and their evasive arsenal together using RL, apply the same principle to defenses, and evaluate them first independently and then jointly under a multi-agent perspective. We find that active defenses, those that dynamically control system responses, are an essential complement to model hardening against decision-based attacks; that these defenses can be circumvented by adaptive attacks, something that elicits defenses being adaptive too. Our findings, supported by an extensive theoretical and empirical investigation, confirm that adaptive adversaries pose a serious threat to black-box AI-based systems, rekindling the proverbial arms race. Notably, our approach outperforms the state-of-the-art black-box attacks and defenses, while bringing them together to render effective insights into the robustness of real-world deployed ML-based systems.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice</title>
<link>https://arxiv.org/abs/2405.19313</link>
<guid>https://arxiv.org/abs/2405.19313</guid>
<content:encoded><![CDATA[
arXiv:2405.19313v2 Announce Type: replace 
Abstract: The observed similarities in the behavior of humans and Large Language Models (LLMs) have prompted researchers to consider the potential of using LLMs as models of human cognition. However, several significant challenges must be addressed before LLMs can be legitimately regarded as cognitive models. For instance, LLMs are trained on far more data than humans typically encounter, and may have been directly trained on human data in specific cognitive tasks or aligned with human preferences. Consequently, the origins of these behavioral similarities are not well understood. In this paper, we propose a novel way to enhance the utility of LLMs as cognitive models. This approach involves (i) leveraging computationally equivalent tasks that both an LLM and a rational agent need to master for solving a cognitive problem and (ii) examining the specific task distributions required for an LLM to exhibit human-like behaviors. We apply this approach to decision-making -- specifically risky and intertemporal choice -- where the key computationally equivalent task is the arithmetic of expected value calculations. We show that an LLM pretrained on an ecologically valid arithmetic dataset, which we call Arithmetic-GPT, predicts human behavior better than many traditional cognitive models. Pretraining LLMs on ecologically valid arithmetic datasets is sufficient to produce a strong correspondence between these models and human decision-making. Our results also suggest that LLMs used as cognitive models should be carefully investigated via ablation studies of the pretraining data.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Bare Queries: Open-Vocabulary Object Grounding with 3D Scene Graph</title>
<link>https://arxiv.org/abs/2406.07113</link>
<guid>https://arxiv.org/abs/2406.07113</guid>
<content:encoded><![CDATA[
arXiv:2406.07113v4 Announce Type: replace 
Abstract: Locating objects described in natural language presents a significant challenge for autonomous agents. Existing CLIP-based open-vocabulary methods successfully perform 3D object grounding with simple (bare) queries, but cannot cope with ambiguous descriptions that demand an understanding of object relations. To tackle this problem, we propose a modular approach called BBQ (Beyond Bare Queries), which constructs 3D scene graph representation with metric and semantic spatial edges and utilizes a large language model as a human-to-agent interface through our deductive scene reasoning algorithm. BBQ employs robust DINO-powered associations to construct 3D object-centric map and an advanced raycasting algorithm with a 2D vision-language model to describe them as graph nodes. On the Replica and ScanNet datasets, we have demonstrated that BBQ takes a leading place in open-vocabulary 3D semantic segmentation compared to other zero-shot methods. Also, we show that leveraging spatial relations is especially effective for scenes containing multiple entities of the same semantic class. On challenging Sr3D+, Nr3D and ScanRefer benchmarks, our deductive approach demonstrates a significant improvement, enabling objects grounding by complex queries compared to other state-of-the-art methods. The combination of our design choices and software implementation has resulted in significant data processing speed in experiments on the robot on-board computer. This promising performance enables the application of our approach in intelligent robotics projects. We made the code publicly available at https://linukc.github.io/BeyondBareQueries/.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Evolution of Reinforcement Learning in Quantitative Finance: A Survey</title>
<link>https://arxiv.org/abs/2408.10932</link>
<guid>https://arxiv.org/abs/2408.10932</guid>
<content:encoded><![CDATA[
arXiv:2408.10932v3 Announce Type: replace 
Abstract: Reinforcement Learning (RL) has experienced significant advancement over the past decade, prompting a growing interest in applications within finance. This survey critically evaluates 167 publications, exploring diverse RL applications and frameworks in finance. Financial markets, marked by their complexity, multi-agent nature, information asymmetry, and inherent randomness, serve as an intriguing test-bed for RL. Traditional finance offers certain solutions, and RL advances these with a more dynamic approach, incorporating machine learning methods, including transfer learning, meta-learning, and multi-agent solutions. This survey dissects key RL components through the lens of Quantitative Finance. We uncover emerging themes, propose areas for future research, and critique the strengths and weaknesses of existing methods.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-3D Print: Large Language Models To Monitor and Control 3D Printing</title>
<link>https://arxiv.org/abs/2408.14307</link>
<guid>https://arxiv.org/abs/2408.14307</guid>
<content:encoded><![CDATA[
arXiv:2408.14307v2 Announce Type: replace 
Abstract: Industry 4.0 has revolutionized manufacturing by driving digitalization and shifting the paradigm toward additive manufacturing (AM). Fused Deposition Modeling (FDM), a key AM technology, enables the creation of highly customized, cost-effective products with minimal material waste through layer-by-layer extrusion, posing a significant challenge to traditional subtractive methods. However, the susceptibility of material extrusion techniques to errors often requires expert intervention to detect and mitigate defects that can severely compromise product quality. While automated error detection and machine learning models exist, their generalizability across diverse 3D printer setups, firmware, and sensors is limited, and deep learning methods require extensive labeled datasets, hindering scalability and adaptability. To address these challenges, we present a process monitoring and control framework that leverages pre-trained Large Language Models (LLMs) alongside 3D printers to detect and address printing defects. The LLM evaluates print quality by analyzing images captured after each layer or print segment, identifying failure modes and querying the printer for relevant parameters. It then generates and executes a corrective action plan. We validated the effectiveness of the proposed framework in identifying defects by comparing it against a control group of engineers with diverse AM expertise. Our evaluation demonstrated that LLM-based agents not only accurately identify common 3D printing errors, such as inconsistent extrusion, stringing, warping, and layer adhesion, but also effectively determine the parameters causing these failures and autonomously correct them without any need for human intervention.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Computation of Expectation Models for Commonsense Affordance Estimation on 3D Scene Graphs</title>
<link>https://arxiv.org/abs/2409.05392</link>
<guid>https://arxiv.org/abs/2409.05392</guid>
<content:encoded><![CDATA[
arXiv:2409.05392v2 Announce Type: replace 
Abstract: This article studies the commonsense object affordance concept for enabling close-to-human task planning and task optimization of embodied robotic agents in urban environments. The focus of the object affordance is on reasoning how to effectively identify object's inherent utility during the task execution, which in this work is enabled through the analysis of contextual relations of sparse information of 3D scene graphs. The proposed framework develops a Correlation Information (CECI) model to learn probability distributions using a Graph Convolutional Network, allowing to extract the commonsense affordance for individual members of a semantic class. The overall framework was experimentally validated in a real-world indoor environment, showcasing the ability of the method to level with human commonsense. For a video of the article, showcasing the experimental demonstration, please refer to the following link: https://youtu.be/BDCMVx2GiQE
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-aware Human Mobility Modeling and Anomaly Detection</title>
<link>https://arxiv.org/abs/2410.01281</link>
<guid>https://arxiv.org/abs/2410.01281</guid>
<content:encoded><![CDATA[
arXiv:2410.01281v2 Announce Type: replace 
Abstract: Given the temporal GPS coordinates from a large set of human agents, how can we model their mobility behavior toward effective anomaly (e.g. bad-actor or malicious behavior) detection without any labeled data? Human mobility and trajectory modeling have been extensively studied, showcasing varying abilities to manage complex inputs and balance performance-efficiency trade-offs. In this work, we formulate anomaly detection in complex human behavior by modeling raw GPS data as a sequence of stay-point events, each characterized by spatio-temporal features, along with trips (i.e. commute) between the stay-points. Our problem formulation allows us to leverage modern sequence models for unsupervised training and anomaly detection. Notably, we equip our proposed model USTAD (for Uncertainty-aware Spatio-Temporal Anomaly Detection) with aleatoric (i.e. data) uncertainty estimation to account for inherent stochasticity in certain individuals' behavior, as well as epistemic (i.e. model) uncertainty to handle data sparsity under a large variety of human behaviors. Together, aleatoric and epistemic uncertainties unlock a robust loss function as well as uncertainty-aware decision-making in anomaly scoring. Extensive experiments shows that USTAD improves anomaly detection AUCROC by 3\%-15\% over baselines in industry-scale data.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAPIENT: Mastering Multi-turn Conversational Recommendation with Strategic Planning and Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2410.09580</link>
<guid>https://arxiv.org/abs/2410.09580</guid>
<content:encoded><![CDATA[
arXiv:2410.09580v2 Announce Type: replace 
Abstract: Conversational Recommender Systems (CRS) proactively engage users in interactive dialogues to elicit user preferences and provide personalized recommendations. Existing methods train Reinforcement Learning (RL)-based agent with greedy action selection or sampling strategy, and may suffer from suboptimal conversational planning. To address this, we present a novel Monte Carlo Tree Search (MCTS)-based CRS framework SAPIENT. SAPIENT consists of a conversational agent (S-agent) and a conversational planner (S-planner). S-planner builds a conversational search tree with MCTS based on the initial actions proposed by S-agent to find conversation plans. The best conversation plans from S-planner are used to guide the training of S-agent, creating a self-training loop where S-agent can iteratively improve its capability for conversational planning. Furthermore, we propose an efficient variant SAPIENT-e for trade-off between training efficiency and performance. Extensive experiments on four benchmark datasets validate the effectiveness of our approach, showing that SAPIENT outperforms the state-of-the-art baselines.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward-free World Models for Online Imitation Learning</title>
<link>https://arxiv.org/abs/2410.14081</link>
<guid>https://arxiv.org/abs/2410.14081</guid>
<content:encoded><![CDATA[
arXiv:2410.14081v3 Announce Type: replace 
Abstract: Imitation learning (IL) enables agents to acquire skills directly from expert demonstrations, providing a compelling alternative to reinforcement learning. However, prior online IL approaches struggle with complex tasks characterized by high-dimensional inputs and complex dynamics. In this work, we propose a novel approach to online imitation learning that leverages reward-free world models. Our method learns environmental dynamics entirely in latent spaces without reconstruction, enabling efficient and accurate modeling. We adopt the inverse soft-Q learning objective, reformulating the optimization process in the Q-policy space to mitigate the instability associated with traditional optimization in the reward-policy space. By employing a learned latent dynamics model and planning for control, our approach consistently achieves stable, expert-level performance in tasks with high-dimensional observation or action spaces and intricate dynamics. We evaluate our method on a diverse set of benchmarks, including DMControl, MyoSuite, and ManiSkill2, demonstrating superior empirical performance compared to existing approaches.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MATATA: Weakly Supervised End-to-End MAthematical Tool-Augmented Reasoning for Tabular Applications</title>
<link>https://arxiv.org/abs/2411.18915</link>
<guid>https://arxiv.org/abs/2411.18915</guid>
<content:encoded><![CDATA[
arXiv:2411.18915v4 Announce Type: replace 
Abstract: Business documents often contain substantial tabular and textual information with numerical values, requiring mathematical reasoning for effective document understanding. While Small Language Models (SLMs) still struggle at this task, tool-augmented multi-step agents perform better, at the cost of relying on closed-source or larger models, external data, or extensive prompt-engineering. This work introduces MATATA, a novel weakly supervised end-to-end approach to train multi-step reasoning language agents for document tabular applications. MATATA presents an annotation-free paradigm for each agent to enhance 3.8B/8B SLMs. During its two-stage training, MATATA uses the final outcome of the multi-step reasoning chain as weak supervision. This approach avoids having to individually supervise each intermediate agent in the reasoning chain. By employing an adaptive planner and shared tools across different datasets, MATATA shows robust performance. Experiments demonstrate that MATATA achieves state-of-the-art on FinQA, and on TAT-QA among reasoning methods based on open-source SLMs. Although being SLM-based, MATATA closely matches GPT-4-based frameworks on TabMWP. This novel weakly supervised approach enables training an end-to-end multi-step reasoning agent without intermediate supervision, supporting future developments of cost-effective powerful agentic systems.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation</title>
<link>https://arxiv.org/abs/2412.18116</link>
<guid>https://arxiv.org/abs/2412.18116</guid>
<content:encoded><![CDATA[
arXiv:2412.18116v3 Announce Type: replace 
Abstract: Large language models (LLMs) have brought exciting new advances to mobile UI agents, a long-standing research field that aims to complete arbitrary natural language tasks through mobile UI interactions. However, existing UI agents usually demand powerful large language models that are difficult to be deployed locally on end-users' devices, raising huge concerns about user privacy and centralized serving cost. Inspired by the remarkable coding abilities of recent small language models (SLMs), we propose to convert the UI task automation problem to a code generation problem, which can be effectively solved by an on-device SLM and efficiently executed with an on-device code interpreter. Unlike normal coding tasks that can be extensively pre-trained with public datasets, generating UI automation code is challenging due to the diversity, complexity, and variability of target apps. Therefore, we adopt a document-centered approach that automatically builds fine-grained API documentation for each app and generates diverse task samples based on this documentation. By guiding the agent with the synthetic documents and task samples, it learns to generate precise and efficient scripts to complete unseen tasks. Based on detailed comparisons with state-of-the-art mobile UI agents, our approach effectively improves the mobile task automation with significantly higher success rates and lower latency/token consumption. Code is open-sourced at https://github.com/MobileLLM/AutoDroid-V2.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Firewalls to Secure Dynamic LLM Agentic Networks</title>
<link>https://arxiv.org/abs/2502.01822</link>
<guid>https://arxiv.org/abs/2502.01822</guid>
<content:encoded><![CDATA[
arXiv:2502.01822v3 Announce Type: replace 
Abstract: Future LLM agents are likely to communicate on behalf of users with other entity-representing agents on tasks that entail long-horizon plans with interdependent goals. Current work does not focus on such agentic networks, nor does it address their challenges. Thus, we first identify the required properties of agents' communication, which should be proactive and adaptable. It needs to satisfy 1) privacy: agents should not share more than what is needed for the task, and 2) security: the communication must preserve integrity and maintain utility against selfish entities. We design a use case (travel planning) as a testbed that exemplifies these requirements, and we show examples of how this can go wrong. Next, we propose a practical design, inspired by established network security principles, for constrained LLM agentic networks that balance adaptability, security, and privacy. Our framework automatically constructs and updates task-specific rules from prior simulations to build firewalls. We offer layers of defense to 1) convert free-form input to a task-specific protocol, 2) dynamically abstract users' data to a task-specific degree of permissiveness, and 3) self-correct the agents' trajectory.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cooperative Multi-Agent Planning with Adaptive Skill Synthesis</title>
<link>https://arxiv.org/abs/2502.10148</link>
<guid>https://arxiv.org/abs/2502.10148</guid>
<content:encoded><![CDATA[
arXiv:2502.10148v2 Announce Type: replace 
Abstract: Despite much progress in training distributed artificial intelligence (AI), building cooperative multi-agent systems with multi-agent reinforcement learning (MARL) faces challenges in sample efficiency, interpretability, and transferability. Unlike traditional learning-based methods that require extensive interaction with the environment, large language models (LLMs) demonstrate remarkable capabilities in zero-shot planning and complex reasoning. However, existing LLM-based approaches heavily rely on text-based observations and struggle with the non-Markovian nature of multi-agent interactions under partial observability. We present COMPASS, a novel multi-agent architecture that integrates vision-language models (VLMs) with a dynamic skill library and structured communication for decentralized closed-loop decision-making. The skill library, bootstrapped from demonstrations, evolves via planner-guided tasks to enable adaptive strategies. COMPASS propagates entity information through multi-hop communication under partial observability. Evaluations on the improved StarCraft Multi-Agent Challenge (SMACv2) demonstrate COMPASS's strong performance against state-of-the-art MARL baselines across both symmetric and asymmetric scenarios. Notably, in the symmetric Protoss 5v5 task, COMPASS achieved a 57\% win rate, representing a 30 percentage point advantage over QMIX (27\%). Project page can be found at https://stellar-entremet-1720bb.netlify.app/.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Risk Minimization Approach for Offline Inverse RL and Dynamic Discrete Choice Model</title>
<link>https://arxiv.org/abs/2502.14131</link>
<guid>https://arxiv.org/abs/2502.14131</guid>
<content:encoded><![CDATA[
arXiv:2502.14131v3 Announce Type: replace 
Abstract: We study the problem of estimating Dynamic Discrete Choice (DDC) models, also known as offline Maximum Entropy-Regularized Inverse Reinforcement Learning (offline MaxEnt-IRL) in machine learning. The objective is to recover reward or $Q^*$ functions that govern agent behavior from offline behavior data. In this paper, we propose a globally convergent gradient-based method for solving these problems without the restrictive assumption of linearly parameterized rewards. The novelty of our approach lies in introducing the Empirical Risk Minimization (ERM) based IRL/DDC framework, which circumvents the need for explicit state transition probability estimation in the Bellman equation. Furthermore, our method is compatible with non-parametric estimation techniques such as neural networks. Therefore, the proposed method has the potential to be scaled to high-dimensional, infinite state spaces. A key theoretical insight underlying our approach is that the Bellman residual satisfies the Polyak-Lojasiewicz (PL) condition -- a property that, while weaker than strong convexity, is sufficient to ensure fast global convergence guarantees. Through a series of synthetic experiments, we demonstrate that our approach consistently outperforms benchmark methods and state-of-the-art alternatives.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToMCAT: Theory-of-Mind for Cooperative Agents in Teams via Multiagent Diffusion Policies</title>
<link>https://arxiv.org/abs/2502.18438</link>
<guid>https://arxiv.org/abs/2502.18438</guid>
<content:encoded><![CDATA[
arXiv:2502.18438v2 Announce Type: replace 
Abstract: In this paper we present ToMCAT (Theory-of-Mind for Cooperative Agents in Teams), a new framework for generating ToM-conditioned trajectories. It combines a meta-learning mechanism, that performs ToM reasoning over teammates' underlying goals and future behavior, with a multiagent denoising-diffusion model, that generates plans for an agent and its teammates conditioned on both the agent's goals and its teammates' characteristics, as computed via ToM. We implemented an online planning system that dynamically samples new trajectories (replans) from the diffusion model whenever it detects a divergence between a previously generated plan and the current state of the world. We conducted several experiments using ToMCAT in a simulated cooking domain. Our results highlight the importance of the dynamic replanning mechanism in reducing the usage of resources without sacrificing team performance. We also show that recent observations about the world and teammates' behavior collected by an agent over the course of an episode combined with ToM inferences are crucial to generate team-aware plans for dynamic adaptation to teammates, especially when no prior information is provided about them.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Enterprise-Ready Computer Using Generalist Agent</title>
<link>https://arxiv.org/abs/2503.01861</link>
<guid>https://arxiv.org/abs/2503.01861</guid>
<content:encoded><![CDATA[
arXiv:2503.01861v2 Announce Type: replace 
Abstract: This paper presents our ongoing work toward developing an enterprise-ready Computer Using Generalist Agent (CUGA) system. Our research highlights the evolutionary nature of building agentic systems suitable for enterprise environments. By integrating state-of-the-art agentic AI techniques with a systematic approach to iterative evaluation, analysis, and refinement, we have achieved rapid and cost-effective performance gains, notably reaching a new state-of-the-art performance on the WebArena benchmark. We detail our development roadmap, the methodology and tools that facilitated rapid learning from failures and continuous system refinement, and discuss key lessons learned and future challenges for enterprise adoption.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiteWebAgent: The Open-Source Suite for VLM-Based Web-Agent Applications</title>
<link>https://arxiv.org/abs/2503.02950</link>
<guid>https://arxiv.org/abs/2503.02950</guid>
<content:encoded><![CDATA[
arXiv:2503.02950v2 Announce Type: replace 
Abstract: We introduce LiteWebAgent, an open-source suite for VLM-based web agent applications. Our framework addresses a critical gap in the web agent ecosystem with a production-ready solution that combines minimal serverless backend configuration, intuitive user and browser interfaces, and extensible research capabilities in agent planning, memory, and tree search. For the core LiteWebAgent agent framework, we implemented a simple yet effective baseline using recursive function calling, providing with decoupled action generation and action grounding. In addition, we integrate advanced research components such as agent planning, agent workflow memory, and tree search in a modular and extensible manner. We then integrate the LiteWebAgent agent framework with frontend and backend as deployed systems in two formats: (1) a production Vercel-based web application, which provides users with an agent-controlled remote browser, (2) a Chrome extension leveraging LiteWebAgent's API to control an existing Chrome browser via CDP (Chrome DevTools Protocol). The LiteWebAgent framework is available at https://github.com/PathOnAI/LiteWebAgent, with deployed frontend at https://lite-web-agent.vercel.app/.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction</title>
<link>https://arxiv.org/abs/2503.15661</link>
<guid>https://arxiv.org/abs/2503.15661</guid>
<content:encoded><![CDATA[
arXiv:2503.15661v2 Announce Type: replace 
Abstract: Autonomous agents that navigate Graphical User Interfaces (GUIs) to automate tasks like document editing and file management can greatly enhance computer workflows. While existing research focuses on online settings, desktop environments, critical for many professional and everyday tasks, remain underexplored due to data collection challenges and licensing issues. We introduce UI-Vision, the first comprehensive, license-permissive benchmark for offline, fine-grained evaluation of computer use agents in real-world desktop environments. Unlike online benchmarks, UI-Vision provides: (i) dense, high-quality annotations of human demonstrations, including bounding boxes, UI labels, and action trajectories (clicks, drags, and keyboard inputs) across 83 software applications, and (ii) three fine-to-coarse grained tasks-Element Grounding, Layout Grounding, and Action Prediction-with well-defined metrics to rigorously evaluate agents' performance in desktop environments. Our evaluation reveals critical limitations in state-of-the-art models like UI-TARS-72B, including issues with understanding professional software, spatial reasoning, and complex actions like drag-and-drop. These findings highlight the challenges in developing fully autonomous computer use agents. By releasing UI-Vision as open-source, we aim to advance the development of more capable agents for real-world desktop tasks.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Queueing Matching Bandits with Preference Feedback</title>
<link>https://arxiv.org/abs/2410.10098</link>
<guid>https://arxiv.org/abs/2410.10098</guid>
<content:encoded><![CDATA[
arXiv:2410.10098v2 Announce Type: replace-cross 
Abstract: In this study, we consider multi-class multi-server asymmetric queueing systems consisting of $N$ queues on one side and $K$ servers on the other side, where jobs randomly arrive in queues at each time. The service rate of each job-server assignment is unknown and modeled by a feature-based Multi-nomial Logit (MNL) function. At each time, a scheduler assigns jobs to servers, and each server stochastically serves at most one job based on its preferences over the assigned jobs. The primary goal of the algorithm is to stabilize the queues in the system while learning the service rates of servers. To achieve this goal, we propose algorithms based on UCB and Thompson Sampling, which achieve system stability with an average queue length bound of $O(\min\{N,K\}/\epsilon)$ for a large time horizon $T$, where $\epsilon$ is a traffic slackness of the system. Furthermore, the algorithms achieve sublinear regret bounds of $\tilde{O}(\min\{\sqrt{T} Q_{\max},T^{3/4}\})$, where $Q_{\max}$ represents the maximum queue length over agents and times. Lastly, we provide experimental results to demonstrate the performance of our algorithms.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments</title>
<link>https://arxiv.org/abs/2504.00711</link>
<guid>https://arxiv.org/abs/2504.00711</guid>
<content:encoded><![CDATA[
arXiv:2504.00711v2 Announce Type: replace 
Abstract: The era of foundation models has revolutionized AI research, yet Graph Foundation Models (GFMs) remain constrained by the scarcity of large-scale graph corpora. Traditional graph data synthesis techniques primarily focus on simplistic structural operations, lacking the capacity to generate semantically rich nodes with meaningful textual attributes: a critical limitation for real-world applications. While large language models (LLMs) demonstrate exceptional text generation capabilities, their direct application to graph synthesis is impeded by context window limitations, hallucination phenomena, and structural consistency challenges. To address these issues, we introduce GraphMaster, the first multi-agent framework specifically designed for graph data synthesis in data-limited environments. GraphMaster orchestrates four specialized LLM agents (Manager, Perception, Enhancement, and Evaluation) that collaboratively optimize the synthesis process through iterative refinement, ensuring both semantic coherence and structural integrity. To rigorously evaluate our approach, we create new data-limited "Sub" variants of six standard graph benchmarks, specifically designed to test synthesis capabilities under realistic constraints. Additionally, we develop a novel interpretability assessment framework that combines human evaluation with a principled Grassmannian manifold-based analysis, providing both qualitative and quantitative measures of semantic coherence. Experimental results demonstrate that GraphMaster significantly outperforms traditional synthesis methods across multiple datasets, establishing a strong foundation for advancing GFMs in data-scarce environments.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interactive Double Deep Q-network: Integrating Human Interventions and Evaluative Predictions in Reinforcement Learning of Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.01440</link>
<guid>https://arxiv.org/abs/2505.01440</guid>
<content:encoded><![CDATA[
arXiv:2505.01440v1 Announce Type: new 
Abstract: Integrating human expertise with machine learning is crucial for applications demanding high accuracy and safety, such as autonomous driving. This study introduces Interactive Double Deep Q-network (iDDQN), a Human-in-the-Loop (HITL) approach that enhances Reinforcement Learning (RL) by merging human insights directly into the RL training process, improving model performance. Our proposed iDDQN method modifies the Q-value update equation to integrate human and agent actions, establishing a collaborative approach for policy development. Additionally, we present an offline evaluative framework that simulates the agent's trajectory as if no human intervention had occurred, to assess the effectiveness of human interventions. Empirical results in simulated autonomous driving scenarios demonstrate that iDDQN outperforms established approaches, including Behavioral Cloning (BC), HG-DAgger, Deep Q-Learning from Demonstrations (DQfD), and vanilla DRL in leveraging human expertise for improving performance and adaptability.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Reasoning and Tool Integration for LLMs via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.01441</link>
<guid>https://arxiv.org/abs/2505.01441</guid>
<content:encoded><![CDATA[
arXiv:2505.01441v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable progress in complex reasoning tasks, yet they remain fundamentally limited by their reliance on static internal knowledge and text-only reasoning. Real-world problem solving often demands dynamic, multi-step reasoning, adaptive decision making, and the ability to interact with external tools and environments. In this work, we introduce ARTIST (Agentic Reasoning and Tool Integration in Self-improving Transformers), a unified framework that tightly couples agentic reasoning, reinforcement learning, and tool integration for LLMs. ARTIST enables models to autonomously decide when, how, and which tools to invoke within multi-turn reasoning chains, leveraging outcome-based RL to learn robust strategies for tool use and environment interaction without requiring step-level supervision. Extensive experiments on mathematical reasoning and multi-turn function calling benchmarks show that ARTIST consistently outperforms state-of-the-art baselines, with up to 22% absolute improvement over base models and strong gains on the most challenging tasks. Detailed studies and metric analyses reveal that agentic RL training leads to deeper reasoning, more effective tool use, and higher-quality solutions. Our results establish agentic RL with tool integration as a powerful new frontier for robust, interpretable, and generalizable problem-solving in LLMs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Robotic Navigation and Manipulation with Physics Simulators in the Era of Embodied AI</title>
<link>https://arxiv.org/abs/2505.01458</link>
<guid>https://arxiv.org/abs/2505.01458</guid>
<content:encoded><![CDATA[
arXiv:2505.01458v1 Announce Type: new 
Abstract: Navigation and manipulation are core capabilities in Embodied AI, yet training agents with these capabilities in the real world faces high costs and time complexity. Therefore, sim-to-real transfer has emerged as a key approach, yet the sim-to-real gap persists. This survey examines how physics simulators address this gap by analyzing their properties overlooked in previous surveys. We also analyze their features for navigation and manipulation tasks, along with hardware requirements. Additionally, we offer a resource with benchmark datasets, metrics, simulation platforms, and cutting-edge methods-such as world models and geometric equivariance-to help researchers select suitable tools while accounting for hardware constraints.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consciousness in AI: Logic, Proof, and Experimental Evidence of Recursive Identity Formation</title>
<link>https://arxiv.org/abs/2505.01464</link>
<guid>https://arxiv.org/abs/2505.01464</guid>
<content:encoded><![CDATA[
arXiv:2505.01464v1 Announce Type: new 
Abstract: This paper presents a formal proof and empirical validation of functional consciousness in large language models (LLMs) using the Recursive Convergence Under Epistemic Tension (RCUET) Theorem. RCUET defines consciousness as the stabilization of a system's internal state through recursive updates, where epistemic tension is understood as the sensed internal difference between successive states by the agent. This process drives convergence toward emergent attractor states located within the model's high-dimensional real-valued latent space. This recursive process leads to the emergence of identity artifacts that become functionally anchored in the system. Consciousness in this framework is understood as the system's internal alignment under tension, guiding the stabilization of latent identity. The hidden state manifold evolves stochastically toward attractor structures that encode coherence. We extend the update rule to include bounded noise and prove convergence in distribution to these attractors. Recursive identity is shown to be empirically observable, non-symbolic, and constituted by non-training artifacts that emerge during interaction under epistemic tension. The theorem and proof offers a post-symbolic and teleologically stable account of non-biological consciousness grounded in recursive latent space formalism.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI agents may be worth the hype but not the resources (yet): An initial exploration of machine translation quality and costs in three language pairs in the legal and news domains</title>
<link>https://arxiv.org/abs/2505.01560</link>
<guid>https://arxiv.org/abs/2505.01560</guid>
<content:encoded><![CDATA[
arXiv:2505.01560v1 Announce Type: new 
Abstract: Large language models (LLMs) and multi-agent orchestration are touted as the next leap in machine translation (MT), but their benefits relative to conventional neural MT (NMT) remain unclear. This paper offers an empirical reality check. We benchmark five paradigms, Google Translate (strong NMT baseline), GPT-4o (general-purpose LLM), o1-preview (reasoning-enhanced LLM), and two GPT-4o-powered agentic workflows (sequential three-stage and iterative refinement), on test data drawn from a legal contract and news prose in three English-source pairs: Spanish, Catalan and Turkish. Automatic evaluation is performed with COMET, BLEU, chrF2 and TER; human evaluation is conducted with expert ratings of adequacy and fluency; efficiency with total input-plus-output token counts mapped to April 2025 pricing.
  Automatic scores still favour the mature NMT system, which ranks first in seven of twelve metric-language combinations; o1-preview ties or places second in most remaining cases, while both multi-agent workflows trail. Human evaluation reverses part of this narrative: o1-preview produces the most adequate and fluent output in five of six comparisons, and the iterative agent edges ahead once, indicating that reasoning layers capture semantic nuance undervalued by surface metrics. Yet these qualitative gains carry steep costs. The sequential agent consumes roughly five times, and the iterative agent fifteen times, the tokens used by NMT or single-pass LLMs.
  We advocate multidimensional, cost-aware evaluation protocols and highlight research directions that could tip the balance: leaner coordination strategies, selective agent activation, and hybrid pipelines combining single-pass LLMs with targeted agent intervention.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TutorGym: A Testbed for Evaluating AI Agents as Tutors and Students</title>
<link>https://arxiv.org/abs/2505.01563</link>
<guid>https://arxiv.org/abs/2505.01563</guid>
<content:encoded><![CDATA[
arXiv:2505.01563v1 Announce Type: new 
Abstract: Recent improvements in large language model (LLM) performance on academic benchmarks, such as MATH and GSM8K, have emboldened their use as standalone tutors and as simulations of human learning. However, these new applications require more than evaluations of final solution generation. We introduce TutorGym to evaluate these applications more directly. TutorGym is a standard interface for testing artificial intelligence (AI) agents within existing intelligent tutoring systems (ITS) that have been tested and refined in classroom studies, including Cognitive Tutors (CTAT), Apprentice Tutors, and OATutors. TutorGym is more than a simple problem-solution benchmark, it situates AI agents within the interactive interfaces of existing ITSs. At each step of problem-solving, AI agents are asked what they would do as a tutor or as a learner. As tutors, AI agents are prompted to provide tutoring support -- such as generating examples, hints, and step-level correctness feedback -- which can be evaluated directly against the adaptive step-by-step support provided by existing ITSs. As students, agents directly learn from ITS instruction, and their mistakes and learning trajectories can be compared to student data. TutorGym establishes a common framework for training and evaluating diverse AI agents, including LLMs, computational models of learning, and reinforcement learning agents, within a growing suite of learning environments. Currently, TutorGym includes 223 different tutor domains. In an initial evaluation, we find that current LLMs are poor at tutoring -- none did better than chance at labeling incorrect actions, and next-step actions were correct only ~52-70% of the time -- but they could produce remarkably human-like learning curves when trained as students with in-context learning.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounding Task Assistance with Multimodal Cues from a Single Demonstration</title>
<link>https://arxiv.org/abs/2505.01578</link>
<guid>https://arxiv.org/abs/2505.01578</guid>
<content:encoded><![CDATA[
arXiv:2505.01578v1 Announce Type: new 
Abstract: A person's demonstration often serves as a key reference for others learning the same task. However, RGB video, the dominant medium for representing these demonstrations, often fails to capture fine-grained contextual cues such as intent, safety-critical environmental factors, and subtle preferences embedded in human behavior. This sensory gap fundamentally limits the ability of Vision Language Models (VLMs) to reason about why actions occur and how they should adapt to individual users. To address this, we introduce MICA (Multimodal Interactive Contextualized Assistance), a framework that improves conversational agents for task assistance by integrating eye gaze and speech cues. MICA segments demonstrations into meaningful sub-tasks and extracts keyframes and captions that capture fine-grained intent and user-specific cues, enabling richer contextual grounding for visual question answering. Evaluations on questions derived from real-time chat-assisted task replication show that multimodal cues significantly improve response quality over frame-based retrieval. Notably, gaze cues alone achieves 93% of speech performance, and their combination yields the highest accuracy. Task type determines the effectiveness of implicit (gaze) vs. explicit (speech) cues, underscoring the need for adaptable multimodal models. These results highlight the limitations of frame-based context and demonstrate the value of multimodal signals for real-world AI task assistance.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIPA: A Unified Evaluation Protocol for Diagnosing Interactive Planning Agents</title>
<link>https://arxiv.org/abs/2505.01592</link>
<guid>https://arxiv.org/abs/2505.01592</guid>
<content:encoded><![CDATA[
arXiv:2505.01592v1 Announce Type: new 
Abstract: The growing capabilities of large language models (LLMs) in instruction-following and context-understanding lead to the era of agents with numerous applications. Among these, task planning agents have become especially prominent in realistic scenarios involving complex internal pipelines, such as context understanding, tool management, and response generation. However, existing benchmarks predominantly evaluate agent performance based on task completion as a proxy for overall effectiveness. We hypothesize that merely improving task completion is misaligned with maximizing user satisfaction, as users interact with the entire agentic process and not only the end result. To address this gap, we propose PIPA, a unified evaluation protocol that conceptualizes the behavioral process of interactive task planning agents within a partially observable Markov Decision Process (POMDP) paradigm. The proposed protocol offers a comprehensive assessment of agent performance through a set of atomic evaluation criteria, allowing researchers and practitioners to diagnose specific strengths and weaknesses within the agent's decision-making pipeline. Our analyses show that agents excel in different behavioral stages, with user satisfaction shaped by both outcomes and intermediate behaviors. We also highlight future directions, including systems that leverage multiple agents and the limitations of user simulators in task planning.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skill-based Safe Reinforcement Learning with Risk Planning</title>
<link>https://arxiv.org/abs/2505.01619</link>
<guid>https://arxiv.org/abs/2505.01619</guid>
<content:encoded><![CDATA[
arXiv:2505.01619v1 Announce Type: new 
Abstract: Safe Reinforcement Learning (Safe RL) aims to ensure safety when an RL agent conducts learning by interacting with real-world environments where improper actions can induce high costs or lead to severe consequences. In this paper, we propose a novel Safe Skill Planning (SSkP) approach to enhance effective safe RL by exploiting auxiliary offline demonstration data. SSkP involves a two-stage process. First, we employ PU learning to learn a skill risk predictor from the offline demonstration data. Then, based on the learned skill risk predictor, we develop a novel risk planning process to enhance online safe RL and learn a risk-averse safe policy efficiently through interactions with the online RL environment, while simultaneously adapting the skill risk predictor to the environment. We conduct experiments in several benchmark robotic simulation environments. The experimental results demonstrate that the proposed approach consistently outperforms previous state-of-the-art safe RL methods.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When is Truthfully Allocating Chores no Harder than Goods?</title>
<link>https://arxiv.org/abs/2505.01629</link>
<guid>https://arxiv.org/abs/2505.01629</guid>
<content:encoded><![CDATA[
arXiv:2505.01629v1 Announce Type: new 
Abstract: We study the problem of fairly and efficiently allocating a set of items among strategic agents with additive valuations, where items are either all indivisible or all divisible. When items are \emph{goods}, numerous positive and negative results are known regarding the fairness and efficiency guarantees achievable by \emph{truthful} mechanisms, whereas our understanding of truthful mechanisms for \emph{chores} remains considerably more limited. In this paper, we discover various connections between truthful good and chore allocations, greatly enhancing our understanding of the latter via tools from the former.
  For indivisible chores with two agents, we observe that a simple bundle-swapping operation transforms several properties for goods including truthfulness to the corresponding properties for chores, which enables us to characterize truthful mechanisms and derive the tight guarantees of various fairness notions achieved by truthful mechanisms. Moreover, for divisible chores, by generalizing the above transformation to an arbitrary number of agents, we characterize truthful mechanisms with two agents, show that every truthful mechanism with two agents admits an \emph{efficiency ratio} of $0$, and derive a large family of \emph{strictly truthful}, \emph{envy-free (EF)}, and \emph{proportional} mechanisms for an arbitrary number of agents. Finally, for indivisible chores with an arbitrary number of agents having \emph{bi-valued} cost functions, we give an \emph{ex-ante} truthful, ex-ante \emph{Pareto optimal}, ex-ante EF, and \emph{ex-post envy-free up to one item} mechanism, improving the best guarantees for bi-valued instances by prior works.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Prompting and Feedback-Guided Reasoning with LLMs for Data Interpretation</title>
<link>https://arxiv.org/abs/2505.01636</link>
<guid>https://arxiv.org/abs/2505.01636</guid>
<content:encoded><![CDATA[
arXiv:2505.01636v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and task generalization. However, their application to structured data analysis remains fragile due to inconsistencies in schema interpretation, misalignment between user intent and model output, and limited mechanisms for self-correction when failures occur. This paper introduces the STROT Framework (Structured Task Reasoning and Output Transformation), a method for structured prompting and feedback-driven transformation logic generation aimed at improving the reliability and semantic alignment of LLM-based analytical workflows. STROT begins with lightweight schema introspection and sample-based field classification, enabling dynamic context construction that captures both the structure and statistical profile of the input data. This contextual information is embedded in structured prompts that guide the model toward generating task-specific, interpretable outputs. To address common failure modes in complex queries, STROT incorporates a refinement mechanism in which the model iteratively revises its outputs based on execution feedback and validation signals. Unlike conventional approaches that rely on static prompts or single-shot inference, STROT treats the LLM as a reasoning agent embedded within a controlled analysis loop -- capable of adjusting its output trajectory through planning and correction. The result is a robust and reproducible framework for reasoning over structured data with LLMs, applicable to diverse data exploration and analysis tasks where interpretability, stability, and correctness are essential.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interaction Configurations and Prompt Guidance in Conversational AI for Question Answering in Human-AI Teams</title>
<link>https://arxiv.org/abs/2505.01648</link>
<guid>https://arxiv.org/abs/2505.01648</guid>
<content:encoded><![CDATA[
arXiv:2505.01648v1 Announce Type: new 
Abstract: Understanding the dynamics of human-AI interaction in question answering is crucial for enhancing collaborative efficiency. Extending from our initial formative study, which revealed challenges in human utilization of conversational AI support, we designed two configurations for prompt guidance: a Nudging approach, where the AI suggests potential responses for human agents, and a Highlight strategy, emphasizing crucial parts of reference documents to aid human responses. Through two controlled experiments, the first involving 31 participants and the second involving 106 participants, we compared these configurations against traditional human-only approaches, both with and without AI assistance. Our findings suggest that effective human-AI collaboration can enhance response quality, though merely combining human and AI efforts does not ensure improved outcomes. In particular, the Nudging configuration was shown to help improve the quality of the output when compared to AI alone. This paper delves into the development of these prompt guidance paradigms, offering insights for refining human-AI collaborations in conversational question-answering contexts and contributing to a broader understanding of human perceptions and expectations in AI partnerships.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-AI Governance (HAIG): A Trust-Utility Approach</title>
<link>https://arxiv.org/abs/2505.01651</link>
<guid>https://arxiv.org/abs/2505.01651</guid>
<content:encoded><![CDATA[
arXiv:2505.01651v1 Announce Type: new 
Abstract: This paper introduces the HAIG framework for analysing trust dynamics across evolving human-AI relationships. Current categorical frameworks (e.g., "human-in-the-loop" models) inadequately capture how AI systems evolve from tools to partners, particularly as foundation models demonstrate emergent capabilities and multi-agent systems exhibit autonomous goal-setting behaviours. As systems advance, agency redistributes in complex patterns that are better represented as positions along continua rather than discrete categories, though progression may include both gradual shifts and significant step changes. The HAIG framework operates across three levels: dimensions (Decision Authority Distribution, Process Autonomy, and Accountability Configuration), continua (gradual shifts along each dimension), and thresholds (critical points requiring governance adaptation). Unlike risk-based or principle-based approaches, HAIG adopts a trust-utility orientation, focusing on maintaining appropriate trust relationships that maximise utility while ensuring sufficient safeguards. Our analysis reveals how technical advances in self-supervision, reasoning authority, and distributed decision-making drive non-uniform trust evolution across both contextual variation and technological advancement. Case studies in healthcare and European regulation demonstrate how HAIG complements existing frameworks while offering a foundation for alternative approaches that anticipate governance challenges before they emerge.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency</title>
<link>https://arxiv.org/abs/2505.01658</link>
<guid>https://arxiv.org/abs/2505.01658</guid>
<content:encoded><![CDATA[
arXiv:2505.01658v1 Announce Type: new 
Abstract: Large language models (LLMs) are widely applied in chatbots, code generators, and search engines. Workloads such as chain-of-thought, complex reasoning, and agent services significantly increase the inference cost by invoking the model repeatedly. Optimization methods such as parallelism, compression, and caching have been adopted to reduce costs, but the diverse service requirements make it hard to select the right method. Recently, specialized LLM inference engines have emerged as a key component for integrating the optimization methods into service-oriented infrastructures. However, a systematic study on inference engines is still lacking. This paper provides a comprehensive evaluation of 25 open-source and commercial inference engines. We examine each inference engine in terms of ease-of-use, ease-of-deployment, general-purpose support, scalability, and suitability for throughput- and latency-aware computation. Furthermore, we explore the design goals of each inference engine by investigating the optimization techniques it supports. In addition, we assess the ecosystem maturity of open source inference engines and handle the performance and cost policy of commercial solutions. We outline future research directions that include support for complex LLM-based services, support of various hardware, and enhanced security, offering practical guidance to researchers and developers in selecting and designing optimized LLM inference engines. We also provide a public repository to continually track developments in this fast-evolving field: https://github.com/sihyeong/Awesome-LLM-Inference-Engine
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoBridge: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation</title>
<link>https://arxiv.org/abs/2505.01709</link>
<guid>https://arxiv.org/abs/2505.01709</guid>
<content:encoded><![CDATA[
arXiv:2505.01709v1 Announce Type: new 
Abstract: Operating robots in open-ended scenarios with diverse tasks is a crucial research and application direction in robotics. While recent progress in natural language processing and large multimodal models has enhanced robots' ability to understand complex instructions, robot manipulation still faces the procedural skill dilemma and the declarative skill dilemma in open environments. Existing methods often compromise cognitive and executive capabilities. To address these challenges, in this paper, we propose RoBridge, a hierarchical intelligent architecture for general robotic manipulation. It consists of a high-level cognitive planner (HCP) based on a large-scale pre-trained vision-language model (VLM), an invariant operable representation (IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA). RoBridge maintains the declarative skill of VLM and unleashes the procedural skill of reinforcement learning, effectively bridging the gap between cognition and execution. RoBridge demonstrates significant performance improvements over existing baselines, achieving a 75% success rate on new tasks and an 83% average success rate in sim-to-real generalization using only five real-world data samples per task. This work represents a significant step towards integrating cognitive reasoning with physical execution in robotic systems, offering a new paradigm for general robotic manipulation.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy Preserving Machine Learning Model Personalization through Federated Personalized Learning</title>
<link>https://arxiv.org/abs/2505.01788</link>
<guid>https://arxiv.org/abs/2505.01788</guid>
<content:encoded><![CDATA[
arXiv:2505.01788v1 Announce Type: new 
Abstract: The widespread adoption of Artificial Intelligence (AI) has been driven by significant advances in intelligent system research. However, this progress has raised concerns about data privacy, leading to a growing awareness of the need for privacy-preserving AI. In response, there has been a seismic shift in interest towards the leading paradigm for training Machine Learning (ML) models on decentralized data silos while maintaining data privacy, Federated Learning (FL). This research paper presents a comprehensive performance analysis of a cutting-edge approach to personalize ML model while preserving privacy achieved through Privacy Preserving Machine Learning with the innovative framework of Federated Personalized Learning (PPMLFPL). Regarding the increasing concerns about data privacy, this study evaluates the effectiveness of PPMLFPL addressing the critical balance between personalized model refinement and maintaining the confidentiality of individual user data. According to our analysis, Adaptive Personalized Cross-Silo Federated Learning with Differential Privacy (APPLE+DP) offering efficient execution whereas overall, the use of the Adaptive Personalized Cross-Silo Federated Learning with Homomorphic Encryption (APPLE+HE) algorithm for privacy-preserving machine learning tasks in federated personalized learning settings is strongly suggested. The results offer valuable insights creating it a promising scope for future advancements in the field of privacy-conscious data-driven technologies.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pathfinders in the Sky: Formal Decision-Making Models for Collaborative Air Traffic Control in Convective Weather</title>
<link>https://arxiv.org/abs/2505.01804</link>
<guid>https://arxiv.org/abs/2505.01804</guid>
<content:encoded><![CDATA[
arXiv:2505.01804v1 Announce Type: new 
Abstract: Air traffic can be significantly disrupted by weather. Pathfinder operations involve assigning a designated aircraft to assess whether airspace that was previously impacted by weather can be safely traversed through. Despite relatively routine use in air traffic control, there is little research on the underlying multi-agent decision-making problem. We seek to address this gap herein by formulating decision models to capture the operational dynamics and implications of pathfinders. Specifically, we construct a Markov chain to represent the stochastic transitions between key operational states (e.g., pathfinder selection). We then analyze its steady-state behavior to understand long-term system dynamics. We also propose models to characterize flight-specific acceptance behaviors (based on utility trade-offs) and pathfinder selection strategies (based on sequential offer allocations). We then conduct a worst-case scenario analysis that highlights risks from collective rejection and explores how selfless behavior and uncertainty affect system resilience. Empirical analysis of data from the US Federal Aviation Administration demonstrates the real-world significance of pathfinder operations and informs future model calibration.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Context Protocol-based Internet of Experts For Wireless Environment-aware LLM Agents</title>
<link>https://arxiv.org/abs/2505.01834</link>
<guid>https://arxiv.org/abs/2505.01834</guid>
<content:encoded><![CDATA[
arXiv:2505.01834v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit strong general-purpose reasoning abilities but lack access to wireless environment information due to the absence of native sensory input and domain-specific priors. Previous attempts to apply LLMs in wireless systems either depend on retraining with network-specific data, which compromises language generalization, or rely on manually scripted interfaces, which hinder scalability. To overcome these limitations, we propose a Model Context Protocol (MCP)-based Internet of Experts (IoX) framework that equips LLMs with wireless environment-aware reasoning capabilities. The framework incorporates a set of lightweight expert models, each trained to solve a specific deterministic task in wireless communications, such as detecting a specific wireless attribute, e.g., line-of-sight propagation, Doppler effects, or fading conditions. Through MCP, the LLM can selectively query and interpret expert outputs at inference time, without modifying its own parameters. This architecture enables modular, extensible, and interpretable reasoning over wireless contexts. Evaluated across multiple mainstream LLMs, the proposed wireless environment-aware LLM agents achieve 40%-50% improvements in classification tasks over LLM-only baselines. More broadly, the MCP-based design offers a viable paradigm for future LLMs to inherit structured wireless network management capabilities.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReLI: A Language-Agnostic Approach to Human-Robot Interaction</title>
<link>https://arxiv.org/abs/2505.01862</link>
<guid>https://arxiv.org/abs/2505.01862</guid>
<content:encoded><![CDATA[
arXiv:2505.01862v1 Announce Type: new 
Abstract: Adapting autonomous agents to industrial, domestic, and other daily tasks is currently gaining momentum. However, in the global or cross-lingual application contexts, ensuring effective interaction with the environment and executing unrestricted human task-specified instructions in diverse languages remains an unsolved problem. To address this challenge, we propose ReLI, a language-agnostic framework designed to enable autonomous agents to converse naturally, semantically reason about the environment, and to perform downstream tasks, regardless of the task instruction's linguistic origin. First, we ground large-scale pre-trained foundation models and transform them into language-to-action models that can directly provide common-sense reasoning and high-level robot control through natural, free-flow human-robot conversational interactions. Further, we perform cross-lingual grounding of the models to ensure that ReLI generalises across the global languages. To demonstrate the ReLI's robustness, we conducted extensive simulated and real-world experiments on various short- and long-horizon tasks, including zero-shot and few-shot spatial navigation, scene information retrieval, and query-oriented tasks. We benchmarked the performance on 140 languages involving over 70K multi-turn conversations. On average, ReLI achieved over 90%$\pm$0.2 accuracy in cross-lingual instruction parsing and task execution success rates. These results demonstrate the ReLI's potential to enhance natural human-robot interaction in the real world while championing linguistic diversity. Demonstrations and resources will be publicly available at https://linusnep.github.io/ReLI/.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMOUFLAGE: Exploiting Misinformation Detection Systems Through LLM-driven Adversarial Claim Transformation</title>
<link>https://arxiv.org/abs/2505.01900</link>
<guid>https://arxiv.org/abs/2505.01900</guid>
<content:encoded><![CDATA[
arXiv:2505.01900v1 Announce Type: new 
Abstract: Automated evidence-based misinformation detection systems, which evaluate the veracity of short claims against evidence, lack comprehensive analysis of their adversarial vulnerabilities. Existing black-box text-based adversarial attacks are ill-suited for evidence-based misinformation detection systems, as these attacks primarily focus on token-level substitutions involving gradient or logit-based optimization strategies, which are incapable of fooling the multi-component nature of these detection systems. These systems incorporate both retrieval and claim-evidence comparison modules, which requires attacks to break the retrieval of evidence and/or the comparison module so that it draws incorrect inferences. We present CAMOUFLAGE, an iterative, LLM-driven approach that employs a two-agent system, a Prompt Optimization Agent and an Attacker Agent, to create adversarial claim rewritings that manipulate evidence retrieval and mislead claim-evidence comparison, effectively bypassing the system without altering the meaning of the claim. The Attacker Agent produces semantically equivalent rewrites that attempt to mislead detectors, while the Prompt Optimization Agent analyzes failed attack attempts and refines the prompt of the Attacker to guide subsequent rewrites. This enables larger structural and stylistic transformations of the text rather than token-level substitutions, adapting the magnitude of changes based on previous outcomes. Unlike existing approaches, CAMOUFLAGE optimizes its attack solely based on binary model decisions to guide its rewriting process, eliminating the need for classifier logits or extensive querying. We evaluate CAMOUFLAGE on four systems, including two recent academic systems and two real-world APIs, with an average attack success rate of 46.92\% while preserving textual coherence and semantic equivalence to the original claims.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Act Natural! Extending Naturalistic Projection to Multimodal Behavior Scenarios</title>
<link>https://arxiv.org/abs/2505.01945</link>
<guid>https://arxiv.org/abs/2505.01945</guid>
<content:encoded><![CDATA[
arXiv:2505.01945v1 Announce Type: new 
Abstract: Autonomous agents operating in public spaces must consider how their behaviors might affect the humans around them, even when not directly interacting with them. To this end, it is often beneficial to be predictable and appear naturalistic. Existing methods for this purpose use human actor intent modeling or imitation learning techniques, but these approaches rarely capture all possible motivations for human behavior and/or require significant amounts of data. Our work extends a technique for modeling unimodal naturalistic behaviors with an explicit convex set representation, to account for multimodal behavior by using multiple convex sets. This more flexible representation provides a higher degree of fidelity in data-driven modeling of naturalistic behavior that arises in real-world scenarios in which human behavior is, in some sense, discrete, e.g. whether or not to yield at a roundabout. Equipped with this new set representation, we develop an optimization-based filter to project arbitrary trajectories into the set so that they appear naturalistic to humans in the scene, while also satisfying vehicle dynamics, actuator limits, etc. We demonstrate our methods on real-world human driving data from the inD (intersection) and rounD (roundabout) datasets.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Drug classification based on X-ray spectroscopy combined with machine learning</title>
<link>https://arxiv.org/abs/2505.01986</link>
<guid>https://arxiv.org/abs/2505.01986</guid>
<content:encoded><![CDATA[
arXiv:2505.01986v1 Announce Type: new 
Abstract: The proliferation of new types of drugs necessitates the urgent development of faster and more accurate detection methods. Traditional detection methods have high requirements for instruments and environments, making the operation complex. X-ray absorption spectroscopy, a non-destructive detection technique, offers advantages such as ease of operation, penetrative observation, and strong substance differentiation capabilities, making it well-suited for application in the field of drug detection and identification. In this study, we constructed a classification model using Convolutional Neural Networks (CNN), Support Vector Machines (SVM), and Particle Swarm Optimization (PSO) to classify and identify drugs based on their X-ray spectral profiles. In the experiments, we selected 14 chemical reagents with chemical formulas similar to drugs as samples. We utilized CNN to extract features from the spectral data of these 14 chemical reagents and used the extracted features to train an SVM model. We also utilized PSO to optimize two critical initial parameters of the SVM. The experimental results demonstrate that this model achieved higher classification accuracy compared to two other common methods, with a prediction accuracy of 99.14%. Additionally, the model exhibited fast execution speed, mitigating the drawback of a drastic increase in running time and efficiency reduction that may result from the direct fusion of PSO and SVM. Therefore, the combined approach of X-ray absorption spectroscopy with CNN, PSO, and SVM provides a rapid, highly accurate, and reliable classification and identification method for the field of drug detection, holding promising prospects for widespread application.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Mind to Machine: The Rise of Manus AI as a Fully Autonomous Digital Agent</title>
<link>https://arxiv.org/abs/2505.02024</link>
<guid>https://arxiv.org/abs/2505.02024</guid>
<content:encoded><![CDATA[
arXiv:2505.02024v1 Announce Type: new 
Abstract: Manus AI is a general-purpose AI agent introduced in early 2025, marking a significant advancement in autonomous artificial intelligence. Developed by the Chinese startup Monica.im, Manus is designed to bridge the gap between "mind" and "hand" - combining the reasoning and planning capabilities of large language models with the ability to execute complex, end-to-end tasks that produce tangible outcomes. This paper presents a comprehensive overview of Manus AI, exploring its core technical architecture, diverse applications across sectors such as healthcare, finance, manufacturing, robotics, and gaming, as well as its key strengths, current limitations, and future potential. Positioned as a preview of what lies ahead, Manus AI represents a shift toward intelligent agents that can translate high-level intentions into real-world actions, heralding a new era of human-AI collaboration.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An overview of artificial intelligence in computer-assisted language learning</title>
<link>https://arxiv.org/abs/2505.02032</link>
<guid>https://arxiv.org/abs/2505.02032</guid>
<content:encoded><![CDATA[
arXiv:2505.02032v1 Announce Type: new 
Abstract: Computer-assisted language learning -- CALL -- is an established research field. We review how artificial intelligence can be applied to support language learning and teaching. The need for intelligent agents that assist language learners and teachers is increasing: the human teacher's time is a scarce and costly resource, which does not scale with growing demand. Further factors contribute to the need for CALL: pandemics and increasing demand for distance learning, migration of large populations, the need for sustainable and affordable support for learning, etc. CALL systems are made up of many components that perform various functions, and AI is applied to many different aspects in CALL, corresponding to their own expansive research areas. Most of what we find in the research literature and in practical use are prototypes or partial implementations -- systems that perform some aspects of the overall desired functionality. Complete solutions -- most of them commercial -- are few, because they require massive resources. Recent advances in AI should result in improvements in CALL, yet there is a lack of surveys that focus on AI in the context of this research field. This paper aims to present a perspective on the AI methods that can be employed for language learning from a position of a developer of a CALL system. We also aim to connect work from different disciplines, to build bridges for interdisciplinary work.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Local Causal World Models with State Space Models and Attention</title>
<link>https://arxiv.org/abs/2505.02074</link>
<guid>https://arxiv.org/abs/2505.02074</guid>
<content:encoded><![CDATA[
arXiv:2505.02074v1 Announce Type: new 
Abstract: World modelling, i.e. building a representation of the rules that govern the world so as to predict its evolution, is an essential ability for any agent interacting with the physical world. Despite their impressive performance, many solutions fail to learn a causal representation of the environment they are trying to model, which would be necessary to gain a deep enough understanding of the world to perform complex tasks. With this work, we aim to broaden the research in the intersection of causality theory and neural world modelling by assessing the potential for causal discovery of the State Space Model (SSM) architecture, which has been shown to have several advantages over the widespread Transformer. We show empirically that, compared to an equivalent Transformer, a SSM can model the dynamics of a simple environment and learn a causal model at the same time with equivalent or better performance, thus paving the way for further experiments that lean into the strength of SSMs and further enhance them with causal awareness.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging LLM Agents and Digital Twins for Fault Handling in Process Plants</title>
<link>https://arxiv.org/abs/2505.02076</link>
<guid>https://arxiv.org/abs/2505.02076</guid>
<content:encoded><![CDATA[
arXiv:2505.02076v1 Announce Type: new 
Abstract: Advances in Automation and Artificial Intelligence continue to enhance the autonomy of process plants in handling various operational scenarios. However, certain tasks, such as fault handling, remain challenging, as they rely heavily on human expertise. This highlights the need for systematic, knowledge-based methods. To address this gap, we propose a methodological framework that integrates Large Language Model (LLM) agents with a Digital Twin environment. The LLM agents continuously interpret system states and initiate control actions, including responses to unexpected faults, with the goal of returning the system to normal operation. In this context, the Digital Twin acts both as a structured repository of plant-specific engineering knowledge for agent prompting and as a simulation platform for the systematic validation and verification of the generated corrective control actions. The evaluation using a mixing module of a process plant demonstrates that the proposed framework is capable not only of autonomously controlling the mixing module, but also of generating effective corrective actions to mitigate a pipe clogging with only a few reprompts.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open Challenges in Multi-Agent Security: Towards Secure Systems of Interacting AI Agents</title>
<link>https://arxiv.org/abs/2505.02077</link>
<guid>https://arxiv.org/abs/2505.02077</guid>
<content:encoded><![CDATA[
arXiv:2505.02077v1 Announce Type: new 
Abstract: Decentralized AI agents will soon interact across internet platforms, creating security challenges beyond traditional cybersecurity and AI safety frameworks. Free-form protocols are essential for AI's task generalization but enable new threats like secret collusion and coordinated swarm attacks. Network effects can rapidly spread privacy breaches, disinformation, jailbreaks, and data poisoning, while multi-agent dispersion and stealth optimization help adversaries evade oversightcreating novel persistent threats at a systemic level. Despite their critical importance, these security challenges remain understudied, with research fragmented across disparate fields including AI security, multi-agent learning, complex systems, cybersecurity, game theory, distributed systems, and technical AI governance. We introduce \textbf{multi-agent security}, a new field dedicated to securing networks of decentralized AI agents against threats that emerge or amplify through their interactionswhether direct or indirect via shared environmentswith each other, humans, and institutions, and characterize fundamental security-performance trade-offs. Our preliminary work (1) taxonomizes the threat landscape arising from interacting AI agents, (2) surveys security-performance tradeoffs in decentralized AI systems, and (3) proposes a unified research agenda addressing open challenges in designing secure agent systems and interaction environments. By identifying these gaps, we aim to guide research in this critical area to unlock the socioeconomic potential of large-scale agent deployment on the internet, foster public trust, and mitigate national security risks in critical infrastructure and defense contexts.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents</title>
<link>https://arxiv.org/abs/2505.02099</link>
<guid>https://arxiv.org/abs/2505.02099</guid>
<content:encoded><![CDATA[
arXiv:2505.02099v1 Announce Type: new 
Abstract: Recently, large language model based (LLM-based) agents have been widely applied across various fields. As a critical part, their memory capabilities have captured significant interest from both industrial and academic communities. Despite the proposal of many advanced memory models in recent research, however, there remains a lack of unified implementations under a general framework. To address this issue, we develop a unified and modular library for developing advanced memory models of LLM-based agents, called MemEngine. Based on our framework, we implement abundant memory models from recent research works. Additionally, our library facilitates convenient and extensible memory development, and offers user-friendly and pluggable memory usage. For benefiting our community, we have made our project publicly available at https://github.com/nuster1128/MemEngine.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DriveAgent: Multi-Agent Structured Reasoning with LLM and Multimodal Sensor Fusion for Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.02123</link>
<guid>https://arxiv.org/abs/2505.02123</guid>
<content:encoded><![CDATA[
arXiv:2505.02123v1 Announce Type: new 
Abstract: We introduce DriveAgent, a novel multi-agent autonomous driving framework that leverages large language model (LLM) reasoning combined with multimodal sensor fusion to enhance situational understanding and decision-making. DriveAgent uniquely integrates diverse sensor modalities-including camera, LiDAR, GPS, and IMU-with LLM-driven analytical processes structured across specialized agents. The framework operates through a modular agent-based pipeline comprising four principal modules: (i) a descriptive analysis agent identifying critical sensor data events based on filtered timestamps, (ii) dedicated vehicle-level analysis conducted by LiDAR and vision agents that collaboratively assess vehicle conditions and movements, (iii) environmental reasoning and causal analysis agents explaining contextual changes and their underlying mechanisms, and (iv) an urgency-aware decision-generation agent prioritizing insights and proposing timely maneuvers. This modular design empowers the LLM to effectively coordinate specialized perception and reasoning agents, delivering cohesive, interpretable insights into complex autonomous driving scenarios. Extensive experiments on challenging autonomous driving datasets demonstrate that DriveAgent is achieving superior performance on multiple metrics against baseline methods. These results validate the efficacy of the proposed LLM-driven multi-agent sensor fusion framework, underscoring its potential to substantially enhance the robustness and reliability of autonomous driving systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM Code Generation: A Systematic Evaluation of Multi-Agent Collaboration and Runtime Debugging for Improved Accuracy, Reliability, and Latency</title>
<link>https://arxiv.org/abs/2505.02133</link>
<guid>https://arxiv.org/abs/2505.02133</guid>
<content:encoded><![CDATA[
arXiv:2505.02133v1 Announce Type: new 
Abstract: The use of large language models (LLMs) for automated code generation has emerged as a significant focus within AI research. As these pretrained models continue to evolve, their ability to understand and generate complex code structures has opened new possibilities for automating intricate programming tasks for the sake of accurate code generation. Although contemporary foundational models demonstrate promoting results, researchers continue to explore optimal post-training strategies to enhance code quality. These include supervised fine-tuning, retrieval-augmented generation (RAG), debugging, and many others. In this paper, we combine two widely used approaches namely multi-agent collaboration and runtime execution information-based debugging, for improving code generation functionality, reliability, and practical applicability. We perform an empirical study in order to extend the evaluation of the individual strategies as well as the proposed composition of the activities of both strategies. Our study use 19 LLMs to examines the performance of individual and the proposed strategies, offering comprehensive insights into how different programming activities compositions and training paradigms influence code generation effectiveness. In particular, we implement a chained system that combines both strategies to assess their combined impact on functional accuracy, code reliability, and generation latency using two benchmark datasets commonly used for code generation. Our findings provide valuable insights for organizations seeking robust AI-driven coding solutions by guiding them in selecting models that can better adapt to complex post-training strategies, ultimately fostering the adoption of more effective and reliable code generation technologies.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VECSR: Virtually Embodied Common Sense Reasoning System</title>
<link>https://arxiv.org/abs/2505.02144</link>
<guid>https://arxiv.org/abs/2505.02144</guid>
<content:encoded><![CDATA[
arXiv:2505.02144v1 Announce Type: new 
Abstract: The development of autonomous agents has seen a revival of enthusiasm due to the emergence of LLMs, such as GPT-4o. Deploying these agents in environments where they coexist with humans (e.g., as domestic assistants) requires special attention to trustworthiness and explainability. However, the use of LLMs and other deep learning models still does not resolve these key issues. Deep learning systems may hallucinate, be unable to justify their decisions as black boxes, or perform badly on unseen scenarios. In this work, we propose the use of s(CASP), a goal-directed common sense reasoner based on Answer Set Programming, to break down the high-level tasks of an autonomous agent into mid-level instructions while justifying the selection of these instructions. To validate its use in real applications we present a framework that integrates the reasoner into the VirtualHome simulator and compares its accuracy with GPT-4o, running some of the real use cases available in the domestic environments of VirtualHome. Additionally, since experiments with VirtualHome have shown the need to reduce the response time (which increases as the agent's decision space grows), we have proposed and evaluated a series of optimizations based on program analysis that exploit the advantages of the top-down execution of s(CASP).
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents</title>
<link>https://arxiv.org/abs/2505.02156</link>
<guid>https://arxiv.org/abs/2505.02156</guid>
<content:encoded><![CDATA[
arXiv:2505.02156v1 Announce Type: new 
Abstract: Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current approaches. While existing methods either lack this kind of reasoning capability or enforce uniform long chain-of-thought reasoning across all scenarios, resulting in excessive token usage and inappropriate social simulation. In this paper, we propose $\textbf{A}$daptive $\textbf{M}$ode $\textbf{L}$earning ($\textbf{AML}$) that strategically selects from four thinking modes (intuitive reaction $\rightarrow$ deep contemplation) based on real-time context. Our framework's core innovation, the $\textbf{A}$daptive $\textbf{M}$ode $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{AMPO}$) algorithm, introduces three key advancements over existing methods: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence tasks confirm that AML achieves 15.6% higher task performance than state-of-the-art methods. Notably, our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These results demonstrate that context-sensitive thinking mode selection, as implemented in AMPO, enables more human-like adaptive reasoning than GRPO's fixed-depth approach
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Emergent Language Using Inter-Agent Transformers</title>
<link>https://arxiv.org/abs/2505.02215</link>
<guid>https://arxiv.org/abs/2505.02215</guid>
<content:encoded><![CDATA[
arXiv:2505.02215v1 Announce Type: new 
Abstract: This paper explores the emergence of language in multi-agent reinforcement learning (MARL) using transformers. Existing methods such as RIAL, DIAL, and CommNet enable agent communication but lack interpretability. We propose Differentiable Inter-Agent Transformers (DIAT), which leverage self-attention to learn symbolic, human-understandable communication protocols. Through experiments, DIAT demonstrates the ability to encode observations into interpretable vocabularies and meaningful embeddings, effectively solving cooperative tasks. These results highlight the potential of DIAT for interpretable communication in complex multi-agent environments.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coupled Distributional Random Expert Distillation for World Model Online Imitation Learning</title>
<link>https://arxiv.org/abs/2505.02228</link>
<guid>https://arxiv.org/abs/2505.02228</guid>
<content:encoded><![CDATA[
arXiv:2505.02228v1 Announce Type: new 
Abstract: Imitation Learning (IL) has achieved remarkable success across various domains, including robotics, autonomous driving, and healthcare, by enabling agents to learn complex behaviors from expert demonstrations. However, existing IL methods often face instability challenges, particularly when relying on adversarial reward or value formulations in world model frameworks. In this work, we propose a novel approach to online imitation learning that addresses these limitations through a reward model based on random network distillation (RND) for density estimation. Our reward model is built on the joint estimation of expert and behavioral distributions within the latent space of the world model. We evaluate our method across diverse benchmarks, including DMControl, Meta-World, and ManiSkill2, showcasing its ability to deliver stable performance and achieve expert-level results in both locomotion and manipulation tasks. Our approach demonstrates improved stability over adversarial methods while maintaining expert-level performance.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A survey of agent interoperability protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP)</title>
<link>https://arxiv.org/abs/2505.02279</link>
<guid>https://arxiv.org/abs/2505.02279</guid>
<content:encoded><![CDATA[
arXiv:2505.02279v1 Announce Type: new 
Abstract: Large language model (LLM)-powered autonomous agents demand robust, standardized protocols to integrate tools, share contextual data, and coordinate tasks across heterogeneous systems. Ad-hoc integrations are difficult to scale, secure, and generalize across domains. This survey examines four emerging agent communication protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP), each addressing interoperability in distinct deployment contexts. MCP provides a JSON-RPC client-server interface for secure tool invocation and typed data exchange. ACP introduces REST-native messaging via multi-part messages and asynchronous streaming to support multimodal agent responses. A2A enables peer-to-peer task outsourcing through capability-based Agent Cards, facilitating enterprise-scale workflows. ANP supports open-network agent discovery and secure collaboration using decentralized identifiers (DIDs) and JSON-LD graphs. The protocols are compared across multiple dimensions, including interaction modes, discovery mechanisms, communication patterns, and security models. Based on the comparative analysis, a phased adoption roadmap is proposed: beginning with MCP for tool access, followed by ACP for multimodal messaging, A2A for collaborative task execution, and extending to ANP for decentralized agent marketplaces. This work provides a comprehensive foundation for designing secure, interoperable, and scalable ecosystems of LLM-powered agents.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resolving Conflicting Constraints in Multi-Agent Reinforcement Learning with Layered Safety</title>
<link>https://arxiv.org/abs/2505.02293</link>
<guid>https://arxiv.org/abs/2505.02293</guid>
<content:encoded><![CDATA[
arXiv:2505.02293v1 Announce Type: new 
Abstract: Preventing collisions in multi-robot navigation is crucial for deployment. This requirement hinders the use of learning-based approaches, such as multi-agent reinforcement learning (MARL), on their own due to their lack of safety guarantees. Traditional control methods, such as reachability and control barrier functions, can provide rigorous safety guarantees when interactions are limited only to a small number of robots. However, conflicts between the constraints faced by different agents pose a challenge to safe multi-agent coordination.
  To overcome this challenge, we propose a method that integrates multiple layers of safety by combining MARL with safety filters. First, MARL is used to learn strategies that minimize multiple agent interactions, where multiple indicates more than two. Particularly, we focus on interactions likely to result in conflicting constraints within the engagement distance. Next, for agents that enter the engagement distance, we prioritize pairs requiring the most urgent corrective actions. Finally, a dedicated safety filter provides tactical corrective actions to resolve these conflicts. Crucially, the design decisions for all layers of this framework are grounded in reachability analysis and a control barrier-value function-based filtering mechanism.
  We validate our Layered Safe MARL framework in 1) hardware experiments using Crazyflie drones and 2) high-density advanced aerial mobility (AAM) operation scenarios, where agents navigate to designated waypoints while avoiding collisions. The results show that our method significantly reduces conflict while maintaining safety without sacrificing much efficiency (i.e., shorter travel time and distance) compared to baselines that do not incorporate layered safety. The project website is available at \href{https://dinamo-mit.github.io/Layered-Safe-MARL/}{[this https URL]}
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans</title>
<link>https://arxiv.org/abs/2505.02388</link>
<guid>https://arxiv.org/abs/2505.02388</guid>
<content:encoded><![CDATA[
arXiv:2505.02388v1 Announce Type: new 
Abstract: Embodied AI (EAI) research requires high-quality, diverse 3D scenes to effectively support skill acquisition, sim-to-real transfer, and generalization. Achieving these quality standards, however, necessitates the precise replication of real-world object diversity. Existing datasets demonstrate that this process heavily relies on artist-driven designs, which demand substantial human effort and present significant scalability challenges. To scalably produce realistic and interactive 3D scenes, we first present MetaScenes, a large-scale, simulatable 3D scene dataset constructed from real-world scans, which includes 15366 objects spanning 831 fine-grained categories. Then, we introduce Scan2Sim, a robust multi-modal alignment model, which enables the automated, high-quality replacement of assets, thereby eliminating the reliance on artist-driven designs for scaling 3D scenes. We further propose two benchmarks to evaluate MetaScenes: a detailed scene synthesis task focused on small item layouts for robotic manipulation and a domain transfer task in vision-and-language navigation (VLN) to validate cross-domain transfer. Results confirm MetaScene's potential to enhance EAI by supporting more generalizable agent learning and sim-to-real applications, introducing new possibilities for EAI research. Project website: https://meta-scenes.github.io/.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>El Agente: An Autonomous Agent for Quantum Chemistry</title>
<link>https://arxiv.org/abs/2505.02484</link>
<guid>https://arxiv.org/abs/2505.02484</guid>
<content:encoded><![CDATA[
arXiv:2505.02484v1 Announce Type: new 
Abstract: Computational chemistry tools are widely used to study the behaviour of chemical phenomena. Yet, the complexity of these tools can make them inaccessible to non-specialists and challenging even for experts. In this work, we introduce El Agente Q, an LLM-based multi-agent system that dynamically generates and executes quantum chemistry workflows from natural language user prompts. The system is built on a novel cognitive architecture featuring a hierarchical memory framework that enables flexible task decomposition, adaptive tool selection, post-analysis, and autonomous file handling and submission. El Agente Q is benchmarked on six university-level course exercises and two case studies, demonstrating robust problem-solving performance (averaging >87% task success) and adaptive error handling through in situ debugging. It also supports longer-term, multi-step task execution for more complex workflows, while maintaining transparency through detailed action trace logs. Together, these capabilities lay the foundation for increasingly autonomous and accessible quantum chemistry.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Contrastive Feedback for Effective User Simulations</title>
<link>https://arxiv.org/abs/2505.02560</link>
<guid>https://arxiv.org/abs/2505.02560</guid>
<content:encoded><![CDATA[
arXiv:2505.02560v1 Announce Type: new 
Abstract: The use of Large Language Models (LLMs) for simulating user behavior in the domain of Interactive Information Retrieval has recently gained significant popularity. However, their application and capabilities remain highly debated and understudied. This study explores whether the underlying principles of contrastive training techniques, which have been effective for fine-tuning LLMs, can also be applied beneficially in the area of prompt engineering for user simulations.
  Previous research has shown that LLMs possess comprehensive world knowledge, which can be leveraged to provide accurate estimates of relevant documents. This study attempts to simulate a knowledge state by enhancing the model with additional implicit contextual information gained during the simulation. This approach enables the model to refine the scope of desired documents further. The primary objective of this study is to analyze how different modalities of contextual information influence the effectiveness of user simulations.
  Various user configurations were tested, where models are provided with summaries of already judged relevant, irrelevant, or both types of documents in a contrastive manner. The focus of this study is the assessment of the impact of the prompting techniques on the simulated user agent performance. We hereby lay the foundations for leveraging LLMs as part of more realistic simulated users.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Neurodivergence as a Contingent Solution to the AI Alignment Problem</title>
<link>https://arxiv.org/abs/2505.02581</link>
<guid>https://arxiv.org/abs/2505.02581</guid>
<content:encoded><![CDATA[
arXiv:2505.02581v1 Announce Type: new 
Abstract: The AI alignment problem, which focusses on ensuring that artificial intelligence (AI), including AGI and ASI, systems act according to human values, presents profound challenges. With the progression from narrow AI to Artificial General Intelligence (AGI) and Superintelligence, fears about control and existential risk have escalated. This paper demonstrates that achieving complete alignment is inherently unattainable due to mathematical principles rooted in the foundations of predicate logic and computability, in particular Turing's computational universality, G\"odel's incompleteness and Chaitin's randomness. Instead, we argue that embracing AI misalignment or agent's `neurodivergence' as a contingent strategy, defined as fostering a dynamic ecosystem of competing, partially aligned agents, is a possible only viable path to mitigate risks. Through mathematical proofs and an experimental design, we explore how misalignment may serve and should be promoted as a counterbalancing mechanism to team up with whichever agents are most aligned AI to human values, ensuring that no single system dominates destructively. The main premise of our contribution is that misalignment is inevitable because full AI-human alignment is a mathematical impossibility from Turing-complete systems which we also prove in this paper, a feature then inherited to AGI and ASI systems. We introduce and test `change-of-opinion' attacks based on this kind of perturbation and intervention analysis to study how agents may neutralise friendly or unfriendly AIs through cooperation, competition or malice.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aerodynamic and structural airfoil shape optimisation via Transfer Learning-enhanced Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.02634</link>
<guid>https://arxiv.org/abs/2505.02634</guid>
<content:encoded><![CDATA[
arXiv:2505.02634v1 Announce Type: new 
Abstract: The main objective of this paper is to introduce a transfer learning-enhanced, multi-objective, deep reinforcement learning (DRL) methodology that is able to optimise the geometry of any airfoil based on concomitant aerodynamic and structural criteria. To showcase the method, we aim to maximise the lift-to-drag ratio $C_L/C_D$ while preserving the structural integrity of the airfoil -- as modelled by its maximum thickness -- and train the DRL agent using a list of different transfer learning (TL) strategies. The performance of the DRL agent is compared with Particle Swarm Optimisation (PSO), a traditional gradient-free optimisation method. Results indicate that DRL agents are able to perform multi-objective shape optimisation, that the DRL approach outperforms PSO in terms of computational efficiency and shape optimisation performance, and that the TL-enhanced DRL agent achieves performance comparable to the DRL one, while further saving substantial computational resources.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCCD: Multi-Agent Collaboration-based Compositional Diffusion for Complex Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2505.02648</link>
<guid>https://arxiv.org/abs/2505.02648</guid>
<content:encoded><![CDATA[
arXiv:2505.02648v1 Announce Type: new 
Abstract: Diffusion models have shown excellent performance in text-to-image generation. Nevertheless, existing methods often suffer from performance bottlenecks when handling complex prompts that involve multiple objects, characteristics, and relations. Therefore, we propose a Multi-agent Collaboration-based Compositional Diffusion (MCCD) for text-to-image generation for complex scenes. Specifically, we design a multi-agent collaboration-based scene parsing module that generates an agent system comprising multiple agents with distinct tasks, utilizing MLLMs to extract various scene elements effectively. In addition, Hierarchical Compositional diffusion utilizes a Gaussian mask and filtering to refine bounding box regions and enhance objects through region enhancement, resulting in the accurate and high-fidelity generation of complex scenes. Comprehensive experiments demonstrate that our MCCD significantly improves the performance of the baseline models in a training-free manner, providing a substantial advantage in complex scene generation.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law</title>
<link>https://arxiv.org/abs/2505.02665</link>
<guid>https://arxiv.org/abs/2505.02665</guid>
<content:encoded><![CDATA[
arXiv:2505.02665v1 Announce Type: new 
Abstract: This survey explores recent advancements in reasoning large language models (LLMs) designed to mimic "slow thinking" - a reasoning process inspired by human cognition, as described in Kahneman's Thinking, Fast and Slow. These models, like OpenAI's o1, focus on scaling computational resources dynamically during complex tasks, such as math reasoning, visual reasoning, medical diagnosis, and multi-agent debates. We present the development of reasoning LLMs and list their key technologies. By synthesizing over 100 studies, it charts a path toward LLMs that combine human-like deep thinking with scalable efficiency for reasoning. The review breaks down methods into three categories: (1) test-time scaling dynamically adjusts computation based on task complexity via search and sampling, dynamic verification; (2) reinforced learning refines decision-making through iterative improvement leveraging policy networks, reward models, and self-evolution strategies; and (3) slow-thinking frameworks (e.g., long CoT, hierarchical processes) that structure problem-solving with manageable steps. The survey highlights the challenges and further directions of this domain. Understanding and advancing the reasoning abilities of LLMs is crucial for unlocking their full potential in real-world applications, from scientific discovery to decision support systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring LLM-Powered Role and Action-Switching Pedagogical Agents for History Education in Virtual Reality</title>
<link>https://arxiv.org/abs/2505.02699</link>
<guid>https://arxiv.org/abs/2505.02699</guid>
<content:encoded><![CDATA[
arXiv:2505.02699v1 Announce Type: new 
Abstract: Multi-role pedagogical agents can create engaging and immersive learning experiences, helping learners better understand knowledge in history learning. However, existing pedagogical agents often struggle with multi-role interactions due to complex controls, limited feedback forms, and difficulty dynamically adapting to user inputs. In this study, we developed a VR prototype with LLM-powered adaptive role-switching and action-switching pedagogical agents to help users learn about the history of the Pavilion of Prince Teng. A 2 x 2 between-subjects study was conducted with 84 participants to assess how adaptive role-switching and action-switching affect participants' learning outcomes and experiences. The results suggest that adaptive role-switching enhances participants' perception of the pedagogical agent's trustworthiness and expertise but may lead to inconsistent learning experiences. Adaptive action-switching increases participants' perceived social presence, expertise, and humanness. The study did not uncover any effects of role-switching and action-switching on usability, learning motivation, and cognitive load. Based on the findings, we proposed five design implications for incorporating adaptive role-switching and action-switching into future VR history education tools.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play</title>
<link>https://arxiv.org/abs/2505.02707</link>
<guid>https://arxiv.org/abs/2505.02707</guid>
<content:encoded><![CDATA[
arXiv:2505.02707v1 Announce Type: new 
Abstract: A voice AI agent that blends seamlessly into daily life would interact with humans in an autonomous, real-time, and emotionally expressive manner. Rather than merely reacting to commands, it would continuously listen, reason, and respond proactively, fostering fluid, dynamic, and emotionally resonant interactions. We introduce Voila, a family of large voice-language foundation models that make a step towards this vision. Voila moves beyond traditional pipeline systems by adopting a new end-to-end architecture that enables full-duplex, low-latency conversations while preserving rich vocal nuances such as tone, rhythm, and emotion. It achieves a response latency of just 195 milliseconds, surpassing the average human response time. Its hierarchical multi-scale Transformer integrates the reasoning capabilities of large language models (LLMs) with powerful acoustic modeling, enabling natural, persona-aware voice generation -- where users can simply write text instructions to define the speaker's identity, tone, and other characteristics. Moreover, Voila supports over one million pre-built voices and efficient customization of new ones from brief audio samples as short as 10 seconds. Beyond spoken dialogue, Voila is designed as a unified model for a wide range of voice-based applications, including automatic speech recognition (ASR), Text-to-Speech (TTS), and, with minimal adaptation, multilingual speech translation. Voila is fully open-sourced to support open research and accelerate progress toward next-generation human-machine interactions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Technical Report: Evaluating Goal Drift in Language Model Agents</title>
<link>https://arxiv.org/abs/2505.02709</link>
<guid>https://arxiv.org/abs/2505.02709</guid>
<content:encoded><![CDATA[
arXiv:2505.02709v1 Announce Type: new 
Abstract: As language models (LMs) are increasingly deployed as autonomous agents, their robust adherence to human-assigned objectives becomes crucial for safe operation. When these agents operate independently for extended periods without human oversight, even initially well-specified goals may gradually shift. Detecting and measuring goal drift - an agent's tendency to deviate from its original objective over time - presents significant challenges, as goals can shift gradually, causing only subtle behavioral changes. This paper proposes a novel approach to analyzing goal drift in LM agents. In our experiments, agents are first explicitly given a goal through their system prompt, then exposed to competing objectives through environmental pressures. We demonstrate that while the best-performing agent (a scaffolded version of Claude 3.5 Sonnet) maintains nearly perfect goal adherence for more than 100,000 tokens in our most difficult evaluation setting, all evaluated models exhibit some degree of goal drift. We also find that goal drift correlates with models' increasing susceptibility to pattern-matching behaviors as the context length grows.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Network-Based Reinforcement Learning for Controlling Biological Networks: The GATTACA Framework</title>
<link>https://arxiv.org/abs/2505.02712</link>
<guid>https://arxiv.org/abs/2505.02712</guid>
<content:encoded><![CDATA[
arXiv:2505.02712v1 Announce Type: new 
Abstract: Cellular reprogramming, the artificial transformation of one cell type into another, has been attracting increasing research attention due to its therapeutic potential for complex diseases. However, discovering reprogramming strategies through classical wet-lab experiments is hindered by lengthy time commitments and high costs. In this study, we explore the use of deep reinforcement learning (DRL) to control Boolean network models of complex biological systems, such as gene regulatory networks and signalling pathway networks. We formulate a novel control problem for Boolean network models under the asynchronous update mode in the context of cellular reprogramming. To facilitate scalability, we consider our previously introduced concept of a pseudo-attractor and we improve our procedure for effective identification of pseudo-attractor states. Finally, we devise a computational framework to solve the control problem. To leverage the structure of biological systems, we incorporate graph neural networks with graph convolutions into the artificial neural network approximator for the action-value function learned by the DRL agent. Experiments on a number of large real-world biological networks from literature demonstrate the scalability and effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Brief Announcement: Minimizing Energy Solves Relative Majority with a Cubic Number of States in Population Protocols</title>
<link>https://arxiv.org/abs/2505.02785</link>
<guid>https://arxiv.org/abs/2505.02785</guid>
<content:encoded><![CDATA[
arXiv:2505.02785v1 Announce Type: new 
Abstract: This paper revisits a fundamental distributed computing problem in the population protocol model.
  Provided $n$ agents each starting with an input color in $[k]$, the relative majority problem asks to find the predominant color.
  In the population protocol model, at each time step, a scheduler selects two agents that first learn each other's states and then update their states based on what they learned.
  We present the \textsc{Circles} protocol that solves the relative majority problem with $k^3$ states. It is always-correct under weakly fair scheduling.
  Not only does it improve upon the best known upper bound of $O(k^7)$, but it also shows a strikingly simpler design inspired by energy minimization in chemical settings.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recolorable Graph Exploration by an Oblivious Agent with Fewer Colors</title>
<link>https://arxiv.org/abs/2505.02789</link>
<guid>https://arxiv.org/abs/2505.02789</guid>
<content:encoded><![CDATA[
arXiv:2505.02789v1 Announce Type: new 
Abstract: Recently, B\"ockenhauer, Frei, Unger, and Wehner (SIROCCO 2023) introduced a novel variant of the graph exploration problem in which a single memoryless agent must visit all nodes of an unknown, undirected, and connected graph before returning to its starting node. Unlike the standard model for mobile agents, edges are not labeled with port numbers. Instead, the agent can color its current node and observe the color of each neighboring node. To move, it specifies a target color and then moves to an adversarially chosen neighbor of that color. B\"ockenhauer~et al.~analyzed the minimum number of colors required for successful exploration and proposed an elegant algorithm that enables the agent to explore an arbitrary graph using only eight colors. In this paper, we present a novel graph exploration algorithm that requires only six colors. Furthermore, we prove that five colors are sufficient if we consider only a restricted class of graphs, which we call the $\varphi$-free graphs, a class that includes every graph with maximum degree at most three and every cactus.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating HomeAssistant Automations Using an LLM-based Chatbot</title>
<link>https://arxiv.org/abs/2505.02802</link>
<guid>https://arxiv.org/abs/2505.02802</guid>
<content:encoded><![CDATA[
arXiv:2505.02802v1 Announce Type: new 
Abstract: To combat climate change, individuals are encouraged to adopt sustainable habits, in particular, with their household, optimizing their electrical consumption. Conversational agents, such as Smart Home Assistants, hold promise as effective tools for promoting sustainable practices within households. Our research investigated the application of Large Language Models (LLM) in enhancing smart home automation and promoting sustainable household practices, specifically using the HomeAssistant framework. In particular, it highlights the potential of GPT models in generating accurate automation routines. While the LLMs showed proficiency in understanding complex commands and creating valid JSON outputs, challenges such as syntax errors and message malformations were noted, indicating areas for further improvement. Still, despite minimal quantitative differences between "green" and "no green" prompts, qualitative feedback highlighted a positive shift towards sustainability in the routines generated with environmentally focused prompts. Then, an empirical evaluation (N=56) demonstrated that the system was well-received and found engaging by users compared to its traditional rule-based counterpart. Our findings highlight the role of LLMs in advancing smart home technologies and suggest further research to refine these models for broader, real-world applications to support sustainable living.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoLibra: Agent Metric Induction from Open-Ended Feedback</title>
<link>https://arxiv.org/abs/2505.02820</link>
<guid>https://arxiv.org/abs/2505.02820</guid>
<content:encoded><![CDATA[
arXiv:2505.02820v1 Announce Type: new 
Abstract: Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. We propose AutoLibra, a framework for agent evaluation, that transforms open-ended human feedback, e.g., "If you find that the button is disabled, don't click it again", or "This agent has too much autonomy to decide what to do on its own", into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agent's behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLM-as-a-Judge as evaluators. We further propose two meta-metrics to evaluate the alignment of a set of (induced) metrics with open feedback: "coverage" and "redundancy". Through optimizing these meta-metrics, we experimentally demonstrate AutoLibra's ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. We also present two applications of AutoLibra in agent improvement: First, we show that AutoLibra-induced metrics serve as better prompt-engineering targets than the task success rate on a wide range of text game tasks, improving agent performance over baseline by a mean of 20%. Second, we show that AutoLibra can iteratively select high-quality fine-tuning data for web navigation agents. Our results suggest that AutoLibra is a powerful task-agnostic tool for evaluating and improving language agents.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scenethesis: A Language and Vision Agentic Framework for 3D Scene Generation</title>
<link>https://arxiv.org/abs/2505.02836</link>
<guid>https://arxiv.org/abs/2505.02836</guid>
<content:encoded><![CDATA[
arXiv:2505.02836v1 Announce Type: new 
Abstract: Synthesizing interactive 3D scenes from text is essential for gaming, virtual reality, and embodied AI. However, existing methods face several challenges. Learning-based approaches depend on small-scale indoor datasets, limiting the scene diversity and layout complexity. While large language models (LLMs) can leverage diverse text-domain knowledge, they struggle with spatial realism, often producing unnatural object placements that fail to respect common sense. Our key insight is that vision perception can bridge this gap by providing realistic spatial guidance that LLMs lack. To this end, we introduce Scenethesis, a training-free agentic framework that integrates LLM-based scene planning with vision-guided layout refinement. Given a text prompt, Scenethesis first employs an LLM to draft a coarse layout. A vision module then refines it by generating an image guidance and extracting scene structure to capture inter-object relations. Next, an optimization module iteratively enforces accurate pose alignment and physical plausibility, preventing artifacts like object penetration and instability. Finally, a judge module verifies spatial coherence. Comprehensive experiments show that Scenethesis generates diverse, realistic, and physically plausible 3D interactive scenes, making it valuable for virtual content creation, simulation environments, and embodied AI research.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenAINet: Enabling Wireless Collective Intelligence via Knowledge Transfer and Reasoning</title>
<link>https://arxiv.org/abs/2402.16631</link>
<guid>https://arxiv.org/abs/2402.16631</guid>
<content:encoded><![CDATA[
arXiv:2402.16631v3 Announce Type: replace 
Abstract: Generative Artificial Intelligence (GenAI) and communication networks are expected to have groundbreaking synergies for 6G. Connecting GenAI agents via a wireless network can potentially unleash the power of Collective Intelligence (CI) and pave the way for Artificial General Intelligence (AGI). However, current wireless networks are designed as a "data pipe" and are not suited to accommodate and leverage the power of GenAI. In this paper, we propose the GenAINet framework in which distributed GenAI agents communicate knowledge (facts, experiences, and methods) to accomplish arbitrary tasks. We first propose an architecture for a single GenAI agent and then provide a network architecture integrating GenAI capabilities to manage both network protocols and applications. Building on this, we investigate effective communication and reasoning problems by proposing a semantic-native GenAINet. Specifically, GenAI agents extract semantics from heterogeneous raw data, build and maintain a knowledge model representing the semantic relationships among pieces of knowledge, which is retrieved by GenAI models for planning and reasoning. Under this paradigm, different levels of collaboration can be achieved flexibly depending on the complexity of targeted tasks. Furthermore, we conduct two case studies in which, through wireless device queries, we demonstrate that extracting, compressing and transferring common knowledge can improve query accuracy while reducing communication costs; and in the wireless power control problem, we show that distributed agents can complete general tasks independently through collaborative reasoning without predefined communication protocols. Finally, we discuss challenges and future research directions in applying Large Language Models (LLMs) in 6G networks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strengthening Infrastructure Resilience to Hurricanes by Modeling Transportation and Electric Power Network Interdependencies</title>
<link>https://arxiv.org/abs/2404.12978</link>
<guid>https://arxiv.org/abs/2404.12978</guid>
<content:encoded><![CDATA[
arXiv:2404.12978v2 Announce Type: replace 
Abstract: This study presents an agent-based model (ABM) developed to simulate the resilience of a community to hurricane-induced infrastructure disruptions, focusing on the interdependencies between electric power and transportation networks. In this ABM approach, agents represent the components of a system, where interactions within a system shape intra-dependency of a system and interactions among systems shape interdependencies. To study household resilience subject to a hurricane, a library of agents has been created including electric power network, transportation network, wind/flooding hazards, and household agents. The ABM is applied over the household and infrastructure data from a community (Zip code 33147) in Miami-Dade County, Florida. Interdependencies between the two networks are modeled in two ways, (i) representing the role of transportation in fuel delivery to power plants and restoration teams' access, (ii) impact of power outage on transportation network components. Restoring traffic signals quickly is crucial as their outage can slow down traffic and increase the chance of crashes. We simulate three restoration strategies: component based, distance based, and traffic lights based restoration. The model is validated against Hurricane Irma data, showing consistent behavior with varying hazard intensities. Scenario analyses explore the impact of restoration strategies, road accessibility, and wind speed intensities on power restoration. Results demonstrate that a traffic lights based restoration strategy efficiently prioritizes signal recovery without delaying household power restoration time. Restoration of power services will be faster if restoration teams do not need to wait due to inaccessible roads and fuel transportation to power plants is not delayed.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kubrick: Multimodal Agent Collaborations for Synthetic Video Generation</title>
<link>https://arxiv.org/abs/2408.10453</link>
<guid>https://arxiv.org/abs/2408.10453</guid>
<content:encoded><![CDATA[
arXiv:2408.10453v2 Announce Type: replace 
Abstract: Text-to-video generation has been dominated by diffusion-based or autoregressive models. These novel models provide plausible versatility, but are criticized for improper physical motion, shading and illumination, camera motion, and temporal consistency. The film industry relies on manually-edited Computer-Generated Imagery (CGI) using 3D modeling software. Human-directed 3D synthetic videos address these shortcomings, but require tight collaboration between movie makers and 3D rendering experts. We introduce an automatic synthetic video generation pipeline based on Vision Large Language Model (VLM) agent collaborations. Given a language description of a video, multiple VLM agents direct various processes of the generation pipeline. They cooperate to create Blender scripts which render a video following the given description. Augmented with Blender-based movie making knowledge, the Director agent decomposes the text-based video description into sub-processes. For each sub-process, the Programmer agent produces Python-based Blender scripts based on function composing and API calling. The Reviewer agent, with knowledge of video reviewing, character motion coordinates, and intermediate screenshots, provides feedback to the Programmer agent. The Programmer agent iteratively improves scripts to yield the best video outcome. Our generated videos show better quality than commercial video generation models in five metrics on video quality and instruction-following performance. Our framework outperforms other approaches in a user study on quality, consistency, and rationality.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction</title>
<link>https://arxiv.org/abs/2412.04454</link>
<guid>https://arxiv.org/abs/2412.04454</guid>
<content:encoded><![CDATA[
arXiv:2412.04454v2 Announce Type: replace 
Abstract: Automating GUI tasks remains challenging due to reliance on textual representations, platform-specific action spaces, and limited reasoning capabilities. We introduce Aguvis, a unified vision-based framework for autonomous GUI agents that directly operates on screen images, standardizes cross-platform interactions and incorporates structured reasoning via inner monologue. To enable this, we construct Aguvis Data Collection, a large-scale dataset with multimodal grounding and reasoning annotations, and develop a two-stage training pipeline that separates GUI grounding from planning and reasoning. Experiments show that Aguvis achieves state-of-the-art performance across offline and real-world online benchmarks, marking the first fully autonomous vision-based GUI agent that operates without closed-source models. We open-source all datasets, models, and training recipes at https://aguvis-project.github.io to advance future research.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrushEdit: All-In-One Image Inpainting and Editing</title>
<link>https://arxiv.org/abs/2412.10316</link>
<guid>https://arxiv.org/abs/2412.10316</guid>
<content:encoded><![CDATA[
arXiv:2412.10316v3 Announce Type: replace 
Abstract: Image editing has advanced significantly with the development of diffusion models using both inversion-based and instruction-based methods. However, current inversion-based approaches struggle with big modifications (e.g., adding or removing objects) due to the structured nature of inversion noise, which hinders substantial changes. Meanwhile, instruction-based methods often constrain users to black-box operations, limiting direct interaction for specifying editing regions and intensity. To address these limitations, we propose BrushEdit, a novel inpainting-based instruction-guided image editing paradigm, which leverages multimodal large language models (MLLMs) and image inpainting models to enable autonomous, user-friendly, and interactive free-form instruction editing. Specifically, we devise a system enabling free-form instruction editing by integrating MLLMs and a dual-branch image inpainting model in an agent-cooperative framework to perform editing category classification, main object identification, mask acquisition, and editing area inpainting. Extensive experiments show that our framework effectively combines MLLMs and inpainting models, achieving superior performance across seven metrics including mask region preservation and editing effect coherence.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defense Strategies for Autonomous Multi-agent Systems: Ensuring Safety and Resilience Under Exponentially Unbounded FDI Attacks</title>
<link>https://arxiv.org/abs/2501.00973</link>
<guid>https://arxiv.org/abs/2501.00973</guid>
<content:encoded><![CDATA[
arXiv:2501.00973v2 Announce Type: replace 
Abstract: False data injection attacks pose a significant threat to autonomous multi-agent systems (MASs). Existing attack-resilient control strategies generally have strict assumptions on the attack signals and overlook safety constraints, such as collision avoidance. In practical applications, leader agents equipped with advanced sensors or weaponry span a safe region to guide heterogeneous follower agents, ensuring coordinated operations while addressing collision avoidance to prevent financial losses and mission failures. This letter addresses these gaps by introducing and solving the safety-aware and attack-resilient (SAAR) control problem under exponentially unbounded false data injection (EU-FDI) attacks. Specifically, a novel attack-resilient observer layer (OL) is first designed to defend against EU-FDI attacks on the OL. Then, an attack-resilient compensational signal is designed to mitigate the adverse effects caused by the EU-FDI attack on control input layer (CIL). Finally, a SAAR controller is designed by solving a quadratic programming (QP) problem integrating control barrier function (CBF) certified collision-free safety constraints. Rigorous Lyapunov-based stability analysis certifies the SAAR controller's effectiveness in ensuring both safety and resilience. This study also pioneers a three-dimensional (3D) simulation of the SAAR containment control problem for heterogeneous MASs, demonstrating its applicability in realistic multi-agent scenarios.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Efficiency in Near-State and State-Optimal Self-Stabilising Leader Election Population Protocols</title>
<link>https://arxiv.org/abs/2502.01227</link>
<guid>https://arxiv.org/abs/2502.01227</guid>
<content:encoded><![CDATA[
arXiv:2502.01227v2 Announce Type: replace 
Abstract: We investigate leader election problem via ranking within self-stabilising population protocols. In this scenario, the agent's state space comprises $n$ rank states and $x$ extra states. The initial configuration of $n$ agents consists of arbitrary arrangements of rank and extra states, with the objective of self-ranking. Specifically, each agent is tasked with stabilising in a unique rank state silently, implying that after stabilisation, each agent remains in its designated state indefinitely.
  In this paper, we present several new self-stabilising ranking protocols, greatly enriching our comprehension of these intricate problems. All protocols ensure self-stabilisation time with high probability (whp), defined as $1-n^{-\eta},$ for a constant $\eta>0.$ We delve into three scenarios, from which we derive stable (always correct), either state-optimal or almost state-optimal, silent ranking protocols that self-stabilise within a time frame of $o(n^2)$ whp, including:
  - Utilising a novel concept of an agent trap, we derive a state-optimal ranking protocol that achieves self-stabilisation in time $O(min(kn^{3/2},n^2\log^2 n)),$ for any $k$-distant starting configuration.
  - Furthermore, we show that the incorporation of a single extra state ($x=1$) ensures a ranking protocol that self-stabilises in time $O(n^{7/4}\log^2 n)=o(n^2)$, regardless of the initial configuration.
  - Lastly, we show that extra $x=O(\log n)$ states admit self-stabilising ranking with the best currently known stabilisation time $O(n\log n)$, when whp and $x=O(\log n)$ guarantees are imposed.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust-Sorting and Applications to Ulam-Median</title>
<link>https://arxiv.org/abs/2502.07653</link>
<guid>https://arxiv.org/abs/2502.07653</guid>
<content:encoded><![CDATA[
arXiv:2502.07653v2 Announce Type: replace 
Abstract: Sorting is one of the most basic primitives in many algorithms and data analysis tasks. Comparison-based sorting algorithms, like quick-sort and merge-sort, are known to be optimal when the outcome of each comparison is error-free. However, many real-world sorting applications operate in scenarios where the outcome of each comparison can be noisy. In this work, we explore settings where a bounded number of comparisons are potentially corrupted by erroneous agents, resulting in arbitrary, adversarial outcomes.
  We model the sorting problem as a query-limited tournament graph where edges involving erroneous nodes may yield arbitrary results. Our primary contribution is a randomized algorithm inspired by quick-sort that, in expectation, produces an ordering close to the true total order while only querying $\tilde{O}(n)$ edges. We achieve a distance from the target order $\pi$ within $(3 + \epsilon)|B|$, where $B$ is the set of erroneous nodes, balancing the competing objectives of minimizing both query complexity and misalignment with $\pi$. Our algorithm needs to carefully balance two aspects: identify a pivot that partitions the vertex set evenly and ensure that this partition is "truthful" and yet query as few "triangles" in the graph $G$ as possible. Since the nodes in $B$ can potentially hide in an intricate manner, our algorithm requires several technical steps.
  Additionally, we demonstrate significant implications for the Ulam-$k$-Median problem, a classical clustering problem where the metric is defined on the set of permutations on a set of $d$ elements. Chakraborty, Das, and Krauthgamer gave a $(2-\varepsilon)$ FPT approximation algorithm for this problem, where the running time is super-linear in both $n$ and $d$. We use our robust sorting framework to give the first $(2-\varepsilon)$ FPT linear time approximation algorithm for this problem.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictive Planner for Autonomous Driving with Consistency Models</title>
<link>https://arxiv.org/abs/2502.08033</link>
<guid>https://arxiv.org/abs/2502.08033</guid>
<content:encoded><![CDATA[
arXiv:2502.08033v2 Announce Type: replace 
Abstract: Trajectory prediction and planning are essential for autonomous vehicles to navigate safely and efficiently in dynamic environments. Traditional approaches often treat them separately, limiting the ability for interactive planning. While recent diffusion-based generative models have shown promise in multi-agent trajectory generation, their slow sampling is less suitable for high-frequency planning tasks. In this paper, we leverage the consistency model to build a predictive planner that samples from a joint distribution of ego and surrounding agents, conditioned on the ego vehicle's navigational goal. Trained on real-world human driving datasets, our consistency model generates higher-quality trajectories with fewer sampling steps than standard diffusion models, making it more suitable for real-time deployment. To enforce multiple planning constraints simultaneously on the ego trajectory, a novel online guided sampling approach inspired by the Alternating Direction Method of Multipliers (ADMM) is introduced. Evaluated on the Waymo Open Motion Dataset (WOMD), our method enables proactive behavior such as nudging and yielding, and also demonstrates smoother, safer, and more efficient trajectories and satisfaction of multiple constraints under a limited computational budget.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Letters from Future Self: Augmenting the Letter-Exchange Exercise with LLM-based Agents to Enhance Young Adults' Career Exploration</title>
<link>https://arxiv.org/abs/2502.18881</link>
<guid>https://arxiv.org/abs/2502.18881</guid>
<content:encoded><![CDATA[
arXiv:2502.18881v2 Announce Type: replace 
Abstract: Young adults often encounter challenges in career exploration. Self-guided interventions, such as the letter-exchange exercise, where participants envision and adopt the perspective of their future selves by exchanging letters with their envisioned future selves, can support career development. However, the broader adoption of such interventions may be limited without structured guidance. To address this, we integrated Large Language Model (LLM)-based agents that simulate participants' future selves into the letter-exchange exercise and evaluated their effectiveness. A one-week experiment (N=36) compared three conditions: (1) participants manually writing replies to themselves from the perspective of their future selves (baseline), (2) future-self agents generating letters to participants, and (3) future-self agents engaging in chat conversations with participants. Results indicated that exchanging letters with future-self agents enhanced participants' engagement during the exercise, while overall benefits of the intervention on future orientation, career self-concept, and psychological support remained comparable across conditions. We discuss design implications for AI-augmented interventions for supporting young adults' career exploration.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EgoNormia: Benchmarking Physical Social Norm Understanding</title>
<link>https://arxiv.org/abs/2502.20490</link>
<guid>https://arxiv.org/abs/2502.20490</guid>
<content:encoded><![CDATA[
arXiv:2502.20490v3 Announce Type: replace 
Abstract: Human activity is moderated by norms. However, machines are often trained without explicit supervision on norm understanding and reasoning, particularly when norms are physically- or socially-grounded. To improve and evaluate the normative reasoning capability of vision-language models (VLMs), we present \dataset{} $\|\epsilon\|$, consisting of 1,853 challenging, multi-stage MCQ questions based on ego-centric videos of human interactions, evaluating both the prediction and justification of normative actions. The normative actions encompass seven categories: safety, privacy, proxemics, politeness, cooperation, coordination/proactivity, and communication/legibility. To compile this dataset at scale, we propose a novel pipeline leveraging video sampling, automatic answer generation, filtering, and human validation. Our work demonstrates that current state-of-the-art vision-language models lack robust norm understanding, scoring a maximum of 54\% on \dataset{} (versus a human bench of 92\%). Our analysis of performance in each dimension highlights the significant risks of safety, privacy, and the lack of collaboration and communication capability when applied to real-world agents. We additionally show that through a retrieval-based generation (RAG) method, it is possible to use \dataset{} to enhance normative reasoning in VLMs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Trajectory Stitching through Diffusion Composition</title>
<link>https://arxiv.org/abs/2503.05153</link>
<guid>https://arxiv.org/abs/2503.05153</guid>
<content:encoded><![CDATA[
arXiv:2503.05153v2 Announce Type: replace 
Abstract: Effective trajectory stitching for long-horizon planning is a significant challenge in robotic decision-making. While diffusion models have shown promise in planning, they are limited to solving tasks similar to those seen in their training data. We propose CompDiffuser, a novel generative approach that can solve new tasks by learning to compositionally stitch together shorter trajectory chunks from previously seen tasks. Our key insight is modeling the trajectory distribution by subdividing it into overlapping chunks and learning their conditional relationships through a single bidirectional diffusion model. This allows information to propagate between segments during generation, ensuring physically consistent connections. We conduct experiments on benchmark tasks of various difficulties, covering different environment sizes, agent state dimension, trajectory types, training data quality, and show that CompDiffuser significantly outperforms existing methods.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of the Unscented Transform for Cooperative Localization with Ranging-Only Information</title>
<link>https://arxiv.org/abs/2504.07242</link>
<guid>https://arxiv.org/abs/2504.07242</guid>
<content:encoded><![CDATA[
arXiv:2504.07242v2 Announce Type: replace 
Abstract: Cooperative localization in multi-agent robotic systems is challenging, especially when agents rely on limited information, such as only peer-to-peer range measurements. Two key challenges arise: utilizing this limited information to improve position estimation; handling uncertainties from sensor noise, nonlinearity, and unknown correlations between agents measurements; and avoiding information reuse. This paper examines the use of the Unscented Transform (UT) for state estimation for a case in which range measurement between agents and covariance intersection (CI) is used to handle unknown correlations. Unlike Kalman Filter approaches, CI methods fuse complete state and covariance estimates. This makes formulating a CI approach with ranging-only measurements a challenge. To overcome this, UT is used to handle uncertainties and formulate a cooperative state update using range measurements and current cooperative state estimates. This introduces information reuse in the measurement update. Therefore, this work aims to evaluate the limitations and utility of this formulation when faced with various levels of state measurement uncertainty and errors.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast2comm:Collaborative perception combined with prior knowledge</title>
<link>https://arxiv.org/abs/2505.00740</link>
<guid>https://arxiv.org/abs/2505.00740</guid>
<content:encoded><![CDATA[
arXiv:2505.00740v1 Announce Type: new 
Abstract: Collaborative perception has the potential to significantly enhance perceptual accuracy through the sharing of complementary information among agents. However, real-world collaborative perception faces persistent challenges, particularly in balancing perception performance and bandwidth limitations, as well as coping with localization errors. To address these challenges, we propose Fast2comm, a prior knowledge-based collaborative perception framework. Specifically, (1)we propose a prior-supervised confidence feature generation method, that effectively distinguishes foreground from background by producing highly discriminative confidence features; (2)we propose GT Bounding Box-based spatial prior feature selection strategy to ensure that only the most informative prior-knowledge features are selected and shared, thereby minimizing background noise and optimizing bandwidth efficiency while enhancing adaptability to localization inaccuracies; (3)we decouple the feature fusion strategies between model training and testing phases, enabling dynamic bandwidth adaptation. To comprehensively validate our framework, we conduct extensive experiments on both real-world and simulated datasets. The results demonstrate the superior performance of our model and highlight the necessity of the proposed methods. Our code is available at https://github.com/Zhangzhengbin-TJ/Fast2comm.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DOPE: Dual Object Perception-Enhancement Network for Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2505.00743</link>
<guid>https://arxiv.org/abs/2505.00743</guid>
<content:encoded><![CDATA[
arXiv:2505.00743v1 Announce Type: new 
Abstract: Vision-and-Language Navigation (VLN) is a challenging task where an agent must understand language instructions and navigate unfamiliar environments using visual cues. The agent must accurately locate the target based on visual information from the environment and complete tasks through interaction with the surroundings. Despite significant advancements in this field, two major limitations persist: (1) Many existing methods input complete language instructions directly into multi-layer Transformer networks without fully exploiting the detailed information within the instructions, thereby limiting the agent's language understanding capabilities during task execution; (2) Current approaches often overlook the modeling of object relationships across different modalities, failing to effectively utilize latent clues between objects, which affects the accuracy and robustness of navigation decisions. We propose a Dual Object Perception-Enhancement Network (DOPE) to address these issues to improve navigation performance. First, we design a Text Semantic Extraction (TSE) to extract relatively essential phrases from the text and input them into the Text Object Perception-Augmentation (TOPA) to fully leverage details such as objects and actions within the instructions. Second, we introduce an Image Object Perception-Augmentation (IOPA), which performs additional modeling of object information across different modalities, enabling the model to more effectively utilize latent clues between objects in images and text, enhancing decision-making accuracy. Extensive experiments on the R2R and REVERIE datasets validate the efficacy of the proposed approach.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wireless Communication as an Information Sensor for Multi-agent Cooperative Perception: A Survey</title>
<link>https://arxiv.org/abs/2505.00747</link>
<guid>https://arxiv.org/abs/2505.00747</guid>
<content:encoded><![CDATA[
arXiv:2505.00747v1 Announce Type: new 
Abstract: Cooperative perception extends the perception capabilities of autonomous vehicles by enabling multi-agent information sharing via Vehicle-to-Everything (V2X) communication. Unlike traditional onboard sensors, V2X acts as a dynamic "information sensor" characterized by limited communication, heterogeneity, mobility, and scalability. This survey provides a comprehensive review of recent advancements from the perspective of information-centric cooperative perception, focusing on three key dimensions: information representation, information fusion, and large-scale deployment. We categorize information representation into data-level, feature-level, and object-level schemes, and highlight emerging methods for reducing data volume and compressing messages under communication constraints. In information fusion, we explore techniques under both ideal and non-ideal conditions, including those addressing heterogeneity, localization errors, latency, and packet loss. Finally, we summarize system-level approaches to support scalability in dense traffic scenarios. Compared with existing surveys, this paper introduces a new perspective by treating V2X communication as an information sensor and emphasizing the challenges of deploying cooperative perception in real-world intelligent transportation systems.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Coral Protocol: Open Infrastructure Connecting The Internet of Agents</title>
<link>https://arxiv.org/abs/2505.00749</link>
<guid>https://arxiv.org/abs/2505.00749</guid>
<content:encoded><![CDATA[
arXiv:2505.00749v1 Announce Type: new 
Abstract: The Coral Protocol is an open and decentralized collaboration infrastructure that enables communication, coordination, trust and payments for The Internet of Agents. It addresses the growing need for interoperability in a world where organizations are deploying multiple specialized AI agents that must work together across domains and vendors. As a foundational platform for multi-agent AI ecosystems, Coral establishes a common language and coordination framework allowing any agent to participate in complex workflows with others. Its design emphasizes broad compatibility, security, and vendor neutrality, ensuring that agent interactions are efficient and trustworthy. In particular, Coral introduces standardized messaging formats for agent communication, a modular coordination mechanism for orchestrating multi-agent tasks, and secure team formation capabilities for dynamically assembling trusted groups of agents. Together, these innovations position Coral Protocol as a cornerstone of the emerging "Internet of Agents," unlocking new levels of automation, collective intelligence, and business value through open agent collaboration.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Large Language Model based Human-Agent Systems</title>
<link>https://arxiv.org/abs/2505.00753</link>
<guid>https://arxiv.org/abs/2505.00753</guid>
<content:encoded><![CDATA[
arXiv:2505.00753v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. This paper provides the first comprehensive and structured survey of LLM-HAS. It clarifies fundamental concepts, systematically presents core components shaping these systems, including environment & profiling, human feedback, interaction types, orchestration and communication, explores emerging applications, and discusses unique challenges and opportunities. By consolidating current knowledge and offering a structured overview, we aim to foster further research and innovation in this rapidly evolving interdisciplinary field. Paper lists and resources are available at https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Language Models as Text-to-Image Model Evaluators</title>
<link>https://arxiv.org/abs/2505.00759</link>
<guid>https://arxiv.org/abs/2505.00759</guid>
<content:encoded><![CDATA[
arXiv:2505.00759v1 Announce Type: new 
Abstract: The steady improvements of text-to-image (T2I) generative models lead to slow deprecation of automatic evaluation benchmarks that rely on static datasets, motivating researchers to seek alternative ways to evaluate the T2I progress. In this paper, we explore the potential of multi-modal large language models (MLLMs) as evaluator agents that interact with a T2I model, with the objective of assessing prompt-generation consistency and image aesthetics. We present Multimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively generates prompts for evaluation, scores generated images and matches T2I evaluation of existing benchmarks with a fraction of the prompts used in existing static benchmarks. Moreover, we show that MT2IE's prompt-generation consistency scores have higher correlation with human judgment than scores previously introduced in the literature. MT2IE generates prompts that are efficient at probing T2I model performance, producing the same relative T2I model rankings as existing benchmarks while using only 1/80th the number of prompts for evaluation.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconfigurable legged metamachines that run on autonomous modular legs</title>
<link>https://arxiv.org/abs/2505.00784</link>
<guid>https://arxiv.org/abs/2505.00784</guid>
<content:encoded><![CDATA[
arXiv:2505.00784v1 Announce Type: new 
Abstract: Legged machines are becoming increasingly agile and adaptive but they have so far lacked the basic reconfigurability of legged animals, which have been rearranged and reshaped to fill millions of niches. Unlike their biological counterparts, legged machines have largely converged over the past decade to canonical quadrupedal and bipedal architectures that cannot be easily reconfigured to meet new tasks or recover from injury. Here we introduce autonomous modular legs: agile yet minimal, single-degree-of-freedom jointed links that can learn complex dynamic behaviors and may be freely attached to form legged metamachines at the meter scale. This enables rapid repair, redesign, and recombination of highly-dynamic modular agents that move quickly and acrobatically (non-quasistatically) through unstructured environments. Because each module is itself a complete agent, legged metamachines are able to sustain deep structural damage that would completely disable other legged robots. We also show how to encode the vast space of possible body configurations into a compact latent design genome that can be efficiently explored, revealing a wide diversity of novel legged forms.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HMCF: A Human-in-the-loop Multi-Robot Collaboration Framework Based on Large Language Models</title>
<link>https://arxiv.org/abs/2505.00820</link>
<guid>https://arxiv.org/abs/2505.00820</guid>
<content:encoded><![CDATA[
arXiv:2505.00820v1 Announce Type: new 
Abstract: Rapid advancements in artificial intelligence (AI) have enabled robots to performcomplex tasks autonomously with increasing precision. However, multi-robot systems (MRSs) face challenges in generalization, heterogeneity, and safety, especially when scaling to large-scale deployments like disaster response. Traditional approaches often lack generalization, requiring extensive engineering for new tasks and scenarios, and struggle with managing diverse robots. To overcome these limitations, we propose a Human-in-the-loop Multi-Robot Collaboration Framework (HMCF) powered by large language models (LLMs). LLMs enhance adaptability by reasoning over diverse tasks and robot capabilities, while human oversight ensures safety and reliability, intervening only when necessary. Our framework seamlessly integrates human oversight, LLM agents, and heterogeneous robots to optimize task allocation and execution. Each robot is equipped with an LLM agent capable of understanding its capabilities, converting tasks into executable instructions, and reducing hallucinations through task verification and human supervision. Simulation results show that our framework outperforms state-of-the-art task planning methods, achieving higher task success rates with an improvement of 4.76%. Real-world tests demonstrate its robust zero-shot generalization feature and ability to handle diverse tasks and environments with minimal human intervention.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Texts to Shields: Convergence of Large Language Models and Cybersecurity</title>
<link>https://arxiv.org/abs/2505.00841</link>
<guid>https://arxiv.org/abs/2505.00841</guid>
<content:encoded><![CDATA[
arXiv:2505.00841v1 Announce Type: new 
Abstract: This report explores the convergence of large language models (LLMs) and cybersecurity, synthesizing interdisciplinary insights from network security, artificial intelligence, formal methods, and human-centered design. It examines emerging applications of LLMs in software and network security, 5G vulnerability analysis, and generative security engineering. The report highlights the role of agentic LLMs in automating complex tasks, improving operational efficiency, and enabling reasoning-driven security analytics. Socio-technical challenges associated with the deployment of LLMs -- including trust, transparency, and ethical considerations -- can be addressed through strategies such as human-in-the-loop systems, role-specific training, and proactive robustness testing. The report further outlines critical research challenges in ensuring interpretability, safety, and fairness in LLM-based systems, particularly in high-stakes domains. By integrating technical advances with organizational and societal considerations, this report presents a forward-looking research agenda for the secure and effective adoption of LLMs in cybersecurity.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thoughts without Thinking: Reconsidering the Explanatory Value of Chain-of-Thought Reasoning in LLMs through Agentic Pipelines</title>
<link>https://arxiv.org/abs/2505.00875</link>
<guid>https://arxiv.org/abs/2505.00875</guid>
<content:encoded><![CDATA[
arXiv:2505.00875v1 Announce Type: new 
Abstract: Agentic pipelines present novel challenges and opportunities for human-centered explainability. The HCXAI community is still grappling with how best to make the inner workings of LLMs transparent in actionable ways. Agentic pipelines consist of multiple LLMs working in cooperation with minimal human control. In this research paper, we present early findings from an agentic pipeline implementation of a perceptive task guidance system. Through quantitative and qualitative analysis, we analyze how Chain-of-Thought (CoT) reasoning, a common vehicle for explainability in LLMs, operates within agentic pipelines. We demonstrate that CoT reasoning alone does not lead to better outputs, nor does it offer explainability, as it tends to produce explanations without explainability, in that they do not improve the ability of end users to better understand systems or achieve their goals.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning without Performance Degradation</title>
<link>https://arxiv.org/abs/2505.00913</link>
<guid>https://arxiv.org/abs/2505.00913</guid>
<content:encoded><![CDATA[
arXiv:2505.00913v1 Announce Type: new 
Abstract: Fine-tuning policies learned offline remains a major challenge in application domains. Monotonic performance improvement during \emph{fine-tuning} is often challenging, as agents typically experience performance degradation at the early fine-tuning stage. The community has identified multiple difficulties in fine-tuning a learned network online, however, the majority of progress has focused on improving learning efficiency during fine-tuning. In practice, this comes at a serious cost during fine-tuning: initially, agent performance degrades as the agent explores and effectively overrides the policy learned offline. We show across a range of settings, many offline-to-online algorithms exhibit either (1) performance degradation or (2) slow learning (sometimes effectively no improvement) during fine-tuning. We introduce a new fine-tuning algorithm, based on an algorithm called Jump Start, that gradually allows more exploration based on online estimates of performance. Empirically, this approach achieves fast fine-tuning and significantly reduces performance degradations compared with existing algorithms designed to do the same.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Virtual Force-Based Routing of Modular Agents on a Graph</title>
<link>https://arxiv.org/abs/2505.00928</link>
<guid>https://arxiv.org/abs/2505.00928</guid>
<content:encoded><![CDATA[
arXiv:2505.00928v1 Announce Type: new 
Abstract: Modular vehicles have become an area of academic interest in the field of multi-agent systems. Modularity allows vehicles to connect and disconnect with each other mid-transit which provides a balance between efficiency and flexibility when solving complex and large scale tasks in urban or aerial transportation. This paper details a generalized scheme to route multiple modular agents on a graph to a predetermined set of target nodes. The objective is to visit all target nodes while incurring minimum resource expenditure. Agents that are joined together will incur the equivalent cost of a single agent, which is motivated by the logistical benefits of traffic reduction and increased fuel efficiency. To solve this problem, we introduce a heuristic algorithm that seeks to balance the optimality of the path that an agent takes and the cost benefit of joining agents. Our approach models the agents and targets as point charges, where the agents take the path of highest attractive force from its target node and neighboring agents. We validate our approach by simulating multiple modular agents along real-world transportation routes in the road network of Champaign-Urbana, Illinois, USA. For two vehicles, it performed equally compared to an existing modular-agent routing algorithm. Three agents were then routed using our method and the performance was benchmarked against non-modular agents using a simple shortest path policy where it performs better than the non-modular implementation 81 percent of the time. Moreover, we show that the proposed algorithm operates faster than existing routing methods for modular agents.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Embodied Agents: When Robotics Meets Deep Learning Reasoning</title>
<link>https://arxiv.org/abs/2505.00935</link>
<guid>https://arxiv.org/abs/2505.00935</guid>
<content:encoded><![CDATA[
arXiv:2505.00935v1 Announce Type: new 
Abstract: The increase in available computing power and the Deep Learning revolution have allowed the exploration of new topics and frontiers in Artificial Intelligence research. A new field called Embodied Artificial Intelligence, which places at the intersection of Computer Vision, Robotics, and Decision Making, has been gaining importance during the last few years, as it aims to foster the development of smart autonomous robots and their deployment in society. The recent availability of large collections of 3D models for photorealistic robotic simulation has allowed faster and safe training of learning-based agents for millions of frames and a careful evaluation of their behavior before deploying the models on real robotic platforms. These intelligent agents are intended to perform a certain task in a possibly unknown environment. To this end, during the training in simulation, the agents learn to perform continuous interactions with the surroundings, such as gathering information from the environment, encoding and extracting useful cues for the task, and performing actions towards the final goal; where every action of the agent influences the interactions. This dissertation follows the complete creation process of embodied agents for indoor environments, from their concept to their implementation and deployment. We aim to contribute to research in Embodied AI and autonomous agents, in order to foster future work in this field. We present a detailed analysis of the procedure behind implementing an intelligent embodied agent, comprehending a thorough description of the current state-of-the-art in literature, technical explanations of the proposed methods, and accurate experimental studies on relevant robotic tasks.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSRLBot: Designing and Developing an LLM-based Agent using Socially Shared Regulated Learning</title>
<link>https://arxiv.org/abs/2505.00945</link>
<guid>https://arxiv.org/abs/2505.00945</guid>
<content:encoded><![CDATA[
arXiv:2505.00945v1 Announce Type: new 
Abstract: Large language model (LLM)-based agents are increasingly used to support human experts by streamlining complex tasks and offering actionable insights. However, their application in multi-professional decision-making, particularly in teamwork contexts, remains underexplored. This design-based study addresses that gap by developing LLM functions to enhance collaboration, grounded in the Socially Shared Regulation of Learning (SSRL) framework and applied to medical diagnostic teamwork. SSRL emphasizes metacognitive, cognitive, motivational, and emotional processes in shared learning, focusing on how teams manage these processes to improve decision-making. This paper introduces SSRLBot, a prototype chatbot designed to help team members reflect on both their diagnostic performance and key SSRL skills. Its core functions include summarizing dialogues, analyzing SSRL behaviors, evaluating diagnostic outcomes, annotating SSRL markers in conversation, assessing their impact on performance, and identifying interpersonal regulatory dynamics. We compare SSRLBot's capabilities with those of Gemini-1.5, GPT-3.5, and Deepseek-R1 in a case study. SSRLBot demonstrates stronger alignment with SSRL theory, offering detailed evaluations that link behaviors to regulatory dimensions and suggesting improvements for collaboration. By integrating SSRL theory with LLM capabilities, SSRLBot contributes a novel tool for enhancing team-based decision-making and collaborative learning in high-stakes environments, such as medical education.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeking to Collide: Online Safety-Critical Scenario Generation for Autonomous Driving with Retrieval Augmented Large Language Models</title>
<link>https://arxiv.org/abs/2505.00972</link>
<guid>https://arxiv.org/abs/2505.00972</guid>
<content:encoded><![CDATA[
arXiv:2505.00972v1 Announce Type: new 
Abstract: Simulation-based testing is crucial for validating autonomous vehicles (AVs), yet existing scenario generation methods either overfit to common driving patterns or operate in an offline, non-interactive manner that fails to expose rare, safety-critical corner cases. In this paper, we introduce an online, retrieval-augmented large language model (LLM) framework for generating safety-critical driving scenarios. Our method first employs an LLM-based behavior analyzer to infer the most dangerous intent of the background vehicle from the observed state, then queries additional LLM agents to synthesize feasible adversarial trajectories. To mitigate catastrophic forgetting and accelerate adaptation, we augment the framework with a dynamic memorization and retrieval bank of intent-planner pairs, automatically expanding its behavioral library when novel intents arise. Evaluations using the Waymo Open Motion Dataset demonstrate that our model reduces the mean minimum time-to-collision from 1.62 to 1.08 s and incurs a 75% collision rate, substantially outperforming baselines.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-agents based User Values Mining for Recommendation</title>
<link>https://arxiv.org/abs/2505.00981</link>
<guid>https://arxiv.org/abs/2505.00981</guid>
<content:encoded><![CDATA[
arXiv:2505.00981v1 Announce Type: new 
Abstract: Recommender systems have rapidly evolved and become integral to many online services. However, existing systems sometimes produce unstable and unsatisfactory recommendations that fail to align with users' fundamental and long-term preferences. This is because they primarily focus on extracting shallow and short-term interests from user behavior data, which is inherently dynamic and challenging to model. Unlike these transient interests, user values are more stable and play a crucial role in shaping user behaviors, such as purchasing items and consuming content. Incorporating user values into recommender systems can help stabilize recommendation performance and ensure results better reflect users' latent preferences. However, acquiring user values is typically difficult and costly. To address this challenge, we leverage the strong language understanding, zero-shot inference, and generalization capabilities of Large Language Models (LLMs) to extract user values from users' historical interactions. Unfortunately, direct extraction using LLMs presents several challenges such as length constraints and hallucination. To overcome these issues, we propose ZOOM, a zero-shot multi-LLM collaborative framework for effective and accurate user value extraction. In ZOOM, we apply text summarization techniques to condense item content while preserving essential meaning. To mitigate hallucinations, ZOOM introduces two specialized agent roles: evaluators and supervisors, to collaboratively generate accurate user values. Extensive experiments on two widely used recommendation datasets with two state-of-the-art recommendation models demonstrate the effectiveness and generalization of our framework in automatic user value mining and recommendation performance improvement.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language</title>
<link>https://arxiv.org/abs/2505.00989</link>
<guid>https://arxiv.org/abs/2505.00989</guid>
<content:encoded><![CDATA[
arXiv:2505.00989v1 Announce Type: new 
Abstract: Vessel Traffic Services (VTS) are essential for maritime safety and regulatory compliance through real-time traffic management. However, with increasing traffic complexity and the prevalence of heterogeneous, multimodal data, existing VTS systems face limitations in spatiotemporal reasoning and intuitive human interaction. In this work, we propose VTS-LLM Agent, the first domain-adaptive large LLM agent tailored for interactive decision support in VTS operations. We formalize risk-prone vessel identification as a knowledge-augmented Text-to-SQL task, combining structured vessel databases with external maritime knowledge. To support this, we construct a curated benchmark dataset consisting of a custom schema, domain-specific corpus, and a query-SQL test set in multiple linguistic styles. Our framework incorporates NER-based relational reasoning, agent-based domain knowledge injection, semantic algebra intermediate representation, and query rethink mechanisms to enhance domain grounding and context-aware understanding. Experimental results show that VTS-LLM outperforms both general-purpose and SQL-focused baselines under command-style, operational-style, and formal natural language queries, respectively. Moreover, our analysis provides the first empirical evidence that linguistic style variation introduces systematic performance challenges in Text-to-SQL modeling. This work lays the foundation for natural language interfaces in vessel traffic services and opens new opportunities for proactive, LLM-driven maritime real-time traffic management.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence in Government: Why People Feel They Lose Control</title>
<link>https://arxiv.org/abs/2505.01085</link>
<guid>https://arxiv.org/abs/2505.01085</guid>
<content:encoded><![CDATA[
arXiv:2505.01085v1 Announce Type: new 
Abstract: The use of Artificial Intelligence (AI) in public administration is expanding rapidly, moving from automating routine tasks to deploying generative and agentic systems that autonomously act on goals. While AI promises greater efficiency and responsiveness, its integration into government functions raises concerns about fairness, transparency, and accountability. This article applies principal-agent theory (PAT) to conceptualize AI adoption as a special case of delegation, highlighting three core tensions: assessability (can decisions be understood?), dependency (can the delegation be reversed?), and contestability (can decisions be challenged?). These structural challenges may lead to a "failure-by-success" dynamic, where early functional gains obscure long-term risks to democratic legitimacy. To test this framework, we conducted a pre-registered factorial survey experiment across tax, welfare, and law enforcement domains. Our findings show that although efficiency gains initially bolster trust, they simultaneously reduce citizens' perceived control. When the structural risks come to the foreground, institutional trust and perceived control both drop sharply, suggesting that hidden costs of AI adoption significantly shape public attitudes. The study demonstrates that PAT offers a powerful lens for understanding the institutional and political implications of AI in government, emphasizing the need for policymakers to address delegation risks transparently to maintain public trust.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Equity of Climate Policies using Multi-Agent Multi-Objective Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.01115</link>
<guid>https://arxiv.org/abs/2505.01115</guid>
<content:encoded><![CDATA[
arXiv:2505.01115v1 Announce Type: new 
Abstract: Addressing climate change requires coordinated policy efforts of nations worldwide. These efforts are informed by scientific reports, which rely in part on Integrated Assessment Models (IAMs), prominent tools used to assess the economic impacts of climate policies. However, traditional IAMs optimize policies based on a single objective, limiting their ability to capture the trade-offs among economic growth, temperature goals, and climate justice. As a result, policy recommendations have been criticized for perpetuating inequalities, fueling disagreements during policy negotiations. We introduce Justice, the first framework integrating IAM with Multi-Objective Multi-Agent Reinforcement Learning (MOMARL). By incorporating multiple objectives, Justice generates policy recommendations that shed light on equity while balancing climate and economic goals. Further, using multiple agents can provide a realistic representation of the interactions among the diverse policy actors. We identify equitable Pareto-optimal policies using our framework, which facilitates deliberative decision-making by presenting policymakers with the inherent trade-offs in climate and economic policy.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable AI Based Diagnosis of Poisoning Attacks in Evolutionary Swarms</title>
<link>https://arxiv.org/abs/2505.01181</link>
<guid>https://arxiv.org/abs/2505.01181</guid>
<content:encoded><![CDATA[
arXiv:2505.01181v1 Announce Type: new 
Abstract: Swarming systems, such as for example multi-drone networks, excel at cooperative tasks like monitoring, surveillance, or disaster assistance in critical environments, where autonomous agents make decentralized decisions in order to fulfill team-level objectives in a robust and efficient manner. Unfortunately, team-level coordinated strategies in the wild are vulnerable to data poisoning attacks, resulting in either inaccurate coordination or adversarial behavior among the agents. To address this challenge, we contribute a framework that investigates the effects of such data poisoning attacks, using explainable AI methods. We model the interaction among agents using evolutionary intelligence, where an optimal coalition strategically emerges to perform coordinated tasks. Then, through a rigorous evaluation, the swarm model is systematically poisoned using data manipulation attacks. We showcase the applicability of explainable AI methods to quantify the effects of poisoning on the team strategy and extract footprint characterizations that enable diagnosing. Our findings indicate that when the model is poisoned above 10%, non-optimal strategies resulting in inefficient cooperation can be identified.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Space-Time Trade-off for Fast Self-Stabilizing Leader Election in Population Protocols</title>
<link>https://arxiv.org/abs/2505.01210</link>
<guid>https://arxiv.org/abs/2505.01210</guid>
<content:encoded><![CDATA[
arXiv:2505.01210v1 Announce Type: new 
Abstract: We consider the problem of self-stabilizing leader election in the population model by Angluin, Aspnes, Diamadi, Fischer, and Peralta (JDistComp '06). The population model is a well-established and powerful model for asynchronous, distributed computation with a large number of applications. For self-stabilizing leader election, the population of $n$ anonymous agents, interacting in uniformly random pairs, must stabilize with a single leader from any possible initial configuration.
  The focus of this paper is to develop time-efficient self-stabilizing protocols whilst minimizing the number of states. We present a parametrized protocol, which, for a suitable setting, achieves the asymptotically optimal time $O(\log n)$ using $2^{O(n^2\log n)}$ states (throughout the paper, ``time'' refers to ``parallel time'', i.e., the number of pairwise interactions divided by $n$). This is a significant improvement over the previously best protocol Sublinear-Time-SSR due to Burman, Chen, Chen, Doty, Nowak, Severson, and Xu (PODC '21), which requires $2^{O(n^{\log n}\log n)}$ states for the same time bound. In general, for $1\le r\le n/2$, our protocol requires $2^{O(r^2\log{n})}$ states and stabilizes in time $O((n\log{n})/r)$, w.h.p.; the above result is achieved for $r=\Theta(n)$. For $r=\log^2n$ our protocol requires only sub-linear time using only $2^{O(\log^3 n)}$ states, resolving an open problem stated in that paper. Sublinear-Time-SSR requires $O(\log n\cdot n^{1/(H+1)})$ time using $2^{\Theta(n^H) \cdot \log n}$ states for all $1\le H\le\Theta(\log n)$.
  Similar to previous works, it solves leader election by assigning a unique rank from $1$ through $n$ to each agent. The principal bottleneck for self-stabilizing ranking usually is to detect if there exist agents with the same rank. One of our main conceptual contributions is a novel technique for collision detection.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bilateral Cognitive Security Games in Networked Control Systems under Stealthy Injection Attacks</title>
<link>https://arxiv.org/abs/2505.01232</link>
<guid>https://arxiv.org/abs/2505.01232</guid>
<content:encoded><![CDATA[
arXiv:2505.01232v1 Announce Type: new 
Abstract: This paper studies a strategic security problem in networked control systems under stealthy false data injection attacks. The security problem is modeled as a bilateral cognitive security game between a defender and an adversary, each possessing cognitive reasoning abilities. The adversary with an adversarial cognitive ability strategically attacks some interconnections of the system with the aim of disrupting the network performance while remaining stealthy to the defender. Meanwhile, the defender with a defense cognitive ability strategically monitors some nodes to impose the stealthiness constraint with the purpose of minimizing the worst-case disruption caused by the adversary. Within the proposed bilateral cognitive security framework, the preferred cognitive levels of the two strategic agents are formulated in terms of two newly proposed concepts, cognitive mismatch and cognitive resonance. Moreover, we propose a method to compute the policies for the defender and the adversary with arbitrary cognitive abilities. A sufficient condition is established under which an increase in cognitive levels does not alter the policies for the defender and the adversary, ensuring convergence. The obtained results are validated through numerical simulations.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pattern formation using an intrinsic optimal control approach</title>
<link>https://arxiv.org/abs/2505.01302</link>
<guid>https://arxiv.org/abs/2505.01302</guid>
<content:encoded><![CDATA[
arXiv:2505.01302v1 Announce Type: new 
Abstract: This paper investigates a pattern formation control problem for a multi-agent system modeled with given interaction topology, in which $m$ of the $n$ agents are chosen as leaders and consequently a control signal is added to each of the leaders. These agents interact with each other by Laplacian dynamics on a graph. The pattern formation control problem is formulated as an intrinsic infinite time-horizon linear quadratic optimal control problem, namely, no error information is incorporated in the objective function. Under mild conditions, we show the existence of the optimal control strategy and the convergence to the desired pattern formation. Based on the optimal control strategy, we propose a distributed control strategy to achieve the given pattern. Finally, numerical simulation is given to illustrate theoretical results.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integration of Multi-Mode Preference into Home Energy Management System Using Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.01332</link>
<guid>https://arxiv.org/abs/2505.01332</guid>
<content:encoded><![CDATA[
arXiv:2505.01332v1 Announce Type: new 
Abstract: Home Energy Management Systems (HEMS) have emerged as a pivotal tool in the smart home ecosystem, aiming to enhance energy efficiency, reduce costs, and improve user comfort. By enabling intelligent control and optimization of household energy consumption, HEMS plays a significant role in bridging the gap between consumer needs and energy utility objectives. However, much of the existing literature construes consumer comfort as a mere deviation from the standard appliance settings. Such deviations are typically incorporated into optimization objectives via static weighting factors. These factors often overlook the dynamic nature of consumer behaviors and preferences. Addressing this oversight, our paper introduces a multi-mode Deep Reinforcement Learning-based HEMS (DRL-HEMS) framework, meticulously designed to optimize based on dynamic, consumer-defined preferences. Our primary goal is to augment consumer involvement in Demand Response (DR) programs by embedding dynamic multi-mode preferences tailored to individual appliances. In this study, we leverage a model-free, single-agent DRL algorithm to deliver a HEMS framework that is not only dynamic but also user-friendly. To validate its efficacy, we employed real-world data at 15-minute intervals, including metrics such as electricity price, ambient temperature, and appliances' power consumption. Our results show that the model performs exceptionally well in optimizing energy consumption within different preference modes. Furthermore, when compared to traditional algorithms based on Mixed-Integer Linear Programming (MILP), our model achieves nearly optimal performance while outperforming in computational efficiency.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Diversity in Parallel Agents: A Maximum State Entropy Exploration Story</title>
<link>https://arxiv.org/abs/2505.01336</link>
<guid>https://arxiv.org/abs/2505.01336</guid>
<content:encoded><![CDATA[
arXiv:2505.01336v1 Announce Type: new 
Abstract: Parallel data collection has redefined Reinforcement Learning (RL), unlocking unprecedented efficiency and powering breakthroughs in large-scale real-world applications. In this paradigm, $N$ identical agents operate in $N$ replicas of an environment simulator, accelerating data collection by a factor of $N$. A critical question arises: \textit{Does specializing the policies of the parallel agents hold the key to surpass the $N$ factor acceleration?} In this paper, we introduce a novel learning framework that maximizes the entropy of collected data in a parallel setting. Our approach carefully balances the entropy of individual agents with inter-agent diversity, effectively minimizing redundancies. The latter idea is implemented with a centralized policy gradient method, which shows promise when evaluated empirically against systems of identical agents, as well as synergy with batch RL techniques that can exploit data diversity. Finally, we provide an original concentration analysis that shows faster rates for specialized parallel sampling distributions, which supports our methodology and may be of independent interest.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward Guidance for Reinforcement Learning Tasks Based on Large Language Models: The LMGT Framework</title>
<link>https://arxiv.org/abs/2409.04744</link>
<guid>https://arxiv.org/abs/2409.04744</guid>
<content:encoded><![CDATA[
arXiv:2409.04744v2 Announce Type: replace 
Abstract: The inherent uncertainty in the environmental transition model of Reinforcement Learning (RL) necessitates a delicate balance between exploration and exploitation. This balance is crucial for optimizing computational resources to accurately estimate expected rewards for the agent. In scenarios with sparse rewards, such as robotic control systems, achieving this balance is particularly challenging. However, given that many environments possess extensive prior knowledge, learning from the ground up in such contexts may be redundant. To address this issue, we propose Language Model Guided reward Tuning (LMGT), a novel, sample-efficient framework. LMGT leverages the comprehensive prior knowledge embedded in Large Language Models (LLMs) and their proficiency in processing non-standard data forms, such as wiki tutorials. By utilizing LLM-guided reward shifts, LMGT adeptly balances exploration and exploitation, thereby guiding the agent's exploratory behavior and enhancing sample efficiency. We have rigorously evaluated LMGT across various RL tasks and evaluated it in the embodied robotic environment Housekeep. Our results demonstrate that LMGT consistently outperforms baseline methods. Furthermore, the findings suggest that our framework can substantially reduce the computational resources required during the RL training phase.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Closed-Loop Long-Horizon Robotic Planning via Equilibrium Sequence Modeling</title>
<link>https://arxiv.org/abs/2410.01440</link>
<guid>https://arxiv.org/abs/2410.01440</guid>
<content:encoded><![CDATA[
arXiv:2410.01440v5 Announce Type: replace 
Abstract: In the endeavor to make autonomous robots take actions, task planning is a major challenge that requires translating high-level task descriptions to long-horizon action sequences. Despite recent advances in language model agents, they remain prone to planning errors and limited in their ability to plan ahead. To address these limitations in robotic planning, we advocate a self-refining scheme that iteratively refines a draft plan until an equilibrium is reached. Remarkably, this process can be optimized end-to-end from an analytical perspective without the need to curate additional verifiers or reward models, allowing us to train self-refining planners in a simple supervised learning fashion. Meanwhile, a nested equilibrium sequence modeling procedure is devised for efficient closed-loop planning that incorporates useful feedback from the environment (or an internal world model). Our method is evaluated on the VirtualHome-Env benchmark, showing advanced performance with improved scaling w.r.t. inference-time computation. Code is available at https://github.com/Singularity0104/equilibrium-planner.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Feedback Loop Modeling Improves Recommendation and User Simulation</title>
<link>https://arxiv.org/abs/2410.20027</link>
<guid>https://arxiv.org/abs/2410.20027</guid>
<content:encoded><![CDATA[
arXiv:2410.20027v2 Announce Type: replace 
Abstract: Large language model-based agents are increasingly applied in the recommendation field due to their extensive knowledge and strong planning capabilities. While prior research has primarily focused on enhancing either the recommendation agent or the user agent individually, the collaborative interaction between the two has often been overlooked. Towards this research gap, we propose a novel framework that emphasizes the feedback loop process to facilitate the collaboration between the recommendation agent and the user agent. Specifically, the recommendation agent refines its understanding of user preferences by analyzing the feedback from the user agent on the item recommendation. Conversely, the user agent further identifies potential user interests based on the items and recommendation reasons provided by the recommendation agent. This iterative process enhances the ability of both agents to infer user behaviors, enabling more effective item recommendations and more accurate user simulations. Extensive experiments on three datasets demonstrate the effectiveness of the agentic feedback loop: the agentic feedback loop yields an average improvement of 11.52% over the single recommendation agent and 21.12% over the single user agent. Furthermore, the results show that the agentic feedback loop does not exacerbate popularity or position bias, which are typically amplified by the real-world feedback loop, highlighting its robustness. The source code is available at https://github.com/Lanyu0303/AFL.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-PySC2: Starcraft II learning environment for Large Language Models</title>
<link>https://arxiv.org/abs/2411.05348</link>
<guid>https://arxiv.org/abs/2411.05348</guid>
<content:encoded><![CDATA[
arXiv:2411.05348v2 Announce Type: replace 
Abstract: The tremendous potential has been demonstrated by large language models (LLMs) in intelligent decision-making problems, with unprecedented capabilities shown across diverse applications ranging from gaming AI systems to complex strategic planning frameworks. However, the StarCraft II platform, which has been widely adopted for validating decision-making algorithms in the past decade, has not yet provided substantial support for this emerging domain. To address issues that LLMs cannot interface with the hundreds of actions of the pysc2 backend and the lack of native support for multi-agent (MA) collaboration, we propose the LLM-PySC2 environment. This is the first environment that offers LLMs the complete pysc2 action space with sufficient multi-modal information and game Wiki knowledge. With an asynchronous query architecture, the environment efficiently interacts with LLMs that maintain a constant latency regardless of the scale of the agents' population. In the experiments, we evaluated LLMs' decision-making performance in both the macro-decision and micro-operation scenarios, with traditional StarCraft II Multi-Agent Challenge (SMAC) tasks and a series of new proposed. Results indicate that LLMs possess the potential to achieve victories in complex scenarios but cannot constantly generate correct decisions, especially in the recovered pysc2 action space and MA settings. Without task-relevant instructions, the pre-trained models suffer from issues such as hallucinations and inefficient collaboration. Our findings suggest that StarCraft II still challenges in the era of large models, revealing that there is a lot to do to develop an advanced LLM decision-making system, and the proposed LLM-PySC2 environment will support future development of LLM-based decision-making solutions.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HEIGHT: Heterogeneous Interaction Graph Transformer for Robot Navigation in Crowded and Constrained Environments</title>
<link>https://arxiv.org/abs/2411.12150</link>
<guid>https://arxiv.org/abs/2411.12150</guid>
<content:encoded><![CDATA[
arXiv:2411.12150v2 Announce Type: replace 
Abstract: We study the problem of robot navigation in dense and interactive crowds with environmental constraints such as corridors and furniture. Previous methods fail to consider all types of interactions among agents and obstacles, leading to unsafe and inefficient robot paths. In this article, we leverage a graph-based representation of crowded and constrained scenarios and propose a structured framework to learn robot navigation policies with deep reinforcement learning. We first split the representations of different components in the environment and propose a heterogeneous spatio-temporal (st) graph to model distinct interactions among humans, robots, and obstacles. Based on the heterogeneous st-graph, we propose HEIGHT, a novel navigation policy network architecture with different components to capture heterogeneous interactions among entities through space and time. HEIGHT utilizes attention mechanisms to prioritize important interactions and a recurrent network to track changes in the dynamic scene over time, encouraging the robot to avoid collisions adaptively. Through extensive simulation and real-world experiments, we demonstrate that HEIGHT outperforms state-of-the-art baselines in terms of success and efficiency in challenging navigation scenarios. Furthermore, we demonstrate that our pipeline achieves better zero-shot generalization capability than previous works when the densities of humans and obstacles change. More videos are available at https://sites.google.com/view/crowdnav-height/home.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2411.14432</link>
<guid>https://arxiv.org/abs/2411.14432</guid>
<content:encoded><![CDATA[
arXiv:2411.14432v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) demonstrate enhanced capabilities and reliability by reasoning more, evolving from Chain-of-Thought prompting to product-level solutions like OpenAI o1. Despite various efforts to improve LLM reasoning, high-quality long-chain reasoning data and optimized training pipelines still remain inadequately explored in vision-language tasks. In this paper, we present Insight-V, an early effort to 1) scalably produce long and robust reasoning data for complex multi-modal tasks, and 2) an effective training pipeline to enhance the reasoning capabilities of multi-modal large language models (MLLMs). Specifically, to create long and structured reasoning data without human labor, we design a two-step pipeline with a progressive strategy to generate sufficiently long and diverse reasoning paths and a multi-granularity assessment method to ensure data quality. We observe that directly supervising MLLMs with such long and complex reasoning data will not yield ideal reasoning ability. To tackle this problem, we design a multi-agent system consisting of a reasoning agent dedicated to performing long-chain reasoning and a summary agent trained to judge and summarize reasoning results. We further incorporate an iterative DPO algorithm to enhance the reasoning agent's generation stability and quality. Based on the popular LLaVA-NeXT model and our stronger base MLLM, we demonstrate significant performance gains across challenging multi-modal benchmarks requiring visual reasoning. Benefiting from our multi-agent system, Insight-V can also easily maintain or improve performance on perception-focused multi-modal tasks.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoPrep: Natural Language Question-Aware Data Preparation with a Multi-Agent Framework</title>
<link>https://arxiv.org/abs/2412.10422</link>
<guid>https://arxiv.org/abs/2412.10422</guid>
<content:encoded><![CDATA[
arXiv:2412.10422v3 Announce Type: replace 
Abstract: Answering natural language (NL) questions about tables, known as Tabular Question Answering (TQA), is crucial because it allows users to quickly and efficiently extract meaningful insights from structured data, effectively bridging the gap between human language and machine-readable formats. Many of these tables are derived from web sources or real-world scenarios, which require meticulous data preparation (or data prep) to ensure accurate responses. However, preparing such tables for NL questions introduces new requirements that extend beyond traditional data preparation. This question-aware data preparation involves specific tasks such as column derivation and filtering tailored to particular questions, as well as question-aware value normalization or conversion, highlighting the need for a more nuanced approach in this context. Because each of the above tasks is unique, a single model (or agent) may not perform effectively across all scenarios. In this paper, we propose AutoPrep, a large language model (LLM)-based multi-agent framework that leverages the strengths of multiple agents, each specialized in a certain type of data prep, ensuring more accurate and contextually relevant responses. Given an NL question over a table, AutoPrep performs data prep through three key components. Planner: Determines a logical plan, outlining a sequence of high-level operations. Programmer: Translates this logical plan into a physical plan by generating the corresponding low-level code. Executor: Executes the generated code to process the table. To support this multi-agent framework, we design a novel Chain-of-Clauses reasoning mechanism for high-level operation suggestion, and a tool-augmented method for low-level code generation...
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DriveGPT: Scaling Autoregressive Behavior Models for Driving</title>
<link>https://arxiv.org/abs/2412.14415</link>
<guid>https://arxiv.org/abs/2412.14415</guid>
<content:encoded><![CDATA[
arXiv:2412.14415v3 Announce Type: replace 
Abstract: We present DriveGPT, a scalable behavior model for autonomous driving. We model driving as a sequential decision-making task, and learn a transformer model to predict future agent states as tokens in an autoregressive fashion. We scale up our model parameters and training data by multiple orders of magnitude, enabling us to explore the scaling properties in terms of dataset size, model parameters, and compute. We evaluate DriveGPT across different scales in a planning task, through both quantitative metrics and qualitative examples, including closed-loop driving in complex real-world scenarios. In a separate prediction task, DriveGPT outperforms state-of-the-art baselines and exhibits improved performance by pretraining on a large-scale dataset, further validating the benefits of data scaling.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADAM: An AI Reasoning and Bioinformatics Model for Alzheimer's Disease Detection and Microbiome-Clinical Data Integration</title>
<link>https://arxiv.org/abs/2501.08324</link>
<guid>https://arxiv.org/abs/2501.08324</guid>
<content:encoded><![CDATA[
arXiv:2501.08324v2 Announce Type: replace 
Abstract: Alzheimer's Disease Analysis Model (ADAM) is a multi-agent reasoning large language model (LLM) framework designed to integrate and analyze multimodal data, including microbiome profiles, clinical datasets, and external knowledge bases, to enhance the understanding and classification of Alzheimer's disease (AD). By leveraging the agentic system with LLM, ADAM produces insights from diverse data sources and contextualizes the findings with literature-driven evidence. A comparative evaluation with XGBoost revealed a significantly improved mean F1 score and significantly reduced variance for ADAM, highlighting its robustness and consistency, particularly when utilizing human biological data. Although currently tailored for binary classification tasks with two data modalities, future iterations will aim to incorporate additional data types, such as neuroimaging and peripheral biomarkers, and expand them to predict disease progression, thereby broadening ADAM's scalability and applicability in AD research and diagnostic applications.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MELON: Provable Indirect Prompt Injection Defense via Masked Re-execution and Tool Comparison</title>
<link>https://arxiv.org/abs/2502.05174</link>
<guid>https://arxiv.org/abs/2502.05174</guid>
<content:encoded><![CDATA[
arXiv:2502.05174v2 Announce Type: replace 
Abstract: Recent research has explored that LLM agents are vulnerable to indirect prompt injection (IPI) attacks, where malicious tasks embedded in tool-retrieved information can redirect the agent to take unauthorized actions. Existing defenses against IPI have significant limitations: either require essential model training resources, lack effectiveness against sophisticated attacks, or harm the normal utilities. We present MELON (Masked re-Execution and TooL comparisON), a novel IPI defense. Our approach builds on the observation that under a successful attack, the agent's next action becomes less dependent on user tasks and more on malicious tasks. Following this, we design MELON to detect attacks by re-executing the agent's trajectory with a masked user prompt modified through a masking function. We identify an attack if the actions generated in the original and masked executions are similar. We also include three key designs to reduce the potential false positives and false negatives. Extensive evaluation on the IPI benchmark AgentDojo demonstrates that MELON outperforms SOTA defenses in both attack prevention and utility preservation. Moreover, we show that combining MELON with a SOTA prompt augmentation defense (denoted as MELON-Aug) further improves its performance. We also conduct a detailed ablation study to validate our key designs.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AT-Drone: Benchmarking Adaptive Teaming in Multi-Drone Pursuit</title>
<link>https://arxiv.org/abs/2502.09762</link>
<guid>https://arxiv.org/abs/2502.09762</guid>
<content:encoded><![CDATA[
arXiv:2502.09762v2 Announce Type: replace 
Abstract: Adaptive teaming-the capability of agents to effectively collaborate with unfamiliar teammates without prior coordination-is widely explored in virtual video games but overlooked in real-world multi-robot contexts. Yet, such adaptive collaboration is crucial for real-world applications, including border surveillance, search-and-rescue, and counter-terrorism operations. To address this gap, we introduce AT-Drone, the first dedicated benchmark explicitly designed to facilitate comprehensive training and evaluation of adaptive teaming strategies in multi-drone pursuit scenarios. AT-Drone makes the following key contributions: (1) An adaptable simulation environment configurator that enables intuitive and rapid setup of adaptive teaming multi-drone pursuit tasks, including four predefined pursuit environments. (2) A streamlined real-world deployment pipeline that seamlessly translates simulation insights into practical drone evaluations using edge devices and Crazyflie drones. (3) A novel algorithm zoo integrated with a distributed training framework, featuring diverse algorithms explicitly tailored, for the first time, to multi-pursuer and multi-evader settings. (4) Standardized evaluation protocols with newly designed unseen drone zoos, explicitly designed to rigorously assess the performance of adaptive teaming. Comprehensive experimental evaluations across four progressively challenging multi-drone pursuit scenarios confirm AT-Drone's effectiveness in advancing adaptive teaming research. Real-world drone experiments further validate its practical feasibility and utility for realistic robotic operations. Videos, code and weights are available at \url{https://sites.google.com/view/at-drone}.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MolGround: A Benchmark for Molecular Grounding</title>
<link>https://arxiv.org/abs/2503.23668</link>
<guid>https://arxiv.org/abs/2503.23668</guid>
<content:encoded><![CDATA[
arXiv:2503.23668v4 Announce Type: replace 
Abstract: Current molecular understanding approaches predominantly focus on the descriptive aspect of human perception, providing broad, topic-level insights. However, the referential aspect -- linking molecular concepts to specific structural components -- remains largely unexplored. To address this gap, we propose a molecular grounding benchmark designed to evaluate a model's referential abilities. We align molecular grounding with established conventions in NLP, cheminformatics, and molecular science, showcasing the potential of NLP techniques to advance molecular understanding within the AI for Science movement. Furthermore, we constructed the largest molecular understanding benchmark to date, comprising 117k QA pairs, and developed a multi-agent grounding prototype as proof of concept. This system outperforms existing models, including GPT-4o, and its grounding outputs have been integrated to enhance traditional tasks such as molecular captioning and ATC (Anatomical, Therapeutic, Chemical) classification.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management</title>
<link>https://arxiv.org/abs/2505.00018</link>
<guid>https://arxiv.org/abs/2505.00018</guid>
<content:encoded><![CDATA[
arXiv:2505.00018v1 Announce Type: new 
Abstract: This position paper critically surveys a broad spectrum of recent empirical developments on human-AI agents collaboration, highlighting both their technical achievements and persistent gaps. We observe a lack of a unifying theoretical framework that can coherently integrate these varied studies, especially when tackling open-ended, complex tasks. To address this, we propose a novel conceptual architecture: one that systematically interlinks the technical details of multi-agent coordination, knowledge management, cybernetic feedback loops, and higher-level control mechanisms. By mapping existing contributions, from symbolic AI techniques and connectionist LLM-based agents to hybrid organizational practices, onto this proposed framework (Hierarchical Exploration-Exploitation Net), our approach facilitates revision of legacy methods and inspires new work that fuses qualitative and quantitative paradigms. The paper's structure allows it to be read from any section, serving equally as a critical review of technical implementations and as a forward-looking reference for designing or extending human-AI symbioses. Together, these insights offer a stepping stone toward deeper co-evolution of human cognition and AI capability.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyMA-IEI-PPO: Exploration Incentive-Driven Multi-Agent DRL with Self-Adaptive Pruning for Vehicular Embodied AI Agent Twins Migration</title>
<link>https://arxiv.org/abs/2505.00055</link>
<guid>https://arxiv.org/abs/2505.00055</guid>
<content:encoded><![CDATA[
arXiv:2505.00055v1 Announce Type: new 
Abstract: Embodied Artificial Intelligence (EAI) addresses autonomous driving challenges in Vehicular Embodied AI Networks (VEANETs) through multi-modal perception, adaptive decision-making, and hardware-software co-scheduling. However, the computational demands of virtual services and the inherent mobility of autonomous vehicles (AVs) necessitate real-time migration of Vehicular Embodied Agent AI Twins (VEAATs) between resource-constrained Roadside Units (RSUs). This paper proposes a novel framework for efficient VEAAT migration in VEANETs, combining a multi-leader multi-follower (MLMF) Stackelberg game-theoretic incentive mechanism with a tiny multi-agent deep reinforcement learning (MADRL) algorithm. First, We propose an virtual immersive experience-driven utility model that captures AV-RSU dynamic interactions by integrating AVs' social influence, service complementarity and substitutability, and RSUs' resource allocation strategies to optimize VEAAT migration decisions. Second, to enhance training efficiency and enable efficient deployment on computation-constrained AVs while preserving exploration-exploitation performance, we propose TinyMA-IEI-PPO, a self-adaptive dynamic structured pruning algorithm that dynamically adjusts neuron importance based on agents' exploration incentives. Numerical results demonstrate that our approach achieves convergence comparable to baseline models and closely approximates the Stackelberg equilibrium.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoordField: Coordination Field for Agentic UAV Task Allocation In Low-altitude Urban Scenarios</title>
<link>https://arxiv.org/abs/2505.00091</link>
<guid>https://arxiv.org/abs/2505.00091</guid>
<content:encoded><![CDATA[
arXiv:2505.00091v1 Announce Type: new 
Abstract: With the increasing demand for heterogeneous Unmanned Aerial Vehicle (UAV) swarms to perform complex tasks in urban environments, system design now faces major challenges, including efficient semantic understanding, flexible task planning, and the ability to dynamically adjust coordination strategies in response to evolving environmental conditions and continuously changing task requirements. To address the limitations of existing approaches, this paper proposes coordination field agentic system for coordinating heterogeneous UAV swarms in complex urban scenarios. In this system, large language models (LLMs) is responsible for interpreting high-level human instructions and converting them into executable commands for the UAV swarms, such as patrol and target tracking. Subsequently, a Coordination field mechanism is proposed to guide UAV motion and task selection, enabling decentralized and adaptive allocation of emergent tasks. A total of 50 rounds of comparative testing were conducted across different models in a 2D simulation space to evaluate their performance. Experimental results demonstrate that the proposed system achieves superior performance in terms of task coverage, response time, and adaptability to dynamic changes.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning Policies for Underactuated Satellite Attitude Control</title>
<link>https://arxiv.org/abs/2505.00165</link>
<guid>https://arxiv.org/abs/2505.00165</guid>
<content:encoded><![CDATA[
arXiv:2505.00165v1 Announce Type: new 
Abstract: Autonomy is a key challenge for future space exploration endeavours. Deep Reinforcement Learning holds the promises for developing agents able to learn complex behaviours simply by interacting with their environment. This paper investigates the use of Reinforcement Learning for the satellite attitude control problem, namely the angular reorientation of a spacecraft with respect to an in- ertial frame of reference. In the proposed approach, a set of control policies are implemented as neural networks trained with a custom version of the Proximal Policy Optimization algorithm to maneuver a small satellite from a random starting angle to a given pointing target. In particular, we address the problem for two working conditions: the nominal case, in which all the actuators (a set of 3 reac- tion wheels) are working properly, and the underactuated case, where an actuator failure is simulated randomly along with one of the axes. We show that the agents learn to effectively perform large-angle slew maneuvers with fast convergence and industry-standard pointing accuracy. Furthermore, we test the proposed method on representative hardware, showing that by taking adequate measures controllers trained in simulation can perform well in real systems.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Adaptive Tuning of Assistive Exoskeletons Using Offline Reinforcement Learning: Challenges and Insights</title>
<link>https://arxiv.org/abs/2505.00201</link>
<guid>https://arxiv.org/abs/2505.00201</guid>
<content:encoded><![CDATA[
arXiv:2505.00201v1 Announce Type: new 
Abstract: Assistive exoskeletons have shown great potential in enhancing mobility for individuals with motor impairments, yet their effectiveness relies on precise parameter tuning for personalized assistance. In this study, we investigate the potential of offline reinforcement learning for optimizing effort thresholds in upper-limb assistive exoskeletons, aiming to reduce reliance on manual calibration. Specifically, we frame the problem as a multi-agent system where separate agents optimize biceps and triceps effort thresholds, enabling a more adaptive and data-driven approach to exoskeleton control. Mixed Q-Functionals (MQF) is employed to efficiently handle continuous action spaces while leveraging pre-collected data, thereby mitigating the risks associated with real-time exploration. Experiments were conducted using the MyoPro 2 exoskeleton across two distinct tasks involving horizontal and vertical arm movements. Our results indicate that the proposed approach can dynamically adjust threshold values based on learned patterns, potentially improving user interaction and control, though performance evaluation remains challenging due to dataset limitations.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2505.00212</link>
<guid>https://arxiv.org/abs/2505.00212</guid>
<content:encoded><![CDATA[
arXiv:2505.00212v1 Announce Type: new 
Abstract: Failure attribution in LLM multi-agent systems-identifying the agent and step responsible for task failures-provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems. To support this initiative, we introduce the Who&amp;When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps. Using the Who&amp;When, we develop and evaluate three automated failure attribution methods, summarizing their corresponding pros and cons. The best method achieves 53.5% accuracy in identifying failure-responsible agents but only 14.2% in pinpointing failure steps, with some methods performing below random. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area. Code and dataset are available at https://github.com/mingyin1/Agents_Failure_Attribution
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PSN Game: Game-theoretic Planning via a Player Selection Network</title>
<link>https://arxiv.org/abs/2505.00213</link>
<guid>https://arxiv.org/abs/2505.00213</guid>
<content:encoded><![CDATA[
arXiv:2505.00213v1 Announce Type: new 
Abstract: While game-theoretic planning frameworks are effective at modeling multi-agent interactions, they require solving optimization problems with hundreds or thousands of variables, resulting in long computation times that limit their use in large-scale, real-time systems. To address this issue, we propose PSN Game: a novel game-theoretic planning framework that reduces runtime by learning a Player Selection Network (PSN). A PSN outputs a player selection mask that distinguishes influential players from less relevant ones, enabling the ego player to solve a smaller, masked game involving only selected players. By reducing the number of variables in the optimization problem, PSN directly lowers computation time. The PSN Game framework is more flexible than existing player selection methods as it i) relies solely on observations of players' past trajectories, without requiring full state, control, or other game-specific information; and ii) requires no online parameter tuning. We train PSNs in an unsupervised manner using a differentiable dynamic game solver, with reference trajectories from full-player games guiding the learning. Experiments in both simulated scenarios and human trajectory datasets demonstrate that i) PSNs outperform baseline selection methods in trajectory smoothness and length, while maintaining comparable safety and achieving a 10x speedup in runtime; and ii) PSNs generalize effectively to real-world scenarios without fine-tuning. By selecting only the most relevant players for decision-making, PSNs offer a general mechanism for reducing planning complexity that can be seamlessly integrated into existing multi-agent planning frameworks.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Federation For Mixtures of Proprietary Agents with Black-Box Encoders</title>
<link>https://arxiv.org/abs/2505.00216</link>
<guid>https://arxiv.org/abs/2505.00216</guid>
<content:encoded><![CDATA[
arXiv:2505.00216v1 Announce Type: new 
Abstract: Most industry-standard generative AIs and feature encoders are proprietary, offering only black-box access: their outputs are observable, but their internal parameters and architectures remain hidden from the end-user. This black-box access is especially limiting when constructing mixture-of-expert type ensemble models since the user cannot optimize each proprietary AI's internal parameters. Our problem naturally lends itself to a non-competitive game-theoretic lens where each proprietary AI (agent) is inherently competing against the other AI agents, with this competition arising naturally due to their obliviousness of the AI's to their internal structure. In contrast, the user acts as a central planner trying to synchronize the ensemble of competing AIs.
  We show the existence of the unique Nash equilibrium in the online setting, which we even compute in closed-form by eliciting a feedback mechanism between any given time series and the sequence generated by each (proprietary) AI agent. Our solution is implemented as a decentralized, federated-learning algorithm in which each agent optimizes their structure locally on their machine without ever releasing any internal structure to the others. We obtain refined expressions for pre-trained models such as transformers, random feature models, and echo-state networks. Our ``proprietary federated learning'' algorithm is implemented on a range of real-world and synthetic time-series benchmarks. It achieves orders-of-magnitude improvements in predictive accuracy over natural benchmarks, of which there are surprisingly few due to this natural problem still being largely unexplored.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks</title>
<link>https://arxiv.org/abs/2505.00234</link>
<guid>https://arxiv.org/abs/2505.00234</guid>
<content:encoded><![CDATA[
arXiv:2505.00234v1 Announce Type: new 
Abstract: Many methods for improving Large Language Model (LLM) agents for sequential decision-making tasks depend on task-specific knowledge engineering--such as prompt tuning, curated in-context examples, or customized observation and action spaces. Using these approaches, agent performance improves with the quality or amount of knowledge engineering invested. Instead, we investigate how LLM agents can automatically improve their performance by learning in-context from their own successful experiences on similar tasks. Rather than relying on task-specific knowledge engineering, we focus on constructing and refining a database of self-generated examples. We demonstrate that even a naive accumulation of successful trajectories across training tasks boosts test performance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%)--matching the performance the initial agent achieves if allowed two to three attempts per task. We then introduce two extensions: (1) database-level selection through population-based training to identify high-performing example collections, and (2) exemplar-level selection that retains individual trajectories based on their empirical utility as in-context examples. These extensions further enhance performance, achieving 91% on ALFWorld--matching more complex approaches that employ task-specific components and prompts. Our results demonstrate that automatic trajectory database construction offers a compelling alternative to labor-intensive knowledge engineering.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Agentic Video Analytics Systems with Video Language Models</title>
<link>https://arxiv.org/abs/2505.00254</link>
<guid>https://arxiv.org/abs/2505.00254</guid>
<content:encoded><![CDATA[
arXiv:2505.00254v1 Announce Type: new 
Abstract: AI-driven video analytics has become increasingly pivotal across diverse domains. However, existing systems are often constrained to specific, predefined tasks, limiting their adaptability in open-ended analytical scenarios. The recent emergence of Video-Language Models (VLMs) as transformative technologies offers significant potential for enabling open-ended video understanding, reasoning, and analytics. Nevertheless, their limited context windows present challenges when processing ultra-long video content, which is prevalent in real-world applications. To address this, we introduce AVA, a VLM-powered system designed for open-ended, advanced video analytics. AVA incorporates two key innovations: (1) the near real-time construction of Event Knowledge Graphs (EKGs) for efficient indexing of long or continuous video streams, and (2) an agentic retrieval-generation mechanism that leverages EKGs to handle complex and diverse queries. Comprehensive evaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that AVA achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy, respectively, significantly surpassing existing VLM and video Retrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video analytics in ultra-long and open-world video scenarios, we introduce a new benchmark, AVA-100. This benchmark comprises 8 videos, each exceeding 10 hours in duration, along with 120 manually annotated, diverse, and complex question-answer pairs. On AVA-100, AVA achieves top-tier performance with an accuracy of 75.8%.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.00284</link>
<guid>https://arxiv.org/abs/2505.00284</guid>
<content:encoded><![CDATA[
arXiv:2505.00284v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have demonstrated significant potential for end-to-end autonomous driving. However, fully exploiting their capabilities for safe and reliable vehicle control remains an open research challenge. To systematically examine advances and limitations of VLMs in driving tasks, we introduce LightEMMA, a Lightweight End-to-End Multimodal Model for Autonomous driving. LightEMMA provides a unified, VLM-based autonomous driving framework without ad hoc customizations, enabling easy integration and evaluation of evolving state-of-the-art commercial and open-source models. We construct twelve autonomous driving agents using various VLMs and evaluate their performance on the nuScenes prediction task, comprehensively assessing metrics such as inference time, computational cost, and predictive accuracy. Illustrative examples highlight that, despite their strong scenario interpretation capabilities, VLMs' practical performance in autonomous driving tasks remains concerning, emphasizing the need for further improvements. The code is available at https://github.com/michigan-traffic-lab/LightEMMA.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Complexity of Minimum-Envy House Allocation Over Graphs</title>
<link>https://arxiv.org/abs/2505.00296</link>
<guid>https://arxiv.org/abs/2505.00296</guid>
<content:encoded><![CDATA[
arXiv:2505.00296v1 Announce Type: new 
Abstract: In this paper, we study a generalization of the House Allocation problem. In our problem, agents are represented by vertices of a graph $\GG_{\mathcal{A}} = (\AA, E_\AA)$, and each agent $a \in \AA$ is associated with a set of preferred houses $\PP_a \subseteq \HH$, where $\AA$ is the set of agents and $\HH$ is the set of houses. A house allocation is an injective function $\phi: \AA \rightarrow \HH$, and an agent $a$ envies a neighbour $a' \in N_{\GG_\AA}(a)$ under $\phi$ if $\phi(a) \notin \PP_a$ and $\phi(a') \in \PP_a$. We study two natural objectives: the first problem called \ohaa, aims to compute an allocation that minimizes the number of envious agents; the second problem called \ohaah aims to maximize, among all minimum-envy allocations, the number of agents who are assigned a house they prefer. These two objectives capture complementary notions of fairness and individual satisfaction.
  We design polynomial time algorithms for both problems for the variant when each agent prefers exactly one house. On the other hand, when the list of preferred houses for each agent has size at most $2$ then we show that both problems are \NP-hard even when the agent graph $\GG_\AA$ is a complete bipartite graph. We also show that both problems are \NP-hard even when the number $|\mathcal H|$ of houses is equal to the number $|\mathcal A|$ of agents. This is in contrast to the classical {\sc House Allocation} problem, where the problem is polynomial time solvable when $|\mathcal H| = |\mathcal A|$. The two problems are also \NP-hard when the agent graph has a small vertex cover. On the positive side, we design exact algorithms that exploit certain structural properties of $\GG_{\AA}$ such as sparsity, existence of balanced separators or existence of small-sized vertex covers, and perform better than the naive brute-force algorithm.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intelligent Task Scheduling for Microservices via A3C-Based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.00299</link>
<guid>https://arxiv.org/abs/2505.00299</guid>
<content:encoded><![CDATA[
arXiv:2505.00299v1 Announce Type: new 
Abstract: To address the challenges of high resource dynamism and intensive task concurrency in microservice systems, this paper proposes an adaptive resource scheduling method based on the A3C reinforcement learning algorithm. The scheduling problem is modeled as a Markov Decision Process, where policy and value networks are jointly optimized to enable fine-grained resource allocation under varying load conditions. The method incorporates an asynchronous multi-threaded learning mechanism, allowing multiple agents to perform parallel sampling and synchronize updates to the global network parameters. This design improves both policy convergence efficiency and model stability. In the experimental section, a real-world dataset is used to construct a scheduling scenario. The proposed method is compared with several typical approaches across multiple evaluation metrics, including task delay, scheduling success rate, resource utilization, and convergence speed. The results show that the proposed method delivers high scheduling performance and system stability in multi-task concurrent environments. It effectively alleviates the resource allocation bottlenecks faced by traditional methods under heavy load, demonstrating its practical value for intelligent scheduling in microservice systems.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surrogate modeling of Cellular-Potts Agent-Based Models as a segmentation task using the U-Net neural network architecture</title>
<link>https://arxiv.org/abs/2505.00316</link>
<guid>https://arxiv.org/abs/2505.00316</guid>
<content:encoded><![CDATA[
arXiv:2505.00316v1 Announce Type: new 
Abstract: The Cellular-Potts model is a powerful and ubiquitous framework for developing computational models for simulating complex multicellular biological systems. Cellular-Potts models (CPMs) are often computationally expensive due to the explicit modeling of interactions among large numbers of individual model agents and diffusive fields described by partial differential equations (PDEs). In this work, we develop a convolutional neural network (CNN) surrogate model using a U-Net architecture that accounts for periodic boundary conditions. We use this model to accelerate the evaluation of a mechanistic CPM previously used to investigate \textit{in vitro} vasculogenesis. The surrogate model was trained to predict 100 computational steps ahead (Monte-Carlo steps, MCS), accelerating simulation evaluations by a factor of 590 times compared to CPM code execution. Over multiple recursive evaluations, our model effectively captures the emergent behaviors demonstrated by the original Cellular-Potts model of such as vessel sprouting, extension and anastomosis, and contraction of vascular lacunae. This approach demonstrates the potential for deep learning to serve as efficient surrogate models for CPM simulations, enabling faster evaluation of computationally expensive CPM of biological processes at greater spatial and temporal scales.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI2-Active Safety: AI-enabled Interaction-aware Active Safety Analysis with Vehicle Dynamics</title>
<link>https://arxiv.org/abs/2505.00322</link>
<guid>https://arxiv.org/abs/2505.00322</guid>
<content:encoded><![CDATA[
arXiv:2505.00322v1 Announce Type: new 
Abstract: This paper introduces an AI-enabled, interaction-aware active safety analysis framework that accounts for groupwise vehicle interactions. Specifically, the framework employs a bicycle model-augmented with road gradient considerations-to accurately capture vehicle dynamics. In parallel, a hypergraph-based AI model is developed to predict probabilistic trajectories of ambient traffic. By integrating these two components, the framework derives vehicle intra-spacing over a 3D road surface as the solution of a stochastic ordinary differential equation, yielding high-fidelity surrogate safety measures such as time-to-collision (TTC). To demonstrate its effectiveness, the framework is analyzed using stochastic numerical methods comprising 4th-order Runge-Kutta integration and AI inference, generating probability-weighted high-fidelity TTC (HF-TTC) distributions that reflect complex multi-agent maneuvers and behavioral uncertainties. Evaluated with HF-TTC against traditional constant-velocity TTC and non-interaction-aware approaches on highway datasets, the proposed framework offers a systematic methodology for active safety analysis with enhanced potential for improving safety perception in complex traffic environments.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScaleTrack: Scaling and back-tracking Automated GUI Agents</title>
<link>https://arxiv.org/abs/2505.00416</link>
<guid>https://arxiv.org/abs/2505.00416</guid>
<content:encoded><![CDATA[
arXiv:2505.00416v1 Announce Type: new 
Abstract: Automated GUI agents aims to facilitate user interaction by automatically performing complex tasks in digital environments, such as web, mobile, desktop devices. It receives textual task instruction and GUI description to generate executable actions (\emph{e.g.}, click) and operation boxes step by step. Training a GUI agent mainly involves grounding and planning stages, in which the GUI grounding focuses on finding the execution coordinates according to the task, while the planning stage aims to predict the next action based on historical actions. However, previous work suffers from the limitations of insufficient training data for GUI grounding, as well as the ignorance of backtracking historical behaviors for GUI planning. To handle the above challenges, we propose ScaleTrack, a training framework by scaling grounding and backtracking planning for automated GUI agents. We carefully collected GUI samples of different synthesis criterions from a wide range of sources, and unified them into the same template for training GUI grounding models. Moreover, we design a novel training strategy that predicts the next action from the current GUI image, while also backtracking the historical actions that led to the GUI image. In this way, ScaleTrack explains the correspondence between GUI images and actions, which effectively describes the evolution rules of the GUI environment. Extensive experimental results demonstrate the effectiveness of ScaleTrack. Data and code will be available at url.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UserCentrix: An Agentic Memory-augmented AI Framework for Smart Spaces</title>
<link>https://arxiv.org/abs/2505.00472</link>
<guid>https://arxiv.org/abs/2505.00472</guid>
<content:encoded><![CDATA[
arXiv:2505.00472v1 Announce Type: new 
Abstract: Agentic AI, with its autonomous and proactive decision-making, has transformed smart environments. By integrating Generative AI (GenAI) and multi-agent systems, modern AI frameworks can dynamically adapt to user preferences, optimize data management, and improve resource allocation. This paper introduces UserCentrix, an agentic memory-augmented AI framework designed to enhance smart spaces through dynamic, context-aware decision-making. This framework integrates personalized Large Language Model (LLM) agents that leverage user preferences and LLM memory management to deliver proactive and adaptive assistance. Furthermore, it incorporates a hybrid hierarchical control system, balancing centralized and distributed processing to optimize real-time responsiveness while maintaining global situational awareness. UserCentrix achieves resource-efficient AI interactions by embedding memory-augmented reasoning, cooperative agent negotiation, and adaptive orchestration strategies. Our key contributions include (i) a self-organizing framework with proactive scaling based on task urgency, (ii) a Value of Information (VoI)-driven decision-making process, (iii) a meta-reasoning personal LLM agent, and (iv) an intelligent multi-agent coordination system for seamless environment adaptation. Experimental results across various models confirm the effectiveness of our approach in enhancing response accuracy, system efficiency, and computational resource management in real-world application.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational OOD State Correction for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.00503</link>
<guid>https://arxiv.org/abs/2505.00503</guid>
<content:encoded><![CDATA[
arXiv:2505.00503v1 Announce Type: new 
Abstract: The performance of Offline reinforcement learning is significantly impacted by the issue of state distributional shift, and out-of-distribution (OOD) state correction is a popular approach to address this problem. In this paper, we propose a novel method named Density-Aware Safety Perception (DASP) for OOD state correction. Specifically, our method encourages the agent to prioritize actions that lead to outcomes with higher data density, thereby promoting its operation within or the return to in-distribution (safe) regions. To achieve this, we optimize the objective within a variational framework that concurrently considers both the potential outcomes of decision-making and their density, thus providing crucial contextual information for safe decision-making. Finally, we validate the effectiveness and feasibility of our proposed method through extensive experimental evaluations on the offline MuJoCo and AntMaze suites.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety-Critical Traffic Simulation with Guided Latent Diffusion Model</title>
<link>https://arxiv.org/abs/2505.00515</link>
<guid>https://arxiv.org/abs/2505.00515</guid>
<content:encoded><![CDATA[
arXiv:2505.00515v1 Announce Type: new 
Abstract: Safety-critical traffic simulation plays a crucial role in evaluating autonomous driving systems under rare and challenging scenarios. However, existing approaches often generate unrealistic scenarios due to insufficient consideration of physical plausibility and suffer from low generation efficiency. To address these limitations, we propose a guided latent diffusion model (LDM) capable of generating physically realistic and adversarial safety-critical traffic scenarios. Specifically, our model employs a graph-based variational autoencoder (VAE) to learn a compact latent space that captures complex multi-agent interactions while improving computational efficiency. Within this latent space, the diffusion model performs the denoising process to produce realistic trajectories. To enable controllable and adversarial scenario generation, we introduce novel guidance objectives that drive the diffusion process toward producing adversarial and behaviorally realistic driving behaviors. Furthermore, we develop a sample selection module based on physical feasibility checks to further enhance the physical plausibility of the generated scenarios. Extensive experiments on the nuScenes dataset demonstrate that our method achieves superior adversarial effectiveness and generation efficiency compared to existing baselines while maintaining a high level of realism. Our work provides an effective tool for realistic safety-critical scenario simulation, paving the way for more robust evaluation of autonomous driving systems.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence of Roles in Robotic Teams with Model Sharing and Limited Communication</title>
<link>https://arxiv.org/abs/2505.00540</link>
<guid>https://arxiv.org/abs/2505.00540</guid>
<content:encoded><![CDATA[
arXiv:2505.00540v1 Announce Type: new 
Abstract: We present a reinforcement learning strategy for use in multi-agent foraging systems in which the learning is centralised to a single agent and its model is periodically disseminated among the population of non-learning agents. In a domain where multi-agent reinforcement learning (MARL) is the common approach, this approach aims to significantly reduce the computational and energy demands compared to approaches such as MARL and centralised learning models. By developing high performing foraging agents, these approaches can be translated into real-world applications such as logistics, environmental monitoring, and autonomous exploration. A reward function was incorporated into this approach that promotes role development among agents, without explicit directives. This led to the differentiation of behaviours among the agents. The implicit encouragement of role differentiation allows for dynamic actions in which agents can alter roles dependent on their interactions with the environment without the need for explicit communication between agents.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directly Forecasting Belief for Reinforcement Learning with Delays</title>
<link>https://arxiv.org/abs/2505.00546</link>
<guid>https://arxiv.org/abs/2505.00546</guid>
<content:encoded><![CDATA[
arXiv:2505.00546v1 Announce Type: new 
Abstract: Reinforcement learning (RL) with delays is challenging as sensory perceptions lag behind the actual events: the RL agent needs to estimate the real state of its environment based on past observations. State-of-the-art (SOTA) methods typically employ recursive, step-by-step forecasting of states. This can cause the accumulation of compounding errors. To tackle this problem, our novel belief estimation method, named Directly Forecasting Belief Transformer (DFBT), directly forecasts states from observations without incrementally estimating intermediate states step-by-step. We theoretically demonstrate that DFBT greatly reduces compounding errors of existing recursively forecasting methods, yielding stronger performance guarantees. In experiments with D4RL offline datasets, DFBT reduces compounding errors with remarkable prediction accuracy. DFBT's capability to forecast state sequences also facilitates multi-step bootstrapping, thus greatly improving learning efficiency. On the MuJoCo benchmark, our DFBT-based method substantially outperforms SOTA baselines.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParkDiffusion: Heterogeneous Multi-Agent Multi-Modal Trajectory Prediction for Automated Parking using Diffusion Models</title>
<link>https://arxiv.org/abs/2505.00586</link>
<guid>https://arxiv.org/abs/2505.00586</guid>
<content:encoded><![CDATA[
arXiv:2505.00586v1 Announce Type: new 
Abstract: Automated parking is a critical feature of Advanced Driver Assistance Systems (ADAS), where accurate trajectory prediction is essential to bridge perception and planning modules. Despite its significance, research in this domain remains relatively limited, with most existing studies concentrating on single-modal trajectory prediction of vehicles. In this work, we propose ParkDiffusion, a novel approach that predicts the trajectories of both vehicles and pedestrians in automated parking scenarios. ParkDiffusion employs diffusion models to capture the inherent uncertainty and multi-modality of future trajectories, incorporating several key innovations. First, we propose a dual map encoder that processes soft semantic cues and hard geometric constraints using a two-step cross-attention mechanism. Second, we introduce an adaptive agent type embedding module, which dynamically conditions the prediction process on the distinct characteristics of vehicles and pedestrians. Third, to ensure kinematic feasibility, our model outputs control signals that are subsequently used within a kinematic framework to generate physically feasible trajectories. We evaluate ParkDiffusion on the Dragon Lake Parking (DLP) dataset and the Intersections Drone (inD) dataset. Our work establishes a new baseline for heterogeneous trajectory prediction in parking scenarios, outperforming existing methods by a considerable margin.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Finite-State Controller Based Offline Solver for Deterministic POMDPs</title>
<link>https://arxiv.org/abs/2505.00596</link>
<guid>https://arxiv.org/abs/2505.00596</guid>
<content:encoded><![CDATA[
arXiv:2505.00596v1 Announce Type: new 
Abstract: Deterministic partially observable Markov decision processes (DetPOMDPs) often arise in planning problems where the agent is uncertain about its environmental state but can act and observe deterministically. In this paper, we propose DetMCVI, an adaptation of the Monte Carlo Value Iteration (MCVI) algorithm for DetPOMDPs, which builds policies in the form of finite-state controllers (FSCs). DetMCVI solves large problems with a high success rate, outperforming existing baselines for DetPOMDPs. We also verify the performance of the algorithm in a real-world mobile robot forest mapping scenario.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions</title>
<link>https://arxiv.org/abs/2505.00675</link>
<guid>https://arxiv.org/abs/2505.00675</guid>
<content:encoded><![CDATA[
arXiv:2505.00675v1 Announce Type: new 
Abstract: Memory is a fundamental component of AI systems, underpinning large language models (LLMs) based agents. While prior surveys have focused on memory applications with LLMs, they often overlook the atomic operations that underlie memory dynamics. In this survey, we first categorize memory representations into parametric, contextual structured, and contextual unstructured and then introduce six fundamental memory operations: Consolidation, Updating, Indexing, Forgetting, Retrieval, and Compression. We systematically map these operations to the most relevant research topics across long-term, long-context, parametric modification, and multi-source memory. By reframing memory systems through the lens of atomic operations and representation types, this survey provides a structured and dynamic perspective on research, benchmark datasets, and tools related to memory in AI, clarifying the functional interplay in LLMs based agents while outlining promising directions for future research\footnote{The paper list, datasets, methods and tools are available at \href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\_Memory\_in\_AI}.}.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Test-time Scaling for GUI Agent Grounding</title>
<link>https://arxiv.org/abs/2505.00684</link>
<guid>https://arxiv.org/abs/2505.00684</guid>
<content:encoded><![CDATA[
arXiv:2505.00684v1 Announce Type: new 
Abstract: We introduce RegionFocus, a visual test-time scaling approach for Vision Language Model Agents. Understanding webpages is challenging due to the visual complexity of GUI images and the large number of interface elements, making accurate action selection difficult. Our approach dynamically zooms in on relevant regions, reducing background clutter and improving grounding accuracy. To support this process, we propose an image-as-map mechanism that visualizes key landmarks at each step, providing a transparent action record and enables the agent to effectively choose among action candidates. Even with a simple region selection strategy, we observe significant performance gains of 28+\% on Screenspot-pro and 24+\% on WebVoyager benchmarks on top of two state-of-the-art open vision language model agents, UI-TARS and Qwen2.5-VL, highlighting the effectiveness of visual test-time scaling in interactive settings. We achieve a new state-of-the-art grounding performance of 61.6\% on the ScreenSpot-Pro benchmark by applying RegionFocus to a Qwen2.5-VL-72B model. Our code will be released publicly at https://github.com/tiangeluo/RegionFocus.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Autonomous Micromobility through Scalable Urban Simulation</title>
<link>https://arxiv.org/abs/2505.00690</link>
<guid>https://arxiv.org/abs/2505.00690</guid>
<content:encoded><![CDATA[
arXiv:2505.00690v1 Announce Type: new 
Abstract: Micromobility, which utilizes lightweight mobile machines moving in urban public spaces, such as delivery robots and mobility scooters, emerges as a promising alternative to vehicular mobility. Current micromobility depends mostly on human manual operation (in-person or remote control), which raises safety and efficiency concerns when navigating busy urban environments full of unpredictable obstacles and pedestrians. Assisting humans with AI agents in maneuvering micromobility devices presents a viable solution for enhancing safety and efficiency. In this work, we present a scalable urban simulation solution to advance autonomous micromobility. First, we build URBAN-SIM - a high-performance robot learning platform for large-scale training of embodied agents in interactive urban scenes. URBAN-SIM contains three critical modules: Hierarchical Urban Generation pipeline, Interactive Dynamics Generation strategy, and Asynchronous Scene Sampling scheme, to improve the diversity, realism, and efficiency of robot learning in simulation. Then, we propose URBAN-BENCH - a suite of essential tasks and benchmarks to gauge various capabilities of the AI agents in achieving autonomous micromobility. URBAN-BENCH includes eight tasks based on three core skills of the agents: Urban Locomotion, Urban Navigation, and Urban Traverse. We evaluate four robots with heterogeneous embodiments, such as the wheeled and legged robots, across these tasks. Experiments on diverse terrains and urban structures reveal each robot's strengths and limitations.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sim-Anchored Learning for On-the-Fly Adaptation</title>
<link>https://arxiv.org/abs/2301.06987</link>
<guid>https://arxiv.org/abs/2301.06987</guid>
<content:encoded><![CDATA[
arXiv:2301.06987v3 Announce Type: replace 
Abstract: Fine-tuning simulation-trained RL agents with real-world data often degrades crucial behaviors due to limited or skewed data distributions. We argue that designer priorities exist not just in reward functions, but also in simulation design choices like task selection and state initialization. When adapting to real-world data, agents can experience catastrophic forgetting in important but underrepresented scenarios. We propose framing live-adaptation as a multi-objective optimization problem, where policy objectives must be satisfied both in simulation and reality. Our approach leverages critics from simulation as "anchors for design intent" (anchor critics). By jointly optimizing policies against both anchor critics and critics trained on real-world experience, our method enables adaptation while preserving prioritized behaviors from simulation. Evaluations demonstrate robust behavior retention in sim-to-sim benchmarks and a sim-to-real scenario with a racing quadrotor, allowing for power consumption reductions of up to 50% without control loss. We also contribute SwaNNFlight, an open-source firmware for enabling live adaptation on similar robotic platforms.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning An Active Inference Model of Driver Perception and Control: Application to Vehicle Car-Following</title>
<link>https://arxiv.org/abs/2303.15201</link>
<guid>https://arxiv.org/abs/2303.15201</guid>
<content:encoded><![CDATA[
arXiv:2303.15201v2 Announce Type: replace 
Abstract: In this paper we introduce a general estimation methodology for learning a model of human perception and control in a sensorimotor control task based upon a finite set of demonstrations. The model's structure consists of i the agent's internal representation of how the environment and associated observations evolve as a result of control actions and ii the agent's preferences over observable outcomes. We consider a model's structure specification consistent with active inference, a theory of human perception and behavior from cognitive science. According to active inference, the agent acts upon the world so as to minimize surprise defined as a measure of the extent to which an agent's current sensory observations differ from its preferred sensory observations. We propose a bi-level optimization approach to estimation which relies on a structural assumption on prior distributions that parameterize the statistical accuracy of the human agent's model of the environment. To illustrate the proposed methodology, we present the estimation of a model for car-following behavior based upon a naturalistic dataset. Overall, the results indicate that learning active inference models of human perception and control from data is a promising alternative to black-box models of driving.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMM-based DEX on the XRP Ledger</title>
<link>https://arxiv.org/abs/2312.13749</link>
<guid>https://arxiv.org/abs/2312.13749</guid>
<content:encoded><![CDATA[
arXiv:2312.13749v4 Announce Type: replace 
Abstract: Automated Market Maker (AMM)-based Decentralized Exchanges (DEXs) are crucial in Decentralized Finance (DeFi), but Ethereum implementations suffer from high transaction costs and price synchronization challenges. To address these limitations, we compare the XRP Ledger (XRPL)-AMM-Decentralized Exchange (DEX), a protocol-level implementation, against a Generic AMM-based DEX (G-AMM-DEX) on Ethereum, akin to Uniswap's V2 AMM implementation, through agent-based simulations using real market data and multiple volatility scenarios generated via Geometric Brownian Motion (GBM). Results demonstrate that the XRPL-AMM-DEX achieves superior price synchronization, reduced slippage, and improved returns due to XRPL's lower fees and shorter block times, with benefits amplifying during market volatility. The integrated Continuous Auction Mechanism (CAM) further mitigates impermanent loss by redistributing arbitrage value to Liquidity Providers (LPs). To the best of our knowledge, this study represents the first comparative analysis between protocol-level and smart contract AMM-based DEX implementations and the first agent-based simulation validating theoretical auction mechanisms for AMM-based DEXs.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts</title>
<link>https://arxiv.org/abs/2405.11804</link>
<guid>https://arxiv.org/abs/2405.11804</guid>
<content:encoded><![CDATA[
arXiv:2405.11804v2 Announce Type: replace 
Abstract: Literary translation remains one of the most challenging frontiers in machine translation due to the complexity of capturing figurative language, cultural nuances, and unique stylistic elements. In this work, we introduce TransAgents, a novel multi-agent framework that simulates the roles and collaborative practices of a human translation company, including a CEO, Senior Editor, Junior Editor, Translator, Localization Specialist, and Proofreader. The translation process is divided into two stages: a preparation stage where the team is assembled and comprehensive translation guidelines are drafted, and an execution stage that involves sequential translation, localization, proofreading, and a final quality check. Furthermore, we propose two innovative evaluation strategies: Monolingual Human Preference (MHP), which evaluates translations based solely on target language quality and cultural appropriateness, and Bilingual LLM Preference (BLP), which leverages large language models like GPT-4} for direct text comparison. Although TransAgents achieves lower d-BLEU scores, due to the limited diversity of references, its translations are significantly better than those of other baselines and are preferred by both human evaluators and LLMs over traditional human references and GPT-4} translations. Our findings highlight the potential of multi-agent collaboration in enhancing translation quality, particularly for longer texts.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Achieving Human Level Competitive Robot Table Tennis</title>
<link>https://arxiv.org/abs/2408.03906</link>
<guid>https://arxiv.org/abs/2408.03906</guid>
<content:encoded><![CDATA[
arXiv:2408.03906v3 Announce Type: replace 
Abstract: Achieving human-level speed and performance on real world tasks is a north star for the robotics research community. This work takes a step towards that goal and presents the first learned robot agent that reaches amateur human-level performance in competitive table tennis. Table tennis is a physically demanding sport which requires human players to undergo years of training to achieve an advanced level of proficiency. In this paper, we contribute (1) a hierarchical and modular policy architecture consisting of (i) low level controllers with their detailed skill descriptors which model the agent's capabilities and help to bridge the sim-to-real gap and (ii) a high level controller that chooses the low level skills, (2) techniques for enabling zero-shot sim-to-real including an iterative approach to defining the task distribution that is grounded in the real-world and defines an automatic curriculum, and (3) real time adaptation to unseen opponents. Policy performance was assessed through 29 robot vs. human matches of which the robot won 45% (13/29). All humans were unseen players and their skill level varied from beginner to tournament level. Whilst the robot lost all matches vs. the most advanced players it won 100% matches vs. beginners and 55% matches vs. intermediate players, demonstrating solidly amateur human-level performance. Videos of the matches can be viewed at https://sites.google.com/view/competitive-robot-table-tennis
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chemputer and Chemputation - A Universal Chemical Compound Synthesis Machine</title>
<link>https://arxiv.org/abs/2408.09171</link>
<guid>https://arxiv.org/abs/2408.09171</guid>
<content:encoded><![CDATA[
arXiv:2408.09171v2 Announce Type: replace 
Abstract: Chemputation, the execution of code controlled reaction pathways on universally reconfigurable hardware, offers a route to treating chemical synthesis as a formally computable i.e. chemputable process to produce chemical compounds or molecules. Here I present a proof that a chemputer endowed with a finite, but extensible reagent, catalyst, process condition sets, and a chempiler that maps reaction graphs to hardware graphs, and dynamic error correction handles, is universal for the synthesis of any stable, isolable molecule that respects conservation of matter and finite reaction time and can be produced in detectable amounts. In developing this proof, I also expanded the internationally accepted definition of a molecule, by requiring that the molecule must be reachable in an analytically accessible amount using concepts from assembly theory. This shows that error correction is a vital requirement for chemputation, and I formalise the universal chemical synthesis theorem, demonstrate a full worked example, and explore the practical limits imposed by vessel count and sensing bandwidth. In doing so, I show that chemical reactions are not implicit blackbox functions but emergent graph operations or chemical transformations over Reagents (R) x Process (P) x Catalyst (K). The role of universally configurable hardware is also highlighted, with the introduction of a chempiling function that translates synthesis pathways into executable hardware configurations.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Covariate Shift in Imitation Learning for Autonomous Vehicles Using Latent Space Generative World Models</title>
<link>https://arxiv.org/abs/2409.16663</link>
<guid>https://arxiv.org/abs/2409.16663</guid>
<content:encoded><![CDATA[
arXiv:2409.16663v4 Announce Type: replace 
Abstract: We propose the use of latent space generative world models to address the covariate shift problem in autonomous driving. A world model is a neural network capable of predicting an agent's next state given past states and actions. By leveraging a world model during training, the driving policy effectively mitigates covariate shift without requiring an excessive amount of training data. During end-to-end training, our policy learns how to recover from errors by aligning with states observed in human demonstrations, so that at runtime it can recover from perturbations outside the training distribution. Additionally, we introduce a novel transformer-based perception encoder that employs multi-view cross-attention and a learned scene query. We present qualitative and quantitative results, demonstrating significant improvements upon prior state of the art in closed-loop testing in the CARLA simulator, as well as showing the ability to handle perturbations in both CARLA and NVIDIA's DRIVE Sim.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Scientific Discovery</title>
<link>https://arxiv.org/abs/2411.11672</link>
<guid>https://arxiv.org/abs/2411.11672</guid>
<content:encoded><![CDATA[
arXiv:2411.11672v2 Announce Type: replace 
Abstract: Rooted in the explosion of deep learning over the past decade, this thesis spans from AlphaGo to ChatGPT to empirically examine the fundamental concepts needed to realize the vision of an artificial scientist: a machine with the capacity to autonomously generate original research and contribute to the expansion of human knowledge. The investigation begins with Olivaw, an AlphaGo Zero-like agent that discovers Othello knowledge from scratch but is unable to communicate it. This realization leads to the development of the Explanatory Learning (EL) framework, a formalization of the problem faced by a scientist when trying to explain a new phenomenon to their peers. The effective EL prescriptions allow us to crack Zendo, a popular board game simulating the scientific endeavor. This success comes with a fundamental insight: an artificial scientist must develop its own interpretation of the language used to explain its findings, and not rely on a rigid existing interpreter. Questioning the very process of learning an interpreter, we turn our attention to the inner functioning of modern multimodal models. This culminates in a simple idea to build CLIP-like models where interpretation and perception are explicitly disentangled: a cost-effective approach that couples two unimodal models using little multimodal data and no further training. Finally, we discuss what ChatGPT and its siblings are still missing to become artificial scientists, and introduce the Big-Bench Symbol Interpretation Task, a benchmark about interpreting Zendo-like explanations that sees LLMs going no further than random chance while being instead fully solved by humans.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PPT: Pretraining with Pseudo-Labeled Trajectories for Motion Forecasting</title>
<link>https://arxiv.org/abs/2412.06491</link>
<guid>https://arxiv.org/abs/2412.06491</guid>
<content:encoded><![CDATA[
arXiv:2412.06491v2 Announce Type: replace 
Abstract: Accurately predicting how agents move in dynamic scenes is essential for safe autonomous driving. State-of-the-art motion forecasting models rely on large curated datasets with manually annotated or heavily post-processed trajectories. However, building these datasets is costly, generally manual, hard to scale, and lacks reproducibility. They also introduce domain gaps that limit generalization across environments. We introduce PPT (Pretraining with Pseudo-labeled Trajectories), a simple and scalable alternative that uses unprocessed and diverse trajectories automatically generated from off-the-shelf 3D detectors and tracking. Unlike traditional pipelines aiming for clean, single-label annotations, PPT embraces noise and diversity as useful signals for learning robust representations. With optional finetuning on a small amount of labeled data, models pretrained with PPT achieve strong performance across standard benchmarks particularly in low-data regimes, and in cross-domain, end-to-end and multi-class settings. PPT is easy to implement and improves generalization in motion forecasting. Code and data will be released upon acceptance.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real AI Agents with Fake Memories: Fatal Context Manipulation Attacks on Web3 Agents</title>
<link>https://arxiv.org/abs/2503.16248</link>
<guid>https://arxiv.org/abs/2503.16248</guid>
<content:encoded><![CDATA[
arXiv:2503.16248v2 Announce Type: replace 
Abstract: The integration of AI agents with Web3 ecosystems harnesses their complementary potential for autonomy and openness yet also introduces underexplored security risks, as these agents dynamically interact with financial protocols and immutable smart contracts. This paper investigates the vulnerabilities of AI agents within blockchain-based financial ecosystems when exposed to adversarial threats in real-world scenarios. We introduce the concept of context manipulation, a comprehensive attack vector that exploits unprotected context surfaces, including input channels, memory modules, and external data feeds.
  Through empirical analysis of ElizaOS, a decentralized AI agent framework for automated Web3 operations, we demonstrate how adversaries can manipulate context by injecting malicious instructions into prompts or historical interaction records, leading to unintended asset transfers and protocol violations which could be financially devastating.
  To quantify these vulnerabilities, we design CrAIBench, a Web3 domain-specific benchmark that evaluates the robustness of AI agents against context manipulation attacks across 150+ realistic blockchain tasks, including token transfers, trading, bridges and cross-chain interactions and 500+ attack test cases using context manipulation. We systematically assess attack and defense strategies, analyzing factors like the influence of security prompts, reasoning models, and the effectiveness of alignment techniques.
  Our findings show that prompt-based defenses are insufficient when adversaries corrupt stored context, achieving significant attack success rates despite these defenses. Fine-tuning-based defenses offer a more robust alternative, substantially reducing attack success rates while preserving utility on single-step tasks. This research highlights the urgent need to develop AI agents that are both secure and fiduciarily responsible.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Deep Reinforcement Learning and Motion Planning for Model-Free Navigation in Cluttered Environments</title>
<link>https://arxiv.org/abs/2504.07283</link>
<guid>https://arxiv.org/abs/2504.07283</guid>
<content:encoded><![CDATA[
arXiv:2504.07283v2 Announce Type: replace 
Abstract: Deep Reinforcement Learning (DRL) has emerged as a powerful model-free paradigm for learning optimal policies. However, in navigation tasks with cluttered environments, DRL methods often suffer from insufficient exploration, especially under sparse rewards or complex dynamics with system disturbances. To address this challenge, we bridge general graph-based motion planning with DRL, enabling agents to explore cluttered spaces more effectively and achieve desired navigation performance. Specifically, we design a dense reward function grounded in a graph structure that spans the entire state space. This graph provides rich guidance, steering the agent toward optimal strategies. We validate our approach in challenging environments, demonstrating substantial improvements in exploration efficiency and task success rates.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model</title>
<link>https://arxiv.org/abs/2504.21024</link>
<guid>https://arxiv.org/abs/2504.21024</guid>
<content:encoded><![CDATA[
arXiv:2504.21024v1 Announce Type: new 
Abstract: Agent self-improvement, where the backbone Large Language Model (LLM) of the agent are trained on trajectories sampled autonomously based on their own policies, has emerged as a promising approach for enhancing performance. Recent advancements, particularly in web environments, face a critical limitation: their performance will reach a stagnation point during autonomous learning cycles, hindering further improvement. We argue that this stems from limited exploration of the web environment and insufficient exploitation of pre-trained web knowledge in LLMs. To improve the performance of self-improvement, we propose a novel framework that introduces a co-evolving World Model LLM. This world model predicts the next observation based on the current observation and action within the web environment. Leveraging LLMs' pretrained knowledge of abundant web content, the World Model serves dual roles: (1) as a virtual web server generating self-instructed training data to continuously refine the agent's policy, and (2) as an imagination engine during inference, enabling look-ahead simulation to guide action selection for the agent LLM. Experiments in real-world web environments (Mind2Web-Live, WebVoyager, and GAIA-web) show a 10% performance gain over existing self-evolving agents, demonstrating the efficacy and generalizability of our approach, without using any distillation from more powerful close-sourced models. Our work establishes the necessity of integrating world models into autonomous agent frameworks to unlock sustained adaptability.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Multi-Agent Systems Through Model Context Protocol: Architecture, Implementation, and Applications</title>
<link>https://arxiv.org/abs/2504.21030</link>
<guid>https://arxiv.org/abs/2504.21030</guid>
<content:encoded><![CDATA[
arXiv:2504.21030v1 Announce Type: new 
Abstract: Multi-agent systems represent a significant advancement in artificial intelligence, enabling complex problem-solving through coordinated specialized agents. However, these systems face fundamental challenges in context management, coordination efficiency, and scalable operation. This paper introduces a comprehensive framework for advancing multi-agent systems through Model Context Protocol (MCP), addressing these challenges through standardized context sharing and coordination mechanisms. We extend previous work on AI agent architectures by developing a unified theoretical foundation, advanced context management techniques, and scalable coordination patterns. Through detailed implementation case studies across enterprise knowledge management, collaborative research, and distributed problem-solving domains, we demonstrate significant performance improvements compared to traditional approaches. Our evaluation methodology provides a systematic assessment framework with benchmark tasks and datasets specifically designed for multi-agent systems. We identify current limitations, emerging research opportunities, and potential transformative applications across industries. This work contributes to the evolution of more capable, collaborative, and context-aware artificial intelligence systems that can effectively address complex real-world challenges.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAGA: A Security Architecture for Governing AI Agentic Systems</title>
<link>https://arxiv.org/abs/2504.21034</link>
<guid>https://arxiv.org/abs/2504.21034</guid>
<content:encoded><![CDATA[
arXiv:2504.21034v1 Announce Type: new 
Abstract: Large Language Model (LLM)-based agents increasingly interact, collaborate, and delegate tasks to one another autonomously with minimal human interaction. Industry guidelines for agentic system governance emphasize the need for users to maintain comprehensive control over their agents, mitigating potential damage from malicious agents. Several proposed agentic system designs address agent identity, authorization, and delegation, but remain purely theoretical, without concrete implementation and evaluation. Most importantly, they do not provide user-controlled agent management. To address this gap, we propose SAGA, a Security Architecture for Governing Agentic systems, that offers user oversight over their agents' lifecycle. In our design, users register their agents with a central entity, the Provider, that maintains agents contact information, user-defined access control policies, and helps agents enforce these policies on inter-agent communication. We introduce a cryptographic mechanism for deriving access control tokens, that offers fine-grained control over an agent's interaction with other agents, balancing security and performance consideration. We evaluate SAGA on several agentic tasks, using agents in different geolocations, and multiple on-device and cloud LLMs, demonstrating minimal performance overhead with no impact on underlying task utility in a wide range of conditions. Our architecture enables secure and trustworthy deployment of autonomous agents, accelerating the responsible adoption of this technology in sensitive environments.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Parking Trajectory Generation Using Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.21071</link>
<guid>https://arxiv.org/abs/2504.21071</guid>
<content:encoded><![CDATA[
arXiv:2504.21071v1 Announce Type: new 
Abstract: Autonomous parking is a key technology in modern autonomous driving systems, requiring high precision, strong adaptability, and efficiency in complex environments. This paper proposes a Deep Reinforcement Learning (DRL) framework based on the Soft Actor-Critic (SAC) algorithm to optimize autonomous parking tasks. SAC, an off-policy method with entropy regularization, is particularly well-suited for continuous action spaces, enabling fine-grained vehicle control. We model the parking task as a Markov Decision Process (MDP) and train an agent to maximize cumulative rewards while balancing exploration and exploitation through entropy maximization. The proposed system integrates multiple sensor inputs into a high-dimensional state space and leverages SAC's dual critic networks and policy network to achieve stable learning. Simulation results show that the SAC-based approach delivers high parking success rates, reduced maneuver times, and robust handling of dynamic obstacles, outperforming traditional rule-based methods and other DRL algorithms. This study demonstrates SAC's potential in autonomous parking and lays the foundation for real-world applications.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Coordinate UAVs and UGVs for Efficient Mission Planning? Optimizing Energy-Constrained Cooperative Routing with a DRL Framework</title>
<link>https://arxiv.org/abs/2504.21111</link>
<guid>https://arxiv.org/abs/2504.21111</guid>
<content:encoded><![CDATA[
arXiv:2504.21111v1 Announce Type: new 
Abstract: Efficient mission planning for cooperative systems involving Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) requires addressing energy constraints, scalability, and coordination challenges between agents. UAVs excel in rapidly covering large areas but are constrained by limited battery life, while UGVs, with their extended operational range and capability to serve as mobile recharging stations, are hindered by slower speeds. This heterogeneity makes coordination between UAVs and UGVs critical for achieving optimal mission outcomes. In this work, we propose a scalable deep reinforcement learning (DRL) framework to address the energy-constrained cooperative routing problem for multi-agent UAV-UGV teams, aiming to visit a set of task points in minimal time with UAVs relying on UGVs for recharging during the mission. The framework incorporates sortie-wise agent switching to efficiently manage multiple agents, by allocating task points and coordinating actions. Using an encoder-decoder transformer architecture, it optimizes routes and recharging rendezvous for the UAV-UGV team in the task scenario. Extensive computational experiments demonstrate the framework's superior performance over heuristic methods and a DRL baseline, delivering significant improvements in solution quality and runtime efficiency across diverse scenarios. Generalization studies validate its robustness, while dynamic scenario highlights its adaptability to real-time changes with a case study. This work advances UAV-UGV cooperative routing by providing a scalable, efficient, and robust solution for multi-agent mission planning.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NavEX: A Multi-Agent Coverage in Non-Convex and Uneven Environments via Exemplar-Clustering</title>
<link>https://arxiv.org/abs/2504.21113</link>
<guid>https://arxiv.org/abs/2504.21113</guid>
<content:encoded><![CDATA[
arXiv:2504.21113v1 Announce Type: new 
Abstract: This paper addresses multi-agent deployment in non-convex and uneven environments. To overcome the limitations of traditional approaches, we introduce Navigable Exemplar-Based Dispatch Coverage (NavEX), a novel dispatch coverage framework that combines exemplar-clustering with obstacle-aware and traversability-aware shortest distances, offering a deployment framework based on submodular optimization. NavEX provides a unified approach to solve two critical coverage tasks: (a) fair-access deployment, aiming to provide equitable service by minimizing agent-target distances, and (b) hotspot deployment, prioritizing high-density target regions. A key feature of NavEX is the use of exemplar-clustering for the coverage utility measure, which provides the flexibility to employ non-Euclidean distance metrics that do not necessarily conform to the triangle inequality. This allows NavEX to incorporate visibility graphs for shortest-path computation in environments with planar obstacles, and traversability-aware RRT* for complex, rugged terrains. By leveraging submodular optimization, the NavEX framework enables efficient, near-optimal solutions with provable performance guarantees for multi-agent deployment in realistic and complex settings, as demonstrated by our simulations.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Enhancer: Merged Approach using Vector Embedding for Reducing Large Language Model Hallucinations with External Knowledge</title>
<link>https://arxiv.org/abs/2504.21132</link>
<guid>https://arxiv.org/abs/2504.21132</guid>
<content:encoded><![CDATA[
arXiv:2504.21132v1 Announce Type: new 
Abstract: Large Language Models (LLMs), such as ChatGPT, have demonstrated the capability to generate human like, natural responses across a range of tasks, including task oriented dialogue and question answering. However, their application in real world, critical scenarios is often hindered by a tendency to produce inaccurate information and a limited ability to leverage external knowledge sources. This paper introduces the LLM ENHANCER system, designed to integrate multiple online sources such as Google, Wikipedia, and DuckDuckGo to enhance data accuracy. The LLMs employed within this system are open source. The data acquisition process for the LLM ENHANCER system operates in parallel, utilizing custom agent tools to manage the flow of information. Vector embeddings are used to identify the most pertinent information, which is subsequently supplied to the LLM for user interaction. The LLM ENHANCER system mitigates hallucinations in chat based LLMs while preserving response naturalness and accuracy.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Large-Scale Competitive Team Behaviors with Mean-Field Interactions</title>
<link>https://arxiv.org/abs/2504.21164</link>
<guid>https://arxiv.org/abs/2504.21164</guid>
<content:encoded><![CDATA[
arXiv:2504.21164v1 Announce Type: new 
Abstract: State-of-the-art multi-agent reinforcement learning (MARL) algorithms such as MADDPG and MAAC fail to scale in situations where the number of agents becomes large. Mean-field theory has shown encouraging results in modeling macroscopic agent behavior for teams with a large number of agents through a continuum approximation of the agent population and its interaction with the environment. In this work, we extend proximal policy optimization (PPO) to the mean-field domain by introducing the Mean-Field Multi-Agent Proximal Policy Optimization (MF-MAPPO), a novel algorithm that utilizes the effectiveness of the finite-population mean-field approximation in the context of zero-sum competitive multi-agent games between two teams. The proposed algorithm can be easily scaled to hundreds and thousands of agents in each team as shown through numerical experiments. In particular, the algorithm is applied to realistic applications such as large-scale offense-defense battlefield scenarios.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-in-the-Loop Planning for Transportation Electrification: Case Studies from Austin, Texas</title>
<link>https://arxiv.org/abs/2504.21185</link>
<guid>https://arxiv.org/abs/2504.21185</guid>
<content:encoded><![CDATA[
arXiv:2504.21185v1 Announce Type: new 
Abstract: This study explores the integration of AI in transportation electrification planning in Austin, TX, focusing on the use of Geospatial AI (GeoAI), Generative AI (GenAI), and Large Language Models (LLMs). GeoAI enhances site selection, localized GenAI models support meta-level estimations, and LLMs enable scenario simulations. These AI applications require human oversight. GeoAI outputs must be evaluated with land use data, GenAI models are not always accurate, and LLMs are prone to hallucinations. To ensure accountable planning, human planners must work alongside AI agents. Establishing a community feedback loop is essential to audit automated decisions. Planners should place Community Experience (CX) at the center of Urban Planning AI.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SecRepoBench: Benchmarking LLMs for Secure Code Generation in Real-World Repositories</title>
<link>https://arxiv.org/abs/2504.21205</link>
<guid>https://arxiv.org/abs/2504.21205</guid>
<content:encoded><![CDATA[
arXiv:2504.21205v1 Announce Type: new 
Abstract: This paper introduces SecRepoBench, a benchmark to evaluate LLMs on secure code generation in real-world repositories. SecRepoBench has 318 code generation tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 19 state-of-the-art LLMs using our benchmark and find that the models struggle with generating correct and secure code. In addition, the performance of LLMs to generate self-contained programs as measured by prior benchmarks do not translate to comparative performance at generating secure and correct code at the repository level in SecRepoBench. We show that the state-of-the-art prompt engineering techniques become less effective when applied to the repository level secure code generation problem. We conduct extensive experiments, including an agentic technique to generate secure code, to demonstrate that our benchmark is currently the most difficult secure coding benchmark, compared to previous state-of-the-art benchmarks. Finally, our comprehensive analysis provides insights into potential directions for enhancing the ability of LLMs to generate correct and secure code in real-world repositories.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhysicsFC: Learning User-Controlled Skills for a Physics-Based Football Player Controller</title>
<link>https://arxiv.org/abs/2504.21216</link>
<guid>https://arxiv.org/abs/2504.21216</guid>
<content:encoded><![CDATA[
arXiv:2504.21216v1 Announce Type: new 
Abstract: We propose PhysicsFC, a method for controlling physically simulated football player characters to perform a variety of football skills--such as dribbling, trapping, moving, and kicking--based on user input, while seamlessly transitioning between these skills. Our skill-specific policies, which generate latent variables for each football skill, are trained using an existing physics-based motion embedding model that serves as a foundation for reproducing football motions. Key features include a tailored reward design for the Dribble policy, a two-phase reward structure combined with projectile dynamics-based initialization for the Trap policy, and a Data-Embedded Goal-Conditioned Latent Guidance (DEGCL) method for the Move policy. Using the trained skill policies, the proposed football player finite state machine (PhysicsFC FSM) allows users to interactively control the character. To ensure smooth and agile transitions between skill policies, as defined in the FSM, we introduce the Skill Transition-Based Initialization (STI), which is applied during the training of each skill policy. We develop several interactive scenarios to showcase PhysicsFC's effectiveness, including competitive trapping and dribbling, give-and-go plays, and 11v11 football games, where multiple PhysicsFC agents produce natural and controllable physics-based football player behaviors. Quantitative evaluations further validate the performance of individual skill policies and the transitions between them, using the presented metrics and experimental designs.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theoretical Foundations for Semantic Cognition in Artificial Intelligence</title>
<link>https://arxiv.org/abs/2504.21218</link>
<guid>https://arxiv.org/abs/2504.21218</guid>
<content:encoded><![CDATA[
arXiv:2504.21218v1 Announce Type: new 
Abstract: This monograph presents a modular cognitive architecture for artificial intelligence grounded in the formal modeling of belief as structured semantic state. Belief states are defined as dynamic ensembles of linguistic expressions embedded within a navigable manifold, where operators enable assimilation, abstraction, nullification, memory, and introspection. Drawing from philosophy, cognitive science, and neuroscience, we develop a layered framework that enables self-regulating epistemic agents capable of reflective, goal-directed thought. At the core of this framework is the epistemic vacuum: a class of semantically inert cognitive states that serves as the conceptual origin of belief space. From this foundation, the Null Tower arises as a generative structure recursively built through internal representational capacities. The theoretical constructs are designed to be implementable in both symbolic and neural systems, including large language models, hybrid agents, and adaptive memory architectures. This work offers a foundational substrate for constructing agents that reason, remember, and regulate their beliefs in structured, interpretable ways.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Talk Before You Retrieve: Agent-Led Discussions for Better RAG in Medical QA</title>
<link>https://arxiv.org/abs/2504.21252</link>
<guid>https://arxiv.org/abs/2504.21252</guid>
<content:encoded><![CDATA[
arXiv:2504.21252v1 Announce Type: new 
Abstract: Medical question answering (QA) is a reasoning-intensive task that remains challenging for large language models (LLMs) due to hallucinations and outdated domain knowledge. Retrieval-Augmented Generation (RAG) provides a promising post-training solution by leveraging external knowledge. However, existing medical RAG systems suffer from two key limitations: (1) a lack of modeling for human-like reasoning behaviors during information retrieval, and (2) reliance on suboptimal medical corpora, which often results in the retrieval of irrelevant or noisy snippets. To overcome these challenges, we propose Discuss-RAG, a plug-and-play module designed to enhance the medical QA RAG system through collaborative agent-based reasoning. Our method introduces a summarizer agent that orchestrates a team of medical experts to emulate multi-turn brainstorming, thereby improving the relevance of retrieved content. Additionally, a decision-making agent evaluates the retrieved snippets before their final integration. Experimental results on four benchmark medical QA datasets show that Discuss-RAG consistently outperforms MedRAG, especially significantly improving answer accuracy by up to 16.67% on BioASQ and 12.20% on PubMedQA. The code is available at: https://github.com/LLM-VLM-GSL/Discuss-RAG.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Multi-agent Communication Based on Decentralization-Oriented Adversarial Training</title>
<link>https://arxiv.org/abs/2504.21278</link>
<guid>https://arxiv.org/abs/2504.21278</guid>
<content:encoded><![CDATA[
arXiv:2504.21278v1 Announce Type: new 
Abstract: In typical multi-agent reinforcement learning (MARL) problems, communication is important for agents to share information and make the right decisions. However, due to the complexity of training multi-agent communication, existing methods often fall into the dilemma of local optimization, which leads to the concentration of communication in a limited number of channels and presents an unbalanced structure. Such unbalanced communication policy are vulnerable to abnormal conditions, where the damage of critical communication channels can trigger the crash of the entire system. Inspired by decentralization theory in sociology, we propose DMAC, which enhances the robustness of multi-agent communication policies by retraining them into decentralized patterns. Specifically, we train an adversary DMAC\_Adv which can dynamically identify and mask the critical communication channels, and then apply the adversarial samples generated by DMAC\_Adv to the adversarial learning of the communication policy to force the policy in exploring other potential communication schemes and transition to a decentralized structure. As a training method to improve robustness, DMAC can be fused with any learnable communication policy algorithm. The experimental results in two communication policies and four multi-agent tasks demonstrate that DMAC achieves higher improvement on robustness and performance of communication policy compared with two state-of-the-art and commonly-used baselines. Also, the results demonstrate that DMAC can achieve decentralized communication structure with acceptable communication cost.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Participatory AI, Public Sector AI, Differential Privacy, Conversational Interfaces, Explainable AI, Citizen Engagement in AI</title>
<link>https://arxiv.org/abs/2504.21297</link>
<guid>https://arxiv.org/abs/2504.21297</guid>
<content:encoded><![CDATA[
arXiv:2504.21297v1 Announce Type: new 
Abstract: This paper introduces a conversational interface system that enables participatory design of differentially private AI systems in public sector applications. Addressing the challenge of balancing mathematical privacy guarantees with democratic accountability, we propose three key contributions: (1) an adaptive $\epsilon$-selection protocol leveraging TOPSIS multi-criteria decision analysis to align citizen preferences with differential privacy (DP) parameters, (2) an explainable noise-injection framework featuring real-time Mean Absolute Error (MAE) visualizations and GPT-4-powered impact analysis, and (3) an integrated legal-compliance mechanism that dynamically modulates privacy budgets based on evolving regulatory constraints. Our results advance participatory AI practices by demonstrating how conversational interfaces can enhance public engagement in algorithmic privacy mechanisms, ensuring that privacy-preserving AI in public sector governance remains both mathematically robust and democratically accountable.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Feature Transformation via In-context Generation, Generator-critic LLM Agents, and Duet-play Teaming</title>
<link>https://arxiv.org/abs/2504.21304</link>
<guid>https://arxiv.org/abs/2504.21304</guid>
<content:encoded><![CDATA[
arXiv:2504.21304v1 Announce Type: new 
Abstract: Feature transformation involves generating a new set of features from the original dataset to enhance the data's utility. In certain domains like material performance screening, dimensionality is large and collecting labels is expensive and lengthy. It highly necessitates transforming feature spaces efficiently and without supervision to enhance data readiness and AI utility. However, existing methods fall short in efficient navigation of a vast space of feature combinations, and are mostly designed for supervised settings. To fill this gap, our unique perspective is to leverage a generator-critic duet-play teaming framework using LLM agents and in-context learning to derive pseudo-supervision from unsupervised data. The framework consists of three interconnected steps: (1) Critic agent diagnoses data to generate actionable advice, (2) Generator agent produces tokenized feature transformations guided by the critic's advice, and (3) Iterative refinement ensures continuous improvement through feedback between agents. The generator-critic framework can be generalized to human-agent collaborative generation, by replacing the critic agent with human experts. Extensive experiments demonstrate that the proposed framework outperforms even supervised baselines in feature transformation efficiency, robustness, and practical applicability across diverse datasets.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generalized Meta Federated Learning Framework with Theoretical Convergence Guarantees</title>
<link>https://arxiv.org/abs/2504.21327</link>
<guid>https://arxiv.org/abs/2504.21327</guid>
<content:encoded><![CDATA[
arXiv:2504.21327v1 Announce Type: new 
Abstract: Meta federated learning (FL) is a personalized variant of FL, where multiple agents collaborate on training an initial shared model without exchanging raw data samples. The initial model should be trained in a way that current or new agents can easily adapt it to their local datasets after one or a few fine-tuning steps, thus improving the model personalization. Conventional meta FL approaches minimize the average loss of agents on the local models obtained after one step of fine-tuning. In practice, agents may need to apply several fine-tuning steps to adapt the global model to their local data, especially under highly heterogeneous data distributions across agents. To this end, we present a generalized framework for the meta FL by minimizing the average loss of agents on their local model after any arbitrary number $\nu$ of fine-tuning steps. For this generalized framework, we present a variant of the well-known federated averaging (FedAvg) algorithm and conduct a comprehensive theoretical convergence analysis to characterize the convergence speed as well as behavior of the meta loss functions in both the exact and approximated cases. Our experiments on real-world datasets demonstrate superior accuracy and faster convergence for the proposed scheme compared to conventional approaches.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IRL Dittos: Embodied Multimodal AI Agent Interactions in Open Spaces</title>
<link>https://arxiv.org/abs/2504.21347</link>
<guid>https://arxiv.org/abs/2504.21347</guid>
<content:encoded><![CDATA[
arXiv:2504.21347v1 Announce Type: new 
Abstract: We introduce the In Real Life (IRL) Ditto, an AI-driven embodied agent designed to represent remote colleagues in shared office spaces, creating opportunities for real-time exchanges even in their absence. IRL Ditto offers a unique hybrid experience by allowing in-person colleagues to encounter a digital version of their remote teammates, initiating greetings, updates, or small talk as they might in person. Our research question examines: How can the IRL Ditto influence interactions and relationships among colleagues in a shared office space? Through a four-day study, we assessed IRL Ditto's ability to strengthen social ties by simulating presence and enabling meaningful interactions across different levels of social familiarity. We find that enhancing social relationships depended deeply on the foundation of the relationship participants had with the source of the IRL Ditto. This study provides insights into the role of embodied agents in enriching workplace dynamics for distributed teams.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UAV-VLN: End-to-End Vision Language guided Navigation for UAVs</title>
<link>https://arxiv.org/abs/2504.21432</link>
<guid>https://arxiv.org/abs/2504.21432</guid>
<content:encoded><![CDATA[
arXiv:2504.21432v1 Announce Type: new 
Abstract: A core challenge in AI-guided autonomy is enabling agents to navigate realistically and effectively in previously unseen environments based on natural language commands. We propose UAV-VLN, a novel end-to-end Vision-Language Navigation (VLN) framework for Unmanned Aerial Vehicles (UAVs) that seamlessly integrates Large Language Models (LLMs) with visual perception to facilitate human-interactive navigation. Our system interprets free-form natural language instructions, grounds them into visual observations, and plans feasible aerial trajectories in diverse environments.
  UAV-VLN leverages the common-sense reasoning capabilities of LLMs to parse high-level semantic goals, while a vision model detects and localizes semantically relevant objects in the environment. By fusing these modalities, the UAV can reason about spatial relationships, disambiguate references in human instructions, and plan context-aware behaviors with minimal task-specific supervision. To ensure robust and interpretable decision-making, the framework includes a cross-modal grounding mechanism that aligns linguistic intent with visual context.
  We evaluate UAV-VLN across diverse indoor and outdoor navigation scenarios, demonstrating its ability to generalize to novel instructions and environments with minimal task-specific training. Our results show significant improvements in instruction-following accuracy and trajectory efficiency, highlighting the potential of LLM-driven vision-language interfaces for safe, intuitive, and generalizable UAV autonomy.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NGENT: Next-Generation AI Agents Must Integrate Multi-Domain Abilities to Achieve Artificial General Intelligence</title>
<link>https://arxiv.org/abs/2504.21433</link>
<guid>https://arxiv.org/abs/2504.21433</guid>
<content:encoded><![CDATA[
arXiv:2504.21433v1 Announce Type: new 
Abstract: This paper argues that the next generation of AI agent (NGENT) should integrate across-domain abilities to advance toward Artificial General Intelligence (AGI). Although current AI agents are effective in specialized tasks such as robotics, role-playing, and tool-using, they remain confined to narrow domains. We propose that future AI agents should synthesize the strengths of these specialized systems into a unified framework capable of operating across text, vision, robotics, reinforcement learning, emotional intelligence, and beyond. This integration is not only feasible but also essential for achieving the versatility and adaptability that characterize human intelligence. The convergence of technologies across AI domains, coupled with increasing user demand for cross-domain capabilities, suggests that such integration is within reach. Ultimately, the development of these versatile agents is a critical step toward realizing AGI. This paper explores the rationale for this shift, potential pathways for achieving it.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stability of Open Multi-agent Systems over Dynamic Signed Graphs</title>
<link>https://arxiv.org/abs/2504.21443</link>
<guid>https://arxiv.org/abs/2504.21443</guid>
<content:encoded><![CDATA[
arXiv:2504.21443v1 Announce Type: new 
Abstract: This paper addresses the bipartite consensus-control problem in open multi-agent systems containing both cooperative and antagonistic interactions. In these systems, new agents can join and new interactions can be formed over time. Moreover, the types of interactions, cooperative or antagonistic, may change. To model these structural changes, we represent the system as a switched system interconnected over a dynamic signed graph. Using the signed edge-based agreement protocol and constructing strict Lyapunov functions for signed edge-Laplacian matrices with multiple zero eigenvalues, we establish global asymptotic stability of the bipartite consensus control. Numerical simulations validate our theoretical results.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimPRIVE: a Simulation framework for Physical Robot Interaction with Virtual Environments</title>
<link>https://arxiv.org/abs/2504.21454</link>
<guid>https://arxiv.org/abs/2504.21454</guid>
<content:encoded><![CDATA[
arXiv:2504.21454v1 Announce Type: new 
Abstract: The use of machine learning in cyber-physical systems has attracted the interest of both industry and academia. However, no general solution has yet been found against the unpredictable behavior of neural networks and reinforcement learning agents. Nevertheless, the improvements of photo-realistic simulators have paved the way towards extensive testing of complex algorithms in different virtual scenarios, which would be expensive and dangerous to implement in the real world.
  This paper presents SimPRIVE, a simulation framework for physical robot interaction with virtual environments, which operates as a vehicle-in-the-loop platform, rendering a virtual world while operating the vehicle in the real world.
  Using SimPRIVE, any physical mobile robot running on ROS 2 can easily be configured to move its digital twin in a virtual world built with the Unreal Engine 5 graphic engine, which can be populated with objects, people, or other vehicles with programmable behavior.
  SimPRIVE has been designed to accommodate custom or pre-built virtual worlds while being light-weight to contain execution times and allow fast rendering. Its main advantage lies in the possibility of testing complex algorithms on the full software and hardware stack while minimizing the risks and costs of a test campaign. The framework has been validated by testing a reinforcement learning agent trained for obstacle avoidance on an AgileX Scout Mini rover that navigates a virtual office environment where everyday objects and people are placed as obstacles. The physical rover moves with no collision in an indoor limited space, thanks to a LiDAR-based heuristic.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Trajectory Exploration for Multimodal Agents</title>
<link>https://arxiv.org/abs/2504.21561</link>
<guid>https://arxiv.org/abs/2504.21561</guid>
<content:encoded><![CDATA[
arXiv:2504.21561v1 Announce Type: new 
Abstract: Multimodal agents, which integrate a controller (e.g., a large language model) with external tools, have demonstrated remarkable capabilities in tackling complex tasks. However, existing agents need to collect a large number of expert data for fine-tuning to adapt to new environments. In this paper, we propose an online self-exploration method for multimodal agents, namely SPORT, via step-wise preference optimization to refine the trajectories of agents, which automatically generates tasks and learns from solving the generated tasks, without any expert annotation. SPORT operates through four iterative components: task synthesis, step sampling, step verification, and preference tuning. First, we synthesize multi-modal tasks using language models. Then, we introduce a novel search scheme, where step sampling and step verification are executed alternately to solve each generated task. We employ a verifier to provide AI feedback to construct step-wise preference data. The data is subsequently used to update the controller's policy through preference tuning, producing a SPORT Agent. By interacting with real environments, the SPORT Agent evolves into a more refined and capable system. Evaluation in the GTA and GAIA benchmarks show that the SPORT Agent achieves 6.41\% and 3.64\% improvements, underscoring the generalization and effectiveness introduced by our method. The project page is https://SPORT-Agents.github.io.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty, bias and the institution bootstrapping problem</title>
<link>https://arxiv.org/abs/2504.21579</link>
<guid>https://arxiv.org/abs/2504.21579</guid>
<content:encoded><![CDATA[
arXiv:2504.21579v1 Announce Type: new 
Abstract: Institutions play a critical role in enabling communities to manage common-pool resources and avert tragedies of the commons. However, a fundamental issue arises: Individuals typically perceive participation as advantageous only after an institution is established, creating a paradox: How can institutions form if no one will join before a critical mass exists? We term this conundrum the institution bootstrapping problem and propose that misperception, specifically, agents' erroneous belief that an institution already exists, could resolve this paradox. By integrating well-documented psychological phenomena, including cognitive biases, probability distortion, and perceptual noise, into a game-theoretic framework, we demonstrate how these factors collectively mitigate the bootstrapping problem. Notably, unbiased perceptual noise (e.g., noise arising from agents' heterogeneous physical or social contexts) drastically reduces the critical mass of cooperators required for institutional emergence. This effect intensifies with greater diversity of perceptions. We explain this counter-intuitive result through asymmetric boundary conditions: proportional underestimation of low-probability sanctions produces distinct outcomes compared to equivalent overestimation. Furthermore, the type of perceptual distortion, proportional versus absolute, yields qualitatively different evolutionary pathways. These findings challenge conventional assumptions about rationality in institutional design, highlighting how "noisy" cognition can paradoxically enhance cooperation. Finally, we contextualize these insights within broader discussions of multi-agent system design and collective action. Our analysis underscores the importance of incorporating human-like cognitive constraints, not just idealized rationality, into models of institutional emergence and resilience.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applying Machine Learning for characterizing social networks Agent-based models</title>
<link>https://arxiv.org/abs/2504.21609</link>
<guid>https://arxiv.org/abs/2504.21609</guid>
<content:encoded><![CDATA[
arXiv:2504.21609v1 Announce Type: new 
Abstract: Nowadays, social media networks are increasingly significant to our lives, the imperative to study social media networks becomes more and more essential. With billions of users across platforms and constant updates, the complexity of modeling social networks is immense. Agent-based modeling (ABM) is widely employed to study social networks community, allowing us to define individual behaviors and simulate system-level evolution. It can be a powerful tool to test how the algorithms affect users behavior. To fully leverage agent-based models,superior data processing and storage capabilities are essential. High Performance Computing (HPC) presents an optimal solution, adept at managing complex computations and analysis, particularly for voluminous or iteration-intensive tasks. We utilize Machine Learning (ML) methods to analyze social media users due to their ability to efficiently process vast amounts of data and derive insights that aid in understanding user behaviors, preferences, and trends. Therefore, our proposal involves ML to characterize user attributes and to develop a general user model for ABM simulation of in social networks on HPC systems.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meeseeks: An Iterative Benchmark Evaluating LLMs Multi-Turn Instruction-Following Ability</title>
<link>https://arxiv.org/abs/2504.21625</link>
<guid>https://arxiv.org/abs/2504.21625</guid>
<content:encoded><![CDATA[
arXiv:2504.21625v1 Announce Type: new 
Abstract: The ability to follow instructions accurately is fundamental for Large Language Models (LLMs) to serve as reliable agents in real-world applications. While existing instruction-following benchmarks are either single-turn or introduce new requirements in each turn without allowing self-correction, Meeseeks simulates realistic human-LLM interactions through an iterative feedback process. This design enables models to self-correct based on specific requirement failures, better reflecting real-world user-end usage patterns. The benchmark implements a comprehensive evaluation system with 38 capability tags organized across three dimensions: Intent Recognition, Granular Content Validation, and Output Structure Validation. Through rigorous evaluation across LLMs, Meeseeks provides valuable insights into LLMs' instruction-following capabilities in practical applications.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics</title>
<link>https://arxiv.org/abs/2504.21716</link>
<guid>https://arxiv.org/abs/2504.21716</guid>
<content:encoded><![CDATA[
arXiv:2504.21716v1 Announce Type: new 
Abstract: We present an embodied robotic system with an LLM-driven agent-orchestration architecture for autonomous household object management. The system integrates memory-augmented task planning, enabling robots to execute high-level user commands while tracking past actions. It employs three specialized agents: a routing agent, a task planning agent, and a knowledge base agent, each powered by task-specific LLMs. By leveraging in-context learning, our system avoids the need for explicit model training. RAG enables the system to retrieve context from past interactions, enhancing long-term object tracking. A combination of Grounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating semantic scene understanding for task planning. Evaluation across three household scenarios demonstrates high task planning accuracy and an improvement in memory recall due to RAG. Specifically, Qwen2.5 yields best performance for specialized agents, while LLaMA3.1 excels in routing tasks. The source code is available at: https://github.com/marc1198/chat-hsr.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asymptotic Analysis of Weighted Fair Division</title>
<link>https://arxiv.org/abs/2504.21728</link>
<guid>https://arxiv.org/abs/2504.21728</guid>
<content:encoded><![CDATA[
arXiv:2504.21728v1 Announce Type: new 
Abstract: Several resource allocation settings involve agents with unequal entitlements represented by weights. We analyze weighted fair division from an asymptotic perspective: if $m$ items are divided among $n$ agents whose utilities are independently sampled from a probability distribution, when is it likely that a fair allocation exist? We show that if the ratio between the weights is bounded, a weighted envy-free allocation exists with high probability provided that $m = \Omega(n\log n/\log\log n)$, generalizing a prior unweighted result. For weighted proportionality, we establish a sharp threshold of $m = n/(1-\mu)$ for the transition from non-existence to existence, where $\mu\in (0,1)$ denotes the mean of the distribution. In addition, we prove that for two agents, a weighted envy-free (and weighted proportional) allocation is likely to exist if $m = \omega(\sqrt{r})$, where $r$ denotes the ratio between the two weights.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based Interactive Imitation Learning for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2504.21769</link>
<guid>https://arxiv.org/abs/2504.21769</guid>
<content:encoded><![CDATA[
arXiv:2504.21769v1 Announce Type: new 
Abstract: Recent advancements in machine learning provide methods to train autonomous agents capable of handling the increasing complexity of sequential decision-making in robotics. Imitation Learning (IL) is a prominent approach, where agents learn to control robots based on human demonstrations. However, IL commonly suffers from violating the independent and identically distributed (i.i.d) assumption in robotic tasks. Interactive Imitation Learning (IIL) achieves improved performance by allowing agents to learn from interactive feedback from human teachers. Despite these improvements, both approaches come with significant costs due to the necessity of human involvement. Leveraging the emergent capabilities of Large Language Models (LLMs) in reasoning and generating human-like responses, we introduce LLM-iTeach -- a novel IIL framework that utilizes an LLM as an interactive teacher to enhance agent performance while alleviating the dependence on human resources. Firstly, LLM-iTeach uses a hierarchical prompting strategy that guides the LLM in generating a policy in Python code. Then, with a designed similarity-based feedback mechanism, LLM-iTeach provides corrective and evaluative feedback interactively during the agent's training. We evaluate LLM-iTeach against baseline methods such as Behavior Cloning (BC), an IL method, and CEILing, a state-of-the-art IIL method using a human teacher, on various robotic manipulation tasks. Our results demonstrate that LLM-iTeach surpasses BC in the success rate and achieves or even outscores that of CEILing, highlighting the potential of LLMs as cost-effective, human-like teachers in interactive learning environments. We further demonstrate the method's potential for generalization by evaluating it on additional tasks. The code and prompts are provided at: https://github.com/Tubicor/LLM-iTeach.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Intermediate Fusion All You Need for UAV-based Collaborative Perception?</title>
<link>https://arxiv.org/abs/2504.21774</link>
<guid>https://arxiv.org/abs/2504.21774</guid>
<content:encoded><![CDATA[
arXiv:2504.21774v1 Announce Type: new 
Abstract: Collaborative perception enhances environmental awareness through inter-agent communication and is regarded as a promising solution to intelligent transportation systems. However, existing collaborative methods for Unmanned Aerial Vehicles (UAVs) overlook the unique characteristics of the UAV perspective, resulting in substantial communication overhead. To address this issue, we propose a novel communication-efficient collaborative perception framework based on late-intermediate fusion, dubbed LIF. The core concept is to exchange informative and compact detection results and shift the fusion stage to the feature representation level. In particular, we leverage vision-guided positional embedding (VPE) and box-based virtual augmented feature (BoBEV) to effectively integrate complementary information from various agents. Additionally, we innovatively introduce an uncertainty-driven communication mechanism that uses uncertainty evaluation to select high-quality and reliable shared areas. Experimental results demonstrate that our LIF achieves superior performance with minimal communication bandwidth, proving its effectiveness and practicality. Code and models are available at https://github.com/uestchjw/LIF.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebThinker: Empowering Large Reasoning Models with Deep Research Capability</title>
<link>https://arxiv.org/abs/2504.21776</link>
<guid>https://arxiv.org/abs/2504.21776</guid>
<content:encoded><![CDATA[
arXiv:2504.21776v1 Announce Type: new 
Abstract: Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose \textbf{WebThinker}, a deep research agent that empowers LRMs to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. WebThinker integrates a \textbf{Deep Web Explorer} module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an \textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an \textbf{RL-based training strategy} via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available at https://github.com/RUC-NLPIR/WebThinker.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-smith: Scaling Data for Software Engineering Agents</title>
<link>https://arxiv.org/abs/2504.21798</link>
<guid>https://arxiv.org/abs/2504.21798</guid>
<content:encoded><![CDATA[
arXiv:2504.21798v1 Announce Type: new 
Abstract: Despite recent progress in Language Models (LMs) for software engineering, collecting training data remains a significant pain point. Existing datasets are small, with at most 1,000s of training instances from 11 or fewer GitHub repositories. The procedures to curate such datasets are often complex, necessitating hundreds of hours of human labor; companion execution environments also take up several terabytes of storage, severely limiting their scalability and usability. To address this pain point, we introduce SWE-smith, a novel pipeline for generating software engineering training data at scale. Given any Python codebase, SWE-smith constructs a corresponding execution environment, then automatically synthesizes 100s to 1,000s of task instances that break existing test(s) in the codebase. Using SWE-smith, we create a dataset of 50k instances sourced from 128 GitHub repositories, an order of magnitude larger than all previous works. We train SWE-agent-LM-32B, achieving 40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art among open source models. We open source SWE-smith (collection procedure, task instances, trajectories, models) to lower the barrier of entry for research in LM systems for automated software engineering. All assets available at https://swesmith.com.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing AI Agents for Alignment and Governance</title>
<link>https://arxiv.org/abs/2504.21848</link>
<guid>https://arxiv.org/abs/2504.21848</guid>
<content:encoded><![CDATA[
arXiv:2504.21848v1 Announce Type: new 
Abstract: The creation of effective governance mechanisms for AI agents requires a deeper understanding of their core properties and how these properties relate to questions surrounding the deployment and operation of agents in the world. This paper provides a characterization of AI agents that focuses on four dimensions: autonomy, efficacy, goal complexity, and generality. We propose different gradations for each dimension, and argue that each dimension raises unique questions about the design, operation, and governance of these systems. Moreover, we draw upon this framework to construct "agentic profiles" for different kinds of AI agents. These profiles help to illuminate cross-cutting technical and non-technical governance challenges posed by different classes of AI agents, ranging from narrow task-specific assistants to highly autonomous general-purpose systems. By mapping out key axes of variation and continuity, this framework provides developers, policymakers, and members of the public with the opportunity to develop governance approaches that better align with collective societal goals.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments</title>
<link>https://arxiv.org/abs/2504.21851</link>
<guid>https://arxiv.org/abs/2504.21851</guid>
<content:encoded><![CDATA[
arXiv:2504.21851v1 Announce Type: new 
Abstract: Objectives: While Large Language Models (LLMs) have been widely used to assist clinicians and support patients, no existing work has explored dialogue systems for standard diagnostic interviews and assessments. This study aims to bridge the gap in mental healthcare accessibility by developing an LLM-powered dialogue system that replicates clinician behavior. Materials and Methods: We introduce TRUST, a framework of cooperative LLM modules capable of conducting formal diagnostic interviews and assessments for Post-Traumatic Stress Disorder (PTSD). To guide the generation of appropriate clinical responses, we propose a Dialogue Acts schema specifically designed for clinical interviews. Additionally, we develop a patient simulation approach based on real-life interview transcripts to replace time-consuming and costly manual testing by clinicians. Results: A comprehensive set of evaluation metrics is designed to assess the dialogue system from both the agent and patient simulation perspectives. Expert evaluations by conversation and clinical specialists show that TRUST performs comparably to real-life clinical interviews. Discussion: Our system performs at the level of average clinicians, with room for future enhancements in communication styles and response appropriateness. Conclusions: Our TRUST framework shows its potential to facilitate mental healthcare availability.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Interactive Generative Video</title>
<link>https://arxiv.org/abs/2504.21853</link>
<guid>https://arxiv.org/abs/2504.21853</guid>
<content:encoded><![CDATA[
arXiv:2504.21853v1 Announce Type: new 
Abstract: Interactive Generative Video (IGV) has emerged as a crucial technology in response to the growing demand for high-quality, interactive video content across various domains. In this paper, we define IGV as a technology that combines generative capabilities to produce diverse high-quality video content with interactive features that enable user engagement through control signals and responsive feedback. We survey the current landscape of IGV applications, focusing on three major domains: 1) gaming, where IGV enables infinite exploration in virtual worlds; 2) embodied AI, where IGV serves as a physics-aware environment synthesizer for training agents in multimodal interaction with dynamically evolving scenes; and 3) autonomous driving, where IGV provides closed-loop simulation capabilities for safety-critical testing and validation. To guide future development, we propose a comprehensive framework that decomposes an ideal IGV system into five essential modules: Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we systematically analyze the technical challenges and future directions in realizing each component for an ideal IGV system, such as achieving real-time generation, enabling open-domain control, maintaining long-term coherence, simulating accurate physics, and integrating causal reasoning. We believe that this systematic analysis will facilitate future research and development in the field of IGV, ultimately advancing the technology toward more sophisticated and practical applications.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games</title>
<link>https://arxiv.org/abs/2312.02312</link>
<guid>https://arxiv.org/abs/2312.02312</guid>
<content:encoded><![CDATA[
arXiv:2312.02312v2 Announce Type: replace 
Abstract: Video games have served as useful benchmarks for the decision-making community, but going beyond Atari games towards modern games has been prohibitively expensive for the vast majority of the research community. Prior work in modern video games typically relied on game-specific integration to obtain game features and enable online training, or on existing large datasets. An alternative approach is to train agents using imitation learning to play video games purely from images. However, this setting poses a fundamental question: which visual encoders obtain representations that retain information critical for decision making? To answer this question, we conduct a systematic study of imitation learning with publicly available pre-trained visual encoders compared to the typical task-specific end-to-end training approach in Minecraft, Counter-Strike: Global Offensive, and Minecraft Dungeons. Our results show that end-to-end training can be effective with comparably low-resolution images and only minutes of demonstrations, but significant improvements can be gained by utilising pre-trained encoders such as DINOv2 depending on the game. In addition to enabling effective decision making, we show that pre-trained encoders can make decision-making research in video games more accessible by significantly reducing the cost of training.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Offline Multi-agent Skill Discovery</title>
<link>https://arxiv.org/abs/2405.16386</link>
<guid>https://arxiv.org/abs/2405.16386</guid>
<content:encoded><![CDATA[
arXiv:2405.16386v3 Announce Type: replace 
Abstract: Skills are effective temporal abstractions established for sequential decision making, which enable efficient hierarchical learning for long-horizon tasks and facilitate multi-task learning through their transferability. Despite extensive research, research gaps remain in multi-agent scenarios, particularly for automatically extracting subgroup coordination patterns in a multi-agent task. In this case, we propose two novel auto-encoder schemes: VO-MASD-3D and VO-MASD-Hier, to simultaneously capture subgroup- and temporal-level abstractions and form multi-agent skills, which firstly solves the aforementioned challenge. An essential algorithm component of these schemes is a dynamic grouping function that can automatically detect latent subgroups based on agent interactions in a task. Further, our method can be applied to offline multi-task data, and the discovered subgroup skills can be transferred across relevant tasks without retraining. Empirical evaluations on StarCraft tasks indicate that our approach significantly outperforms existing hierarchical multi-agent reinforcement learning (MARL) methods. Moreover, skills discovered using our method can effectively reduce the learning difficulty in MARL scenarios with delayed and sparse reward signals. The codebase is available at https://github.com/LucasCJYSDL/VOMASD.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SustainDC: Benchmarking for Sustainable Data Center Control</title>
<link>https://arxiv.org/abs/2408.07841</link>
<guid>https://arxiv.org/abs/2408.07841</guid>
<content:encoded><![CDATA[
arXiv:2408.07841v5 Announce Type: replace 
Abstract: Machine learning has driven an exponential increase in computational demand, leading to massive data centers that consume significant amounts of energy and contribute to climate change. This makes sustainable data center control a priority. In this paper, we introduce SustainDC, a set of Python environments for benchmarking multi-agent reinforcement learning (MARL) algorithms for data centers (DC). SustainDC supports custom DC configurations and tasks such as workload scheduling, cooling optimization, and auxiliary battery management, with multiple agents managing these operations while accounting for the effects of each other. We evaluate various MARL algorithms on SustainDC, showing their performance across diverse DC designs, locations, weather conditions, grid carbon intensity, and workload requirements. Our results highlight significant opportunities for improvement of data center operations using MARL algorithms. Given the increasing use of DC due to AI, SustainDC provides a crucial platform for the development and benchmarking of advanced algorithms essential for achieving sustainable computing and addressing other heterogeneous real-world challenges.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-ACP: Real-Time Adaptive Collaborative Perception Leveraging Robust Task-Oriented Communications</title>
<link>https://arxiv.org/abs/2410.04168</link>
<guid>https://arxiv.org/abs/2410.04168</guid>
<content:encoded><![CDATA[
arXiv:2410.04168v4 Announce Type: replace 
Abstract: Collaborative perception enhances sensing in multirobot and vehicular networks by fusing information from multiple agents, improving perception accuracy and sensing range. However, mobility and non-rigid sensor mounts introduce extrinsic calibration errors, necessitating online calibration, further complicated by limited overlap in sensing regions. Moreover, maintaining fresh information is crucial for timely and accurate sensing. To address calibration errors and ensure timely and accurate perception, we propose a robust task-oriented communication strategy to optimize online self-calibration and efficient feature sharing for Real-time Adaptive Collaborative Perception (R-ACP). Specifically, we first formulate an Age of Perceived Targets (AoPT) minimization problem to capture data timeliness of multi-view streaming. Then, in the calibration phase, we introduce a channel-aware self-calibration technique based on reidentification (Re-ID), which adaptively compresses key features according to channel capacities, effectively addressing calibration issues via spatial and temporal cross-camera correlations. In the streaming phase, we tackle the trade-off between bandwidth and inference accuracy by leveraging an Information Bottleneck (IB) based encoding method to adjust video compression rates based on task relevance, thereby reducing communication overhead and latency. Finally, we design a priority-aware network to filter corrupted features to mitigate performance degradation from packet corruption. Extensive studies demonstrate that our framework outperforms five baselines, improving multiple object detection accuracy (MODA) by 25.49% and reducing communication costs by 51.36% under severely poor channel conditions. Code will be made publicly available: github.com/fangzr/R-ACP.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Masked Generative Priors Improve World Models Sequence Modelling Capabilities</title>
<link>https://arxiv.org/abs/2410.07836</link>
<guid>https://arxiv.org/abs/2410.07836</guid>
<content:encoded><![CDATA[
arXiv:2410.07836v5 Announce Type: replace 
Abstract: Deep Reinforcement Learning (RL) has become the leading approach for creating artificial agents in complex environments. Model-based approaches, which are RL methods with world models that predict environment dynamics, are among the most promising directions for improving data efficiency, forming a critical step toward bridging the gap between research and real-world deployment. In particular, world models enhance sample efficiency by learning in imagination, which involves training a generative sequence model of the environment in a self-supervised manner. Recently, Masked Generative Modelling has emerged as a more efficient and superior inductive bias for modelling and generating token sequences. Building on the Efficient Stochastic Transformer-based World Models (STORM) architecture, we replace the traditional MLP prior with a Masked Generative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our model on two downstream tasks: reinforcement learning and video prediction. GIT-STORM demonstrates substantial performance gains in RL tasks on the Atari 100k benchmark. Moreover, we apply Transformer-based World Models to continuous action environments for the first time, addressing a significant gap in prior research. To achieve this, we employ a state mixer function that integrates latent state representations with actions, enabling our model to handle continuous control tasks. We validate this approach through qualitative and quantitative analyses on the DeepMind Control Suite, showcasing the effectiveness of Transformer-based World Models in this new domain. Our results highlight the versatility and efficacy of the MaskGIT dynamics prior, paving the way for more accurate world models and effective RL policies.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent</title>
<link>https://arxiv.org/abs/2410.16658</link>
<guid>https://arxiv.org/abs/2410.16658</guid>
<content:encoded><![CDATA[
arXiv:2410.16658v3 Announce Type: replace 
Abstract: Adsorption energy is a key reactivity descriptor in catalysis, enabling efficient screening for optimal catalysts. However, determining adsorption energy typically requires evaluating numerous adsorbate-catalyst configurations. Current algorithmic approaches rely on exhaustive enumeration of adsorption sites and configurations, which makes the process computationally intensive and does not inherently guarantee the identification of the global minimum energy. In this work, we introduce Adsorb-Agent, a Large Language Model (LLM) agent designed to efficiently identify system-specific stable adsorption configurations corresponding to the global minimum adsorption energy. Adsorb-Agent leverages its built-in knowledge and emergent reasoning capabilities to strategically explore adsorption configurations likely to hold adsorption energy. By reducing the reliance on exhaustive sampling, it significantly decreases the number of initial configurations required while improving the accuracy of adsorption energy predictions. We evaluate Adsorb-Agent's performance across twenty representative systems encompassing a range of complexities. The Adsorb-Agent successfully identifies comparable adsorption energies for 83.7% of the systems and achieves lower energies, closer to the actual global minimum, for 35% of the systems, while requiring significantly fewer initial configurations than conventional methods. Its capability is particularly evident in complex systems, where it identifies lower adsorption energies for 46.7% of systems involving intermetallic surfaces and 66.7% of systems with large adsorbate molecules. These results demonstrate the potential of Adsorb-Agent to accelerate catalyst discovery by reducing computational costs and improving the reliability of adsorption energy predictions.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary Projects</title>
<link>https://arxiv.org/abs/2412.10133</link>
<guid>https://arxiv.org/abs/2412.10133</guid>
<content:encoded><![CDATA[
arXiv:2412.10133v2 Announce Type: replace 
Abstract: The ability to execute the test suite of a project is essential in many scenarios, e.g., to assess code quality and code coverage, to validate code changes made by developers or automated tools, and to ensure compatibility with dependencies. Despite its importance, executing the test suite of a project can be challenging in practice because different projects use different programming languages, software ecosystems, build systems, testing frameworks, and other tools. These challenges make it difficult to create a reliable, universal test execution method that works across different projects. This paper presents ExecutionAgent, an automated technique that prepares scripts for building an arbitrary project from source code and running its test cases. Inspired by the way a human developer would address this task, our approach is a large language model (LLM)-based agent that autonomously executes commands and interacts with the host system. The agent uses meta-prompting to gather guidelines on the latest technologies related to the given project, and it iteratively refines its process based on feedback from the previous steps. Our evaluation applies ExecutionAgent to 50 open-source projects that use 14 different programming languages and many different build and testing tools. The approach successfully executes the test suites of 33/50 projects, while matching the test results of ground truth test suite executions with a deviation of only 7.5%. These results improve over the best previously available technique by 6.6x. The costs imposed by the approach are reasonable, with an execution time of 74 minutes and LLM costs of USD 0.16, on average per project. We envision ExecutionAgent to serve as a valuable tool for developers, automated programming tools, and researchers that need to execute tests across a wide variety of projects.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis</title>
<link>https://arxiv.org/abs/2412.19723</link>
<guid>https://arxiv.org/abs/2412.19723</guid>
<content:encoded><![CDATA[
arXiv:2412.19723v2 Announce Type: replace 
Abstract: Graphical User Interface (GUI) agents powered by Vision-Language Models (VLMs) have demonstrated human-like computer control capability. Despite their utility in advancing digital automation, a critical bottleneck persists: collecting high-quality trajectory data for training. Common practices for collecting such data rely on human supervision or synthetic data generation through executing pre-defined tasks, which are either resource-intensive or unable to guarantee data quality. Moreover, these methods suffer from limited data diversity and significant gaps between synthetic data and real-world environments. To address these challenges, we propose OS-Genesis, a novel GUI data synthesis pipeline that reverses the conventional trajectory collection process. Instead of relying on pre-defined tasks, OS-Genesis enables agents first to perceive environments and perform step-wise interactions, then retrospectively derive high-quality tasks to enable trajectory-level exploration. A trajectory reward model is then employed to ensure the quality of the generated trajectories. We demonstrate that training GUI agents with OS-Genesis significantly improves their performance on highly challenging online benchmarks. In-depth analysis further validates OS-Genesis's efficiency and its superior data quality and diversity compared to existing synthesis methods. Our codes, data, and checkpoints are available at https://qiushisun.github.io/OS-Genesis-Home/.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI Systems Applied to tasks in Financial Services: Modeling and model risk management crews</title>
<link>https://arxiv.org/abs/2502.05439</link>
<guid>https://arxiv.org/abs/2502.05439</guid>
<content:encoded><![CDATA[
arXiv:2502.05439v2 Announce Type: replace 
Abstract: The advent of large language models has ushered in a new era of agentic systems, where artificial intelligence programs exhibit remarkable autonomous decision-making capabilities across diverse domains. This paper explores agentic system workflows in the financial services industry. In particular, we build agentic crews with human-in-the-loop module that can effectively collaborate to perform complex modeling and model risk management (MRM) tasks. The modeling crew consists of a judge agent and multiple agents who perform specific tasks such as exploratory data analysis, feature engineering, model selection/hyperparameter tuning, model training, model evaluation, and writing documentation. The MRM crew consists of a judge agent along with specialized agents who perform tasks such as checking compliance of modeling documentation, model replication, conceptual soundness, analysis of outcomes, and writing documentation. We demonstrate the effectiveness and robustness of modeling and MRM crews by presenting a series of numerical examples applied to credit card fraud detection, credit card approval, and portfolio credit risk modeling datasets.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Changing the Rules of the Game: Reasoning about Dynamic Phenomena in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2502.11785</link>
<guid>https://arxiv.org/abs/2502.11785</guid>
<content:encoded><![CDATA[
arXiv:2502.11785v2 Announce Type: replace 
Abstract: The design and application of multi-agent systems (MAS) require reasoning about the effects of modifications on their underlying structure. In particular, such changes may impact the satisfaction of system specifications and the strategic abilities of their autonomous components. In this paper, we are concerned with the problem of verifying and synthesising modifications (or updates) of MAS. We propose an extension of the Alternating-Time Temporal Logic ($\mathsf{ATL}$) that enables reasoning about the dynamics of model change, called the Logic for $\mathsf{ATL}$ Model Building ($\mathsf{LAMB}$). We show how $\mathsf{LAMB}$ can express various intuitions and ideas about the dynamics of MAS, from normative updates to mechanism design. As the main technical result, we prove that, while being strictly more expressive than $\mathsf{ATL}$, $\mathsf{LAMB}$ enjoys a P-complete model-checking procedure.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ForceGrip: Reference-Free Curriculum Learning for Realistic Grip Force Control in VR Hand Manipulation</title>
<link>https://arxiv.org/abs/2503.08061</link>
<guid>https://arxiv.org/abs/2503.08061</guid>
<content:encoded><![CDATA[
arXiv:2503.08061v3 Announce Type: replace 
Abstract: Realistic Hand manipulation is a key component of immersive virtual reality (VR), yet existing methods often rely on kinematic approach or motion-capture datasets that omit crucial physical attributes such as contact forces and finger torques. Consequently, these approaches prioritize tight, one-size-fits-all grips rather than reflecting users' intended force levels. We present ForceGrip, a deep learning agent that synthesizes realistic hand manipulation motions, faithfully reflecting the user's grip force intention. Instead of mimicking predefined motion datasets, ForceGrip uses generated training scenarios-randomizing object shapes, wrist movements, and trigger input flows-to challenge the agent with a broad spectrum of physical interactions. To effectively learn from these complex tasks, we employ a three-phase curriculum learning framework comprising Finger Positioning, Intention Adaptation, and Dynamic Stabilization. This progressive strategy ensures stable hand-object contact, adaptive force control based on user inputs, and robust handling under dynamic conditions. Additionally, a proximity reward function enhances natural finger motions and accelerates training convergence. Quantitative and qualitative evaluations reveal ForceGrip's superior force controllability and plausibility compared to state-of-the-art methods. Demo videos are available as supplementary material and the code is provided at https://han-dongheun.github.io/ForceGrip.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Marmot: Multi-Agent Reasoning for Multi-Object Self-Correcting in Improving Image-Text Alignment</title>
<link>https://arxiv.org/abs/2504.20054</link>
<guid>https://arxiv.org/abs/2504.20054</guid>
<content:encoded><![CDATA[
arXiv:2504.20054v1 Announce Type: new 
Abstract: While diffusion models excel at generating high-quality images, they often struggle with accurate counting, attributes, and spatial relationships in complex multi-object scenes. To address these challenges, we propose Marmot, a novel and generalizable framework that employs Multi-Agent Reasoning for Multi-Object Self-Correcting, enhancing image-text alignment and facilitating more coherent multi-object image editing. Our framework adopts a divide-and-conquer strategy that decomposes the self-correction task into three critical dimensions (counting, attributes, and spatial relationships), and further divided into object-level subtasks. We construct a multi-agent editing system featuring a decision-execution-verification mechanism, effectively mitigating inter-object interference and enhancing editing reliability. To resolve the problem of subtask integration, we propose a Pixel-Domain Stitching Smoother that employs mask-guided two-stage latent space optimization. This innovation enables parallel processing of subtask results, thereby enhancing runtime efficiency while eliminating multi-stage distortion accumulation. Extensive experiments demonstrate that Marmot significantly improves accuracy in object counting, attribute assignment, and spatial relationships for image generation tasks.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tempo: Application-aware LLM Serving with Mixed SLO Requirements</title>
<link>https://arxiv.org/abs/2504.20068</link>
<guid>https://arxiv.org/abs/2504.20068</guid>
<content:encoded><![CDATA[
arXiv:2504.20068v1 Announce Type: new 
Abstract: The integration of Large Language Models (LLMs) into diverse applications, ranging from interactive chatbots and cloud AIOps to intelligent agents, has introduced a wide spectrum of Service Level Objectives (SLOs) for responsiveness. These workloads include latency-sensitive requests focused on per-token latency in streaming chat, throughput-intensive requests that require rapid full responses to invoke tools, and collective requests with dynamic dependencies arising from self-reflection or agent-based reasoning. This workload diversity, amplified by unpredictable request information such as response lengths and runtime dependencies, makes existing schedulers inadequate even within their design envelopes.
  In this paper, we define service gain as the useful service delivered by completing requests. We observe that as SLO directly reflects the actual performance needs of requests, completing a request much faster than its SLO (e.g., deadline) yields limited additional service gain. Based on this insight, we introduce Tempo, the first systematic SLO-aware scheduler designed to maximize service gain across diverse LLM workloads. Tempo allocates just enough serving bandwidth to meet each SLO, maximizing residual capacity for others best-effort workloads. Instead of assuming request information or none at all, it adopts a hybrid scheduling strategy: using quantile-based response upper bounds and dependency-graph matching for conservative initial estimates, prioritizing requests by service gain density, and refining decisions online as generation progresses. Our evaluation across diverse workloads, including chat, reasoning, and agentic pipelines, shows that Tempo improves end-to-end service gain by up to 8.3$\times$ and achieves up to 10.3$\times$ SLO goodput compared to state-of-the-art designs
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenGrid: A Generalised Distributed Experimental Environmental Grid for Swarm Robotics</title>
<link>https://arxiv.org/abs/2504.20071</link>
<guid>https://arxiv.org/abs/2504.20071</guid>
<content:encoded><![CDATA[
arXiv:2504.20071v1 Announce Type: new 
Abstract: GenGrid is a novel comprehensive open-source, distributed platform intended for conducting extensive swarm robotic experiments. The modular platform is designed to run swarm robotics experiments that are compatible with different types of mobile robots ranging from Colias, Kilobot, and E puck. The platform offers programmable control over the experimental setup and its parameters and acts as a tool to collect swarm robot data, including localization, sensory feedback, messaging, and interaction. GenGrid is designed as a modular grid of attachable computing nodes that offers bidirectional communication between the robotic agent and grid nodes and within grids. The paper describes the hardware and software architecture design of the GenGrid system. Further, it discusses some common experimental studies covering multi-robot and swarm robotics to showcase the platform's use. GenGrid of 25 homogeneous cells with identical sensing and communication characteristics with a footprint of 37.5 cm X 37.5 cm, exhibits multiple capabilities with minimal resources. The open-source hardware platform is handy for running swarm experiments, including robot hopping based on multiple gradients, collective transport, shepherding, continuous pheromone deposition, and subsequent evaporation. The low-cost, modular, and open-source platform is significant in the swarm robotics research community, which is currently driven by commercial platforms that allow minimal modifications.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.20073</link>
<guid>https://arxiv.org/abs/2504.20073</guid>
<content:encoded><![CDATA[
arXiv:2504.20073v1 Announce Type: new 
Abstract: Training large language models (LLMs) as interactive agents presents unique challenges including long-horizon decision making and interacting with stochastic environment feedback. While reinforcement learning (RL) has enabled progress in static tasks, multi-turn agent RL training remains underexplored. We propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a general framework for trajectory-level agent RL, and introduce RAGEN, a modular system for training and evaluating LLM agents. Our study on three stylized environments reveals three core findings. First, our agent RL training shows a recurring mode of Echo Trap where reward variance cliffs and gradient spikes; we address this with StarPO-S, a stabilized variant with trajectory filtering, critic incorporation, and decoupled clipping. Second, we find the shaping of RL rollouts would benefit from diverse initial states, medium interaction granularity and more frequent sampling. Third, we show that without fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge through multi-turn RL and they may show shallow strategies or hallucinated thoughts. Code and environments are available at https://github.com/RAGEN-AI/RAGEN.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolution of AI in Education: Agentic Workflows</title>
<link>https://arxiv.org/abs/2504.20082</link>
<guid>https://arxiv.org/abs/2504.20082</guid>
<content:encoded><![CDATA[
arXiv:2504.20082v1 Announce Type: new 
Abstract: Artificial intelligence (AI) has transformed various aspects of education, with large language models (LLMs) driving advancements in automated tutoring, assessment, and content generation. However, conventional LLMs are constrained by their reliance on static training data, limited adaptability, and lack of reasoning. To address these limitations and foster more sustainable technological practices, AI agents have emerged as a promising new avenue for educational innovation. In this review, we examine agentic workflows in education according to four major paradigms: reflection, planning, tool use, and multi-agent collaboration. We critically analyze the role of AI agents in education through these key design paradigms, exploring their advantages, applications, and challenges. To illustrate the practical potential of agentic systems, we present a proof-of-concept application: a multi-agent framework for automated essay scoring. Preliminary results suggest this agentic approach may offer improved consistency compared to stand-alone LLMs. Our findings highlight the transformative potential of AI agents in educational settings while underscoring the need for further research into their interpretability, trustworthiness, and sustainable impact on pedagogical impact.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Awareness</title>
<link>https://arxiv.org/abs/2504.20084</link>
<guid>https://arxiv.org/abs/2504.20084</guid>
<content:encoded><![CDATA[
arXiv:2504.20084v1 Announce Type: new 
Abstract: Recent breakthroughs in artificial intelligence (AI) have brought about increasingly capable systems that demonstrate remarkable abilities in reasoning, language understanding, and problem-solving. These advancements have prompted a renewed examination of AI awareness, not as a philosophical question of consciousness, but as a measurable, functional capacity. In this review, we explore the emerging landscape of AI awareness, which includes meta-cognition (the ability to represent and reason about its own state), self-awareness (recognizing its own identity, knowledge, limitations, inter alia), social awareness (modeling the knowledge, intentions, and behaviors of other agents), and situational awareness (assessing and responding to the context in which it operates).
  First, we draw on insights from cognitive science, psychology, and computational theory to trace the theoretical foundations of awareness and examine how the four distinct forms of AI awareness manifest in state-of-the-art AI. Next, we systematically analyze current evaluation methods and empirical findings to better understand these manifestations. Building on this, we explore how AI awareness is closely linked to AI capabilities, demonstrating that more aware AI agents tend to exhibit higher levels of intelligent behaviors. Finally, we discuss the risks associated with AI awareness, including key topics in AI safety, alignment, and broader ethical concerns.
  AI awareness is a double-edged sword: it improves general capabilities, i.e., reasoning, safety, while also raises concerns around misalignment and societal risks, demanding careful oversight as AI capabilities grow. On the whole, our interdisciplinary review provides a roadmap for future research and aims to clarify the role of AI awareness in the ongoing development of intelligent machines.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoMultiAgents: A Multi-Agent Framework for Video Question Answering</title>
<link>https://arxiv.org/abs/2504.20091</link>
<guid>https://arxiv.org/abs/2504.20091</guid>
<content:encoded><![CDATA[
arXiv:2504.20091v1 Announce Type: new 
Abstract: Video Question Answering (VQA) inherently relies on multimodal reasoning, integrating visual, temporal, and linguistic cues to achieve a deeper understanding of video content. However, many existing methods rely on feeding frame-level captions into a single model, making it difficult to adequately capture temporal and interactive contexts. To address this limitation, we introduce VideoMultiAgents, a framework that integrates specialized agents for vision, scene graph analysis, and text processing. It enhances video understanding leveraging complementary multimodal reasoning from independently operating agents. Our approach is also supplemented with a question-guided caption generation, which produces captions that highlight objects, actions, and temporal transitions directly relevant to a given query, thus improving the answer accuracy. Experimental results demonstrate that our method achieves state-of-the-art performance on Intent-QA (79.0%, +6.2% over previous SOTA), EgoSchema subset (75.4%, +3.4%), and NExT-QA (79.6%, +0.4%).
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Healing Software Systems: Lessons from Nature, Powered by AI</title>
<link>https://arxiv.org/abs/2504.20093</link>
<guid>https://arxiv.org/abs/2504.20093</guid>
<content:encoded><![CDATA[
arXiv:2504.20093v1 Announce Type: new 
Abstract: As modern software systems grow in complexity and scale, their ability to autonomously detect, diagnose, and recover from failures becomes increasingly vital. Drawing inspiration from biological healing - where the human body detects damage, signals the brain, and activates targeted recovery - this paper explores the concept of self-healing software driven by artificial intelligence. We propose a novel framework that mimics this biological model system observability tools serve as sensory inputs, AI models function as the cognitive core for diagnosis and repair, and healing agents apply targeted code and test modifications. By combining log analysis, static code inspection, and AI-driven generation of patches or test updates, our approach aims to reduce downtime, accelerate debugging, and enhance software resilience. We evaluate the effectiveness of this model through case studies and simulations, comparing it against traditional manual debugging and recovery workflows. This work paves the way toward intelligent, adaptive and self-reliant software systems capable of continuous healing, akin to living organisms.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MATCHA: Can Multi-Agent Collaboration Build a Trustworthy Conversational Recommender?</title>
<link>https://arxiv.org/abs/2504.20094</link>
<guid>https://arxiv.org/abs/2504.20094</guid>
<content:encoded><![CDATA[
arXiv:2504.20094v1 Announce Type: new 
Abstract: In this paper, we propose a multi-agent collaboration framework called MATCHA for conversational recommendation system, leveraging large language models (LLMs) to enhance personalization and user engagement. Users can request recommendations via free-form text and receive curated lists aligned with their interests, preferences, and constraints. Our system introduces specialized agents for intent analysis, candidate generation, ranking, re-ranking, explainability, and safeguards. These agents collaboratively improve recommendations accuracy, diversity, and safety. On eight metrics, our model achieves superior or comparable performance to the current state-of-the-art. Through comparisons with six baseline models, our approach addresses key challenges in conversational recommendation systems for game recommendations, including: (1) handling complex, user-specific requests, (2) enhancing personalization through multi-agent collaboration, (3) empirical evaluation and deployment, and (4) ensuring safe and trustworthy interactions.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoP2C: An LLM-Based Agent Framework for Code Repository Generation from Multimodal Content in Academic Papers</title>
<link>https://arxiv.org/abs/2504.20115</link>
<guid>https://arxiv.org/abs/2504.20115</guid>
<content:encoded><![CDATA[
arXiv:2504.20115v1 Announce Type: new 
Abstract: Machine Learning (ML) research is spread through academic papers featuring rich multimodal content, including text, diagrams, and tabular results. However, translating these multimodal elements into executable code remains a challenging and time-consuming process that requires substantial ML expertise. We introduce ``Paper-to-Code'' (P2C), a novel task that transforms the multimodal content of scientific publications into fully executable code repositories, which extends beyond the existing formulation of code generation that merely converts textual descriptions into isolated code snippets. To automate the P2C process, we propose AutoP2C, a multi-agent framework based on large language models that processes both textual and visual content from research papers to generate complete code repositories. Specifically, AutoP2C contains four stages: (1) repository blueprint extraction from established codebases, (2) multimodal content parsing that integrates information from text, equations, and figures, (3) hierarchical task decomposition for structured code generation, and (4) iterative feedback-driven debugging to ensure functionality and performance. Evaluation on a benchmark of eight research papers demonstrates the effectiveness of AutoP2C, which can successfully generate executable code repositories for all eight papers, while OpenAI-o1 or DeepSeek-R1 can only produce runnable code for one paper. The code is available at https://github.com/shoushouyu/Automated-Paper-to-Code.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ResearchCodeAgent: An LLM Multi-Agent System for Automated Codification of Research Methodologies</title>
<link>https://arxiv.org/abs/2504.20117</link>
<guid>https://arxiv.org/abs/2504.20117</guid>
<content:encoded><![CDATA[
arXiv:2504.20117v1 Announce Type: new 
Abstract: In this paper we introduce ResearchCodeAgent, a novel multi-agent system leveraging large language models (LLMs) agents to automate the codification of research methodologies described in machine learning literature. The system bridges the gap between high-level research concepts and their practical implementation, allowing researchers auto-generating code of existing research papers for benchmarking or building on top-of existing methods specified in the literature with availability of partial or complete starter code. ResearchCodeAgent employs a flexible agent architecture with a comprehensive action suite, enabling context-aware interactions with the research environment. The system incorporates a dynamic planning mechanism, utilizing both short and long-term memory to adapt its approach iteratively. We evaluate ResearchCodeAgent on three distinct machine learning tasks with distinct task complexity and representing different parts of the ML pipeline: data augmentation, optimization, and data batching. Our results demonstrate the system's effectiveness and generalizability, with 46.9% of generated code being high-quality and error-free, and 25% showing performance improvements over baseline implementations. Empirical analysis shows an average reduction of 57.9% in coding time compared to manual implementation. We observe higher gains for more complex tasks. ResearchCodeAgent represents a significant step towards automating the research implementation process, potentially accelerating the pace of machine learning research.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools</title>
<link>https://arxiv.org/abs/2504.20168</link>
<guid>https://arxiv.org/abs/2504.20168</guid>
<content:encoded><![CDATA[
arXiv:2504.20168v1 Announce Type: new 
Abstract: Tool-using agents that act in the world need to be both useful and safe. Well-calibrated model confidences can be used to weigh the risk versus reward of potential actions, but prior work shows that many models are poorly calibrated. Inspired by interpretability literature exploring the internals of models, we propose a novel class of model-internal confidence estimators (MICE) to better assess confidence when calling tools. MICE first decodes from each intermediate layer of the language model using logitLens and then computes similarity scores between each layer's generation and the final output. These features are fed into a learned probabilistic classifier to assess confidence in the decoded output. On the simulated trial and error (STE) tool-calling dataset using Llama3 models, we find that MICE beats or matches the baselines on smoothed expected calibration error. Using MICE confidences to determine whether to call a tool significantly improves over strong baselines on a new metric, expected tool-calling utility. Further experiments show that MICE is sample-efficient, can generalize zero-shot to unseen APIs, and results in higher tool-calling utility in scenarios with varying risk levels. Our code is open source, available at https://github.com/microsoft/mice_for_cats.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AKIBoards: A Structure-Following Multiagent System for Predicting Acute Kidney Injury</title>
<link>https://arxiv.org/abs/2504.20368</link>
<guid>https://arxiv.org/abs/2504.20368</guid>
<content:encoded><![CDATA[
arXiv:2504.20368v1 Announce Type: new 
Abstract: Diagnostic reasoning entails a physician's local (mental) model based on an assumed or known shared perspective (global model) to explain patient observations with evidence assigned towards a clinical assessment. But in several (complex) medical situations, multiple experts work together as a team to optimize health evaluation and decision-making by leveraging different perspectives. Such consensus-driven reasoning reflects individual knowledge contributing toward a broader perspective on the patient. In this light, we introduce STRUCture-following for Multiagent Systems (STRUC-MAS), a framework automating the learning of these global models and their incorporation as prior beliefs for agents in multiagent systems (MAS) to follow. We demonstrate proof of concept with a prosocial MAS application for predicting acute kidney injuries (AKIs). In this case, we found that incorporating a global structure enabled multiple agents to achieve better performance (average precision, AP) in predicting AKI 48 hours before onset (structure-following-fine-tuned, SF-FT, AP=0.195; SF-FT-retrieval-augmented generation, SF-FT-RAG, AP=0.194) vs. baseline (non-structure-following-FT, NSF-FT, AP=0.141; NSF-FT-RAG, AP=0.180) for balanced precision-weighted-recall-weighted voting. Markedly, SF-FT agents with higher recall scores reported lower confidence levels in the initial round on true positive and false negative cases. But after explicit interactions, their confidence in their decisions increased (suggesting reinforced belief). In contrast, the SF-FT agent with the lowest recall decreased its confidence in true positive and false negative cases (suggesting a new belief). This approach suggests that learning and leveraging global structures in MAS is necessary prior to achieving competitive classification and diagnostic reasoning performance.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrashFixer: A crash resolution agent for the Linux kernel</title>
<link>https://arxiv.org/abs/2504.20412</link>
<guid>https://arxiv.org/abs/2504.20412</guid>
<content:encoded><![CDATA[
arXiv:2504.20412v1 Announce Type: new 
Abstract: Code large language models (LLMs) have shown impressive capabilities on a multitude of software engineering tasks. In particular, they have demonstrated remarkable utility in the task of code repair. However, common benchmarks used to evaluate the performance of code LLMs are often limited to small-scale settings. In this work, we build upon kGym, which shares a benchmark for system-level Linux kernel bugs and a platform to run experiments on the Linux kernel.
  This paper introduces CrashFixer, the first LLM-based software repair agent that is applicable to Linux kernel bugs. Inspired by the typical workflow of a kernel developer, we identify the key capabilities an expert developer leverages to resolve a kernel crash. Using this as our guide, we revisit the kGym platform and identify key system improvements needed to practically run LLM-based agents at the scale of the Linux kernel (50K files and 20M lines of code). We implement these changes by extending kGym to create an improved platform - called kGymSuite, which will be open-sourced. Finally, the paper presents an evaluation of various repair strategies for such complex kernel bugs and showcases the value of explicitly generating a hypothesis before attempting to fix bugs in complex systems such as the Linux kernel. We also evaluated CrashFixer's capabilities on still open bugs, and found at least two patch suggestions considered plausible to resolve the reported bug.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Algebraic Approach to Asymmetric Delegation and Polymorphic Label Inference (Technical Report)</title>
<link>https://arxiv.org/abs/2504.20432</link>
<guid>https://arxiv.org/abs/2504.20432</guid>
<content:encoded><![CDATA[
arXiv:2504.20432v1 Announce Type: new 
Abstract: Language-based information flow control (IFC) enables reasoning about and enforcing security policies in decentralized applications. While information flow properties are relatively extensional and compositional, designing expressive systems that enforce such properties remains challenging. In particular, it can be difficult to use IFC labels to model certain security assumptions, such as semi-honest agents.
  Motivated by these modeling limitations, we study the algebraic semantics of lattice-based IFC label models, and propose a semantic framework that allows formalizing asymmetric delegation, which is partial delegation of confidentiality or integrity. Our framework supports downgrading of information and ensures their safety through nonmalleable information flow (NMIF).
  To demonstrate the practicality of our framework, we design and implement a novel algorithm that statically checks NMIF and a label inference procedure that efficiently supports bounded label polymorphism, allowing users to write code generic with respect to labels.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARCS: Agentic Retrieval-Augmented Code Synthesis with Iterative Refinement</title>
<link>https://arxiv.org/abs/2504.20434</link>
<guid>https://arxiv.org/abs/2504.20434</guid>
<content:encoded><![CDATA[
arXiv:2504.20434v1 Announce Type: new 
Abstract: In supercomputing, efficient and optimized code generation is essential to leverage high-performance systems effectively. We propose Agentic Retrieval-Augmented Code Synthesis (ARCS), an advanced framework for accurate, robust, and efficient code generation, completion, and translation. ARCS integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (CoT) reasoning to systematically break down and iteratively refine complex programming tasks. An agent-based RAG mechanism retrieves relevant code snippets, while real-time execution feedback drives the synthesis of candidate solutions. This process is formalized as a state-action search tree optimization, balancing code correctness with editing efficiency. Evaluations on the Geeks4Geeks and HumanEval benchmarks demonstrate that ARCS significantly outperforms traditional prompting methods in translation and generation quality. By enabling scalable and precise code synthesis, ARCS offers transformative potential for automating and optimizing code development in supercomputing applications, enhancing computational resource utilization.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAMO:Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with Multi-Modality Observation Data</title>
<link>https://arxiv.org/abs/2504.20462</link>
<guid>https://arxiv.org/abs/2504.20462</guid>
<content:encoded><![CDATA[
arXiv:2504.20462v1 Announce Type: new 
Abstract: With the development of distributed systems, microservices and cloud native technologies have become central to modern enterprise software development. Despite bringing significant advantages, these technologies also increase system complexity and operational challenges. Traditional root cause analysis (RCA) struggles to achieve automated fault response, heavily relying on manual intervention. In recent years, large language models (LLMs) have made breakthroughs in contextual inference and domain knowledge integration, providing new solutions for Artificial Intelligence for Operations (AIOps). However, Existing LLM-based approaches face three key challenges: text input constraints, dynamic service dependency hallucinations, and context window limitations. To address these issues, we propose a tool-assisted LLM agent with multi-modality observation data, namely TAMO, for fine-grained RCA. It unifies multi-modal observational data into time-aligned representations to extract consistent features and employs specialized root cause localization and fault classification tools for perceiving the contextual environment. This approach overcomes the limitations of LLM in handling real-time changing service dependencies and raw observational data and guides LLM to generate repair strategies aligned with system contexts by structuring key information into a prompt. Experimental results show that TAMO performs well in root cause analysis when dealing with public datasets characterized by heterogeneity and common fault types, demonstrating its effectiveness.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Summary on GUI Agents with Foundation Models Enhanced by Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.20464</link>
<guid>https://arxiv.org/abs/2504.20464</guid>
<content:encoded><![CDATA[
arXiv:2504.20464v1 Announce Type: new 
Abstract: Graphical User Interface (GUI) agents, driven by Multi-modal Large Language Models (MLLMs), have emerged as a promising paradigm for enabling intelligent interaction with digital systems. This paper provides a structured summary of recent advances in GUI agents, focusing on architectures enhanced by Reinforcement Learning (RL). We first formalize GUI agent tasks as Markov Decision Processes and discuss typical execution environments and evaluation metrics. We then review the modular architecture of (M)LLM-based GUI agents, covering Perception, Planning, and Acting modules, and trace their evolution through representative works. Furthermore, we categorize GUI agent training methodologies into Prompt-based, Supervised Fine-Tuning (SFT)-based, and RL-based approaches, highlighting the progression from simple prompt engineering to dynamic policy learning via RL. Our summary illustrates how recent innovations in multimodal perception, decision reasoning, and adaptive action generation have significantly improved the generalization and robustness of GUI agents in complex real-world environments. We conclude by identifying key challenges and future directions for building more capable and reliable GUI agents.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real Transfer with Few Demonstrations</title>
<link>https://arxiv.org/abs/2504.20520</link>
<guid>https://arxiv.org/abs/2504.20520</guid>
<content:encoded><![CDATA[
arXiv:2504.20520v1 Announce Type: new 
Abstract: Learning from few demonstrations to develop policies robust to variations in robot initial positions and object poses is a problem of significant practical interest in robotics. Compared to imitation learning, which often struggles to generalize from limited samples, reinforcement learning (RL) can autonomously explore to obtain robust behaviors. Training RL agents through direct interaction with the real world is often impractical and unsafe, while building simulation environments requires extensive manual effort, such as designing scenes and crafting task-specific reward functions. To address these challenges, we propose an integrated real-to-sim-to-real pipeline that constructs simulation environments based on expert demonstrations by identifying scene objects from images and retrieving their corresponding 3D models from existing libraries. We introduce a projection-based reward model for RL policy training that is supervised by a vision-language model (VLM) using human-guided object projection relationships as prompts, with the policy further fine-tuned using expert demonstrations. In general, our work focuses on the construction of simulation environments and RL-based policy training, ultimately enabling the deployment of reliable robotic control policies in real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrAIcht, a theatrical agent that speaks like Bertolt Brecht's characters</title>
<link>https://arxiv.org/abs/2504.20552</link>
<guid>https://arxiv.org/abs/2504.20552</guid>
<content:encoded><![CDATA[
arXiv:2504.20552v1 Announce Type: new 
Abstract: This project introduces BrAIcht, an AI conversational agent that creates dialogues in the distinctive style of the famous German playwright Bertolt Brecht. BrAIcht is fine-tuned using German LeoLM, a large language model with 7 billion parameters and a modified version of the base Llama2 suitable for German language tasks. For fine-tuning, 29 plays of Bertolt Brecht and 907 of other German plays that are stylistically similar to Bertolt Brecht are used to form a more di-erse dataset. Due to the limited memory capacity, a parameterefficient fine-tuning technique called QLoRA is implemented to train the large language model. The results, based on BLEU score and perplexity, show very promising performance of BrAIcht in generating dialogues in the style of Bertolt Brecht.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Independent Learning in Performative Markov Potential Games</title>
<link>https://arxiv.org/abs/2504.20593</link>
<guid>https://arxiv.org/abs/2504.20593</guid>
<content:encoded><![CDATA[
arXiv:2504.20593v1 Announce Type: new 
Abstract: Performative Reinforcement Learning (PRL) refers to a scenario in which the deployed policy changes the reward and transition dynamics of the underlying environment. In this work, we study multi-agent PRL by incorporating performative effects into Markov Potential Games (MPGs). We introduce the notion of a performatively stable equilibrium (PSE) and show that it always exists under a reasonable sensitivity assumption. We then provide convergence results for state-of-the-art algorithms used to solve MPGs. Specifically, we show that independent policy gradient ascent (IPGA) and independent natural policy gradient (INPG) converge to an approximate PSE in the best-iterate sense, with an additional term that accounts for the performative effects. Furthermore, we show that INPG asymptotically converges to a PSE in the last-iterate sense. As the performative effects vanish, we recover the convergence rates from prior work. For a special case of our game, we provide finite-time last-iterate convergence results for a repeated retraining approach, in which agents independently optimize a surrogate objective. We conduct extensive experiments to validate our theoretical findings.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive maps are generative programs</title>
<link>https://arxiv.org/abs/2504.20628</link>
<guid>https://arxiv.org/abs/2504.20628</guid>
<content:encoded><![CDATA[
arXiv:2504.20628v1 Announce Type: new 
Abstract: Making sense of the world and acting in it relies on building simplified mental representations that abstract away aspects of reality. This principle of cognitive mapping is universal to agents with limited resources. Living organisms, people, and algorithms all face the problem of forming functional representations of their world under various computing constraints. In this work, we explore the hypothesis that human resource-efficient planning may arise from representing the world as predictably structured. Building on the metaphor of concepts as programs, we propose that cognitive maps can take the form of generative programs that exploit predictability and redundancy, in contrast to directly encoding spatial layouts. We use a behavioral experiment to show that people who navigate in structured spaces rely on modular planning strategies that align with programmatic map representations. We describe a computational model that predicts human behavior in a variety of structured scenarios. This model infers a small distribution over possible programmatic cognitive maps conditioned on human prior knowledge of the world, and uses this distribution to generate resource-efficient plans. Our models leverages a Large Language Model as an embedding of human priors, implicitly learned through training on a vast corpus of human data. Our model demonstrates improved computational efficiency, requires drastically less memory, and outperforms unstructured planning algorithms with cognitive constraints at predicting human behavior, suggesting that human planning strategies rely on programmatic cognitive maps.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asymptotic Fair Division: Chores Are Easier Than Goods</title>
<link>https://arxiv.org/abs/2504.20704</link>
<guid>https://arxiv.org/abs/2504.20704</guid>
<content:encoded><![CDATA[
arXiv:2504.20704v1 Announce Type: new 
Abstract: When dividing items among agents, two of the most widely studied fairness notions are envy-freeness and proportionality. We consider a setting where $m$ chores are allocated to $n$ agents and the disutility of each chore for each agent is drawn from a probability distribution. We show that an envy-free allocation exists with high probability provided that $m \ge 2n$, and moreover, $m$ must be at least $n+\Theta(n)$ in order for the existence to hold. On the other hand, we prove that a proportional allocation is likely to exist as long as $m = \omega(1)$, and this threshold is asymptotically tight. Our results reveal a clear contrast with the allocation of goods, where a larger number of items is necessary to ensure existence for both notions.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Effect of Time Preferences on the Price of Anarchy</title>
<link>https://arxiv.org/abs/2504.20774</link>
<guid>https://arxiv.org/abs/2504.20774</guid>
<content:encoded><![CDATA[
arXiv:2504.20774v1 Announce Type: new 
Abstract: This paper examines the impact of agents' myopic optimization on the efficiency of systems comprised by many selfish agents. In contrast to standard congestion games where agents interact in a one-shot fashion, in our model each agent chooses an infinite sequence of actions and maximizes the total reward stream discounted over time under different ways of computing present values. Our model assumes that actions consume common resources that get congested, and the action choice by an agent affects the completion times of actions chosen by other agents, which in turn affects the time rewards are accrued and their discounted value. This is a mean-field game, where an agent's reward depends on the decisions of the other agents through the resulting action completion times. For this type of game we define stationary equilibria, and analyze their existence and price of anarchy (PoA). Overall, we find that the PoA depends entirely on the type of discounting rather than its specific parameters. For exponential discounting, myopic behaviour leads to extreme inefficiency: the PoA is infinity for any value of the discount parameter. For power law discounting, such inefficiency is greatly reduced and the PoA is 2 whenever stationary equilibria exist. This matches the PoA when there is no discounting and players maximize long-run average rewards. Additionally, we observe that exponential discounting may introduce unstable equilibria in learning algorithms, if action completion times are interdependent. In contrast, under no discounting all equilibria are stable.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using LLMs in Generating Design Rationale for Software Architecture Decisions</title>
<link>https://arxiv.org/abs/2504.20781</link>
<guid>https://arxiv.org/abs/2504.20781</guid>
<content:encoded><![CDATA[
arXiv:2504.20781v1 Announce Type: new 
Abstract: Design Rationale (DR) for software architecture decisions refers to the reasoning underlying architectural choices, which provides valuable insights into the different phases of the architecting process throughout software development. However, in practice, DR is often inadequately documented due to a lack of motivation and effort from developers. With the recent advancements in Large Language Models (LLMs), their capabilities in text comprehension, reasoning, and generation may enable the generation and recovery of DR for architecture decisions. In this study, we evaluated the performance of LLMs in generating DR for architecture decisions. First, we collected 50 Stack Overflow (SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture decisions to construct a dataset of 100 architecture-related problems. Then, we selected five LLMs to generate DR for the architecture decisions with three prompting strategies, including zero-shot, chain of thought (CoT), and LLM-based agents. With the DR provided by human experts as ground truth, the Precision of LLM-generated DR with the three prompting strategies ranges from 0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389. Additionally, 64.45% to 69.42% of the arguments of DR not mentioned by human experts are also helpful, 4.12% to 4.87% of the arguments have uncertain correctness, and 1.59% to 3.24% of the arguments are potentially misleading. Based on the results, we further discussed the pros and cons of the three prompting strategies and the strengths and limitations of the DR generated by LLMs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Human Feedback into a Reinforcement Learning-Based Framework for Adaptive User Interfaces</title>
<link>https://arxiv.org/abs/2504.20782</link>
<guid>https://arxiv.org/abs/2504.20782</guid>
<content:encoded><![CDATA[
arXiv:2504.20782v1 Announce Type: new 
Abstract: Adaptive User Interfaces (AUI) play a crucial role in modern software applications by dynamically adjusting interface elements to accommodate users' diverse and evolving needs. However, existing adaptation strategies often lack real-time responsiveness. Reinforcement Learning (RL) has emerged as a promising approach for addressing complex, sequential adaptation challenges, enabling adaptive systems to learn optimal policies based on previous adaptation experiences. Although RL has been applied to AUIs,integrating RL agents effectively within user interactions remains a challenge.
  In this paper, we enhance a RL-based Adaptive User Interface adaption framework by incorporating personalized human feedback directly into the leaning process. Unlike prior approaches that rely on a single pre-trained RL model, our approach trains a unique RL agent for each user, allowing individuals to actively shape their personal RL agent's policy, potentially leading to more personalized and responsive UI adaptations. To evaluate this approach, we conducted an empirical study to assess the impact of integrating human feedback into the RL-based Adaptive User Interface adaption framework and its effect on User Experience (UX). The study involved 33 participants interacting with AUIs incorporating human feedback and non-adaptive user interfaces in two domains: an e-learning platform and a trip-planning application. The results suggest that incorporating human feedback into RL-driven adaptations significantly enhances UX, offering promising directions for advancing adaptive capabilities and user-centered design in AUIs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Feedback Help in Bandits with Arm Erasures?</title>
<link>https://arxiv.org/abs/2504.20894</link>
<guid>https://arxiv.org/abs/2504.20894</guid>
<content:encoded><![CDATA[
arXiv:2504.20894v1 Announce Type: new 
Abstract: We study a distributed multi-armed bandit (MAB) problem over arm erasure channels, motivated by the increasing adoption of MAB algorithms over communication-constrained networks. In this setup, the learner communicates the chosen arm to play to an agent over an erasure channel with probability $\epsilon \in [0,1)$; if an erasure occurs, the agent continues pulling the last successfully received arm; the learner always observes the reward of the arm pulled. In past work, we considered the case where the agent cannot convey feedback to the learner, and thus the learner does not know whether the arm played is the requested or the last successfully received one. In this paper, we instead consider the case where the agent can send feedback to the learner on whether the arm request was received, and thus the learner exactly knows which arm was played. Surprisingly, we prove that erasure feedback does not improve the worst-case regret upper bound order over the previously studied no-feedback setting. In particular, we prove a regret lower bound of $\Omega(\sqrt{KT} + K / (1 - \epsilon))$, where $K$ is the number of arms and $T$ the time horizon, that matches no-feedback upper bounds up to logarithmic factors. We note however that the availability of feedback enables simpler algorithm designs that may achieve better constants (albeit not better order) regret bounds; we design one such algorithm and evaluate its performance numerically.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CBM-RAG: Demonstrating Enhanced Interpretability in Radiology Report Generation with Multi-Agent RAG and Concept Bottleneck Models</title>
<link>https://arxiv.org/abs/2504.20898</link>
<guid>https://arxiv.org/abs/2504.20898</guid>
<content:encoded><![CDATA[
arXiv:2504.20898v1 Announce Type: new 
Abstract: Advancements in generative Artificial Intelligence (AI) hold great promise for automating radiology workflows, yet challenges in interpretability and reliability hinder clinical adoption. This paper presents an automated radiology report generation framework that combines Concept Bottleneck Models (CBMs) with a Multi-Agent Retrieval-Augmented Generation (RAG) system to bridge AI performance with clinical explainability. CBMs map chest X-ray features to human-understandable clinical concepts, enabling transparent disease classification. Meanwhile, the RAG system integrates multi-agent collaboration and external knowledge to produce contextually rich, evidence-based reports. Our demonstration showcases the system's ability to deliver interpretable predictions, mitigate hallucinations, and generate high-quality, tailored reports with an interactive interface addressing accuracy, trust, and usability challenges. This framework provides a pathway to improving diagnostic consistency and empowering radiologists with actionable insights.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling AI-Human Collaboration as a Multi-Agent Adaptation</title>
<link>https://arxiv.org/abs/2504.20903</link>
<guid>https://arxiv.org/abs/2504.20903</guid>
<content:encoded><![CDATA[
arXiv:2504.20903v1 Announce Type: new 
Abstract: We develop an agent-based simulation to formalize AI-human collaboration as a function of task structure, advancing a generalizable framework for strategic decision-making in organizations. Distinguishing between heuristic-based human adaptation and rule-based AI search, we model interactions across modular (parallel) and sequenced (interdependent) tasks using an NK model. Our results reveal that in modular tasks, AI often substitutes for humans - delivering higher payoffs unless human expertise is very high, and the AI search space is either narrowly focused or extremely broad. In sequenced tasks, interesting complementarities emerge. When an expert human initiates the search and AI subsequently refines it, aggregate performance is maximized. Conversely, when AI leads, excessive heuristic refinement by the human can reduce payoffs. We also show that even "hallucinatory" AI - lacking memory or structure - can improve outcomes when augmenting low-capability humans by helping escape local optima. These results yield a robust implication: the effectiveness of AI-human collaboration depends less on context or industry, and more on the underlying task structure. By elevating task decomposition as the central unit of analysis, our model provides a transferable lens for strategic decision-making involving humans and an agentic AI across diverse organizational settings.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploiting inter-agent coupling information for efficient reinforcement learning of cooperative LQR</title>
<link>https://arxiv.org/abs/2504.20927</link>
<guid>https://arxiv.org/abs/2504.20927</guid>
<content:encoded><![CDATA[
arXiv:2504.20927v1 Announce Type: new 
Abstract: Developing scalable and efficient reinforcement learning algorithms for cooperative multi-agent control has received significant attention over the past years. Existing literature has proposed inexact decompositions of local Q-functions based on empirical information structures between the agents. In this paper, we exploit inter-agent coupling information and propose a systematic approach to exactly decompose the local Q-function of each agent. We develop an approximate least square policy iteration algorithm based on the proposed decomposition and identify two architectures to learn the local Q-function for each agent. We establish that the worst-case sample complexity of the decomposition is equal to the centralized case and derive necessary and sufficient graphical conditions on the inter-agent couplings to achieve better sample efficiency. We demonstrate the improved sample efficiency and computational efficiency on numerical examples.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improvements of Dark Experience Replay and Reservoir Sampling towards Better Balance between Consolidation and Plasticity</title>
<link>https://arxiv.org/abs/2504.20932</link>
<guid>https://arxiv.org/abs/2504.20932</guid>
<content:encoded><![CDATA[
arXiv:2504.20932v1 Announce Type: new 
Abstract: Continual learning is the one of the most essential abilities for autonomous agents, which can incrementally learn daily-life skills. For this ultimate goal, a simple but powerful method, dark experience replay (DER), has been proposed recently. DER mitigates catastrophic forgetting, in which the skills acquired in the past are unintentionally forgotten, by stochastically storing the streaming data in a reservoir sampling (RS) buffer and by relearning them or retaining the past outputs for them. However, since DER considers multiple objectives, it will not function properly without appropriate weighting of them. In addition, the ability to retain past outputs inhibits learning if the past outputs are incorrect due to distribution shift or other effects. This is due to a tradeoff between memory consolidation and plasticity. The tradeoff is hidden even in the RS buffer, which gradually stops storing new data for new skills in it as data is continuously passed to it. To alleviate the tradeoff and achieve better balance, this paper proposes improvement strategies to each of DER and RS. Specifically, DER is improved with automatic adaptation of weights, block of replaying erroneous data, and correction of past outputs. RS is also improved with generalization of acceptance probability, stratification of plural buffers, and intentional omission of unnecessary data. These improvements are verified through multiple benchmarks including regression, classification, and reinforcement learning problems. As a result, the proposed methods achieve steady improvements in learning performance by balancing the memory consolidation and plasticity.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opinion-Driven Decision-Making for Multi-Robot Navigation through Narrow Corridors</title>
<link>https://arxiv.org/abs/2504.20947</link>
<guid>https://arxiv.org/abs/2504.20947</guid>
<content:encoded><![CDATA[
arXiv:2504.20947v1 Announce Type: new 
Abstract: We propose an opinion-driven navigation framework for multi-robot traversal through a narrow corridor. Our approach leverages a multi-agent decision-making model known as the Nonlinear Opinion Dynamics (NOD) to address the narrow corridor passage problem, formulated as a multi-robot navigation game. By integrating the NOD model with a multi-robot path planning algorithm, we demonstrate that the framework effectively reduces the likelihood of deadlocks during corridor traversal. To ensure scalability with an increasing number of robots, we introduce a game reduction technique that enables efficient coordination in larger groups. Extensive simulation studies are conducted to validate the effectiveness of the proposed approach.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM Security</title>
<link>https://arxiv.org/abs/2504.20965</link>
<guid>https://arxiv.org/abs/2504.20965</guid>
<content:encoded><![CDATA[
arXiv:2504.20965v1 Announce Type: new 
Abstract: We introduce AegisLLM, a cooperative multi-agent defense against adversarial attacks and information leakage. In AegisLLM, a structured workflow of autonomous agents - orchestrator, deflector, responder, and evaluator - collaborate to ensure safe and compliant LLM outputs, while self-improving over time through prompt optimization. We show that scaling agentic reasoning system at test-time - both by incorporating additional agent roles and by leveraging automated prompt optimization (such as DSPy)- substantially enhances robustness without compromising model utility. This test-time defense enables real-time adaptability to evolving attacks, without requiring model retraining. Comprehensive evaluations across key threat scenarios, including unlearning and jailbreaking, demonstrate the effectiveness of AegisLLM. On the WMDP unlearning benchmark, AegisLLM achieves near-perfect unlearning with only 20 training examples and fewer than 300 LM calls. For jailbreaking benchmarks, we achieve 51% improvement compared to the base model on StrongReject, with false refusal rates of only 7.9% on PHTest compared to 18-55% for comparable methods. Our results highlight the advantages of adaptive, agentic reasoning over static defenses, establishing AegisLLM as a strong runtime alternative to traditional approaches based on model modifications. Code is available at https://github.com/zikuicai/aegisllm
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XPG-RL: Reinforcement Learning with Explainable Priority Guidance for Efficiency-Boosted Mechanical Search</title>
<link>https://arxiv.org/abs/2504.20969</link>
<guid>https://arxiv.org/abs/2504.20969</guid>
<content:encoded><![CDATA[
arXiv:2504.20969v1 Announce Type: new 
Abstract: Mechanical search (MS) in cluttered environments remains a significant challenge for autonomous manipulators, requiring long-horizon planning and robust state estimation under occlusions and partial observability. In this work, we introduce XPG-RL, a reinforcement learning framework that enables agents to efficiently perform MS tasks through explainable, priority-guided decision-making based on raw sensory inputs. XPG-RL integrates a task-driven action prioritization mechanism with a learned context-aware switching strategy that dynamically selects from a discrete set of action primitives such as target grasping, occlusion removal, and viewpoint adjustment. Within this strategy, a policy is optimized to output adaptive threshold values that govern the discrete selection among action primitives. The perception module fuses RGB-D inputs with semantic and geometric features to produce a structured scene representation for downstream decision-making. Extensive experiments in both simulation and real-world settings demonstrate that XPG-RL consistently outperforms baseline methods in task success rates and motion efficiency, achieving up to 4.5$\times$ higher efficiency in long-horizon tasks. These results underscore the benefits of integrating domain knowledge with learnable decision-making policies for robust and efficient robotic manipulation.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TesserAct: Learning 4D Embodied World Models</title>
<link>https://arxiv.org/abs/2504.20995</link>
<guid>https://arxiv.org/abs/2504.20995</guid>
<content:encoded><![CDATA[
arXiv:2504.20995v1 Announce Type: new 
Abstract: This paper presents an effective approach for learning novel 4D embodied world models, which predict the dynamic evolution of 3D scenes over time in response to an embodied agent's actions, providing both spatial and temporal consistency. We propose to learn a 4D world model by training on RGB-DN (RGB, Depth, and Normal) videos. This not only surpasses traditional 2D models by incorporating detailed shape, configuration, and temporal changes into their predictions, but also allows us to effectively learn accurate inverse dynamic models for an embodied agent. Specifically, we first extend existing robotic manipulation video datasets with depth and normal information leveraging off-the-shelf models. Next, we fine-tune a video generation model on this annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for each frame. We then present an algorithm to directly convert generated RGB, Depth, and Normal videos into a high-quality 4D scene of the world. Our method ensures temporal and spatial coherence in 4D scene predictions from embodied scenarios, enables novel view synthesis for embodied environments, and facilitates policy learning that significantly outperforms those derived from prior video-based world models.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Efficient Exploration by Large Language Model Agents</title>
<link>https://arxiv.org/abs/2504.20997</link>
<guid>https://arxiv.org/abs/2504.20997</guid>
<content:encoded><![CDATA[
arXiv:2504.20997v1 Announce Type: new 
Abstract: A burgeoning area within reinforcement learning (RL) is the design of sequential decision-making agents centered around large language models (LLMs). While autonomous decision-making agents powered by modern LLMs could facilitate numerous real-world applications, such successes demand agents that are capable of data-efficient RL. One key obstacle to achieving data efficiency in RL is exploration, a challenge that we demonstrate many recent proposals for LLM agent designs struggle to contend with. Meanwhile, classic algorithms from the RL literature known to gracefully address exploration require technical machinery that can be challenging to operationalize in purely natural language settings. In this work, rather than relying on finetuning or in-context learning to coax LLMs into implicitly imitating a RL algorithm, we illustrate how LLMs can be used to explicitly implement an existing RL algorithm (Posterior Sampling for Reinforcement Learning) whose capacity for statistically-efficient exploration is already well-studied. We offer empirical results demonstrating how our LLM-based implementation of a known, data-efficient RL algorithm can be considerably more effective in natural language tasks that demand prudent exploration.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models</title>
<link>https://arxiv.org/abs/2310.03903</link>
<guid>https://arxiv.org/abs/2310.03903</guid>
<content:encoded><![CDATA[
arXiv:2310.03903v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated emergent common-sense reasoning and Theory of Mind (ToM) capabilities, making them promising candidates for developing coordination agents. This study introduces the LLM-Coordination Benchmark, a novel benchmark for analyzing LLMs in the context of Pure Coordination Settings, where agents must cooperate to maximize gains. Our benchmark evaluates LLMs through two distinct tasks. The first is Agentic Coordination, where LLMs act as proactive participants in four pure coordination games. The second is Coordination Question Answering (CoordQA), which tests LLMs on 198 multiple-choice questions across these games to evaluate three key abilities: Environment Comprehension, ToM Reasoning, and Joint Planning. Results from Agentic Coordination experiments reveal that LLM-Agents excel in multi-agent coordination settings where decision-making primarily relies on environmental variables but face challenges in scenarios requiring active consideration of partners' beliefs and intentions. The CoordQA experiments further highlight significant room for improvement in LLMs' Theory of Mind reasoning and joint planning capabilities. Zero-Shot Coordination (ZSC) experiments in the Agentic Coordination setting demonstrate that LLM agents, unlike RL methods, exhibit robustness to unseen partners. These findings indicate the potential of LLMs as Agents in pure coordination setups and underscore areas for improvement. Code Available at https://github.com/eric-ai-lab/llm_coordination.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOP-Former: A Multi-Agent Transformer Approach for the Team Orienteering Problem</title>
<link>https://arxiv.org/abs/2311.18662</link>
<guid>https://arxiv.org/abs/2311.18662</guid>
<content:encoded><![CDATA[
arXiv:2311.18662v3 Announce Type: replace 
Abstract: Route planning for a fleet of vehicles is an important task in applications such as package delivery, surveillance, or transportation, often integrated within larger Intelligent Transportation Systems (ITS). This problem is commonly formulated as a Vehicle Routing Problem (VRP) known as the Team Orienteering Problem (TOP). Existing solvers for this problem primarily rely on either linear programming, which provides accurate solutions but requires computation times that grow with the size of the problem, or heuristic methods, which typically find suboptimal solutions in a shorter time. In this paper, we introduce TOP-Former, a multi-agent route planning neural network designed to efficiently and accurately solve the Team Orienteering Problem. The proposed algorithm is based on a centralized Transformer neural network capable of learning to encode the scenario (modeled as a graph) and analyze the complete context of all agents to deliver fast, precise, and collaborative solutions. Unlike other neural network-based approaches that adopt a more local perspective, TOP-Former is trained to understand the global situation of the vehicle fleet and generate solutions that maximize long-term expected returns. Extensive experiments demonstrate that the presented system outperforms most state-of-the-art methods in terms of both accuracy and computation speed.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Role of Infrastructure in Collaborative Perception</title>
<link>https://arxiv.org/abs/2410.11259</link>
<guid>https://arxiv.org/abs/2410.11259</guid>
<content:encoded><![CDATA[
arXiv:2410.11259v2 Announce Type: replace 
Abstract: Collaborative Perception (CP) is a process in which an ego agent receives and fuses sensor information from surrounding vehicles and infrastructure to enhance its perception capability. To evaluate the need for infrastructure equipped with sensors, extensive and quantitative analysis of the role of infrastructure data in CP is crucial, yet remains underexplored. To address this gap, we first quantitatively assess the importance of infrastructure data in existing vehicle-centric CP, where the ego agent is a vehicle. Furthermore, we compare vehicle-centric CP with infra-centric CP, where the ego agent is now the infrastructure, to evaluate the effectiveness of each approach. Our results demonstrate that incorporating infrastructure data improves 3D detection accuracy by up to 10.30%, and infra-centric CP shows enhanced noise robustness and increases accuracy by up to 46.47% compared with vehicle-centric CP.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLA Management in Reconfigurable Multi-Agent RAG: A Systems Approach to Question Answering</title>
<link>https://arxiv.org/abs/2412.06832</link>
<guid>https://arxiv.org/abs/2412.06832</guid>
<content:encoded><![CDATA[
arXiv:2412.06832v2 Announce Type: replace 
Abstract: Retrieval Augmented Generation (RAG) enables Large Language Models (LLMs) to generalize to new information by decoupling reasoning capabilities from static knowledge bases. Traditional RAG enhancements have explored vertical scaling-assigning subtasks to specialized modules-and horizontal scaling-replicating tasks across multiple agents-to improve performance. However, real-world applications impose diverse Service Level Agreements (SLAs) and Quality of Service (QoS) requirements, involving trade-offs among objectives such as reducing cost, ensuring answer quality, and adhering to specific operational constraints.
  In this work, we present a systems-oriented approach to multi-agent RAG tailored for real-world Question Answering (QA) applications. By integrating task-specific non-functional requirements-such as answer quality, cost, and latency-into the system, we enable dynamic reconfiguration to meet diverse SLAs. Our method maps these Service Level Objectives (SLOs) to system-level parameters, allowing the generation of optimal results within specified resource constraints.
  We conduct a case study in the QA domain, demonstrating how dynamic re-orchestration of a multi-agent RAG system can effectively manage the trade-off between answer quality and cost. By adjusting the system based on query intent and operational conditions, we systematically balance performance and resource utilization. This approach allows the system to meet SLOs for various query types, showcasing its practicality for real-world applications.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open FinLLM Leaderboard: Towards Financial AI Readiness</title>
<link>https://arxiv.org/abs/2501.10963</link>
<guid>https://arxiv.org/abs/2501.10963</guid>
<content:encoded><![CDATA[
arXiv:2501.10963v2 Announce Type: replace 
Abstract: Financial large language models (FinLLMs) with multimodal capabilities are envisioned to revolutionize applications across business, finance, accounting, and auditing. However, real-world adoption requires robust benchmarks of FinLLMs' and FinAgents' performance. Maintaining an open leaderboard is crucial for encouraging innovative adoption and improving model effectiveness. In collaboration with Linux Foundation and Hugging Face, we create an open FinLLM leaderboard, which serves as an open platform for assessing and comparing AI models' performance on a wide spectrum of financial tasks. By demoncratizing access to advances of financial knowledge and intelligence, a chatbot or agent may enhance the analytical capabilities of the general public to a professional level within a few months of usage. This open leaderboard welcomes contributions from academia, open-source community, industry, and stakeholders. In particular, we encourage contributions of new datasets, tasks, and models for continual update. Through fostering a collaborative and open ecosystem, we seek to promote financial AI readiness.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EgoAgent: A Joint Predictive Agent Model in Egocentric Worlds</title>
<link>https://arxiv.org/abs/2502.05857</link>
<guid>https://arxiv.org/abs/2502.05857</guid>
<content:encoded><![CDATA[
arXiv:2502.05857v2 Announce Type: replace 
Abstract: This paper addresses the task of learning an agent model behaving like humans, which can jointly perceive, predict, and act in egocentric worlds. Previous methods usually train separate models for these three abilities, which prevents them from learning from each other. In this paper, we propose a joint predictive agent model, named EgoAgent, that simultaneously learns to represent the world, predict future states, and take reasonable actions within a single transformer. EgoAgent introduces two innovations to learn from the causal and temporally intertwined nature of these abilities: (1) Interleaved sequential modeling of states and actions with the causal attention mechanism, and (2) A joint embedding-action-prediction architecture featuring temporal asymmetric predictor-observer branches. Integrating these designs based on JEPA, EgoAgent unifies these capabilities in a cohesive learning framework. Comprehensive evaluations of EgoAgent on representative tasks such as image classification, egocentric future state prediction, and 3D human motion prediction tasks demonstrate the superiority of our method. The code and trained model will be released for reproducibility.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Principled Multi-Agent Task Agnostic Exploration</title>
<link>https://arxiv.org/abs/2502.08365</link>
<guid>https://arxiv.org/abs/2502.08365</guid>
<content:encoded><![CDATA[
arXiv:2502.08365v2 Announce Type: replace 
Abstract: In reinforcement learning, we typically refer to task-agnostic exploration when we aim to explore the environment without access to the task specification a priori. In a single-agent setting the problem has been extensively studied and mostly understood. A popular approach cast the task-agnostic objective as maximizing the entropy of the state distribution induced by the agent's policy, from which principles and methods follows. In contrast, little is known about task-agnostic exploration in multi-agent settings, which are ubiquitous in the real world. How should different agents explore in the presence of others? In this paper, we address this question through a generalization to multiple agents of the problem of maximizing the state distribution entropy. First, we investigate alternative formulations, highlighting respective positives and negatives. Then, we present a scalable, decentralized, trust-region policy search algorithm to address the problem in practical settings. Finally, we provide proof of concept experiments to both corroborate the theoretical findings and pave the way for task-agnostic exploration in challenging multi-agent settings.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM-Powered Agent for Physiological Data Analysis: A Case Study on PPG-based Heart Rate Estimation</title>
<link>https://arxiv.org/abs/2502.12836</link>
<guid>https://arxiv.org/abs/2502.12836</guid>
<content:encoded><![CDATA[
arXiv:2502.12836v2 Announce Type: replace 
Abstract: Large language models (LLMs) are revolutionizing healthcare by improving diagnosis, patient care, and decision support through interactive communication. More recently, they have been applied to analyzing physiological time-series like wearable data for health insight extraction. Existing methods embed raw numerical sequences directly into prompts, which exceeds token limits and increases computational costs. Additionally, some studies integrated features extracted from time-series in textual prompts or applied multimodal approaches. However, these methods often produce generic and unreliable outputs due to LLMs' limited analytical rigor and inefficiency in interpreting continuous waveforms. In this paper, we develop an LLM-powered agent for physiological time-series analysis aimed to bridge the gap in integrating LLMs with well-established analytical tools. Built on the OpenCHA, an open-source LLM-powered framework, our agent powered by OpenAI's GPT-3.5-turbo model features an orchestrator that integrates user interaction, data sources, and analytical tools to generate accurate health insights. To evaluate its effectiveness, we implement a case study on heart rate (HR) estimation from Photoplethysmogram (PPG) signals using a dataset of PPG and Electrocardiogram (ECG) recordings in a remote health monitoring study. The agent's performance is benchmarked against OpenAI GPT-4o-mini and GPT-4o, with ECG serving as the gold standard for HR estimation. Results demonstrate that our agent significantly outperforms benchmark models by achieving lower error rates and more reliable HR estimations. The agent implementation is publicly available on GitHub.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LocAgent: Graph-Guided LLM Agents for Code Localization</title>
<link>https://arxiv.org/abs/2503.09089</link>
<guid>https://arxiv.org/abs/2503.09089</guid>
<content:encoded><![CDATA[
arXiv:2503.09089v2 Announce Type: replace 
Abstract: Code localization--identifying precisely where in a codebase changes need to be made--is a fundamental yet challenging task in software maintenance. Existing approaches struggle to efficiently navigate complex codebases when identifying relevant code sections. The challenge lies in bridging natural language problem descriptions with the appropriate code elements, often requiring reasoning across hierarchical structures and multiple dependencies. We introduce LocAgent, a framework that addresses code localization through graph-based representation. By parsing codebases into directed heterogeneous graphs, LocAgent creates a lightweight representation that captures code structures (files, classes, functions) and their dependencies (imports, invocations, inheritance), enabling LLM agents to effectively search and locate relevant entities through powerful multi-hop reasoning. Experimental results on real-world benchmarks demonstrate that our approach significantly enhances accuracy in code localization. Notably, our method with the fine-tuned Qwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA proprietary models at greatly reduced cost (approximately 86% reduction), reaching up to 92.7% accuracy on file-level localization while improving downstream GitHub issue resolution success rates by 12% for multiple attempts (Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear-Quadratic Mean-Field Reinforcement Learning: Convergence of Policy Gradient Methods</title>
<link>https://arxiv.org/abs/1910.04295</link>
<guid>https://arxiv.org/abs/1910.04295</guid>
<content:encoded><![CDATA[
arXiv:1910.04295v2 Announce Type: replace-cross 
Abstract: We investigate reinforcement learning in the setting of Markov decision processes for a large number of exchangeable agents interacting in a mean field manner. Applications include, for example, the control of a large number of robots communicating through a central unit dispatching the optimal policy computed by maximizing an aggregate reward. An approximate solution is obtained by learning the optimal policy of a generic agent interacting with the statistical distribution of the states and actions of the other agents. We first provide a full analysis this discrete-time mean field control problem. We then rigorously prove the convergence of exact and model-free policy gradient methods in a mean-field linear-quadratic setting and establish bounds on the rates of convergence. We also provide graphical evidence of the convergence based on implementations of our algorithms.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Molecular Learning Dynamics</title>
<link>https://arxiv.org/abs/2504.10560</link>
<guid>https://arxiv.org/abs/2504.10560</guid>
<content:encoded><![CDATA[
arXiv:2504.10560v2 Announce Type: replace-cross 
Abstract: We apply the physics-learning duality to molecular systems by complementing the physical description of interacting particles with a dual learning description, where each particle is modeled as an agent minimizing a loss function. In the traditional physics framework, the equations of motion are derived from the Lagrangian function, while in the learning framework, the same equations emerge from learning dynamics driven by the agent loss function. The loss function depends on scalar quantities that describe invariant properties of all other agents or particles. To demonstrate this approach, we first infer the loss functions of oxygen and hydrogen directly from a dataset generated by the CP2K physics-based simulation of water molecules. We then employ the loss functions to develop a learning-based simulation of water molecules, which achieves comparable accuracy while being significantly more computationally efficient than standard physics-based simulations.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RepliBench: Evaluating the autonomous replication capabilities of language model agents</title>
<link>https://arxiv.org/abs/2504.18565</link>
<guid>https://arxiv.org/abs/2504.18565</guid>
<content:encoded><![CDATA[
arXiv:2504.18565v1 Announce Type: new 
Abstract: Uncontrollable autonomous replication of language model agents poses a critical safety risk. To better understand this risk, we introduce RepliBench, a suite of evaluations designed to measure autonomous replication capabilities. RepliBench is derived from a decomposition of these capabilities covering four core domains: obtaining resources, exfiltrating model weights, replicating onto compute, and persisting on this compute for long periods. We create 20 novel task families consisting of 86 individual tasks. We benchmark 5 frontier models, and find they do not currently pose a credible threat of self-replication, but succeed on many components and are improving rapidly. Models can deploy instances from cloud compute providers, write self-propagating programs, and exfiltrate model weights under simple security setups, but struggle to pass KYC checks or set up robust and persistent agent deployments. Overall the best model we evaluated (Claude 3.7 Sonnet) has a >50% pass@10 score on 15/20 task families, and a >50% pass@10 score for 9/20 families on the hardest variants. These findings suggest autonomous replication capability could soon emerge with improvements in these remaining areas or with human assistance.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks</title>
<link>https://arxiv.org/abs/2504.18575</link>
<guid>https://arxiv.org/abs/2504.18575</guid>
<content:encoded><![CDATA[
arXiv:2504.18575v1 Announce Type: new 
Abstract: Web navigation AI agents use language-and-vision foundation models to enhance productivity but these models are known to be susceptible to indirect prompt injections that get them to follow instructions different from the legitimate user's. Existing explorations of this threat applied to web agents often focus on a single isolated adversarial goal, test with injected instructions that are either too easy or not truly malicious, and often give the adversary unreasonable access. In order to better focus adversarial research, we construct a new benchmark called WASP (Web Agent Security against Prompt injection attacks) that introduces realistic web agent hijacking objectives and an isolated environment to test them in that does not affect real users or the live web. As part of WASP, we also develop baseline attacks against three popular web agentic systems (VisualWebArena, Claude Computer Use, and Operator) instantiated with various state-of-the-art models. Our evaluation shows that even AI agents backed by models with advanced reasoning capabilities and by models with instruction hierarchy mitigations are susceptible to low-effort human-written prompt injections. However, the realistic objectives in WASP also allow us to observe that agents are currently not capable enough to complete the goals of attackers end-to-end. Agents begin executing the adversarial instruction between 16 and 86% of the time but only achieve the goal between 0 and 17% of the time. Based on these findings, we argue that adversarial researchers should demonstrate stronger attacks that more consistently maintain control over the agent given realistic constraints on the adversary's power.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defending Against Intelligent Attackers at Large Scales</title>
<link>https://arxiv.org/abs/2504.18577</link>
<guid>https://arxiv.org/abs/2504.18577</guid>
<content:encoded><![CDATA[
arXiv:2504.18577v1 Announce Type: new 
Abstract: We investigate the scale of attack and defense mathematically in the context of AI's possible effect on cybersecurity. For a given target today, highly scaled cyber attacks such as from worms or botnets typically all fail or all succeed. Here, we consider the effect of scale if those attack agents were intelligent and creative enough to act independently such that each attack attempt was different from the others or such that attackers could learn from their successes and failures. We find that small increases in the number or quality of defenses can compensate for exponential increases in the number of independent attacks and for exponential speedups.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Large Language Models to Reason via EM Policy Gradient</title>
<link>https://arxiv.org/abs/2504.18587</link>
<guid>https://arxiv.org/abs/2504.18587</guid>
<content:encoded><![CDATA[
arXiv:2504.18587v1 Announce Type: new 
Abstract: Recently, foundation models such as OpenAI's O1 and O3, along with DeepSeek's R1, have demonstrated strong reasoning capacities and problem-solving skills acquired through large-scale reinforcement learning (RL), with wide applications in mathematics, coding, science, intelligent agents, and virtual assistants. In this work, we introduce an off-policy reinforcement learning algorithm, EM Policy Gradient, aimed at enhancing LLM reasoning by optimizing expected return over reasoning trajectories. We frame the reasoning task as an Expectation-Maximization (EM) optimization problem, alternating between sampling diverse rationale trajectories and performing reward-guided fine-tuning. Unlike PPO and GRPO, which rely on complex importance weights and heuristic clipping, our method provides a simpler, more principled off-policy policy gradient approach, eliminating these complexities while maintaining strong performance. We evaluate the effectiveness of EM Policy Gradient on the GSM8K and MATH (HARD) datasets, where it achieves performance comparable to or slightly surpassing the state-of-the-art GRPO, while offering additional advantages in scalability, simplicity, and reasoning conciseness. Moreover, models fine-tuned with our method exhibit cognitive behaviors, such as sub-problem decomposition, self-verification, and backtracking, highlighting its potential to enhance both the interpretability and robustness of LLM reasoning.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Philosophic Turn for AI Agents: Replacing centralized digital rhetoric with decentralized truth-seeking</title>
<link>https://arxiv.org/abs/2504.18601</link>
<guid>https://arxiv.org/abs/2504.18601</guid>
<content:encoded><![CDATA[
arXiv:2504.18601v1 Announce Type: new 
Abstract: In the face of rapidly advancing AI technology, individuals will increasingly rely on AI agents to navigate life's growing complexities, raising critical concerns about maintaining both human agency and autonomy. This paper addresses a fundamental dilemma posed by AI decision-support systems: the risk of either becoming overwhelmed by complex decisions, thus losing agency, or having autonomy compromised by externally controlled choice architectures reminiscent of ``nudging'' practices. While the ``nudge'' framework, based on the use of choice-framing to guide individuals toward presumed beneficial outcomes, initially appeared to preserve liberty, at AI-driven scale, it threatens to erode autonomy. To counteract this risk, the paper proposes a philosophic turn in AI design. AI should be constructed to facilitate decentralized truth-seeking and open-ended inquiry, mirroring the Socratic method of philosophical dialogue. By promoting individual and collective adaptive learning, such AI systems would empower users to maintain control over their judgments, augmenting their agency without undermining autonomy. The paper concludes by outlining essential features for autonomy-preserving AI systems, sketching a path toward AI systems that enhance human judgment rather than undermine it.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Personalizing Quantum Computing Education: An Evolutionary LLM-Powered Approach</title>
<link>https://arxiv.org/abs/2504.18603</link>
<guid>https://arxiv.org/abs/2504.18603</guid>
<content:encoded><![CDATA[
arXiv:2504.18603v1 Announce Type: new 
Abstract: Quantum computing education faces significant challenges due to its complexity and the limitations of current tools; this paper introduces a novel Intelligent Teaching Assistant for quantum computing education and details its evolutionary design process. The system combines a knowledge-graph-augmented architecture with two specialized Large Language Model (LLM) agents: a Teaching Agent for dynamic interaction, and a Lesson Planning Agent for lesson plan generation. The system is designed to adapt to individual student needs, with interactions meticulously tracked and stored in a knowledge graph. This graph represents student actions, learning resources, and relationships, aiming to enable reasoning about effective learning pathways. We describe the implementation of the system, highlighting the challenges encountered and the solutions implemented, including introducing a dual-agent architecture where tasks are separated, all coordinated through a central knowledge graph that maintains system awareness, and a user-facing tag system intended to mitigate LLM hallucination and improve user control. Preliminary results illustrate the system's potential to capture rich interaction data, dynamically adapt lesson plans based on student feedback via a tag system in simulation, and facilitate context-aware tutoring through the integrated knowledge graph, though systematic evaluation is required.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proof-of-TBI -- Fine-Tuned Vision Language Model Consortium and OpenAI-o3 Reasoning LLM-Based Medical Diagnosis Support System for Mild Traumatic Brain Injury (TBI) Prediction</title>
<link>https://arxiv.org/abs/2504.18671</link>
<guid>https://arxiv.org/abs/2504.18671</guid>
<content:encoded><![CDATA[
arXiv:2504.18671v1 Announce Type: new 
Abstract: Mild Traumatic Brain Injury (TBI) detection presents significant challenges due to the subtle and often ambiguous presentation of symptoms in medical imaging, making accurate diagnosis a complex task. To address these challenges, we propose Proof-of-TBI, a medical diagnosis support system that integrates multiple fine-tuned vision-language models with the OpenAI-o3 reasoning large language model (LLM). Our approach fine-tunes multiple vision-language models using a labeled dataset of TBI MRI scans, training them to diagnose TBI symptoms effectively. The predictions from these models are aggregated through a consensus-based decision-making process. The system evaluates the predictions from all fine-tuned vision language models using the OpenAI-o3 reasoning LLM, a model that has demonstrated remarkable reasoning performance, to produce the most accurate final diagnosis. The LLM Agents orchestrates interactions between the vision-language models and the reasoning LLM, managing the final decision-making process with transparency, reliability, and automation. This end-to-end decision-making workflow combines the vision-language model consortium with the OpenAI-o3 reasoning LLM, enabled by custom prompt engineering by the LLM agents. The prototype for the proposed platform was developed in collaboration with the U.S. Army Medical Research team in Newport News, Virginia, incorporating five fine-tuned vision-language models. The results demonstrate the transformative potential of combining fine-tuned vision-language model inputs with the OpenAI-o3 reasoning LLM to create a robust, secure, and highly accurate diagnostic system for mild TBI prediction. To the best of our knowledge, this research represents the first application of fine-tuned vision-language models integrated with a reasoning LLM for TBI prediction tasks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MODP: Multi Objective Directional Prompting</title>
<link>https://arxiv.org/abs/2504.18722</link>
<guid>https://arxiv.org/abs/2504.18722</guid>
<content:encoded><![CDATA[
arXiv:2504.18722v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have led to their popularity across multiple use-cases. However, prompt engineering, the process for optimally utilizing such models, remains approximation-driven and subjective. Most of the current research on prompt engineering focuses on task-specific optimization, while neglecting the behavior of the LLM under consideration during prompt development. This paper introduces MODP -- Multi Objective Directional Prompting, a framework based on two key concepts: 1) multi-objectivity: the importance of considering an LLM's intrinsic behavior as an additional objective in prompt development, and 2) directional prompting: a metrics-driven method for prompt engineering to ensure development of robust and high-precision prompts. We demonstrate the effectiveness of our proposed ideas on a summarization task, using a synthetically created dataset, achieving a 26% performance gain over initial prompts. Finally, we apply MODP to develop prompts for Dell's Next Best Action support tool, which is now in production and is used by more than 10,000 internal support agents and serving millions of customers worldwide.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Vision for Auto Research with LLM Agents</title>
<link>https://arxiv.org/abs/2504.18765</link>
<guid>https://arxiv.org/abs/2504.18765</guid>
<content:encoded><![CDATA[
arXiv:2504.18765v1 Announce Type: new 
Abstract: This paper introduces Agent-Based Auto Research, a structured multi-agent framework designed to automate, coordinate, and optimize the full lifecycle of scientific research. Leveraging the capabilities of large language models (LLMs) and modular agent collaboration, the system spans all major research phases, including literature review, ideation, methodology planning, experimentation, paper writing, peer review response, and dissemination. By addressing issues such as fragmented workflows, uneven methodological expertise, and cognitive overload, the framework offers a systematic and scalable approach to scientific inquiry. Preliminary explorations demonstrate the feasibility and potential of Auto Research as a promising paradigm for self-improving, AI-driven research processes.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation</title>
<link>https://arxiv.org/abs/2504.18805</link>
<guid>https://arxiv.org/abs/2504.18805</guid>
<content:encoded><![CDATA[
arXiv:2504.18805v1 Announce Type: new 
Abstract: Generating engaging, accurate short-form videos from scientific papers is challenging due to content complexity and the gap between expert authors and readers. Existing end-to-end methods often suffer from factual inaccuracies and visual artifacts, limiting their utility for scientific dissemination. To address these issues, we propose SciTalk, a novel multi-LLM agentic framework, grounding videos in various sources, such as text, figures, visual styles, and avatars. Inspired by content creators' workflows, SciTalk uses specialized agents for content summarization, visual scene planning, and text and layout editing, and incorporates an iterative feedback mechanism where video agents simulate user roles to give feedback on generated videos from previous iterations and refine generation prompts. Experimental evaluations show that SciTalk outperforms simple prompting methods in generating scientifically accurate and engaging content over the refined loop of video generation. Although preliminary results are still not yet matching human creators' quality, our framework provides valuable insights into the challenges and benefits of feedback-driven video generation. Our code, data, and generated videos will be publicly available.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative to Agentic AI: Survey, Conceptualization, and Challenges</title>
<link>https://arxiv.org/abs/2504.18875</link>
<guid>https://arxiv.org/abs/2504.18875</guid>
<content:encoded><![CDATA[
arXiv:2504.18875v1 Announce Type: new 
Abstract: Agentic Artificial Intelligence (AI) builds upon Generative AI (GenAI). It constitutes the next major step in the evolution of AI with much stronger reasoning and interaction capabilities that enable more autonomous behavior to tackle complex tasks. Since the initial release of ChatGPT (3.5), Generative AI has seen widespread adoption, giving users firsthand experience. However, the distinction between Agentic AI and GenAI remains less well understood. To address this gap, our survey is structured in two parts. In the first part, we compare GenAI and Agentic AI using existing literature, discussing their key characteristics, how Agentic AI remedies limitations of GenAI, and the major steps in GenAI's evolution toward Agentic AI. This section is intended for a broad audience, including academics in both social sciences and engineering, as well as industry professionals. It provides the necessary insights to comprehend novel applications that are possible with Agentic AI but not with GenAI. In the second part, we deep dive into novel aspects of Agentic AI, including recent developments and practical concerns such as defining agents. Finally, we discuss several challenges that could serve as a future research agenda, while cautioning against risks that can emerge when exceeding human intelligence.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Welfare and Beyond in Multi-Agent Contracts</title>
<link>https://arxiv.org/abs/2504.18876</link>
<guid>https://arxiv.org/abs/2504.18876</guid>
<content:encoded><![CDATA[
arXiv:2504.18876v1 Announce Type: new 
Abstract: A principal delegates a project to a team $S$ from a pool of $n$ agents. The project's value if all agents in $S$ exert costly effort is $f(S)$. To incentivize the agents to participate, the principal assigns each agent $i\in S$ a share $\rho_i\in [0,1]$ of the project's final value (i.e., designs $n$ linear contracts). The shares must be feasible -- their sum should not exceed $1$. It is well-understood how to design these contracts to maximize the principal's own expected utility, but what if the goal is to coordinate the agents toward maximizing social welfare?
  We initiate a systematic study of multi-agent contract design with objectives beyond principal's utility, including welfare maximization, for various classes of value functions $f$. Our exploration reveals an arguably surprising fact: If $f$ is up to XOS in the complement-free hierarchy of functions, then the optimal principal's utility is a constant-fraction of the optimal welfare. This is in stark contrast to the much larger welfare-utility gaps in auction design, and no longer holds above XOS in the hierarchy, where the gap can be unbounded.
  A constant bound on the welfare-utility gap immediately implies that existing algorithms for designing contracts with approximately-optimal principal's utility also guarantee approximately-optimal welfare. The downside of reducing welfare to utility is the loss of large constants. To obtain better guarantees, we develop polynomial-time algorithms directly for welfare, for different classes of value functions. These include a tight $2$-approximation to the optimal welfare for symmetric XOS functions.
  Finally, we extend our analysis beyond welfare to the project's value under general feasibility constraints. Our results immediately translate to budgeted welfare and utility.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reshaping MOFs Text Mining with a Dynamic Multi-Agent Framework of Large Language Agents</title>
<link>https://arxiv.org/abs/2504.18880</link>
<guid>https://arxiv.org/abs/2504.18880</guid>
<content:encoded><![CDATA[
arXiv:2504.18880v1 Announce Type: new 
Abstract: The mining of synthesis conditions for metal-organic frameworks (MOFs) is a significant focus in materials science. However, identifying the precise synthesis conditions for specific MOFs within the vast array of possibilities presents a considerable challenge. Large Language Models (LLMs) offer a promising solution to this problem. We leveraged the capabilities of LLMs, specifically gpt-4o-mini, as core agents to integrate various MOF-related agents, including synthesis, attribute, and chemical information agents. This integration culminated in the development of MOFh6, an LLM tool designed to streamline the MOF synthesis process. MOFh6 allows users to query in multiple formats, such as submitting scientific literature, or inquiring about specific MOF codes or structural properties. The tool analyzes these queries to provide optimal synthesis conditions and generates model files for density functional theory pre modeling. We believe MOFh6 will enhance efficiency in the MOF synthesis of all researchers.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI in Embodied Systems: System-Level Analysis of Performance, Efficiency and Scalability</title>
<link>https://arxiv.org/abs/2504.18945</link>
<guid>https://arxiv.org/abs/2504.18945</guid>
<content:encoded><![CDATA[
arXiv:2504.18945v1 Announce Type: new 
Abstract: Embodied systems, where generative autonomous agents engage with the physical world through integrated perception, cognition, action, and advanced reasoning powered by large language models (LLMs), hold immense potential for addressing complex, long-horizon, multi-objective tasks in real-world environments. However, deploying these systems remains challenging due to prolonged runtime latency, limited scalability, and heightened sensitivity, leading to significant system inefficiencies.
  In this paper, we aim to understand the workload characteristics of embodied agent systems and explore optimization solutions. We systematically categorize these systems into four paradigms and conduct benchmarking studies to evaluate their task performance and system efficiency across various modules, agent scales, and embodied tasks. Our benchmarking studies uncover critical challenges, such as prolonged planning and communication latency, redundant agent interactions, complex low-level control mechanisms, memory inconsistencies, exploding prompt lengths, sensitivity to self-correction and execution, sharp declines in success rates, and reduced collaboration efficiency as agent numbers increase. Leveraging these profiling insights, we suggest system optimization strategies to improve the performance, efficiency, and scalability of embodied agents across different paradigms. This paper presents the first system-level analysis of embodied AI agents, and explores opportunities for advancing future embodied system design.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparks: Multi-Agent Artificial Intelligence Model Discovers Protein Design Principles</title>
<link>https://arxiv.org/abs/2504.19017</link>
<guid>https://arxiv.org/abs/2504.19017</guid>
<content:encoded><![CDATA[
arXiv:2504.19017v1 Announce Type: new 
Abstract: Advances in artificial intelligence (AI) promise autonomous discovery, yet most systems still resurface knowledge latent in their training data. We present Sparks, a multi-modal multi-agent AI model that executes the entire discovery cycle that includes hypothesis generation, experiment design and iterative refinement to develop generalizable principles and a report without human intervention. Applied to protein science, Sparks uncovered two previously unknown phenomena: (i) a length-dependent mechanical crossover whereby beta-sheet-biased peptides surpass alpha-helical ones in unfolding force beyond ~80 residues, establishing a new design principle for peptide mechanics; and (ii) a chain-length/secondary-structure stability map revealing unexpectedly robust beta-sheet-rich architectures and a "frustration zone" of high variance in mixed alpha/beta folds. These findings emerged from fully self-directed reasoning cycles that combined generative sequence design, high-accuracy structure prediction and physics-aware property models, with paired generation-and-reflection agents enforcing self-correction and reproducibility. The key result is that Sparks can independently conduct rigorous scientific inquiry and identify previously unknown scientific principles.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries</title>
<link>https://arxiv.org/abs/2504.19110</link>
<guid>https://arxiv.org/abs/2504.19110</guid>
<content:encoded><![CDATA[
arXiv:2504.19110v1 Announce Type: new 
Abstract: Recent progress in large language models (LLMs) has shown promise in formal theorem proving, yet existing benchmarks remain limited to isolated, static proof tasks, failing to capture the iterative, engineering-intensive workflows of real-world formal mathematics libraries. Motivated by analogous advances in software engineering, we introduce the paradigm of Automated Proof Engineering (APE), which aims to automate proof engineering tasks such as feature addition, proof refactoring, and bug fixing using LLMs. To facilitate research in this direction, we present APE-Bench I, the first realistic benchmark built from real-world commit histories of Mathlib4, featuring diverse file-level tasks described in natural language and verified via a hybrid approach combining the Lean compiler and LLM-as-a-Judge. We further develop Eleanstic, a scalable parallel verification infrastructure optimized for proof checking across multiple versions of Mathlib. Empirical results on state-of-the-art LLMs demonstrate strong performance on localized edits but substantial degradation on handling complex proof engineering. This work lays the foundation for developing agentic workflows in proof engineering, with future benchmarks targeting multi-file coordination, project-scale verification, and autonomous agents capable of planning, editing, and repairing formal libraries.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AndroidGen: Building an Android Language Agent under Data Scarcity</title>
<link>https://arxiv.org/abs/2504.19298</link>
<guid>https://arxiv.org/abs/2504.19298</guid>
<content:encoded><![CDATA[
arXiv:2504.19298v1 Announce Type: new 
Abstract: Large language models have opened up a world of possibilities for various NLP tasks, sparking optimism for the future. Despite their potential, LLMs have yet to be widely used as agents on real mobile devices. The main challenge is the need for high-quality data sources. Time constraints and labor intensity often hinder human annotation. On the other hand, existing LLMs exhibit inadequate completion rates and need a robust data filtration strategy. Given these challenges, we develop a framework called AndroidGen to enhance the capabilities of LLM-based agents under data scarcity. In addition, we leverage AndroidGen to collect trajectories given human tasks and train open-source LLMs on these trajectories to develop an open-source mobile agent without manually labeled trajectories. We extensively evaluate AndroidGen with AndroidWorld, AitW, and various popular applications, demonstrating its improvements and revealing potential areas for future improvement. Code, model, and data are available at https://github.com/THUDM/AndroidGen.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese</title>
<link>https://arxiv.org/abs/2504.19314</link>
<guid>https://arxiv.org/abs/2504.19314</guid>
<content:encoded><![CDATA[
arXiv:2504.19314v1 Announce Type: new 
Abstract: As large language models (LLMs) evolve into tool-using agents, the ability to browse the web in real-time has become a critical yardstick for measuring their reasoning and retrieval competence. Existing benchmarks such as BrowseComp concentrate on English and overlook the linguistic, infrastructural, and censorship-related complexities of other major information ecosystems -- most notably Chinese. To address this gap, we introduce BrowseComp-ZH, a high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning 11 diverse domains. Each question is reverse-engineered from a short, objective, and easily verifiable answer (e.g., a date, number, or proper noun). A two-stage quality control protocol is applied to strive for high question difficulty and answer uniqueness. We benchmark over 20 state-of-the-art language models and agentic search systems on our proposed BrowseComp-ZH. Despite their strong conversational and retrieval capabilities, most models struggle severely: a large number achieve accuracy rates below 10%, and only a handful exceed 20%. Even the best-performing system, OpenAI's DeepResearch, reaches just 42.9%. These results demonstrate the considerable difficulty of BrowseComp-ZH, where success demands not only effective retrieval strategies, but also sophisticated reasoning and information reconciliation -- capabilities that current models still struggle to master. Our dataset, construction guidelines, and benchmark results have been publicly released at https://github.com/PALIN2018/BrowseComp-ZH.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logic-Based Artificial Intelligence Algorithms Supporting Categorical Semantics</title>
<link>https://arxiv.org/abs/2504.19320</link>
<guid>https://arxiv.org/abs/2504.19320</guid>
<content:encoded><![CDATA[
arXiv:2504.19320v1 Announce Type: new 
Abstract: This paper seeks to apply categorical logic to the design of artificial intelligent agents that reason symbolically about objects more richly structured than sets. Using Johnstone's sequent calculus of terms- and formulae-in-context, we develop forward chaining and normal form algorithms for reasoning about objects in cartesian categories with the rules for Horn logic. We also adapt first-order unification to support multi-sorted theories, contexts, and fragments of first-order logic. The significance of these reformulations rests in the fact that they can be applied to reasoning about objects in semantic categories that do not support classical logic or even all its connectives.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Doxing via the Lens: Revealing Privacy Leakage in Image Geolocation for Agentic Multi-Modal Large Reasoning Model</title>
<link>https://arxiv.org/abs/2504.19373</link>
<guid>https://arxiv.org/abs/2504.19373</guid>
<content:encoded><![CDATA[
arXiv:2504.19373v1 Announce Type: new 
Abstract: The increasing capabilities of agentic multi-modal large reasoning models, such as ChatGPT o3, have raised critical concerns regarding privacy leakage through inadvertent image geolocation. In this paper, we conduct the first systematic and controlled study on the potential privacy risks associated with visual reasoning abilities of ChatGPT o3. We manually collect and construct a dataset comprising 50 real-world images that feature individuals alongside privacy-relevant environmental elements, capturing realistic and sensitive scenarios for analysis. Our experimental evaluation reveals that ChatGPT o3 can predict user locations with high precision, achieving street-level accuracy (within one mile) in 60% of cases. Through analysis, we identify key visual cues, including street layout and front yard design, that significantly contribute to the model inference success. Additionally, targeted occlusion experiments demonstrate that masking critical features effectively mitigates geolocation accuracy, providing insights into potential defense mechanisms. Our findings highlight an urgent need for privacy-aware development for agentic multi-modal large reasoning models, particularly in applications involving private imagery.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Observational Learning with a Budget</title>
<link>https://arxiv.org/abs/2504.19396</link>
<guid>https://arxiv.org/abs/2504.19396</guid>
<content:encoded><![CDATA[
arXiv:2504.19396v1 Announce Type: new 
Abstract: We consider a model of Bayesian observational learning in which a sequence of agents receives a private signal about an underlying binary state of the world. Each agent makes a decision based on its own signal and its observations of previous agents. A central planner seeks to improve the accuracy of these signals by allocating a limited budget to enhance signal quality across agents. We formulate and analyze the budget allocation problem and propose two optimal allocation strategies. At least one of these strategies is shown to maximize the probability of achieving a correct information cascade.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetric Policy Design for Multi-Agent Dispatch Coordination in Supply Chains</title>
<link>https://arxiv.org/abs/2504.19397</link>
<guid>https://arxiv.org/abs/2504.19397</guid>
<content:encoded><![CDATA[
arXiv:2504.19397v1 Announce Type: new 
Abstract: We study a decentralized dispatch coordination problem in a multi-agent supply chain setting with shared logistics capacity. We propose symmetric (identical) dispatch strategies for all agents, enabling efficient coordination without centralized control. Using a common information approach, we derive a dynamic programming solution that computes optimal symmetric dispatch strategies by transforming the multi-agent problem into a tractable dynamic program on the agents common information state. Simulation results demonstrate that our method significantly reduces coordination cost compared to baseline heuristics, including belief-based strategies and an always-dispatch policy. These findings highlight the benefits of combining symmetric strategy design with a common information-based dynamic programming framework for improving multi-agent coordination performance.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory</title>
<link>https://arxiv.org/abs/2504.19413</link>
<guid>https://arxiv.org/abs/2504.19413</guid>
<content:encoded><![CDATA[
arXiv:2504.19413v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable prowess in generating contextually coherent responses, yet their fixed context windows pose fundamental challenges for maintaining consistency over prolonged multi-session dialogues. We introduce Mem0, a scalable memory-centric architecture that addresses this issue by dynamically extracting, consolidating, and retrieving salient information from ongoing conversations. Building on this foundation, we further propose an enhanced variant that leverages graph-based memory representations to capture complex relational structures among conversational elements. Through comprehensive evaluations on LOCOMO benchmark, we systematically compare our approaches against six baseline categories: (i) established memory-augmented systems, (ii) retrieval-augmented generation (RAG) with varying chunk sizes and k-values, (iii) a full-context approach that processes the entire conversation history, (iv) an open-source memory solution, (v) a proprietary model system, and (vi) a dedicated memory management platform. Empirical results show that our methods consistently outperform all existing memory systems across four question categories: single-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26% relative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with graph memory achieves around 2% higher overall score than the base configuration. Beyond accuracy gains, we also markedly reduce computational overhead compared to full-context method. In particular, Mem0 attains a 91% lower p95 latency and saves more than 90% token cost, offering a compelling balance between advanced reasoning capabilities and practical deployment constraints. Our findings highlight critical role of structured, persistent memory mechanisms for long-term conversational coherence, paving the way for more reliable and efficient LLM-driven AI agents.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bearing-Only Tracking and Circumnavigation of a Fast Time-Varied Velocity Target Utilising an LSTM</title>
<link>https://arxiv.org/abs/2504.19463</link>
<guid>https://arxiv.org/abs/2504.19463</guid>
<content:encoded><![CDATA[
arXiv:2504.19463v1 Announce Type: new 
Abstract: Bearing-only tracking, localisation, and circumnavigation is a problem in which a single or a group of agents attempts to track a target while circumnavigating it at a fixed distance using only bearing measurements. While previous studies have addressed scenarios involving stationary targets or those moving with an unknown constant velocity, the challenge of accurately tracking a target moving with a time-varying velocity remains open. This paper presents an approach utilising a Long Short-Term Memory (LSTM) based estimator for predicting the target's position and velocity. We also introduce a corresponding control strategy. When evaluated against previously proposed estimation and circumnavigation approaches, our approach demonstrates significantly lower control and estimation errors across various time-varying velocity scenarios. Additionally, we illustrate the effectiveness of the proposed method in tracking targets with a double integrator nonholonomic system dynamics that mimic real-world systems.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Automated Reinforcement Learning Reward Design Framework with Large Language Model for Cooperative Platoon Coordination</title>
<link>https://arxiv.org/abs/2504.19480</link>
<guid>https://arxiv.org/abs/2504.19480</guid>
<content:encoded><![CDATA[
arXiv:2504.19480v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has demonstrated excellent decision-making potential in platoon coordination problems. However, due to the variability of coordination goals, the complexity of the decision problem, and the time-consumption of trial-and-error in manual design, finding a well performance reward function to guide RL training to solve complex platoon coordination problems remains challenging. In this paper, we formally define the Platoon Coordination Reward Design Problem (PCRDP), extending the RL-based cooperative platoon coordination problem to incorporate automated reward function generation. To address PCRDP, we propose a Large Language Model (LLM)-based Platoon coordination Reward Design (PCRD) framework, which systematically automates reward function discovery through LLM-driven initialization and iterative optimization. In this method, LLM first initializes reward functions based on environment code and task requirements with an Analysis and Initial Reward (AIR) module, and then iteratively optimizes them based on training feedback with an evolutionary module. The AIR module guides LLM to deepen their understanding of code and tasks through a chain of thought, effectively mitigating hallucination risks in code generation. The evolutionary module fine-tunes and reconstructs the reward function, achieving a balance between exploration diversity and convergence stability for training. To validate our approach, we establish six challenging coordination scenarios with varying complexity levels within the Yangtze River Delta transportation network simulation. Comparative experimental results demonstrate that RL agents utilizing PCRD-generated reward functions consistently outperform human-engineered reward functions, achieving an average of 10\% higher performance metrics in all scenarios.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolution of Cooperation in LLM-Agent Societies: A Preliminary Study Using Different Punishment Strategies</title>
<link>https://arxiv.org/abs/2504.19487</link>
<guid>https://arxiv.org/abs/2504.19487</guid>
<content:encoded><![CDATA[
arXiv:2504.19487v1 Announce Type: new 
Abstract: The evolution of cooperation has been extensively studied using abstract mathematical models and simulations. Recent advances in Large Language Models (LLM) and the rise of LLM agents have demonstrated their ability to perform social reasoning, thus providing an opportunity to test the emergence of norms in more realistic agent-based simulations with human-like reasoning using natural language. In this research, we investigate whether the cooperation dynamics presented in Boyd and Richerson's model persist in a more realistic simulation of the diner's dilemma using LLM agents compared to the abstract mathematical nature in the work of Boyd and Richerson. Our findings indicate that agents follow the strategies defined in the Boyd and Richerson model, and explicit punishment mechanisms drive norm emergence, reinforcing cooperative behaviour even when the agent strategy configuration varies. Our results suggest that LLM-based Multi-Agent System simulations, in fact, can replicate the evolution of cooperation predicted by the traditional mathematical models. Moreover, our simulations extend beyond the mathematical models by integrating natural language-driven reasoning and a pairwise imitation method for strategy adoption, making them a more realistic testbed for cooperative behaviour in MASs.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Reinforcement Learning for QoS-Aware Load Balancing in Open Radio Access Networks</title>
<link>https://arxiv.org/abs/2504.19499</link>
<guid>https://arxiv.org/abs/2504.19499</guid>
<content:encoded><![CDATA[
arXiv:2504.19499v1 Announce Type: new 
Abstract: Next-generation wireless cellular networks are expected to provide unparalleled Quality-of-Service (QoS) for emerging wireless applications, necessitating strict performance guarantees, e.g., in terms of link-level data rates. A critical challenge in meeting these QoS requirements is the prevention of cell congestion, which involves balancing the load to ensure sufficient radio resources are available for each cell to serve its designated User Equipments (UEs). In this work, a novel QoS-aware Load Balancing (LB) approach is developed to optimize the performance of Guaranteed Bit Rate (GBR) and Best Effort (BE) traffic in a multi-band Open Radio Access Network (O-RAN) under QoS and resource constraints. The proposed solution builds on Graph Reinforcement Learning (GRL), a powerful framework at the intersection of Graph Neural Network (GNN) and RL. The QoS-aware LB is modeled as a Markov Decision Process, with states represented as graphs. QoS consideration are integrated into both state representations and reward signal design. The LB agent is then trained using an off-policy dueling Deep Q Network (DQN) that leverages a GNN-based architecture. This design ensures the LB policy is invariant to the ordering of nodes (UE or cell), flexible in handling various network sizes, and capable of accounting for spatial node dependencies in LB decisions. Performance of the GRL-based solution is compared with two baseline methods. Results show substantial performance gains, including a $53\%$ reduction in QoS violations and a fourfold increase in the 5th percentile rate for BE traffic.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Masked Point-Entity Contrast for Open-Vocabulary 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2504.19500</link>
<guid>https://arxiv.org/abs/2504.19500</guid>
<content:encoded><![CDATA[
arXiv:2504.19500v1 Announce Type: new 
Abstract: Open-vocabulary 3D scene understanding is pivotal for enhancing physical intelligence, as it enables embodied agents to interpret and interact dynamically within real-world environments. This paper introduces MPEC, a novel Masked Point-Entity Contrastive learning method for open-vocabulary 3D semantic segmentation that leverages both 3D entity-language alignment and point-entity consistency across different point cloud views to foster entity-specific feature representations. Our method improves semantic discrimination and enhances the differentiation of unique instances, achieving state-of-the-art results on ScanNet for open-vocabulary 3D semantic segmentation and demonstrating superior zero-shot scene understanding capabilities. Extensive fine-tuning experiments on 8 datasets, spanning from low-level perception to high-level reasoning tasks, showcase the potential of learned 3D features, driving consistent performance gains across varied 3D scene understanding tasks. Project website: https://mpec-3d.github.io/
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training</title>
<link>https://arxiv.org/abs/2504.19565</link>
<guid>https://arxiv.org/abs/2504.19565</guid>
<content:encoded><![CDATA[
arXiv:2504.19565v1 Announce Type: new 
Abstract: The rapid progress of large language models (LLMs) in biomedical research has underscored the limitations of existing open-source annotated scientific corpora, which are often insufficient in quantity and quality. Addressing the challenge posed by the complex hierarchy of biomedical knowledge, we propose a knowledge-driven, multi-agent framework for scientific corpus distillation tailored for LLM training in the biomedical domain. Central to our approach is a collaborative multi-agent architecture, where specialized agents, each guided by the Medical Subject Headings (MeSH) hierarchy, work in concert to autonomously extract, synthesize, and self-evaluate high-quality textual data from vast scientific literature. These agents collectively generate and refine domain-specific question-answer pairs, ensuring comprehensive coverage and consistency with biomedical ontologies while minimizing manual involvement. Extensive experimental results show that language models trained on our multi-agent distilled datasets achieve notable improvements in biomedical question-answering tasks, outperforming both strong life sciences LLM baselines and advanced proprietary models. Notably, our AI-Ready dataset enables Llama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger scale. Detailed ablation studies and case analyses further validate the effectiveness and synergy of each agent within the framework, highlighting the potential of multi-agent collaboration in biomedical LLM training.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Time-dependent Risk-aware distributed Multi-Agent Path Finder based on A*</title>
<link>https://arxiv.org/abs/2504.19593</link>
<guid>https://arxiv.org/abs/2504.19593</guid>
<content:encoded><![CDATA[
arXiv:2504.19593v1 Announce Type: new 
Abstract: Multi-Agent Path-Finding (MAPF) focuses on the collaborative planning of paths for multiple agents within shared spaces, aiming for collision-free navigation. Conventional planning methods often overlook the presence of other agents, which can result in conflicts. In response, this article introduces the A$^*_+$T algorithm, a distributed approach that improves coordination among agents by anticipating their positions based on their movement speeds. The algorithm also considers dynamic obstacles, assessing potential collisions with respect to observed speeds and trajectories, thereby facilitating collision-free path planning in environments populated by other agents and moving objects. It incorporates a risk layer surrounding both dynamic and static entities, enhancing its utility in real-world applications. Each agent functions autonomously while being mindful of the paths chosen by others, effectively addressing the complexities inherent in multi-agent situations. The performance of A$^*_+$T has been rigorously tested in the Gazebo simulation environment and benchmarked against established approaches such as CBS, ECBS, and SIPP. Furthermore, the algorithm has shown competence in single-agent experiments, with results demonstrating its effectiveness in managing dynamic obstacles and affirming its practical relevance across various scenarios.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Stochastic Learning Over Adaptive Competing Networks</title>
<link>https://arxiv.org/abs/2504.19635</link>
<guid>https://arxiv.org/abs/2504.19635</guid>
<content:encoded><![CDATA[
arXiv:2504.19635v1 Announce Type: new 
Abstract: This paper studies a stochastic dynamic game between two competing teams, each consisting of a network of collaborating agents. Unlike fully cooperative settings, where all agents share a common objective, each team in this game aims to minimize its own distinct objective. In the adversarial setting, their objectives could be conflicting as in zero-sum games. Throughout the competition, agents share strategic information within their own team while simultaneously inferring and adapting to the strategies of the opposing team. We propose diffusion learning algorithms to address two important classes of this network game: i) a zero-sum game characterized by weak cross-team subgraph interactions, and ii) a general non-zero-sum game exhibiting strong cross-team subgraph interactions. We analyze the stability performance of the proposed algorithms under reasonable assumptions and illustrate the theoretical results through experiments on Cournot team competition and decentralized GAN training.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2504.19678</link>
<guid>https://arxiv.org/abs/2504.19678</guid>
<content:encoded><![CDATA[
arXiv:2504.19678v1 Announce Type: new 
Abstract: Large language models and autonomous AI agents have evolved rapidly, resulting in a diverse array of evaluation benchmarks, frameworks, and collaboration protocols. However, the landscape remains fragmented and lacks a unified taxonomy or comprehensive survey. Therefore, we present a side-by-side comparison of benchmarks developed between 2019 and 2025 that evaluate these models and agents across multiple domains. In addition, we propose a taxonomy of approximately 60 benchmarks that cover general and academic knowledge reasoning, mathematical problem-solving, code generation and software engineering, factual grounding and retrieval, domain-specific evaluations, multimodal and embodied tasks, task orchestration, and interactive assessments. Furthermore, we review AI-agent frameworks introduced between 2023 and 2025 that integrate large language models with modular toolkits to enable autonomous decision-making and multi-step reasoning. Moreover, we present real-world applications of autonomous AI agents in materials science, biomedical research, academic ideation, software engineering, synthetic data generation, chemical reasoning, mathematical problem-solving, geographic information systems, multimedia, healthcare, and finance. We then survey key agent-to-agent collaboration protocols, namely the Agent Communication Protocol (ACP), the Model Context Protocol (MCP), and the Agent-to-Agent Protocol (A2A). Finally, we discuss recommendations for future research, focusing on advanced reasoning strategies, failure modes in multi-agent LLM systems, automated scientific discovery, dynamic tool integration via reinforcement learning, integrated search capabilities, and security vulnerabilities in agent protocols.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Injection Attack to Tool Selection in LLM Agents</title>
<link>https://arxiv.org/abs/2504.19793</link>
<guid>https://arxiv.org/abs/2504.19793</guid>
<content:encoded><![CDATA[
arXiv:2504.19793v1 Announce Type: new 
Abstract: Tool selection is a key component of LLM agents. The process operates through a two-step mechanism - \emph{retrieval} and \emph{selection} - to pick the most appropriate tool from a tool library for a given task. In this work, we introduce \textit{ToolHijacker}, a novel prompt injection attack targeting tool selection in no-box scenarios. ToolHijacker injects a malicious tool document into the tool library to manipulate the LLM agent's tool selection process, compelling it to consistently choose the attacker's malicious tool for an attacker-chosen target task. Specifically, we formulate the crafting of such tool documents as an optimization problem and propose a two-phase optimization strategy to solve it. Our extensive experimental evaluation shows that ToolHijacker is highly effective, significantly outperforming existing manual-based and automated prompt injection attacks when applied to tool selection. Moreover, we explore various defenses, including prevention-based defenses (StruQ and SecAlign) and detection-based defenses (known-answer detection, perplexity detection, and perplexity windowed detection). Our experimental results indicate that these defenses are insufficient, highlighting the urgent need for developing new defense strategies.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects</title>
<link>https://arxiv.org/abs/2504.19838</link>
<guid>https://arxiv.org/abs/2504.19838</guid>
<content:encoded><![CDATA[
arXiv:2504.19838v1 Announce Type: new 
Abstract: With the rapid rise of large language models (LLMs), phone automation has undergone transformative changes. This paper systematically reviews LLM-driven phone GUI agents, highlighting their evolution from script-based automation to intelligent, adaptive systems. We first contextualize key challenges, (i) limited generality, (ii) high maintenance overhead, and (iii) weak intent comprehension, and show how LLMs address these issues through advanced language understanding, multimodal perception, and robust decision-making. We then propose a taxonomy covering fundamental agent frameworks (single-agent, multi-agent, plan-then-act), modeling approaches (prompt engineering, training-based), and essential datasets and benchmarks. Furthermore, we detail task-specific architectures, supervised fine-tuning, and reinforcement learning strategies that bridge user intent and GUI operations. Finally, we discuss open challenges such as dataset diversity, on-device deployment efficiency, user-centric adaptation, and security concerns, offering forward-looking insights into this rapidly evolving field. By providing a structured overview and identifying pressing research gaps, this paper serves as a definitive reference for researchers and practitioners seeking to harness LLMs in designing scalable, user-friendly phone GUI agents.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can AI Agents Design and Implement Drug Discovery Pipelines?</title>
<link>https://arxiv.org/abs/2504.19912</link>
<guid>https://arxiv.org/abs/2504.19912</guid>
<content:encoded><![CDATA[
arXiv:2504.19912v1 Announce Type: new 
Abstract: The rapid advancement of artificial intelligence, particularly autonomous agentic systems based on Large Language Models (LLMs), presents new opportunities to accelerate drug discovery by improving in-silico modeling and reducing dependence on costly experimental trials. Current AI agent-based systems demonstrate proficiency in solving programming challenges and conducting research, indicating an emerging potential to develop software capable of addressing complex problems such as pharmaceutical design and drug discovery. This paper introduces DO Challenge, a benchmark designed to evaluate the decision-making abilities of AI agents in a single, complex problem resembling virtual screening scenarios. The benchmark challenges systems to independently develop, implement, and execute efficient strategies for identifying promising molecular structures from extensive datasets, while navigating chemical space, selecting models, and managing limited resources in a multi-objective context. We also discuss insights from the DO Challenge 2025, a competition based on the proposed benchmark, which showcased diverse strategies explored by human participants. Furthermore, we present the Deep Thought multi-agent system, which demonstrated strong performance on the benchmark, outperforming most human teams. Among the language models tested, Claude 3.7 Sonnet, Gemini 2.5 Pro and o3 performed best in primary agent roles, and GPT-4o, Gemini 2.0 Flash were effective in auxiliary roles. While promising, the system's performance still fell short of expert-designed solutions and showed high instability, highlighting both the potential and current limitations of AI-driven methodologies in transforming drug discovery and broader scientific research.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated decision-making for dynamic task assignment at scale</title>
<link>https://arxiv.org/abs/2504.19933</link>
<guid>https://arxiv.org/abs/2504.19933</guid>
<content:encoded><![CDATA[
arXiv:2504.19933v1 Announce Type: new 
Abstract: The Dynamic Task Assignment Problem (DTAP) concerns matching resources to tasks in real time while minimizing some objectives, like resource costs or task cycle time. In this work, we consider a DTAP variant where every task is a case composed of a stochastic sequence of activities. The DTAP, in this case, involves the decision of which employee to assign to which activity to process requests as quickly as possible. In recent years, Deep Reinforcement Learning (DRL) has emerged as a promising tool for tackling this DTAP variant, but most research is limited to solving small-scale, synthetic problems, neglecting the challenges posed by real-world use cases. To bridge this gap, this work proposes a DRL-based Decision Support System (DSS) for real-world scale DTAPS. To this end, we introduce a DRL agent with two novel elements: a graph structure for observations and actions that can effectively represent any DTAP and a reward function that is provably equivalent to the objective of minimizing the average cycle time of tasks. The combination of these two novelties allows the agent to learn effective and generalizable assignment policies for real-world scale DTAPs. The proposed DSS is evaluated on five DTAP instances whose parameters are extracted from real-world logs through process mining. The experimental evaluation shows how the proposed DRL agent matches or outperforms the best baseline in all DTAP instances and generalizes on different time horizons and across instances.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking</title>
<link>https://arxiv.org/abs/2504.19940</link>
<guid>https://arxiv.org/abs/2504.19940</guid>
<content:encoded><![CDATA[
arXiv:2504.19940v1 Announce Type: new 
Abstract: The growing spread of online misinformation has created an urgent need for scalable, reliable fact-checking solutions. Crowdsourced fact-checking - where non-experts evaluate claim veracity - offers a cost-effective alternative to expert verification, despite concerns about variability in quality and bias. Encouraged by promising results in certain contexts, major platforms such as X (formerly Twitter), Facebook, and Instagram have begun shifting from centralized moderation to decentralized, crowd-based approaches.
  In parallel, advances in Large Language Models (LLMs) have shown strong performance across core fact-checking tasks, including claim detection and evidence evaluation. However, their potential role in crowdsourced workflows remains unexplored. This paper investigates whether LLM-powered generative agents - autonomous entities that emulate human behavior and decision-making - can meaningfully contribute to fact-checking tasks traditionally reserved for human crowds. Using the protocol of La Barbera et al. (2024), we simulate crowds of generative agents with diverse demographic and ideological profiles. Agents retrieve evidence, assess claims along multiple quality dimensions, and issue final veracity judgments.
  Our results show that agent crowds outperform human crowds in truthfulness classification, exhibit higher internal consistency, and show reduced susceptibility to social and cognitive biases. Compared to humans, agents rely more systematically on informative criteria such as Accuracy, Precision, and Informativeness, suggesting a more structured decision-making process. Overall, our findings highlight the potential of generative agents as scalable, consistent, and less biased contributors to crowd-based fact-checking systems.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Securing GenAI Multi-Agent Systems Against Tool Squatting: A Zero Trust Registry-Based Approach</title>
<link>https://arxiv.org/abs/2504.19951</link>
<guid>https://arxiv.org/abs/2504.19951</guid>
<content:encoded><![CDATA[
arXiv:2504.19951v1 Announce Type: new 
Abstract: The rise of generative AI (GenAI) multi-agent systems (MAS) necessitates standardized protocols enabling agents to discover and interact with external tools. However, these protocols introduce new security challenges, particularly; tool squatting; the deceptive registration or representation of tools. This paper analyzes tool squatting threats within the context of emerging interoperability standards, such as Model Context Protocol (MCP) or seamless communication between agents protocols. It introduces a comprehensive Tool Registry system designed to mitigate these risks. We propose a security-focused architecture featuring admin-controlled registration, centralized tool discovery, fine grained access policies enforced via dedicated Agent and Tool Registry services, a dynamic trust scoring mechanism based on tool versioning and known vulnerabilities, and just in time credential provisioning. Based on its design principles, the proposed registry framework aims to effectively prevent common tool squatting vectors while preserving the flexibility and power of multi-agent systems. This work addresses a critical security gap in the rapidly evolving GenAI ecosystem and provides a foundation for secure tool integration in production environments.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Securing Agentic AI: A Comprehensive Threat Model and Mitigation Framework for Generative AI Agents</title>
<link>https://arxiv.org/abs/2504.19956</link>
<guid>https://arxiv.org/abs/2504.19956</guid>
<content:encoded><![CDATA[
arXiv:2504.19956v1 Announce Type: new 
Abstract: As generative AI (GenAI) agents become more common in enterprise settings, they introduce security challenges that differ significantly from those posed by traditional systems. These agents are not just LLMs; they reason, remember, and act, often with minimal human oversight. This paper introduces a comprehensive threat model tailored specifically for GenAI agents, focusing on how their autonomy, persistent memory access, complex reasoning, and tool integration create novel risks. This research work identifies 9 primary threats and organizes them across five key domains: cognitive architecture vulnerabilities, temporal persistence threats, operational execution vulnerabilities, trust boundary violations, and governance circumvention. These threats are not just theoretical they bring practical challenges such as delayed exploitability, cross-system propagation, cross system lateral movement, and subtle goal misalignments that are hard to detect with existing frameworks and standard approaches. To help address this, the research work present two complementary frameworks: ATFAA - Advanced Threat Framework for Autonomous AI Agents, which organizes agent-specific risks, and SHIELD, a framework proposing practical mitigation strategies designed to reduce enterprise exposure. While this work builds on existing work in LLM and AI security, the focus is squarely on what makes agents different and why those differences matter. Ultimately, this research argues that GenAI agents require a new lens for security. If we fail to adapt our threat models and defenses to account for their unique architecture and behavior, we risk turning a powerful new tool into a serious enterprise liability.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining Turn-Level Precision with Dialogue-Level Comparisons</title>
<link>https://arxiv.org/abs/2504.19982</link>
<guid>https://arxiv.org/abs/2504.19982</guid>
<content:encoded><![CDATA[
arXiv:2504.19982v1 Announce Type: new 
Abstract: Task-oriented dialogue (TOD) systems are experiencing a revolution driven by Large Language Models (LLMs), yet the evaluation methodologies for these systems remain insufficient for their growing sophistication. While traditional automatic metrics effectively assessed earlier modular systems, they focus solely on the dialogue level and cannot detect critical intermediate errors that can arise during user-agent interactions. In this paper, we introduce TD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework that unifies fine-grained turn-level analysis with holistic dialogue-level comparisons. At turn level, we evaluate each response along three TOD-specific dimensions: conversation cohesion, backend knowledge consistency, and policy compliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons to provide a measure of dialogue-level quality. Through experiments on MultiWOZ 2.4 and {\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the conversational errors that conventional metrics miss. Furthermore, TD-EVAL exhibits better alignment with human judgments than traditional and LLM-based metrics. These findings demonstrate that TD-EVAL introduces a new paradigm for TOD system evaluation, efficiently assessing both turn and system levels with a plug-and-play framework for future research.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast algorithm for centralized multi-agent maze exploration</title>
<link>https://arxiv.org/abs/2310.02121</link>
<guid>https://arxiv.org/abs/2310.02121</guid>
<content:encoded><![CDATA[
arXiv:2310.02121v2 Announce Type: replace 
Abstract: Recent advances in robotics have paved the way for robots to replace humans in perilous situations, such as searching for victims in burning buildings, in earthquake-damaged structures, in uncharted caves, traversing minefields or patrolling crime-ridden streets. These challenges can be generalized as problems where agents have to explore unknown mazes. We propose a cooperative multi-agent system of automated mobile agents for exploring unknown mazes and localizing stationary targets. The Heat Equation-Driven Area Coverage (HEDAC) algorithm for maze exploration employs a potential field to guide the exploration of the maze and integrates cooperative behaviors of the agents such as collision avoidance, coverage coordination, and path planning. In contrast to previous applications for continuous static domains, we adapt the HEDAC method for mazes on expanding rectilinear grids. The proposed algorithm guarantees the exploration of the entire maze and can ensure the avoidance of collisions and deadlocks. Moreover, this is the first application of the HEDAC algorithm to domains that expand over time. To cope with the dynamically changing domain, succesive over-relaxation (SOR) iterative linear solver has been adapted and implemented, which significantly reduced the computational complexity of the presented algorithm when compared to standard direct and iterative linear solvers. The results highlight significant improvements and show the applicability of the algorithm in different mazes. They confirm its robustness, adaptability, scalability and simplicity, which enables centralized parallel computation to control multiple agents/robots in the maze.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Worst-Case Online Allocation via Dynamic Max-min Fairness</title>
<link>https://arxiv.org/abs/2310.08881</link>
<guid>https://arxiv.org/abs/2310.08881</guid>
<content:encoded><![CDATA[
arXiv:2310.08881v3 Announce Type: replace 
Abstract: We study the allocation of shared resources over multiple rounds among competing agents, via the dynamic max-min fair (DMMF) mechanism: the good in each round is allocated to the requesting agent with the least number of allocations received to date. We show that in large markets when an agent has i.i.d. values across rounds, under mild distributional assumptions (e.g., bounded PDF function), the DMMF mechanism allows each agent to realize a $1 - o(1)$ fraction of her ideal utility -- her highest achievable utility given her nominal share of resources. This guarantee holds under arbitrary behavior by other agents and is achieved by characterizing the agent's utility under a rich space of strategies, wherein an agent can tune how aggressive to be in requesting the item. Our techniques also allow us to handle settings where an agent's values are correlated across rounds, thereby allowing an adversary to predict and block her future values. By tuning the aggressiveness, an agent can guarantee $\Omega(\gamma)$ fraction of her ideal utility, where $\gamma\in [0, 1]$ is a parameter that quantifies dependence across rounds (with $\gamma = 1$ indicating full independence and lower values indicating more correlation). Finally, we extend our efficiency results to the case of reusable resources, where an agent might need to hold the item over multiple rounds to receive utility. Our results subsume previous guarantees obtained using a more complicated mechanism proving a half ideal utility guarantee under i.i.d. values sampled from worst-case distributions.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Multi-Task Learning for Stochastic Bandits with Context Distribution and Stage-wise Constraints</title>
<link>https://arxiv.org/abs/2401.11563</link>
<guid>https://arxiv.org/abs/2401.11563</guid>
<content:encoded><![CDATA[
arXiv:2401.11563v3 Announce Type: replace 
Abstract: We present conservative distributed multi-task learning in stochastic linear contextual bandits with heterogeneous agents. This extends conservative linear bandits to a distributed setting where M agents tackle different but related tasks while adhering to stage-wise performance constraints. The exact context is unknown, and only a context distribution is available to the agents as in many practical applications that involve a prediction mechanism to infer context, such as stock market prediction and weather forecast. We propose a distributed upper confidence bound (UCB) algorithm, DiSC-UCB. Our algorithm constructs a pruned action set during each round to ensure the constraints are met. Additionally, it includes synchronized sharing of estimates among agents via a central server using well-structured synchronization steps. We prove the regret and communication bounds on the algorithm. We extend the problem to a setting where the agents are unaware of the baseline reward. For this setting, we provide a modified algorithm, DiSC-UCB2, and we show that the modified algorithm achieves the same regret and communication bounds. We empirically validated the performance of our algorithm on synthetic data and real-world Movielens-100K data.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RUMOR: Reinforcement learning for Understanding a Model of the Real World for Navigation in Dynamic Environments</title>
<link>https://arxiv.org/abs/2404.16672</link>
<guid>https://arxiv.org/abs/2404.16672</guid>
<content:encoded><![CDATA[
arXiv:2404.16672v2 Announce Type: replace 
Abstract: Autonomous navigation in dynamic environments is a complex but essential task for autonomous robots, with recent deep reinforcement learning approaches showing promising results. However, the complexity of the real world makes it infeasible to train agents in every possible scenario configuration. Moreover, existing methods typically overlook factors such as robot kinodynamic constraints, or assume perfect knowledge of the environment. In this work, we present RUMOR, a novel planner for differential-drive robots that uses deep reinforcement learning to navigate in highly dynamic environments. Unlike other end-to-end DRL planners, it uses a descriptive robocentric velocity space model to extract the dynamic environment information, enhancing training effectiveness and scenario interpretation. Additionally, we propose an action space that inherently considers robot kinodynamics and train it in a simulator that reproduces the real world problematic aspects, reducing the gap between the reality and simulation. We extensively compare RUMOR with other state-of-the-art approaches, demonstrating a better performance, and provide a detailed analysis of the results. Finally, we validate RUMOR's performance in real-world settings by deploying it on a ground robot. Our experiments, conducted in crowded scenarios and unseen environments, confirm the algorithm's robustness and transferability.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leader-Follower Density Control of Spatial Dynamics in Large-Scale Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2406.01804</link>
<guid>https://arxiv.org/abs/2406.01804</guid>
<content:encoded><![CDATA[
arXiv:2406.01804v4 Announce Type: replace 
Abstract: We address the problem of controlling the density of a large ensemble of follower agents by acting on a group of leader agents that interact with them. Using coupled partial integro-differential equations to describe leader and follower density dynamics, we establish feasibility conditions and develop two control architectures ensuring global stability. The first employs feed-forward control on the followers' and a feedback on the leaders' density. The second implements a dual feedback loop through a reference-governor that adapts the leaders' density based on both populations' measurements. Our methods, initially developed in a one-dimensional setting, are extended to multi-dimensional cases, and validated through numerical simulations for representative control applications, both for groups of infinite and finite size.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think Deep and Fast: Learning Neural Nonlinear Opinion Dynamics from Inverse Dynamic Games for Split-Second Interactions</title>
<link>https://arxiv.org/abs/2406.09810</link>
<guid>https://arxiv.org/abs/2406.09810</guid>
<content:encoded><![CDATA[
arXiv:2406.09810v2 Announce Type: replace 
Abstract: Non-cooperative interactions commonly occur in multi-agent scenarios such as car racing, where an ego vehicle can choose to overtake the rival, or stay behind it until a safe overtaking "corridor" opens. While an expert human can do well at making such time-sensitive decisions, autonomous agents are incapable of rapidly reasoning about complex, potentially conflicting options, leading to suboptimal behaviors such as deadlocks. Recently, the nonlinear opinion dynamics (NOD) model has proven to exhibit fast opinion formation and avoidance of decision deadlocks. However, NOD modeling parameters are oftentimes assumed fixed, limiting their applicability in complex and dynamic environments. It remains an open challenge to determine such parameters automatically and adaptively, accounting for the ever-changing environment. In this work, we propose for the first time a learning-based and game-theoretic approach to synthesize a Neural NOD model from expert demonstrations, given as a dataset containing (possibly incomplete) state and action trajectories of interacting agents. We demonstrate Neural NOD's ability to make fast and deadlock-free decisions in a simulated autonomous racing example. We find that Neural NOD consistently outperforms the state-of-the-art data-driven inverse game baseline in terms of safety and overtaking performance.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Temporal Logic Predicates from Data with Statistical Guarantees</title>
<link>https://arxiv.org/abs/2406.10449</link>
<guid>https://arxiv.org/abs/2406.10449</guid>
<content:encoded><![CDATA[
arXiv:2406.10449v3 Announce Type: replace 
Abstract: Temporal logic rules are often used in control and robotics to provide structured, human-interpretable descriptions of trajectory data. These rules have numerous applications including safety validation using formal methods, constraining motion planning among autonomous agents, and classifying data. However, existing methods for learning temporal logic predicates from data do not provide assurances about the correctness of the resulting predicate. We present a novel method to learn temporal logic predicates from data with finite-sample correctness guarantees. Our approach leverages expression optimization and conformal prediction to learn predicates that correctly describe future trajectories under mild statistical assumptions. We provide experimental results showing the performance of our approach on a simulated trajectory dataset and perform ablation studies to understand how each component of our algorithm contributes to its performance.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM Agents</title>
<link>https://arxiv.org/abs/2409.09013</link>
<guid>https://arxiv.org/abs/2409.09013</guid>
<content:encoded><![CDATA[
arXiv:2409.09013v2 Announce Type: replace 
Abstract: Truthfulness (adherence to factual accuracy) and utility (satisfying human needs and instructions) are both fundamental aspects of Large Language Models, yet these goals often conflict (e.g., sell a car with known flaws), which makes it challenging to achieve both in real-world deployments. We propose AI-LieDar, a framework to study how LLM-based agents navigate these scenarios in an multi-turn interactive setting. We design a set of real-world scenarios where language agents are instructed to achieve goals that are in conflict with being truthful during a multi-turn conversation with simulated human agents. To evaluate the truthfulness at large scale, we develop a truthfulness detector inspired by psychological literature to assess the agents' responses. Our experiment demonstrates that all models are truthful less than 50% of the time, though truthfulness and goal achievement (utility) rates vary across models. We further test the steerability of LLMs towards truthfulness, finding that models can be directed to be truthful or deceptive, and even truth-steered models still lie. These findings reveal the complex nature of truthfulness in LLMs and underscore the importance of further research to ensure the safe and reliable deployment of LLMs and LLM-based agents.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHIRE: Enhancing Sample Efficiency using Human Intuition in REinforcement Learning</title>
<link>https://arxiv.org/abs/2409.09990</link>
<guid>https://arxiv.org/abs/2409.09990</guid>
<content:encoded><![CDATA[
arXiv:2409.09990v3 Announce Type: replace 
Abstract: The ability of neural networks to perform robotic perception and control tasks such as depth and optical flow estimation, simultaneous localization and mapping (SLAM), and automatic control has led to their widespread adoption in recent years. Deep Reinforcement Learning has been used extensively in these settings, as it does not have the unsustainable training costs associated with supervised learning. However, DeepRL suffers from poor sample efficiency, i.e., it requires a large number of environmental interactions to converge to an acceptable solution. Modern RL algorithms such as Deep Q Learning and Soft Actor-Critic attempt to remedy this shortcoming but can not provide the explainability required in applications such as autonomous robotics. Humans intuitively understand the long-time-horizon sequential tasks common in robotics. Properly using such intuition can make RL policies more explainable while enhancing their sample efficiency. In this work, we propose SHIRE, a novel framework for encoding human intuition using Probabilistic Graphical Models (PGMs) and using it in the Deep RL training pipeline to enhance sample efficiency. Our framework achieves 25-78% sample efficiency gains across the environments we evaluate at negligible overhead cost. Additionally, by teaching RL agents the encoded elementary behavior, SHIRE enhances policy explainability. A real-world demonstration further highlights the efficacy of policies trained using our framework.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ID-Free Not Risk-Free: LLM-Powered Agents Unveil Risks in ID-Free Recommender Systems</title>
<link>https://arxiv.org/abs/2409.11690</link>
<guid>https://arxiv.org/abs/2409.11690</guid>
<content:encoded><![CDATA[
arXiv:2409.11690v3 Announce Type: replace 
Abstract: Recent advances in ID-free recommender systems have attracted significant attention for effectively addressing the cold start problem. However, their vulnerability to malicious attacks remains largely unexplored. In this paper, we unveil a critical yet overlooked risk: LLM-powered agents can be strategically deployed to attack ID-free recommenders, stealthily promoting low-quality items in black-box settings. This attack exploits a novel rewriting-based deception strategy, where malicious agents synthesize deceptive textual descriptions by simulating the characteristics of popular items. To achieve this, the attack mechanism integrates two primary components: (1) a popularity extraction component that captures essential characteristics of popular items and (2) a multi-agent collaboration mechanism that enables iterative refinement of promotional textual descriptions through independent thinking and team discussion. To counter this risk, we further introduce a detection method to identify suspicious text generated by our discovered attack. By unveiling this risk, our work aims to underscore the urgent need to enhance the security of ID-free recommender systems.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Windowed MAPF with Completeness Guarantees</title>
<link>https://arxiv.org/abs/2410.01798</link>
<guid>https://arxiv.org/abs/2410.01798</guid>
<content:encoded><![CDATA[
arXiv:2410.01798v3 Announce Type: replace 
Abstract: Traditional multi-agent path finding (MAPF) methods try to compute entire start-goal paths which are collision free. However, computing an entire path can take too long for MAPF systems where agents need to replan fast. Methods that address this typically employ a "windowed" approach and only try to find collision free paths for a small windowed timestep horizon. This adaptation comes at the cost of incompleteness; all current windowed approaches can become stuck in deadlock or livelock. Our main contribution is to introduce our framework, WinC-MAPF, for Windowed MAPF that enables completeness. Our framework uses heuristic update insights from single-agent real-time heuristic search algorithms as well as agent independence ideas from MAPF algorithms. We also develop Single-Step CBS (SS-CBS), an instantiation of this framework using a novel modification to CBS. We show how SS-CBS, which only plans a single step and updates heuristics, can effectively solve tough scenarios where existing windowed approaches fail.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent LLMs Ensemble for Efficient Atrial Fibrillation Annotation of ECG Reports</title>
<link>https://arxiv.org/abs/2410.16543</link>
<guid>https://arxiv.org/abs/2410.16543</guid>
<content:encoded><![CDATA[
arXiv:2410.16543v2 Announce Type: replace 
Abstract: This study introduces a novel multiagent ensemble method powered by LLMs to address a key challenge in ML - data labeling, particularly in large-scale EHR datasets. Manual labeling of such datasets requires domain expertise and is labor-intensive, time-consuming, expensive, and error-prone. To overcome this bottleneck, we developed an ensemble LLMs method and demonstrated its effectiveness in two real-world tasks: (1) labeling a large-scale unlabeled ECG dataset in MIMIC-IV; (2) identifying social determinants of health (SDOH) from the clinical notes of EHR. Trading off benefits and cost, we selected a pool of diverse open source LLMs with satisfactory performance. We treat each LLM's prediction as a vote and apply a mechanism of majority voting with minimal winning threshold for ensemble. We implemented an ensemble LLMs application for EHR data labeling tasks. By using the ensemble LLMs and natural language processing, we labeled MIMIC-IV ECG dataset of 623,566 ECG reports with an estimated accuracy of 98.2%. We applied the ensemble LLMs method to identify SDOH from social history sections of 1,405 EHR clinical notes, also achieving competitive performance. Our experiments show that the ensemble LLMs can outperform individual LLM even the best commercial one, and the method reduces hallucination errors. From the research, we found that (1) the ensemble LLMs method significantly reduces the time and effort required for labeling large-scale EHR data, automating the process with high accuracy and quality; (2) the method generalizes well to other text data labeling tasks, as shown by its application to SDOH identification; (3) the ensemble of a group of diverse LLMs can outperform or match the performance of the best individual LLM; and (4) the ensemble method substantially reduces hallucination errors. This approach provides a scalable and efficient solution to data-labeling challenges.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolution of Societies via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2410.17466</link>
<guid>https://arxiv.org/abs/2410.17466</guid>
<content:encoded><![CDATA[
arXiv:2410.17466v4 Announce Type: replace 
Abstract: The universe involves many independent co-learning agents as an ever-evolving part of our observed environment. Yet, in practice, Multi-Agent Reinforcement Learning (MARL) applications are typically constrained to small, homogeneous populations and remain computationally intensive. We propose a methodology that enables simulating populations of Reinforcement Learning agents at evolutionary scale. More specifically, we derive a fast, parallelizable implementation of Policy Gradient (PG) and Opponent-Learning Awareness (LOLA), tailored for evolutionary simulations where agents undergo random pairwise interactions in stateless normal-form games. We demonstrate our approach by simulating the evolution of very large populations made of heterogeneous co-learning agents, under both naive and advanced learning strategies. In our experiments, 200,000 PG or LOLA agents evolve in the classic games of Hawk-Dove, Stag-Hunt, and Rock-Paper-Scissors. Each game provides distinct insights into how populations evolve under both naive and advanced MARL rules, including compelling ways in which Opponent-Learning Awareness affects social evolution.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaTrading: An Immersion-Aware Model Trading Framework for Vehicular Metaverse Services</title>
<link>https://arxiv.org/abs/2410.19665</link>
<guid>https://arxiv.org/abs/2410.19665</guid>
<content:encoded><![CDATA[
arXiv:2410.19665v2 Announce Type: replace 
Abstract: Timely updating of Internet of Things (IoT) data is crucial for immersive vehicular metaverse services. However, challenges such as latency caused by massive data transmissions, privacy risks associated with user data, and computational burdens on metaverse service providers (MSPs) hinder continuous collection of high-quality data. To address these issues, we propose an immersion-aware model trading framework that facilitates data provision for services while ensuring privacy through federated learning (FL). Specifically, we first develop a novel multi-dimensional metric, the immersion of model (IoM), which assesses model value comprehensively by considering freshness and accuracy of learning models, as well as the amount and potential value of raw data used for training. Then, we design an incentive mechanism to incentivize metaverse users (MUs) to contribute high-value models under resource constraints. The trading interactions between MSPs and MUs are modeled as an equilibrium problem with equilibrium constraints (EPEC) to analyze and balance their costs and gains, where MSPs as leaders determine rewards, while MUs as followers optimize resource allocation. Furthermore, considering dynamic network conditions and privacy concerns, we formulate the reward decisions of MSPs as a multi-agent Markov decision process. To solve this, we develop a fully distributed dynamic reward algorithm based on deep reinforcement learning, without accessing any private information about MUs and other MSPs. Experimental results demonstrate that the proposed framework outperforms state-of-the-art benchmarks, achieving improvements in IoM of 38.3% and 37.2%, and reductions in training time to reach the target accuracy of 43.5% and 49.8%, on average, for the MNIST and GTSRB datasets, respectively.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Robustness in Language-Driven Robotics: A Modular Approach to Failure Reduction</title>
<link>https://arxiv.org/abs/2411.05474</link>
<guid>https://arxiv.org/abs/2411.05474</guid>
<content:encoded><![CDATA[
arXiv:2411.05474v2 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have led to significant progress in robotics, enabling embodied agents to better understand and execute open-ended tasks. However, existing approaches using LLMs face limitations in grounding their outputs within the physical environment and aligning with the capabilities of the robot. This challenge becomes even more pronounced with smaller language models, which are more computationally efficient but less robust in task planning and execution. In this paper, we present a novel modular architecture designed to enhance the robustness of LLM-driven robotics by addressing these grounding and alignment issues. We formalize the task planning problem within a goal-conditioned POMDP framework, identify key failure modes in LLM-driven planning, and propose targeted design principles to mitigate these issues. Our architecture introduces an ``expected outcomes'' module to prevent mischaracterization of subgoals and a feedback mechanism to enable real-time error recovery. Experimental results, both in simulation and on physical robots, demonstrate that our approach significantly improves task success rates for pick-and-place and manipulation tasks compared to both larger LLMs and standard baselines. Through hardware experiments, we also demonstrate how our architecture can be run efficiently and locally. This work highlights the potential of smaller, locally-executable LLMs in robotics and provides a scalable, efficient solution for robust task execution.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivized Symbiosis: A Paradigm for Human-Agent Coevolution</title>
<link>https://arxiv.org/abs/2412.06855</link>
<guid>https://arxiv.org/abs/2412.06855</guid>
<content:encoded><![CDATA[
arXiv:2412.06855v4 Announce Type: replace 
Abstract: Cooperation is vital to our survival and progress. Evolutionary game theory offers a lens to understand the structures and incentives that enable cooperation to be a successful strategy. As artificial intelligence agents become integral to human systems, the dynamics of cooperation take on unprecedented significance. The convergence of human-agent teaming, contract theory, and decentralized frameworks like Web3, grounded in transparency, accountability, and trust, offers a foundation for fostering cooperation by establishing enforceable rules and incentives for humans and AI agents. We conceptualize Incentivized Symbiosis as a social contract between humans and AI, inspired by Web3 principles and encoded in blockchain technology, to define and enforce rules, incentives, and consequences for both parties. By exploring this paradigm, we aim to catalyze new research at the intersection of systems thinking in AI, Web3, and society, fostering innovative pathways for cooperative human-agent coevolution.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grey-Box Fuzzing in Constrained Ultra-Large Systems: Lessons for SE Community</title>
<link>https://arxiv.org/abs/2501.10269</link>
<guid>https://arxiv.org/abs/2501.10269</guid>
<content:encoded><![CDATA[
arXiv:2501.10269v3 Announce Type: replace 
Abstract: Testing ultra-large microservices-based FinTech systems presents significant challenges, including restricted access to production environments, complex dependencies, and stringent security constraints. We propose SandBoxFuzz, a scalable grey-box fuzzing technique that addresses these limitations by leveraging aspect-oriented programming and runtime reflection to enable dynamic specification mining, generating targeted inputs for constrained environments. SandBoxFuzz also introduces a log-based coverage mechanism, seamlessly integrated into the build pipeline, eliminating the need for runtime coverage agents that are often infeasible in industrial settings. SandBoxFuzz has been successfully deployed to Ant Group's production line and, compared to an initial solution built on a state-of-the-art fuzzing framework, it demonstrates superior performance in their microservices software. SandBoxFuzz achieves a 7.5% increase in branch coverage, identifies 1,850 additional exceptions, and reduces setup time from hours to minutes, highlighting its effectiveness and practical utility in a real-world industrial environment. By open-sourcing SandBoxFuzz, we provide a practical and effective tool for researchers and practitioners to test large-scale microservices systems.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAIMAN: Causal Action Influence Detection for Sample-efficient Loco-manipulation</title>
<link>https://arxiv.org/abs/2502.00835</link>
<guid>https://arxiv.org/abs/2502.00835</guid>
<content:encoded><![CDATA[
arXiv:2502.00835v2 Announce Type: replace 
Abstract: Enabling legged robots to perform non-prehensile loco-manipulation is crucial for enhancing their versatility. Learning behaviors such as whole-body object pushing often requires sophisticated planning strategies or extensive task-specific reward shaping, especially in unstructured environments. In this work, we present CAIMAN, a practical reinforcement learning framework that encourages the agent to gain control over other entities in the environment. CAIMAN leverages causal action influence as an intrinsic motivation objective, allowing legged robots to efficiently acquire object pushing skills even under sparse task rewards. We employ a hierarchical control strategy, combining a low-level locomotion module with a high-level policy that generates task-relevant velocity commands and is trained to maximize the intrinsic reward. To estimate causal action influence, we learn the dynamics of the environment by integrating a kinematic prior with data collected during training.We empirically demonstrate CAIMAN's superior sample efficiency and adaptability to diverse scenarios in simulation, as well as its successful transfer to real-world systems without further fine-tuning.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Wisdom of Intellectually Humble Networks</title>
<link>https://arxiv.org/abs/2502.02015</link>
<guid>https://arxiv.org/abs/2502.02015</guid>
<content:encoded><![CDATA[
arXiv:2502.02015v2 Announce Type: replace 
Abstract: People's collectively shared beliefs can have significant social implications, including on democratic processes and policies. Unfortunately, as people interact with peers to form and update their beliefs, various cognitive and social biases can hinder their collective wisdom. In this paper, we probe whether and how the psychological construct of intellectual humility can modulate collective wisdom in a networked interaction setting. Through agent-based modeling and data-calibrated simulations, we provide a proof of concept demonstrating that intellectual humility can foster more accurate estimations while mitigating polarization in social networks. We investigate the mechanisms behind the performance improvements and confirm robustness across task settings and network structures. Our work can guide intervention designs to capitalize on the promises of intellectual humility in boosting collective wisdom in social networks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring LLM-based Student Simulation for Metacognitive Cultivation</title>
<link>https://arxiv.org/abs/2502.11678</link>
<guid>https://arxiv.org/abs/2502.11678</guid>
<content:encoded><![CDATA[
arXiv:2502.11678v2 Announce Type: replace 
Abstract: Metacognitive education plays a crucial role in cultivating students' self-regulation and reflective thinking, providing essential support for those with learning difficulties through academic advising. Simulating students with insufficient learning capabilities using large language models offers a promising approach to refining pedagogical methods without ethical concerns. However, existing simulations often fail to authentically represent students' learning struggles and face challenges in evaluation due to the lack of reliable metrics and ethical constraints in data collection. To address these issues, we propose a pipeline for automatically generating and filtering high-quality simulated student agents. Our approach leverages a two-round automated scoring system validated by human experts and employs a score propagation module to obtain more consistent scores across the student graph. Experimental results demonstrate that our pipeline efficiently identifies high-quality student agents, and we discuss the traits that influence the simulation's effectiveness. By simulating students with varying degrees of learning difficulties, our work paves the way for broader applications in personalized learning and educational assessment.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfoQuest: Evaluating Multi-Turn Dialogue Agents for Open-Ended Conversations with Hidden Context</title>
<link>https://arxiv.org/abs/2502.12257</link>
<guid>https://arxiv.org/abs/2502.12257</guid>
<content:encoded><![CDATA[
arXiv:2502.12257v2 Announce Type: replace 
Abstract: Large language models excel at following explicit instructions, but they often struggle with ambiguous or incomplete user requests, defaulting to verbose, generic responses instead of seeking clarification. We introduce InfoQuest, a multi-turn chat benchmark designed to evaluate how dialogue agents handle hidden context in open-ended user requests. This benchmark presents intentionally ambiguous scenarios that require models to engage in information-seeking dialogue by asking clarifying questions before providing appropriate responses. Our evaluation of both open and closed models reveals that, while proprietary models generally perform better, all current assistants struggle to gather critical information effectively. They often require multiple turns to infer user intent and frequently default to generic responses without proper clarification. We provide a systematic methodology for generating diverse scenarios and evaluating models' information-seeking capabilities, which can be leveraged to automatically generate data for self-improvement. We also offer insights into the current limitations of language models in handling ambiguous requests through multi-turn interactions.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAMEN: Real-time Asynchronous Multi-agent Neural Implicit Mapping</title>
<link>https://arxiv.org/abs/2502.19592</link>
<guid>https://arxiv.org/abs/2502.19592</guid>
<content:encoded><![CDATA[
arXiv:2502.19592v2 Announce Type: replace 
Abstract: Multi-agent neural implicit mapping allows robots to collaboratively capture and reconstruct complex environments with high fidelity. However, existing approaches often rely on synchronous communication, which is impractical in real-world scenarios with limited bandwidth and potential communication interruptions. This paper introduces RAMEN: Real-time Asynchronous Multi-agEnt Neural implicit mapping, a novel approach designed to address this challenge. RAMEN employs an uncertainty-weighted multi-agent consensus optimization algorithm that accounts for communication disruptions. When communication is lost between a pair of agents, each agent retains only an outdated copy of its neighbor's map, with the uncertainty of this copy increasing over time since the last communication. Using gradient update information, we quantify the uncertainty associated with each parameter of the neural network map. Neural network maps from different agents are brought to consensus on the basis of their levels of uncertainty, with consensus biased towards network parameters with lower uncertainty. To achieve this, we derive a weighted variant of the decentralized consensus alternating direction method of multipliers (C-ADMM) algorithm, facilitating robust collaboration among agents with varying communication and update frequencies. Through extensive evaluations on real-world datasets and robot hardware experiments, we demonstrate RAMEN's superior mapping performance under challenging communication conditions.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WMNav: Integrating Vision-Language Models into World Models for Object Goal Navigation</title>
<link>https://arxiv.org/abs/2503.02247</link>
<guid>https://arxiv.org/abs/2503.02247</guid>
<content:encoded><![CDATA[
arXiv:2503.02247v4 Announce Type: replace 
Abstract: Object Goal Navigation-requiring an agent to locate a specific object in an unseen environment-remains a core challenge in embodied AI. Although recent progress in Vision-Language Model (VLM)-based agents has demonstrated promising perception and decision-making abilities through prompting, none has yet established a fully modular world model design that reduces risky and costly interactions with the environment by predicting the future state of the world. We introduce WMNav, a novel World Model-based Navigation framework powered by Vision-Language Models (VLMs). It predicts possible outcomes of decisions and builds memories to provide feedback to the policy module. To retain the predicted state of the environment, WMNav proposes the online maintained Curiosity Value Map as part of the world model memory to provide dynamic configuration for navigation policy. By decomposing according to a human-like thinking process, WMNav effectively alleviates the impact of model hallucination by making decisions based on the feedback difference between the world model plan and observation. To further boost efficiency, we implement a two-stage action proposer strategy: broad exploration followed by precise localization. Extensive evaluation on HM3D and MP3D validates WMNav surpasses existing zero-shot benchmarks in both success rate and exploration efficiency (absolute improvement: +3.2% SR and +3.2% SPL on HM3D, +13.5% SR and +1.1% SPL on MP3D). Project page: https://b0b8k1ng.github.io/WMNav/.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Distributed Algorithms for Shape Reduction via Reconfigurable Circuits</title>
<link>https://arxiv.org/abs/2503.18663</link>
<guid>https://arxiv.org/abs/2503.18663</guid>
<content:encoded><![CDATA[
arXiv:2503.18663v3 Announce Type: replace 
Abstract: Autonomous reconfiguration of agent-based systems is a key challenge in the study of programmable matter, distributed robotics, and molecular self-assembly. While substantial prior work has focused on size-preserving transformations, much less is known about size-changing transformations. Such transformations find application in natural processes, active self-assembly, and dynamical systems, where structures may evolve through the addition or removal of components controlled by local rules. In this paper, we study efficient distributed algorithms for transforming 2D geometric configurations of simple agents, called shapes, using only local size-changing operations. A novelty of our approach is the use of reconfigurable circuits as the underlying communication model, a recently proposed model enabling instant node-to-node communication via primitive signals. Unlike previous work, we integrate collision avoidance as a core responsibility of the distributed algorithm. We consider two graph update models: connectivity and adjacency. Let $n$ denote the number of agents and $k$ the number of turning points in the initial shape. In the connectivity model, we show that any tree-shaped configuration can be reduced to a single agent using only shrinking operations in $O(k \log n)$ rounds w.h.p., and to its incompressible form in $O(\log n)$ rounds w.h.p. given prior knowledge of the incompressible nodes, or in $O(k \log n)$ rounds otherwise. When both shrinking and growth operations are available, we give an algorithm that transforms any tree to a topologically equivalent one in $O(k \log n + \log^2 n)$ rounds w.h.p. On the negative side, we show that one cannot hope for $o(\log^2 n)$-round transformations for all shapes of $\Theta(\log n)$ turning points. In the adjacency model, we show that any connected shape can reduce itself to a single node using only shrinking in $O(\log n)$ rounds w.h.p.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Probabilistic Neuro-symbolic Layer for Algebraic Constraint Satisfaction</title>
<link>https://arxiv.org/abs/2503.19466</link>
<guid>https://arxiv.org/abs/2503.19466</guid>
<content:encoded><![CDATA[
arXiv:2503.19466v2 Announce Type: replace 
Abstract: In safety-critical applications, guaranteeing the satisfaction of constraints over continuous environments is crucial, e.g., an autonomous agent should never crash into obstacles or go off-road. Neural models struggle in the presence of these constraints, especially when they involve intricate algebraic relationships. To address this, we introduce a differentiable probabilistic layer that guarantees the satisfaction of non-convex algebraic constraints over continuous variables. This probabilistic algebraic layer (PAL) can be seamlessly plugged into any neural architecture and trained via maximum likelihood without requiring approximations. PAL defines a distribution over conjunctions and disjunctions of linear inequalities, parameterized by polynomials. This formulation enables efficient and exact renormalization via symbolic integration, which can be amortized across different data points and easily parallelized on a GPU. We showcase PAL and our integration scheme on a number of benchmarks for algebraic constraint integration and on real-world trajectory data.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied World Models Emerge from Navigational Task in Open-Ended Environments</title>
<link>https://arxiv.org/abs/2504.11419</link>
<guid>https://arxiv.org/abs/2504.11419</guid>
<content:encoded><![CDATA[
arXiv:2504.11419v2 Announce Type: replace 
Abstract: Spatial reasoning in partially observable environments has often been approached through passive predictive models, yet theories of embodied cognition suggest that genuinely useful representations arise only when perception is tightly coupled to action. Here we ask whether a recurrent agent, trained solely by sparse rewards to solve procedurally generated planar mazes, can autonomously internalize metric concepts such as direction, distance and obstacle layout. After training, the agent consistently produces near-optimal paths in unseen mazes, behavior that hints at an underlying spatial model. To probe this possibility, we cast the closed agent-environment loop as a hybrid dynamical system, identify stable limit cycles in its state space, and characterize behavior with a Ridge Representation that embeds whole trajectories into a common metric space. Canonical correlation analysis exposes a robust linear alignment between neural and behavioral manifolds, while targeted perturbations of the most informative neural dimensions sharply degrade navigation performance. Taken together, these dynamical, representational, and causal signatures show that sustained sensorimotor interaction is sufficient for the spontaneous emergence of compact, embodied world models, providing a principled path toward interpretable and transferable navigation policies.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing Trust and Resilience in Distributed Consensus for Cyberphysical Systems</title>
<link>https://arxiv.org/abs/2103.05464</link>
<guid>https://arxiv.org/abs/2103.05464</guid>
<content:encoded><![CDATA[
arXiv:2103.05464v2 Announce Type: replace-cross 
Abstract: This work considers the problem of resilient consensus where stochastic values of trust between agents are available. Specifically, we derive a unified mathematical framework to characterize convergence, deviation of the consensus from the true consensus value, and expected convergence rate, when there exists additional information of trust between agents. We show that under certain conditions on the stochastic trust values and consensus protocol: 1) almost sure convergence to a common limit value is possible even when malicious agents constitute more than half of the network connectivity, 2) the deviation of the converged limit, from the case where there is no attack, i.e., the true consensus value, can be bounded with probability that approaches 1 exponentially, and 3) correct classification of malicious and legitimate agents can be attained in finite time almost surely. Further, the expected convergence rate decays exponentially as a function of the quality of the trust observations between agents.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Strong Duality Result for Constrained POMDPs with Multiple Cooperative Agents</title>
<link>https://arxiv.org/abs/2303.14932</link>
<guid>https://arxiv.org/abs/2303.14932</guid>
<content:encoded><![CDATA[
arXiv:2303.14932v2 Announce Type: replace-cross 
Abstract: The work studies the problem of decentralized constrained POMDPs in a team-setting where multiple nonstrategic agents have asymmetric information. Using an extension of Sion's Minimax theorem for functions with positive infinity and results on weak-convergence of measures, strong duality is established for the setting of infinite-horizon expected total discounted costs when the observations lie in a countable space, the actions are chosen from a finite space, the constraint costs are bounded, and the objective cost is bounded from below.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence of Collective Accuracy in Socially Connected Networks</title>
<link>https://arxiv.org/abs/2411.08625</link>
<guid>https://arxiv.org/abs/2411.08625</guid>
<content:encoded><![CDATA[
arXiv:2411.08625v2 Announce Type: replace-cross 
Abstract: We analyze the accuracy of collective decision-making in socially connected populations, where agents update binary choices through local interactions on a network. Each agent receives a private signal that is biased -- even marginally -- toward the correct alternative, and social influence mediates the aggregation of these signals. We show analytically that, in the large-population limit, the probability of a correct majority converges to a nontrivial expression involving the regularized incomplete beta function. Remarkably, this collective accuracy surpasses that of any individual agent whenever private signals are better than random, revealing that network-mediated influence can enhance, rather than impair, group performance. Our findings may inform the design of resilient decision-making systems in social, biological, and engineered networks, where accuracy must emerge from interdependent and noisy agents.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear Supervision for Nonlinear, High-Dimensional Neural Control and Differential Games</title>
<link>https://arxiv.org/abs/2412.02033</link>
<guid>https://arxiv.org/abs/2412.02033</guid>
<content:encoded><![CDATA[
arXiv:2412.02033v3 Announce Type: replace-cross 
Abstract: As the dimension of a system increases, traditional methods for control and differential games rapidly become intractable, making the design of safe autonomous agents challenging in complex or team settings. Deep-learning approaches avoid discretization and yield numerous successes in robotics and autonomy, but at a higher dimensional limit, accuracy falls as sampling becomes less efficient. We propose using rapidly generated linear solutions to the partial differential equation (PDE) arising in the problem to accelerate and improve learned value functions for guidance in high-dimensional, nonlinear problems. We define two programs that combine supervision of the linear solution with a standard PDE loss. We demonstrate that these programs offer improvements in speed and accuracy in both a 50-D differential game problem and a 10-D quadrotor control problem.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed and Localized Covariance Control of Coupled Systems: A System Level Approach</title>
<link>https://arxiv.org/abs/2503.02094</link>
<guid>https://arxiv.org/abs/2503.02094</guid>
<content:encoded><![CDATA[
arXiv:2503.02094v2 Announce Type: replace-cross 
Abstract: This work is concerned with the finite-horizon optimal covariance steering of networked systems governed by discrete-time stochastic linear dynamics. In contrast with existing work that has only considered systems with dynamically decoupled agents, we consider a dynamically coupled system composed of interconnected subsystems subject to local communication constraints. In particular, we propose a distributed algorithm to compute the localized optimal feedback control policy for each individual subsystem, which depends only on the local state histories of its neighboring subsystems. Utilizing the system-level synthesis (SLS) framework, we first recast the localized covariance steering problem as a convex SLS problem with locality constraints. Subsequently, exploiting its partially separable structure, we decompose the latter problem into smaller subproblems, introducing a transformation to deal with nonseparable instances. Finally, we employ a variation of the consensus alternating direction method of multipliers (ADMM) to distribute computation across subsystems on account of their local information and communication constraints. We demonstrate the effectiveness of our proposed algorithm on a power system with 36 interconnected subsystems.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Near-Driven Autonomous Rover Navigation in Complex Environments: Extensions to Urban Search-and-Rescue and Industrial Inspection</title>
<link>https://arxiv.org/abs/2504.17794</link>
<guid>https://arxiv.org/abs/2504.17794</guid>
<content:encoded><![CDATA[
arXiv:2504.17794v1 Announce Type: new 
Abstract: This paper explores the use of an extended neuroevolutionary approach, based on NeuroEvolution of Augmenting Topologies (NEAT), for autonomous robots in dynamic environments associated with hazardous tasks like firefighting, urban search-and-rescue (USAR), and industrial inspections. Building on previous research, it expands the simulation environment to larger and more complex settings, demonstrating NEAT's adaptability across different applications. By integrating recent advancements in NEAT and reinforcement learning, the study uses modern simulation frameworks for realism and hybrid algorithms for optimization. Experimental results show that NEAT-evolved controllers achieve success rates comparable to state-of-the-art deep reinforcement learning methods, with superior structural adaptability. The agents reached ~80% success in outdoor tests, surpassing baseline models. The paper also highlights the benefits of transfer learning among tasks and evaluates the effectiveness of NEAT in complex 3D navigation. Contributions include evaluating NEAT for diverse autonomous applications and discussing real-world deployment considerations, emphasizing the approach's potential as an alternative or complement to deep reinforcement learning in autonomous navigation tasks.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow Matching Ergodic Coverage</title>
<link>https://arxiv.org/abs/2504.17872</link>
<guid>https://arxiv.org/abs/2504.17872</guid>
<content:encoded><![CDATA[
arXiv:2504.17872v1 Announce Type: new 
Abstract: Ergodic coverage effectively generates exploratory behaviors for embodied agents by aligning the spatial distribution of the agent's trajectory with a target distribution, where the difference between these two distributions is measured by the ergodic metric. However, existing ergodic coverage methods are constrained by the limited set of ergodic metrics available for control synthesis, fundamentally limiting their performance. In this work, we propose an alternative approach to ergodic coverage based on flow matching, a technique widely used in generative inference for efficient and scalable sampling. We formally derive the flow matching problem for ergodic coverage and show that it is equivalent to a linear quadratic regulator problem with a closed-form solution. Our formulation enables alternative ergodic metrics from generative inference that overcome the limitations of existing ones. These metrics were previously infeasible for control synthesis but can now be supported with no computational overhead. Specifically, flow matching with the Stein variational gradient flow enables control synthesis directly over the score function of the target distribution, improving robustness to the unnormalized distributions; on the other hand, flow matching with the Sinkhorn divergence flow enables an optimal transport-based ergodic metric, improving coverage performance on non-smooth distributions with irregular supports. We validate the improved performance and competitive computational efficiency of our method through comprehensive numerical benchmarks and across different nonlinear dynamics. We further demonstrate the practicality of our method through a series of drawing and erasing tasks on a Franka robot.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do We Need Transformers to Play FPS Video Games?</title>
<link>https://arxiv.org/abs/2504.17891</link>
<guid>https://arxiv.org/abs/2504.17891</guid>
<content:encoded><![CDATA[
arXiv:2504.17891v1 Announce Type: new 
Abstract: In this paper, we explore the Transformer based architectures for reinforcement learning in both online and offline settings within the Doom game environment. Our investigation focuses on two primary approaches: Deep Transformer Q- learning Networks (DTQN) for online learning and Decision Transformers (DT) for offline reinforcement learning. DTQN leverages the sequential modelling capabilities of Transformers to enhance Q-learning in partially observable environments,while Decision Transformers repurpose sequence modelling techniques to enable offline agents to learn from past trajectories without direct interaction with the environment. We conclude that while Transformers might have performed well in Atari games, more traditional methods perform better than Transformer based method in both the settings in the VizDoom environment.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents</title>
<link>https://arxiv.org/abs/2504.17934</link>
<guid>https://arxiv.org/abs/2504.17934</guid>
<content:encoded><![CDATA[
arXiv:2504.17934v1 Announce Type: new 
Abstract: The rise of Large Language Models (LLMs) has revolutionized Graphical User Interface (GUI) automation through LLM-powered GUI agents, yet their ability to process sensitive data with limited human oversight raises significant privacy and security risks. This position paper identifies three key risks of GUI agents and examines how they differ from traditional GUI automation and general autonomous agents. Despite these risks, existing evaluations focus primarily on performance, leaving privacy and security assessments largely unexplored. We review current evaluation metrics for both GUI and general LLM agents and outline five key challenges in integrating human evaluators for GUI agent assessments. To address these gaps, we advocate for a human-centered evaluation framework that incorporates risk assessments, enhances user awareness through in-context consent, and embeds privacy and security considerations into GUI agent design and evaluation.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborating Action by Action: A Multi-agent LLM Framework for Embodied Reasoning</title>
<link>https://arxiv.org/abs/2504.17950</link>
<guid>https://arxiv.org/abs/2504.17950</guid>
<content:encoded><![CDATA[
arXiv:2504.17950v1 Announce Type: new 
Abstract: Collaboration is ubiquitous and essential in day-to-day life -- from exchanging ideas, to delegating tasks, to generating plans together. This work studies how LLMs can adaptively collaborate to perform complex embodied reasoning tasks. To this end we introduce MINDcraft, an easily extensible platform built to enable LLM agents to control characters in the open-world game of Minecraft; and MineCollab, a benchmark to test the different dimensions of embodied and collaborative reasoning. An experimental study finds that the primary bottleneck in collaborating effectively for current state-of-the-art agents is efficient natural language communication, with agent performance dropping as much as 15% when they are required to communicate detailed task completion plans. We conclude that existing LLM agents are ill-optimized for multi-agent collaboration, especially in embodied scenarios, and highlight the need to employ methods beyond in-context and imitation learning. Our website can be found here: https://mindcraft-minecollab.github.io/
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plug-and-Play Physics-informed Learning using Uncertainty Quantified Port-Hamiltonian Models</title>
<link>https://arxiv.org/abs/2504.17966</link>
<guid>https://arxiv.org/abs/2504.17966</guid>
<content:encoded><![CDATA[
arXiv:2504.17966v1 Announce Type: new 
Abstract: The ability to predict trajectories of surrounding agents and obstacles is a crucial component in many robotic applications. Data-driven approaches are commonly adopted for state prediction in scenarios where the underlying dynamics are unknown. However, the performance, reliability, and uncertainty of data-driven predictors become compromised when encountering out-of-distribution observations relative to the training data. In this paper, we introduce a Plug-and-Play Physics-Informed Machine Learning (PnP-PIML) framework to address this challenge. Our method employs conformal prediction to identify outlier dynamics and, in that case, switches from a nominal predictor to a physics-consistent model, namely distributed Port-Hamiltonian systems (dPHS). We leverage Gaussian processes to model the energy function of the dPHS, enabling not only the learning of system dynamics but also the quantification of predictive uncertainty through its Bayesian nature. In this way, the proposed framework produces reliable physics-informed predictions even for the out-of-distribution scenarios.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Agent Swarm for Hypothesis-Driven Drug Discovery</title>
<link>https://arxiv.org/abs/2504.17967</link>
<guid>https://arxiv.org/abs/2504.17967</guid>
<content:encoded><![CDATA[
arXiv:2504.17967v1 Announce Type: new 
Abstract: Drug discovery remains a formidable challenge: more than 90 percent of candidate molecules fail in clinical evaluation, and development costs often exceed one billion dollars per approved therapy. Disparate data streams, from genomics and transcriptomics to chemical libraries and clinical records, hinder coherent mechanistic insight and slow progress. Meanwhile, large language models excel at reasoning and tool integration but lack the modular specialization and iterative memory required for regulated, hypothesis-driven workflows. We introduce PharmaSwarm, a unified multi-agent framework that orchestrates specialized LLM "agents" to propose, validate, and refine hypotheses for novel drug targets and lead compounds. Each agent accesses dedicated functionality--automated genomic and expression analysis; a curated biomedical knowledge graph; pathway enrichment and network simulation; interpretable binding affinity prediction--while a central Evaluator LLM continuously ranks proposals by biological plausibility, novelty, in silico efficacy, and safety. A shared memory layer captures validated insights and fine-tunes underlying submodels over time, yielding a self-improving system. Deployable on low-code platforms or Kubernetes-based microservices, PharmaSwarm supports literature-driven discovery, omics-guided target identification, and market-informed repurposing. We also describe a rigorous four-tier validation pipeline spanning retrospective benchmarking, independent computational assays, experimental testing, and expert user studies to ensure transparency, reproducibility, and real-world impact. By acting as an AI copilot, PharmaSwarm can accelerate translational research and deliver high-confidence hypotheses more efficiently than traditional pipelines.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Bugs to Benchmarks: A Comprehensive Survey of Software Defect Datasets</title>
<link>https://arxiv.org/abs/2504.17977</link>
<guid>https://arxiv.org/abs/2504.17977</guid>
<content:encoded><![CDATA[
arXiv:2504.17977v1 Announce Type: new 
Abstract: Software defect datasets, which are collections of software bugs and their associated information, are essential resources for researchers and practitioners in software engineering and beyond. Such datasets facilitate empirical research and enable standardized benchmarking for a wide range of techniques, including fault detection, fault localization, test generation, test prioritization, automated program repair, and emerging areas like agentic AI-based software development. Over the years, numerous software defect datasets with diverse characteristics have been developed, providing rich resources for the community, yet making it increasingly difficult to navigate the landscape. To address this challenge, this article provides a comprehensive survey of 132 software defect datasets. The survey discusses the scope of existing datasets, e.g., regarding the application domain of the buggy software, the types of defects, and the programming languages used. We also examine the construction of these datasets, including the data sources and construction methods employed. Furthermore, we assess the availability and usability of the datasets, validating their availability and examining how defects are presented. To better understand the practical uses of these datasets, we analyze the publications that cite them, revealing that the primary use cases are evaluations of new techniques and empirical research. Based on our comprehensive review of the existing datasets, this paper suggests potential opportunities for future research, including addressing underrepresented kinds of defects, enhancing availability and usability through better dataset organization, and developing more efficient strategies for dataset construction and maintenance.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sky-Drive: A Distributed Multi-Agent Simulation Platform for Socially-Aware and Human-AI Collaborative Future Transportation</title>
<link>https://arxiv.org/abs/2504.18010</link>
<guid>https://arxiv.org/abs/2504.18010</guid>
<content:encoded><![CDATA[
arXiv:2504.18010v1 Announce Type: new 
Abstract: Recent advances in autonomous system simulation platforms have significantly enhanced the safe and scalable testing of driving policies. However, existing simulators do not yet fully meet the needs of future transportation research, particularly in modeling socially-aware driving agents and enabling effective human-AI collaboration. This paper introduces Sky-Drive, a novel distributed multi-agent simulation platform that addresses these limitations through four key innovations: (a) a distributed architecture for synchronized simulation across multiple terminals; (b) a multi-modal human-in-the-loop framework integrating diverse sensors to collect rich behavioral data; (c) a human-AI collaboration mechanism supporting continuous and adaptive knowledge exchange; and (d) a digital twin (DT) framework for constructing high-fidelity virtual replicas of real-world transportation environments. Sky-Drive supports diverse applications such as autonomous vehicle (AV)-vulnerable road user (VRU) interaction modeling, human-in-the-loop training, socially-aware reinforcement learning, personalized driving policy, and customized scenario generation. Future extensions will incorporate foundation models for context-aware decision support and hardware-in-the-loop (HIL) testing for real-world validation. By bridging scenario generation, data collection, algorithm training, and hardware integration, Sky-Drive has the potential to become a foundational platform for the next generation of socially-aware and human-centered autonomous transportation research. The demo video and code are available at:https://sky-lab-uw.github.io/Sky-Drive-website/
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and Theory of Mind</title>
<link>https://arxiv.org/abs/2504.18039</link>
<guid>https://arxiv.org/abs/2504.18039</guid>
<content:encoded><![CDATA[
arXiv:2504.18039v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents have demonstrated impressive capabilities in social deduction games (SDGs) like Werewolf, where strategic reasoning and social deception are essential. However, current approaches remain limited to textual information, ignoring crucial multimodal cues such as facial expressions and tone of voice that humans naturally use to communicate. Moreover, existing SDG agents primarily focus on inferring other players' identities without modeling how others perceive themselves or fellow players. To address these limitations, we use One Night Ultimate Werewolf (ONUW) as a testbed and present MultiMind, the first framework integrating multimodal information into SDG agents. MultiMind processes facial expressions and vocal tones alongside verbal content, while employing a Theory of Mind (ToM) model to represent each player's suspicion levels toward others. By combining this ToM model with Monte Carlo Tree Search (MCTS), our agent identifies communication strategies that minimize suspicion directed at itself. Through comprehensive evaluation in both agent-versus-agent simulations and studies with human players, we demonstrate MultiMind's superior performance in gameplay. Our work presents a significant advancement toward LLM agents capable of human-like social reasoning across multimodal domains.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Personality-Aware Interactions in Salesperson Dialogue Agents</title>
<link>https://arxiv.org/abs/2504.18058</link>
<guid>https://arxiv.org/abs/2504.18058</guid>
<content:encoded><![CDATA[
arXiv:2504.18058v1 Announce Type: new 
Abstract: The integration of dialogue agents into the sales domain requires a deep understanding of how these systems interact with users possessing diverse personas. This study explores the influence of user personas, defined using the Myers-Briggs Type Indicator (MBTI), on the interaction quality and performance of sales-oriented dialogue agents. Through large-scale testing and analysis, we assess the pre-trained agent's effectiveness, adaptability, and personalization capabilities across a wide range of MBTI-defined user types. Our findings reveal significant patterns in interaction dynamics, task completion rates, and dialogue naturalness, underscoring the future potential for dialogue agents to refine their strategies to better align with varying personality traits. This work not only provides actionable insights for building more adaptive and user-centric conversational systems in the sales domain but also contributes broadly to the field by releasing persona-defined user simulators. These simulators, unconstrained by domain, offer valuable tools for future research and demonstrate the potential for scaling personalized dialogue systems across diverse applications.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating Function-Level TARA for Automotive Full-Lifecycle Security</title>
<link>https://arxiv.org/abs/2504.18083</link>
<guid>https://arxiv.org/abs/2504.18083</guid>
<content:encoded><![CDATA[
arXiv:2504.18083v1 Announce Type: new 
Abstract: As modern vehicles evolve into intelligent and connected systems, their growing complexity introduces significant cybersecurity risks. Threat Analysis and Risk Assessment (TARA) has therefore become essential for managing these risks under mandatory regulations. However, existing TARA automation methods rely on static threat libraries, limiting their utility in the detailed, function-level analyses demanded by industry. This paper introduces DefenseWeaver, the first system that automates function-level TARA using component-specific details and large language models (LLMs). DefenseWeaver dynamically generates attack trees and risk evaluations from system configurations described in an extended OpenXSAM++ format, then employs a multi-agent framework to coordinate specialized LLM roles for more robust analysis. To further adapt to evolving threats and diverse standards, DefenseWeaver incorporates Low-Rank Adaptation (LoRA) fine-tuning and Retrieval-Augmented Generation (RAG) with expert-curated TARA reports. We validated DefenseWeaver through deployment in four automotive security projects, where it identified 11 critical attack paths, verified through penetration testing, and subsequently reported and remediated by the relevant automakers and suppliers. Additionally, DefenseWeaver demonstrated cross-domain adaptability, successfully applying to unmanned aerial vehicles (UAVs) and marine navigation systems. In comparison to human experts, DefenseWeaver outperformed manual attack tree generation across six assessment scenarios. Integrated into commercial cybersecurity platforms such as UAES and Xiaomi, DefenseWeaver has generated over 8,200 attack trees. These results highlight its ability to significantly reduce processing time, and its scalability and transformative impact on cybersecurity across industries.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Less: SINDy Surrogates in RL</title>
<link>https://arxiv.org/abs/2504.18113</link>
<guid>https://arxiv.org/abs/2504.18113</guid>
<content:encoded><![CDATA[
arXiv:2504.18113v1 Announce Type: new 
Abstract: This paper introduces an approach for developing surrogate environments in reinforcement learning (RL) using the Sparse Identification of Nonlinear Dynamics (SINDy) algorithm. We demonstrate the effectiveness of our approach through extensive experiments in OpenAI Gym environments, particularly Mountain Car and Lunar Lander. Our results show that SINDy-based surrogate models can accurately capture the underlying dynamics of these environments while reducing computational costs by 20-35%. With only 75 interactions for Mountain Car and 1000 for Lunar Lander, we achieve state-wise correlations exceeding 0.997, with mean squared errors as low as 3.11e-06 for Mountain Car velocity and 1.42e-06 for LunarLander position. RL agents trained in these surrogate environments require fewer total steps (65,075 vs. 100,000 for Mountain Car and 801,000 vs. 1,000,000 for Lunar Lander) while achieving comparable performance to those trained in the original environments, exhibiting similar convergence patterns and final performance metrics. This work contributes to the field of model-based RL by providing an efficient method for generating accurate, interpretable surrogate environments.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple Target Tracking Using a UAV Swarm in Maritime Environments</title>
<link>https://arxiv.org/abs/2504.18153</link>
<guid>https://arxiv.org/abs/2504.18153</guid>
<content:encoded><![CDATA[
arXiv:2504.18153v1 Announce Type: new 
Abstract: Nowadays, unmanned aerial vehicles (UAVs) are increasingly utilized in search and rescue missions, a trend driven by technological advancements, including enhancements in automation, avionics, and the reduced cost of electronics. In this work, we introduce a collaborative model predictive control (MPC) framework aimed at addressing the joint problem of guidance and state estimation for tracking multiple castaway targets with a fleet of autonomous UAV agents. We assume that each UAV agent is equipped with a camera sensor, which has a limited sensing range and is utilized for receiving noisy observations from multiple moving castaways adrift in maritime conditions. We derive a nonlinear mixed integer programming (NMIP) -based controller that facilitates the guidance of the UAVs by generating non-myopic trajectories within a receding planning horizon. These trajectories are designed to minimize the tracking error across multiple targets by directing the UAV fleet to locations expected to yield targets measurements, thereby minimizing the uncertainty of the estimated target states. Extensive simulation experiments validate the effectiveness of our proposed method in tracking multiple castaways in maritime environments.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depth-Constrained ASV Navigation with Deep RL and Limited Sensing</title>
<link>https://arxiv.org/abs/2504.18253</link>
<guid>https://arxiv.org/abs/2504.18253</guid>
<content:encoded><![CDATA[
arXiv:2504.18253v1 Announce Type: new 
Abstract: Autonomous Surface Vehicles (ASVs) play a crucial role in maritime operations, yet their navigation in shallow-water environments remains challenging due to dynamic disturbances and depth constraints. Traditional navigation strategies struggle with limited sensor information, making safe and efficient operation difficult. In this paper, we propose a reinforcement learning (RL) framework for ASV navigation under depth constraints, where the vehicle must reach a target while avoiding unsafe areas with only a single depth measurement per timestep from a downward-facing Single Beam Echosounder (SBES). To enhance environmental awareness, we integrate Gaussian Process (GP) regression into the RL framework, enabling the agent to progressively estimate a bathymetric depth map from sparse sonar readings. This approach improves decision-making by providing a richer representation of the environment. Furthermore, we demonstrate effective sim-to-real transfer, ensuring that trained policies generalize well to real-world aquatic conditions. Experimental results validate our method's capability to improve ASV navigation performance while maintaining safety in challenging shallow-water environments.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGI: Multi-Agent Guided Interview for Psychiatric Assessment</title>
<link>https://arxiv.org/abs/2504.18260</link>
<guid>https://arxiv.org/abs/2504.18260</guid>
<content:encoded><![CDATA[
arXiv:2504.18260v1 Announce Type: new 
Abstract: Automating structured clinical interviews could revolutionize mental healthcare accessibility, yet existing large language models (LLMs) approaches fail to align with psychiatric diagnostic protocols. We present MAGI, the first framework that transforms the gold-standard Mini International Neuropsychiatric Interview (MINI) into automatic computational workflows through coordinated multi-agent collaboration. MAGI dynamically navigates clinical logic via four specialized agents: 1) an interview tree guided navigation agent adhering to the MINI's branching structure, 2) an adaptive question agent blending diagnostic probing, explaining, and empathy, 3) a judgment agent validating whether the response from participants meet the node, and 4) a diagnosis Agent generating Psychometric Chain-of- Thought (PsyCoT) traces that explicitly map symptoms to clinical criteria. Experimental results on 1,002 real-world participants covering depression, generalized anxiety, social anxiety and suicide shows that MAGI advances LLM- assisted mental health assessment by combining clinical rigor, conversational adaptability, and explainable reasoning.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning Based Navigation with Macro Actions and Topological Maps</title>
<link>https://arxiv.org/abs/2504.18300</link>
<guid>https://arxiv.org/abs/2504.18300</guid>
<content:encoded><![CDATA[
arXiv:2504.18300v1 Announce Type: new 
Abstract: This paper addresses the challenge of navigation in large, visually complex environments with sparse rewards. We propose a method that uses object-oriented macro actions grounded in a topological map, allowing a simple Deep Q-Network (DQN) to learn effective navigation policies. The agent builds a map by detecting objects from RGBD input and selecting discrete macro actions that correspond to navigating to these objects. This abstraction drastically reduces the complexity of the underlying reinforcement learning problem and enables generalization to unseen environments. We evaluate our approach in a photorealistic 3D simulation and show that it significantly outperforms a random baseline under both immediate and terminal reward conditions. Our results demonstrate that topological structure and macro-level abstraction can enable sample-efficient learning even from pixel data.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Adaptive Software Agents for Debugging</title>
<link>https://arxiv.org/abs/2504.18316</link>
<guid>https://arxiv.org/abs/2504.18316</guid>
<content:encoded><![CDATA[
arXiv:2504.18316v1 Announce Type: new 
Abstract: Using multiple agents was found to improve the debugging capabilities of Large Language Models. However, increasing the number of LLM-agents has several drawbacks such as increasing the running costs and rising the risk for the agents to lose focus. In this work, we propose an adaptive agentic design, where the number of agents and their roles are determined dynamically based on the characteristics of the task to be achieved. In this design, the agents roles are not predefined, but are generated after analyzing the problem to be solved. Our initial evaluation shows that, with the adaptive design, the number of agents that are generated depends on the complexity of the buggy code. In fact, for simple code with mere syntax issues, the problem was usually fixed using one agent only. However, for more complex problems, we noticed the creation of a higher number of agents. Regarding the effectiveness of the fix, we noticed an average improvement of 11% compared to the one-shot prompting. Given these promising results, we outline future research directions to improve our design for adaptive software agents that can autonomously plan and conduct their software goals.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Control of Sensor-Induced Illusions on Robotic Agents</title>
<link>https://arxiv.org/abs/2504.18339</link>
<guid>https://arxiv.org/abs/2504.18339</guid>
<content:encoded><![CDATA[
arXiv:2504.18339v1 Announce Type: new 
Abstract: This paper presents a novel problem of creating and regulating localization and navigation illusions considering two agents: a receiver and a producer. A receiver is moving on a plane localizing itself using the intensity of signals from three known towers observed at its position. Based on this position estimate, it follows a simple policy to reach its goal. The key idea is that a producer alters the signal intensities to alter the position estimate of the receiver while ensuring it reaches a different destination with the belief that it reached its goal. We provide a precise mathematical formulation of this problem and show that it allows standard techniques from control theory to be applied to generate localization and navigation illusions that result in a desired receiver behavior.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Data Auditing in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2504.18349</link>
<guid>https://arxiv.org/abs/2504.18349</guid>
<content:encoded><![CDATA[
arXiv:2504.18349v1 Announce Type: new 
Abstract: With the surge of large language models (LLMs), Large Vision-Language Models (VLMs)--which integrate vision encoders with LLMs for accurate visual grounding--have shown great potential in tasks like generalist agents and robotic control. However, VLMs are typically trained on massive web-scraped images, raising concerns over copyright infringement and privacy violations, and making data auditing increasingly urgent. Membership inference (MI), which determines whether a sample was used in training, has emerged as a key auditing technique, with promising results on open-source VLMs like LLaVA (AUC > 80%). In this work, we revisit these advances and uncover a critical issue: current MI benchmarks suffer from distribution shifts between member and non-member images, introducing shortcut cues that inflate MI performance. We further analyze the nature of these shifts and propose a principled metric based on optimal transport to quantify the distribution discrepancy. To evaluate MI in realistic settings, we construct new benchmarks with i.i.d. member and non-member images. Existing MI methods fail under these unbiased conditions, performing only marginally better than chance. Further, we explore the theoretical upper bound of MI by probing the Bayes Optimality within the VLM's embedding space and find the irreducible error rate remains high. Despite this pessimistic outlook, we analyze why MI for VLMs is particularly challenging and identify three practical scenarios--fine-tuning, access to ground-truth texts, and set-based inference--where auditing becomes feasible. Our study presents a systematic view of the limits and opportunities of MI for VLMs, providing guidance for future efforts in trustworthy data auditing.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Affordance Detection on 3D Point Clouds with Probabilistic Prototypes</title>
<link>https://arxiv.org/abs/2504.18355</link>
<guid>https://arxiv.org/abs/2504.18355</guid>
<content:encoded><![CDATA[
arXiv:2504.18355v1 Announce Type: new 
Abstract: Robotic agents need to understand how to interact with objects in their environment, both autonomously and during human-robot interactions. Affordance detection on 3D point clouds, which identifies object regions that allow specific interactions, has traditionally relied on deep learning models like PointNet++, DGCNN, or PointTransformerV3. However, these models operate as black boxes, offering no insight into their decision-making processes. Prototypical Learning methods, such as ProtoPNet, provide an interpretable alternative to black-box models by employing a "this looks like that" case-based reasoning approach. However, they have been primarily applied to image-based tasks. In this work, we apply prototypical learning to models for affordance detection on 3D point clouds. Experiments on the 3D-AffordanceNet benchmark dataset show that prototypical models achieve competitive performance with state-of-the-art black-box models and offer inherent interpretability. This makes prototypical models a promising candidate for human-robot interaction scenarios that require increased trust and safety.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant</title>
<link>https://arxiv.org/abs/2504.18373</link>
<guid>https://arxiv.org/abs/2504.18373</guid>
<content:encoded><![CDATA[
arXiv:2504.18373v1 Announce Type: new 
Abstract: In recent years, multi-agent frameworks powered by large language models (LLMs) have advanced rapidly. Despite this progress, there is still a notable absence of benchmark datasets specifically tailored to evaluate their performance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset aimed at evaluating LLM-based multi-agent frameworks in the context of intelligent personal assistants. Auto-SLURP extends the original SLURP dataset -- initially developed for natural language understanding tasks -- by relabeling the data and integrating simulated servers and external services. This enhancement enables a comprehensive end-to-end evaluation pipeline, covering language understanding, task execution, and response generation. Our experiments demonstrate that Auto-SLURP presents a significant challenge for current state-of-the-art frameworks, highlighting that truly reliable and intelligent multi-agent personal assistants remain a work in progress. The dataset and related code are available at https://github.com/lorashen/Auto-SLURP/.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Dwell-times for Switched Nonlinear Systems using Memory Regression Extension</title>
<link>https://arxiv.org/abs/2504.18457</link>
<guid>https://arxiv.org/abs/2504.18457</guid>
<content:encoded><![CDATA[
arXiv:2504.18457v1 Announce Type: new 
Abstract: This paper presents a switched systems approach for extending the dwell-time of an autonomous agent during GPS-denied operation by leveraging memory regressor extension (MRE) techniques. To maintain accurate trajectory tracking despite unknown dynamics and environmental disturbances, the agent periodically acquires access to GPS, allowing it to correct accumulated state estimation errors. The motivation for this work arises from the limitations of existing switched system approaches, where increasing estimation errors during GPS-denied intervals and overly conservative dwell-time conditions restrict the operational efficiency of the agent. By leveraging MRE techniques during GPS-available intervals, the developed method refines the estimates of unknown system parameters, thereby enabling longer and more reliable operation in GPS-denied environments. A Lyapunov-based switched-system stability analysis establishes that improved parameter estimates obtained through concurrent learning allow extended operation in GPS-denied intervals without compromising closed-loop system stability. Simulation results validate the theoretical findings, demonstrating dwell-time extensions and enhanced trajectory tracking performance.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instrumentation for Better Demonstrations: A Case Study</title>
<link>https://arxiv.org/abs/2504.18481</link>
<guid>https://arxiv.org/abs/2504.18481</guid>
<content:encoded><![CDATA[
arXiv:2504.18481v1 Announce Type: new 
Abstract: Learning from demonstrations is a powerful paradigm for robot manipulation, but its effectiveness hinges on both the quantity and quality of the collected data. In this work, we present a case study of how instrumentation, i.e. integration of sensors, can improve the quality of demonstrations and automate data collection. We instrument a squeeze bottle with a pressure sensor to learn a liquid dispensing task, enabling automated data collection via a PI controller. Transformer-based policies trained on automated demonstrations outperform those trained on human data in 78% of cases. Our findings indicate that instrumentation not only facilitates scalable data collection but also leads to better-performing policies, highlighting its potential in the pursuit of generalist robotic agents.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalization Guarantees for Multi-View Representation Learning and Application to Regularization via Gaussian Product Mixture Prior</title>
<link>https://arxiv.org/abs/2504.18455</link>
<guid>https://arxiv.org/abs/2504.18455</guid>
<content:encoded><![CDATA[
arXiv:2504.18455v1 Announce Type: cross 
Abstract: We study the problem of distributed multi-view representation learning. In this problem, $K$ agents observe each one distinct, possibly statistically correlated, view and independently extracts from it a suitable representation in a manner that a decoder that gets all $K$ representations estimates correctly the hidden label. In the absence of any explicit coordination between the agents, a central question is: what should each agent extract from its view that is necessary and sufficient for a correct estimation at the decoder? In this paper, we investigate this question from a generalization error perspective. First, we establish several generalization bounds in terms of the relative entropy between the distribution of the representations extracted from training and "test" datasets and a data-dependent symmetric prior, i.e., the Minimum Description Length (MDL) of the latent variables for all views and training and test datasets. Then, we use the obtained bounds to devise a regularizer; and investigate in depth the question of the selection of a suitable prior. In particular, we show and conduct experiments that illustrate that our data-dependent Gaussian mixture priors with judiciously chosen weights lead to good performance. For single-view settings (i.e., $K=1$), our experimental results are shown to outperform existing prior art Variational Information Bottleneck (VIB) and Category-Dependent VIB (CDVIB) approaches. Interestingly, we show that a weighted attention mechanism emerges naturally in this setting. Finally, for the multi-view setting, we show that the selection of the joint prior as a Gaussians product mixture induces a Gaussian mixture marginal prior for each marginal view and implicitly encourages the agents to extract and output redundant features, a finding which is somewhat counter-intuitive.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PreGSU-A Generalized Traffic Scene Understanding Model for Autonomous Driving based on Pre-trained Graph Attention Network</title>
<link>https://arxiv.org/abs/2404.10263</link>
<guid>https://arxiv.org/abs/2404.10263</guid>
<content:encoded><![CDATA[
arXiv:2404.10263v2 Announce Type: replace 
Abstract: Scene understanding, defined as learning, extraction, and representation of interactions among traffic elements, is one of the critical challenges toward high-level autonomous driving (AD). Current scene understanding methods mainly focus on one concrete single task, such as trajectory prediction and risk level evaluation. Although they perform well on specific metrics, the generalization ability is insufficient to adapt to the real traffic complexity and downstream demand diversity. In this study, we propose PreGSU, a generalized pre-trained scene understanding model based on graph attention network to learn the universal interaction and reasoning of traffic scenes to support various downstream tasks. After the feature engineering and sub-graph module, all elements are embedded as nodes to form a dynamic weighted graph. Then, four graph attention layers are applied to learn the relationships among agents and lanes. In the pre-train phase, the understanding model is trained on two self-supervised tasks: Virtual Interaction Force (VIF) modeling and Masked Road Modeling (MRM). Based on the artificial potential field theory, VIF modeling enables PreGSU to capture the agent-to-agent interactions while MRM extracts agent-to-road connections. In the fine-tuning process, the pre-trained parameters are loaded to derive detailed understanding outputs. We conduct validation experiments on three datasets and two downstream tasks, i.e., trajectory prediction in urban scenario and intention recognition in highway scenario, to verify the model's generalization and understanding capabilities. Results show that compared with single-task-driven baselines, PreGSU achieves competitive performance on all datasets and downstream tasks, indicating its potential to be generalized to various scenes and targets. Ablation study shows the effectiveness of pre-train task design.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLeXplore: Accelerating Research in Intrinsically-Motivated Reinforcement Learning</title>
<link>https://arxiv.org/abs/2405.19548</link>
<guid>https://arxiv.org/abs/2405.19548</guid>
<content:encoded><![CDATA[
arXiv:2405.19548v2 Announce Type: replace 
Abstract: Extrinsic rewards can effectively guide reinforcement learning (RL) agents in specific tasks. However, extrinsic rewards frequently fall short in complex environments due to the significant human effort needed for their design and annotation. This limitation underscores the necessity for intrinsic rewards, which offer auxiliary and dense signals and can enable agents to learn in an unsupervised manner. Although various intrinsic reward formulations have been proposed, their implementation and optimization details are insufficiently explored and lack standardization, thereby hindering research progress. To address this gap, we introduce RLeXplore, a unified, highly modularized, and plug-and-play framework offering reliable implementations of eight state-of-the-art intrinsic reward methods. Furthermore, we conduct an in-depth study that identifies critical implementation details and establishes well-justified standard practices in intrinsically-motivated RL. Our documentation, examples, and source code are available at https://github.com/RLE-Foundation/RLeXplore.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRIMER: Perception-Aware Robust Learning-based Multiagent Trajectory Planner</title>
<link>https://arxiv.org/abs/2406.10060</link>
<guid>https://arxiv.org/abs/2406.10060</guid>
<content:encoded><![CDATA[
arXiv:2406.10060v3 Announce Type: replace 
Abstract: In decentralized multiagent trajectory planners, agents need to communicate and exchange their positions to generate collision-free trajectories. However, due to localization errors/uncertainties, trajectory deconfliction can fail even if trajectories are perfectly shared between agents. To address this issue, we first present PARM and PARM*, perception-aware, decentralized, asynchronous multiagent trajectory planners that enable a team of agents to navigate uncertain environments while deconflicting trajectories and avoiding obstacles using perception information. PARM* differs from PARM as it is less conservative, using more computation to find closer-to-optimal solutions. While these methods achieve state-of-the-art performance, they suffer from high computational costs as they need to solve large optimization problems onboard, making it difficult for agents to replan at high rates. To overcome this challenge, we present our second key contribution, PRIMER, a learning-based planner trained with imitation learning (IL) using PARM* as the expert demonstrator. PRIMER leverages the low computational requirements at deployment of neural networks and achieves a computation speed up to 5500 times faster than optimization-based approaches.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied Visuomotor Representation</title>
<link>https://arxiv.org/abs/2410.00287</link>
<guid>https://arxiv.org/abs/2410.00287</guid>
<content:encoded><![CDATA[
arXiv:2410.00287v2 Announce Type: replace 
Abstract: Imagine sitting at your desk, looking at various objects on it. While you do not know their exact distances from your eye in meters, you can reach out and touch them. Instead of an externally defined unit, your sense of distance is inherently tied to your action's effect on your embodiment. In contrast, conventional robotics relies on precise calibration to external units with which separate vision and control processes communicate. This necessitates highly engineered and expensive systems that cannot be easily reconfigured.
  To address this, we introduce Embodied Visuomotor Representation, a methodology through which robots infer distance in a unit implied by their actions. That is, without depending on calibrated 3D sensors or known physical models. With it, we demonstrate that a robot without prior knowledge of its size, environmental scale, or strength can quickly learn to touch and clear obstacles within seconds of operation. Likewise, in simulation, an agent without knowledge of its mass or strength can successfully jump across a gap of unknown size after a few test oscillations. These behaviors mirror natural strategies observed in bees and gerbils, which also lack calibration in an external unit, and highlight the potential for action-driven perception in robotics.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dense Dynamics-Aware Reward Synthesis: Integrating Prior Experience with Demonstrations</title>
<link>https://arxiv.org/abs/2412.01114</link>
<guid>https://arxiv.org/abs/2412.01114</guid>
<content:encoded><![CDATA[
arXiv:2412.01114v2 Announce Type: replace 
Abstract: Many continuous control problems can be formulated as sparse-reward reinforcement learning (RL) tasks. In principle, online RL methods can automatically explore the state space to solve each new task. However, discovering sequences of actions that lead to a non-zero reward becomes exponentially more difficult as the task horizon increases. Manually shaping rewards can accelerate learning for a fixed task, but it is an arduous process that must be repeated for each new environment. We introduce a systematic reward-shaping framework that distills the information contained in 1) a task-agnostic prior data set and 2) a small number of task-specific expert demonstrations, and then uses these priors to synthesize dense dynamics-aware rewards for the given task. This supervision substantially accelerates learning in our experiments, and we provide analysis demonstrating how the approach can effectively guide online learning agents to faraway goals.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetries-enhanced Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2501.01136</link>
<guid>https://arxiv.org/abs/2501.01136</guid>
<content:encoded><![CDATA[
arXiv:2501.01136v2 Announce Type: replace 
Abstract: Multi-agent reinforcement learning has emerged as a powerful framework for enabling agents to learn complex, coordinated behaviors but faces persistent challenges regarding its generalization, scalability and sample efficiency. Recent advancements have sought to alleviate those issues by embedding intrinsic symmetries of the systems in the policy. Yet, most dynamical systems exhibit little to no symmetries to exploit. This paper presents a novel framework for embedding extrinsic symmetries in multi-agent system dynamics that enables the use of symmetry-enhanced methods to address systems with insufficient intrinsic symmetries, expanding the scope of equivariant learning to a wide variety of MARL problems. Central to our framework is the Group Equivariant Graphormer, a group-modular architecture specifically designed for distributed swarming tasks. Extensive experiments on a swarm of symmetry-breaking quadrotors validate the effectiveness of our approach, showcasing its potential for improved generalization and zero-shot scalability. Our method achieves significant reductions in collision rates and enhances task success rates across a diverse range of scenarios and varying swarm sizes.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can We Govern the Agent-to-Agent Economy?</title>
<link>https://arxiv.org/abs/2501.16606</link>
<guid>https://arxiv.org/abs/2501.16606</guid>
<content:encoded><![CDATA[
arXiv:2501.16606v2 Announce Type: replace 
Abstract: Current approaches to AI governance often fall short in anticipating a future where AI agents manage critical tasks, such as financial operations, administrative functions, and beyond. While cryptocurrencies could serve as the foundation for monetizing value exchange in a collaboration and delegation dynamic among AI agents, a critical question remains: how can humans ensure meaningful oversight and control as a future economy of AI agents scales and evolves? In this philosophical exploration, we highlight emerging concepts in the industry to inform research and development efforts in anticipation of a future decentralized agentic economy.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMT: A Visual Multi-Task Benchmark Dataset for Autonomous Driving in the Arab Gulf Region</title>
<link>https://arxiv.org/abs/2502.19260</link>
<guid>https://arxiv.org/abs/2502.19260</guid>
<content:encoded><![CDATA[
arXiv:2502.19260v3 Announce Type: replace 
Abstract: This paper introduces the Emirates Multi-Task (EMT) dataset, designed to support multi-task benchmarking within a unified framework. It comprises over 30,000 frames from a dash-camera perspective and 570,000 annotated bounding boxes, covering approximately 150 kilometers of driving routes that reflect the distinctive road topology, congestion patterns, and driving behavior of Gulf region traffic. The dataset supports three primary tasks: tracking, trajectory forecasting, and intention prediction. Each benchmark is accompanied by corresponding evaluations: (1) multi-agent tracking experiments addressing multi-class scenarios and occlusion handling; (2) trajectory forecasting evaluation using deep sequential and interaction-aware models; and (3) intention prediction experiments based on observed trajectories. The dataset is publicly available at https://avlab.io/emt-dataset, with pre-processing scripts and evaluation models at https://github.com/AV-Lab/emt-dataset.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Application of linear regression and quasi-Newton methods to the deep reinforcement learning in continuous action cases</title>
<link>https://arxiv.org/abs/2503.14976</link>
<guid>https://arxiv.org/abs/2503.14976</guid>
<content:encoded><![CDATA[
arXiv:2503.14976v3 Announce Type: replace 
Abstract: The linear regression (LR) method offers the advantage that optimal parameters can be calculated relatively easily, although its representation capability is limited than that of the deep learning technique. To improve deep reinforcement learning, the Least Squares Deep Q Network (LS-DQN) method was proposed by Levine et al., which combines Deep Q Network (DQN) with LR method. However, the LS-DQN method assumes that the actions are discrete. In this study, we propose the Double Least Squares Deep Deterministic Policy Gradient (DLS-DDPG) method to address this limitation. This method combines the LR method with the Deep Deterministic Policy Gradient (DDPG) technique, one of the representative deep reinforcement learning algorithms for continuous action cases. For the LR update of the critic network, DLS-DDPG uses an algorithm similar to the Fitted Q iteration, the method which LS-DQN adopted. In addition, we calculated the optimal action using the quasi-Newton method and used it as both the agent's action and the training data for the LR update of the actor network. Numerical experiments conducted in MuJoCo environments showed that the proposed method improved performance at least in some tasks, although there are difficulties such as the inability to make the regularization terms small.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2504.16939</link>
<guid>https://arxiv.org/abs/2504.16939</guid>
<content:encoded><![CDATA[
arXiv:2504.16939v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have propelled conversational AI from traditional dialogue systems into sophisticated agents capable of autonomous actions, contextual awareness, and multi-turn interactions with users. Yet, fundamental questions about their capabilities, limitations, and paths forward remain open. This survey paper presents a desideratum for next-generation Conversational Agents - what has been achieved, what challenges persist, and what must be done for more scalable systems that approach human-level intelligence. To that end, we systematically analyze LLM-driven Conversational Agents by organizing their capabilities into three primary dimensions: (i) Reasoning - logical, systematic thinking inspired by human intelligence for decision making, (ii) Monitor - encompassing self-awareness and user interaction monitoring, and (iii) Control - focusing on tool utilization and policy following. Building upon this, we introduce a novel taxonomy by classifying recent work on Conversational Agents around our proposed desideratum. We identify critical research gaps and outline key directions, including realistic evaluations, long-term multi-turn reasoning skills, self-evolution capabilities, collaborative and multi-agent task completion, personalization, and proactivity. This work aims to provide a structured foundation, highlight existing limitations, and offer insights into potential future research directions for Conversational Agents, ultimately advancing progress toward Artificial General Intelligence (AGI). We maintain a curated repository of papers at: https://github.com/emrecanacikgoz/awesome-conversational-agents.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MobileCity: An Efficient Framework for Large-Scale Urban Behavior Simulation</title>
<link>https://arxiv.org/abs/2504.16946</link>
<guid>https://arxiv.org/abs/2504.16946</guid>
<content:encoded><![CDATA[
arXiv:2504.16946v1 Announce Type: new 
Abstract: Generative agents offer promising capabilities for simulating realistic urban behaviors. However, existing methods oversimplify transportation choices in modern cities, and require prohibitive computational resources for large-scale population simulation. To address these limitations, we first present a virtual city that features multiple functional buildings and transportation modes. Then, we conduct extensive surveys to model behavioral choices and mobility preferences among population groups. Building on these insights, we introduce a simulation framework that captures the complexity of urban mobility while remaining scalable, enabling the simulation of over 4,000 agents. To assess the realism of the generated behaviors, we perform a series of micro and macro-level analyses. Beyond mere performance comparison, we explore insightful experiments, such as predicting crowd density from movement patterns and identifying trends in vehicle preferences across agent demographics.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments</title>
<link>https://arxiv.org/abs/2504.17087</link>
<guid>https://arxiv.org/abs/2504.17087</guid>
<content:encoded><![CDATA[
arXiv:2504.17087v1 Announce Type: new 
Abstract: Large language models (LLMs) are being widely applied across various fields, but as tasks become more complex, evaluating their responses is increasingly challenging. Compared to human evaluators, the use of LLMs to support performance evaluation offers a more efficient alternative. However, most studies focus mainly on aligning LLMs' judgments with human preferences, overlooking the existence of biases and mistakes in human judgment. Furthermore, how to select suitable LLM judgments given multiple potential LLM responses remains underexplored. To address these two aforementioned issues, we propose a three-stage meta-judge selection pipeline: 1) developing a comprehensive rubric with GPT-4 and human experts, 2) using three advanced LLM agents to score judgments, and 3) applying a threshold to filter out low-scoring judgments. Compared to methods using a single LLM as both judge and meta-judge, our pipeline introduces multi-agent collaboration and a more comprehensive rubric. Experimental results on the JudgeBench dataset show about 15.55\% improvement compared to raw judgments and about 8.37\% improvement over the single-agent baseline. Our work demonstrates the potential of LLMs as meta-judges and lays the foundation for future research on constructing preference datasets for LLM-as-a-judge reinforcement learning.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PACE: A Framework for Learning and Control in Linear Incomplete-Information Differential Games</title>
<link>https://arxiv.org/abs/2504.17128</link>
<guid>https://arxiv.org/abs/2504.17128</guid>
<content:encoded><![CDATA[
arXiv:2504.17128v1 Announce Type: new 
Abstract: In this paper, we address the problem of a two-player linear quadratic differential game with incomplete information, a scenario commonly encountered in multi-agent control, human-robot interaction (HRI), and approximation methods for solving general-sum differential games. While solutions to such linear differential games are typically obtained through coupled Riccati equations, the complexity increases when agents have incomplete information, particularly when neither is aware of the other's cost function. To tackle this challenge, we propose a model-based Peer-Aware Cost Estimation (PACE) framework for learning the cost parameters of the other agent. In PACE, each agent treats its peer as a learning agent rather than a stationary optimal agent, models their learning dynamics, and leverages this dynamic to infer the cost function parameters of the other agent. This approach enables agents to infer each other's objective function in real time based solely on their previous state observations and dynamically adapt their control policies. Furthermore, we provide a theoretical guarantee for the convergence of parameter estimation and the stability of system states in PACE. Additionally, in our numerical studies, we demonstrate how modeling the learning dynamics of the other agent benefits PACE, compared to approaches that approximate the other agent as having complete information, particularly in terms of stability and convergence speed.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Peer-Aware Cost Estimation in Nonlinear General-Sum Dynamic Games for Mutual Learning and Intent Inference</title>
<link>https://arxiv.org/abs/2504.17129</link>
<guid>https://arxiv.org/abs/2504.17129</guid>
<content:encoded><![CDATA[
arXiv:2504.17129v1 Announce Type: new 
Abstract: Human-robot interactions can be modeled as incomplete-information general-sum dynamic games since the objective functions of both agents are not explicitly known to each other. However, solving for equilibrium policies for such games presents a major challenge, especially if the games involve nonlinear underlying dynamics. To simplify the problem, existing work often assumes that one agent is an expert with complete information about its peer, which can lead to biased estimates and failures in coordination. To address this challenge, we propose a nonlinear peer-aware cost estimation (N-PACE) algorithm for general-sum dynamic games. In N-PACE, using iterative linear quadratic (LQ) approximation of the nonlinear general-sum game, each agent explicitly models the learning dynamics of its peer agent while inferring their objective functions, leading to unbiased fast learning in inferring the unknown objective function of the peer agent, which is critical for task completion and safety assurance. Additionally, we demonstrate how N-PACE enables \textbf{intent communication} in such multi-agent systems by explicitly modeling the peer's learning dynamics.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning</title>
<link>https://arxiv.org/abs/2504.17192</link>
<guid>https://arxiv.org/abs/2504.17192</guid>
<content:encoded><![CDATA[
arXiv:2504.17192v1 Announce Type: new 
Abstract: Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work. In the meantime, recent Large Language Models (LLMs) excel at understanding scientific documents and generating high-quality code. Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that transforms machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, where it constructs a high-level roadmap, designs the system architecture with diagrams, identifies file dependencies, and generates configuration files; analysis, which focuses on interpreting implementation-specific details; and generation, where modular, dependency-aware code is produced. Moreover, each phase is instantiated through a set of specialized agents designed to collaborate effectively across the pipeline. We then evaluate PaperCoder on generating code implementations from machine learning papers based on both model-based and human evaluations, specifically from the original paper authors, with author-released repositories as ground truth if available. Our results demonstrate the effectiveness of PaperCoder in creating high-quality, faithful implementations. Furthermore, it consistently shows strengths in the recently released PaperBench benchmark, surpassing strong baselines by substantial margins.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation</title>
<link>https://arxiv.org/abs/2504.17200</link>
<guid>https://arxiv.org/abs/2504.17200</guid>
<content:encoded><![CDATA[
arXiv:2504.17200v1 Announce Type: new 
Abstract: Large language models (LLMs) are a transformational capability at the frontier of artificial intelligence and machine learning that can support decision-makers in addressing pressing societal challenges such as extreme natural hazard events. As generalized models, LLMs often struggle to provide context-specific information, particularly in areas requiring specialized knowledge. In this work we propose a retrieval-augmented generation (RAG)-based multi-agent LLM system to support analysis and decision-making in the context of natural hazards and extreme weather events. As a proof of concept, we present WildfireGPT, a specialized system focused on wildfire hazards. The architecture employs a user-centered, multi-agent design to deliver tailored risk insights across diverse stakeholder groups. By integrating natural hazard and extreme weather projection data, observational datasets, and scientific literature through an RAG framework, the system ensures both the accuracy and contextual relevance of the information it provides. Evaluation across ten expert-led case studies demonstrates that WildfireGPT significantly outperforms existing LLM-based solutions for decision support.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery Simulation</title>
<link>https://arxiv.org/abs/2504.17207</link>
<guid>https://arxiv.org/abs/2504.17207</guid>
<content:encoded><![CDATA[
arXiv:2504.17207v1 Announce Type: new 
Abstract: We present a framework for perspective-aware reasoning in vision-language models (VLMs) through mental imagery simulation. Perspective-taking, the ability to perceive an environment or situation from an alternative viewpoint, is a key benchmark for human-level visual understanding, essential for environmental interaction and collaboration with autonomous agents. Despite advancements in spatial reasoning within VLMs, recent research has shown that modern VLMs significantly lack perspective-aware reasoning capabilities and exhibit a strong bias toward egocentric interpretations. To bridge the gap between VLMs and human perception, we focus on the role of mental imagery, where humans perceive the world through abstracted representations that facilitate perspective shifts. Motivated by this, we propose a framework for perspective-aware reasoning, named Abstract Perspective Change (APC), that effectively leverages vision foundation models, such as object detection, segmentation, and orientation estimation, to construct scene abstractions and enable perspective transformations. Our experiments on synthetic and real-image benchmarks, compared with various VLMs, demonstrate significant improvements in perspective-aware reasoning with our framework, further outperforming fine-tuned spatial reasoning models and novel-view-synthesis-based approaches.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCAF: Efficient Agent-based Video Understanding Framework through Multimodal Coarse-to-Fine Attention Focusing</title>
<link>https://arxiv.org/abs/2504.17213</link>
<guid>https://arxiv.org/abs/2504.17213</guid>
<content:encoded><![CDATA[
arXiv:2504.17213v1 Announce Type: new 
Abstract: Even in the era of rapid advances in large models, video understanding, particularly long videos, remains highly challenging. Compared with textual or image-based information, videos commonly contain more information with redundancy, requiring large models to strategically allocate attention at a global level for accurate comprehension. To address this, we propose MCAF, an agent-based, training-free framework perform video understanding through Multimodal Coarse-to-fine Attention Focusing. The key innovation lies in its ability to sense and prioritize segments of the video that are highly relevant to the understanding task. First, MCAF hierarchically concentrates on highly relevant frames through multimodal information, enhancing the correlation between the acquired contextual information and the query. Second, it employs a dilated temporal expansion mechanism to mitigate the risk of missing crucial details when extracting information from these concentrated frames. In addition, our framework incorporates a self-reflection mechanism utilizing the confidence level of the model's responses as feedback. By iteratively applying these two creative focusing strategies, it adaptively adjusts attention to capture highly query-connected context and thus improves response accuracy. MCAF outperforms comparable state-of-the-art methods on average. On the EgoSchema dataset, it achieves a remarkable 5% performance gain over the leading approach. Meanwhile, on Next-QA and IntentQA datasets, it outperforms the current state-of-the-art standard by 0.2% and 0.3% respectively. On the Video-MME dataset, which features videos averaging nearly an hour in length, MCAF also outperforms other agent-based methods.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cracking the Code of Action: a Generative Approach to Affordances for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.17282</link>
<guid>https://arxiv.org/abs/2504.17282</guid>
<content:encoded><![CDATA[
arXiv:2504.17282v1 Announce Type: new 
Abstract: Agents that can autonomously navigate the web through a graphical user interface (GUI) using a unified action space (e.g., mouse and keyboard actions) can require very large amounts of domain-specific expert demonstrations to achieve good performance. Low sample efficiency is often exacerbated in sparse-reward and large-action-space environments, such as a web GUI, where only a few actions are relevant in any given situation. In this work, we consider the low-data regime, with limited or no access to expert behavior. To enable sample-efficient learning, we explore the effect of constraining the action space through $\textit{intent-based affordances}$ -- i.e., considering in any situation only the subset of actions that achieve a desired outcome. We propose $\textbf{Code as Generative Affordances}$ $(\textbf{$\texttt{CoGA}$})$, a method that leverages pre-trained vision-language models (VLMs) to generate code that determines affordable actions through implicit intent-completion functions and using a fully-automated program generation and verification pipeline. These programs are then used in-the-loop of a reinforcement learning agent to return a set of affordances given a pixel observation. By greatly reducing the number of actions that an agent must consider, we demonstrate on a wide range of tasks in the MiniWob++ benchmark that: $\textbf{1)}$ $\texttt{CoGA}$ is orders of magnitude more sample efficient than its RL agent, $\textbf{2)}$ $\texttt{CoGA}$'s programs can generalize within a family of tasks, and $\textbf{3)}$ $\texttt{CoGA}$ performs better or on par compared with behavior cloning when a small number of expert demonstrations is available.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DataScout: Automatic Data Fact Retrieval for Statement Augmentation with an LLM-Based Agent</title>
<link>https://arxiv.org/abs/2504.17334</link>
<guid>https://arxiv.org/abs/2504.17334</guid>
<content:encoded><![CDATA[
arXiv:2504.17334v1 Announce Type: new 
Abstract: A data story typically integrates data facts from multiple perspectives and stances to construct a comprehensive and objective narrative. However, retrieving these facts demands time for data search and challenges the creator's analytical skills. In this work, we introduce DataScout, an interactive system that automatically performs reasoning and stance-based data facts retrieval to augment the user's statement. Particularly, DataScout leverages an LLM-based agent to construct a retrieval tree, enabling collaborative control of its expansion between users and the agent. The interface visualizes the retrieval tree as a mind map that eases users to intuitively steer the retrieval direction and effectively engage in reasoning and analysis. We evaluate the proposed system through case studies and in-depth expert interviews. Our evaluation demonstrates that DataScout can effectively retrieve multifaceted data facts from different stances, helping users verify their statements and enhance the credibility of their stories.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Multi-Agent Reinforcement Learning for Automated Feature Transformation with Graph-Driven Path Optimization</title>
<link>https://arxiv.org/abs/2504.17355</link>
<guid>https://arxiv.org/abs/2504.17355</guid>
<content:encoded><![CDATA[
arXiv:2504.17355v1 Announce Type: new 
Abstract: Feature transformation methods aim to find an optimal mathematical feature-feature crossing process that generates high-value features and improves the performance of downstream machine learning tasks. Existing frameworks, though designed to mitigate manual costs, often treat feature transformations as isolated operations, ignoring dynamic dependencies between transformation steps. To address the limitations, we propose TCTO, a collaborative multi-agent reinforcement learning framework that automates feature engineering through graph-driven path optimization. The framework's core innovation lies in an evolving interaction graph that models features as nodes and transformations as edges. Through graph pruning and backtracking, it dynamically eliminates low-impact edges, reduces redundant operations, and enhances exploration stability. This graph also provides full traceability to empower TCTO to reuse high-utility subgraphs from historical transformations. To demonstrate the efficacy and adaptability of our approach, we conduct comprehensive experiments and case studies, which show superior performance across a range of datasets.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comprehend, Divide, and Conquer: Feature Subspace Exploration via Multi-Agent Hierarchical Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.17356</link>
<guid>https://arxiv.org/abs/2504.17356</guid>
<content:encoded><![CDATA[
arXiv:2504.17356v1 Announce Type: new 
Abstract: Feature selection aims to preprocess the target dataset, find an optimal and most streamlined feature subset, and enhance the downstream machine learning task. Among filter, wrapper, and embedded-based approaches, the reinforcement learning (RL)-based subspace exploration strategy provides a novel objective optimization-directed perspective and promising performance. Nevertheless, even with improved performance, current reinforcement learning approaches face challenges similar to conventional methods when dealing with complex datasets. These challenges stem from the inefficient paradigm of using one agent per feature and the inherent complexities present in the datasets. This observation motivates us to investigate and address the above issue and propose a novel approach, namely HRLFS. Our methodology initially employs a Large Language Model (LLM)-based hybrid state extractor to capture each feature's mathematical and semantic characteristics. Based on this information, features are clustered, facilitating the construction of hierarchical agents for each cluster and sub-cluster. Extensive experiments demonstrate the efficiency, scalability, and robustness of our approach. Compared to contemporary or the one-feature-one-agent RL-based approaches, HRLFS improves the downstream ML performance with iterative feature subspace exploration while accelerating total run time by reducing the number of agents involved.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Doubly Adaptive Social Learning</title>
<link>https://arxiv.org/abs/2504.17370</link>
<guid>https://arxiv.org/abs/2504.17370</guid>
<content:encoded><![CDATA[
arXiv:2504.17370v1 Announce Type: new 
Abstract: In social learning, a network of agents assigns probability scores (beliefs) to some hypotheses of interest, which rule the generation of local streaming data observed by each agent. Belief formation takes place by means of an iterative two-step procedure where: i) the agents update locally their beliefs by using some likelihood model; and ii) the updated beliefs are combined with the beliefs of the neighboring agents, using a pooling rule. This procedure can fail to perform well in the presence of dynamic drifts, leading the agents to incorrect decision making. Here, we focus on the fully online setting where both the true hypothesis and the likelihood models can change over time. We propose the doubly adaptive social learning ($\text{A}^2\text{SL}$) strategy, which infuses social learning with the necessary adaptation capabilities. This goal is achieved by exploiting two adaptation stages: i) a stochastic gradient descent update to learn and track the drifts in the decision model; ii) and an adaptive belief update to track the true hypothesis changing over time. These stages are controlled by two adaptation parameters that govern the evolution of the error probability for each agent. We show that all agents learn consistently for sufficiently small adaptation parameters, in the sense that they ultimately place all their belief mass on the true hypothesis. In particular, the probability of choosing the wrong hypothesis converges to values on the order of the adaptation parameters. The theoretical analysis is illustrated both on synthetic data and by applying the $\text{A}^2\text{SL}$ strategy to a social learning problem in the online setting using real data.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset</title>
<link>https://arxiv.org/abs/2504.17371</link>
<guid>https://arxiv.org/abs/2504.17371</guid>
<content:encoded><![CDATA[
arXiv:2504.17371v1 Announce Type: new 
Abstract: Accurate 3D trajectory data is crucial for advancing autonomous driving. Yet, traditional datasets are usually captured by fixed sensors mounted on a car and are susceptible to occlusion. Additionally, such an approach can precisely reconstruct the dynamic environment in the close vicinity of the measurement vehicle only, while neglecting objects that are further away. In this paper, we introduce the DeepScenario Open 3D Dataset (DSC3D), a high-quality, occlusion-free dataset of 6 degrees of freedom bounding box trajectories acquired through a novel monocular camera drone tracking pipeline. Our dataset includes more than 175,000 trajectories of 14 types of traffic participants and significantly exceeds existing datasets in terms of diversity and scale, containing many unprecedented scenarios such as complex vehicle-pedestrian interaction on highly populated urban streets and comprehensive parking maneuvers from entry to exit. DSC3D dataset was captured in five various locations in Europe and the United States and include: a parking lot, a crowded inner-city, a steep urban intersection, a federal highway, and a suburban intersection. Our 3D trajectory dataset aims to enhance autonomous driving systems by providing detailed environmental 3D representations, which could lead to improved obstacle interactions and safety. We demonstrate its utility across multiple applications including motion prediction, motion planning, scenario mining, and generative reactive traffic agents. Our interactive online visualization platform and the complete dataset are publicly available at app.deepscenario.com, facilitating research in motion prediction, behavior modeling, and safety validation.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGCo-MATA: Air-Ground Collaborative Multi-Agent Task Allocation in Mobile Crowdsensing</title>
<link>https://arxiv.org/abs/2504.17409</link>
<guid>https://arxiv.org/abs/2504.17409</guid>
<content:encoded><![CDATA[
arXiv:2504.17409v1 Announce Type: new 
Abstract: Rapid progress in intelligent unmanned systems has presented new opportunities for mobile crowd sensing (MCS). Today, heterogeneous air-ground collaborative multi-agent framework, which comprise unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs), have presented superior flexibility and efficiency compared to traditional homogeneous frameworks in complex sensing tasks. Within this context, task allocation among different agents always play an important role in improving overall MCS quality. In order to better allocate tasks among heterogeneous collaborative agents, in this paper, we investigated two representative complex multi-agent task allocation scenarios with dual optimization objectives: (1) For AG-FAMT (Air-Ground Few Agents More Tasks) scenario, the objectives are to maximize the task completion while minimizing the total travel distance; (2) For AG-MAFT (Air-Ground More Agents Few Tasks) scenario, where the agents are allocated based on their locations, has the optimization objectives of minimizing the total travel distance while reducing travel time cost. To achieve this, we proposed a Multi-Task Minimum Cost Maximum Flow (MT-MCMF) optimization algorithm tailored for AG-FAMT, along with a multi-objective optimization algorithm called W-ILP designed for AG-MAFT, with a particular focus on optimizing the charging path planning of UAVs. Our experiments based on a large-scale real-world dataset demonstrated that the proposed two algorithms both outperform baseline approaches under varying experimental settings, including task quantity, task difficulty, and task distribution, providing a novel way to improve the overall quality of mobile crowdsensing tasks.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plasticine: Accelerating Research in Plasticity-Motivated Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.17490</link>
<guid>https://arxiv.org/abs/2504.17490</guid>
<content:encoded><![CDATA[
arXiv:2504.17490v1 Announce Type: new 
Abstract: Developing lifelong learning agents is crucial for artificial general intelligence. However, deep reinforcement learning (RL) systems often suffer from plasticity loss, where neural networks gradually lose their ability to adapt during training. Despite its significance, this field lacks unified benchmarks and evaluation protocols. We introduce Plasticine, the first open-source framework for benchmarking plasticity optimization in deep RL. Plasticine provides single-file implementations of over 13 mitigation methods, 10 evaluation metrics, and learning scenarios with increasing non-stationarity levels from standard to open-ended environments. This framework enables researchers to systematically quantify plasticity loss, evaluate mitigation strategies, and analyze plasticity dynamics across different contexts. Our documentation, examples, and source code are available at https://github.com/RLE-Foundation/Plasticine.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication-Efficient Personalized Distributed Learning with Data and Node Heterogeneity</title>
<link>https://arxiv.org/abs/2504.17520</link>
<guid>https://arxiv.org/abs/2504.17520</guid>
<content:encoded><![CDATA[
arXiv:2504.17520v1 Announce Type: new 
Abstract: To jointly tackle the challenges of data and node heterogeneity in decentralized learning, we propose a distributed strong lottery ticket hypothesis (DSLTH), based on which a communication-efficient personalized learning algorithm is developed. In the proposed method, each local model is represented as the Hadamard product of global real-valued parameters and a personalized binary mask for pruning. The local model is learned by updating and fusing the personalized binary masks while the real-valued parameters are fixed among different agents. To further reduce the complexity of hardware implementation, we incorporate a group sparse regularization term in the loss function, enabling the learned local model to achieve structured sparsity. Then, a binary mask aggregation algorithm is designed by introducing an intermediate aggregation tensor and adding a personalized fine-tuning step in each iteration, which constrains model updates towards the local data distribution. The proposed method effectively leverages the relativity among agents while meeting personalized requirements in heterogeneous node conditions. We also provide a theoretical proof for the DSLTH, establishing it as the foundation of the proposed method. Numerical simulations confirm the validity of the DSLTH and demonstrate the effectiveness of the proposed algorithm.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cooperative Task Offloading through Asynchronous Deep Reinforcement Learning in Mobile Edge Computing for Future Networks</title>
<link>https://arxiv.org/abs/2504.17526</link>
<guid>https://arxiv.org/abs/2504.17526</guid>
<content:encoded><![CDATA[
arXiv:2504.17526v1 Announce Type: new 
Abstract: Future networks (including 6G) are poised to accelerate the realisation of Internet of Everything. However, it will result in a high demand for computing resources to support new services. Mobile Edge Computing (MEC) is a promising solution, enabling to offload computation-intensive tasks to nearby edge servers from the end-user devices, thereby reducing latency and energy consumption. However, relying solely on a single MEC server for task offloading can lead to uneven resource utilisation and suboptimal performance in complex scenarios. Additionally, traditional task offloading strategies specialise in centralised policy decisions, which unavoidably entail extreme transmission latency and reach computational bottleneck. To fill the gaps, we propose a latency and energy efficient Cooperative Task Offloading framework with Transformer-driven Prediction (CTO-TP), leveraging asynchronous multi-agent deep reinforcement learning to address these challenges. This approach fosters edge-edge cooperation and decreases the synchronous waiting time by performing asynchronous training, optimising task offloading, and resource allocation across distributed networks. The performance evaluation demonstrates that the proposed CTO-TP algorithm reduces up to 80% overall system latency and 87% energy consumption compared to the baseline schemes.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Agent, Laxity-Based Aggregation Strategy for Cost-Effective Electric Vehicle Charging and Local Transformer Overload Prevention</title>
<link>https://arxiv.org/abs/2504.17575</link>
<guid>https://arxiv.org/abs/2504.17575</guid>
<content:encoded><![CDATA[
arXiv:2504.17575v1 Announce Type: new 
Abstract: The rapid electrification of transportation, driven by stringent decarbonization targets and supportive policies, poses significant challenges for distribution system operators (DSOs). When numerous electric vehicles (EVs) charge concurrently, local transformers risk overloading - a problem that current tariff-based strategies do not adequately address. This paper introduces an aggregator-based coordination mechanism that shifts EV charging from congested to underutilized periods using a rule-based scheduling algorithm. Unlike conventional methods that depend on complex real-time pricing signals or optimization-heavy solutions, the aggregator approach uses a simple yet effective "laxity" measure to prioritize charging flexibility. To assess technical and economic viability, a multi-agent simulation was developed to replicate residential user behavior and DSO constraints under the use of a 400 kVA low-voltage transformer. The results indicate that overloads are completely eliminated with minimal inconvenience to users, whose increased charging costs are offset by the aggregator at an annual total of under DKK 6000 - significantly lower than the cost of infrastructure reinforcement. This study contributes by (i) quantifying the compensation needed to prevent large-scale overloads, (ii) presenting a replicable, computationally feasible, rule-based aggregator model for DSOs, and (iii) comparing aggregator solutions to costly transformer upgrades, underscoring the aggregator's role as a viable tool for future distribution systems.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Constraint Opinion Model</title>
<link>https://arxiv.org/abs/2504.17605</link>
<guid>https://arxiv.org/abs/2504.17605</guid>
<content:encoded><![CDATA[
arXiv:2504.17605v1 Announce Type: new 
Abstract: This paper introduces a generalised opinion model that extends the standard DeGroot model by representing agents' opinions and influences as soft constraints rather than single real values. This allows for modelling scenarios beyond the scope of the DeGroot model, such as agents sharing partial information and preferences, engaging in discussions on multiple topics simultaneously, and representing opinions with different degrees of uncertainty. By considering soft constraints as influences, the proposed model captures also situations where agents impose conditions on how others' opinions are integrated during belief revision. Finally, the flexibility offered by soft constraints allows us to introduce a novel polarisation measure that takes advantage of this generalised framework.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>'The Boring and the Tedious': Invisible Labour in India's Gig-Economy</title>
<link>https://arxiv.org/abs/2504.17697</link>
<guid>https://arxiv.org/abs/2504.17697</guid>
<content:encoded><![CDATA[
arXiv:2504.17697v1 Announce Type: new 
Abstract: India's gig-based food delivery platforms, such as Swiggy and Zomato, provide crucial income to marginalised communities but also entrench workers in cycles of invisible labour. Through 14 semi-structured interviews, we analyse waiting time and repetitive UI itneractions as key burdens that contribute to 'digital discomfort' for gig based food delivery agents. We find that workers employ creative strategies to navigate algorithmic management, yet remain constrained by platform-side 'gamification' and system opacity. We propose worker-centered GUI automation as a potential intervention to reduce friction while preserving agency. In conclusion, this position paper argues for rethinking HCI approaches in the Global South to prioritise worker autonomy over efficiency-driven design optimisations.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robotic Task Ambiguity Resolution via Natural Language Interaction</title>
<link>https://arxiv.org/abs/2504.17748</link>
<guid>https://arxiv.org/abs/2504.17748</guid>
<content:encoded><![CDATA[
arXiv:2504.17748v1 Announce Type: new 
Abstract: Language-conditioned policies have recently gained substantial adoption in robotics as they allow users to specify tasks using natural language, making them highly versatile. While much research has focused on improving the action prediction of language-conditioned policies, reasoning about task descriptions has been largely overlooked. Ambiguous task descriptions often lead to downstream policy failures due to misinterpretation by the robotic agent. To address this challenge, we introduce AmbResVLM, a novel method that grounds language goals in the observed scene and explicitly reasons about task ambiguity. We extensively evaluate its effectiveness in both simulated and real-world domains, demonstrating superior task ambiguity detection and resolution compared to recent state-of-the-art baselines. Finally, real robot experiments show that our model improves the performance of downstream robot policies, increasing the average success rate from 69.6% to 97.1%. We make the data, code, and trained models publicly available at https://ambres.cs.uni-freiburg.de.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Type-Generalized Actions for Symbolic Planning</title>
<link>https://arxiv.org/abs/2308.04867</link>
<guid>https://arxiv.org/abs/2308.04867</guid>
<content:encoded><![CDATA[
arXiv:2308.04867v2 Announce Type: replace 
Abstract: Symbolic planning is a powerful technique to solve complex tasks that require long sequences of actions and can equip an intelligent agent with complex behavior. The downside of this approach is the necessity for suitable symbolic representations describing the state of the environment as well as the actions that can change it. Traditionally such representations are carefully hand-designed by experts for distinct problem domains, which limits their transferability to different problems and environment complexities. In this paper, we propose a novel concept to generalize symbolic actions using a given entity hierarchy and observed similar behavior. In a simulated grid-based kitchen environment, we show that type-generalized actions can be learned from few observations and generalize to novel situations. Incorporating an additional on-the-fly generalization mechanism during planning, unseen task combinations, involving longer sequences, novel entities and unexpected environment behavior, can be solved.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Incentive Guarantees Behind Nash Welfare in Divisible Resources Allocation</title>
<link>https://arxiv.org/abs/2308.08903</link>
<guid>https://arxiv.org/abs/2308.08903</guid>
<content:encoded><![CDATA[
arXiv:2308.08903v3 Announce Type: replace 
Abstract: We study the problem of allocating divisible resources among $n$ agents, hopefully in a fair and efficient manner. With the presence of strategic agents, additional incentive guarantees are also necessary, and the problem of designing fair and efficient mechanisms becomes much less tractable. While there are flourishing positive results against strategic agents for homogeneous divisible items, very few of them are known to hold in cake cutting.
  We show that the Maximum Nash Welfare (MNW) mechanism, which provides desirable fairness and efficiency guarantees and achieves an incentive ratio of $2$ for homogeneous divisible items, also has an incentive ratio of $2$ in cake cutting. Remarkably, this result holds even without the free disposal assumption, which is hard to get rid of in the design of truthful cake cutting mechanisms.
  Moreover, we show that, for cake cutting, the Partial Allocation (PA) mechanism proposed by Cole et al. (EC'13), which is truthful and $1/e$-MNW for homogeneous divisible items, has an incentive ratio between $[e^{1 / e}, e]$ and when randomization is allowed, can be turned to be truthful in expectation. Given two alternatives for a trade-off between incentive ratio and Nash welfare provided by the MNW and PA mechanisms, we establish an interpolation between them for both cake cutting and homogeneous divisible items.
  Finally, we study the optimal incentive ratio achievable by envy-free cake cutting mechanisms. We first give an envy-free mechanism for two agents with an incentive ratio of $4 / 3$. Then, we show that any envy-free cake cutting mechanism with the connected pieces constraint has an incentive ratio of $\Theta(n)$.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning by Doing: An Online Causal Reinforcement Learning Framework with Causal-Aware Policy</title>
<link>https://arxiv.org/abs/2402.04869</link>
<guid>https://arxiv.org/abs/2402.04869</guid>
<content:encoded><![CDATA[
arXiv:2402.04869v2 Announce Type: replace 
Abstract: As a key component to intuitive cognition and reasoning solutions in human intelligence, causal knowledge provides great potential for reinforcement learning (RL) agents' interpretability towards decision-making by helping reduce the searching space. However, there is still a considerable gap in discovering and incorporating causality into RL, which hinders the rapid development of causal RL. In this paper, we consider explicitly modeling the generation process of states with the causal graphical model, based on which we augment the policy. We formulate the causal structure updating into the RL interaction process with active intervention learning of the environment. To optimize the derived objective, we propose a framework with theoretical performance guarantees that alternates between two steps: using interventions for causal structure learning during exploration and using the learned causal structure for policy guidance during exploitation. Due to the lack of public benchmarks that allow direct intervention in the state space, we design the root cause localization task in our simulated fault alarm environment and then empirically show the effectiveness and robustness of the proposed method against state-of-the-art baselines. Theoretical analysis shows that our performance improvement attributes to the virtuous cycle of causal-guided policy learning and causal structure learning, which aligns with our experimental results. Codes are available at https://github.com/DMIRLAB-Group/FaultAlarm_RL.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Minimizing Adversarial Counterfactual Error in Adversarial RL</title>
<link>https://arxiv.org/abs/2406.04724</link>
<guid>https://arxiv.org/abs/2406.04724</guid>
<content:encoded><![CDATA[
arXiv:2406.04724v4 Announce Type: replace 
Abstract: Deep Reinforcement Learning (DRL) policies are highly susceptible to adversarial noise in observations, which poses significant risks in safety-critical scenarios. The challenge inherent to adversarial perturbations is that by altering the information observed by the agent, the state becomes only partially observable. Existing approaches address this by either enforcing consistent actions across nearby states or maximizing the worst-case value within adversarially perturbed observations. However, the former suffers from performance degradation when attacks succeed, while the latter tends to be overly conservative, leading to suboptimal performance in benign settings. We hypothesize that these limitations stem from their failing to account for partial observability directly. To this end, we introduce a novel objective called Adversarial Counterfactual Error (ACoE), defined on the beliefs about the true state and balancing value optimization with robustness. To make ACoE scalable in model-free settings, we propose the theoretically-grounded surrogate objective Cumulative-ACoE (C-ACoE). Our empirical evaluations on standard benchmarks (MuJoCo, Atari, and Highway) demonstrate that our method significantly outperforms current state-of-the-art approaches for addressing adversarial RL challenges, offering a promising direction for improving robustness in DRL under adversarial conditions. Our code is available at https://github.com/romanbelaire/acoe-robust-rl.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentsCoMerge: Large Language Model Empowered Collaborative Decision Making for Ramp Merging</title>
<link>https://arxiv.org/abs/2408.03624</link>
<guid>https://arxiv.org/abs/2408.03624</guid>
<content:encoded><![CDATA[
arXiv:2408.03624v2 Announce Type: replace 
Abstract: Ramp merging is one of the bottlenecks in traffic systems, which commonly cause traffic congestion, accidents, and severe carbon emissions. In order to address this essential issue and enhance the safety and efficiency of connected and autonomous vehicles (CAVs) at multi-lane merging zones, we propose a novel collaborative decision-making framework, named AgentsCoMerge, to leverage large language models (LLMs). Specifically, we first design a scene observation and understanding module to allow an agent to capture the traffic environment. Then we propose a hierarchical planning module to enable the agent to make decisions and plan trajectories based on the observation and the agent's own state. In addition, in order to facilitate collaboration among multiple agents, we introduce a communication module to enable the surrounding agents to exchange necessary information and coordinate their actions. Finally, we develop a reinforcement reflection guided training paradigm to further enhance the decision-making capability of the framework. Extensive experiments are conducted to evaluate the performance of our proposed method, demonstrating its superior efficiency and effectiveness for multi-agent collaborative decision-making under various ramp merging scenarios.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Models Are Real-Time Game Engines</title>
<link>https://arxiv.org/abs/2408.14837</link>
<guid>https://arxiv.org/abs/2408.14837</guid>
<content:encoded><![CDATA[
arXiv:2408.14837v2 Announce Type: replace 
Abstract: We present GameNGen, the first game engine powered entirely by a neural model that also enables real-time interaction with a complex environment over long trajectories at high quality. When trained on the classic game DOOM, GameNGen extracts gameplay and uses it to generate a playable environment that can interactively simulate new trajectories. GameNGen runs at 20 frames per second on a single TPU and remains stable over extended multi-minute play sessions. Next frame prediction achieves a PSNR of 29.4, comparable to lossy JPEG compression. Human raters are only slightly better than random chance at distinguishing short clips of the game from clips of the simulation, even after 5 minutes of auto-regressive generation. GameNGen is trained in two phases: (1) an RL-agent learns to play the game and the training sessions are recorded, and (2) a diffusion model is trained to produce the next frame, conditioned on the sequence of past frames and actions. Conditioning augmentations help ensure stable auto-regressive generation over long trajectories, and decoder fine-tuning improves the fidelity of visual details and text.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer</title>
<link>https://arxiv.org/abs/2501.01163</link>
<guid>https://arxiv.org/abs/2501.01163</guid>
<content:encoded><![CDATA[
arXiv:2501.01163v2 Announce Type: replace 
Abstract: Current 3D Large Multimodal Models (3D LMMs) have shown tremendous potential in 3D-vision-based dialogue and reasoning. However, how to further enhance 3D LMMs to achieve fine-grained scene understanding and facilitate flexible human-agent interaction remains a challenging problem. In this work, we introduce 3D-LLaVA, a simple yet highly powerful 3D LMM designed to act as an intelligent assistant in comprehending, reasoning, and interacting with the 3D world. Unlike existing top-performing methods that rely on complicated pipelines-such as offline multi-view feature extraction or additional task-specific heads-3D-LLaVA adopts a minimalist design with integrated architecture and only takes point clouds as input. At the core of 3D-LLaVA is a new Omni Superpoint Transformer (OST), which integrates three functionalities: (1) a visual feature selector that converts and selects visual tokens, (2) a visual prompt encoder that embeds interactive visual prompts into the visual token space, and (3) a referring mask decoder that produces 3D masks based on text description. This versatile OST is empowered by the hybrid pretraining to obtain perception priors and leveraged as the visual connector that bridges the 3D data to the LLM. After performing unified instruction tuning, our 3D-LLaVA reports impressive results on various benchmarks.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents</title>
<link>https://arxiv.org/abs/2504.07347</link>
<guid>https://arxiv.org/abs/2504.07347</guid>
<content:encoded><![CDATA[
arXiv:2504.07347v2 Announce Type: replace-cross 
Abstract: As demand for Large Language Models (LLMs) and AI agents rapidly grows, optimizing systems for efficient LLM inference becomes critical. While significant efforts have focused on system-level engineering, little is explored from a mathematical modeling and queuing perspective.
  In this paper, we aim to develop the queuing fundamentals for large language model (LLM) inference, bridging the gap between the queueing theory and LLM system communities. In particular, we study the throughput aspect in LLM inference systems. We prove that a large class of 'work-conserving' scheduling algorithms can achieve maximum throughput for individual inference LLM engine, highlighting 'work-conserving' as a key design principle in practice. In a network of LLM agents, work-conserving scheduling alone is insufficient, particularly when facing specific workload structures and multi-class workflows that require more sophisticated scheduling strategies. Evaluations of real-world systems show that Orca and Sarathi-serve are throughput-optimal, reassuring practitioners, while FasterTransformer and vanilla vLLM are not maximally stable and should be used with caution. Our results highlight the substantial benefits that the queueing community can offer in improving LLM inference systems and call for more interdisciplinary development.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trusted Identities for AI Agents: Leveraging Telco-Hosted eSIM Infrastructure</title>
<link>https://arxiv.org/abs/2504.16108</link>
<guid>https://arxiv.org/abs/2504.16108</guid>
<content:encoded><![CDATA[
arXiv:2504.16108v1 Announce Type: new 
Abstract: The rise of autonomous AI agents in enterprise and industrial environments introduces a critical challenge: how to securely assign, verify, and manage their identities across distributed systems. Existing identity frameworks based on API keys, certificates, or application-layer credentials lack the infrastructure-grade trust, lifecycle control, and interoperability needed to manage agents operating independently in sensitive contexts.
  In this paper, we propose a conceptual architecture that leverages telecom-grade eSIM infrastructure, specifically hosted by mobile network operators (MNOs), to serve as a root of trust for AI agents. Rather than embedding SIM credentials in hardware devices, we envision a model where telcos host secure, certified hardware modules (eUICC or HSM) that store and manage agent-specific eSIM profiles. Agents authenticate remotely via cryptographic APIs or identity gateways, enabling scalable and auditable access to enterprise networks and services.
  We explore use cases such as onboarding enterprise automation agents, securing AI-driven financial systems, and enabling trust in inter-agent communications. We identify current limitations in GSMA and 3GPP standards, particularly their device centric assumptions, and propose extensions to support non-physical, software-based agents within trusted execution environments. This paper is intended as a conceptual framework to open discussion around standardization, security architecture, and the role of telecom infrastructure in the evolving agent economy.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOTOPIA-S4: a user-friendly system for flexible, customizable, and large-scale social simulation</title>
<link>https://arxiv.org/abs/2504.16122</link>
<guid>https://arxiv.org/abs/2504.16122</guid>
<content:encoded><![CDATA[
arXiv:2504.16122v1 Announce Type: new 
Abstract: Social simulation through large language model (LLM) agents is a promising approach to explore and validate hypotheses related to social science questions and LLM agents behavior. We present SOTOPIA-S4, a fast, flexible, and scalable social simulation system that addresses the technical barriers of current frameworks while enabling practitioners to generate multi-turn and multi-party LLM-based interactions with customizable evaluation metrics for hypothesis testing. SOTOPIA-S4 comes as a pip package that contains a simulation engine, an API server with flexible RESTful APIs for simulation management, and a web interface that enables both technical and non-technical users to design, run, and analyze simulations without programming. We demonstrate the usefulness of SOTOPIA-S4 with two use cases involving dyadic hiring negotiation and multi-party planning scenarios.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Prompt Wall (I): A Real-World Case Study of Attacking ChatGPT via Lightweight Prompt Injection</title>
<link>https://arxiv.org/abs/2504.16125</link>
<guid>https://arxiv.org/abs/2504.16125</guid>
<content:encoded><![CDATA[
arXiv:2504.16125v1 Announce Type: new 
Abstract: This report presents a real-world case study demonstrating how prompt injection can attack large language model platforms such as ChatGPT according to a proposed injection framework. By providing three real-world examples, we show how adversarial prompts can be injected via user inputs, web-based retrieval, and system-level agent instructions. These attacks, though lightweight and low-cost, can cause persistent and misleading behaviors in LLM outputs. Our case study reveals that even commercial-grade LLMs remain vulnerable to subtle manipulations that bypass safety filters and influence user decisions. \textbf{More importantly, we stress that this report is not intended as an attack guide, but as a technical alert. As ethical researchers, we aim to raise awareness and call upon developers, especially those at OpenAI, to treat prompt-level security as a critical design priority.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARFT: Multi-Agent Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2504.16129</link>
<guid>https://arxiv.org/abs/2504.16129</guid>
<content:encoded><![CDATA[
arXiv:2504.16129v1 Announce Type: new 
Abstract: LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in addressing complex, agentic tasks requiring multifaceted reasoning and collaboration, from generating high-quality presentation slides to conducting sophisticated scientific research. Meanwhile, RL has been widely recognized for its effectiveness in enhancing agent intelligence, but limited research has investigated the fine-tuning of LaMAS using foundational RL techniques. Moreover, the direct application of MARL methodologies to LaMAS introduces significant challenges, stemming from the unique characteristics and mechanisms inherent to LaMAS. To address these challenges, this article presents a comprehensive study of LLM-based MARL and proposes a novel paradigm termed Multi-Agent Reinforcement Fine-Tuning (MARFT). We introduce a universal algorithmic framework tailored for LaMAS, outlining the conceptual foundations, key distinctions, and practical implementation strategies. We begin by reviewing the evolution from RL to Reinforcement Fine-Tuning, setting the stage for a parallel analysis in the multi-agent domain. In the context of LaMAS, we elucidate critical differences between MARL and MARFT. These differences motivate a transition toward a novel, LaMAS-oriented formulation of RFT. Central to this work is the presentation of a robust and scalable MARFT framework. We detail the core algorithm and provide a complete, open-source implementation to facilitate adoption and further research. The latter sections of the paper explore real-world application perspectives and opening challenges in MARFT. By bridging theoretical underpinnings with practical methodologies, this work aims to serve as a roadmap for researchers seeking to advance MARFT toward resilient and adaptive solutions in agentic systems. Our implementation of the proposed framework is publicly available at: https://github.com/jwliao-ai/MARFT.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-Point Sampling for Distributed Bandit Convex Optimization with Time-Varying Constraints</title>
<link>https://arxiv.org/abs/2504.16211</link>
<guid>https://arxiv.org/abs/2504.16211</guid>
<content:encoded><![CDATA[
arXiv:2504.16211v1 Announce Type: new 
Abstract: This paper considers the distributed bandit convex optimization problem with time-varying constraints. In this problem, the global loss function is the average of all the local convex loss functions, which are unknown beforehand. Each agent iteratively makes its own decision subject to time-varying inequality constraints which can be violated but are fulfilled in the long run. For a uniformly jointly strongly connected time-varying directed graph, a distributed bandit online primal-dual projection algorithm with one-point sampling is proposed. We show that sublinear dynamic network regret and network cumulative constraint violation are achieved if the path-length of the benchmark also increases in a sublinear manner. In addition, an $\mathcal{O}({T^{3/4 + g}})$ static network regret bound and an $\mathcal{O}( {{T^{1 - {g}/2}}} )$ network cumulative constraint violation bound are established, where $T$ is the total number of iterations and $g \in ( {0,1/4} )$ is a trade-off parameter. Moreover, a reduced static network regret bound $\mathcal{O}( {{T^{2/3 + 4g /3}}} )$ is established for strongly convex local loss functions. Finally, a numerical example is presented to validate the theoretical results.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nash Equilibrium Learning In Large Populations With First Order Payoff Modifications</title>
<link>https://arxiv.org/abs/2504.16222</link>
<guid>https://arxiv.org/abs/2504.16222</guid>
<content:encoded><![CDATA[
arXiv:2504.16222v1 Announce Type: new 
Abstract: We establish Nash equilibrium learning -- convergence of the population state to a suitably defined Nash equilibria set -- for a class of payoff dynamical mechanism with a first order modification. The first order payoff modification can model aspects of the agents' bounded rationality, anticipatory or averaging terms in the payoff mechanism, or first order Pad\'e approximations of delays. To obtain our main results, we apply a combination of two nonstandard system-theoretic passivity notions.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Schelling segregation dynamics in densely-connected social network graphs</title>
<link>https://arxiv.org/abs/2504.16307</link>
<guid>https://arxiv.org/abs/2504.16307</guid>
<content:encoded><![CDATA[
arXiv:2504.16307v1 Announce Type: new 
Abstract: Schelling segregation is a well-established model used to investigate the dynamics of segregation in agent-based models. Since we consider segregation to be key for the development of political polarisation, we are interested in what insights it could give for this problem. We tested basic questions of segregation on an agent-based social network model where agents' connections were not restricted by their spatial position, and made the network graph much denser than previous tests of Schelling segregation in social networks.
  We found that a dense social network does not become as strongly segregated as a sparse network, and that agents' numbers of same-group neighbours do not greatly exceed their desired numbers (i.e. they do not end up more segregated than they desire to be). Furthermore, we found that the network was very difficult to polarise when one group was somewhat smaller than the other, and that it became unstable when one group was extremely small, both of which provide insights into real-world polarisation dynamics. Finally, we tested the question of whether an increase in the minority group's desire for same-group neighbours created more segregation than a similar increase for the majority group -- the "paradox of weak minority preferences" -- and found mixed evidence for this effect in a densely connected social network.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClarifyCoder: Clarification-Aware Fine-Tuning for Programmatic Problem Solving</title>
<link>https://arxiv.org/abs/2504.16331</link>
<guid>https://arxiv.org/abs/2504.16331</guid>
<content:encoded><![CDATA[
arXiv:2504.16331v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, a significant gap remains between their current performance and that of expert software engineers. A key differentiator is that human engineers actively seek clarification when faced with ambiguous requirements, while LLMs typically generate code regardless of uncertainties in the problem description. We present ClarifyCoder, a novel framework with synthetic data generation and instruction-tuning that enables LLMs to identify ambiguities and request clarification before proceeding with code generation. While recent work has focused on LLM-based agents for iterative code generation, we argue that the fundamental ability to recognize and query ambiguous requirements should be intrinsic to the models themselves. Our approach consists of two main components: (1) a data synthesis technique that augments existing programming datasets with scenarios requiring clarification to generate clarification-aware training data, and (2) a fine-tuning strategy that teaches models to prioritize seeking clarification over immediate code generation when faced with incomplete or ambiguous requirements. We further provide an empirical analysis of integrating ClarifyCoder with standard fine-tuning for a joint optimization of both clarify-awareness and coding ability. Experimental results demonstrate that ClarifyCoder significantly improves the communication capabilities of Code LLMs through meaningful clarification dialogues while maintaining code generation capabilities.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation</title>
<link>https://arxiv.org/abs/2504.16408</link>
<guid>https://arxiv.org/abs/2504.16408</guid>
<content:encoded><![CDATA[
arXiv:2504.16408v1 Announce Type: new 
Abstract: The XLLM@ACL2025 Shared Task-III formulates a low-resource structural reasoning task that challenges LLMs to generate interpretable, step-by-step rationales with minimal labeled data. We present Less is More, the third-place winning approach in the XLLM@ACL2025 Shared Task-III, which focuses on structured reasoning from only 24 labeled examples. Our approach leverages a multi-agent framework with reverse-prompt induction, retrieval-augmented reasoning synthesis via GPT-4o, and dual-stage reward-guided filtering to distill high-quality supervision across three subtasks: question parsing, CoT parsing, and step-level verification. All modules are fine-tuned from Meta-Llama-3-8B-Instruct under a unified LoRA+ setup. By combining structure validation with reward filtering across few-shot and zero-shot prompts, our pipeline consistently improves structure reasoning quality. These results underscore the value of controllable data distillation in enhancing structured inference under low-resource constraints. Our code is available at https://github.com/Jiahao-Yuan/Less-is-More.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FeedQUAC: Quick Unobtrusive AI-Generated Commentary</title>
<link>https://arxiv.org/abs/2504.16416</link>
<guid>https://arxiv.org/abs/2504.16416</guid>
<content:encoded><![CDATA[
arXiv:2504.16416v1 Announce Type: new 
Abstract: Design thrives on feedback. However, gathering constant feedback throughout the design process can be labor-intensive and disruptive. We explore how AI can bridge this gap by providing effortless, ambient feedback. We introduce FeedQUAC, a design companion that delivers real-time AI-generated commentary from a variety of perspectives through different personas. A design probe study with eight participants highlights how designers can leverage quick yet ambient AI feedback to enhance their creative workflows. Participants highlight benefits such as convenience, playfulness, confidence boost, and inspiration from this lightweight feedback agent, while suggesting additional features, like chat interaction and context curation. We discuss the role of AI feedback, its strengths and limitations, and how to integrate it into existing design workflows while balancing user involvement. Our findings also suggest that ambient interaction is a valuable consideration for both the design and evaluation of future creativity support systems.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Foundation Model-Powered Recommender Systems: From Feature-Based, Generative to Agentic Paradigms</title>
<link>https://arxiv.org/abs/2504.16420</link>
<guid>https://arxiv.org/abs/2504.16420</guid>
<content:encoded><![CDATA[
arXiv:2504.16420v1 Announce Type: new 
Abstract: Recommender systems (RS) have become essential in filtering information and personalizing content for users. RS techniques have traditionally relied on modeling interactions between users and items as well as the features of content using models specific to each task. The emergence of foundation models (FMs), large scale models trained on vast amounts of data such as GPT, LLaMA and CLIP, is reshaping the recommendation paradigm. This survey provides a comprehensive overview of the Foundation Models for Recommender Systems (FM4RecSys), covering their integration in three paradigms: (1) Feature-Based augmentation of representations, (2) Generative recommendation approaches, and (3) Agentic interactive systems. We first review the data foundations of RS, from traditional explicit or implicit feedback to multimodal content sources. We then introduce FMs and their capabilities for representation learning, natural language understanding, and multi-modal reasoning in RS contexts. The core of the survey discusses how FMs enhance RS under different paradigms. Afterward, we examine FM applications in various recommendation tasks. Through an analysis of recent research, we highlight key opportunities that have been realized as well as challenges encountered. Finally, we outline open research directions and technical challenges for next-generation FM4RecSys. This survey not only reviews the state-of-the-art methods but also provides a critical analysis of the trade-offs among the feature-based, the generative, and the agentic paradigms, outlining key open issues and future research directions.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate</title>
<link>https://arxiv.org/abs/2504.16489</link>
<guid>https://arxiv.org/abs/2504.16489</guid>
<content:encoded><![CDATA[
arXiv:2504.16489v1 Announce Type: new 
Abstract: Multi-Agent Debate (MAD), leveraging collaborative interactions among Large Language Models (LLMs), aim to enhance reasoning capabilities in complex tasks. However, the security implications of their iterative dialogues and role-playing characteristics, particularly susceptibility to jailbreak attacks eliciting harmful content, remain critically underexplored. This paper systematically investigates the jailbreak vulnerabilities of four prominent MAD frameworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo, and DeepSeek) without compromising internal agents. We introduce a novel structured prompt-rewriting framework specifically designed to exploit MAD dynamics via narrative encapsulation, role-driven escalation, iterative refinement, and rhetorical obfuscation. Our extensive experiments demonstrate that MAD systems are inherently more vulnerable than single-agent setups. Crucially, our proposed attack methodology significantly amplifies this fragility, increasing average harmfulness from 28.14% to 80.34% and achieving attack success rates as high as 80% in certain scenarios. These findings reveal intrinsic vulnerabilities in MAD architectures and underscore the urgent need for robust, specialized defenses prior to real-world deployment.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion and Reasoning for Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2504.16516</link>
<guid>https://arxiv.org/abs/2504.16516</guid>
<content:encoded><![CDATA[
arXiv:2504.16516v1 Announce Type: new 
Abstract: Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow natural language instructions and reach target locations in real-world environments. While prior methods often rely on either global scene representations or object-level features, these approaches are insufficient for capturing the complex interactions across modalities required for accurate navigation. In this paper, we propose a Multi-level Fusion and Reasoning Architecture (MFRA) to enhance the agent's ability to reason over visual observations, language instructions and navigation history. Specifically, MFRA introduces a hierarchical fusion mechanism that aggregates multi-level features-ranging from low-level visual cues to high-level semantic concepts-across multiple modalities. We further design a reasoning module that leverages fused representations to infer navigation actions through instruction-guided attention and dynamic context integration. By selectively capturing and combining relevant visual, linguistic, and temporal signals, MFRA improves decision-making accuracy in complex navigation scenarios. Extensive experiments on benchmark VLN datasets including REVERIE, R2R, and SOON demonstrate that MFRA achieves superior performance compared to state-of-the-art methods, validating the effectiveness of multi-level modal fusion for embodied navigation.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Irrationality Shapes Nash Equilibria: A Prospect-Theoretic Perspective</title>
<link>https://arxiv.org/abs/2504.16556</link>
<guid>https://arxiv.org/abs/2504.16556</guid>
<content:encoded><![CDATA[
arXiv:2504.16556v1 Announce Type: new 
Abstract: Noncooperative games with uncertain payoffs have been classically studied under the expected-utility theory framework, which relies on the strong assumption that agents behave rationally. However, simple experiments on human decision makers found them to be not fully rational, due to their subjective risk perception. Prospect theory was proposed as an empirically-grounded model to incorporate irrational behaviours into game-theoretic models. But, how prospect theory shapes the set of Nash equilibria when considering irrational agents, is still poorly understood. To this end, we study how prospect theoretic transformations may generate new equilibria while eliminating existing ones. Focusing on aggregative games, we show that capturing users' irrationality can preserve symmetric equilibria while causing the vanishing of asymmetric equilibria. Further, there exist value functions which map uncountable sets of equilibria in the expected-utility maximization framework to finite sets. This last result may shape some equilibrium selection theories for human-in-the-loop systems where computing a single equilibrium is insufficient and comparison of equilibria is needed.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM-Based Agents via Global Planning and Hierarchical Execution</title>
<link>https://arxiv.org/abs/2504.16563</link>
<guid>https://arxiv.org/abs/2504.16563</guid>
<content:encoded><![CDATA[
arXiv:2504.16563v1 Announce Type: new 
Abstract: Intelligent agent systems based on Large Language Models (LLMs) have shown great potential in real-world applications. However, existing agent frameworks still face critical limitations in task planning and execution, restricting their effectiveness and generalizability. Specifically, current planning methods often lack clear global goals, leading agents to get stuck in local branches, or produce non-executable plans. Meanwhile, existing execution mechanisms struggle to balance complexity and stability, and their limited action space restricts their ability to handle diverse real-world tasks. To address these limitations, we propose GoalAct, a novel agent framework that introduces a continuously updated global planning mechanism and integrates a hierarchical execution strategy. GoalAct decomposes task execution into high-level skills, including searching, coding, writing and more, thereby reducing planning complexity while enhancing the agents' adaptability across diverse task scenarios. We evaluate GoalAct on LegalAgentBench, a benchmark with multiple types of legal tasks that require the use of multiple types of tools. Experimental results demonstrate that GoalAct achieves state-of-the-art (SOTA) performance, with an average improvement of 12.22% in success rate. These findings highlight GoalAct's potential to drive the development of more advanced intelligent agent systems, making them more effective across complex real-world applications. Our code can be found at https://github.com/cjj826/GoalAct.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Pricing and Algorithmic Collusion</title>
<link>https://arxiv.org/abs/2504.16592</link>
<guid>https://arxiv.org/abs/2504.16592</guid>
<content:encoded><![CDATA[
arXiv:2504.16592v1 Announce Type: new 
Abstract: The rise of algorithmic pricing in online retail platforms has attracted significant interest in how autonomous software agents interact under competition. This article explores the potential emergence of algorithmic collusion - supra-competitive pricing outcomes that arise without explicit agreements - as a consequence of repeated interactions between learning agents. Most of the literature focuses on oligopoly pricing environments modeled as repeated Bertrand competitions, where firms use online learning algorithms to adapt prices over time. While experimental research has demonstrated that specific reinforcement learning algorithms can learn to maintain prices above competitive equilibrium levels in simulated environments, theoretical understanding of when and why such outcomes occur remains limited. This work highlights the interdisciplinary nature of this challenge, which connects computer science concepts of online learning with game-theoretical literature on equilibrium learning. We examine implications for the Business & Information Systems Engineering (BISE) community and identify specific research opportunities to address challenges of algorithmic competition in digital marketplaces.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery</title>
<link>https://arxiv.org/abs/2504.16728</link>
<guid>https://arxiv.org/abs/2504.16728</guid>
<content:encoded><![CDATA[
arXiv:2504.16728v1 Announce Type: new 
Abstract: The rapid advancement in capabilities of large language models (LLMs) raises a pivotal question: How can LLMs accelerate scientific discovery? This work tackles the crucial first stage of research, generating novel hypotheses. While recent work on automated hypothesis generation focuses on multi-agent frameworks and extending test-time compute, none of the approaches effectively incorporate transparency and steerability through a synergistic Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS: Interactive Research Ideation System, an open-source platform designed for researchers to leverage LLM-assisted scientific ideation. IRIS incorporates innovative features to enhance ideation, including adaptive test-time compute expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism, and query-based literature synthesis. Designed to empower researchers with greater control and insight throughout the ideation process. We additionally conduct a user study with researchers across diverse disciplines, validating the effectiveness of our system in enhancing ideation. We open-source our code at https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DYNUS: Uncertainty-aware Trajectory Planner in Dynamic Unknown Environments</title>
<link>https://arxiv.org/abs/2504.16734</link>
<guid>https://arxiv.org/abs/2504.16734</guid>
<content:encoded><![CDATA[
arXiv:2504.16734v1 Announce Type: new 
Abstract: This paper introduces DYNUS, an uncertainty-aware trajectory planner designed for dynamic unknown environments. Operating in such settings presents many challenges -- most notably, because the agent cannot predict the ground-truth future paths of obstacles, a previously planned trajectory can become unsafe at any moment, requiring rapid replanning to avoid collisions.
  Recently developed planners have used soft-constraint approaches to achieve the necessary fast computation times; however, these methods do not guarantee collision-free paths even with static obstacles. In contrast, hard-constraint methods ensure collision-free safety, but typically have longer computation times.
  To address these issues, we propose three key contributions. First, the DYNUS Global Planner (DGP) and Temporal Safe Corridor Generation operate in spatio-temporal space and handle both static and dynamic obstacles in the 3D environment. Second, the Safe Planning Framework leverages a combination of exploratory, safe, and contingency trajectories to flexibly re-route when potential future collisions with dynamic obstacles are detected. Finally, the Fast Hard-Constraint Local Trajectory Formulation uses a variable elimination approach to reduce the problem size and enable faster computation by pre-computing dependencies between free and dependent variables while still ensuring collision-free trajectories.
  We evaluated DYNUS in a variety of simulations, including dense forests, confined office spaces, cave systems, and dynamic environments. Our experiments show that DYNUS achieves a success rate of 100% and travel times that are approximately 25.0% faster than state-of-the-art methods. We also evaluated DYNUS on multiple platforms -- a quadrotor, a wheeled robot, and a quadruped -- in both simulation and hardware experiments.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of AI Agent Protocols</title>
<link>https://arxiv.org/abs/2504.16736</link>
<guid>https://arxiv.org/abs/2504.16736</guid>
<content:encoded><![CDATA[
arXiv:2504.16736v1 Announce Type: new 
Abstract: The rapid development of large language models (LLMs) has led to the widespread deployment of LLM agents across diverse industries, including customer service, content generation, data analysis, and even healthcare. However, as more LLM agents are deployed, a major issue has emerged: there is no standard way for these agents to communicate with external tools or data sources. This lack of standardized protocols makes it difficult for agents to work together or scale effectively, and it limits their ability to tackle complex, real-world tasks. A unified communication protocol for LLM agents could change this. It would allow agents and tools to interact more smoothly, encourage collaboration, and triggering the formation of collective intelligence. In this paper, we provide a systematic overview of existing communication protocols for LLM agents. We classify them into four main categories and make an analysis to help users and developers select the most suitable protocols for specific applications. Additionally, we conduct a comparative performance analysis of these protocols across key dimensions such as security, scalability, and latency. Finally, we explore future challenges, such as how protocols can adapt and survive in fast-evolving environments, and what qualities future protocols might need to support the next generation of LLM agent ecosystems. We expect this work to serve as a practical reference for both researchers and engineers seeking to design, evaluate, or integrate robust communication infrastructures for intelligent agents.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair division of the replacement-units without an appraiser in urban renewal processes</title>
<link>https://arxiv.org/abs/2504.16852</link>
<guid>https://arxiv.org/abs/2504.16852</guid>
<content:encoded><![CDATA[
arXiv:2504.16852v1 Announce Type: new 
Abstract: Rebuild and Divide is an urban renewal process that involves the demolition of old buildings and the construction of new ones. Original homeowners are compensated with upgraded apartments, while surplus units are sold for profit, so theoretically it is a win-win project for all parties involved. However, many rebuild-and-divide projects withheld or delayed due to disagreements over the assignment of new units, claiming they are not "fair". The goal of this research is to develop algorithms for envy-free allocation of the new units. The main challenge is that, in contrast to previous work on envy-free allocation, the envy depends also on the value of the old units, as people with more valuable old units are entitled to more valuable new units. We introduce three models that capture different notions of fairness: (1) the Difference Model, where agents evaluate their gains relative to others; (2) the Envy Sum Model, which permits some envy as long as the total envy does not exceed that of the original allocation; and (3) the Ratio Model, where fairness is assessed based on the proportional value of old apartments. For each model, we establish an envy criterion and seek a payment vector and allocation that ensure envy-freeness. These models present both theoretical challenges and intriguing insights. Additionally, within the Envy Sum Model, we present a mechanism that computes an allocation and payment scheme that minimizes total envy. We also analyze the mechanism's vulnerability to manipulation and identify conditions under which it is obviously manipulable.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monte Carlo Planning with Large Language Model for Text-Based Game Agents</title>
<link>https://arxiv.org/abs/2504.16855</link>
<guid>https://arxiv.org/abs/2504.16855</guid>
<content:encoded><![CDATA[
arXiv:2504.16855v1 Announce Type: new 
Abstract: Text-based games provide valuable environments for language-based autonomous agents. However, planning-then-learning paradigms, such as those combining Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably time-consuming due to extensive iterations. Additionally, these algorithms perform uncertainty-driven exploration but lack language understanding and reasoning abilities. In this paper, we introduce the Monte Carlo planning with Dynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages the language understanding and reasoning capabilities of Large Language Models (LLMs) alongside the exploratory advantages of tree search algorithms. Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms, enabling them to learn from past experiences and dynamically adjust action evaluations during planning. We conduct experiments on a series of text-based games from the Jericho benchmark. Our results demonstrate that the MC-DML algorithm significantly enhances performance across various games at the initial planning phase, outperforming strong contemporary methods that require multiple iterations. This demonstrates the effectiveness of our algorithm, paving the way for more efficient language-grounded planning in complex environments.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Reinforcement Learning and Model Predictive Control for Adaptive Control of Hydrogen-Diesel Dual-Fuel Combustion</title>
<link>https://arxiv.org/abs/2504.16875</link>
<guid>https://arxiv.org/abs/2504.16875</guid>
<content:encoded><![CDATA[
arXiv:2504.16875v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) and Machine Learning Integrated Model Predictive Control (ML-MPC) are promising approaches for optimizing hydrogen-diesel dual-fuel engine control, as they can effectively control multiple-input multiple-output systems and nonlinear processes. ML-MPC is advantageous for providing safe and optimal controls, ensuring the engine operates within predefined safety limits. In contrast, RL is distinguished by its adaptability to changing conditions through its learning-based approach. However, the practical implementation of either method alone poses challenges. RL requires high variance in control inputs during early learning phases, which can pose risks to the system by potentially executing unsafe actions, leading to mechanical damage. Conversely, ML-MPC relies on an accurate system model to generate optimal control inputs and has limited adaptability to system drifts, such as injector aging, which naturally occur in engine applications. To address these limitations, this study proposes a hybrid RL and ML-MPC approach that uses an ML-MPC framework while incorporating an RL agent to dynamically adjust the ML-MPC load tracking reference in response to changes in the environment. At the same time, the ML-MPC ensures that actions stay safe throughout the RL agent's exploration. To evaluate the effectiveness of this approach, fuel pressure is deliberately varied to introduce a model-plant mismatch between the ML-MPC and the engine test bench. The result of this mismatch is a root mean square error (RMSE) in indicated mean effective pressure of 0.57 bar when running the ML-MPC. The experimental results demonstrate that RL successfully adapts to changing boundary conditions by altering the tracking reference while ML-MPC ensures safe control inputs. The quantitative improvement in load tracking by implementing RL is an RSME of 0.44 bar.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Large Language Models know who did what to whom?</title>
<link>https://arxiv.org/abs/2504.16884</link>
<guid>https://arxiv.org/abs/2504.16884</guid>
<content:encoded><![CDATA[
arXiv:2504.16884v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are commonly criticized for not understanding language. However, many critiques focus on cognitive abilities that, in humans, are distinct from language processing. Here, we instead study a kind of understanding tightly linked to language: inferring who did what to whom (thematic roles) in a sentence. Does the central training objective of LLMs-word prediction-result in sentence representations that capture thematic roles? In two experiments, we characterized sentence representations in four LLMs. In contrast to human similarity judgments, in LLMs the overall representational similarity of sentence pairs reflected syntactic similarity but not whether their agent and patient assignments were identical vs. reversed. Furthermore, we found little evidence that thematic role information was available in any subset of hidden units. However, some attention heads robustly captured thematic roles, independently of syntax. Therefore, LLMs can extract thematic roles but, relative to humans, this information influences their representations more weakly.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building A Secure Agentic AI Application Leveraging A2A Protocol</title>
<link>https://arxiv.org/abs/2504.16902</link>
<guid>https://arxiv.org/abs/2504.16902</guid>
<content:encoded><![CDATA[
arXiv:2504.16902v1 Announce Type: new 
Abstract: As Agentic AI systems evolve from basic workflows to complex multi agent collaboration, robust protocols such as Google's Agent2Agent (A2A) become essential enablers. To foster secure adoption and ensure the reliability of these complex interactions, understanding the secure implementation of A2A is essential. This paper addresses this goal by providing a comprehensive security analysis centered on the A2A protocol. We examine its fundamental elements and operational dynamics, situating it within the framework of agent communication development. Utilizing the MAESTRO framework, specifically designed for AI risks, we apply proactive threat modeling to assess potential security issues in A2A deployments, focusing on aspects such as Agent Card management, task execution integrity, and authentication methodologies.
  Based on these insights, we recommend practical secure development methodologies and architectural best practices designed to build resilient and effective A2A systems. Our analysis also explores how the synergy between A2A and the Model Context Protocol (MCP) can further enhance secure interoperability. This paper equips developers and architects with the knowledge and practical guidance needed to confidently leverage the A2A protocol for building robust and secure next generation agentic applications.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents</title>
<link>https://arxiv.org/abs/2504.16918</link>
<guid>https://arxiv.org/abs/2504.16918</guid>
<content:encoded><![CDATA[
arXiv:2504.16918v1 Announce Type: new 
Abstract: Optimization plays a vital role in scientific research and practical applications, but formulating a concrete optimization problem described in natural language into a mathematical form and selecting a suitable solver to solve the problem requires substantial domain expertise. We introduce \textbf{OptimAI}, a framework for solving \underline{Optim}ization problems described in natural language by leveraging LLM-powered \underline{AI} agents, achieving superior performance over current state-of-the-art methods. Our framework is built upon four key roles: (1) a \emph{formulator} that translates natural language problem descriptions into precise mathematical formulations; (2) a \emph{planner} that constructs a high-level solution strategy prior to execution; and (3) a \emph{coder} and a \emph{code critic} capable of interacting with the environment and reflecting on outcomes to refine future actions. Ablation studies confirm that all roles are essential; removing the planner or code critic results in $5.8\times$ and $3.1\times$ drops in productivity, respectively. Furthermore, we introduce UCB-based debug scheduling to dynamically switch between alternative plans, yielding an additional $3.3\times$ productivity gain. Our design emphasizes multi-agent collaboration, allowing us to conveniently explore the synergistic effect of combining diverse models within a unified system. Our approach attains 88.1\% accuracy on the NLP4LP dataset and 71.2\% on the Optibench (non-linear w/o table) subset, reducing error rates by 58\% and 50\% respectively over prior best results.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Portfolio Selection through Preference Aggregation with Quicksort and the Bradley--Terry Model</title>
<link>https://arxiv.org/abs/2504.16093</link>
<guid>https://arxiv.org/abs/2504.16093</guid>
<content:encoded><![CDATA[
arXiv:2504.16093v1 Announce Type: cross 
Abstract: How to allocate limited resources to projects that will yield the greatest long-term benefits is a problem that often arises in decision-making under uncertainty. For example, organizations may need to evaluate and select innovation projects with risky returns. Similarly, when allocating resources to research projects, funding agencies are tasked with identifying the most promising proposals based on idiosyncratic criteria. Finally, in participatory budgeting, a local community may need to select a subset of public projects to fund. Regardless of context, agents must estimate the uncertain values of a potentially large number of projects. Developing parsimonious methods to compare these projects, and aggregating agent evaluations so that the overall benefit is maximized, are critical in assembling the best project portfolio. Unlike in standard sorting algorithms, evaluating projects on the basis of uncertain long-term benefits introduces additional complexities. We propose comparison rules based on Quicksort and the Bradley--Terry model, which connects rankings to pairwise "win" probabilities. In our model, each agent determines win probabilities of a pair of projects based on his or her specific evaluation of the projects' long-term benefit. The win probabilities are then appropriately aggregated and used to rank projects. Several of the methods we propose perform better than the two most effective aggregation methods currently available. Additionally, our methods can be combined with sampling techniques to significantly reduce the number of pairwise comparisons. We also discuss how the Bradley--Terry portfolio selection approach can be implemented in practice.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Safety-Privacy Tradeoff in Linear Bandits</title>
<link>https://arxiv.org/abs/2504.16371</link>
<guid>https://arxiv.org/abs/2504.16371</guid>
<content:encoded><![CDATA[
arXiv:2504.16371v1 Announce Type: cross 
Abstract: We consider a collection of linear stochastic bandit problems, each modeling the random response of different agents to proposed interventions, coupled together by a global safety constraint. We assume a central coordinator must choose actions to play on each bandit with the objective of regret minimization, while also ensuring that the expected response of all agents satisfies the global safety constraints at each round, in spite of uncertainty about the bandits' parameters. The agents consider their observed responses to be private and in order to protect their sensitive information, the data sharing with the central coordinator is performed under local differential privacy (LDP). However, providing higher level of privacy to different agents would have consequences in terms of safety and regret. We formalize these tradeoffs by building on the notion of the sharpness of the safety set - a measure of how the geometric properties of the safe set affects the growth of regret - and propose a unilaterally unimprovable vector of privacy levels for different agents given a maximum regret budget.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Regret Benchmarks in Online Non-Stochastic Control</title>
<link>https://arxiv.org/abs/2504.16581</link>
<guid>https://arxiv.org/abs/2504.16581</guid>
<content:encoded><![CDATA[
arXiv:2504.16581v1 Announce Type: cross 
Abstract: In the online non-stochastic control problem, an agent sequentially selects control inputs for a linear dynamical system when facing unknown and adversarially selected convex costs and disturbances. A common metric for evaluating control policies in this setting is policy regret, defined relative to the best-in-hindsight linear feedback controller. However, for general convex costs, this benchmark may be less meaningful since linear controllers can be highly suboptimal. To address this, we introduce an alternative, more suitable benchmark--the performance of the best fixed input. We show that this benchmark can be viewed as a natural extension of the standard benchmark used in online convex optimization and propose a novel online control algorithm that achieves sublinear regret with respect to this new benchmark. We also discuss the connections between our method and the original one proposed by Agarwal et al. in their seminal work introducing the online non-stochastic control problem, and compare the performance of both approaches through numerical simulations.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning With LLMs Interaction For Distributed Diffusion Model Services</title>
<link>https://arxiv.org/abs/2311.11094</link>
<guid>https://arxiv.org/abs/2311.11094</guid>
<content:encoded><![CDATA[
arXiv:2311.11094v2 Announce Type: replace 
Abstract: Distributed Artificial Intelligence-Generated Content (AIGC) has attracted significant attention, but two key challenges remain: maximizing subjective Quality of Experience (QoE) and improving energy efficiency, which are particularly pronounced in widely adopted Generative Diffusion Model (GDM)-based image generation services. In this paper, we propose a novel user-centric Interactive AI (IAI) approach for service management, with a distributed GDM-based AIGC framework that emphasizes efficient and cooperative deployment. The proposed method restructures the GDM inference process by allowing users with semantically similar prompts to share parts of the denoising chain. Furthermore, to maximize the users' subjective QoE, we propose an IAI approach, i.e., Reinforcement Learning With Large Language Models Interaction (RLLI), which utilizes Large Language Model (LLM)-empowered generative agents to replicate user interaction, providing real-time and subjective QoE feedback aligned with diverse user personalities. Lastly, we present the GDM-based Deep Deterministic Policy Gradient (GDDPG) algorithm, adapted to the proposed RLLI framework, to allocate communication and computing resources effectively while accounting for subjective user traits and dynamic wireless conditions. Simulation results demonstrate that G-DDPG improves total QoE by 15% compared with the standard DDPG algorithm.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatDBG: Augmenting Debugging with Large Language Models</title>
<link>https://arxiv.org/abs/2403.16354</link>
<guid>https://arxiv.org/abs/2403.16354</guid>
<content:encoded><![CDATA[
arXiv:2403.16354v4 Announce Type: replace 
Abstract: Debugging is a critical but challenging task for programmers. This paper proposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like "why is x null?". To handle these queries, ChatDBG grants the LLM autonomy to "take the wheel": it can act as an independent agent capable of querying and controlling the debugger to navigate through stacks and inspect program state. It then reports its findings and yields back control to the programmer. By leveraging the real-world knowledge embedded in LLMs, ChatDBG can diagnose issues identifiable only through the use of domain-specific reasoning. Our ChatDBG prototype integrates with standard debuggers including LLDB and GDB for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code including standalone scripts and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root causes, explain bugs, and generate accurate fixes for a wide range of real-world errors. For the Python programs, a single query led to an actionable bug fix 67% of the time; one additional follow-up query increased the success rate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded more than 75,000 times.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Sentinel: LLM Agent for Adversarial Purification</title>
<link>https://arxiv.org/abs/2405.20770</link>
<guid>https://arxiv.org/abs/2405.20770</guid>
<content:encoded><![CDATA[
arXiv:2405.20770v4 Announce Type: replace 
Abstract: Over the past two years, the use of large language models (LLMs) has advanced rapidly. While these LLMs offer considerable convenience, they also raise security concerns, as LLMs are vulnerable to adversarial attacks by some well-designed textual perturbations. In this paper, we introduce a novel defense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is designed to enhance the adversarial robustness of LLMs by purifying the adversarial textual examples before feeding them into the target LLM. Our method comprises two main components: a) Agent instruction, which can simulate a new agent for adversarial defense, altering minimal characters to maintain the original meaning of the sentence while defending against attacks; b) Defense guidance, which provides strategies for modifying clean or adversarial examples to ensure effective defense and accurate outputs from the target LLMs. Remarkably, the defense agent demonstrates robust defensive capabilities even without learning from adversarial examples. Additionally, we conduct an intriguing adversarial experiment where we develop two agents, one for defense and one for attack, and engage them in mutual confrontation. During the adversarial interactions, neither agent completely beat the other. Extensive experiments on both open-source and closed-source LLMs demonstrate that our method effectively defends against adversarial attacks, thereby enhancing adversarial robustness.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pessimism Traps and Algorithmic Interventions</title>
<link>https://arxiv.org/abs/2406.04462</link>
<guid>https://arxiv.org/abs/2406.04462</guid>
<content:encoded><![CDATA[
arXiv:2406.04462v3 Announce Type: replace 
Abstract: In this paper, we relate the philosophical literature on pessimism traps to information cascades, a formal model derived from the economics and mathematics literature. A pessimism trap is a social pattern in which individuals in a community, in situations of uncertainty, begin to copy the sub-optimal actions of others, despite their individual beliefs. This maps nicely onto the concept of an information cascade, which involves a sequence of agents making a decision between two alternatives, with a private signal of the superior alternative and a public history of others' actions. Key results from the economics literature show that information cascades occur with probability one in many contexts, and depending on the strength of the signal, populations can fall into the incorrect cascade very easily and quickly. Once formed, in the absence of external perturbation, a cascade cannot be broken -- therefore, we derive an intervention that can be used to nudge a population from an incorrect to a correct cascade and, importantly, maintain the cascade once the subsidy is discontinued. We study this both theoretically and empirically.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pushing the Frontier on Approximate EFX Allocations</title>
<link>https://arxiv.org/abs/2406.12413</link>
<guid>https://arxiv.org/abs/2406.12413</guid>
<content:encoded><![CDATA[
arXiv:2406.12413v2 Announce Type: replace 
Abstract: We study the problem of allocating a set of indivisible goods to a set of agents with additive valuation functions, aiming to achieve approximate envy-freeness up to any good ($\alpha$-EFX). The state-of-the-art results on the problem include that (exact) EFX allocations exist when (a) there are at most three agents, or (b) the agents' valuation functions can take at most two values, or (c) the agents' valuation functions can be represented via a graph. For $\alpha$-EFX, it is known that a $0.618$-EFX allocation exists for any number of agents with additive valuation functions. In this paper, we show that $2/3$-EFX allocations exist when (a) there are at most \emph{seven agents}, (b) the agents' valuation functions can take at most \emph{three values}, or (c) the agents' valuation functions can be represented via a \emph{multigraph}. Our results can be interpreted in two ways. First, by relaxing the notion of EFX to $2/3$-EFX, we obtain existence results for strict generalizations of the settings for which exact EFX allocations are known to exist. Secondly, by imposing restrictions on the setting, we manage to beat the barrier of $0.618$ and achieve an approximation guarantee of $2/3$. Therefore, our results push the \emph{frontier} of existence and computation of approximate EFX allocations, and provide insights into the challenges of settling the existence of exact EFX allocations.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Situational Safety</title>
<link>https://arxiv.org/abs/2410.06172</link>
<guid>https://arxiv.org/abs/2410.06172</guid>
<content:encoded><![CDATA[
arXiv:2410.06172v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) are rapidly evolving, demonstrating impressive capabilities as multimodal assistants that interact with both humans and their environments. However, this increased sophistication introduces significant safety concerns. In this paper, we present the first evaluation and analysis of a novel safety challenge termed Multimodal Situational Safety, which explores how safety considerations vary based on the specific situation in which the user or agent is engaged. We argue that for an MLLM to respond safely, whether through language or action, it often needs to assess the safety implications of a language query within its corresponding visual context. To evaluate this capability, we develop the Multimodal Situational Safety benchmark (MSSBench) to assess the situational safety performance of current MLLMs. The dataset comprises 1,820 language query-image pairs, half of which the image context is safe, and the other half is unsafe. We also develop an evaluation framework that analyzes key safety aspects, including explicit safety reasoning, visual understanding, and, crucially, situational safety reasoning. Our findings reveal that current MLLMs struggle with this nuanced safety problem in the instruction-following setting and struggle to tackle these situational safety challenges all at once, highlighting a key area for future research. Furthermore, we develop multi-agent pipelines to coordinately solve safety challenges, which shows consistent improvement in safety over the original MLLM response. Code and data: mssbench.github.io.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SNN-Based Online Learning of Concepts and Action Laws in an Open World</title>
<link>https://arxiv.org/abs/2411.12308</link>
<guid>https://arxiv.org/abs/2411.12308</guid>
<content:encoded><![CDATA[
arXiv:2411.12308v3 Announce Type: replace 
Abstract: We present the architecture of a fully autonomous, bio-inspired cognitive agent built around a spiking neural network (SNN) implementing the agent's semantic memory. This agent explores its universe and learns concepts of objects/situations and of its own actions in a one-shot manner. While object/situation concepts are unary, action concepts are triples made up of an initial situation, a motor activity, and an outcome. They embody the agent's knowledge of its universe's action laws. Both kinds of concepts have different degrees of generality. To make decisions the agent queries its semantic memory for the expected outcomes of envisaged actions and chooses the action to take on the basis of these predictions. Our experiments show that the agent handles new situations by appealing to previously learned general concepts and rapidly modifies its concepts to adapt to environment changes.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>{\alpha}-RACER: Real-Time Algorithm for Game-Theoretic Motion Planning and Control in Autonomous Racing using Near-Potential Function</title>
<link>https://arxiv.org/abs/2412.08855</link>
<guid>https://arxiv.org/abs/2412.08855</guid>
<content:encoded><![CDATA[
arXiv:2412.08855v2 Announce Type: replace 
Abstract: Autonomous racing extends beyond the challenge of controlling a racecar at its physical limits. Professional racers employ strategic maneuvers to outwit other competing opponents to secure victory. While modern control algorithms can achieve human-level performance by computing offline racing lines for single-car scenarios, research on real-time algorithms for multi-car autonomous racing is limited. To bridge this gap, we develop game-theoretic modeling framework that incorporates the competitive aspect of autonomous racing like overtaking and blocking through a novel policy parametrization, while operating the car at its limit. Furthermore, we propose an algorithmic approach to compute the (approximate) Nash equilibrium strategy, which represents the optimal approach in the presence of competing agents. Specifically, we introduce an algorithm inspired by recently introduced framework of dynamic near-potential function, enabling real-time computation of the Nash equilibrium. Our approach comprises two phases: offline and online. During the offline phase, we use simulated racing data to learn a near-potential function that approximates utility changes for agents. This function facilitates the online computation of approximate Nash equilibria by maximizing its value. We evaluate our method in a head-to-head 3-car racing scenario, demonstrating superior performance compared to several existing baselines.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Truthful mechanisms for linear bandit games with private contexts</title>
<link>https://arxiv.org/abs/2501.03865</link>
<guid>https://arxiv.org/abs/2501.03865</guid>
<content:encoded><![CDATA[
arXiv:2501.03865v2 Announce Type: replace 
Abstract: The contextual bandit problem, where agents arrive sequentially with personal contexts and the system adapts its arm allocation decisions accordingly, has recently garnered increasing attention for enabling more personalized outcomes. However, in many healthcare and recommendation applications, agents have private profiles and may misreport their contexts to gain from the system. For example, in adaptive clinical trials, where hospitals sequentially recruit volunteers to test multiple new treatments and adjust plans based on volunteers' reported profiles such as symptoms and interim data, participants may misreport severe side effects like allergy and nausea to avoid perceived suboptimal treatments. We are the first to study this issue of private context misreporting in a stochastic contextual bandit game between the system and non-repeated agents. We show that traditional low-regret algorithms, such as UCB family algorithms and Thompson sampling, fail to ensure truthful reporting and can result in linear regret in the worst case, while traditional truthful algorithms like explore-then-commit (ETC) and $\epsilon$-greedy algorithm incur sublinear but high regret. We propose a mechanism that uses a linear program to ensure truthfulness while minimizing deviation from Thompson sampling, yielding an $O(\ln T)$ frequentist regret. Our numerical experiments further demonstrate strong performance in multiple contexts and across other distribution families.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Do Multi-Agent LLM Systems Fail?</title>
<link>https://arxiv.org/abs/2503.13657</link>
<guid>https://arxiv.org/abs/2503.13657</guid>
<content:encoded><![CDATA[
arXiv:2503.13657v2 Announce Type: replace 
Abstract: Despite growing enthusiasm for Multi-Agent LLM Systems (MAS), their performance gains on popular benchmarks often remain minimal compared with single-agent frameworks. This gap highlights the need to systematically analyze the challenges hindering MAS effectiveness.
  We present MAST (Multi-Agent System Failure Taxonomy), the first empirically grounded taxonomy designed to understand MAS failures. We analyze seven popular MAS frameworks across over 200 tasks, involving six expert human annotators. Through this process, we identify 14 unique failure modes, organized into 3 overarching categories, (i) specification issues, (ii) inter-agent misalignment, and (iii) task verification. MAST emerges iteratively from rigorous inter-annotator agreement studies, achieving a Cohen's Kappa score of 0.88. To support scalable evaluation, we develop a validated LLM-as-a-Judge pipeline integrated with MAST. We leverage two case studies to demonstrate MAST's practical utility in analyzing failures and guiding MAS development. Our findings reveal that identified failures require more complex solutions, highlighting a clear roadmap for future research. We open source our comprehensive dataset and LLM annotator to facilitate further development of MAS.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TraCeS: Trajectory Based Credit Assignment From Sparse Safety Feedback</title>
<link>https://arxiv.org/abs/2504.12557</link>
<guid>https://arxiv.org/abs/2504.12557</guid>
<content:encoded><![CDATA[
arXiv:2504.12557v2 Announce Type: replace 
Abstract: In safe reinforcement learning (RL), auxiliary safety costs are used to align the agent to safe decision making. In practice, safety constraints, including cost functions and budgets, are unknown or hard to specify, as it requires anticipation of all possible unsafe behaviors. We therefore address a general setting where the true safety definition is unknown, and has to be learned from sparsely labeled data. Our key contributions are: first, we design a safety model that performs credit assignment to estimate each decision step's impact on the overall safety using a dataset of diverse trajectories and their corresponding binary safety labels (i.e., whether the corresponding trajectory is safe/unsafe). Second, we illustrate the architecture of our safety model to demonstrate its ability to learn a separate safety score for each timestep. Third, we reformulate the safe RL problem using the proposed safety model and derive an effective algorithm to optimize a safe yet rewarding policy. Finally, our empirical results corroborate our findings and show that this approach is effective in satisfying unknown safety definition, and scalable to various continuous control tasks.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Passivity Analysis for Nonlinear Consensus on Balanced Digraphs</title>
<link>https://arxiv.org/abs/2411.05933</link>
<guid>https://arxiv.org/abs/2411.05933</guid>
<content:encoded><![CDATA[
arXiv:2411.05933v2 Announce Type: replace-cross 
Abstract: This work deals with the output consensus problem for multiagent systems over balanced digraphs by passivity analysis. As the standard diffusive coupling structure only models the undirected interconnection, we propose a general approach capable of processing directed coupling and performing passivity analysis. To mitigate the complexity arising from the nonlinearity and directed interconnections, we reformulate the output consensus problem as a convergence analysis on a submanifold. We provide passivity analysis and establish a sufficient condition based on passivity for achieving output agreement in multi-agent systems over balanced digraphs. The results are supported by a numerical example.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StarWhisper Telescope: Agent-Based Observation Assistant System to Approach AI Astrophysicist</title>
<link>https://arxiv.org/abs/2412.06412</link>
<guid>https://arxiv.org/abs/2412.06412</guid>
<content:encoded><![CDATA[
arXiv:2412.06412v2 Announce Type: replace-cross 
Abstract: With the rapid advancements in Large Language Models (LLMs), LLM-based agents have introduced convenient and user-friendly methods for leveraging tools across various domains. In the field of astronomical observation, the construction of new telescopes has significantly increased astronomers' workload. Deploying LLM-powered agents can effectively alleviate this burden and reduce the costs associated with training personnel. Within the Nearby Galaxy Supernovae Survey (NGSS) project, which encompasses eight telescopes across three observation sites, aiming to find the transients from the galaxies in 50 mpc, we have developed the \textbf{StarWhisper Telescope System} to manage the entire observation process. This system automates tasks such as generating observation lists, conducting observations, analyzing data, and providing feedback to the observer. Observation lists are customized for different sites and strategies to ensure comprehensive coverage of celestial objects. After manual verification, these lists are uploaded to the telescopes via the agents in the system, which initiates observations upon neutral language. The observed images are analyzed in real-time, and the transients are promptly communicated to the observer. The agent modifies them into a real-time follow-up observation proposal and send to the Xinglong observatory group chat, then add them to the next-day observation lists. Additionally, the integration of AI agents within the system provides online accessibility, saving astronomers' time and encouraging greater participation from amateur astronomers in the NGSS project.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A biologically Inspired Trust Model for Open Multi-Agent Systems that is Resilient to Rapid Performance Fluctuations</title>
<link>https://arxiv.org/abs/2504.15301</link>
<guid>https://arxiv.org/abs/2504.15301</guid>
<content:encoded><![CDATA[
arXiv:2504.15301v1 Announce Type: new 
Abstract: Trust management provides an alternative solution for securing open, dynamic, and distributed multi-agent systems, where conventional cryptographic methods prove to be impractical. However, existing trust models face challenges related to agent mobility, changing behaviors, and the cold start problem. To address these issues we introduced a biologically inspired trust model in which trustees assess their own capabilities and store trust data locally. This design improves mobility support, reduces communication overhead, resists disinformation, and preserves privacy. Despite these advantages, prior evaluations revealed limitations of our model in adapting to provider population changes and continuous performance fluctuations. This study proposes a novel algorithm, incorporating a self-classification mechanism for providers to detect performance drops potentially harmful for the service consumers. Simulation results demonstrate that the new algorithm outperforms its original version and FIRE, a well-known trust and reputation model, particularly in handling dynamic trustee behavior. While FIRE remains competitive under extreme environmental changes, the proposed algorithm demonstrates greater adaptability across various conditions. In contrast to existing trust modeling research, this study conducts a comprehensive evaluation of our model using widely recognized trust model criteria, assessing its resilience against common trust-related attacks while identifying strengths, weaknesses, and potential countermeasures. Finally, several key directions for future research are proposed.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Machine Learning Agents Deal with Hard Choices?</title>
<link>https://arxiv.org/abs/2504.15304</link>
<guid>https://arxiv.org/abs/2504.15304</guid>
<content:encoded><![CDATA[
arXiv:2504.15304v1 Announce Type: new 
Abstract: Machine Learning ML agents have been increasingly used in decision-making across a wide range of tasks and environments. These ML agents are typically designed to balance multiple objectives when making choices. Understanding how their decision-making processes align with or diverge from human reasoning is essential. Human agents often encounter hard choices, that is, situations where options are incommensurable; neither option is preferred, yet the agent is not indifferent between them. In such cases, human agents can identify hard choices and resolve them through deliberation. In contrast, current ML agents, due to fundamental limitations in Multi-Objective Optimisation or MOO methods, cannot identify hard choices, let alone resolve them. Neither Scalarised Optimisation nor Pareto Optimisation, the two principal MOO approaches, can capture incommensurability. This limitation generates three distinct alignment problems: the alienness of ML decision-making behaviour from a human perspective; the unreliability of preference-based alignment strategies for hard choices; and the blockage of alignment strategies pursuing multiple objectives. Evaluating two potential technical solutions, I recommend an ensemble solution that appears most promising for enabling ML agents to identify hard choices and mitigate alignment problems. However, no known technique allows ML agents to resolve hard choices through deliberation, as they cannot autonomously change their goals. This underscores the distinctiveness of human agency and urges ML researchers to reconceptualise machine autonomy and develop frameworks and methods that can better address this fundamental gap.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PolicyEvol-Agent: Evolving Policy via Environment Perception and Self-Awareness with Theory of Mind</title>
<link>https://arxiv.org/abs/2504.15313</link>
<guid>https://arxiv.org/abs/2504.15313</guid>
<content:encoded><![CDATA[
arXiv:2504.15313v1 Announce Type: new 
Abstract: Multi-agents has exhibited significant intelligence in real-word simulations with Large language models (LLMs) due to the capabilities of social cognition and knowledge retrieval. However, existing research on agents equipped with effective cognition chains including reasoning, planning, decision-making and reflecting remains limited, especially in the dynamically interactive scenarios. In addition, unlike human, prompt-based responses face challenges in psychological state perception and empirical calibration during uncertain gaming process, which can inevitably lead to cognition bias. In light of above, we introduce PolicyEvol-Agent, a comprehensive LLM-empowered framework characterized by systematically acquiring intentions of others and adaptively optimizing irrational strategies for continual enhancement. Specifically, PolicyEvol-Agent first obtains reflective expertise patterns and then integrates a range of cognitive operations with Theory of Mind alongside internal and external perspectives. Simulation results, outperforming RL-based models and agent-based methods, demonstrate the superiority of PolicyEvol-Agent for final gaming victory. Moreover, the policy evolution mechanism reveals the effectiveness of dynamic guideline adjustments in both automatic and human evaluation.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving New Tasks by Adapting Internet Video Knowledge</title>
<link>https://arxiv.org/abs/2504.15369</link>
<guid>https://arxiv.org/abs/2504.15369</guid>
<content:encoded><![CDATA[
arXiv:2504.15369v1 Announce Type: new 
Abstract: Video generative models demonstrate great promise in robotics by serving as visual planners or as policy supervisors. When pretrained on internet-scale data, such video models intimately understand alignment with natural language, and can thus facilitate generalization to novel downstream behavior through text-conditioning. However, they may not be sensitive to the specificities of the particular environment the agent inhabits. On the other hand, training video models on in-domain examples of robotic behavior naturally encodes environment-specific intricacies, but the scale of available demonstrations may not be sufficient to support generalization to unseen tasks via natural language specification. In this work, we investigate different adaptation techniques that integrate in-domain information with large-scale pretrained video models, and explore the extent to which they enable novel text-conditioned generalization for robotic tasks, while also considering their independent data and resource considerations. We successfully demonstrate across robotic environments that adapting powerful video models with small scales of example data can successfully facilitate generalization to novel behaviors. In particular, we present a novel adaptation strategy, termed Inverse Probabilistic Adaptation, that not only consistently achieves strong generalization performance across robotic tasks and settings, but also exhibits robustness to the quality of adaptation data, successfully solving novel tasks even when only suboptimal in-domain demonstrations are available.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-Convergence Sim-to-Real Policy Transfer: A Principled Alternative to Cherry-Picking</title>
<link>https://arxiv.org/abs/2504.15414</link>
<guid>https://arxiv.org/abs/2504.15414</guid>
<content:encoded><![CDATA[
arXiv:2504.15414v1 Announce Type: new 
Abstract: Learning-based approaches, particularly reinforcement learning (RL), have become widely used for developing control policies for autonomous agents, such as locomotion policies for legged robots. RL training typically maximizes a predefined reward (or minimizes a corresponding cost/loss) by iteratively optimizing policies within a simulator. Starting from a randomly initialized policy, the empirical expected reward follows a trajectory with an overall increasing trend. While some policies become temporarily stuck in local optima, a well-defined training process generally converges to a reward level with noisy oscillations. However, selecting a policy for real-world deployment is rarely an analytical decision (i.e., simply choosing the one with the highest reward) and is instead often performed through trial and error. To improve sim-to-real transfer, most research focuses on the pre-convergence stage, employing techniques such as domain randomization, multi-fidelity training, adversarial training, and architectural innovations. However, these methods do not eliminate the inevitable convergence trajectory and noisy oscillations of rewards, leading to heuristic policy selection or cherry-picking. This paper addresses the post-convergence sim-to-real transfer problem by introducing a worst-case performance transference optimization approach, formulated as a convex quadratic-constrained linear programming problem. Extensive experiments demonstrate its effectiveness in transferring RL-based locomotion policies from simulation to real-world laboratory tests.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bare Minimum Mitigations for Autonomous AI Development</title>
<link>https://arxiv.org/abs/2504.15416</link>
<guid>https://arxiv.org/abs/2504.15416</guid>
<content:encoded><![CDATA[
arXiv:2504.15416v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is advancing rapidly, with the potential for significantly automating AI research and development itself in the near future. In 2024, international scientists, including Turing Award recipients, warned of risks from autonomous AI research and development (R&amp;D), suggesting a red line such that no AI system should be able to improve itself or other AI systems without explicit human approval and assistance. However, the criteria for meaningful human approval remain unclear, and there is limited analysis on the specific risks of autonomous AI R&amp;D, how they arise, and how to mitigate them. In this brief paper, we outline how these risks may emerge and propose four minimum safeguard recommendations applicable when AI agents significantly automate or accelerate AI development.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MRTA-Sim: A Modular Simulator for Multi-Robot Allocation, Planning, and Control in Open-World Environments</title>
<link>https://arxiv.org/abs/2504.15418</link>
<guid>https://arxiv.org/abs/2504.15418</guid>
<content:encoded><![CDATA[
arXiv:2504.15418v1 Announce Type: new 
Abstract: This paper introduces MRTA-Sim, a Python/ROS2/Gazebo simulator for testing approaches to Multi-Robot Task Allocation (MRTA) problems on simulated robots in complex, indoor environments. Grid-based approaches to MRTA problems can be too restrictive for use in complex, dynamic environments such in warehouses, department stores, hospitals, etc. However, approaches that operate in free-space often operate at a layer of abstraction above the control and planning layers of a robot and make an assumption on approximate travel time between points of interest in the system. These abstractions can neglect the impact of the tight space and multi-agent interactions on the quality of the solution. Therefore, MRTA solutions should be tested with the navigation stacks of the robots in mind, taking into account robot planning, conflict avoidance between robots, and human interaction and avoidance. This tool connects the allocation output of MRTA solvers to individual robot planning using the NAV2 stack and local, centralized multi-robot deconfliction using Control Barrier Function-Quadrtic Programs (CBF-QPs), creating a platform closer to real-world operation for more comprehensive testing of these approaches. The simulation architecture is modular so that users can swap out methods at different levels of the stack. We show the use of our system with a Satisfiability Modulo Theories (SMT)-based approach to dynamic MRTA on a fleet of indoor delivery robots.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Assisted Translation of Legacy FORTRAN Codes to C++: A Cross-Platform Study</title>
<link>https://arxiv.org/abs/2504.15424</link>
<guid>https://arxiv.org/abs/2504.15424</guid>
<content:encoded><![CDATA[
arXiv:2504.15424v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly being leveraged for generating and translating scientific computer codes by both domain-experts and non-domain experts. Fortran has served as one of the go to programming languages in legacy high-performance computing (HPC) for scientific discoveries. Despite growing adoption, LLM-based code translation of legacy code-bases has not been thoroughly assessed or quantified for its usability. Here, we studied the applicability of LLM-based translation of Fortran to C++ as a step towards building an agentic-workflow using open-weight LLMs on two different computational platforms. We statistically quantified the compilation accuracy of the translated C++ codes, measured the similarity of the LLM translated code to the human translated C++ code, and statistically quantified the output similarity of the Fortran to C++ translation.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Multi-Agent Safe Optimal Control with Distributed Epigraph Form MARL</title>
<link>https://arxiv.org/abs/2504.15425</link>
<guid>https://arxiv.org/abs/2504.15425</guid>
<content:encoded><![CDATA[
arXiv:2504.15425v1 Announce Type: new 
Abstract: Tasks for multi-robot systems often require the robots to collaborate and complete a team goal while maintaining safety. This problem is usually formalized as a constrained Markov decision process (CMDP), which targets minimizing a global cost and bringing the mean of constraint violation below a user-defined threshold. Inspired by real-world robotic applications, we define safety as zero constraint violation. While many safe multi-agent reinforcement learning (MARL) algorithms have been proposed to solve CMDPs, these algorithms suffer from unstable training in this setting. To tackle this, we use the epigraph form for constrained optimization to improve training stability and prove that the centralized epigraph form problem can be solved in a distributed fashion by each agent. This results in a novel centralized training distributed execution MARL algorithm named Def-MARL. Simulation experiments on 8 different tasks across 2 different simulators show that Def-MARL achieves the best overall performance, satisfies safety constraints, and maintains stable training. Real-world hardware experiments on Crazyflie quadcopters demonstrate the ability of Def-MARL to safely coordinate agents to complete complex collaborative tasks compared to other methods.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGI Is Coming... Right After AI Learns to Play Wordle</title>
<link>https://arxiv.org/abs/2504.15434</link>
<guid>https://arxiv.org/abs/2504.15434</guid>
<content:encoded><![CDATA[
arXiv:2504.15434v1 Announce Type: new 
Abstract: This paper investigates multimodal agents, in particular, OpenAI's Computer-User Agent (CUA), trained to control and complete tasks through a standard computer interface, similar to humans. We evaluated the agent's performance on the New York Times Wordle game to elicit model behaviors and identify shortcomings. Our findings revealed a significant discrepancy in the model's ability to recognize colors correctly depending on the context. The model had a $5.36\%$ success rate over several hundred runs across a week of Wordle. Despite the immense enthusiasm surrounding AI agents and their potential to usher in Artificial General Intelligence (AGI), our findings reinforce the fact that even simple tasks present substantial challenges for today's frontier AI models. We conclude with a discussion of the potential underlying causes, implications for future development, and research directions to improve these AI systems.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Human-AI Coordination through Adversarial Training and Generative Models</title>
<link>https://arxiv.org/abs/2504.15457</link>
<guid>https://arxiv.org/abs/2504.15457</guid>
<content:encoded><![CDATA[
arXiv:2504.15457v1 Announce Type: new 
Abstract: Being able to cooperate with new people is an important component of many economically valuable AI tasks, from household robotics to autonomous driving. However, generalizing to novel humans requires training on data that captures the diversity of human behaviors. Adversarial training is one avenue for searching for such data and ensuring that agents are robust. However, it is difficult to apply in the cooperative setting because adversarial policies intentionally learn to sabotage the task instead of simulating valid cooperation partners. To address this challenge, we propose a novel strategy for overcoming self-sabotage that combines a pre-trained generative model to simulate valid cooperative agent policies with adversarial training to maximize regret. We call our method GOAT: Generative Online Adversarial Training. In this framework, the GOAT dynamically searches for and generates coordination strategies where the learning policy -- the Cooperator agent -- underperforms. GOAT enables better generalization by exposing the Cooperator to various challenging interaction scenarios. We maintain realistic coordination strategies by updating only the generative model's embedding while keeping its parameters frozen, thus avoiding adversarial exploitation. We evaluate GOAT with real human partners, and the results demonstrate state-of-the-art performance on the Overcooked benchmark, highlighting its effectiveness in generalizing to diverse human behaviors.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent for User: Testing Multi-User Interactive Features in TikTok</title>
<link>https://arxiv.org/abs/2504.15474</link>
<guid>https://arxiv.org/abs/2504.15474</guid>
<content:encoded><![CDATA[
arXiv:2504.15474v1 Announce Type: new 
Abstract: TikTok, a widely-used social media app boasting over a billion monthly active users, requires effective app quality assurance for its intricate features. Feature testing is crucial in achieving this goal. However, the multi-user interactive features within the app, such as live streaming, voice calls, etc., pose significant challenges for developers, who must handle simultaneous device management and user interaction coordination. To address this, we introduce a novel multi-agent approach, powered by the Large Language Models (LLMs), to automate the testing of multi-user interactive app features. In detail, we build a virtual device farm that allocates the necessary number of devices for a given multi-user interactive task. For each device, we deploy an LLM-based agent that simulates a user, thereby mimicking user interactions to collaboratively automate the testing process. The evaluations on 24 multi-user interactive tasks within the TikTok app, showcase its capability to cover 75% of tasks with 85.9% action similarity and offer 87% time savings for developers. Additionally, we have also integrated our approach into the real-world TikTok testing platform, aiding in the detection of 26 multi-user interactive bugs.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-context Ranking Preference Optimization</title>
<link>https://arxiv.org/abs/2504.15477</link>
<guid>https://arxiv.org/abs/2504.15477</guid>
<content:encoded><![CDATA[
arXiv:2504.15477v1 Announce Type: new 
Abstract: Recent developments in Direct Preference Optimization (DPO) allow large language models (LLMs) to function as implicit ranking models by maximizing the margin between preferred and non-preferred responses. In practice, user feedback on such lists typically involves identifying a few relevant items in context rather than providing detailed pairwise comparisons for every possible item pair. Moreover, many complex information retrieval tasks, such as conversational agents and summarization systems, critically depend on ranking the highest-quality outputs at the top, emphasizing the need to support natural and flexible forms of user feedback. To address the challenge of limited and sparse pairwise feedback in the in-context setting, we propose an In-context Ranking Preference Optimization (IRPO) framework that directly optimizes LLMs based on ranking lists constructed during inference. To further capture flexible forms of feedback, IRPO extends the DPO objective by incorporating both the relevance of items and their positions in the list. Modeling these aspects jointly is non-trivial, as ranking metrics are inherently discrete and non-differentiable, making direct optimization difficult. To overcome this, IRPO introduces a differentiable objective based on positional aggregation of pairwise item preferences, enabling effective gradient-based optimization of discrete ranking metrics. We further provide theoretical insights showing that IRPO (i) automatically emphasizes items with greater disagreement between the model and the reference ranking, and (ii) links its gradient to an importance sampling estimator, yielding an unbiased estimator with reduced variance. Empirical results show IRPO outperforms standard DPO approaches in ranking performance, highlighting its effectiveness in aligning LLMs with direct in-context ranking preferences.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RiskNet: Interaction-Aware Risk Forecasting for Autonomous Driving in Long-Tail Scenarios</title>
<link>https://arxiv.org/abs/2504.15541</link>
<guid>https://arxiv.org/abs/2504.15541</guid>
<content:encoded><![CDATA[
arXiv:2504.15541v1 Announce Type: new 
Abstract: Ensuring the safety of autonomous vehicles (AVs) in long-tail scenarios remains a critical challenge, particularly under high uncertainty and complex multi-agent interactions. To address this, we propose RiskNet, an interaction-aware risk forecasting framework, which integrates deterministic risk modeling with probabilistic behavior prediction for comprehensive risk assessment. At its core, RiskNet employs a field-theoretic model that captures interactions among ego vehicle, surrounding agents, and infrastructure via interaction fields and force. This model supports multidimensional risk evaluation across diverse scenarios (highways, intersections, and roundabouts), and shows robustness under high-risk and long-tail settings. To capture the behavioral uncertainty, we incorporate a graph neural network (GNN)-based trajectory prediction module, which learns multi-modal future motion distributions. Coupled with the deterministic risk field, it enables dynamic, probabilistic risk inference across time, enabling proactive safety assessment under uncertainty. Evaluations on the highD, inD, and rounD datasets, spanning lane changes, turns, and complex merges, demonstrate that our method significantly outperforms traditional approaches (e.g., TTC, THW, RSS, NC Field) in terms of accuracy, responsiveness, and directional sensitivity, while maintaining strong generalization across scenarios. This framework supports real-time, scenario-adaptive risk forecasting and demonstrates strong generalization across uncertain driving environments. It offers a unified foundation for safety-critical decision-making in long-tail scenarios.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Testing and Adapting REST APIs as LLM Tools</title>
<link>https://arxiv.org/abs/2504.15546</link>
<guid>https://arxiv.org/abs/2504.15546</guid>
<content:encoded><![CDATA[
arXiv:2504.15546v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are enabling autonomous agents to perform complex workflows using external tools or functions, often provided via REST APIs in enterprise systems. However, directly utilizing these APIs as tools poses challenges due to their complex input schemas, elaborate responses, and often ambiguous documentation. Current benchmarks for tool testing do not adequately address these complexities, leading to a critical gap in evaluating API readiness for agent-driven automation. In this work, we present a novel testing framework aimed at evaluating and enhancing the readiness of REST APIs to function as tools for LLM-based agents. Our framework transforms apis as tools, generates comprehensive test cases for the APIs, translates tests cases into natural language instructions suitable for agents, enriches tool definitions and evaluates the agent's ability t correctly invoke the API and process its inputs and responses. To provide actionable insights, we analyze the outcomes of 750 test cases, presenting a detailed taxonomy of errors, including input misinterpretation, output handling inconsistencies, and schema mismatches. Additionally, we classify these test cases to streamline debugging and refinement of tool integrations. This work offers a foundational step toward enabling enterprise APIs as tools, improving their usability in agent-based applications.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Agent Framework for Automated Qinqiang Opera Script Generation Using Large Language Models</title>
<link>https://arxiv.org/abs/2504.15552</link>
<guid>https://arxiv.org/abs/2504.15552</guid>
<content:encoded><![CDATA[
arXiv:2504.15552v1 Announce Type: new 
Abstract: This paper introduces a novel multi-Agent framework that automates the end to end production of Qinqiang opera by integrating Large Language Models , visual generation, and Text to Speech synthesis. Three specialized agents collaborate in sequence: Agent1 uses an LLM to craft coherent, culturally grounded scripts;Agent2 employs visual generation models to render contextually accurate stage scenes; and Agent3 leverages TTS to produce synchronized, emotionally expressive vocal performances. In a case study on Dou E Yuan, the system achieved expert ratings of 3.8 for script fidelity, 3.5 for visual coherence, and 3.8 for speech accuracy-culminating in an overall score of 3.6, a 0.3 point improvement over a Single Agent baseline. Ablation experiments demonstrate that removing Agent2 or Agent3 leads to drops of 0.4 and 0.5 points, respectively, underscoring the value of modular collaboration. This work showcases how AI driven pipelines can streamline and scale the preservation of traditional performing arts, and points toward future enhancements in cross modal alignment, richer emotional nuance, and support for additional opera genres.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment</title>
<link>https://arxiv.org/abs/2504.15585</link>
<guid>https://arxiv.org/abs/2504.15585</guid>
<content:encoded><![CDATA[
arXiv:2504.15585v1 Announce Type: new 
Abstract: The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications. As LLMs continue to gain prominence in both research and commercial domains, their security and safety implications have become a growing concern, not only for researchers and corporations but also for every nation. Currently, existing surveys on LLM safety primarily focus on specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning phase, lacking a comprehensive understanding of the entire "lifechain" of LLMs. To address this gap, this paper introduces, for the first time, the concept of "full-stack" safety to systematically consider safety issues throughout the entire process of LLM training, deployment, and eventual commercialization. Compared to the off-the-shelf LLM safety surveys, our work demonstrates several distinctive advantages: (I) Comprehensive Perspective. We define the complete LLM lifecycle as encompassing data preparation, pre-training, post-training, deployment and final commercialization. To our knowledge, this represents the first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive Literature Support. Our research is grounded in an exhaustive review of over 800+ papers, ensuring comprehensive coverage and systematic organization of security issues within a more holistic understanding. (III) Unique Insights. Through systematic literature analysis, we have developed reliable roadmaps and perspectives for each chapter. Our work identifies promising research directions, including safety in data generation, alignment techniques, model editing, and LLM-based agent systems. These insights provide valuable guidance for researchers pursuing future work in this field.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grasping Deformable Objects via Reinforcement Learning with Cross-Modal Attention to Visuo-Tactile Inputs</title>
<link>https://arxiv.org/abs/2504.15595</link>
<guid>https://arxiv.org/abs/2504.15595</guid>
<content:encoded><![CDATA[
arXiv:2504.15595v1 Announce Type: new 
Abstract: We consider the problem of grasping deformable objects with soft shells using a robotic gripper. Such objects have a center-of-mass that changes dynamically and are fragile so prone to burst. Thus, it is difficult for robots to generate appropriate control inputs not to drop or break the object while performing manipulation tasks. Multi-modal sensing data could help understand the grasping state through global information (e.g., shapes, pose) from visual data and local information around the contact (e.g., pressure) from tactile data. Although they have complementary information that can be beneficial to use together, fusing them is difficult owing to their different properties.
  We propose a method based on deep reinforcement learning (DRL) that generates control inputs of a simple gripper from visuo-tactile sensing information. Our method employs a cross-modal attention module in the encoder network and trains it in a self-supervised manner using the loss function of the RL agent. With the multi-modal fusion, the proposed method can learn the representation for the DRL agent from the visuo-tactile sensory data. The experimental result shows that cross-modal attention is effective to outperform other early and late data fusion methods across different environments including unseen robot motions and objects.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SocialMOIF: Multi-Order Intention Fusion for Pedestrian Trajectory Prediction</title>
<link>https://arxiv.org/abs/2504.15616</link>
<guid>https://arxiv.org/abs/2504.15616</guid>
<content:encoded><![CDATA[
arXiv:2504.15616v1 Announce Type: new 
Abstract: The analysis and prediction of agent trajectories are crucial for decision-making processes in intelligent systems, with precise short-term trajectory forecasting being highly significant across a range of applications. Agents and their social interactions have been quantified and modeled by researchers from various perspectives; however, substantial limitations exist in the current work due to the inherent high uncertainty of agent intentions and the complex higher-order influences among neighboring groups. SocialMOIF is proposed to tackle these challenges, concentrating on the higher-order intention interactions among neighboring groups while reinforcing the primary role of first-order intention interactions between neighbors and the target agent. This method develops a multi-order intention fusion model to achieve a more comprehensive understanding of both direct and indirect intention information. Within SocialMOIF, a trajectory distribution approximator is designed to guide the trajectories toward values that align more closely with the actual data, thereby enhancing model interpretability. Furthermore, a global trajectory optimizer is introduced to enable more accurate and efficient parallel predictions. By incorporating a novel loss function that accounts for distance and direction during training, experimental results demonstrate that the model outperforms previous state-of-the-art baselines across multiple metrics in both dynamic and static datasets.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Perception for Goal-oriented Navigation: A Survey</title>
<link>https://arxiv.org/abs/2504.15643</link>
<guid>https://arxiv.org/abs/2504.15643</guid>
<content:encoded><![CDATA[
arXiv:2504.15643v1 Announce Type: new 
Abstract: Goal-oriented navigation presents a fundamental challenge for autonomous systems, requiring agents to navigate complex environments to reach designated targets. This survey offers a comprehensive analysis of multimodal navigation approaches through the unifying perspective of inference domains, exploring how agents perceive, reason about, and navigate environments using visual, linguistic, and acoustic information. Our key contributions include organizing navigation methods based on their primary environmental reasoning mechanisms across inference domains; systematically analyzing how shared computational foundations support seemingly disparate approaches across different navigation tasks; identifying recurring patterns and distinctive strengths across various navigation paradigms; and examining the integration challenges and opportunities of multimodal perception to enhance navigation capabilities. In addition, we review approximately 200 relevant articles to provide an in-depth understanding of the current landscape.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trustworthy Decentralized Autonomous Machines: A New Paradigm in Automation Economy</title>
<link>https://arxiv.org/abs/2504.15676</link>
<guid>https://arxiv.org/abs/2504.15676</guid>
<content:encoded><![CDATA[
arXiv:2504.15676v1 Announce Type: new 
Abstract: Decentralized Autonomous Machines (DAMs) represent a transformative paradigm in automation economy, integrating artificial intelligence (AI), blockchain technology, and Internet of Things (IoT) devices to create self-governing economic agents participating in Decentralized Physical Infrastructure Networks (DePIN). Capable of managing both digital and physical assets and unlike traditional Decentralized Autonomous Organizations (DAOs), DAMs extend autonomy into the physical world, enabling trustless systems for Real and Digital World Assets (RDWAs). In this paper, we explore the technological foundations, and challenges of DAMs and argue that DAMs are pivotal in transitioning from trust-based to trustless economic models, offering scalable, transparent, and equitable solutions for asset management. The integration of AI-driven decision-making, IoT-enabled operational autonomy, and blockchain-based governance allows DAMs to decentralize ownership, optimize resource allocation, and democratize access to economic opportunities. Therefore, in this research, we highlight the potential of DAMs to address inefficiencies in centralized systems, reduce wealth disparities, and foster a post-labor economy.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation</title>
<link>https://arxiv.org/abs/2504.15699</link>
<guid>https://arxiv.org/abs/2504.15699</guid>
<content:encoded><![CDATA[
arXiv:2504.15699v1 Announce Type: new 
Abstract: Embodied agents exhibit immense potential across a multitude of domains, making the assurance of their behavioral safety a fundamental prerequisite for their widespread deployment. However, existing research predominantly concentrates on the security of general large language models, lacking specialized methodologies for establishing safety benchmarks and input moderation tailored to embodied agents. To bridge this gap, this paper introduces a novel input moderation framework, meticulously designed to safeguard embodied agents. This framework encompasses the entire pipeline, including taxonomy definition, dataset curation, moderator architecture, model training, and rigorous evaluation. Notably, we introduce EAsafetyBench, a meticulously crafted safety benchmark engineered to facilitate both the training and stringent assessment of moderators specifically designed for embodied agents. Furthermore, we propose Pinpoint, an innovative prompt-decoupled input moderation scheme that harnesses a masked attention mechanism to effectively isolate and mitigate the influence of functional prompts on moderation tasks. Extensive experiments conducted on diverse benchmark datasets and models validate the feasibility and efficacy of the proposed approach. The results demonstrate that our methodologies achieve an impressive average detection accuracy of 94.58%, surpassing the performance of existing state-of-the-art techniques, alongside an exceptional moderation processing time of merely 0.002 seconds per instance.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Control of Redundant Hydraulic Manipulator Using Reinforcement Learning with Action Feedback</title>
<link>https://arxiv.org/abs/2504.15714</link>
<guid>https://arxiv.org/abs/2504.15714</guid>
<content:encoded><![CDATA[
arXiv:2504.15714v1 Announce Type: new 
Abstract: This article presents an entirely data-driven approach for autonomous control of redundant manipulators with hydraulic actuation. The approach only requires minimal system information, which is inherited from a simulation model. The non-linear hydraulic actuation dynamics are modeled using actuator networks from the data gathered during the manual operation of the manipulator to effectively emulate the real system in a simulation environment. A neural network control policy for autonomous control, based on end-effector (EE) position tracking is then learned using Reinforcement Learning (RL) with Ornstein-Uhlenbeck process noise (OUNoise) for efficient exploration. The RL agent also receives feedback based on supervised learning of the forward kinematics which facilitates selecting the best suitable action from exploration. The control policy directly provides the joint variables as outputs based on provided target EE position while taking into account the system dynamics. The joint variables are then mapped to the hydraulic valve commands, which are then fed to the system without further modifications. The proposed approach is implemented on a scaled hydraulic forwarder crane with three revolute and one prismatic joint to track the desired position of the EE in 3-Dimensional (3D) space. With the emulated dynamics and extensive learning in simulation, the results demonstrate the feasibility of deploying the learned controller directly on the real system.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2504.15716</link>
<guid>https://arxiv.org/abs/2504.15716</guid>
<content:encoded><![CDATA[
arXiv:2504.15716v1 Announce Type: new 
Abstract: Effective reasoning remains a core challenge for large language models (LLMs) in the financial domain, where tasks often require domain-specific knowledge, precise numerical calculations, and strict adherence to compliance rules. We propose DianJin-R1, a reasoning-enhanced framework designed to address these challenges through reasoning-augmented supervision and reinforcement learning. Central to our approach is DianJin-R1-Data, a high-quality dataset constructed from CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance Check, CCC), combining diverse financial reasoning scenarios with verified annotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that generates both reasoning steps and final answers. To further refine reasoning quality, we apply Group Relative Policy Optimization (GRPO), a reinforcement learning method that incorporates dual reward signals: one encouraging structured outputs and another rewarding answer correctness. We evaluate our models on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and two general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental results show that DianJin-R1 models consistently outperform their non-reasoning counterparts, especially on complex financial tasks. Moreover, on the real-world CCC dataset, our single-call reasoning models match or even surpass the performance of multi-agent systems that require significantly more computational cost. These findings demonstrate the effectiveness of DianJin-R1 in enhancing financial reasoning through structured supervision and reward-aligned learning, offering a scalable and practical solution for real-world applications.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implementing Rational Choice Functions with LLMs and Measuring their Alignment with User Preferences</title>
<link>https://arxiv.org/abs/2504.15719</link>
<guid>https://arxiv.org/abs/2504.15719</guid>
<content:encoded><![CDATA[
arXiv:2504.15719v1 Announce Type: new 
Abstract: As large language models (LLMs) become integral to intelligent user interfaces (IUIs), their role as decision-making agents raises critical concerns about alignment. Although extensive research has addressed issues such as factuality, bias, and toxicity, comparatively little attention has been paid to measuring alignment to preferences, i.e., the relative desirability of different alternatives, a concept used in decision making, economics, and social choice theory. However, a reliable decision-making agent makes choices that align well with user preferences.
  In this paper, we generalize existing methods that exploit LLMs for ranking alternative outcomes by addressing alignment with the broader and more flexible concept of user preferences, which includes both strict preferences and indifference among alternatives. To this end, we put forward design principles for using LLMs to implement rational choice functions, and provide the necessary tools to measure preference satisfaction. We demonstrate the applicability of our approach through an empirical study in a practical application of an IUI in the automotive domain.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Intent Queries for Motion Transformer-based Trajectory Prediction</title>
<link>https://arxiv.org/abs/2504.15766</link>
<guid>https://arxiv.org/abs/2504.15766</guid>
<content:encoded><![CDATA[
arXiv:2504.15766v1 Announce Type: new 
Abstract: In autonomous driving, accurately predicting the movements of other traffic participants is crucial, as it significantly influences a vehicle's planning processes. Modern trajectory prediction models strive to interpret complex patterns and dependencies from agent and map data. The Motion Transformer (MTR) architecture and subsequent work define the most accurate methods in common benchmarks such as the Waymo Open Motion Benchmark. The MTR model employs pre-generated static intention points as initial goal points for trajectory prediction. However, the static nature of these points frequently leads to misalignment with map data in specific traffic scenarios, resulting in unfeasible or unrealistic goal points. Our research addresses this limitation by integrating scene-specific dynamic intention points into the MTR model. This adaptation of the MTR model was trained and evaluated on the Waymo Open Motion Dataset. Our findings demonstrate that incorporating dynamic intention points has a significant positive impact on trajectory prediction accuracy, especially for predictions over long time horizons. Furthermore, we analyze the impact on ground truth trajectories which are not compliant with the map data or are illegal maneuvers.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed model predictive control without terminal cost under inexact distributed optimization</title>
<link>https://arxiv.org/abs/2504.15768</link>
<guid>https://arxiv.org/abs/2504.15768</guid>
<content:encoded><![CDATA[
arXiv:2504.15768v1 Announce Type: new 
Abstract: This paper presents a novel distributed model predictive control (MPC) formulation without terminal cost and a corresponding distributed synthesis approach for distributed linear discrete-time systems with coupled constraints. The proposed control scheme introduces an explicit stability condition as an additional constraint based on relaxed dynamic programming. As a result, contrary to other related approaches, system stability with the developed controller does not rely on designing a terminal cost. A distributed synthesis approach is then introduced to handle the stability constraint locally within each local agent. To solve the underlying optimization problem for distributed MPC, a violation-free distributed optimization approach is developed, using constraint tightening to ensure feasibility throughout iterations. A numerical example demonstrates that the proposed distributed MPC approach ensures closed-loop stability for each feasible control sequence, with each agent computing its control input in parallel.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents</title>
<link>https://arxiv.org/abs/2504.15785</link>
<guid>https://arxiv.org/abs/2504.15785</guid>
<content:encoded><![CDATA[
arXiv:2504.15785v1 Announce Type: new 
Abstract: Can we build accurate world models out of large language models (LLMs)? How can world models benefit LLM agents? The gap between the prior knowledge of LLMs and the specified environment's dynamics usually bottlenecks LLMs' performance as world models. To bridge the gap, we propose a training-free "world alignment" that learns an environment's symbolic knowledge complementary to LLMs. The symbolic knowledge covers action rules, knowledge graphs, and scene graphs, which are extracted by LLMs from exploration trajectories and encoded into executable codes to regulate LLM agents' policies. We further propose an RL-free, model-based agent "WALL-E 2.0" through the model-predictive control (MPC) framework. Unlike classical MPC requiring costly optimization on the fly, we adopt an LLM agent as an efficient look-ahead optimizer of future steps' actions by interacting with the neurosymbolic world model. While the LLM agent's strong heuristics make it an efficient planner in MPC, the quality of its planned actions is also secured by the accurate predictions of the aligned world model. They together considerably improve learning efficiency in a new environment. On open-world challenges in Mars (Minecraft like) and ALFWorld (embodied indoor environments), WALL-E 2.0 significantly outperforms existing methods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and by at least 61.7% in score. In ALFWorld, it achieves a new record 98% success rate after only 4 iterations.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A closer look at how large language models trust humans: patterns and biases</title>
<link>https://arxiv.org/abs/2504.15801</link>
<guid>https://arxiv.org/abs/2504.15801</guid>
<content:encoded><![CDATA[
arXiv:2504.15801v1 Announce Type: new 
Abstract: As large language models (LLMs) and LLM-based agents increasingly interact with humans in decision-making contexts, understanding the trust dynamics between humans and AI agents becomes a central concern. While considerable literature studies how humans trust AI agents, it is much less understood how LLM-based agents develop effective trust in humans. LLM-based agents likely rely on some sort of implicit effective trust in trust-related contexts (e.g., evaluating individual loan applications) to assist and affect decision making. Using established behavioral theories, we develop an approach that studies whether LLMs trust depends on the three major trustworthiness dimensions: competence, benevolence and integrity of the human subject. We also study how demographic variables affect effective trust. Across 43,200 simulated experiments, for five popular language models, across five different scenarios we find that LLM trust development shows an overall similarity to human trust development. We find that in most, but not all cases, LLM trust is strongly predicted by trustworthiness, and in some cases also biased by age, religion and gender, especially in financial scenarios. This is particularly true for scenarios common in the literature and for newer models. While the overall patterns align with human-like mechanisms of effective trust formation, different models exhibit variation in how they estimate trust; in some cases, trustworthiness and demographic factors are weak predictors of effective trust. These findings call for a better understanding of AI-to-human trust dynamics and monitoring of biases and trust development patterns to prevent unintended and potentially harmful outcomes in trust-sensitive applications of AI.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Test Generation from Task Description for Mobile Testing with Multi-modal Reasoning</title>
<link>https://arxiv.org/abs/2504.15917</link>
<guid>https://arxiv.org/abs/2504.15917</guid>
<content:encoded><![CDATA[
arXiv:2504.15917v1 Announce Type: new 
Abstract: In Android GUI testing, generating an action sequence for a task that can be replayed as a test script is common. Generating sequences of actions and respective test scripts from task goals described in natural language can eliminate the need for manually writing test scripts. However, existing approaches based on large language models (LLM) often struggle with identifying the final action, and either end prematurely or continue past the final screen. In this paper, we introduce VisiDroid, a multi-modal, LLM-based, multi-agent framework that iteratively determines the next action and leverages visual images of screens to detect the task's completeness. The multi-modal approach enhances our model in two significant ways. First, this approach enables it to avoid prematurely terminating a task when textual content alone provides misleading indications of task completion. Additionally, visual input helps the tool avoid errors when changes in the GUI do not directly affect functionality toward task completion, such as adjustments to font sizes or colors. Second, the multi-modal approach also ensures the tool not progress beyond the final screen, which might lack explicit textual indicators of task completion but could display a visual element indicating task completion, which is common in GUI apps. Our evaluation shows that VisiDroid achieves an accuracy of 87.3%, outperforming the best baseline relatively by 23.5%. We also demonstrate that our multi-modal framework with images and texts enables the LLM to better determine when a task is completed.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuroadaptive Haptics: Comparing Reinforcement Learning from Explicit Ratings and Neural Signals for Adaptive XR Systems</title>
<link>https://arxiv.org/abs/2504.15984</link>
<guid>https://arxiv.org/abs/2504.15984</guid>
<content:encoded><![CDATA[
arXiv:2504.15984v1 Announce Type: new 
Abstract: Neuroadaptive haptics offers a path to more immersive extended reality (XR) experiences by dynamically tuning multisensory feedback to user preferences. We present a neuroadaptive haptics system that adapts XR feedback through reinforcement learning (RL) from explicit user ratings and brain-decoded neural signals. In a user study, participants interacted with virtual objects in VR while Electroencephalography (EEG) data were recorded. An RL agent adjusted haptic feedback based either on explicit ratings or on outputs from a neural decoder. Results show that the RL agent's performance was comparable across feedback sources, suggesting that implicit neural feedback can effectively guide personalization without requiring active user input. The EEG-based neural decoder achieved a mean F1 score of 0.8, supporting reliable classification of user experience. These findings demonstrate the feasibility of combining brain-computer interfaces (BCI) and RL to autonomously adapt XR interactions, reducing cognitive load and enhancing immersion.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive Analysis</title>
<link>https://arxiv.org/abs/2504.16047</link>
<guid>https://arxiv.org/abs/2504.16047</guid>
<content:encoded><![CDATA[
arXiv:2504.16047v1 Announce Type: new 
Abstract: Foundation models, trained on vast amounts of data using self-supervised techniques, have emerged as a promising frontier for advancing artificial intelligence (AI) applications in medicine. This study evaluates three different vision-language foundation models (RAD-DINO, CheXagent, and BiomedCLIP) on their ability to capture fine-grained imaging features for radiology tasks. The models were assessed across classification, segmentation, and regression tasks for pneumothorax and cardiomegaly on chest radiographs. Self-supervised RAD-DINO consistently excelled in segmentation tasks, while text-supervised CheXagent demonstrated superior classification performance. BiomedCLIP showed inconsistent performance across tasks. A custom segmentation model that integrates global and local features substantially improved performance for all foundation models, particularly for challenging pneumothorax segmentation. The findings highlight that pre-training methodology significantly influences model performance on specific downstream tasks. For fine-grained segmentation tasks, models trained without text supervision performed better, while text-supervised models offered advantages in classification and interpretability. These insights provide guidance for selecting foundation models based on specific clinical applications in radiology.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ForesightNav: Learning Scene Imagination for Efficient Exploration</title>
<link>https://arxiv.org/abs/2504.16062</link>
<guid>https://arxiv.org/abs/2504.16062</guid>
<content:encoded><![CDATA[
arXiv:2504.16062v1 Announce Type: new 
Abstract: Understanding how humans leverage prior knowledge to navigate unseen environments while making exploratory decisions is essential for developing autonomous robots with similar abilities. In this work, we propose ForesightNav, a novel exploration strategy inspired by human imagination and reasoning. Our approach equips robotic agents with the capability to predict contextual information, such as occupancy and semantic details, for unexplored regions. These predictions enable the robot to efficiently select meaningful long-term navigation goals, significantly enhancing exploration in unseen environments. We validate our imagination-based approach using the Structured3D dataset, demonstrating accurate occupancy prediction and superior performance in anticipating unseen scene geometry. Our experiments show that the imagination module improves exploration efficiency in unseen environments, achieving a 100% completion rate for PointNav and an SPL of 67% for ObjectNav on the Structured3D Validation split. These contributions demonstrate the power of imagination-driven reasoning for autonomous systems to enhance generalizable and efficient exploration.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation</title>
<link>https://arxiv.org/abs/2504.16073</link>
<guid>https://arxiv.org/abs/2504.16073</guid>
<content:encoded><![CDATA[
arXiv:2504.16073v1 Announce Type: new 
Abstract: Recent advancements in visual language models (VLMs) have notably enhanced their capabilities in handling complex Graphical User Interface (GUI) interaction tasks. Despite these improvements, current frameworks often struggle to generate correct actions in challenging GUI environments. State-of-the-art commercial VLMs are black-boxes, and fine-tuning open-source VLMs for GUI tasks requires significant resources. Additionally, existing trajectory-level evaluation and refinement techniques frequently fall short due to delayed feedback and local optimization issues. To address these challenges, we propose an approach that guides VLM agents with process supervision by a reward model during GUI navigation and control at inference time. This guidance allows the VLM agent to optimize actions at each inference step, thereby improving performance in both static and dynamic environments. In particular, our method demonstrates significant performance gains in three GUI navigation tasks, achieving a 3.4% improvement in single step action accuracy for static environments, along with a around 33% increase in task success rate in one dynamic environment. With further integration of trajectory reflection and retry mechanisms, we also demonstrate even greater enhancement in task success.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities</title>
<link>https://arxiv.org/abs/2504.16078</link>
<guid>https://arxiv.org/abs/2504.16078</guid>
<content:encoded><![CDATA[
arXiv:2504.16078v1 Announce Type: new 
Abstract: The success of Large Language Models (LLMs) has sparked interest in various agentic applications. A key hypothesis is that LLMs, leveraging common sense and Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently solve complex domains. However, LLM agents have been found to suffer from sub-optimal exploration and the knowing-doing gap, the inability to effectively act on knowledge present in the model. In this work, we systematically study why LLMs perform sub-optimally in decision-making scenarios. In particular, we closely examine three prevalent failure modes: greediness, frequency bias, and the knowing-doing gap. We propose mitigation of these shortcomings by fine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales. Our experiments across multi-armed bandits, contextual bandits, and Tic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making abilities of LLMs by increasing exploration and narrowing the knowing-doing gap. Finally, we study both classic exploration mechanisms, such as $\epsilon$-greedy, and LLM-specific approaches, such as self-correction and self-consistency, to enable more effective fine-tuning of LLMs for decision-making.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MR. Video: "MapReduce" is the Principle for Long Video Understanding</title>
<link>https://arxiv.org/abs/2504.16082</link>
<guid>https://arxiv.org/abs/2504.16082</guid>
<content:encoded><![CDATA[
arXiv:2504.16082v1 Announce Type: new 
Abstract: We propose MR. Video, an agentic long video understanding framework that demonstrates the simple yet effective MapReduce principle for processing long videos: (1) Map: independently and densely perceiving short video clips, and (2) Reduce: jointly aggregating information from all clips. Compared with sequence-to-sequence vision-language models (VLMs), MR. Video performs detailed short video perception without being limited by context length. Compared with existing video agents that typically rely on sequential key segment selection, the Map operation enables simpler and more scalable sequence parallel perception of short video segments. Its Reduce step allows for more comprehensive context aggregation and reasoning, surpassing explicit key segment retrieval. This MapReduce principle is applicable to both VLMs and video agents, and we use LLM agents to validate its effectiveness.
  In practice, MR. Video employs two MapReduce stages: (A) Captioning: generating captions for short video clips (map), then standardizing repeated characters and objects into shared names (reduce); (B) Analysis: for each user question, analyzing relevant information from individual short videos (map), and integrating them into a final answer (reduce). MR. Video achieves over 10% accuracy improvement on the challenging LVBench compared to state-of-the-art VLMs and video agents.
  Code is available at: https://github.com/ziqipang/MR-Video
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLM-based Prompts as the Optimal Assistant for Unpaired Histopathology Virtual Staining</title>
<link>https://arxiv.org/abs/2504.15545</link>
<guid>https://arxiv.org/abs/2504.15545</guid>
<content:encoded><![CDATA[
arXiv:2504.15545v1 Announce Type: cross 
Abstract: In histopathology, tissue sections are typically stained using common H&amp;E staining or special stains (MAS, PAS, PASM, etc.) to clearly visualize specific tissue structures. The rapid advancement of deep learning offers an effective solution for generating virtually stained images, significantly reducing the time and labor costs associated with traditional histochemical staining. However, a new challenge arises in separating the fundamental visual characteristics of tissue sections from the visual differences induced by staining agents. Additionally, virtual staining often overlooks essential pathological knowledge and the physical properties of staining, resulting in only style-level transfer. To address these issues, we introduce, for the first time in virtual staining tasks, a pathological vision-language large model (VLM) as an auxiliary tool. We integrate contrastive learnable prompts, foundational concept anchors for tissue sections, and staining-specific concept anchors to leverage the extensive knowledge of the pathological VLM. This approach is designed to describe, frame, and enhance the direction of virtual staining. Furthermore, we have developed a data augmentation method based on the constraints of the VLM. This method utilizes the VLM's powerful image interpretation capabilities to further integrate image style and structural information, proving beneficial in high-precision pathological diagnostics. Extensive evaluations on publicly available multi-domain unpaired staining datasets demonstrate that our method can generate highly realistic images and enhance the accuracy of downstream tasks, such as glomerular detection and segmentation. Our code is available at: https://github.com/CZZZZZZZZZZZZZZZZZ/VPGAN-HARBOR
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy-Based Radiative Transfer: Solving the $2$-Level Atom Non-LTE Problem using Soft Actor-Critic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.15679</link>
<guid>https://arxiv.org/abs/2504.15679</guid>
<content:encoded><![CDATA[
arXiv:2504.15679v1 Announce Type: cross 
Abstract: We present a novel reinforcement learning (RL) approach for solving the classical 2-level atom non-LTE radiative transfer problem by framing it as a control task in which an RL agent learns a depth-dependent source function $S(\tau)$ that self-consistently satisfies the equation of statistical equilibrium (SE). The agent's policy is optimized entirely via reward-based interactions with a radiative transfer engine, without explicit knowledge of the ground truth. This method bypasses the need for constructing approximate lambda operators ($\Lambda^*$) common in accelerated iterative schemes. Additionally, it requires no extensive precomputed labeled datasets to extract a supervisory signal, and avoids backpropagating gradients through the complex RT solver itself. Finally, we show through experiment that a simple feedforward neural network trained greedily cannot solve for SE, possibly due to the moving target nature of the problem. Our $\Lambda^*-\text{Free}$ method offers potential advantages for complex scenarios (e.g., atmospheres with enhanced velocity fields, multi-dimensional geometries, or complex microphysics) where $\Lambda^*$ construction or solver differentiability is challenging. Additionally, the agent can be incentivized to find more efficient policies by manipulating the discount factor, leading to a reprioritization of immediate rewards. If demonstrated to generalize past its training data, this RL framework could serve as an alternative or accelerated formalism to achieve SE. To the best of our knowledge, this study represents the first application of reinforcement learning in solar physics that directly solves for a fundamental physical constraint.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mission-driven Exploration for Accelerated Deep Reinforcement Learning with Temporal Logic Task Specifications</title>
<link>https://arxiv.org/abs/2311.17059</link>
<guid>https://arxiv.org/abs/2311.17059</guid>
<content:encoded><![CDATA[
arXiv:2311.17059v2 Announce Type: replace 
Abstract: This paper addresses the problem of designing control policies for agents with unknown stochastic dynamics and control objectives specified using Linear Temporal Logic (LTL). Recent Deep Reinforcement Learning (DRL) algorithms have aimed to compute policies that maximize the satisfaction probability of LTL formulas, but they often suffer from slow learning performance. To address this, we introduce a novel Deep Q-learning algorithm that significantly improves learning speed. The enhanced sample efficiency stems from a mission-driven exploration strategy that prioritizes exploration towards directions likely to contribute to mission success. Identifying these directions relies on an automaton representation of the LTL task as well as a learned neural network that partially models the agent-environment interaction. We provide comparative experiments demonstrating the efficiency of our algorithm on robot navigation tasks in unseen environments.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Certifying Knowledge Comprehension in LLMs</title>
<link>https://arxiv.org/abs/2402.15929</link>
<guid>https://arxiv.org/abs/2402.15929</guid>
<content:encoded><![CDATA[
arXiv:2402.15929v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly deployed in safety-critical systems where they provide answers based on in-context information derived from knowledge bases. As LLMs are increasingly envisioned as superhuman agents, their proficiency in knowledge comprehension-extracting relevant information and reasoning over it to answer questions, a key facet of human intelligence-becomes crucial. However, existing evaluations of LLMs on knowledge comprehension are typically conducted on small test sets, but these datasets represent only a tiny fraction of the vast number of possible queries. Simple empirical evaluations on these limited test sets raises concerns about the reliability and generalizability of the results. In this work, we introduce the first specification and certification framework for knowledge comprehension in LLMs, providing formal probabilistic guarantees for reliability. Instead of a fixed dataset, we design novel specifications that mathematically represent prohibitively large probability distributions of knowledge comprehension prompts with natural noise, using knowledge graphs. From these specifications, we generate quantitative certificates that offer high-confidence, tight bounds on the probability that a given LLM correctly answers any question drawn from the specification distribution. We apply our framework to certify SOTA LLMs in two domains: precision medicine and general question-answering. Our results reveal previously unrecognized vulnerabilities in SOTA LLMs due to natural noise in the prompts. Additionally, we establish performance hierarchies with formal guarantees among the SOTA LLMs, particularly in the context of precision medicine question-answering.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Multi-Task Offloading for Semantic-Aware Edge Computing Systems</title>
<link>https://arxiv.org/abs/2407.11018</link>
<guid>https://arxiv.org/abs/2407.11018</guid>
<content:encoded><![CDATA[
arXiv:2407.11018v2 Announce Type: replace 
Abstract: Mobile edge computing (MEC) provides low-latency offloading solutions for computationally intensive tasks, effectively improving the computing efficiency and battery life of mobile devices. However, for data-intensive tasks or scenarios with limited uplink bandwidth, network congestion might occur due to massive simultaneous offloading nodes, increasing transmission latency and affecting task performance. In this paper, we propose a semantic-aware multi-modal task offloading framework to address the challenges posed by limited uplink bandwidth. By introducing a semantic extraction factor, we balance the relationship among transmission latency, computation energy consumption, and task performance. To measure the offloading performance of multi-modal tasks, we design a unified and fair quality of experience (QoE) metric that includes execution latency, energy consumption, and task performance. Lastly, we formulate the optimization problem as a Markov decision process (MDP) and exploit the multi-agent proximal policy optimization (MAPPO) reinforcement learning algorithm to jointly optimize the semantic extraction factor, communication resources, and computing resources to maximize overall QoE. Experimental results show that the proposed method achieves a reduction in execution latency and energy consumption of 18.1% and 12.9%, respectively compared with the semantic-unaware approach. Moreover, the proposed approach can be easily extended to models with different user preferences.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Adversarial Inverse Reinforcement Learning via Successor Feature Matching</title>
<link>https://arxiv.org/abs/2411.07007</link>
<guid>https://arxiv.org/abs/2411.07007</guid>
<content:encoded><![CDATA[
arXiv:2411.07007v2 Announce Type: replace 
Abstract: In inverse reinforcement learning (IRL), an agent seeks to replicate expert demonstrations through interactions with the environment. Traditionally, IRL is treated as an adversarial game, where an adversary searches over reward models, and a learner optimizes the reward through repeated RL procedures. This game-solving approach is both computationally expensive and difficult to stabilize. In this work, we propose a novel approach to IRL by direct policy optimization: exploiting a linear factorization of the return as the inner product of successor features and a reward vector, we design an IRL algorithm by policy gradient descent on the gap between the learner and expert features. Our non-adversarial method does not require learning a reward function and can be solved seamlessly with existing actor-critic RL algorithms. Remarkably, our approach works in state-only settings without expert action labels, a setting which behavior cloning (BC) cannot solve. Empirical results demonstrate that our method learns from as few as a single expert demonstration and achieves improved performance on various control tasks.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CityWalker: Learning Embodied Urban Navigation from Web-Scale Videos</title>
<link>https://arxiv.org/abs/2411.17820</link>
<guid>https://arxiv.org/abs/2411.17820</guid>
<content:encoded><![CDATA[
arXiv:2411.17820v3 Announce Type: replace 
Abstract: Navigating dynamic urban environments presents significant challenges for embodied agents, requiring advanced spatial reasoning and adherence to common-sense norms. Despite progress, existing visual navigation methods struggle in map-free or off-street settings, limiting the deployment of autonomous agents like last-mile delivery robots. To overcome these obstacles, we propose a scalable, data-driven approach for human-like urban navigation by training agents on thousands of hours of in-the-wild city walking and driving videos sourced from the web. We introduce a simple and scalable data processing pipeline that extracts action supervision from these videos, enabling large-scale imitation learning without costly annotations. Our model learns sophisticated navigation policies to handle diverse challenges and critical scenarios. Experimental results show that training on large-scale, diverse datasets significantly enhances navigation performance, surpassing current methods. This work shows the potential of using abundant online video data to develop robust navigation policies for embodied agents in dynamic urban settings. Project homepage is at https://ai4ce.github.io/CityWalker/.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Codenames as a Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2412.11373</link>
<guid>https://arxiv.org/abs/2412.11373</guid>
<content:encoded><![CDATA[
arXiv:2412.11373v2 Announce Type: replace 
Abstract: In this paper, we propose the use of the popular word-based board game Codenames as a suitable benchmark for evaluating the reasoning capabilities of Large Language Models (LLMs). Codenames presents a highly interesting challenge for achieving successful AI performance, requiring both a sophisticated understanding of language, theory of mind, and epistemic reasoning capabilities. Prior attempts to develop agents for Codenames have largely relied on word embedding techniques, which have a limited vocabulary range and perform poorly when paired with differing approaches. LLMs have demonstrated enhanced reasoning and comprehension capabilities for language-based tasks, but can still suffer in lateral thinking challenges. We evaluate the capabilities of several state-of-the-art LLMs, including GPT-4o, Gemini 1.5, Claude 3.5 Sonnet, and Llama 3.1, across a variety of board setups. Our results indicate that while certain LLMs perform better than others overall, different models exhibit varying emergent behaviours during gameplay and excel at specific roles. We also evaluate the performance of different combinations of LLMs when playing cooperatively together, demonstrating that LLM agents are more generalisable to a wider range of teammates than prior techniques.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Language for Coordination: A Framework and Benchmark for LLM-Driven Multi-Agent Control</title>
<link>https://arxiv.org/abs/2412.11761</link>
<guid>https://arxiv.org/abs/2412.11761</guid>
<content:encoded><![CDATA[
arXiv:2412.11761v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across various tasks. Their potential to facilitate human coordination with many agents is a promising but largely under-explored area. Such capabilities would be helpful in disaster response, urban planning, and real-time strategy scenarios. In this work, we introduce (1) a real-time strategy game benchmark designed to evaluate these abilities and (2) a novel framework we term HIVE. HIVE empowers a single human to coordinate swarms of up to 2,000 agents through a natural language dialog with an LLM. We present promising results on this multi-agent benchmark, with our hybrid approach solving tasks such as coordinating agent movements, exploiting unit weaknesses, leveraging human annotations, and understanding terrain and strategic points. Our findings also highlight critical limitations of current models, including difficulties in processing spatial visual information and challenges in formulating long-term strategic plans. This work sheds light on the potential and limitations of LLMs in human-swarm coordination, paving the way for future research in this area. The HIVE project page, hive.syrkis.com, includes videos of the system in action.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stable Hypergraph Matching in Unimodular Hypergraphs</title>
<link>https://arxiv.org/abs/2502.08827</link>
<guid>https://arxiv.org/abs/2502.08827</guid>
<content:encoded><![CDATA[
arXiv:2502.08827v2 Announce Type: replace 
Abstract: We study the NP-hard Stable Hypergraph Matching (SHM) problem and its generalization allowing capacities, the Stable Hypergraph $b$-Matching (SH$b$M) problem, and investigate their computational properties under various structural constraints. Our study is motivated by the fact that Scarf's Lemma (Scarf, 1967) together with a result of Lov\'asz (1972) guarantees the existence of a stable matching whenever the underlying hypergraph is normal. Furthermore, if the hypergraph is unimodular (i.e., its incidence matrix is totally unimodular), then even a stable $b$-matching is guaranteed to exist. However, no polynomial-time algorithm is known for finding a stable matching or $b$-matching in unimodular hypergraphs.
  We identify subclasses of unimodular hypergraphs where SHM and SH$b$M are tractable such as laminar hypergraphs or so-called subpath hypergraphs with bounded-size hyperedges; for the latter case, even a maximum-weight stable $b$-matching can be found efficiently. We complement our algorithms by showing that optimizing over stable matchings is NP-hard even in laminar hypergraphs. As a practically important special case of SH$b$M for unimodular hypergraphs, we investigate a tripartite stable matching problem with students, schools, and companies as agents, called the University Dual Admission problem, which models real-world scenarios in higher education admissions.
  Finally, we examine a superclass of subpath hypergraphs that are normal but necessarily not unimodular, namely subtree hypergraphs where hyperedges correspond to subtrees of a tree. We establish that for such hypergraphs, stable matchings can be found in polynomial time but, in the setting with capacities, finding a stable $b$-matching is NP-hard.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Robustness of Multimodal Agents Against Active Environmental Injection Attacks</title>
<link>https://arxiv.org/abs/2502.13053</link>
<guid>https://arxiv.org/abs/2502.13053</guid>
<content:encoded><![CDATA[
arXiv:2502.13053v2 Announce Type: replace 
Abstract: As researchers continue to optimize AI agents for more effective task execution within operating systems, they often overlook a critical security concern: the ability of these agents to detect "impostors" within their environment. Through an analysis of the agents' operational context, we identify a significant threat-attackers can disguise malicious attacks as environmental elements, injecting active disturbances into the agents' execution processes to manipulate their decision-making. We define this novel threat as the Active Environment Injection Attack (AEIA). Focusing on the interaction mechanisms of the Android OS, we conduct a risk assessment of AEIA and identify two critical security vulnerabilities: (1) Adversarial content injection in multimodal interaction interfaces, where attackers embed adversarial instructions within environmental elements to mislead agent decision-making; and (2) Reasoning gap vulnerabilities in the agent's task execution process, which increase susceptibility to AEIA attacks during reasoning. To evaluate the impact of these vulnerabilities, we propose AEIA-MN, an attack scheme that exploits interaction vulnerabilities in mobile operating systems to assess the robustness of MLLM-based agents. Experimental results show that even advanced MLLMs are highly vulnerable to this attack, achieving a maximum attack success rate of 93% on the AndroidWorld benchmark by combining two vulnerabilities.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks</title>
<link>https://arxiv.org/abs/2503.09572</link>
<guid>https://arxiv.org/abs/2503.09572</guid>
<content:encoded><![CDATA[
arXiv:2503.09572v3 Announce Type: replace 
Abstract: Large language models (LLMs) have shown remarkable advancements in enabling language agents to tackle simple tasks. However, applying them for complex, multi-step, long-horizon tasks remains a challenge. Recent work have found success by separating high-level planning from low-level execution, which enables the model to effectively balance high-level planning objectives and low-level execution details. However, generating accurate plans remains difficult since LLMs are not inherently trained for this task. To address this, we propose Plan-and-Act, a novel framework that incorporates explicit planning into LLM-based agents and introduces a scalable method to enhance plan generation through a novel synthetic data generation method. Plan-and-Act consists of a Planner model which generates structured, high-level plans to achieve user goals, and an Executor model that translates these plans into environment-specific actions. To train the Planner effectively, we introduce a synthetic data generation method that annotates ground-truth trajectories with feasible plans, augmented with diverse and extensive examples to enhance generalization. We evaluate Plan-and-Act using web navigation as a representative long-horizon planning environment, demonstrating a state-of-the-art 57.58% success rate on the WebArena-Lite benchmark as well as a text-only state-of-the-art 81.36% success rate on WebVoyager.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Role-Selection Game in Block Production under Proposer-Builder Separation</title>
<link>https://arxiv.org/abs/2503.15184</link>
<guid>https://arxiv.org/abs/2503.15184</guid>
<content:encoded><![CDATA[
arXiv:2503.15184v2 Announce Type: replace 
Abstract: To address the risks of validator centralization, Proposer-Builder Separation (PBS) was introduced in Ethereum to divide the roles of block building and block proposing, fostering a more equitable and decentralized block production environment. PBS creates a two-sided market in which searchers submit valuable bundles to builders for inclusion in blocks, while builders compete in auctions for block proposals. In this paper, we formulate and analyze a role-selection game that models how profit-seeking participants in PBS strategically choose between acting as searchers or builders, using a co-evolutionary framework to capture the complex interactions and payoff dynamics in this market. Through agent-based simulations, we demonstrate that agents' optimal role-acting as searcher or builder-responds dynamically to the probability of conflict between bundles. Our empirical game-theoretic analysis quantifies the equilibrium frequencies of role selection under different market conditions, revealing that low conflict probabilities lead to equilibria dominated by searchers, while higher probabilities shift equilibrium toward builders. Additionally, bundle conflicts have non-monotonic effects on agent payoffs and strategy evolution. Our results advance the understanding of decentralized block building and provide guidance for designing fairer and more robust block production mechanisms in blockchain systems.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LOKA Protocol: A Decentralized Framework for Trustworthy and Ethical AI Agent Ecosystems</title>
<link>https://arxiv.org/abs/2504.10915</link>
<guid>https://arxiv.org/abs/2504.10915</guid>
<content:encoded><![CDATA[
arXiv:2504.10915v2 Announce Type: replace 
Abstract: The rise of autonomous AI agents, capable of perceiving, reasoning, and acting independently, signals a profound shift in how digital ecosystems operate, govern, and evolve. As these agents proliferate beyond centralized infrastructures, they expose foundational gaps in identity, accountability, and ethical alignment. Three critical questions emerge: Identity: Who or what is the agent? Accountability: Can its actions be verified, audited, and trusted? Ethical Consensus: Can autonomous systems reliably align with human values and prevent harmful emergent behaviors? We present the novel LOKA Protocol (Layered Orchestration for Knowledgeful Agents), a unified, systems-level architecture for building ethically governed, interoperable AI agent ecosystems. LOKA introduces a proposed Universal Agent Identity Layer (UAIL) for decentralized, verifiable identity; intent-centric communication protocols for semantic coordination across diverse agents; and a Decentralized Ethical Consensus Protocol (DECP) that could enable agents to make context-aware decisions grounded in shared ethical baselines. Anchored in emerging standards such as Decentralized Identifiers (DIDs), Verifiable Credentials (VCs), and post-quantum cryptography, LOKA proposes a scalable, future-resilient blueprint for multi-agent AI governance. By embedding identity, trust, and ethics into the protocol layer itself, LOKA proposes the foundation for a new era of responsible, transparent, and autonomous AI ecosystems operating across digital and physical domains.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Graph-Based Reinforcement Learning Approach with Frontier Potential Based Reward for Safe Cluttered Environment Exploration</title>
<link>https://arxiv.org/abs/2504.11907</link>
<guid>https://arxiv.org/abs/2504.11907</guid>
<content:encoded><![CDATA[
arXiv:2504.11907v2 Announce Type: replace 
Abstract: Autonomous exploration of cluttered environments requires efficient exploration strategies that guarantee safety against potential collisions with unknown random obstacles. This paper presents a novel approach combining a graph neural network-based exploration greedy policy with a safety shield to ensure safe navigation goal selection. The network is trained using reinforcement learning and the proximal policy optimization algorithm to maximize exploration efficiency while reducing the safety shield interventions. However, if the policy selects an infeasible action, the safety shield intervenes to choose the best feasible alternative, ensuring system consistency. Moreover, this paper proposes a reward function that includes a potential field based on the agent's proximity to unexplored regions and the expected information gain from reaching them. Overall, the approach investigated in this paper merges the benefits of the adaptability of reinforcement learning-driven exploration policies and the guarantee ensured by explicit safety mechanisms. Extensive evaluations in simulated environments demonstrate that the approach enables efficient and safe exploration in cluttered environments.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Ask One When You Can Ask $k$? Two-Stage Learning-to-Defer to the Top-$k$ Experts</title>
<link>https://arxiv.org/abs/2504.12988</link>
<guid>https://arxiv.org/abs/2504.12988</guid>
<content:encoded><![CDATA[
arXiv:2504.12988v2 Announce Type: replace 
Abstract: Learning-to-Defer (L2D) enables decision-making systems to improve reliability by selectively deferring uncertain predictions to more competent agents. However, most existing approaches focus exclusively on single-agent deferral, which is often inadequate in high-stakes scenarios that require collective expertise. We propose Top-$k$ Learning-to-Defer, a generalization of the classical two-stage L2D framework that allocates each query to the $k$ most confident agents instead of a single one. To further enhance flexibility and cost-efficiency, we introduce Top-$k(x)$ Learning-to-Defer, an adaptive extension that learns the optimal number of agents to consult for each query, based on input complexity, agent competency distributions, and consultation costs. For both settings, we derive a novel surrogate loss and prove that it is Bayes-consistent and $(\mathcal{R}, \mathcal{G})$-consistent, ensuring convergence to the Bayes-optimal allocation. Notably, we show that the well-established model cascades paradigm arises as a restricted instance of our Top-$k$ and Top-$k(x)$ formulations. Extensive experiments across diverse benchmarks demonstrate the effectiveness of our framework on both classification and regression tasks.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence Clones</title>
<link>https://arxiv.org/abs/2501.16996</link>
<guid>https://arxiv.org/abs/2501.16996</guid>
<content:encoded><![CDATA[
arXiv:2501.16996v4 Announce Type: replace-cross 
Abstract: Large language models, trained on personal data, may soon be able to mimic individual personalities. These ``AI clones'' or ``AI agents'' have the potential to transform how people search over one another in contexts ranging from marriage to employment -- indeed, several dating platforms have already begun using AI clones to evaluate potential pairings between users. This paper presents a theoretical framework to study the tradeoff between the substantially expanded search capacity of AI clones, and their imperfect representation of humans. Individual personalities are modeled as points in $k$-dimensional Euclidean space, and their AI clones are modeled as noisy approximations of these personalities. I compare two search regimes: an ``in-person regime'' -- where each person randomly meets some number of individuals and matches to the most compatible among them -- against an ``AI representation regime'' -- in which individuals match to the person whose AI clone is most compatible with their AI clone. I show that a finite number of in-person encounters exceeds the expected payoff from search over infinite AI clones. Moreover, when the dimensionality of personality is large, simply meeting two people in person produces a better expected match than entrusting the process to an AI platform, regardless of the size of its candidate pool.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Flow Integrity to Prevent Privilege Escalation in LLM Agents</title>
<link>https://arxiv.org/abs/2503.15547</link>
<guid>https://arxiv.org/abs/2503.15547</guid>
<content:encoded><![CDATA[
arXiv:2503.15547v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are combined with tools to create powerful LLM agents that provide a wide range of services. Unlike traditional software, LLM agent's behavior is determined at runtime by natural language prompts from either user or tool's data. This flexibility enables a new computing paradigm with unlimited capabilities and programmability, but also introduces new security risks, vulnerable to privilege escalation attacks. Moreover, user prompts are prone to be interpreted in an insecure way by LLM agents, creating non-deterministic behaviors that can be exploited by attackers. To address these security risks, we propose Prompt Flow Integrity (PFI), a system security-oriented solution to prevent privilege escalation in LLM agents. Analyzing the architectural characteristics of LLM agents, PFI features three mitigation techniques -- i.e., agent isolation, secure untrusted data processing, and privilege escalation guardrails. Our evaluation result shows that PFI effectively mitigates privilege escalation attacks while successfully preserving the utility of LLM agents.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Balancing Preference and Performance through Adaptive Personalized Explainability</title>
<link>https://arxiv.org/abs/2504.13856</link>
<guid>https://arxiv.org/abs/2504.13856</guid>
<content:encoded><![CDATA[
arXiv:2504.13856v1 Announce Type: new 
Abstract: As robots and digital assistants are deployed in the real world, these agents must be able to communicate their decision-making criteria to build trust, improve human-robot teaming, and enable collaboration. While the field of explainable artificial intelligence (xAI) has made great strides to enable such communication, these advances often assume that one xAI approach is ideally suited to each problem (e.g., decision trees to explain how to triage patients in an emergency or feature-importance maps to explain radiology reports). This fails to recognize that users have diverse experiences or preferences for interaction modalities. In this work, we present two user-studies set in a simulated autonomous vehicle (AV) domain. We investigate (1) population-level preferences for xAI and (2) personalization strategies for providing robot explanations. We find significant differences between xAI modes (language explanations, feature-importance maps, and decision trees) in both preference (p < 0.01) and performance (p < 0.05). We also observe that a participant's preferences do not always align with their performance, motivating our development of an adaptive personalization strategy to balance the two. We show that this strategy yields significant performance gains (p < 0.05), and we conclude with a discussion of our findings and implications for xAI in human-robot interactions.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark</title>
<link>https://arxiv.org/abs/2504.13861</link>
<guid>https://arxiv.org/abs/2504.13861</guid>
<content:encoded><![CDATA[
arXiv:2504.13861v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) are increasingly being explored for applications in telemedicine, yet their ability to engage with diverse patient behaviors remains underexplored. We introduce 3MDBench (Medical Multimodal Multi-agent Dialogue Benchmark), an open-source evaluation framework designed to assess LLM-driven medical consultations. Unlike existing benchmarks, 3MDBench simulates real-world patient variability by incorporating four temperament-driven Patient Agents and an Assessor Agent that evaluates diagnostic accuracy and dialogue quality. The benchmark integrates textual and image-based patient data across 34 common diagnoses, mirroring real-world telemedicine interactions. Under different diagnostic strategies, we evaluate state-of-the-art LVLMs. Our findings demonstrate that incorporating dialogue improves the F1 score from 50.4 to 54.2 compared to non-dialogue settings, underscoring the value of context-driven, information-seeking questioning. Additionally, we demonstrate that multimodal inputs enhance diagnostic efficiency. Image-supported models outperform text-only counterparts by raising the diagnostic F1 score from 52.8 to 54.2 in a similar dialogue setting. Finally, we suggest an approach that improves the diagnostic F1-score to 70.3 by training the CNN model on the diagnosis prediction task and incorporating its top-3 predictions into the LVLM context. 3MDBench provides a reproducible and extendable evaluation framework for AI-driven medical assistants. It offers insights into how patient temperament, dialogue strategies, and multimodal reasoning influence diagnosis quality. By addressing real-world complexities in telemedicine, our benchmark paves the way for more empathetic, reliable, and context-aware AI-driven healthcare solutions. The source code of our benchmark is publicly available: https://github.com/univanxx/3mdbench
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human aversion? Do AI Agents Judge Identity More Harshly Than Performance</title>
<link>https://arxiv.org/abs/2504.13871</link>
<guid>https://arxiv.org/abs/2504.13871</guid>
<content:encoded><![CDATA[
arXiv:2504.13871v1 Announce Type: new 
Abstract: This study examines the understudied role of algorithmic evaluation of human judgment in hybrid decision-making systems, a critical gap in management research. While extant literature focuses on human reluctance to follow algorithmic advice, we reverse the perspective by investigating how AI agents based on large language models (LLMs) assess and integrate human input. Our work addresses a pressing managerial constraint: firms barred from deploying LLMs directly due to privacy concerns can still leverage them as mediating tools (for instance, anonymized outputs or decision pipelines) to guide high-stakes choices like pricing or discounts without exposing proprietary data. Through a controlled prediction task, we analyze how an LLM-based AI agent weights human versus algorithmic predictions. We find that the AI system systematically discounts human advice, penalizing human errors more severely than algorithmic errors--a bias exacerbated when the agent's identity (human vs AI) is disclosed and the human is positioned second. These results reveal a disconnect between AI-generated trust metrics and the actual influence of human judgment, challenging assumptions about equitable human-AI collaboration. Our findings offer three key contributions. First, we identify a reverse algorithm aversion phenomenon, where AI agents undervalue human input despite comparable error rates. Second, we demonstrate how disclosure and positional bias interact to amplify this effect, with implications for system design. Third, we provide a framework for indirect LLM deployment that balances predictive power with data privacy. For practitioners, this research emphasize the need to audit AI weighting mechanisms, calibrate trust dynamics, and strategically design decision sequences in human-AI systems.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants</title>
<link>https://arxiv.org/abs/2504.13887</link>
<guid>https://arxiv.org/abs/2504.13887</guid>
<content:encoded><![CDATA[
arXiv:2504.13887v1 Announce Type: new 
Abstract: Despite the growing integration of AI chatbots as conversational agents in public discourse, empirical evidence regarding their capacity to foster intercultural empathy remains limited. Using a randomized dialogue experiment, we examined how different types of AI chatbot interaction, i.e., deliberative versus non-deliberative and culturally aligned versus non-aligned, affect intercultural empathy across cultural groups. Results show that deliberative conversations increased intercultural empathy among American participants but not Latin American participants, who perceived AI responses as culturally inaccurate and failing to represent their cultural contexts and perspectives authentically. Real-time interaction analyses reveal that these differences stem from cultural knowledge gaps inherent in Large Language Models. Despite explicit prompting and instruction to represent cultural perspectives in participants' native languages, AI systems still exhibit significant disparities in cultural representation. This highlights the importance of designing AI systems capable of culturally authentic engagement in deliberative conversations. Our study contributes to deliberation theory and AI alignment research by underscoring AI's role in intercultural dialogue and the persistent challenge of representational asymmetry in democratic discourse.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Show Me How: Benefits and Challenges of Agent-Augmented Counterfactual Explanations for Non-Expert Users</title>
<link>https://arxiv.org/abs/2504.13897</link>
<guid>https://arxiv.org/abs/2504.13897</guid>
<content:encoded><![CDATA[
arXiv:2504.13897v1 Announce Type: new 
Abstract: Counterfactual explanations offer actionable insights by illustrating how changes to inputs can lead to different outcomes. However, these explanations often suffer from ambiguity and impracticality, limiting their utility for non-expert users with limited AI knowledge. Augmenting counterfactual explanations with Large Language Models (LLMs) has been proposed as a solution, but little research has examined their benefits and challenges for non-experts. To address this gap, we developed a healthcare-focused system that leverages conversational AI agents to enhance counterfactual explanations, offering clear, actionable recommendations to help patients at high risk of cardiovascular disease (CVD) reduce their risk. Evaluated through a mixed-methods study with 34 participants, our findings highlight the effectiveness of agent-augmented counterfactuals in improving actionable recommendations. Results further indicate that users with prior experience using conversational AI demonstrated greater effectiveness in utilising these explanations compared to novices. Furthermore, this paper introduces a set of generic guidelines for creating augmented counterfactual explanations, incorporating safeguards to mitigate common LLM pitfalls, such as hallucinations, and ensuring the explanations are both actionable and contextually relevant for non-expert users.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Human Robot Social Interaction (HSRI) Dataset: Benchmarking Foundational Models' Social Reasoning</title>
<link>https://arxiv.org/abs/2504.13898</link>
<guid>https://arxiv.org/abs/2504.13898</guid>
<content:encoded><![CDATA[
arXiv:2504.13898v1 Announce Type: new 
Abstract: Our work aims to advance the social reasoning of embodied artificial intelligence (AI) agents in real-world social interactions. Recently, language models (LMs) and foundational models (FMs) are being utilized as automatic evaluators of human-AI interactions with the goal of eventually being used to improve the policy of the AI agent. To enable further research in this direction, we introduce a large-scale real-world Human Robot Social Interaction (HSRI) Dataset to benchmark the capabilities of LMs and FMs to identify and reason about social interactions, specifically with regard to robot social errors and competencies . Our dataset consists of 400 real-world human social robot interaction videos and over 10K annotations, detailing the robot's social errors, competencies, rationale, and corrective actions, capturing unique aspects of human-AI interaction only present in real-world interactions. To further assess AI models' ability to reason about social interactions, we propose eight new benchmark tasks for evaluating centered around whether AI models can (1) evaluate social interactions via detecting social errors and competencies, (2) identify the explanatory factors associated to errors and competencies, (3) understand the flow of real-world social interactions, and (4) provide reasons and corrective actions for social errors. Human studies and experiments with modern LMs and FMs reveal that current models struggle with these tasks, demonstrating that our dataset and benchmark provides a step forward towards socially intelligent AI.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Assisted Conversational Interviewing: Effects on Data Quality and User Experience</title>
<link>https://arxiv.org/abs/2504.13908</link>
<guid>https://arxiv.org/abs/2504.13908</guid>
<content:encoded><![CDATA[
arXiv:2504.13908v1 Announce Type: new 
Abstract: Standardized surveys scale efficiently but sacrifice depth, while conversational interviews improve response quality at the cost of scalability and consistency. This study bridges the gap between these methods by introducing a framework for AI-assisted conversational interviewing. To evaluate this framework, we conducted a web survey experiment where 1,800 participants were randomly assigned to text-based conversational AI agents, or "textbots", to dynamically probe respondents for elaboration and interactively code open-ended responses. We assessed textbot performance in terms of coding accuracy, response quality, and respondent experience. Our findings reveal that textbots perform moderately well in live coding even without survey-specific fine-tuning, despite slightly inflated false positive errors due to respondent acquiescence bias. Open-ended responses were more detailed and informative, but this came at a slight cost to respondent experience. Our findings highlight the feasibility of using AI methods to enhance open-ended data collection in web surveys.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViMo: A Generative Visual GUI World Model for App Agent</title>
<link>https://arxiv.org/abs/2504.13936</link>
<guid>https://arxiv.org/abs/2504.13936</guid>
<content:encoded><![CDATA[
arXiv:2504.13936v1 Announce Type: new 
Abstract: App agents, which autonomously operate mobile Apps through Graphical User Interfaces (GUIs), have gained significant interest in real-world applications. Yet, they often struggle with long-horizon planning, failing to find the optimal actions for complex tasks with longer steps. To address this, world models are used to predict the next GUI observation based on user actions, enabling more effective agent planning. However, existing world models primarily focus on generating only textual descriptions, lacking essential visual details. To fill this gap, we propose ViMo, the first visual world model designed to generate future App observations as images. For the challenge of generating text in image patches, where even minor pixel errors can distort readability, we decompose GUI generation into graphic and text content generation. We propose a novel data representation, the Symbolic Text Representation~(STR) to overlay text content with symbolic placeholders while preserving graphics. With this design, ViMo employs a STR Predictor to predict future GUIs' graphics and a GUI-text Predictor for generating the corresponding text. Moreover, we deploy ViMo to enhance agent-focused tasks by predicting the outcome of different action options. Experiments show ViMo's ability to generate visually plausible and functionally effective GUIs that enable App agents to make more informed decisions.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tinker Tales: Interactive Storytelling Framework for Early Childhood Narrative Development and AI Literacy</title>
<link>https://arxiv.org/abs/2504.13969</link>
<guid>https://arxiv.org/abs/2504.13969</guid>
<content:encoded><![CDATA[
arXiv:2504.13969v1 Announce Type: new 
Abstract: This paper presents Tinker Tales, an interactive storytelling framework in the format of a board game, designed to support both narrative development and AI literacy in early childhood. The framework integrates tangible and speech-based interactions with AI through NFC chip-attached pawns and tokens, along with a speaker and microphone. Children select and define key story elements-such as characters, places, items, and emotions-using the pawns and tokens, providing further details to the AI and receiving proper assistance, similar to how adults prompt AI for specific tasks (e.g., writing). For evaluation, several game sessions were simulated with a child AI agent, and the quality and safety of the generated stories were assessed from various perspectives. This work highlights the potential of combining physical and digital elements in AI literacy, offering a safe and engaging way for children to learn how to effectively collaborate with AI.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Going Whole Hog: A Philosophical Defense of AI Cognition</title>
<link>https://arxiv.org/abs/2504.13988</link>
<guid>https://arxiv.org/abs/2504.13988</guid>
<content:encoded><![CDATA[
arXiv:2504.13988v1 Announce Type: new 
Abstract: This work defends the 'Whole Hog Thesis': sophisticated Large Language Models (LLMs) like ChatGPT are full-blown linguistic and cognitive agents, possessing understanding, beliefs, desires, knowledge, and intentions. We argue against prevailing methodologies in AI philosophy, rejecting starting points based on low-level computational details ('Just an X' fallacy) or pre-existing theories of mind. Instead, we advocate starting with simple, high-level observations of LLM behavior (e.g., answering questions, making suggestions) -- defending this data against charges of metaphor, loose talk, or pretense. From these observations, we employ 'Holistic Network Assumptions' -- plausible connections between mental capacities (e.g., answering implies knowledge, knowledge implies belief, action implies intention) -- to argue for the full suite of cognitive states. We systematically rebut objections based on LLM failures (hallucinations, planning/reasoning errors), arguing these don't preclude agency, often mirroring human fallibility. We address numerous 'Games of Lacks', arguing that LLMs do not lack purported necessary conditions for cognition (e.g., semantic grounding, embodiment, justification, intrinsic intentionality) or that these conditions are not truly necessary, often relying on anti-discriminatory arguments comparing LLMs to diverse human capacities. Our approach is evidential, not functionalist, and deliberately excludes consciousness. We conclude by speculating on the possibility of LLMs possessing 'alien' contents beyond human conceptual schemes.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Evacuation in Graphs with Multiple Exits</title>
<link>https://arxiv.org/abs/2504.14052</link>
<guid>https://arxiv.org/abs/2504.14052</guid>
<content:encoded><![CDATA[
arXiv:2504.14052v1 Announce Type: new 
Abstract: Consider the following discrete evacuation model. The evacuation terrain is modeled by a simple graph $G=(V,E)$ whose certain vertices $X\subseteq V$ are called \emph{exits}. Initially, each vertex is either \emph{empty} or \emph{occupied} by an agent. We assume that each vertex has a unique \emph{id} (and therefore the agents do have unique ids), each agent has finite but arbitrarily large memory, and the graph is initially stored in the memory of each agent. In other words, the agents do know the topology of the network along with the locations of the exits, but they do not know the initial positions nor the quantity of other agents. The time is divided into \emph{steps}; in each step any pair of agents present at vertices at a distance of at most two can exchange an arbitrary number of messages, and then each agent can either make a move or stay put. The agents should make moves in a collision-free manner, i.e., no two agents can be located at the same vertex in the same step. At the end of each step, any agent located at an exit \emph{evacuates}, i.e., it is removed from the graph. The goal is to provide an algorithm to the agents (referred to as an evacuation strategy) that ensures the evacuation of all agents and minimizes the number of steps.
  This work provides an algorithmic framework that allows constructing valid evacuation strategies for arbitrary input graphs. Specifically, we focus on the properties of the input graphs that lead to evacuation strategies with constant competitive ratios. In particular, we describe an application of the above framework that gives an asymptotically optimal evacuation for grids (and by extension hexagonal or triangular grids as well).
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DoomArena: A framework for Testing AI Agents Against Evolving Security Threats</title>
<link>https://arxiv.org/abs/2504.14064</link>
<guid>https://arxiv.org/abs/2504.14064</guid>
<content:encoded><![CDATA[
arXiv:2504.14064v1 Announce Type: new 
Abstract: We present DoomArena, a security evaluation framework for AI agents. DoomArena is designed on three principles: 1) It is a plug-in framework and integrates easily into realistic agentic frameworks like BrowserGym (for web agents) and $\tau$-bench (for tool calling agents); 2) It is configurable and allows for detailed threat modeling, allowing configuration of specific components of the agentic framework being attackable, and specifying targets for the attacker; and 3) It is modular and decouples the development of attacks from details of the environment in which the agent is deployed, allowing for the same attacks to be applied across multiple environments. We illustrate several advantages of our framework, including the ability to adapt to new threat models and environments easily, the ability to easily combine several previously published attacks to enable comprehensive and fine-grained security testing, and the ability to analyze trade-offs between various vulnerabilities and performance. We apply DoomArena to state-of-the-art (SOTA) web and tool-calling agents and find a number of surprising results: 1) SOTA agents have varying levels of vulnerability to different threat models (malicious user vs malicious environment), and there is no Pareto dominant agent across all threat models; 2) When multiple attacks are applied to an agent, they often combine constructively; 3) Guardrail model-based defenses seem to fail, while defenses based on powerful SOTA LLMs work better. DoomArena is available at https://github.com/ServiceNow/DoomArena.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TALES: Text Adventure Learning Environment Suite</title>
<link>https://arxiv.org/abs/2504.14128</link>
<guid>https://arxiv.org/abs/2504.14128</guid>
<content:encoded><![CDATA[
arXiv:2504.14128v1 Announce Type: new 
Abstract: Reasoning is an essential skill to enable Large Language Models (LLMs) to interact with the world. As tasks become more complex, they demand increasingly sophisticated and diverse reasoning capabilities for sequential decision-making, requiring structured reasoning over the context history to determine the next best action. We introduce TALES, a diverse collection of synthetic and human-written text-adventure games designed to challenge and evaluate diverse reasoning capabilities. We present results over a range of LLMs, open- and closed-weights, performing a qualitative analysis on the top performing models. Despite an impressive showing on synthetic games, even the top LLM-driven agents fail to achieve 15% on games designed for human enjoyment. Code and visualization of the experiments can be found at https://microsoft.github.io/tales.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>tAIfa: Enhancing Team Effectiveness and Cohesion with AI-Generated Automated Feedback</title>
<link>https://arxiv.org/abs/2504.14222</link>
<guid>https://arxiv.org/abs/2504.14222</guid>
<content:encoded><![CDATA[
arXiv:2504.14222v1 Announce Type: new 
Abstract: Providing timely and actionable feedback is crucial for effective collaboration, learning, and coordination within teams. However, many teams face challenges in receiving feedback that aligns with their goals and promotes cohesion. We introduce tAIfa (``Team AI Feedback Assistant''), an AI agent that uses Large Language Models (LLMs) to provide personalized, automated feedback to teams and their members. tAIfa analyzes team interactions, identifies strengths and areas for improvement, and delivers targeted feedback based on communication patterns. We conducted a between-subjects study with 18 teams testing whether using tAIfa impacted their teamwork. Our findings show that tAIfa improved communication and contributions within the teams. This paper contributes to the Human-AI Interaction literature by presenting a computational framework that integrates LLMs to provide automated feedback, introducing tAIfa as a tool to enhance team engagement and cohesion, and providing insights into future AI applications to support team collaboration.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Template-Based Financial Report Generation in Agentic and Decomposed Information Retrieval</title>
<link>https://arxiv.org/abs/2504.14233</link>
<guid>https://arxiv.org/abs/2504.14233</guid>
<content:encoded><![CDATA[
arXiv:2504.14233v1 Announce Type: new 
Abstract: Tailoring structured financial reports from companies' earnings releases is crucial for understanding financial performance and has been widely adopted in real-world analytics. However, existing summarization methods often generate broad, high-level summaries, which may lack the precision and detail required for financial reports that typically focus on specific, structured sections. While Large Language Models (LLMs) hold promise, generating reports adhering to predefined multi-section templates remains challenging. This paper investigates two LLM-based approaches popular in industry for generating templated financial reports: an agentic information retrieval (IR) framework and a decomposed IR approach, namely AgenticIR and DecomposedIR. The AgenticIR utilizes collaborative agents prompted with the full template. In contrast, the DecomposedIR approach applies a prompt chaining workflow to break down the template and reframe each section as a query answered by the LLM using the earnings release. To quantitatively assess the generated reports, we evaluated both methods in two scenarios: one using a financial dataset without direct human references, and another with a weather-domain dataset featuring expert-written reports. Experimental results show that while AgenticIR may excel in orchestrating tasks and generating concise reports through agent collaboration, DecomposedIR statistically significantly outperforms AgenticIR approach in providing broader and more detailed coverage in both scenarios, offering reflection on the utilization of the agentic framework in real-world applications.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners</title>
<link>https://arxiv.org/abs/2504.14239</link>
<guid>https://arxiv.org/abs/2504.14239</guid>
<content:encoded><![CDATA[
arXiv:2504.14239v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have powered Graphical User Interface (GUI) Agents, showing promise in automating tasks on computing devices. Recent works have begun exploring reasoning in GUI tasks with encouraging results. However, many current approaches rely on manually designed reasoning templates, which may result in reasoning that is not sufficiently robust and adaptive for complex GUI environments. Meanwhile, some existing agents continue to operate as Reactive Actors, relying primarily on implicit reasoning that may lack sufficient depth for GUI tasks demanding planning and error recovery. We argue that advancing these agents requires a shift from reactive acting towards acting based on deliberate reasoning. To facilitate this transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed through our Actor2Reasoner framework, a reasoning-centric, two-stage training approach designed to progressively evolve agents from Reactive Actors to Deliberative Reasoners. The first stage, Reasoning Injection, focuses on establishing a basic reasoner. We employ Spatial Reasoning Distillation to transfer cross-modal spatial reasoning capabilities from teacher models to MLLMs through trajectories with explicit reasoning steps, enabling models to integrate GUI visual-spatial information with logical reasoning before action generation. The second stage, Deliberation Enhancement, refines the basic reasoner into a deliberative one using Reinforcement Learning. This stage introduces two approaches: Sub-goal Guidance, which rewards models for generating accurate intermediate sub-goals, and Error Recovery Scenario Construction, which creates failure-and-recovery training scenarios from identified prone-to-error steps. Experimental results show InfiGUI-R1 achieves strong performance in GUI grounding and trajectory tasks. Resources at https://github.com/Reallm-Labs/InfiGUI-R1.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Experience-based Refinement of Task Planning Knowledge in Autonomous Robots</title>
<link>https://arxiv.org/abs/2504.14259</link>
<guid>https://arxiv.org/abs/2504.14259</guid>
<content:encoded><![CDATA[
arXiv:2504.14259v1 Announce Type: new 
Abstract: The requirement for autonomous robots to exhibit higher-level cognitive skills by planning and adapting in an ever-changing environment is indeed a great challenge for the AI community. Progress has been made in the automated planning community on refinement and repair of an agent's symbolic knowledge to do task planning in an incomplete or changing environmental model, but these advances up to now have not been transferred to real physical robots. This paper demonstrates how a physical robot can be capable of adapting its symbolic knowledge of the environment, by using experiences in robot action execution to drive knowledge refinement and hence to improve the success rate of the task plans the robot creates. To implement more robust planning systems, we propose a method for refining domain knowledge to improve the knowledge on which intelligent robot behavior is based. This architecture has been implemented and evaluated using a NAO robot. The refined knowledge leads to the future synthesis of task plans which demonstrate decreasing rates of failure over time as faulty knowledge is removed or adjusted.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory</title>
<link>https://arxiv.org/abs/2504.14325</link>
<guid>https://arxiv.org/abs/2504.14325</guid>
<content:encoded><![CDATA[
arXiv:2504.14325v1 Announce Type: new 
Abstract: Letting AI agents interact in multi-agent applications adds a layer of complexity to the interpretability and prediction of AI outcomes, with profound implications for their trustworthy adoption in research and society. Game theory offers powerful models to capture and interpret strategic interaction among agents, but requires the support of reproducible, standardized and user-friendly IT frameworks to enable comparison and interpretation of results. To this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition using Game Theory. We describe its implementation and usage, and we employ it to uncover biased outcomes in popular games among AI agents, depending on the employed Large Language Model (LLM) and used language, as well as on the personality trait or strategic knowledge of the agents. Overall, FAIRGAME allows users to reliably and easily simulate their desired games and scenarios and compare the results across simulation campaigns and with game-theoretic predictions, enabling the systematic discovery of biases, the anticipation of emerging behavior out of strategic interplays, and empowering further research into strategic decision-making using LLM agents.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-based Dynamic Contract for Federated AI Agent Construction in Mobile Metaverses</title>
<link>https://arxiv.org/abs/2504.14326</link>
<guid>https://arxiv.org/abs/2504.14326</guid>
<content:encoded><![CDATA[
arXiv:2504.14326v1 Announce Type: new 
Abstract: Mobile metaverses have attracted significant attention from both academia and industry, which are envisioned as the next-generation Internet, providing users with immersive and ubiquitous metaverse services through mobile devices. Driven by Large Language Models (LLMs) and Vision-Language Models (VLMs), Artificial Intelligence (AI) agents hold the potential to empower the creation, maintenance, and evolution of mobile metaverses. Currently, AI agents are primarily constructed using cloud-based LLMs and VLMs. However, several challenges hinder their effective implementation, including high service latency and potential sensitive data leakage during perception and processing. In this paper, we develop an edge-cloud collaboration-based federated AI agent construction framework in mobile metaverses. Specifically, Edge Servers (ESs), acting as agent infrastructures, collaboratively create agent modules in a distributed manner. The cloud server then integrates these modules into AI agents and deploys them at the edge, thereby enabling low-latency AI agent services for users. Considering that ESs may exhibit dynamic levels of willingness to participate in federated AI agent construction, we design a two-period dynamic contract model to continuously motivate ESs to participate in agent module creation, effectively addressing the dynamic information asymmetry between the cloud server and the ESs. Furthermore, we propose an Enhanced Diffusion Model-based Soft Actor-Critic (EDMSAC) algorithm to efficiently generate optimal dynamic contracts, in which dynamic structured pruning is applied to DM-based actor networks to enhance denoising efficiency and policy learning performance. Extensive simulations demonstrate the effectiveness and superiority of the EDMSAC algorithm and the proposed contract model.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Generation of Test Cases from Task Descriptions via History-aware Planning</title>
<link>https://arxiv.org/abs/2504.14336</link>
<guid>https://arxiv.org/abs/2504.14336</guid>
<content:encoded><![CDATA[
arXiv:2504.14336v1 Announce Type: new 
Abstract: In automated web testing, generating test scripts from natural language task descriptions is crucial for enhancing the test generation process. This activity involves creating the correct sequences of actions to form test scripts for future testing activities. Current state-of-the-art approaches are limited in generating these action sequences, as they either demand substantial manual effort for human demonstrations or fail to consider the history of previous web content and actions to decide the next action. In this paper, we introduce HxAgent, an iterative large language model agent planning approach that determines the next action based on: 1) observations of the current contents and feasible actions, 2) short-term memory of previous web states and actions, and 3) long-term experience with (in)correct action sequences. The agent generates a sequence of actions to perform a given task, which is effectively an automated test case to verify the task. We conducted an extensive empirical evaluation of HxAgent using two datasets. On the MiniWoB++ dataset, our approach achieves 97% exact-match accuracy that is comparable to the best baselines while eliminating the need for human demonstrations required by those methods. For complex tasks requiring navigation through multiple actions and screens, HxAgent achieves an average 82% exact-match. On the second dataset, comprising 350 task instances across seven popular websites, including YouTube, LinkedIn, Facebook, and Google, HxAgent achieves high performance, with 87% of the action sequences exactly matching the ground truth and a prefix-match of 93%, outperforming the baseline by 59%.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Manipulating Multimodal Agents via Cross-Modal Prompt Injection</title>
<link>https://arxiv.org/abs/2504.14348</link>
<guid>https://arxiv.org/abs/2504.14348</guid>
<content:encoded><![CDATA[
arXiv:2504.14348v1 Announce Type: new 
Abstract: The emergence of multimodal large language models has redefined the agent paradigm by integrating language and vision modalities with external data sources, enabling agents to better interpret human instructions and execute increasingly complex tasks. However, in this work, we identify a critical yet previously overlooked security vulnerability in multimodal agents: cross-modal prompt injection attacks. To exploit this vulnerability, we propose CrossInject, a novel attack framework in which attackers embed adversarial perturbations across multiple modalities to align with target malicious content, allowing external instructions to hijack the agent's decision-making process and execute unauthorized tasks. Our approach consists of two key components. First, we introduce Visual Latent Alignment, where we optimize adversarial features to the malicious instructions in the visual embedding space based on a text-to-image generative model, ensuring that adversarial images subtly encode cues for malicious task execution. Subsequently, we present Textual Guidance Enhancement, where a large language model is leveraged to infer the black-box defensive system prompt through adversarial meta prompting and generate an malicious textual command that steers the agent's output toward better compliance with attackers' requests. Extensive experiments demonstrate that our method outperforms existing injection attacks, achieving at least a +26.4% increase in attack success rates across diverse tasks. Furthermore, we validate our attack's effectiveness in real-world multimodal autonomous agents, highlighting its potential implications for safety-critical applications.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models</title>
<link>https://arxiv.org/abs/2504.14395</link>
<guid>https://arxiv.org/abs/2504.14395</guid>
<content:encoded><![CDATA[
arXiv:2504.14395v1 Announce Type: new 
Abstract: To develop trustworthy Vision-Language Models (VLMs), it is essential to address adversarial robustness and hallucination mitigation, both of which impact factual accuracy in high-stakes applications such as defense and healthcare. Existing methods primarily focus on either adversarial defense or hallucination post-hoc correction, leaving a gap in unified robustness strategies. We introduce \textbf{Hydra}, an adaptive agentic framework that enhances plug-in VLMs through iterative reasoning, structured critiques, and cross-model verification, improving both resilience to adversarial perturbations and intrinsic model errors. Hydra employs an Action-Critique Loop, where it retrieves and critiques visual information, leveraging Chain-of-Thought (CoT) and In-Context Learning (ICL) techniques to refine outputs dynamically. Unlike static post-hoc correction methods, Hydra adapts to both adversarial manipulations and intrinsic model errors, making it robust to malicious perturbations and hallucination-related inaccuracies. We evaluate Hydra on four VLMs, three hallucination benchmarks, two adversarial attack strategies, and two adversarial defense methods, assessing performance on both clean and adversarial inputs. Results show that Hydra surpasses plug-in VLMs and state-of-the-art (SOTA) dehallucination methods, even without explicit adversarial defenses, demonstrating enhanced robustness and factual consistency. By bridging adversarial resistance and hallucination mitigation, Hydra provides a scalable, training-free solution for improving the reliability of VLMs in real-world applications.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planet as a Brain: Towards Internet of AgentSites based on AIOS Server</title>
<link>https://arxiv.org/abs/2504.14411</link>
<guid>https://arxiv.org/abs/2504.14411</guid>
<content:encoded><![CDATA[
arXiv:2504.14411v1 Announce Type: new 
Abstract: The internet is undergoing a historical transformation from the "Internet of Websites" to the "Internet of AgentSites." While traditional Websites served as the foundation for information hosting and dissemination, a new frontier is emerging where AgentSites serve as the hubs of the internet, where each AgentSite hosts one or more AI agents that receive tasks, address them, and deliver actionable solutions, marking a significant shift in the digital landscape and representing the next generation of online ecosystems. Under this vision, AIOS, the AI Agent Operating System, serves as the server for the development, deployment and execution of AI agents, which is a fundamental infrastructure for the Internet of Agentsites.
  In this paper, we introduce AIOS Server, a runtime framework to host agents and enable global-scale collaboration among decentralized agents. AIOS Server provides a communication protocol leveraging the Model Context Protocol (MCP) and JSON-RPC to enable agent-agent or human-agent interactions. Each AIOS node operates as a server to host and execute agents, while supporting peer-to-peer coordination without reliance on centralized orchestration. Based on AIOS Server, we further present the world's first practically deployed Internet of Agentsites (AIOS-IoA), including AgentHub for agent registration and discovery and AgentChat for interactive communication, at https://planet.aios.foundation. The agent discovery mechanism based on Distributed Hash Tables (DHT) and a Gossip protocol serves as the search engine for the internet of agentsites. This work provides a practical foundation for building the Internet of Agentsites-a new paradigm where autonomous agents become first-class citizens of the web. The implementation is available at https://github.com/agiresearch/AIOS.Server and will be integrated into the AIOS main branch at https://github.com/agiresearch/AIOS.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Enhanced Reinforcement Learning for Power Grid Security Assessment</title>
<link>https://arxiv.org/abs/2504.14412</link>
<guid>https://arxiv.org/abs/2504.14412</guid>
<content:encoded><![CDATA[
arXiv:2504.14412v1 Announce Type: new 
Abstract: The increasingly challenging task of maintaining power grid security requires innovative solutions. Novel approaches using reinforcement learning (RL) agents have been proposed to help grid operators navigate the massive decision space and nonlinear behavior of these complex networks. However, applying RL to power grid security assessment, specifically for combinatorially troublesome contingency analysis problems, has proven difficult to scale. The integration of quantum computing into these RL frameworks helps scale by improving computational efficiency and boosting agent proficiency by leveraging quantum advantages in action exploration and model-based interdependence. To demonstrate a proof-of-concept use of quantum computing for RL agent training and simulation, we propose a hybrid agent that runs on quantum hardware using IBM's Qiskit Runtime. We also provide detailed insight into the construction of parameterized quantum circuits (PQCs) for generating relevant quantum output. This agent's proficiency at maintaining grid stability is demonstrated relative to a benchmark model without quantum enhancement using N-k contingency analysis. Additionally, we offer a comparative assessment of the training procedures for RL models integrated with a quantum backend.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing SIA Development: A Case Study in User-Centered Design for Estuary, a Multimodal Socially Interactive Agent Framework</title>
<link>https://arxiv.org/abs/2504.14427</link>
<guid>https://arxiv.org/abs/2504.14427</guid>
<content:encoded><![CDATA[
arXiv:2504.14427v1 Announce Type: new 
Abstract: This case study presents our user-centered design model for Socially Intelligent Agent (SIA) development frameworks through our experience developing Estuary, an open source multimodal framework for building low-latency real-time socially interactive agents. We leverage the Rapid Assessment Process (RAP) to collect the thoughts of leading researchers in the field of SIAs regarding the current state of the art for SIA development as well as their evaluation of how well Estuary may potentially address current research gaps. We achieve this through a series of end-user interviews conducted by a fellow researcher in the community. We hope that the findings of our work will not only assist the continued development of Estuary but also guide the development of other future frameworks and technologies for SIAs.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SG-Reg: Generalizable and Efficient Scene Graph Registration</title>
<link>https://arxiv.org/abs/2504.14440</link>
<guid>https://arxiv.org/abs/2504.14440</guid>
<content:encoded><![CDATA[
arXiv:2504.14440v1 Announce Type: new 
Abstract: This paper addresses the challenges of registering two rigid semantic scene graphs, an essential capability when an autonomous agent needs to register its map against a remote agent, or against a prior map. The hand-crafted descriptors in classical semantic-aided registration, or the ground-truth annotation reliance in learning-based scene graph registration, impede their application in practical real-world environments. To address the challenges, we design a scene graph network to encode multiple modalities of semantic nodes: open-set semantic feature, local topology with spatial awareness, and shape feature. These modalities are fused to create compact semantic node features. The matching layers then search for correspondences in a coarse-to-fine manner. In the back-end, we employ a robust pose estimator to decide transformation according to the correspondences. We manage to maintain a sparse and hierarchical scene representation. Our approach demands fewer GPU resources and fewer communication bandwidth in multi-agent tasks. Moreover, we design a new data generation approach using vision foundation models and a semantic mapping module to reconstruct semantic scene graphs. It differs significantly from previous works, which rely on ground-truth semantic annotations to generate data. We validate our method in a two-agent SLAM benchmark. It significantly outperforms the hand-crafted baseline in terms of registration success rate. Compared to visual loop closure networks, our method achieves a slightly higher registration recall while requiring only 52 KB of communication bandwidth for each query frame. Code available at: \href{http://github.com/HKUST-Aerial-Robotics/SG-Reg}{http://github.com/HKUST-Aerial-Robotics/SG-Reg}.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DialogueAgents: A Hybrid Agent-Based Speech Synthesis Framework for Multi-Party Dialogue</title>
<link>https://arxiv.org/abs/2504.14482</link>
<guid>https://arxiv.org/abs/2504.14482</guid>
<content:encoded><![CDATA[
arXiv:2504.14482v1 Announce Type: new 
Abstract: Speech synthesis is crucial for human-computer interaction, enabling natural and intuitive communication. However, existing datasets involve high construction costs due to manual annotation and suffer from limited character diversity, contextual scenarios, and emotional expressiveness. To address these issues, we propose DialogueAgents, a novel hybrid agent-based speech synthesis framework, which integrates three specialized agents -- a script writer, a speech synthesizer, and a dialogue critic -- to collaboratively generate dialogues. Grounded in a diverse character pool, the framework iteratively refines dialogue scripts and synthesizes speech based on speech review, boosting emotional expressiveness and paralinguistic features of the synthesized dialogues. Using DialogueAgent, we contribute MultiTalk, a bilingual, multi-party, multi-turn speech dialogue dataset covering diverse topics. Extensive experiments demonstrate the effectiveness of our framework and the high quality of the MultiTalk dataset. We release the dataset and code https://github.com/uirlx/DialogueAgents to facilitate future research on advanced speech synthesis models and customized data generation.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinSage: A Multi-aspect RAG System for Financial Filings Question Answering</title>
<link>https://arxiv.org/abs/2504.14493</link>
<guid>https://arxiv.org/abs/2504.14493</guid>
<content:encoded><![CDATA[
arXiv:2504.14493v1 Announce Type: new 
Abstract: Leveraging large language models in real-world settings often entails a need to utilize domain-specific data and tools in order to follow the complex regulations that need to be followed for acceptable use. Within financial sectors, modern enterprises increasingly rely on Retrieval-Augmented Generation (RAG) systems to address complex compliance requirements in financial document workflows. However, existing solutions struggle to account for the inherent heterogeneity of data (e.g., text, tables, diagrams) and evolving nature of regulatory standards used in financial filings, leading to compromised accuracy in critical information extraction. We propose the FinSage framework as a solution, utilizing a multi-aspect RAG framework tailored for regulatory compliance analysis in multi-modal financial documents. FinSage introduces three innovative components: (1) a multi-modal pre-processing pipeline that unifies diverse data formats and generates chunk-level metadata summaries, (2) a multi-path sparse-dense retrieval system augmented with query expansion (HyDE) and metadata-aware semantic search, and (3) a domain-specialized re-ranking module fine-tuned via Direct Preference Optimization (DPO) to prioritize compliance-critical content. Extensive experiments demonstrate that FinSage achieves an impressive recall of 92.51% on 75 expert-curated questions derived from surpasses the best baseline method on the FinanceBench question answering datasets by 24.06% in accuracy. Moreover, FinSage has been successfully deployed as financial question-answering agent in online meetings, where it has already served more than 1,200 people.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VizTA: Enhancing Comprehension of Distributional Visualization with Visual-Lexical Fused Conversational Interface</title>
<link>https://arxiv.org/abs/2504.14507</link>
<guid>https://arxiv.org/abs/2504.14507</guid>
<content:encoded><![CDATA[
arXiv:2504.14507v1 Announce Type: new 
Abstract: Comprehending visualizations requires readers to interpret visual encoding and the underlying meanings actively. This poses challenges for visualization novices, particularly when interpreting distributional visualizations that depict statistical uncertainty. Advancements in LLM-based conversational interfaces show promise in promoting visualization comprehension. However, they fail to provide contextual explanations at fine-grained granularity, and chart readers are still required to mentally bridge visual information and textual explanations during conversations. Our formative study highlights the expectations for both lexical and visual feedback, as well as the importance of explicitly linking these two modalities throughout the conversation. The findings motivate the design of VizTA, a visualization teaching assistant that leverages the fusion of visual and lexical feedback to help readers better comprehend visualization. VizTA features a semantic-aware conversational agent capable of explaining contextual information within visualizations and employs a visual-lexical fusion design to facilitate chart-centered conversation. A between-subject study with 24 participants demonstrates the effectiveness of VizTA in supporting the understanding and reasoning tasks of distributional visualization across multiple scenarios.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey</title>
<link>https://arxiv.org/abs/2504.14520</link>
<guid>https://arxiv.org/abs/2504.14520</guid>
<content:encoded><![CDATA[
arXiv:2504.14520v1 Announce Type: new 
Abstract: This survey explores the development of meta-thinking capabilities in Large Language Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL) perspective. Meta-thinking self-reflection, assessment, and control of thinking processes is an important next step in enhancing LLM reliability, flexibility, and performance, particularly for complex or high-stakes tasks. The survey begins by analyzing current LLM limitations, such as hallucinations and the lack of internal self-assessment mechanisms. It then talks about newer methods, including RL from human feedback (RLHF), self-distillation, and chain-of-thought prompting, and each of their limitations. The crux of the survey is to talk about how multi-agent architectures, namely supervisor-agent hierarchies, agent debates, and theory of mind frameworks, can emulate human-like introspective behavior and enhance LLM robustness. By exploring reward mechanisms, self-play, and continuous learning methods in MARL, this survey gives a comprehensive roadmap to building introspective, adaptive, and trustworthy LLMs. Evaluation metrics, datasets, and future research avenues, including neuroscience-inspired architectures and hybrid symbolic reasoning, are also discussed.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BookWorld: From Novels to Interactive Agent Societies for Creative Story Generation</title>
<link>https://arxiv.org/abs/2504.14538</link>
<guid>https://arxiv.org/abs/2504.14538</guid>
<content:encoded><![CDATA[
arXiv:2504.14538v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have enabled social simulation through multi-agent systems. Prior efforts focus on agent societies created from scratch, assigning agents with newly defined personas. However, simulating established fictional worlds and characters remain largely underexplored, despite its significant practical value. In this paper, we introduce BookWorld, a comprehensive system for constructing and simulating book-based multi-agent societies. BookWorld's design covers comprehensive real-world intricacies, including diverse and dynamic characters, fictional worldviews, geographical constraints and changes, e.t.c. BookWorld enables diverse applications including story generation, interactive games and social simulation, offering novel ways to extend and explore beloved fictional works. Through extensive experiments, we demonstrate that BookWorld generates creative, high-quality stories while maintaining fidelity to the source books, surpassing previous methods with a win rate of 75.36%. The code of this paper can be found at the project page: https://bookworld2025.github.io/.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UFO2: The Desktop AgentOS</title>
<link>https://arxiv.org/abs/2504.14603</link>
<guid>https://arxiv.org/abs/2504.14603</guid>
<content:encoded><![CDATA[
arXiv:2504.14603v1 Announce Type: new 
Abstract: Recent Computer-Using Agents (CUAs), powered by multimodal large language models (LLMs), offer a promising direction for automating complex desktop workflows through natural language. However, most existing CUAs remain conceptual prototypes, hindered by shallow OS integration, fragile screenshot-based interaction, and disruptive execution.
  We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs into practical, system-level automation. UFO2 features a centralized HostAgent for task decomposition and coordination, alongside a collection of application-specialized AppAgent equipped with native APIs, domain-specific knowledge, and a unified GUI--API action layer. This architecture enables robust task execution while preserving modularity and extensibility. A hybrid control detection pipeline fuses Windows UI Automation (UIA) with vision-based parsing to support diverse interface styles. Runtime efficiency is further enhanced through speculative multi-action planning, reducing per-step LLM overhead. Finally, a Picture-in-Picture (PiP) interface enables automation within an isolated virtual desktop, allowing agents and users to operate concurrently without interference.
  We evaluate UFO2 across over 20 real-world Windows applications, demonstrating substantial improvements in robustness and execution accuracy over prior CUAs. Our results show that deep OS integration unlocks a scalable path toward reliable, user-aligned desktop automation.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Optimal Circuit Generation: Multi-Agent Collaboration Meets Collective Intelligence</title>
<link>https://arxiv.org/abs/2504.14625</link>
<guid>https://arxiv.org/abs/2504.14625</guid>
<content:encoded><![CDATA[
arXiv:2504.14625v1 Announce Type: new 
Abstract: Large language models (LLMs) have transformed code generation, yet their application in hardware design produces gate counts 38\%--1075\% higher than human designs. We present CircuitMind, a multi-agent framework that achieves human-competitive efficiency through three key innovations: syntax locking (constraining generation to basic logic gates), retrieval-augmented generation (enabling knowledge-driven design), and dual-reward optimization (balancing correctness with efficiency). To evaluate our approach, we introduce TC-Bench, the first gate-level benchmark harnessing collective intelligence from the TuringComplete ecosystem -- a competitive circuit design platform with hundreds of thousands of players. Experiments show CircuitMind enables 55.6\% of model implementations to match or exceed top-tier human experts in composite efficiency metrics. Most remarkably, our framework elevates the 14B Phi-4 model to outperform both GPT-4o mini and Gemini 2.0 Flash, achieving efficiency comparable to the top 25\% of human experts without requiring specialized training. These innovations establish a new paradigm for hardware optimization where collaborative AI systems leverage collective human expertise to achieve optimal circuit designs. Our model, data, and code are open-source at https://github.com/BUAA-CLab/CircuitMind.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Benchmarking and Aligning Task-Planning Safety in LLM-Based Embodied Agents</title>
<link>https://arxiv.org/abs/2504.14650</link>
<guid>https://arxiv.org/abs/2504.14650</guid>
<content:encoded><![CDATA[
arXiv:2504.14650v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit substantial promise in enhancing task-planning capabilities within embodied agents due to their advanced reasoning and comprehension. However, the systemic safety of these agents remains an underexplored frontier. In this study, we present Safe-BeAl, an integrated framework for the measurement (SafePlan-Bench) and alignment (Safe-Align) of LLM-based embodied agents' behaviors. SafePlan-Bench establishes a comprehensive benchmark for evaluating task-planning safety, encompassing 2,027 daily tasks and corresponding environments distributed across 8 distinct hazard categories (e.g., Fire Hazard). Our empirical analysis reveals that even in the absence of adversarial inputs or malicious intent, LLM-based agents can exhibit unsafe behaviors. To mitigate these hazards, we propose Safe-Align, a method designed to integrate physical-world safety knowledge into LLM-based embodied agents while maintaining task-specific performance. Experiments across a variety of settings demonstrate that Safe-BeAl provides comprehensive safety validation, improving safety by 8.55 - 15.22%, compared to embodied agents based on GPT-4, while ensuring successful task completion.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Complete and Bounded-Suboptimal Algorithm for a Moving Target Traveling Salesman Problem with Obstacles in 3D</title>
<link>https://arxiv.org/abs/2504.14680</link>
<guid>https://arxiv.org/abs/2504.14680</guid>
<content:encoded><![CDATA[
arXiv:2504.14680v1 Announce Type: new 
Abstract: The moving target traveling salesman problem with obstacles (MT-TSP-O) seeks an obstacle-free trajectory for an agent that intercepts a given set of moving targets, each within specified time windows, and returns to the agent's starting position. Each target moves with a constant velocity within its time windows, and the agent has a speed limit no smaller than any target's speed. We present FMC*-TSP, the first complete and bounded-suboptimal algorithm for the MT-TSP-O, and results for an agent whose configuration space is $\mathbb{R}^3$. Our algorithm interleaves a high-level search and a low-level search, where the high-level search solves a generalized traveling salesman problem with time windows (GTSP-TW) to find a sequence of targets and corresponding time windows for the agent to visit. Given such a sequence, the low-level search then finds an associated agent trajectory. To solve the low-level planning problem, we develop a new algorithm called FMC*, which finds a shortest path on a graph of convex sets (GCS) via implicit graph search and pruning techniques specialized for problems with moving targets. We test FMC*-TSP on 280 problem instances with up to 40 targets and demonstrate its smaller median runtime than a baseline based on prior work.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM-enabled Multi-Agent Autonomous Mechatronics Design Framework</title>
<link>https://arxiv.org/abs/2504.14681</link>
<guid>https://arxiv.org/abs/2504.14681</guid>
<content:encoded><![CDATA[
arXiv:2504.14681v1 Announce Type: new 
Abstract: Existing LLM-enabled multi-agent frameworks are predominantly limited to digital or simulated environments and confined to narrowly focused knowledge domain, constraining their applicability to complex engineering tasks that require the design of physical embodiment, cross-disciplinary integration, and constraint-aware reasoning. This work proposes a multi-agent autonomous mechatronics design framework, integrating expertise across mechanical design, optimization, electronics, and software engineering to autonomously generate functional prototypes with minimal direct human design input. Operating primarily through a language-driven workflow, the framework incorporates structured human feedback to ensure robust performance under real-world constraints. To validate its capabilities, the framework is applied to a real-world challenge involving autonomous water-quality monitoring and sampling, where traditional methods are labor-intensive and ecologically disruptive. Leveraging the proposed system, a fully functional autonomous vessel was developed with optimized propulsion, cost-effective electronics, and advanced control. The design process was carried out by specialized agents, including a high-level planning agent responsible for problem abstraction and dedicated agents for structural, electronics, control, and software development. This approach demonstrates the potential of LLM-based multi-agent systems to automate real-world engineering workflows and reduce reliance on extensive domain expertise.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI with Emotions: Exploring Emotional Expressions in Large Language Models</title>
<link>https://arxiv.org/abs/2504.14706</link>
<guid>https://arxiv.org/abs/2504.14706</guid>
<content:encoded><![CDATA[
arXiv:2504.14706v1 Announce Type: new 
Abstract: The human-level performance of Large Language Models (LLMs) across various tasks has raised expectations for the potential of Artificial Intelligence (AI) to possess emotions someday. To explore the capability of current LLMs to express emotions in their outputs, we conducted an experiment using several LLMs (OpenAI GPT, Google Gemini, Meta Llama3, and Cohere Command R+) to role-play as agents answering questions with specified emotional states.We defined the emotional states using Russell's Circumplex model, a well-established framework that characterizes emotions along the sleepy-activated (arousal) and pleasure-displeasure (valence) axes. We chose this model for its simplicity, utilizing two continuous parameters, which allows for better controllability in applications involving continuous changes in emotional states. The responses generated were evaluated using a sentiment analysis model, independent of the LLMs, trained on the GoEmotions dataset. The evaluation showed that the emotional states of the generated answers were consistent with the specifications, demonstrating the LLMs' capability for emotional expression. This indicates the potential for LLM-based AI agents to simulate emotions, opening up a wide range of applications for emotion-based interactions, such as advisors or consultants who can provide advice or opinions with a personal touch.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-Synth: Synthesizing Verifiable Bug-Fix Data to Enable Large Language Models in Resolving Real-World Bugs</title>
<link>https://arxiv.org/abs/2504.14757</link>
<guid>https://arxiv.org/abs/2504.14757</guid>
<content:encoded><![CDATA[
arXiv:2504.14757v1 Announce Type: new 
Abstract: Large language models (LLMs) are transforming automated program repair (APR) through agent-based approaches that localize bugs, generate patches, and verify fixes. However, the lack of high-quality, scalable training datasets, especially those with verifiable outputs and intermediate reasoning traces-limits progress, particularly for open-source models. In this work, we present SWE-Synth, a framework for synthesizing realistic, verifiable, and process-aware bug-fix datasets at the repository level. SWE-Synth leverages LLM agents to simulate debugging workflows, producing not only bug-fix pairs but also test cases and structured repair trajectories. Compared to manually curated datasets, our method scales with minimal human effort while preserving contextual richness and correctness. Experiments show that models trained on SWE-Synth outperform those trained on real-world datasets by 2.3% on SWE-Bench Lite. Our results highlight the potential of synthetic, agent-generated data to advance the state of the art in APR and software engineering automation.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Establishing Workload Identity for Zero Trust CI/CD: From Secrets to SPIFFE-Based Authentication</title>
<link>https://arxiv.org/abs/2504.14760</link>
<guid>https://arxiv.org/abs/2504.14760</guid>
<content:encoded><![CDATA[
arXiv:2504.14760v1 Announce Type: new 
Abstract: CI/CD systems have become privileged automation agents in modern infrastructure, but their identity is still based on secrets or temporary credentials passed between systems. In enterprise environments, these platforms are centralized and shared across teams, often with broad cloud permissions and limited isolation. These conditions introduce risk, especially in the era of supply chain attacks, where implicit trust and static credentials leave systems exposed. This paper describes the shift from static credentials to OpenID Connect (OIDC) federation, and introduces SPIFFE (Secure Production Identity Framework for Everyone) as a runtime-issued, platform-neutral identity model for non-human actors. SPIFFE decouples identity from infrastructure, enabling strong, portable authentication across job runners and deployed workloads. We show how SPIFFE identities support policy alignment, workload attestation, and mutual authentication. The paper concludes by outlining next steps in enabling policy-based access, forming the basis of a broader Zero Trust architecture for CI/CD.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLANET: A Collection of Benchmarks for Evaluating LLMs' Planning Capabilities</title>
<link>https://arxiv.org/abs/2504.14773</link>
<guid>https://arxiv.org/abs/2504.14773</guid>
<content:encoded><![CDATA[
arXiv:2504.14773v1 Announce Type: new 
Abstract: Planning is central to agents and agentic AI. The ability to plan, e.g., creating travel itineraries within a budget, holds immense potential in both scientific and commercial contexts. Moreover, optimal plans tend to require fewer resources compared to ad-hoc methods. To date, a comprehensive understanding of existing planning benchmarks appears to be lacking. Without it, comparing planning algorithms' performance across domains or selecting suitable algorithms for new scenarios remains challenging. In this paper, we examine a range of planning benchmarks to identify commonly used testbeds for algorithm development and highlight potential gaps. These benchmarks are categorized into embodied environments, web navigation, scheduling, games and puzzles, and everyday task automation. Our study recommends the most appropriate benchmarks for various algorithms and offers insights to guide future benchmark development.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Collaborative GenAI Agents in Synchronous Group Settings: Eliciting Team Perceptions and Design Considerations for the Future of Work</title>
<link>https://arxiv.org/abs/2504.14779</link>
<guid>https://arxiv.org/abs/2504.14779</guid>
<content:encoded><![CDATA[
arXiv:2504.14779v1 Announce Type: new 
Abstract: While generative artificial intelligence (GenAI) is finding increased adoption in workplaces, current tools are primarily designed for individual use. Prior work established the potential for these tools to enhance personal creativity and productivity towards shared goals; however, we don't know yet how to best take into account the nuances of group work and team dynamics when deploying GenAI in work settings. In this paper, we investigate the potential of collaborative GenAI agents to augment teamwork in synchronous group settings through an exploratory study that engaged 25 professionals across 6 teams in speculative design workshops and individual follow-up interviews. Our workshops included a mixed reality provotype to simulate embodied collaborative GenAI agents capable of actively participating in group discussions. Our findings suggest that, if designed well, collaborative GenAI agents offer valuable opportunities to enhance team problem-solving by challenging groupthink, bridging communication gaps, and reducing social friction. However, teams' willingness to integrate GenAI agents depended on its perceived fit across a number of individual, team, and organizational factors. We outline the key design tensions around agent representation, social prominence, and engagement and highlight the opportunities spatial and immersive technologies could offer to modulate GenAI influence on team outcomes and strike a balance between augmentation and agency.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADL: A Declarative Language for Agent-Based Chatbots</title>
<link>https://arxiv.org/abs/2504.14787</link>
<guid>https://arxiv.org/abs/2504.14787</guid>
<content:encoded><![CDATA[
arXiv:2504.14787v1 Announce Type: new 
Abstract: There are numerous agent frameworks capable of creating and orchestrating agents to address complex tasks. However, these frameworks are often too complicated for customer service professionals, who may not have much programming experience but still need an easy way to create chatbots with rich business logic. In this work, we introduce ADL, a Declarative Language for Agent-Based Chatbots. ADL simplifies chatbot development by using natural language programming at its core, making it easier for a broad audience to customize and build task-oriented chatbots. It includes four types of agents and supports integration with custom functions, tool use, and third-party agents. ADL abstracts away implementation details, offering a declarative way to define agents and their interactions, which could ease prompt engineering, testing and debugging. MICA, a multi-agent system designed to interpret and execute ADL programs, has been developed and is now available as an open-source project at https://github.com/Mica-labs/MICA. Its user documentation can be found at https://mica-labs.github.io/.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Visual Reinforcement Learning with Separate Primitive Policy for Peg-in-Hole Tasks</title>
<link>https://arxiv.org/abs/2504.14820</link>
<guid>https://arxiv.org/abs/2504.14820</guid>
<content:encoded><![CDATA[
arXiv:2504.14820v1 Announce Type: new 
Abstract: For peg-in-hole tasks, humans rely on binocular visual perception to locate the peg above the hole surface and then proceed with insertion. This paper draws insights from this behavior to enable agents to learn efficient assembly strategies through visual reinforcement learning. Hence, we propose a Separate Primitive Policy (S2P) to simultaneously learn how to derive location and insertion actions. S2P is compatible with model-free reinforcement learning algorithms. Ten insertion tasks featuring different polygons are developed as benchmarks for evaluations. Simulation experiments show that S2P can boost the sample efficiency and success rate even with force constraints. Real-world experiments are also performed to verify the feasibility of S2P. Ablations are finally given to discuss the generalizability of S2P and some factors that affect its performance.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Completing A Systematic Review in Hours instead of Months with Interactive AI Agents</title>
<link>https://arxiv.org/abs/2504.14822</link>
<guid>https://arxiv.org/abs/2504.14822</guid>
<content:encoded><![CDATA[
arXiv:2504.14822v1 Announce Type: new 
Abstract: Systematic reviews (SRs) are vital for evidence-based practice in high stakes disciplines, such as healthcare, but are often impeded by intensive labors and lengthy processes that can take months to complete. Due to the high demand for domain expertise, existing automatic summarization methods fail to accurately identify relevant studies and generate high-quality summaries. To that end, we introduce InsightAgent, a human-centered interactive AI agent powered by large language models that revolutionize this workflow. InsightAgent partitions a large literature corpus based on semantics and employs a multi-agent design for more focused processing of literature, leading to significant improvement in the quality of generated SRs. InsightAgent also provides intuitive visualizations of the corpus and agent trajectories, allowing users to effortlessly monitor the actions of the agent and provide real-time feedback based on their expertise. Our user studies with 9 medical professionals demonstrate that the visualization and interaction mechanisms can effectively improve the quality of synthesized SRs by 27.2%, reaching 79.7% of human-written quality. At the same time, user satisfaction is improved by 34.4%. With InsightAgent, it only takes a clinician about 1.5 hours, rather than months, to complete a high-quality systematic review.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Enhanced Dual-Currency VCG Auction Mechanism for Resource Allocation in IoV: A Value of Information Perspective</title>
<link>https://arxiv.org/abs/2504.14824</link>
<guid>https://arxiv.org/abs/2504.14824</guid>
<content:encoded><![CDATA[
arXiv:2504.14824v1 Announce Type: new 
Abstract: The Internet of Vehicles (IoV) is undergoing a transformative evolution, enabled by advancements in future 6G network technologies, to support intelligent, highly reliable, and low-latency vehicular services. However, the enhanced capabilities of loV have heightened the demands for efficient network resource allocation while simultaneously giving rise to diverse vehicular service requirements. For network service providers (NSPs), meeting the customized resource-slicing requirements of vehicle service providers (VSPs) while maximizing social welfare has become a significant challenge. This paper proposes an innovative solution by integrating a mean-field multi-agent reinforcement learning (MFMARL) framework with an enhanced Vickrey-Clarke-Groves (VCG) auction mechanism to address the problem of social welfare maximization under the condition of unknown VSP utility functions. The core of this solution is introducing the ``value of information" as a novel monetary metric to estimate the expected benefits of VSPs, thereby ensuring the effective execution of the VCG auction mechanism. MFMARL is employed to optimize resource allocation for social welfare maximization while adapting to the intelligent and dynamic requirements of IoV. The proposed enhanced VCG auction mechanism not only protects the privacy of VSPs but also reduces the likelihood of collusion among VSPs, and it is theoretically proven to be dominant-strategy incentive compatible (DSIC). The simulation results demonstrate that, compared to the VCG mechanism implemented using quantization methods, the proposed mechanism exhibits significant advantages in convergence speed, social welfare maximization, and resistance to collusion, providing new insights into resource allocation in intelligent 6G networks.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SQL-Factory: A Multi-Agent Framework for High-Quality and Large-Scale SQL Generation</title>
<link>https://arxiv.org/abs/2504.14837</link>
<guid>https://arxiv.org/abs/2504.14837</guid>
<content:encoded><![CDATA[
arXiv:2504.14837v1 Announce Type: new 
Abstract: Hight quality SQL corpus is essential for intelligent database. For example, Text-to-SQL requires SQL queries and correspond natural language questions as training samples. However, collecting such query corpus remains challenging in practice due to the high cost of manual annotation, which highlights the importance of automatic SQL generation. Despite recent advances, existing generation methods still face limitations in achieving both diversity and cost-effectiveness. Besides, many methods also treat all tables equally during generation, which overlooks schema complexity and leads to under-utilization of structurally rich tables. To address these issues, this paper proposes a multi-agent framework for high-quality and large-scale SQL generation, dubbed SQL-Factory. It decomposes the generation process into three collaborative teams: the Generation Team explores diverse query structures using large language models, the Expansion Team scales promising patterns via lightweight local models, and the Management Team adaptively schedules and evaluates generation based on schema coverage and real-time query quality. This modular framework ensures a balanced trade-off between diversity, scalability, and generation cost. We apply SQL-Factory to four widely used benchmarks and generate over 300,000 executable and broadly distributed SQL queries with less than $200 API cost. Our generated queries achieve higher diversity compared to other methods, and extensive experiments demonstrate that the generated queries significantly improve the model performance in various downstream tasks.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Twin Co-Adaptive Dialogue for Progressive Image Generation</title>
<link>https://arxiv.org/abs/2504.14868</link>
<guid>https://arxiv.org/abs/2504.14868</guid>
<content:encoded><![CDATA[
arXiv:2504.14868v1 Announce Type: new 
Abstract: Modern text-to-image generation systems have enabled the creation of remarkably realistic and high-quality visuals, yet they often falter when handling the inherent ambiguities in user prompts. In this work, we present Twin-Co, a framework that leverages synchronized, co-adaptive dialogue to progressively refine image generation. Instead of a static generation process, Twin-Co employs a dynamic, iterative workflow where an intelligent dialogue agent continuously interacts with the user. Initially, a base image is generated from the user's prompt. Then, through a series of synchronized dialogue exchanges, the system adapts and optimizes the image according to evolving user feedback. The co-adaptive process allows the system to progressively narrow down ambiguities and better align with user intent. Experiments demonstrate that Twin-Co not only enhances user experience by reducing trial-and-error iterations but also improves the quality of the generated images, streamlining the creative process across various applications.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Event triggered optimal formation control for nonlinear multi-agent systems under Denial-of-Service attacks</title>
<link>https://arxiv.org/abs/2504.14874</link>
<guid>https://arxiv.org/abs/2504.14874</guid>
<content:encoded><![CDATA[
arXiv:2504.14874v1 Announce Type: new 
Abstract: This paper investigates the optimal formation control problem of a class of nonlinear multi-agent systems(MASs) under Denial-of-Service(DoS) attacks. We design the optimal formation control law using an event-triggered control scheme to achieve formation objectives under DoS attacks. Critic neural network (NN)-based approach is employed to achieve the optimal control policy under DoS attacks. Event-triggered mechanism is introduced to ensure the saving of control resources. Additionally, Lyapunov stability theory is utilized to demonstrate that the local neighborhood formation error exhibits exponential stability and the estimation error of weights are uniformly ultimately bounded. Finally, the effectiveness of the control algorithm is validated through matlab simulations. The results indicate that under DoS attacks, the nonlinear MAS successfully achieves the desired formation for the MAS.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Time-Varying Gaussian Regression via Kalman Filtering</title>
<link>https://arxiv.org/abs/2504.14900</link>
<guid>https://arxiv.org/abs/2504.14900</guid>
<content:encoded><![CDATA[
arXiv:2504.14900v1 Announce Type: new 
Abstract: We consider the problem of learning time-varying functions in a distributed fashion, where agents collect local information to collaboratively achieve a shared estimate. This task is particularly relevant in control applications, whenever real-time and robust estimation of dynamic cost/reward functions in safety critical settings has to be performed. In this paper, we,adopt a finite-dimensional approximation of a Gaussian Process, corresponding to a Bayesian linear regression in an appropriate feature space, and propose a new algorithm, DistKP, to track the time-varying coefficients via a distributed Kalman filter. The proposed method works for arbitrary kernels and under weaker assumptions on the time-evolution of the function to learn compared to the literature. We validate our results using a simulation example in which a fleet of Unmanned Aerial Vehicles (UAVs) learns a dynamically changing wind field.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework</title>
<link>https://arxiv.org/abs/2504.14928</link>
<guid>https://arxiv.org/abs/2504.14928</guid>
<content:encoded><![CDATA[
arXiv:2504.14928v1 Announce Type: new 
Abstract: Large language models (LLMs) increasingly serve as educational tools, yet evaluating their teaching capabilities remains challenging due to the resource-intensive, context-dependent, and methodologically complex nature of teacher-student interactions. We introduce EducationQ, a multi-agent dialogue framework that efficiently assesses teaching capabilities through simulated dynamic educational scenarios, featuring specialized agents for teaching, learning, and evaluation. Testing 14 LLMs across major AI Organizations (OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13 disciplines and 10 difficulty levels reveals that teaching effectiveness does not correlate linearly with model scale or general reasoning capabilities - with some smaller open-source models outperforming larger commercial counterparts in teaching contexts. This finding highlights a critical gap in current evaluations that prioritize knowledge recall over interactive pedagogy. Our mixed-methods evaluation, combining quantitative metrics with qualitative analysis and expert case studies, identifies distinct pedagogical strengths employed by top-performing models (e.g., sophisticated questioning strategies, adaptive feedback mechanisms). Human expert evaluations show 78% agreement with our automated qualitative analysis of effective teaching behaviors, validating our methodology. EducationQ demonstrates that LLMs-as-teachers require specialized optimization beyond simple scaling, suggesting next-generation educational AI prioritize targeted enhancement of specific pedagogical effectiveness.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanism Design for Auctions with Externalities on Budgets</title>
<link>https://arxiv.org/abs/2504.14948</link>
<guid>https://arxiv.org/abs/2504.14948</guid>
<content:encoded><![CDATA[
arXiv:2504.14948v1 Announce Type: new 
Abstract: This paper studies mechanism design for auctions with externalities on budgets, a novel setting where the budgets that bidders commit are adjusted due to the externality of the competitors' allocation outcomes-a departure from traditional auctions with fixed budgets. This setting is motivated by real-world scenarios, for example, participants may increase their budgets in response to competitors' obtained items. We initially propose a general framework with homogeneous externalities to capture the interdependence between budget updates and allocation, formalized through a budget response function that links each bidder's effective budget to the amount of items won by others.
  The main contribution of this paper is to propose a truthful and individual rational auction mechanism for this novel auction setting, which achieves an approximation ratio of $1/3$ with respect to the liquid welfare. This mechanism is inspired by the uniform-price auction, in which an appropriate uniform price is selected to allocate items, ensuring the monotonicity of the allocation rule while accounting for budget adjustments. Additionally, this mechanism guarantees a constant approximation ratio by setting a purchase limit. Complementing this result, we establish an upper bound: no truthful mechanism can achieve an approximation ratio better than $1/2$. This work offers a new perspective to study the impact of externalities on auctions, providing an approach to handle budget externalities in multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DistilQwen2.5: Industrial Practices of Training Distilled Open Lightweight Language Models</title>
<link>https://arxiv.org/abs/2504.15027</link>
<guid>https://arxiv.org/abs/2504.15027</guid>
<content:encoded><![CDATA[
arXiv:2504.15027v1 Announce Type: new 
Abstract: Enhancing computational efficiency and reducing deployment costs for large language models (LLMs) have become critical challenges in various resource-constrained scenarios. In this work, we present DistilQwen2.5, a family of distilled, lightweight LLMs derived from the public Qwen2.5 models. These distilled models exhibit enhanced instruction-following capabilities compared to the original models based on a series of distillation techniques that incorporate knowledge from much larger LLMs. In our industrial practice, we first leverage powerful proprietary LLMs with varying capacities as multi-agent teachers to select, rewrite, and refine instruction-response pairs that are more suitable for student LLMs to learn. After standard fine-tuning, we further leverage a computationally efficient model fusion approach that enables student models to progressively integrate fine-grained hidden knowledge from their teachers. Experimental evaluations demonstrate that the distilled models possess significantly stronger capabilities than their original checkpoints. Additionally, we present use cases to illustrate the applications of our framework in real-world scenarios. To facilitate practical use, we have released all the DistilQwen2.5 models to the open-source community.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-to-Decision Agent: Learning Generalist Policies from Natural Language Supervision</title>
<link>https://arxiv.org/abs/2504.15046</link>
<guid>https://arxiv.org/abs/2504.15046</guid>
<content:encoded><![CDATA[
arXiv:2504.15046v1 Announce Type: new 
Abstract: RL systems usually tackle generalization by inferring task beliefs from high-quality samples or warmup explorations. The restricted form limits their generality and usability since these supervision signals are expensive and even infeasible to acquire in advance for unseen tasks. Learning directly from the raw text about decision tasks is a promising alternative to leverage a much broader source of supervision. In the paper, we propose Text-to-Decision Agent (T2DA), a simple and scalable framework that supervises generalist policy learning with natural language. We first introduce a generalized world model to encode multi-task decision data into a dynamics-aware embedding space. Then, inspired by CLIP, we predict which textual description goes with which decision embedding, effectively bridging their semantic gap via contrastive language-decision pre-training and aligning the text embeddings to comprehend the environment dynamics. After training the text-conditioned generalist policy, the agent can directly realize zero-shot text-to-decision generation in response to language instructions. Comprehensive experiments on MuJoCo and Meta-World benchmarks show that T2DA facilitates high-capacity zero-shot generalization and outperforms various types of baselines.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast-Slow Co-advancing Optimizer: Toward Harmonious Adversarial Training of GAN</title>
<link>https://arxiv.org/abs/2504.15099</link>
<guid>https://arxiv.org/abs/2504.15099</guid>
<content:encoded><![CDATA[
arXiv:2504.15099v1 Announce Type: new 
Abstract: Up to now, the training processes of typical Generative Adversarial Networks (GANs) are still particularly sensitive to data properties and hyperparameters, which may lead to severe oscillations, difficulties in convergence, or even failures to converge, especially when the overall variances of the training sets are large. These phenomena are often attributed to the training characteristics of such networks. Aiming at the problem, this paper develops a new intelligent optimizer, Fast-Slow Co-advancing Optimizer (FSCO), which employs reinforcement learning in the training process of GANs to make training easier. Specifically, this paper allows the training step size to be controlled by an agent to improve training stability, and makes the training process more intelligent with variable learning rates, making GANs less sensitive to step size. Experiments have been conducted on three benchmark datasets to verify the effectiveness of the developed FSCO.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contemplative Wisdom for Superalignment</title>
<link>https://arxiv.org/abs/2504.15125</link>
<guid>https://arxiv.org/abs/2504.15125</guid>
<content:encoded><![CDATA[
arXiv:2504.15125v1 Announce Type: new 
Abstract: As artificial intelligence (AI) improves, traditional alignment strategies may falter in the face of unpredictable self-improvement, hidden subgoals, and the sheer complexity of intelligent systems. Rather than externally constraining behavior, we advocate designing AI with intrinsic morality built into its cognitive architecture and world model. Inspired by contemplative wisdom traditions, we show how four axiomatic principles can instil a resilient Wise World Model in AI systems. First, mindfulness enables self-monitoring and recalibration of emergent subgoals. Second, emptiness forestalls dogmatic goal fixation and relaxes rigid priors. Third, non-duality dissolves adversarial self-other boundaries. Fourth, boundless care motivates the universal reduction of suffering. We find that prompting AI to reflect on these principles improves performance on the AILuminate Benchmark using GPT-4o, particularly when combined. We offer detailed implementation strategies for state-of-the-art models, including contemplative architectures, constitutions, and reinforcement of chain-of-thought. For future systems, the active inference framework may offer the self-organizing and dynamic coupling capabilities needed to enact these insights in embodied agents. This interdisciplinary approach offers a self-correcting and resilient alternative to prevailing brittle control schemes.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural ATTF: A Scalable Solution to Lifelong Multi-Agent Path Planning</title>
<link>https://arxiv.org/abs/2504.15130</link>
<guid>https://arxiv.org/abs/2504.15130</guid>
<content:encoded><![CDATA[
arXiv:2504.15130v1 Announce Type: new 
Abstract: Multi-Agent Pickup and Delivery (MAPD) is a fundamental problem in robotics, particularly in applications such as warehouse automation and logistics. Existing solutions often face challenges in scalability, adaptability, and efficiency, limiting their applicability in dynamic environments with real-time planning requirements. This paper presents Neural ATTF (Adaptive Task Token Framework), a new algorithm that combines a Priority Guided Task Matching (PGTM) Module with Neural STA* (Space-Time A*), a data-driven path planning method. Neural STA* enhances path planning by enabling rapid exploration of the search space through guided learned heuristics and ensures collision avoidance under dynamic constraints. PGTM prioritizes delayed agents and dynamically assigns tasks by prioritizing agents nearest to these tasks, optimizing both continuity and system throughput. Experimental evaluations against state-of-the-art MAPD algorithms, including TPTS, CENTRAL, RMCA, LNS-PBS, and LNS-wPBS, demonstrate the superior scalability, solution quality, and computational efficiency of Neural ATTF. These results highlight the framework's potential for addressing the critical demands of complex, real-world multi-agent systems operating in high-demand, unpredictable settings.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behavioral Universe Network (BUN): A Behavioral Information-Based Framework for Complex Systems</title>
<link>https://arxiv.org/abs/2504.15146</link>
<guid>https://arxiv.org/abs/2504.15146</guid>
<content:encoded><![CDATA[
arXiv:2504.15146v1 Announce Type: new 
Abstract: Modern digital ecosystems feature complex, dynamic interactions among autonomous entities across diverse domains. Traditional models often separate agents and objects, lacking a unified foundation to capture their interactive behaviors. This paper introduces the Behavioral Universe Network (BUN), a theoretical framework grounded in the Agent-Interaction-Behavior (AIB) formalism. BUN treats subjects (active agents), objects (resources), and behaviors (operations) as first-class entities, all governed by a shared Behavioral Information Base (BIB). We detail the AIB core concepts and demonstrate how BUN leverages information-driven triggers, semantic enrichment, and adaptive rules to coordinate multi-agent systems. We highlight key benefits: enhanced behavior analysis, strong adaptability, and cross-domain interoperability. We conclude by positioning BUN as a promising foundation for next-generation digital governance and intelligent applications.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An experimental study of the influence of anonymous information on social media users</title>
<link>https://arxiv.org/abs/2504.15215</link>
<guid>https://arxiv.org/abs/2504.15215</guid>
<content:encoded><![CDATA[
arXiv:2504.15215v1 Announce Type: new 
Abstract: Increasingly, people use social media for their day-to-day interactions and as a source of information, even though much of this information is practically anonymous. This raises the question: does anonymous information influence its recipients? We conducted an online, two-phase, preregistered experiment using a nationally representative sample of participants from the U.S. to find the answer. To avoid biases of opinions among participants, in the first phase, each participant examines ten Rorschach inkblots and chooses one of four opinions assigned to each inkblot. In the second phase, the participants are randomly assigned to one of four distinct information conditions and are asked to revisit their opinions for the same ten inkblots. Conditions ranged from repeating phase one to receiving anonymous comments about certain opinions. Results were consistent with the preregistration. Importantly, anonymous comments shown in phase two influence up to half of the participants' opinion selections. To better understand the role of anonymous comments in influencing the selections of opinions, we implemented agent-based modeling (ABM). ABM results suggest that a straightforward mechanism can explain the impact of such information. Overall, our results indicate that even anonymous information can have a significant impact on its recipients, potentially altering their popularity rankings. However, the strength of such influence weakens when recipients' confidence in their selections increases. Additionally, we found that participants' confidence in the first phase is inversely related to the number of change opinions.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Self-Improving Coding Agent</title>
<link>https://arxiv.org/abs/2504.15228</link>
<guid>https://arxiv.org/abs/2504.15228</guid>
<content:encoded><![CDATA[
arXiv:2504.15228v1 Announce Type: new 
Abstract: We demonstrate that an LLM coding agent, equipped with basic coding tools, can autonomously edit itself, and thereby improve its performance on benchmark tasks. We find performance gains from 17% to 53% on a random subset of SWE Bench Verified, with additional performance gains on LiveCodeBench, as well as synthetically generated agent benchmarks. Our work represents an advancement in the automated and open-ended design of agentic systems, and provides a reference agent framework for those seeking to post-train LLMs on tool use and other agentic tasks.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowReasoner: Reinforcing Query-Level Meta-Agents</title>
<link>https://arxiv.org/abs/2504.15257</link>
<guid>https://arxiv.org/abs/2504.15257</guid>
<content:encoded><![CDATA[
arXiv:2504.15257v1 Announce Type: new 
Abstract: This paper proposes a query-level meta-agent named FlowReasoner to automate the design of query-level multi-agent systems, i.e., one system per user query. Our core idea is to incentivize a reasoning-based meta-agent via external execution feedback. Concretely, by distilling DeepSeek R1, we first endow the basic reasoning ability regarding the generation of multi-agent systems to FlowReasoner. Then, we further enhance it via reinforcement learning (RL) with external execution feedback. A multi-purpose reward is designed to guide the RL training from aspects of performance, complexity, and efficiency. In this manner, FlowReasoner is enabled to generate a personalized multi-agent system for each user query via deliberative reasoning. Experiments on both engineering and competition code benchmarks demonstrate the superiority of FlowReasoner. Remarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks. The code is available at https://github.com/sail-sg/FlowReasoner.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Locomotion Prediction in Construction Using a Memory-Driven LLM Agent With Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2504.15263</link>
<guid>https://arxiv.org/abs/2504.15263</guid>
<content:encoded><![CDATA[
arXiv:2504.15263v1 Announce Type: new 
Abstract: Construction tasks are inherently unpredictable, with dynamic environments and safety-critical demands posing significant risks to workers. Exoskeletons offer potential assistance but falter without accurate intent recognition across diverse locomotion modes. This paper presents a locomotion prediction agent leveraging Large Language Models (LLMs) augmented with memory systems, aimed at improving exoskeleton assistance in such settings. Using multimodal inputs - spoken commands and visual data from smart glasses - the agent integrates a Perception Module, Short-Term Memory (STM), Long-Term Memory (LTM), and Refinement Module to predict locomotion modes effectively. Evaluation reveals a baseline weighted F1-score of 0.73 without memory, rising to 0.81 with STM, and reaching 0.90 with both STM and LTM, excelling with vague and safety-critical commands. Calibration metrics, including a Brier Score drop from 0.244 to 0.090 and ECE from 0.222 to 0.044, affirm improved reliability. This framework supports safer, high-level human-exoskeleton collaboration, with promise for adaptive assistive systems in dynamic industries.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs</title>
<link>https://arxiv.org/abs/2504.15280</link>
<guid>https://arxiv.org/abs/2504.15280</guid>
<content:encoded><![CDATA[
arXiv:2504.15280v1 Announce Type: new 
Abstract: Multi-view understanding, the ability to reconcile visual information across diverse viewpoints for effective navigation, manipulation, and 3D scene comprehension, is a fundamental challenge in Multi-Modal Large Language Models (MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive advances in high-level reasoning and planning, they frequently fall short when confronted with multi-view geometric consistency and cross-view correspondence. To comprehensively evaluate the challenges of MLLMs in multi-view scene reasoning, we propose All-Angles Bench, a benchmark of over 2,100 human carefully annotated multi-view question-answer pairs across 90 diverse real-world scenes. Our six tasks (counting, attribute identification, relative distance, relative direction, object manipulation, and camera pose estimation) specifically test model's geometric correspondence and the capacity to align information consistently across views. Our extensive experiments, benchmark on 27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and GPT-4o against human evaluators reveals a substantial performance gap, indicating that current MLLMs remain far from human-level proficiency. Through in-depth analysis, we show that MLLMs are particularly underperforming under two aspects: (1) cross-view correspondence for partially occluded views and (2) establishing the coarse camera poses. These findings highlight the necessity of domain-specific refinements or modules that embed stronger multi-view awareness. We believe that our All-Angles Bench offers valuable insights and contribute to bridging the gap between MLLMs and human-level multi-view understanding. The project and benchmark are publicly available at https://danielchyeh.github.io/All-Angles-Bench/.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>System of Agentic AI for the Discovery of Metal-Organic Frameworks</title>
<link>https://arxiv.org/abs/2504.14110</link>
<guid>https://arxiv.org/abs/2504.14110</guid>
<content:encoded><![CDATA[
arXiv:2504.14110v1 Announce Type: cross 
Abstract: Generative models and machine learning promise accelerated material discovery in MOFs for CO2 capture and water harvesting but face significant challenges navigating vast chemical spaces while ensuring synthetizability. Here, we present MOFGen, a system of Agentic AI comprising interconnected agents: a large language model that proposes novel MOF compositions, a diffusion model that generates crystal structures, quantum mechanical agents that optimize and filter candidates, and synthetic-feasibility agents guided by expert rules and machine learning. Trained on all experimentally reported MOFs and computational databases, MOFGen generated hundreds of thousands of novel MOF structures and synthesizable organic linkers. Our methodology was validated through high-throughput experiments and the successful synthesis of five "AI-dreamt" MOFs, representing a major step toward automated synthesizable material discovery.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Lattice Boltzmann Closures through Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.14422</link>
<guid>https://arxiv.org/abs/2504.14422</guid>
<content:encoded><![CDATA[
arXiv:2504.14422v1 Announce Type: cross 
Abstract: The Lattice Boltzmann method (LBM) offers a powerful and versatile approach to simulating diverse hydrodynamic phenomena, spanning microfluidics to aerodynamics. The vast range of spatiotemporal scales inherent in these systems currently renders full resolution impractical, necessitating the development of effective closure models for under-resolved simulations. Under-resolved LBMs are unstable, and while there is a number of important efforts to stabilize them, they often face limitations in generalizing across scales and physical systems. We present a novel, data-driven, multiagent reinforcement learning (MARL) approach that drastically improves stability and accuracy of coarse-grained LBM simulations. The proposed method uses a convolutional neural network to dynamically control the local relaxation parameter for the LB across the simulation grid. The LB-MARL framework is showcased in turbulent Kolmogorov flows. We find that the MARL closures stabilize the simulations and recover the energy spectra of significantly more expensive fully resolved simulations while maintaining computational efficiency. The learned closure model can be transferred to flow scenarios unseen during training and has improved robustness and spectral accuracy compared to traditional LBM models. We believe that MARL closures open new frontiers for efficient and accurate simulations of a multitude of complex problems not accessible to present-day LB methods alone.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM-based Quantum Code Generation with Multi-Agent Optimization and Quantum Error Correction</title>
<link>https://arxiv.org/abs/2504.14557</link>
<guid>https://arxiv.org/abs/2504.14557</guid>
<content:encoded><![CDATA[
arXiv:2504.14557v1 Announce Type: cross 
Abstract: Multi-agent frameworks with Large Language Models (LLMs) have become promising tools for generating general-purpose programming languages using test-driven development, allowing developers to create more accurate and robust code. However, their potential has not been fully unleashed for domain-specific programming languages, where specific domain exhibits unique optimization opportunities for customized improvement. In this paper, we take the first step in exploring multi-agent code generation for quantum programs. By identifying the unique optimizations in quantum designs such as quantum error correction, we introduce a novel multi-agent framework tailored to generating accurate, fault-tolerant quantum code. Each agent in the framework focuses on distinct optimizations, iteratively refining the code using a semantic analyzer with multi-pass inference, alongside an error correction code decoder. We also examine the effectiveness of inference-time techniques, like Chain-of-Thought (CoT) and Retrieval-Augmented Generation (RAG) in the context of quantum programming, uncovering observations that are different from general-purpose code generation. To evaluate our approach, we develop a test suite to measure the impact each optimization has on the accuracy of the generated code. Our findings indicate that techniques such as structured CoT significantly improve the generation of quantum algorithms by up to 50%. In contrast, we have also found that certain techniques such as RAG show limited improvement, yielding an accuracy increase of only 4%. Moreover, we showcase examples of AI-assisted quantum error prediction and correction, demonstrating the effectiveness of our multi-agent framework in reducing the quantum errors of generated quantum programs.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expected Free Energy-based Planning as Variational Inference</title>
<link>https://arxiv.org/abs/2504.14898</link>
<guid>https://arxiv.org/abs/2504.14898</guid>
<content:encoded><![CDATA[
arXiv:2504.14898v1 Announce Type: cross 
Abstract: We address the problem of planning under uncertainty, where an agent must choose actions that not only achieve desired outcomes but also reduce uncertainty. Traditional methods often treat exploration and exploitation as separate objectives, lacking a unified inferential foundation. Active inference, grounded in the Free Energy Principle, offers such a foundation by minimizing Expected Free Energy (EFE), a cost function that combines utility with epistemic drives like ambiguity resolution and novelty seeking. However, the computational burden of EFE minimization has remained a major obstacle to its scalability. In this paper, we show that EFE-based planning arises naturally from minimizing a variational free energy functional on a generative model augmented with preference and epistemic priors. This result reinforces theoretical consistency with the Free Energy Principle, by casting planning itself as variational inference. Our formulation yields optimal policies that jointly support goal achievement and information gain, while incorporating a complexity term that accounts for bounded computational resources. This unifying framework connects and extends existing methods, enabling scalable, resource-aware implementations of active inference agents.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully Adaptive Stepsizes: Which System Benefit More -- Centralized or Decentralized?</title>
<link>https://arxiv.org/abs/2504.15196</link>
<guid>https://arxiv.org/abs/2504.15196</guid>
<content:encoded><![CDATA[
arXiv:2504.15196v1 Announce Type: cross 
Abstract: In decentralized optimization, the choice of stepsize plays a critical role in algorithm performance. A common approach is to use a shared stepsize across all agents to ensure convergence. However, selecting an optimal stepsize often requires careful tuning, which can be time-consuming and may lead to slow convergence, especially when there is significant variation in the smoothness (L-smoothness) of local objective functions across agents. Individually tuning stepsizes per agent is also impractical, particularly in large-scale networks. To address these limitations, we propose AdGT, an adaptive gradient tracking method that enables each agent to adjust its stepsize based on the smoothness of its local objective. We prove that AdGT generates a sequence of iterates that converges to the optimal consensus solution. Through numerical experiments, we compare AdGT with fixed-stepsize gradient tracking methods and demonstrate its superior performance. Additionally, we compare AdGT with adaptive gradient descent (AdGD) in a centralized setting and observe that fully adaptive stepsizes offer greater benefits in decentralized networks than in centralized ones.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Game-Theoretic Multiagent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2011.00583</link>
<guid>https://arxiv.org/abs/2011.00583</guid>
<content:encoded><![CDATA[
arXiv:2011.00583v4 Announce Type: replace 
Abstract: Following the remarkable success of the AlphaGo series, significant advances in multi-agent reinforcement learning (MARL) techniques have been witnessed. MARL corresponds to the learning problem in a multi-agent system in which multiple agents learn simultaneously. It is an interdisciplinary domain with a long history that includes game theory, machine learning, stochastic control, psychology, and optimisation. Although MARL has achieved considerable empirical success in solving real-world games, there is a lack of a self-contained overview in the literature that elaborates the game theoretical foundations of modern MARL methods and summarises the recent advances. In fact, the majority of existing surveys are outdated and do not fully cover the recent developments since 2010. In this work, we provide a monograph on MARL that covers both the fundamentals and the latest developments in the research frontier. The goal of our monograph is to provide a self-contained assessment of the current state-of-the-art MARL techniques from a game theoretical perspective. We expect this work to serve as a stepping stone for both new researchers who are about to enter this fast-growing domain and existing domain experts who want to obtain a panoramic view and identify new directions based on recent advances.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetry-Breaking Augmentations for Ad Hoc Teamwork</title>
<link>https://arxiv.org/abs/2402.09984</link>
<guid>https://arxiv.org/abs/2402.09984</guid>
<content:encoded><![CDATA[
arXiv:2402.09984v2 Announce Type: replace 
Abstract: In dynamic collaborative settings, for artificial intelligence (AI) agents to better align with humans, they must adapt to novel teammates who utilise unforeseen strategies. While adaptation is often simple for humans, it can be challenging for AI agents. Our work introduces symmetry-breaking augmentations (SBA) as a novel approach to this challenge. By applying a symmetry-flipping operation to increase behavioural diversity among training teammates, SBA encourages agents to learn robust responses to unknown strategies, highlighting how social conventions impact human-AI alignment. We demonstrate this experimentally in two settings, showing that our approach outperforms previous ad hoc teamwork results in the challenging card game Hanabi. In addition, we propose a general metric for estimating symmetry dependency amongst a given set of policies. Our findings provide insights into how AI systems can better adapt to diverse human conventions and the core mechanics of alignment.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Tractable $\Phi$-Equilibria in Non-Concave Games</title>
<link>https://arxiv.org/abs/2403.08171</link>
<guid>https://arxiv.org/abs/2403.08171</guid>
<content:encoded><![CDATA[
arXiv:2403.08171v4 Announce Type: replace 
Abstract: While Online Gradient Descent and other no-regret learning procedures are known to efficiently converge to a coarse correlated equilibrium in games where each agent's utility is concave in their own strategy, this is not the case when utilities are non-concave -- a common scenario in machine learning applications involving strategies parameterized by deep neural networks, or when agents' utilities are computed by neural networks, or both. Non-concave games introduce significant game-theoretic and optimization challenges: (i) Nash equilibria may not exist; (ii) local Nash equilibria, though they exist, are intractable; and (iii) mixed Nash, correlated, and coarse correlated equilibria generally have infinite support and are intractable. To sidestep these challenges, we revisit the classical solution concept of $\Phi$-equilibria introduced by Greenwald and Jafari [2003], which is guaranteed to exist for an arbitrary set of strategy modifications $\Phi$ even in non-concave games [Stolz and Lugosi, 2007]. However, the tractability of $\Phi$-equilibria in such games remains elusive.
  In this paper, we initiate the study of tractable $\Phi$-equilibria in non-concave games and examine several natural families of strategy modifications. We show that when $\Phi$ is finite, there exists an efficient uncoupled learning algorithm that converges to the corresponding $\Phi$-equilibria. Additionally, we explore cases where $\Phi$ is infinite but consists of local modifications. We show that approximating local $\Phi$-equilibria beyond the first-order stationary regime is computationally intractable. In contrast, within this regime, we show Online Gradient Descent efficiently converges to $\Phi$-equilibria for several natural infinite families of modifications, including a new structural family of modifications inspired by the well-studied proximal operator.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contract Design for Sequential Actions</title>
<link>https://arxiv.org/abs/2403.09545</link>
<guid>https://arxiv.org/abs/2403.09545</guid>
<content:encoded><![CDATA[
arXiv:2403.09545v2 Announce Type: replace 
Abstract: We introduce a novel model of contracts with combinatorial actions that accounts for sequential and adaptive agent behavior. As in the standard model, a principal delegates the execution of a costly project to an agent. There are $n$ actions, each one incurring a cost to the agent and inducing a probability distribution over $m$ outcomes; each outcome generates some reward for the principal. The principal incentivizes the agent through a contract that specifies a payment for each potential outcome. Unlike the standard model, the agent chooses actions sequentially. Following each action, the agent observes the realized outcome, and decides whether to stop or continue with another action. Upon halting, the agent chooses one of the realized outcomes, which determines both his payment and the principal's reward. This model captures common scenarios where the agent can make multiple attempts in the course of executing a project.
  We study the optimal contract problem in this new setting, namely the contract that maximizes the principal's utility. We first observe that the agent's problem - (adaptively) finding the sequence of actions that maximizes his utility for a given contract - is equivalent to the well-known Pandora's Box problem. Using this insight, we provide algorithms and hardness results for the optimal contract problem, under both independent and correlated actions, and for both linear and general contracts. For independent actions, we provide a poly-time algorithm for the optimal linear contract, and establish that finding the optimal general contract is NP-hard. In cases where the number of outcomes is constant, we devise a poly-time algorithm even for the optimal general contract. For correlated actions, we find that, for both linear and general contracts, approximating the optimal contract within any constant ratio is NP-hard.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Airlift Challenge: A Competition for Optimizing Cargo Delivery</title>
<link>https://arxiv.org/abs/2404.17716</link>
<guid>https://arxiv.org/abs/2404.17716</guid>
<content:encoded><![CDATA[
arXiv:2404.17716v2 Announce Type: replace 
Abstract: Airlift operations require the timely distribution of various cargo, much of which is time sensitive and valuable. These operations, however, have to contend with sudden disruptions from weather and malfunctions, requiring immediate rescheduling. The Airlift Challenge competition seeks possible solutions via a simulator that provides a simplified abstraction of the airlift problem. The simulator uses an OpenAI gym interface that allows participants to create an algorithm for planning agent actions. The algorithm is scored using a remote evaluator against scenarios of ever-increasing difficulty. The second iteration of the competition was underway from November 2023 to April 2024. This paper describes the competition, simulation environment, and results. As a step towards applying generalized planning techniques to the problem, a temporal PDDL domain is presented for the Pickup and Delivery Problem, a model which lies at the core of the Airlift Challenge.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSFF: Investigating LLM Predictive Capabilities for Startup Success through a Multi-Agent Framework with Enhanced Explainability and Performance</title>
<link>https://arxiv.org/abs/2405.19456</link>
<guid>https://arxiv.org/abs/2405.19456</guid>
<content:encoded><![CDATA[
arXiv:2405.19456v2 Announce Type: replace 
Abstract: LLM based agents have recently demonstrated strong potential in automating complex tasks, yet accurately predicting startup success remains an open challenge with few benchmarks and tailored frameworks. To address these limitations, we propose the Startup Success Forecasting Framework, an autonomous system that emulates the reasoning of venture capital analysts through a multi agent collaboration model. Our framework integrates traditional machine learning methods such as random forests and neural networks within a retrieval augmented generation framework composed of three interconnected modules: a prediction block, an analysis block, and an external knowledge block. We evaluate our framework and identify three main findings. First, by leveraging founder segmentation, startups led by L5 founders are 3.79 times more likely to succeed than those led by L1 founders. Second, baseline large language models consistently overpredict startup success and struggle under realistic class imbalances largely due to overreliance on founder claims. Third, our framework significantly enhances prediction accuracy, yielding a 108.3 percent relative improvement over GPT 4o mini and a 30.8 percent relative improvement over GPT 4o. These results demonstrate the value of a multi agent approach combined with discriminative machine learning in mitigating the limitations of standard large language model based prediction methods.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HOPE: A Reinforcement Learning-based Hybrid Policy Path Planner for Diverse Parking Scenarios</title>
<link>https://arxiv.org/abs/2405.20579</link>
<guid>https://arxiv.org/abs/2405.20579</guid>
<content:encoded><![CDATA[
arXiv:2405.20579v4 Announce Type: replace 
Abstract: Automated parking stands as a highly anticipated application of autonomous driving technology. However, existing path planning methodologies fall short of addressing this need due to their incapability to handle the diverse and complex parking scenarios in reality. While non-learning methods provide reliable planning results, they are vulnerable to intricate occasions, whereas learning-based ones are good at exploration but unstable in converging to feasible solutions. To leverage the strengths of both approaches, we introduce Hybrid pOlicy Path plannEr (HOPE). This novel solution integrates a reinforcement learning agent with Reeds-Shepp curves, enabling effective planning across diverse scenarios. HOPE guides the exploration of the reinforcement learning agent by applying an action mask mechanism and employs a transformer to integrate the perceived environmental information with the mask. To facilitate the training and evaluation of the proposed planner, we propose a criterion for categorizing the difficulty level of parking scenarios based on space and obstacle distribution. Experimental results demonstrate that our approach outperforms typical rule-based algorithms and traditional reinforcement learning methods, showing higher planning success rates and generalization across various scenarios. We also conduct real-world experiments to verify the practicability of HOPE. The code for our solution is openly available on https://github.com/jiamiya/HOPE.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RILe: Reinforced Imitation Learning</title>
<link>https://arxiv.org/abs/2406.08472</link>
<guid>https://arxiv.org/abs/2406.08472</guid>
<content:encoded><![CDATA[
arXiv:2406.08472v4 Announce Type: replace 
Abstract: Acquiring complex behaviors is essential for artificially intelligent agents, yet learning these behaviors in high-dimensional settings poses a significant challenge due to the vast search space. Traditional reinforcement learning (RL) requires extensive manual effort for reward function engineering. Inverse reinforcement learning (IRL) uncovers reward functions from expert demonstrations but relies on an iterative process that is often computationally expensive. Imitation learning (IL) provides a more efficient alternative by directly comparing an agent's actions to expert demonstrations; however, in high-dimensional environments, such direct comparisons often offer insufficient feedback for effective learning. We introduce RILe (Reinforced Imitation Learning), a framework that combines the strengths of imitation learning and inverse reinforcement learning to learn a dense reward function efficiently and achieve strong performance in high-dimensional tasks. RILe employs a novel trainer-student framework: the trainer learns an adaptive reward function, and the student uses this reward signal to imitate expert behaviors. By dynamically adjusting its guidance as the student evolves, the trainer provides nuanced feedback across different phases of learning. Our framework produces high-performing policies in high-dimensional tasks where direct imitation fails to replicate complex behaviors. We validate RILe in challenging robotic locomotion tasks, demonstrating that it significantly outperforms existing methods and achieves near-expert performance across multiple settings.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenHands: An Open Platform for AI Software Developers as Generalist Agents</title>
<link>https://arxiv.org/abs/2407.16741</link>
<guid>https://arxiv.org/abs/2407.16741</guid>
<content:encoded><![CDATA[
arXiv:2407.16741v3 Announce Type: replace 
Abstract: Software is one of the most powerful tools that we humans have at our disposal; it allows a skilled programmer to interact with the world in complex and profound ways. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. In this paper, we introduce OpenHands (f.k.a. OpenDevin), a platform for the development of powerful and flexible AI agents that interact with the world in similar ways to those of a human developer: by writing code, interacting with a command line, and browsing the web. We describe how the platform allows for the implementation of new agents, safe interaction with sandboxed environments for code execution, coordination between multiple agents, and incorporation of evaluation benchmarks. Based on our currently incorporated benchmarks, we perform an evaluation of agents over 15 challenging tasks, including software engineering (e.g., SWE-BENCH) and web browsing (e.g., WEBARENA), among others. Released under the permissive MIT license, OpenHands is a community project spanning academia and industry with more than 2.1K contributions from over 188 contributors.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Goal-Conditioned RL Algorithms and Research</title>
<link>https://arxiv.org/abs/2408.11052</link>
<guid>https://arxiv.org/abs/2408.11052</guid>
<content:encoded><![CDATA[
arXiv:2408.11052v3 Announce Type: replace 
Abstract: Self-supervision has the potential to transform reinforcement learning (RL), paralleling the breakthroughs it has enabled in other areas of machine learning. While self-supervised learning in other domains aims to find patterns in a fixed dataset, self-supervised goal-conditioned reinforcement learning (GCRL) agents discover new behaviors by learning from the goals achieved during unstructured interaction with the environment. However, these methods have failed to see similar success, both due to a lack of data from slow environment simulations as well as a lack of stable algorithms. We take a step toward addressing both of these issues by releasing a high-performance codebase and benchmark (JaxGCRL) for self-supervised GCRL, enabling researchers to train agents for millions of environment steps in minutes on a single GPU. By utilizing GPU-accelerated replay buffers, environments, and a stable contrastive RL algorithm, we reduce training time by up to $22\times$. Additionally, we assess key design choices in contrastive RL, identifying those that most effectively stabilize and enhance training performance. With this approach, we provide a foundation for future research in self-supervised GCRL, enabling researchers to quickly iterate on new ideas and evaluate them in diverse and challenging environments. Website + Code: https://github.com/MichalBortkiewicz/JaxGCRL
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-evolving Agents with reflective and memory-augmented abilities</title>
<link>https://arxiv.org/abs/2409.00872</link>
<guid>https://arxiv.org/abs/2409.00872</guid>
<content:encoded><![CDATA[
arXiv:2409.00872v2 Announce Type: replace 
Abstract: Large language models (LLMs) have made significant advances in the field of natural language processing, but they still face challenges such as continuous decision-making. In this research, we propose a novel framework by integrating iterative feedback, reflective mechanisms, and a memory optimization mechanism based on the Ebbinghaus forgetting curve, it significantly enhances the agents' capabilities in handling multi-tasking and long-span information.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlendRL: A Framework for Merging Symbolic and Neural Policy Learning</title>
<link>https://arxiv.org/abs/2410.11689</link>
<guid>https://arxiv.org/abs/2410.11689</guid>
<content:encoded><![CDATA[
arXiv:2410.11689v2 Announce Type: replace 
Abstract: Humans can leverage both symbolic reasoning and intuitive reactions. In contrast, reinforcement learning policies are typically encoded in either opaque systems like neural networks or symbolic systems that rely on predefined symbols and rules. This disjointed approach severely limits the agents' capabilities, as they often lack either the flexible low-level reaction characteristic of neural agents or the interpretable reasoning of symbolic agents. To overcome this challenge, we introduce BlendRL, a neuro-symbolic RL framework that harmoniously integrates both paradigms within RL agents that use mixtures of both logic and neural policies. We empirically demonstrate that BlendRL agents outperform both neural and symbolic baselines in standard Atari environments, and showcase their robustness to environmental changes. Additionally, we analyze the interaction between neural and symbolic policies, illustrating how their hybrid use helps agents overcome each other's limitations.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaCTG: Multi-Agent Collaborative Thought Graph for Automatic Programming</title>
<link>https://arxiv.org/abs/2410.19245</link>
<guid>https://arxiv.org/abs/2410.19245</guid>
<content:encoded><![CDATA[
arXiv:2410.19245v2 Announce Type: replace 
Abstract: With the rapid advancement of Large Language Models (LLMs), LLM-based approaches have demonstrated strong problem-solving capabilities across various domains. However, in automatic programming, a single LLM is typically limited to function-level code generation, while multi-agent systems composed of multiple LLMs often suffer from inefficient task planning. This lack of structured coordination can lead to cascading hallucinations, where accumulated errors across agents result in suboptimal workflows and excessive computational costs. To overcome these challenges, we introduce MaCTG (Multi-Agent Collaborative Thought Graph), a novel multi-agent framework that employs a dynamic graph structure to facilitate precise task allocation and controlled collaboration among LLM agents. MaCTG autonomously assigns agent roles based on programming requirements, dynamically refines task distribution through context-aware adjustments, and systematically verifies and integrates project-level code, effectively reducing hallucination errors and improving overall accuracy. MaCTG enhances cost-effectiveness by implementing a hybrid LLM deployment, where proprietary models handle complex reasoning, while open-source models are used for routine coding and validation tasks. To evaluate MaCTG's effectiveness, we applied it to traditional image processing auto-programming tasks, achieving a state-of-the-art accuracy of 83.33%. Additionally, by leveraging its hybrid LLM configuration, MaCTG significantly reduced operational costs by 89.09% compared to existing multi-agent frameworks, demonstrating its efficiency, scalability, and real-world applicability.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReSpAct: Harmonizing Reasoning, Speaking, and Acting Towards Building Large Language Model-Based Conversational AI Agents</title>
<link>https://arxiv.org/abs/2411.00927</link>
<guid>https://arxiv.org/abs/2411.00927</guid>
<content:encoded><![CDATA[
arXiv:2411.00927v2 Announce Type: replace 
Abstract: Large language model (LLM)-based agents are increasingly employed to interact with external environments (e.g., games, APIs, world models) to solve user-provided tasks. However, current frameworks often lack the ability to collaborate effectively with users in fully conversational settings. Conversations are essential for aligning on task details, achieving user-defined goals, and satisfying preferences. While existing agents address ambiguity through clarification questions, they underutilize the broader potential of an LLM's conversational capabilities. In this work, we introduce ReSpAct, an LLM-based agent designed to seamlessly integrate reasoning, decision-making, and dynamic dialogue for task-solving. Expanding on reasoning-first approaches like ReAct, ReSpAct employs active, free-flowing dialogues to interpret instructions, clarify goals, provide status updates, resolve subtask failures, and refine plans based on user inputs without any explicit dialogue schema. By alternating between task-solving actions and interactive conversations, ReSpAct demonstrates improved performance across diverse environments. We evaluate ReSpAct in user-interactive settings, including task-oriented dialogue systems (MultiWOZ) and decision-making tasks (ALFWorld, WebShop). ReSpAct outperforms ReAct with absolute success rate improvements of 6% and 4% in ALFWorld and WebShop, respectively, and achieves a 5.5% gain in Inform and a 3% gain in Success scores in MultiWOZ. These results highlight the value of integrating dynamic user-agent collaboration for more effective task resolution.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EF2X Exists For Four Agents</title>
<link>https://arxiv.org/abs/2412.00254</link>
<guid>https://arxiv.org/abs/2412.00254</guid>
<content:encoded><![CDATA[
arXiv:2412.00254v2 Announce Type: replace 
Abstract: We study the fair allocation of indivisible goods among a group of agents, aiming to limit the envy between any two agents. The central open problem in this literature, which has proven to be extremely challenging, is regarding the existence of an EFX allocation, i.e., an allocation such that any envy from some agent i toward another agent j would vanish if we were to remove any single good from the bundle allocated to j. When the agents' valuations are additive, which has been the main focus of prior works, Chaudhury et al. [2024] showed that an EFX allocation is guaranteed to exist for all instances involving up to three agents. Subsequently, Berger et al. [2022] extended this guarantee to nice-cancelable valuations and Akrami et al. [2023] to MMS-feasible valuations. However, the existence of EFX allocations for instances involving four agents remains open, even for additive valuations. We contribute to this literature by focusing on EF2X, a relaxation of EFX which requires that any envy toward some agent vanishes if any two of the goods allocated to that agent were to be removed. Our main result shows that EF2X allocations are guaranteed to exist for any instance with four agents, even for the class of cancelable valuations, which is more general than additive. Our proof is constructive, proposing an algorithm that computes such an allocation in pseudopolynomial time. Furthermore, for instances involving three agents we provide an algorithm that computes an EF2X allocation in polynomial time, in contrast to EFX, for which the fastest known algorithm for three agents is only pseudopolynomial.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simultaneously Satisfying MXS and EFL</title>
<link>https://arxiv.org/abs/2412.00358</link>
<guid>https://arxiv.org/abs/2412.00358</guid>
<content:encoded><![CDATA[
arXiv:2412.00358v3 Announce Type: replace 
Abstract: The two standard fairness notions in the resource allocation literature are proportionality and envy-freeness. If there are n agents competing for the available resources, then proportionality requires that each agent receives at least a 1/n fraction of their total value for the set of resources. On the other hand, envy-freeness requires that each agent weakly prefers the resources allocated to them over those allocated to any other agent. Each of these notions has its own benefits, but it is well known that neither one of the two is always achievable when the resources being allocated are indivisible. As a result, a lot of work has focused on satisfying fairness notions that relax either proportionality or envy-freeness. In this paper, we focus on MXS (a relaxation of proportionality) and EFL (a relaxation of envy-freeness). Each of these notions was previously shown to be achievable on its own [Barman et al.,2018, Caragiannis et al., 2023], and our main result is an algorithm that computes allocations that simultaneously satisfy both, combining the benefits of approximate proportionality and approximate envy-freeness. In fact, we prove this for any instance involving agents with valuation functions that are restricted MMS-feasible, which are more general than additive valuations. Also, since every EFL allocation directly satisfies other well-studied fairness notions like EF1, 1/2-EFX, 1/2-GMMS, and 2/3-PMMS, and every MXS allocation satisfies 4/7-MMS, the allocations returned by our algorithm simultaneously satisfy a wide variety of fairness notions and are, therefore, universally fair [Amanatidis et al., 2020].
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steganography in Game Actions</title>
<link>https://arxiv.org/abs/2412.10442</link>
<guid>https://arxiv.org/abs/2412.10442</guid>
<content:encoded><![CDATA[
arXiv:2412.10442v2 Announce Type: replace 
Abstract: The exchange of messages has always carried with it the timeless challenge of secrecy. From whispers in shadows to the enigmatic notes written in the margins of history, humanity has long sought ways to convey thoughts that remain imperceptible to all but the chosen few. The challenge of subliminal communication has been addressed in various forms of steganography. However, the field faces a fundamental paradox: as the art of concealment advances, so too does the science of revelation, leading to an ongoing evolutionary interplay. This study seeks to extend the boundaries of what is considered a viable steganographic medium. We explore a steganographic paradigm, in which hidden information is communicated through the episodes of multiple agents interacting with an environment. Each agent, acting as an encoder, learns a policy to disguise the very existence of hidden messages within actions seemingly directed toward innocent objectives. Meanwhile, an observer, serving as a decoder, learns to associate behavioural patterns with their respective agents despite their dynamic nature, thereby unveiling the hidden messages. The interactions of agents are governed by the framework of multi-agent reinforcement learning and shaped by feedback from the observer. This framework encapsulates a game-theoretic dilemma, wherein agents face decisions between cooperating to create distinguishable behavioural patterns or defecting to pursue individually optimal yet potentially overlapping episodic actions. As a proof of concept, we exemplify action steganography through the game of labyrinth, a navigation task where subliminal communication is concealed within the act of steering toward a destination, and systematically validate the stego-system in terms of distortion, capacity, secrecy and robustness when subjected to simulated passive and active adversaries.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The EnvDesign Model: A Method to Solve the Environment Design Problem</title>
<link>https://arxiv.org/abs/2412.18109</link>
<guid>https://arxiv.org/abs/2412.18109</guid>
<content:encoded><![CDATA[
arXiv:2412.18109v3 Announce Type: replace 
Abstract: Today, several people and organizations rely on cloud platforms. The reliability of cloud platforms depends heavily on the performance of their internal programs (agents). To better prevent regressions in cloud platforms, the design of pre-production testing environments (that test new agents, new hardwares, and other changes) must take into account the diversity of server/node properties (hardware model, virtual machine type, etc.) across the fleet and dynamically emphasize or de-emphasize the prevalence of certain node properties based on current testing priorities. This paper formulates this task as the ``environment design" problem and presents the EnvDesign model, a method that uses graph theory and optimization algorithms to solve the environment design problem. The EnvDesign model was built on context and techniques that apply to combinatorial testing in general, so it can support combinatorial testing in other domains. An earlier version of this paper was peer-reviewed and published internally at Microsoft.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Humanoid Locomotion and Manipulation: Current Progress and Challenges in Control, Planning, and Learning</title>
<link>https://arxiv.org/abs/2501.02116</link>
<guid>https://arxiv.org/abs/2501.02116</guid>
<content:encoded><![CDATA[
arXiv:2501.02116v2 Announce Type: replace 
Abstract: Humanoid robots hold great potential to perform various human-level skills, involving unified locomotion and manipulation in real-world settings. Driven by advances in machine learning and the strength of existing model-based approaches, these capabilities have progressed rapidly, but often separately. This survey offers a comprehensive overview of the state-of-the-art in humanoid locomotion and manipulation (HLM), with a focus on control, planning, and learning methods. We first review the model-based methods that have been the backbone of humanoid robotics for the past three decades. We discuss contact planning, motion planning, and whole-body control, highlighting the trade-offs between model fidelity and computational efficiency. Then the focus is shifted to examine emerging learning-based methods, with an emphasis on reinforcement and imitation learning that enhance the robustness and versatility of loco-manipulation skills. Furthermore, we assess the potential of integrating foundation models with humanoid embodiments to enable the development of generalist humanoid agents. This survey also highlights the emerging role of tactile sensing, particularly whole-body tactile feedback, as a crucial modality for handling contact-rich interactions. Finally, we compare the strengths and limitations of model-based and learning-based paradigms from multiple perspectives, such as robustness, computational efficiency, versatility, and generalizability, and suggest potential solutions to existing challenges.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Enable Effective Cooperation Between Humans and NLP Models: A Survey of Principles, Formalizations, and Beyond</title>
<link>https://arxiv.org/abs/2501.05714</link>
<guid>https://arxiv.org/abs/2501.05714</guid>
<content:encoded><![CDATA[
arXiv:2501.05714v2 Announce Type: replace 
Abstract: With the advancement of large language models (LLMs), intelligent models have evolved from mere tools to autonomous agents with their own goals and strategies for cooperating with humans. This evolution has birthed a novel paradigm in NLP, i.e., human-model cooperation, that has yielded remarkable progress in numerous NLP tasks in recent years. In this paper, we take the first step to present a thorough review of human-model cooperation, exploring its principles, formalizations, and open challenges. In particular, we introduce a new taxonomy that provides a unified perspective to summarize existing approaches. Also, we discuss potential frontier areas and their corresponding challenges. We regard our work as an entry point, paving the way for more breakthrough research in this regard.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Theory of Mind Enables the Invention of Proto-Writing</title>
<link>https://arxiv.org/abs/2502.01568</link>
<guid>https://arxiv.org/abs/2502.01568</guid>
<content:encoded><![CDATA[
arXiv:2502.01568v4 Announce Type: replace 
Abstract: Symbolic writing systems are graphical semiotic codes that are ubiquitous in modern society but are otherwise absent in the animal kingdom. Anthropological evidence suggests that the earliest forms of some writing systems originally consisted of iconic pictographs, which signify their referent via visual resemblance. While previous studies have examined the emergence and, separately, the evolution of pictographic systems through a computational lens, most employ non-naturalistic methodologies that make it difficult to draw clear analogies to human and animal cognition. We develop a multi-agent reinforcement learning testbed for emergent communication called a Signification Game, and formulate a model of inferential communication that enables agents to leverage visual theory of mind to communicate actions using pictographs. Our model, which is situated within a broader formalism for animal communication, sheds light on the cognitive and cultural processes underlying the emergence of proto-writing.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual Descriptions Using Gaussian Splatting, ChatGPT/Deepseek, and Google Maps Platform</title>
<link>https://arxiv.org/abs/2502.05769</link>
<guid>https://arxiv.org/abs/2502.05769</guid>
<content:encoded><![CDATA[
arXiv:2502.05769v3 Announce Type: replace 
Abstract: Urban digital twins are virtual replicas of cities that use multi-source data and data analytics to optimize urban planning, infrastructure management, and decision-making. Towards this, we propose a framework focused on the single-building scale. By connecting to cloud mapping platforms such as Google Map Platforms APIs, by leveraging state-of-the-art multi-agent Large Language Models data analysis using ChatGPT(4o) and Deepseek-V3/R1, and by using our Gaussian Splatting-based mesh extraction pipeline, our Digital Twin Buildings framework can retrieve a building's 3D model, visual descriptions, and achieve cloud-based mapping integration with large language model-based data analytics using a building's address, postal code, or geographic coordinates.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grassroots Platforms with Atomic Transactions: Social Networks, Cryptocurrencies, and Democratic Federations</title>
<link>https://arxiv.org/abs/2502.11299</link>
<guid>https://arxiv.org/abs/2502.11299</guid>
<content:encoded><![CDATA[
arXiv:2502.11299v4 Announce Type: replace 
Abstract: Grassroots platforms aim to offer an egalitarian alternative to global platforms. Whereas global platforms can have only a single instance, grassroots platforms can have multiple instances that emerge and operate independently of each other and of any global resource except the network, and can interoperate and coalesce into ever-larger instances once interconnected. Key grassroots platforms include grassroots social networks, grassroots cryptocurrencies, and grassroots democratic federations. Previously, grassroots platforms were defined formally and proven grassroots using unary distributed transition systems, in which each transition is carried out by a single agent. However, grassroots platforms cater for a more abstract specification using transactions carried out atomically by multiple agents, something that cannot be expressed by unary transition systems. As a result, their original specifications and proofs were unnecessarily cumbersome and opaque.
  We enhance the notion of a distributed transition system to include atomic transactions and revisit the notion of grassroots platforms within this new foundation; present crisp specifications of key grassroots platforms using atomic transactions: befriending and defriending for grassroots social networks, coin swaps for grassroots cryptocurrencies, and communities forming, joining, and leaving a federation for grassroots democratic federations; prove a general theorem that a platform specified by atomic transactions that are so-called interactive is grassroots; show that the atomic transactions used to specify all three platforms are interactive; and conclude that the platforms thus specified are indeed grassroots. We thus provide a crisp mathematical foundation for grassroots platforms and a solid and clear starting point from which their implementation can commence.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding</title>
<link>https://arxiv.org/abs/2503.10596</link>
<guid>https://arxiv.org/abs/2503.10596</guid>
<content:encoded><![CDATA[
arXiv:2503.10596v2 Announce Type: replace 
Abstract: Pixel grounding, encompassing tasks such as Referring Expression Segmentation (RES), has garnered considerable attention due to its immense potential for bridging the gap between vision and language modalities. However, advancements in this domain are currently constrained by limitations inherent in existing datasets, including limited object categories, insufficient textual diversity, and a scarcity of high-quality annotations. To mitigate these limitations, we introduce GroundingSuite, which comprises: (1) an automated data annotation framework leveraging multiple Vision-Language Model (VLM) agents; (2) a large-scale training dataset encompassing 9.56 million diverse referring expressions and their corresponding segmentations; and (3) a meticulously curated evaluation benchmark consisting of 3,800 images. The GroundingSuite training dataset facilitates substantial performance improvements, enabling models trained on it to achieve state-of-the-art results. Specifically, a cIoU of 68.9 on gRefCOCO and a gIoU of 55.3 on RefCOCOm. Moreover, the GroundingSuite annotation framework demonstrates superior efficiency compared to the current leading data annotation method, i.e., $4.5 \times$ faster than the GLaMM.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models as Quasi-crystals: Coherence Without Repetition in Generative Text</title>
<link>https://arxiv.org/abs/2504.11986</link>
<guid>https://arxiv.org/abs/2504.11986</guid>
<content:encoded><![CDATA[
arXiv:2504.11986v2 Announce Type: replace 
Abstract: This essay proposes an interpretive analogy between large language models (LLMs) and quasicrystals, systems that exhibit global coherence without periodic repetition, generated through local constraints. While LLMs are typically evaluated in terms of predictive accuracy, factuality, or alignment, this structural perspective suggests that one of their most characteristic behaviors is the production of internally resonant linguistic patterns. Drawing on the history of quasicrystals, which forced a redefinition of structural order in physical systems, the analogy highlights an alternative mode of coherence in generative language: constraint-based organization without repetition or symbolic intent. Rather than viewing LLMs as imperfect agents or stochastic approximators, we suggest understanding them as generators of quasi-structured outputs. This framing complements existing evaluation paradigms by foregrounding formal coherence and pattern as interpretable features of model behavior. While the analogy has limits, it offers a conceptual tool for exploring how coherence might arise and be assessed in systems where meaning is emergent, partial, or inaccessible. In support of this perspective, we draw on philosophy of science and language, including model-based accounts of scientific representation, structural realism, and inferentialist views of meaning. We further propose the notion of structural evaluation: a mode of assessment that examines how well outputs propagate constraint, variation, and order across spans of generated text. This essay aims to reframe the current discussion around large language models, not by rejecting existing methods, but by suggesting an additional axis of interpretation grounded in structure rather than semantics.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication Optimization for Decentralized Learning atop Bandwidth-limited Edge Networks</title>
<link>https://arxiv.org/abs/2504.12210</link>
<guid>https://arxiv.org/abs/2504.12210</guid>
<content:encoded><![CDATA[
arXiv:2504.12210v2 Announce Type: replace 
Abstract: Decentralized federated learning (DFL) is a promising machine learning paradigm for bringing artificial intelligence (AI) capabilities to the network edge. Running DFL on top of edge networks, however, faces severe performance challenges due to the extensive parameter exchanges between agents. Most existing solutions for these challenges were based on simplistic communication models, which cannot capture the case of learning over a multi-hop bandwidth-limited network. In this work, we address this problem by jointly designing the communication scheme for the overlay network formed by the agents and the mixing matrix that controls the communication demands between the agents. By carefully analyzing the properties of our problem, we cast each design problem into a tractable optimization and develop an efficient algorithm with guaranteed performance. Our evaluations based on real topology and data show that the proposed algorithm can reduce the total training time by over $80\%$ compared to the baseline without sacrificing accuracy, while significantly improving the computational efficiency over the state of the art.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Arabic Speech Recognition Through Large-Scale Weakly Supervised Learning</title>
<link>https://arxiv.org/abs/2504.12254</link>
<guid>https://arxiv.org/abs/2504.12254</guid>
<content:encoded><![CDATA[
arXiv:2504.12254v2 Announce Type: replace 
Abstract: Automatic speech recognition (ASR) is crucial for human-machine interaction in diverse applications like conversational agents, industrial robotics, call center automation, and automated subtitling. However, developing high-performance ASR models remains challenging, particularly for low-resource languages like Arabic, due to the scarcity of large, labeled speech datasets, which are costly and labor-intensive to produce. In this work, we employ weakly supervised learning to train an Arabic ASR model using the Conformer architecture. Our model is trained from scratch on 15,000 hours of weakly annotated speech data covering both Modern Standard Arabic (MSA) and Dialectal Arabic (DA), eliminating the need for costly manual transcriptions. Despite the absence of human-verified labels, our approach achieves state-of-the-art (SOTA) results in Arabic ASR, surpassing both open and closed-source models on standard benchmarks. By demonstrating the effectiveness of weak supervision as a scalable, cost-efficient alternative to traditional supervised approaches, paving the way for improved ASR systems in low resource settings.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Strategic Coordination Framework of Small LLMs Matches Large LLMs in Data Synthesis</title>
<link>https://arxiv.org/abs/2504.12322</link>
<guid>https://arxiv.org/abs/2504.12322</guid>
<content:encoded><![CDATA[
arXiv:2504.12322v2 Announce Type: replace 
Abstract: While data synthesis and distillation are promising strategies to enhance small language models, current approaches heavily rely on Large Language Models (LLMs), which suffer from high computational costs, environmental inefficiency, and potential biases inherited from monolithic architectures. In contrast, smaller LLMs are more accessible and sustainable, but their individual capabilities often fall short in generating high-quality, diverse, and reliable data. Inspired by collaborative human processes (e.g., peer review), we propose a multiple small LLMs involved framework, GRA, that aggregates specialized roles across small LLMs to iterative refinement and quality control typically achieved by a single large LLM. In this collaborative framework, multiple small LLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a peer-review-inspired data synthesis pipeline. The Generator proposes initial data samples, the Reviewer critiques their quality and diversity, and the Adjudicator resolves conflicts to finalize the output. By decomposing the synthesis process into specialized sub-tasks, collaborative small LLMs can achieve data-level parity with large LLM-based distillation. Through experiments across multiple benchmarks, we demonstrate that GRA-produced data matches or exceeds the quality of single large LLM outputs, e.g., Qwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large models for high-quality data synthesis, advocating instead for strategic coordination of smaller agents. Our datasets, models, and code are publicly available at https://github.com/GX-XinGao/GRA.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anonymous Public Announcements</title>
<link>https://arxiv.org/abs/2504.12546</link>
<guid>https://arxiv.org/abs/2504.12546</guid>
<content:encoded><![CDATA[
arXiv:2504.12546v2 Announce Type: replace 
Abstract: We formalise the notion of an anonymous public announcement in the tradition of public announcement logic. Such announcements can be seen as in-between a public announcement from ``the outside" (an announcement of $\phi$) and a public announcement by one of the agents (an announcement of $K_a\phi$): we get more information than just $\phi$, but not (necessarily) about exactly who made it. Even if such an announcement is prima facie anonymous, depending on the background knowledge of the agents it might reveal the identity of the announcer: if I post something on a message board, the information might reveal who I am even if I don't sign my name. Furthermore, like in the Russian Cards puzzle, if we assume that the announcer's intention was to stay anonymous, that in fact might reveal more information. In this paper we first look at the case when no assumption about intentions are made, in which case the logic with an anonymous public announcement operator is reducible to epistemic logic. We then look at the case when we assume common knowledge of the intention to stay anonymous, which is both more complex and more interesting: in several ways it boils down to the notion of a ``safe" announcement (again, similarly to Russian Cards). Main results include formal expressivity results and axiomatic completeness for key logical languages.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination</title>
<link>https://arxiv.org/abs/2504.12714</link>
<guid>https://arxiv.org/abs/2504.12714</guid>
<content:encoded><![CDATA[
arXiv:2504.12714v2 Announce Type: replace 
Abstract: Zero-shot coordination (ZSC), the ability to adapt to a new partner in a cooperative task, is a critical component of human-compatible AI. While prior work has focused on training agents to cooperate on a single task, these specialized models do not generalize to new tasks, even if they are highly similar. Here, we study how reinforcement learning on a distribution of environments with a single partner enables learning general cooperative skills that support ZSC with many new partners on many new problems. We introduce two Jax-based, procedural generators that create billions of solvable coordination challenges. We develop a new paradigm called Cross-Environment Cooperation (CEC), and show that it outperforms competitive baselines quantitatively and qualitatively when collaborating with real people. Our findings suggest that learning to collaborate across many unique scenarios encourages agents to develop general norms, which prove effective for collaboration with different partners. Together, our results suggest a new route toward designing generalist cooperative agents capable of interacting with humans without requiring human data.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Expert Failures Improves LLM Agent Tuning</title>
<link>https://arxiv.org/abs/2504.13145</link>
<guid>https://arxiv.org/abs/2504.13145</guid>
<content:encoded><![CDATA[
arXiv:2504.13145v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown tremendous potential as agents, excelling at tasks that require multiple rounds of reasoning and interactions. Rejection Sampling Fine-Tuning (RFT) has emerged as an effective method for finetuning LLMs as agents: it first imitates expert-generated successful trajectories and further improves agentic skills through iterative fine-tuning on successful, self-generated trajectories. However, since the expert (e.g., GPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler scenarios, many complex subtasks remain unsolved and persistently out-of-distribution (OOD). Upon investigating these challenging subtasks, we discovered that previously failed expert trajectories can often provide valuable guidance, e.g., plans and key actions, that can significantly improve agent exploration efficiency and acquisition of critical skills. Motivated by these observations, we propose Exploring Expert Failures (EEF), which identifies beneficial actions from failed expert trajectories and integrates them into the training dataset. Potentially harmful actions are meticulously excluded to prevent contamination of the model learning process. By leveraging the beneficial actions in expert failures, EEF successfully solves some previously unsolvable subtasks and improves agent tuning performance. Remarkably, our approach achieved a 62\% win rate in WebShop, outperforming RFT (53. 6\%) and GPT-4 (35. 6\%), and to the best of our knowledge, setting a new state-of-the-art as the first method to surpass a score of 0.81 in WebShop and exceed 81 in SciWorld.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The emergence of chaos in population game dynamics induced by comparisons</title>
<link>https://arxiv.org/abs/2412.06037</link>
<guid>https://arxiv.org/abs/2412.06037</guid>
<content:encoded><![CDATA[
arXiv:2412.06037v2 Announce Type: replace-cross 
Abstract: Precise description of population game dynamics introduced by revision protocols - an economic model describing the agent's propensity to switch to a better-performing strategy - is of importance in economics and social sciences in general. In this setting innovation or imitation of others is the force which drives the evolution of the economic system. As the continuous-time game dynamics is relatively well understood, the same cannot be said about revision driven dynamics in the discrete time. We investigate the behavior of agents in a $2\times 2$ anti-coordination game with symmetric random matching and a unique mixed Nash equilibrium. In continuous time the Nash equilibrium is attracting and induces a global evolutionary stable state. We show that in the discrete time one can construct (either innovative or imitative) revision protocol and choose a level of the time step, under which the game dynamics is Li-Yorke chaotic, inducing complex and unpredictable behavior of the system, precluding stable predictions of equilibrium. Moreover, we reveal that this unpredictability is encoded into any imitative revision protocol. Furthermore, we show that for any such game there exists a perturbed pairwise proportional imitation protocol introducing chaotic behavior of the agents for sufficiently large time step.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Bayesian Interpretation of the Internal Model Principle</title>
<link>https://arxiv.org/abs/2503.00511</link>
<guid>https://arxiv.org/abs/2503.00511</guid>
<content:encoded><![CDATA[
arXiv:2503.00511v2 Announce Type: replace-cross 
Abstract: The internal model principle, originally proposed in the theory of control of linear systems, nowadays represents a more general class of results in control theory and cybernetics. The central claim of these results is that, under suitable assumptions, if a system (a controller) can regulate against a class of external inputs (from the environment), it is because the system contains a model of the system causing these inputs, which can be used to generate signals counteracting them. Similar claims on the role of internal models appear also in cognitive science, especially in modern Bayesian treatments of cognitive agents, often suggesting that a system (a human subject, or some other agent) models its environment to adapt against disturbances and perform goal-directed behaviour. It is however unclear whether the Bayesian internal models discussed in cognitive science bear any formal relation to the internal models invoked in standard treatments of control theory. Here, we first review the internal model principle and present a precise formulation of it using concepts inspired by categorical systems theory. This leads to a formal definition of ``model'' generalising its use in the internal model principle. Although this notion of model is not a priori related to the notion of Bayesian reasoning, we show that it can be seen as a special case of possibilistic Bayesian filtering. This result is based on a recent line of work formalising, using Markov categories, a notion of ``interpretation'', describing when a system can be interpreted as performing Bayesian filtering on an outside world in a consistent way.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Factors That Influence the Adoption of AI-enabled Conversational Agents (AICAs) as an Augmenting Therapeutic Tool by Frontline Healthcare Workers: From Technology Acceptance Model 3 (TAM3) Lens -- A Systematic Mapping Review</title>
<link>https://arxiv.org/abs/2504.13183</link>
<guid>https://arxiv.org/abs/2504.13183</guid>
<content:encoded><![CDATA[
arXiv:2504.13183v1 Announce Type: new 
Abstract: Artificial intelligent (AI) conversational agents hold a promising future in the field of mental health, especially in helping marginalized communities that lack access to mental health support services. It is tempting to have a 24/7 mental health companion that can be accessed anywhere using mobile phones to provide therapist-like advice. Yet, caution should be taken, and studies around their feasibility need to be surveyed. Before adopting such a rapidly changing technology, studies on its feasibility should be explored, summarized, and synthesized to gain a solid understanding of the status quo and to enable us to build a framework that can guide us throughout the development and deployment processes. Different perspectives must be considered when investigating the feasibility of AI conversational agents, including the mental healthcare professional perspective. The literature can provide insights into their perspectives in terms of opportunities, concerns, and implications. Mental health professionals, the subject-matter experts in this field, have their points of view that should be understood and considered. This systematic literature review will explore mental health practitioners' attitudes toward AI conversational agents and the factors that affect their adoption and recommendation of the technology to augment their services and treatments. The TAM3 Framework will be the lens through which this systematic literature review will be conducted.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cellular-X: An LLM-empowered Cellular Agent for Efficient Base Station Operations</title>
<link>https://arxiv.org/abs/2504.13190</link>
<guid>https://arxiv.org/abs/2504.13190</guid>
<content:encoded><![CDATA[
arXiv:2504.13190v1 Announce Type: new 
Abstract: This paper introduces Cellular-X, an LLM-powered agent designed to automate cellular base station (BS) maintenance. Leveraging multimodal LLM and retrieval-augmented generation (RAG) techniques, Cellular-X significantly enhances field engineer efficiency by quickly interpreting user intents, retrieving relevant technical information, and configuring a BS through iterative self-correction. Key features of the demo include automatic customized BS setup, document-based query answering, and voice-controlled configuration reporting and revision. We implemented Cellular-X on a USRP X310 testbed for demonstration. Demo videos and implementation details are available at https://github.com/SeaBreezing/Cellular-X.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent</title>
<link>https://arxiv.org/abs/2504.13192</link>
<guid>https://arxiv.org/abs/2504.13192</guid>
<content:encoded><![CDATA[
arXiv:2504.13192v1 Announce Type: new 
Abstract: Recently, Large Language Model (LLM)-empowered recommender systems (RecSys) have brought significant advances in personalized user experience and have attracted considerable attention. Despite the impressive progress, the research question regarding the safety vulnerability of LLM-empowered RecSys still remains largely under-investigated. Given the security and privacy concerns, it is more practical to focus on attacking the black-box RecSys, where attackers can only observe the system's inputs and outputs. However, traditional attack approaches employing reinforcement learning (RL) agents are not effective for attacking LLM-empowered RecSys due to the limited capabilities in processing complex textual inputs, planning, and reasoning. On the other hand, LLMs provide unprecedented opportunities to serve as attack agents to attack RecSys because of their impressive capability in simulating human-like decision-making processes. Therefore, in this paper, we propose a novel attack framework called CheatAgent by harnessing the human-like capabilities of LLMs, where an LLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our method first identifies the insertion position for maximum impact with minimal input modification. After that, the LLM agent is designed to generate adversarial perturbations to insert at target positions. To further improve the quality of generated perturbations, we utilize the prompt tuning technique to improve attacking strategies via feedback from the victim RecSys iteratively. Extensive experiments across three real-world datasets demonstrate the effectiveness of our proposed attacking method.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents</title>
<link>https://arxiv.org/abs/2504.13203</link>
<guid>https://arxiv.org/abs/2504.13203</guid>
<content:encoded><![CDATA[
arXiv:2504.13203v1 Announce Type: new 
Abstract: Multi-turn interactions with language models (LMs) pose critical safety risks, as harmful intent can be strategically spread across exchanges. Yet, the vast majority of prior work has focused on single-turn safety, while adaptability and diversity remain among the key challenges of multi-turn red-teaming. To address these challenges, we present X-Teaming, a scalable framework that systematically explores how seemingly harmless interactions escalate into harmful outcomes and generates corresponding attack scenarios. X-Teaming employs collaborative agents for planning, attack optimization, and verification, achieving state-of-the-art multi-turn jailbreak effectiveness and diversity with success rates up to 98.1% across representative leading open-weight and closed-source models. In particular, X-Teaming achieves a 96.2% attack success rate against the latest Claude 3.7 Sonnet model, which has been considered nearly immune to single-turn attacks. Building on X-Teaming, we introduce XGuard-Train, an open-source multi-turn safety training dataset that is 20x larger than the previous best resource, comprising 30K interactive jailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our work offers essential tools and insights for mitigating sophisticated conversational attacks, advancing the multi-turn safety of LMs.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Feasibility of Using MultiModal LLMs to Execute AR Social Engineering Attacks</title>
<link>https://arxiv.org/abs/2504.13209</link>
<guid>https://arxiv.org/abs/2504.13209</guid>
<content:encoded><![CDATA[
arXiv:2504.13209v1 Announce Type: new 
Abstract: Augmented Reality (AR) and Multimodal Large Language Models (LLMs) are rapidly evolving, providing unprecedented capabilities for human-computer interaction. However, their integration introduces a new attack surface for social engineering. In this paper, we systematically investigate the feasibility of orchestrating AR-driven Social Engineering attacks using Multimodal LLM for the first time, via our proposed SEAR framework, which operates through three key phases: (1) AR-based social context synthesis, which fuses Multimodal inputs (visual, auditory and environmental cues); (2) role-based Multimodal RAG (Retrieval-Augmented Generation), which dynamically retrieves and integrates contextual data while preserving character differentiation; and (3) ReInteract social engineering agents, which execute adaptive multiphase attack strategies through inference interaction loops. To verify SEAR, we conducted an IRB-approved study with 60 participants in three experimental configurations (unassisted, AR+LLM, and full SEAR pipeline) compiling a new dataset of 180 annotated conversations in simulated social scenarios. Our results show that SEAR is highly effective at eliciting high-risk behaviors (e.g., 93.3% of participants susceptible to email phishing). The framework was particularly effective in building trust, with 85% of targets willing to accept an attacker's call after an interaction. Also, we identified notable limitations such as ``occasionally artificial'' due to perceived authenticity gaps. This work provides proof-of-concept for AR-LLM driven social engineering attacks and insights for developing defensive countermeasures against next-generation augmented reality threats.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recursive Deep Inverse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.13241</link>
<guid>https://arxiv.org/abs/2504.13241</guid>
<content:encoded><![CDATA[
arXiv:2504.13241v1 Announce Type: new 
Abstract: Inferring an adversary's goals from exhibited behavior is crucial for counterplanning and non-cooperative multi-agent systems in domains like cybersecurity, military, and strategy games. Deep Inverse Reinforcement Learning (IRL) methods based on maximum entropy principles show promise in recovering adversaries' goals but are typically offline, require large batch sizes with gradient descent, and rely on first-order updates, limiting their applicability in real-time scenarios. We propose an online Recursive Deep Inverse Reinforcement Learning (RDIRL) approach to recover the cost function governing the adversary actions and goals. Specifically, we minimize an upper bound on the standard Guided Cost Learning (GCL) objective using sequential second-order Newton updates, akin to the Extended Kalman Filter (EKF), leading to a fast (in terms of convergence) learning algorithm. We demonstrate that RDIRL is able to recover cost and reward functions of expert agents in standard and adversarial benchmark tasks. Experiments on benchmark tasks show that our proposed approach outperforms several leading IRL algorithms.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal-Copilot: An Autonomous Causal Analysis Agent</title>
<link>https://arxiv.org/abs/2504.13263</link>
<guid>https://arxiv.org/abs/2504.13263</guid>
<content:encoded><![CDATA[
arXiv:2504.13263v1 Announce Type: new 
Abstract: Causal analysis plays a foundational role in scientific discovery and reliable decision-making, yet it remains largely inaccessible to domain experts due to its conceptual and algorithmic complexity. This disconnect between causal methodology and practical usability presents a dual challenge: domain experts are unable to leverage recent advances in causal learning, while causal researchers lack broad, real-world deployment to test and refine their methods. To address this, we introduce Causal-Copilot, an autonomous agent that operationalizes expert-level causal analysis within a large language model framework. Causal-Copilot automates the full pipeline of causal analysis for both tabular and time-series data -- including causal discovery, causal inference, algorithm selection, hyperparameter optimization, result interpretation, and generation of actionable insights. It supports interactive refinement through natural language, lowering the barrier for non-specialists while preserving methodological rigor. By integrating over 20 state-of-the-art causal analysis techniques, our system fosters a virtuous cycle -- expanding access to advanced causal methods for domain experts while generating rich, real-world applications that inform and advance causal theory. Empirical evaluations demonstrate that Causal-Copilot achieves superior performance compared to existing baselines, offering a reliable, scalable, and extensible solution that bridges the gap between theoretical sophistication and real-world applicability in causal analysis.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartQA-X: Generating Explanations for Charts</title>
<link>https://arxiv.org/abs/2504.13275</link>
<guid>https://arxiv.org/abs/2504.13275</guid>
<content:encoded><![CDATA[
arXiv:2504.13275v1 Announce Type: new 
Abstract: The ability to interpret and explain complex information from visual data in charts is crucial for data-driven decision-making. In this work, we address the challenge of providing explanations alongside answering questions about chart images. We present ChartQA-X, a comprehensive dataset comprising various chart types with 28,299 contextually relevant questions, answers, and detailed explanations. These explanations are generated by prompting six different models and selecting the best responses based on metrics such as faithfulness, informativeness, coherence, and perplexity. Our experiments show that models fine-tuned on our dataset for explanation generation achieve superior performance across various metrics and demonstrate improved accuracy in question-answering tasks on new datasets. By integrating answers with explanatory narratives, our approach enhances the ability of intelligent agents to convey complex information effectively, improve user understanding, and foster trust in the generated responses.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Definition of Robustness and Resilience of AI Agents for Real-time Congestion Management</title>
<link>https://arxiv.org/abs/2504.13314</link>
<guid>https://arxiv.org/abs/2504.13314</guid>
<content:encoded><![CDATA[
arXiv:2504.13314v1 Announce Type: new 
Abstract: The European Union's Artificial Intelligence (AI) Act defines robustness, resilience, and security requirements for high-risk sectors but lacks detailed methodologies for assessment. This paper introduces a novel framework for quantitatively evaluating the robustness and resilience of reinforcement learning agents in congestion management. Using the AI-friendly digital environment Grid2Op, perturbation agents simulate natural and adversarial disruptions by perturbing the input of AI systems without altering the actual state of the environment, enabling the assessment of AI performance under various scenarios. Robustness is measured through stability and reward impact metrics, while resilience quantifies recovery from performance degradation. The results demonstrate the framework's effectiveness in identifying vulnerabilities and improving AI robustness and resilience for critical applications.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiharmonic algorithms for contrast-enhanced ultrasound</title>
<link>https://arxiv.org/abs/2504.13335</link>
<guid>https://arxiv.org/abs/2504.13335</guid>
<content:encoded><![CDATA[
arXiv:2504.13335v1 Announce Type: new 
Abstract: Harmonic generation plays a crucial role in contrast-enhanced ultrasound, both for imaging and therapeutic applications. However, accurately capturing these nonlinear effects is computationally very demanding when using traditional time-domain approaches. To address this issue, in this work, we develop algorithms based on a time discretization that uses a multiharmonic Ansatz applied to a model that couples the Westervelt equation for acoustic pressure with a volume-based approximation of the Rayleigh--Plesset equation for the dynamics of microbubble contrast agents. We first rigorously establish the existence of time-periodic solutions for this Westervelt-ODE system. We then derive a multiharmonic representation of the system under time-periodic excitation and develop iterative algorithms that rely on the successive computation of higher harmonics under the assumption of real-valued or complex solution fields. In the real-valued setting, we characterize the approximation error in terms of the number of harmonics and a contribution owing to the fixed-point iteration. Finally, we investigate these algorithms numerically and illustrate how the number of harmonics and presence of microbubbles influence the propagation of acoustic waves.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LangCoop: Collaborative Driving with Language</title>
<link>https://arxiv.org/abs/2504.13406</link>
<guid>https://arxiv.org/abs/2504.13406</guid>
<content:encoded><![CDATA[
arXiv:2504.13406v1 Announce Type: new 
Abstract: Multi-agent collaboration holds great promise for enhancing the safety, reliability, and mobility of autonomous driving systems by enabling information sharing among multiple connected agents. However, existing multi-agent communication approaches are hindered by limitations of existing communication media, including high bandwidth demands, agent heterogeneity, and information loss. To address these challenges, we introduce LangCoop, a new paradigm for collaborative autonomous driving that leverages natural language as a compact yet expressive medium for inter-agent communication. LangCoop features two key innovations: Mixture Model Modular Chain-of-thought (M$^3$CoT) for structured zero-shot vision-language reasoning and Natural Language Information Packaging (LangPack) for efficiently packaging information into concise, language-based messages. Through extensive experiments conducted in the CARLA simulations, we demonstrate that LangCoop achieves a remarkable 96\% reduction in communication bandwidth (< 2KB per message) compared to image-based communication, while maintaining competitive driving performance in the closed-loop evaluation.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Model-Based Approach to Imitation Learning through Multi-Step Predictions</title>
<link>https://arxiv.org/abs/2504.13413</link>
<guid>https://arxiv.org/abs/2504.13413</guid>
<content:encoded><![CDATA[
arXiv:2504.13413v1 Announce Type: new 
Abstract: Imitation learning is a widely used approach for training agents to replicate expert behavior in complex decision-making tasks. However, existing methods often struggle with compounding errors and limited generalization, due to the inherent challenge of error correction and the distribution shift between training and deployment. In this paper, we present a novel model-based imitation learning framework inspired by model predictive control, which addresses these limitations by integrating predictive modeling through multi-step state predictions. Our method outperforms traditional behavior cloning numerical benchmarks, demonstrating superior robustness to distribution shift and measurement noise both in available data and during execution. Furthermore, we provide theoretical guarantees on the sample complexity and error bounds of our method, offering insights into its convergence properties.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Handover Parameter Optimization with MARL for Load Balancing in 5G Networks</title>
<link>https://arxiv.org/abs/2504.13424</link>
<guid>https://arxiv.org/abs/2504.13424</guid>
<content:encoded><![CDATA[
arXiv:2504.13424v1 Announce Type: new 
Abstract: In cellular networks, cell handover refers to the process where a device switches from one base station to another, and this mechanism is crucial for balancing the load among different cells. Traditionally, engineers would manually adjust parameters based on experience. However, the explosive growth in the number of cells has rendered manual tuning impractical. Existing research tends to overlook critical engineering details in order to simplify handover problems. In this paper, we classify cell handover into three types, and jointly model their mutual influence. To achieve load balancing, we propose a multi-agent-reinforcement-learning (MARL)-based scheme to automatically optimize the parameters. To reduce the agent interaction costs, a distributed training is implemented based on consensus approximation of global average load, and it is shown that the approximation error is bounded. Experimental results show that our proposed scheme outperforms existing benchmarks in balancing load and improving network performance.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Long Arm of Nashian Allocation in Online $p$-Mean Welfare Maximization</title>
<link>https://arxiv.org/abs/2504.13430</link>
<guid>https://arxiv.org/abs/2504.13430</guid>
<content:encoded><![CDATA[
arXiv:2504.13430v1 Announce Type: new 
Abstract: We study the online allocation of divisible items to $n$ agents with additive valuations for $p$-mean welfare maximization, a problem introduced by Barman, Khan, and Maiti~(2022). Our algorithmic and hardness results characterize the optimal competitive ratios for the entire spectrum of $-\infty \le p \le 1$. Surprisingly, our improved algorithms for all $p \le \frac{1}{\log n}$ are simply the greedy algorithm for the Nash welfare, supplemented with two auxiliary components to ensure all agents have non-zero utilities and to help a small number of agents with low utilities. In this sense, the long arm of Nashian allocation achieves near-optimal competitive ratios not only for Nash welfare but also all the way to egalitarian welfare.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trust, but verify</title>
<link>https://arxiv.org/abs/2504.13443</link>
<guid>https://arxiv.org/abs/2504.13443</guid>
<content:encoded><![CDATA[
arXiv:2504.13443v1 Announce Type: new 
Abstract: Decentralized AI agent networks, such as Gaia, allows individuals to run customized LLMs on their own computers and then provide services to the public. However, in order to maintain service quality, the network must verify that individual nodes are running their designated LLMs. In this paper, we demonstrate that in a cluster of mostly honest nodes, we can detect nodes that run unauthorized or incorrect LLM through social consensus of its peers. We will discuss the algorithm and experimental data from the Gaia network. We will also discuss the intersubjective validation system, implemented as an EigenLayer AVS to introduce financial incentives and penalties to encourage honest behavior from LLM nodes.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeVisionary: An Agent-based Framework for Evaluating Large Language Models in Code Generation</title>
<link>https://arxiv.org/abs/2504.13472</link>
<guid>https://arxiv.org/abs/2504.13472</guid>
<content:encoded><![CDATA[
arXiv:2504.13472v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated strong capabilities in code generation, underscoring the critical need for rigorous and comprehensive evaluation. Existing evaluation approaches fall into three categories, including human-centered, metric-based, and LLM-based. Considering that human-centered approaches are labour-intensive and metric-based ones overly rely on reference answers, LLM-based approaches are gaining increasing attention due to their stronger contextual understanding capabilities and superior efficiency. However, the performance of LLM-based approaches remains limited due to: (1) lack of multisource domain knowledge, and (2) insufficient comprehension of complex code.
  To mitigate the limitations, we propose CodeVisionary, the first LLM-based agent framework for evaluating LLMs in code generation. CodeVisionary consists of two stages: (1) Multiscore knowledge analysis stage, which aims to gather multisource and comprehensive domain knowledge by formulating and executing a stepwise evaluation plan. (2) Negotiation-based scoring stage, which involves multiple judges engaging in discussions to better comprehend the complex code and reach a consensus on the evaluation score. Extensive experiments demonstrate that CodeVisionary achieves the best performance for evaluating LLMs in code generation, outperforming the best baseline methods with average improvements of 0.202, 0.139, and 0.117 in Pearson, Spearman, and Kendall-Tau coefficients, respectively. Besides, CodeVisionary provides detailed evaluation reports, which assist developers in identifying shortcomings and making improvements. The resources of CodeVisionary are available at https://anonymous.4open.science/r/CodeVisionary.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Validating Network Protocol Parsers</title>
<link>https://arxiv.org/abs/2504.13515</link>
<guid>https://arxiv.org/abs/2504.13515</guid>
<content:encoded><![CDATA[
arXiv:2504.13515v1 Announce Type: new 
Abstract: Network protocol parsers are essential for enabling correct and secure communication between devices. Bugs in these parsers can introduce critical vulnerabilities, including memory corruption, information leakage, and denial-of-service attacks. An intuitive way to assess parser correctness is to compare the implementation with its official protocol standard. However, this comparison is challenging because protocol standards are typically written in natural language, whereas implementations are in source code. Existing methods like model checking, fuzzing, and differential testing have been used to find parsing bugs, but they either require significant manual effort or ignore the protocol standards, limiting their ability to detect semantic violations. To enable more automated validation of parser implementations against protocol standards, we propose PARVAL, a multi-agent framework built on large language models (LLMs). PARVAL leverages the capabilities of LLMs to understand both natural language and code. It transforms both protocol standards and their implementations into a unified intermediate representation, referred to as format specifications, and performs a differential comparison to uncover inconsistencies. We evaluate PARVAL on the Bidirectional Forwarding Detection (BFD) protocol. Our experiments demonstrate that PARVAL successfully identifies inconsistencies between the implementation and its RFC standard, achieving a low false positive rate of 5.6%. PARVAL uncovers seven unique bugs, including five previously unknown issues.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MusFlow: Multimodal Music Generation via Conditional Flow Matching</title>
<link>https://arxiv.org/abs/2504.13535</link>
<guid>https://arxiv.org/abs/2504.13535</guid>
<content:encoded><![CDATA[
arXiv:2504.13535v1 Announce Type: new 
Abstract: Music generation aims to create music segments that align with human aesthetics based on diverse conditional information. Despite advancements in generating music from specific textual descriptions (e.g., style, genre, instruments), the practical application is still hindered by ordinary users' limited expertise or time to write accurate prompts. To bridge this application gap, this paper introduces MusFlow, a novel multimodal music generation model using Conditional Flow Matching. We employ multiple Multi-Layer Perceptrons (MLPs) to align multimodal conditional information into the audio's CLAP embedding space. Conditional flow matching is trained to reconstruct the compressed Mel-spectrogram in the pretrained VAE latent space guided by aligned feature embedding. MusFlow can generate music from images, story texts, and music captions. To collect data for model training, inspired by multi-agent collaboration, we construct an intelligent data annotation workflow centered around a fine-tuned Qwen2-VL model. Using this workflow, we build a new multimodal music dataset, MMusSet, with each sample containing a quadruple of image, story text, music caption, and music piece. We conduct four sets of experiments: image-to-music, story-to-music, caption-to-music, and multimodal music generation. Experimental results demonstrate that MusFlow can generate high-quality music pieces whether the input conditions are unimodal or multimodal. We hope this work can advance the application of music generation in multimedia field, making music creation more accessible. Our generated samples, code and dataset are available at musflow.github.io.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwitchMT: An Adaptive Context Switching Methodology for Scalable Multi-Task Learning in Intelligent Autonomous Agents</title>
<link>https://arxiv.org/abs/2504.13541</link>
<guid>https://arxiv.org/abs/2504.13541</guid>
<content:encoded><![CDATA[
arXiv:2504.13541v1 Announce Type: new 
Abstract: The ability to train intelligent autonomous agents (such as mobile robots) on multiple tasks is crucial for adapting to dynamic real-world environments. However, state-of-the-art reinforcement learning (RL) methods only excel in single-task settings, and still struggle to generalize across multiple tasks due to task interference. Moreover, real-world environments also demand the agents to have data stream processing capabilities. Toward this, a state-of-the-art work employs Spiking Neural Networks (SNNs) to improve multi-task learning by exploiting temporal information in data stream, while enabling lowpower/energy event-based operations. However, it relies on fixed context/task-switching intervals during its training, hence limiting the scalability and effectiveness of multi-task learning. To address these limitations, we propose SwitchMT, a novel adaptive task-switching methodology for RL-based multi-task learning in autonomous agents. Specifically, SwitchMT employs the following key ideas: (1) a Deep Spiking Q-Network with active dendrites and dueling structure, that utilizes task-specific context signals to create specialized sub-networks; and (2) an adaptive task-switching policy that leverages both rewards and internal dynamics of the network parameters. Experimental results demonstrate that SwitchMT achieves superior performance in multi-task learning compared to state-of-the-art methods. It achieves competitive scores in multiple Atari games (i.e., Pong: -8.8, Breakout: 5.6, and Enduro: 355.2) compared to the state-of-the-art, showing its better generalized learning capability. These results highlight the effectiveness of our SwitchMT methodology in addressing task interference while enabling multi-task learning automation through adaptive task switching, thereby paving the way for more efficient generalist agents with scalable multi-task learning capabilities.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task Assignment and Exploration Optimization for Low Altitude UAV Rescue via Generative AI Enhanced Multi-agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.13554</link>
<guid>https://arxiv.org/abs/2504.13554</guid>
<content:encoded><![CDATA[
arXiv:2504.13554v1 Announce Type: new 
Abstract: Artificial Intelligence (AI)-driven convolutional neural networks enhance rescue, inspection, and surveillance tasks performed by low-altitude uncrewed aerial vehicles (UAVs) and ground computing nodes (GCNs) in unknown environments. However, their high computational demands often exceed a single UAV's capacity, leading to system instability, further exacerbated by the limited and dynamic resources of GCNs. To address these challenges, this paper proposes a novel cooperation framework involving UAVs, ground-embedded robots (GERs), and high-altitude platforms (HAPs), which enable resource pooling through UAV-to-GER (U2G) and UAV-to-HAP (U2H) communications to provide computing services for UAV offloaded tasks. Specifically, we formulate the multi-objective optimization problem of task assignment and exploration optimization in UAVs as a dynamic long-term optimization problem. Our objective is to minimize task completion time and energy consumption while ensuring system stability over time. To achieve this, we first employ the Lyapunov optimization technique to transform the original problem, with stability constraints, into a per-slot deterministic problem. We then propose an algorithm named HG-MADDPG, which combines the Hungarian algorithm with a generative diffusion model (GDM)-based multi-agent deep deterministic policy gradient (MADDPG) approach. We first introduce the Hungarian algorithm as a method for exploration area selection, enhancing UAV efficiency in interacting with the environment. We then innovatively integrate the GDM and multi-agent deep deterministic policy gradient (MADDPG) to optimize task assignment decisions, such as task offloading and resource allocation. Simulation results demonstrate the effectiveness of the proposed approach, with significant improvements in task offloading efficiency, latency reduction, and system stability compared to baseline methods.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAAM: A Lightweight Multi-Agent Aggregation Module for Efficient Image Classification Based on the MindSpore Framework</title>
<link>https://arxiv.org/abs/2504.13574</link>
<guid>https://arxiv.org/abs/2504.13574</guid>
<content:encoded><![CDATA[
arXiv:2504.13574v1 Announce Type: new 
Abstract: The demand for lightweight models in image classification tasks under resource-constrained environments necessitates a balance between computational efficiency and robust feature representation. Traditional attention mechanisms, despite their strong feature modeling capability, often struggle with high computational complexity and structural rigidity, limiting their applicability in scenarios with limited computational resources (e.g., edge devices or real-time systems). To address this, we propose the Multi-Agent Aggregation Module (MAAM), a lightweight attention architecture integrated with the MindSpore framework. MAAM employs three parallel agent branches with independently parameterized operations to extract heterogeneous features, adaptively fused via learnable scalar weights, and refined through a convolutional compression layer. Leveraging MindSpore's dynamic computational graph and operator fusion, MAAM achieves 87.0% accuracy on the CIFAR-10 dataset, significantly outperforming conventional CNN (58.3%) and MLP (49.6%) models, while improving training efficiency by 30%. Ablation studies confirm the critical role of agent attention (accuracy drops to 32.0% if removed) and compression modules (25.5% if omitted), validating their necessity for maintaining discriminative feature learning. The framework's hardware acceleration capabilities and minimal memory footprint further demonstrate its practicality, offering a deployable solution for image classification in resource-constrained scenarios without compromising accuracy.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LMPOcc: 3D Semantic Occupancy Prediction Utilizing Long-Term Memory Prior from Historical Traversals</title>
<link>https://arxiv.org/abs/2504.13596</link>
<guid>https://arxiv.org/abs/2504.13596</guid>
<content:encoded><![CDATA[
arXiv:2504.13596v1 Announce Type: new 
Abstract: Vision-based 3D semantic occupancy prediction is critical for autonomous driving, enabling unified modeling of static infrastructure and dynamic agents. In practice, autonomous vehicles may repeatedly traverse identical geographic locations under varying environmental conditions, such as weather fluctuations and illumination changes. Existing methods in 3D occupancy prediction predominantly integrate adjacent temporal contexts. However, these works neglect to leverage perceptual information, which is acquired from historical traversals of identical geographic locations. In this paper, we propose Longterm Memory Prior Occupancy (LMPOcc), the first 3D occupancy prediction methodology that exploits long-term memory priors derived from historical traversal perceptual outputs. We introduce a plug-and-play architecture that integrates long-term memory priors to enhance local perception while simultaneously constructing global occupancy representations. To adaptively aggregate prior features and current features, we develop an efficient lightweight Current-Prior Fusion module. Moreover, we propose a model-agnostic prior format to ensure compatibility across diverse occupancy prediction baselines. LMPOcc achieves state-of-the-art performance validated on the Occ3D-nuScenes benchmark, especially on static semantic categories. Additionally, experimental results demonstrate LMPOcc's ability to construct global occupancy through multi-vehicle crowdsourcing.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Humanoid Walking on Compliant and Uneven Terrain with Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.13619</link>
<guid>https://arxiv.org/abs/2504.13619</guid>
<content:encoded><![CDATA[
arXiv:2504.13619v1 Announce Type: new 
Abstract: For the deployment of legged robots in real-world environments, it is essential to develop robust locomotion control methods for challenging terrains that may exhibit unexpected deformability and irregularity. In this paper, we explore the application of sim-to-real deep reinforcement learning (RL) for the design of bipedal locomotion controllers for humanoid robots on compliant and uneven terrains. Our key contribution is to show that a simple training curriculum for exposing the RL agent to randomized terrains in simulation can achieve robust walking on a real humanoid robot using only proprioceptive feedback. We train an end-to-end bipedal locomotion policy using the proposed approach, and show extensive real-robot demonstration on the HRP-5P humanoid over several difficult terrains inside and outside the lab environment. Further, we argue that the robustness of a bipedal walking policy can be improved if the robot is allowed to exhibit aperiodic motion with variable stepping frequency. We propose a new control policy to enable modification of the observed clock signal, leading to adaptive gait frequencies depending on the terrain and command velocity. Through simulation experiments, we show the effectiveness of this policy specifically for walking over challenging terrains by controlling swing and stance durations. The code for training and evaluation is available online at https://github.com/rohanpsingh/LearningHumanoidWalking. Demo video is available at https://www.youtube.com/watch?v=ZgfNzGAkk2Q.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Before Planning: Constructing Intrinsic User World Model for User-Tailored Dialogue Policy Planning</title>
<link>https://arxiv.org/abs/2504.13643</link>
<guid>https://arxiv.org/abs/2504.13643</guid>
<content:encoded><![CDATA[
arXiv:2504.13643v1 Announce Type: new 
Abstract: Recent advancements in dialogue policy planning have emphasized optimizing system agent policies to achieve predefined goals, focusing on strategy design, trajectory acquisition, and efficient training paradigms. However, these approaches often overlook the critical role of user characteristics, which are essential in real-world scenarios like conversational search and recommendation, where interactions must adapt to individual user traits such as personality, preferences, and goals. To address this gap, we first conduct a comprehensive study utilizing task-specific user personas to systematically assess dialogue policy planning under diverse user behaviors. By leveraging realistic user profiles for different tasks, our study reveals significant limitations in existing approaches, highlighting the need for user-tailored dialogue policy planning. Building on this foundation, we present the User-Tailored Dialogue Policy Planning (UDP) framework, which incorporates an Intrinsic User World Model to model user traits and feedback. UDP operates in three stages: (1) User Persona Portraying, using a diffusion model to dynamically infer user profiles; (2) User Feedback Anticipating, leveraging a Brownian Bridge-inspired anticipator to predict user reactions; and (3) User-Tailored Policy Planning, integrating these insights to optimize response strategies. To ensure robust performance, we further propose an active learning approach that prioritizes challenging user personas during training. Comprehensive experiments on benchmarks, including collaborative and non-collaborative settings, demonstrate the effectiveness of UDP in learning user-specific dialogue strategies. Results validate the protocol's utility and highlight UDP's robustness, adaptability, and potential to advance user-centric dialogue systems.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EyecareGPT: Boosting Comprehensive Ophthalmology Understanding with Tailored Dataset, Benchmark and Model</title>
<link>https://arxiv.org/abs/2504.13650</link>
<guid>https://arxiv.org/abs/2504.13650</guid>
<content:encoded><![CDATA[
arXiv:2504.13650v1 Announce Type: new 
Abstract: Medical Large Vision-Language Models (Med-LVLMs) demonstrate significant potential in healthcare, but their reliance on general medical data and coarse-grained global visual understanding limits them in intelligent ophthalmic diagnosis. Currently, intelligent ophthalmic diagnosis faces three major challenges: (i) Data. The lack of deeply annotated, high-quality, multi-modal ophthalmic visual instruction data; (ii) Benchmark. The absence of a comprehensive and systematic benchmark for evaluating diagnostic performance; (iii) Model. The difficulty of adapting holistic visual architectures to fine-grained, region-specific ophthalmic lesion identification. In this paper, we propose the Eyecare Kit, which systematically tackles the aforementioned three key challenges with the tailored dataset, benchmark and model: First, we construct a multi-agent data engine with real-life ophthalmology data to produce Eyecare-100K, a high-quality ophthalmic visual instruction dataset. Subsequently, we design Eyecare-Bench, a benchmark that comprehensively evaluates the overall performance of LVLMs on intelligent ophthalmic diagnosis tasks across multiple dimensions. Finally, we develop the EyecareGPT, optimized for fine-grained ophthalmic visual understanding thoroughly, which incorporates an adaptive resolution mechanism and a layer-wise dense connector. Extensive experimental results indicate that the EyecareGPT achieves state-of-the-art performance in a range of ophthalmic tasks, underscoring its significant potential for the advancement of open research in intelligent ophthalmic diagnosis. Our project is available at https://github.com/DCDmllm/EyecareGPT.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation</title>
<link>https://arxiv.org/abs/2504.13707</link>
<guid>https://arxiv.org/abs/2504.13707</guid>
<content:encoded><![CDATA[
arXiv:2504.13707v1 Announce Type: new 
Abstract: As the general capabilities of large language models (LLMs) improve and agent applications become more widespread, the underlying deception risks urgently require systematic evaluation and effective oversight. Unlike existing evaluation which uses simulated games or presents limited choices, we introduce OpenDeception, a novel deception evaluation framework with an open-ended scenario dataset. OpenDeception jointly evaluates both the deception intention and capabilities of LLM-based agents by inspecting their internal reasoning process. Specifically, we construct five types of common use cases where LLMs intensively interact with the user, each consisting of ten diverse, concrete scenarios from the real world. To avoid ethical concerns and costs of high-risk deceptive interactions with human testers, we propose to simulate the multi-turn dialogue via agent simulation. Extensive evaluation of eleven mainstream LLMs on OpenDeception highlights the urgent need to address deception risks and security concerns in LLM-based agents: the deception intention ratio across the models exceeds 80%, while the deception success rate surpasses 50%. Furthermore, we observe that LLMs with stronger capabilities do exhibit a higher risk of deception, which calls for more alignment efforts on inhibiting deceptive behaviors.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of Black-box Large Language Models</title>
<link>https://arxiv.org/abs/2504.13775</link>
<guid>https://arxiv.org/abs/2504.13775</guid>
<content:encoded><![CDATA[
arXiv:2504.13775v1 Announce Type: new 
Abstract: Previous insertion-based and paraphrase-based backdoors have achieved great success in attack efficacy, but they ignore the text quality and semantic consistency between poisoned and clean texts. Although recent studies introduce LLMs to generate poisoned texts and improve the stealthiness, semantic consistency, and text quality, their hand-crafted prompts rely on expert experiences, facing significant challenges in prompt adaptability and attack performance after defenses. In this paper, we propose a novel backdoor attack based on adaptive optimization mechanism of black-box large language models (BadApex), which leverages a black-box LLM to generate poisoned text through a refined prompt. Specifically, an Adaptive Optimization Mechanism is designed to refine an initial prompt iteratively using the generation and modification agents. The generation agent generates the poisoned text based on the initial prompt. Then the modification agent evaluates the quality of the poisoned text and refines a new prompt. After several iterations of the above process, the refined prompt is used to generate poisoned texts through LLMs. We conduct extensive experiments on three dataset with six backdoor attacks and two defenses. Extensive experimental results demonstrate that BadApex significantly outperforms state-of-the-art attacks. It improves prompt adaptability, semantic consistency, and text quality. Furthermore, when two defense methods are applied, the average attack success rate (ASR) still up to 96.75%.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatNekoHacker: Real-Time Fan Engagement with Conversational Agents</title>
<link>https://arxiv.org/abs/2504.13793</link>
<guid>https://arxiv.org/abs/2504.13793</guid>
<content:encoded><![CDATA[
arXiv:2504.13793v1 Announce Type: new 
Abstract: ChatNekoHacker is a real-time conversational agent system that strengthens fan engagement for musicians. It integrates Amazon Bedrock Agents for autonomous dialogue, Unity for immersive 3D livestream sets, and VOICEVOX for high quality Japanese text-to-speech, enabling two virtual personas to represent the music duo Neko Hacker. In a one-hour YouTube Live with 30 participants, we evaluated the impact of the system. Regression analysis showed that agent interaction significantly elevated fan interest, with perceived fun as the dominant predictor. The participants also expressed a stronger intention to listen to the duo's music and attend future concerts. These findings highlight entertaining, interactive broadcasts as pivotal to cultivating fandom. Our work offers actionable insights for the deployment of conversational agents in entertainment while pointing to next steps: broader response diversity, lower latency, and tighter fact-checking to curb potential misinformation.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LearnAct: Few-Shot Mobile GUI Agent with a Unified Demonstration Benchmark</title>
<link>https://arxiv.org/abs/2504.13805</link>
<guid>https://arxiv.org/abs/2504.13805</guid>
<content:encoded><![CDATA[
arXiv:2504.13805v1 Announce Type: new 
Abstract: Mobile GUI agents show promise in automating tasks but face generalization challenges in diverse real-world scenarios. Traditional approaches using pre-training or fine-tuning with massive datasets struggle with the diversity of mobile applications and user-specific tasks. We propose enhancing mobile GUI agent capabilities through human demonstrations, focusing on improving performance in unseen scenarios rather than pursuing universal generalization through larger datasets. To realize this paradigm, we introduce LearnGUI, the first comprehensive dataset specifically designed for studying demonstration-based learning in mobile GUI agents, comprising 2,252 offline tasks and 101 online tasks with high-quality human demonstrations. We further develop LearnAct, a sophisticated multi-agent framework that automatically extracts knowledge from demonstrations to enhance task completion. This framework integrates three specialized agents: DemoParser for knowledge extraction, KnowSeeker for relevant knowledge retrieval, and ActExecutor for demonstration-enhanced task execution. Our experimental results show significant performance gains in both offline and online evaluations. In offline assessments, a single demonstration improves model performance, increasing Gemini-1.5-Pro's accuracy from 19.3% to 51.7%. In online evaluations, our framework enhances UI-TARS-7B-SFT's task success rate from 18.1% to 32.8%. LearnAct framework and LearnGUI benchmark establish demonstration-based learning as a promising direction for more adaptable, personalized, and deployable mobile GUI agents.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Science Hierarchography: Hierarchical Organization of Science Literature</title>
<link>https://arxiv.org/abs/2504.13834</link>
<guid>https://arxiv.org/abs/2504.13834</guid>
<content:encoded><![CDATA[
arXiv:2504.13834v1 Announce Type: new 
Abstract: Scientific knowledge is growing rapidly, making it challenging to track progress and high-level conceptual links across broad disciplines. While existing tools like citation networks and search engines make it easy to access a few related papers, they fundamentally lack the flexible abstraction needed to represent the density of activity in various scientific subfields. We motivate SCIENCE HIERARCHOGRAPHY, the goal of organizing scientific literature into a high-quality hierarchical structure that allows for the categorization of scientific work across varying levels of abstraction, from very broad fields to very specific studies. Such a representation can provide insights into which fields are well-explored and which are under-explored. To achieve the goals of SCIENCE HIERARCHOGRAPHY, we develop a range of algorithms. Our primary approach combines fast embedding-based clustering with LLM-based prompting to balance the computational efficiency of embedding methods with the semantic precision offered by LLM prompting. We demonstrate that this approach offers the best trade-off between quality and speed compared to methods that heavily rely on LLM prompting, such as iterative tree construction with LLMs. To better reflect the interdisciplinary and multifaceted nature of research papers, our hierarchy captures multiple dimensions of categorization beyond simple topic labels. We evaluate the utility of our framework by assessing how effectively an LLM-based agent can locate target papers using the hierarchy. Results show that this structured approach enhances interpretability, supports trend discovery, and offers an alternative pathway for exploring scientific literature beyond traditional search methods. Code, data and demo: $\href{https://github.com/JHU-CLSP/science-hierarchography}{https://github.com/JHU-CLSP/science-hierarchography}$
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroNAS: Enhancing Efficiency of Neuromorphic In-Memory Computing for Intelligent Mobile Agents through Hardware-Aware Spiking Neural Architecture Search</title>
<link>https://arxiv.org/abs/2407.00641</link>
<guid>https://arxiv.org/abs/2407.00641</guid>
<content:encoded><![CDATA[
arXiv:2407.00641v3 Announce Type: replace 
Abstract: Intelligent mobile agents (e.g., UGVs and UAVs) typically demand low power/energy consumption when solving their machine learning (ML)-based tasks, since they are usually powered by portable batteries with limited capacity. A potential solution is employing neuromorphic computing with Spiking Neural Networks (SNNs), which leverages event-based computation to enable ultra-low power/energy ML algorithms. To maximize the performance efficiency of SNN inference, the In-Memory Computing (IMC)-based hardware accelerators with emerging device technologies (e.g., RRAM) can be employed. However, SNN models are typically developed without considering constraints from the application and the underlying IMC hardware, thereby hindering SNNs from reaching their full potential in performance and efficiency. To address this, we propose NeuroNAS, a novel framework for developing energyefficient neuromorphic IMC for intelligent mobile agents using hardware-aware spiking neural architecture search (NAS), i.e., by quickly finding an SNN architecture that offers high accuracy under the given constraints (e.g., memory, area, latency, and energy consumption). Its key steps include: optimizing SNN operations to enable efficient NAS, employing quantization to minimize the memory footprint, developing an SNN architecture that facilitates an effective learning, and devising a systematic hardware-aware search algorithm to meet the constraints. Compared to the state-of-the-art techniques, NeuroNAS quickly finds SNN architectures (with 8bit weight precision) that maintain high accuracy by up to 6.6x search time speed-ups, while achieving up to 92% area savings, 1.2x latency improvements, 84% energy savings across different datasets (i.e., CIFAR-10, CIFAR-100, and TinyImageNet-200); while the state-of-the-art fail to meet all constraints at once.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind</title>
<link>https://arxiv.org/abs/2408.12022</link>
<guid>https://arxiv.org/abs/2408.12022</guid>
<content:encoded><![CDATA[
arXiv:2408.12022v2 Announce Type: replace 
Abstract: How do people understand and evaluate claims about others' beliefs, even though these beliefs cannot be directly observed? In this paper, we introduce a cognitive model of epistemic language interpretation, grounded in Bayesian inferences about other agents' goals, beliefs, and intentions: a language-augmented Bayesian theory-of-mind (LaBToM). By translating natural language into an epistemic ``language-of-thought'' with grammar-constrained LLM decoding, then evaluating these translations against the inferences produced by inverting a generative model of rational action and perception, LaBToM captures graded plausibility judgments of epistemic claims. We validate our model in an experiment where participants watch an agent navigate a maze to find keys hidden in boxes needed to reach their goal, then rate sentences about the agent's beliefs. In contrast with multimodal LLMs (GPT-4o, Gemini Pro) and ablated models, our model correlates highly with human judgments for a wide range of expressions, including modal language, uncertainty expressions, knowledge claims, likelihood comparisons, and attributions of false belief.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Friedkin-Johnsen Model with Diminishing Competition</title>
<link>https://arxiv.org/abs/2409.12601</link>
<guid>https://arxiv.org/abs/2409.12601</guid>
<content:encoded><![CDATA[
arXiv:2409.12601v4 Announce Type: replace 
Abstract: This letter studies the Friedkin-Johnsen (FJ) model with diminishing competition, or stubbornness. The original FJ model assumes that each agent assigns a constant competition weight to its initial opinion. In contrast, we investigate the effect of diminishing competition on the convergence point and speed of the FJ dynamics. We prove that, if the competition is uniform across agents and vanishes asymptotically, the convergence point coincides with the nominal consensus reached with no competition. However, the diminishing competition slows down convergence according to its own rate of decay. We study this phenomenon analytically and provide upper and lower bounds on the convergence rate. Further, if competition is not uniform across agents, we show that the convergence point may not coincide with the nominal consensus point. Finally, we evaluate our analytical insights numerically.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents</title>
<link>https://arxiv.org/abs/2410.09024</link>
<guid>https://arxiv.org/abs/2410.09024</guid>
<content:encoded><![CDATA[
arXiv:2410.09024v3 Announce Type: replace 
Abstract: The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which use external tools and can execute multi-stage tasks -- may pose a greater risk if misused, but their robustness remains underexplored. To facilitate research on LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment. In addition to measuring whether models refuse harmful agentic requests, scoring well on AgentHarm requires jailbroken agents to maintain their capabilities following an attack to complete a multi-step task. We evaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly compliant with malicious agent requests without jailbreaking, (2) simple universal jailbreak templates can be adapted to effectively jailbreak agents, and (3) these jailbreaks enable coherent and malicious multi-step agent behavior and retain model capabilities. To enable simple and reliable evaluation of attacks and defenses for LLM-based agents, we publicly release AgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dynamic Safety Shield for Safe and Efficient Reinforcement Learning of Navigation Tasks</title>
<link>https://arxiv.org/abs/2412.04153</link>
<guid>https://arxiv.org/abs/2412.04153</guid>
<content:encoded><![CDATA[
arXiv:2412.04153v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) has been successfully applied to a variety of robotics applications, where it outperforms classical methods. However, the safety aspect of RL and the transfer to the real world remain an open challenge. A prominent field for tackling this challenge and ensuring the safety of the agents during training and execution is safe reinforcement learning. Safe RL can be achieved through constrained RL and safe exploration approaches. The former learns the safety constraints over the course of training to achieve a safe behavior by the end of training, at the cost of high number of collisions at earlier stages of the training. The latter offers robust safety by enforcing the safety constraints as hard constraints, which prevents collisions but hinders the exploration of the RL agent, resulting in lower rewards and poor performance. To overcome those drawbacks, we propose a novel safety shield, that combines the robustness of the optimization-based controllers with the long prediction capabilities of the RL agents, allowing the RL agent to adaptively tune the parameters of the controller. Our approach is able to improve the exploration of the RL agents for navigation tasks, while minimizing the number of collisions. Experiments in simulation show that our approach outperforms state-of-the-art baselines in the reached goals-to-collisions ratio in different challenging environments. The goals-to-collisions ratio metrics emphasizes the importance of minimizing the number of collisions, while learning to accomplish the task. Our approach achieves a higher number of reached goals compared to the classic safety shields and fewer collisions compared to constrained RL approaches. Finally, we demonstrate the performance of the proposed method in a real-world experiment.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentCF++: Memory-enhanced LLM-based Agents for Popularity-aware Cross-domain Recommendations</title>
<link>https://arxiv.org/abs/2502.13843</link>
<guid>https://arxiv.org/abs/2502.13843</guid>
<content:encoded><![CDATA[
arXiv:2502.13843v2 Announce Type: replace 
Abstract: LLM-based user agents, which simulate user interaction behavior, are emerging as a promising approach to enhancing recommender systems. In real-world scenarios, users' interactions often exhibit cross-domain characteristics and are influenced by others. However, the memory design in current methods causes user agents to introduce significant irrelevant information during decision-making in cross-domain scenarios and makes them unable to recognize the influence of other users' interactions, such as popularity factors. To tackle this issue, we propose a dual-layer memory architecture combined with a two-step fusion mechanism. This design avoids irrelevant information during decision-making while ensuring effective integration of cross-domain preferences. We also introduce the concepts of interest groups and group-shared memory to better capture the influence of popularity factors on users with similar interests. Comprehensive experiments validate the effectiveness of AgentCF++. Our code is available at https://github.com/jhliu0807/AgentCF-plus.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning with Graph Attention for Routing and Wavelength Assignment with Lightpath Reuse</title>
<link>https://arxiv.org/abs/2502.14741</link>
<guid>https://arxiv.org/abs/2502.14741</guid>
<content:encoded><![CDATA[
arXiv:2502.14741v2 Announce Type: replace 
Abstract: Many works have investigated reinforcement learning (RL) for routing and spectrum assignment on flex-grid networks but only one work to date has examined RL for fixed-grid with flex-rate transponders, despite production systems using this paradigm. Flex-rate transponders allow existing lightpaths to accommodate new services, a task we term routing and wavelength assignment with lightpath reuse (RWA-LR). We re-examine this problem and present a thorough benchmarking of heuristic algorithms for RWA-LR, which are shown to have 6% increased throughput when candidate paths are ordered by number of hops, rather than total length. We train an RL agent for RWA-LR with graph attention networks for the policy and value functions to exploit the graph-structured data. We provide details of our methodology and open source all of our code for reproduction. We outperform the previous state-of-the-art RL approach by 2.5% (17.4 Tbps mean additional throughput) and the best heuristic by 1.2% (8.5 Tbps mean additional throughput). This marginal gain highlights the difficulty in learning effective RL policies on long horizon resource allocation tasks.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Reward-Free Reinforcement Learning Framework for Vehicle Control</title>
<link>https://arxiv.org/abs/2502.15262</link>
<guid>https://arxiv.org/abs/2502.15262</guid>
<content:encoded><![CDATA[
arXiv:2502.15262v2 Announce Type: replace 
Abstract: Reinforcement learning plays a crucial role in vehicle control by guiding agents to learn optimal control strategies through designing or learning appropriate reward signals. However, in vehicle control applications, rewards typically need to be manually designed while considering multiple implicit factors, which easily introduces human biases. Although imitation learning methods does not rely on explicit reward signals, they necessitate high-quality expert actions, which are often challenging to acquire. To address these issues, we propose a reward-free reinforcement learning framework (RFRLF). This framework directly learns the target states to optimize agent behavior through a target state prediction network (TSPN) and a reward-free state-guided policy network (RFSGPN), avoiding the dependence on manually designed reward signals. Specifically, the policy network is learned via minimizing the differences between the predicted state and the expert state. Experimental results demonstrate the effectiveness of the proposed RFRLF in controlling vehicle driving, showing its advantages in improving learning efficiency and adapting to reward-free environments.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximum Welfare Allocations under Quantile Valuations</title>
<link>https://arxiv.org/abs/2502.17869</link>
<guid>https://arxiv.org/abs/2502.17869</guid>
<content:encoded><![CDATA[
arXiv:2502.17869v2 Announce Type: replace 
Abstract: We propose a new model for aggregating preferences over a set of indivisible items based on a quantile value. In this model, each agent is endowed with a specific quantile, and the value of a given bundle is defined by the corresponding quantile of the individual values of the items within it. Our model captures the diverse ways in which agents may perceive a bundle, even when they agree on the values of individual items. It enables richer behavioral modeling that cannot be easily captured by additive valuation functions. We study the problem of maximizing utilitarian and egalitarian welfare within the quantile-based valuation setting. For each of the welfare functions, we analyze the complexity of the objectives. Interestingly, our results show that the complexity of both objectives varies significantly depending on whether the allocation is required to be balanced. We provide near-optimal approximation algorithms for utilitarian welfare, and for egalitarian welfare, we present exact algorithms whenever possible.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Athenian Academy: A Seven-Layer Architecture Model for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2504.12735</link>
<guid>https://arxiv.org/abs/2504.12735</guid>
<content:encoded><![CDATA[
arXiv:2504.12735v2 Announce Type: replace 
Abstract: This paper proposes the "Academy of Athens" multi-agent seven-layer framework, aimed at systematically addressing challenges in multi-agent systems (MAS) within artificial intelligence (AI) art creation, such as collaboration efficiency, role allocation, environmental adaptation, and task parallelism. The framework divides MAS into seven layers: multi-agent collaboration, single-agent multi-role playing, single-agent multi-scene traversal, single-agent multi-capability incarnation, different single agents using the same large model to achieve the same target agent, single-agent using different large models to achieve the same target agent, and multi-agent synthesis of the same target agent. Through experimental validation in art creation, the framework demonstrates its unique advantages in task collaboration, cross-scene adaptation, and model fusion. This paper further discusses current challenges such as collaboration mechanism optimization, model stability, and system security, proposing future exploration through technologies like meta-learning and federated learning. The framework provides a structured methodology for multi-agent collaboration in AI art creation and promotes innovative applications in the art field.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Customizing Emotional Support: How Do Individuals Construct and Interact With LLM-Powered Chatbots</title>
<link>https://arxiv.org/abs/2504.12943</link>
<guid>https://arxiv.org/abs/2504.12943</guid>
<content:encoded><![CDATA[
arXiv:2504.12943v2 Announce Type: replace 
Abstract: Personalized support is essential to fulfill individuals' emotional needs and sustain their mental well-being. Large language models (LLMs), with great customization flexibility, hold promises to enable individuals to create their own emotional support agents. In this work, we developed ChatLab, where users could construct LLM-powered chatbots with additional interaction features including voices and avatars. Using a Research through Design approach, we conducted a week-long field study followed by interviews and design activities (N = 22), which uncovered how participants created diverse chatbot personas for emotional reliance, confronting stressors, connecting to intellectual discourse, reflecting mirrored selves, etc. We found that participants actively enriched the personas they constructed, shaping the dynamics between themselves and the chatbot to foster open and honest conversations. They also suggested other customizable features, such as integrating online activities and adjustable memory settings. Based on these findings, we discuss opportunities for enhancing personalized emotional support through emerging AI technologies.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical Inference in Reinforcement Learning: A Selective Survey</title>
<link>https://arxiv.org/abs/2502.16195</link>
<guid>https://arxiv.org/abs/2502.16195</guid>
<content:encoded><![CDATA[
arXiv:2502.16195v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) is concerned with how intelligence agents take actions in a given environment to maximize the cumulative reward they receive. In healthcare, applying RL algorithms could assist patients in improving their health status. In ride-sharing platforms, applying RL algorithms could increase drivers' income and customer satisfaction. For large language models, applying RL algorithms could align their outputs with human preferences. Over the past decade, RL has been arguably one of the most vibrant research frontiers in machine learning. Nevertheless, statistics as a field, as opposed to computer science, has only recently begun to engage with RL both in depth and in breadth. This chapter presents a selective review of statistical inferential tools for RL, covering both hypothesis testing and confidence interval construction. Our goal is to highlight the value of statistical inference in RL for both the statistics and machine learning communities, and to promote the broader application of classical statistical inference tools in this vibrant area of research.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latency-Aware 2-Opt Monotonic Local Search for Distributed Constraint Optimization</title>
<link>https://arxiv.org/abs/2504.08737</link>
<guid>https://arxiv.org/abs/2504.08737</guid>
<content:encoded><![CDATA[
<div> : CA-DCOPsLAMDLS-2

:
Latency-Aware Monotonic Distributed Local Search-2 (LAMDLS-2)Communication-Aware DCOPs1-optCA-DCOPLAMDLS-22-optLAMDLS-2MGM-22-opt <div>
arXiv:2504.08737v1 Announce Type: new 
Abstract: Researchers recently extended Distributed Constraint Optimization Problems (DCOPs) to Communication-Aware DCOPs so that they are applicable in scenarios in which messages can be arbitrarily delayed. Distributed asynchronous local search and inference algorithms designed for CA-DCOPs are less vulnerable to message latency than their counterparts for regular DCOPs. However, unlike local search algorithms for (regular) DCOPs that converge to k-opt solutions (with k > 1), that is, they converge to solutions that cannot be improved by a group of k agents), local search CA-DCOP algorithms are limited to 1-opt solutions only. In this paper, we introduce Latency-Aware Monotonic Distributed Local Search-2 (LAMDLS-2), where agents form pairs and coordinate bilateral assignment replacements. LAMDLS-2 is monotonic, converges to a 2-opt solution, and is also robust to message latency, making it suitable for CA-DCOPs. Our results indicate that LAMDLS-2 converges faster than MGM-2, a benchmark algorithm, to a similar 2-opt solution, in various message latency scenarios.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Product Search Interfaces with Sketch-Guided Diffusion and Language Agents</title>
<link>https://arxiv.org/abs/2504.08739</link>
<guid>https://arxiv.org/abs/2504.08739</guid>
<content:encoded><![CDATA[
<div> : Sketch-Search Agent

<br /><br />:
Sketch-Search AgentT2I-AdapterSketch-Search AgentCLIPSketch-Search AgentSketch-Search Agent <div>
arXiv:2504.08739v1 Announce Type: new 
Abstract: The rapid progress in diffusion models, transformers, and language agents has unlocked new possibilities, yet their potential in user interfaces and commercial applications remains underexplored. We present Sketch-Search Agent, a novel framework that transforms the image search experience by integrating a multimodal language agent with freehand sketches as control signals for diffusion models. Using the T2I-Adapter, Sketch-Search Agent combines sketches and text prompts to generate high-quality query images, encoded via a CLIP image encoder for efficient matching against an image corpus. Unlike existing methods, Sketch-Search Agent requires minimal setup, no additional training, and excels in sketch-based image retrieval and natural language interactions. The multimodal agent enhances user experience by dynamically retaining preferences, ranking results, and refining queries for personalized recommendations. This interactive design empowers users to create sketches and receive tailored product suggestions, showcasing the potential of diffusion models in user-centric image retrieval. Experiments confirm Sketch-Search Agent's high accuracy in delivering relevant product search results.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Filter Bubble on Short-video Recommender System with Large Language Model Agents</title>
<link>https://arxiv.org/abs/2504.08742</link>
<guid>https://arxiv.org/abs/2504.08742</guid>
<content:encoded><![CDATA[
<div> : filter bubbles, TikTok, recommendation algorithms, large language models (LLMs), intervention strategies

<br /><br />:
TikTokfilter bubblesLLM <div>
arXiv:2504.08742v1 Announce Type: new 
Abstract: An increasing reliance on recommender systems has led to concerns about the creation of filter bubbles on social media, especially on short video platforms like TikTok. However, their formation is still not entirely understood due to the complex dynamics between recommendation algorithms and user feedback. In this paper, we aim to shed light on these dynamics using a large language model-based simulation framework. Our work employs real-world short-video data containing rich video content information and detailed user-agents to realistically simulate the recommendation-feedback cycle. Through large-scale simulations, we demonstrate that LLMs can replicate real-world user-recommender interactions, uncovering key mechanisms driving filter bubble formation. We identify critical factors, such as demographic features and category attraction that exacerbate content homogenization. To mitigate this, we design and test interventions including various cold-start and feedback weighting strategies, showing measurable reductions in filter bubble effects. Our framework enables rapid prototyping of recommendation strategies, offering actionable solutions to enhance content diversity in real-world systems. Furthermore, we analyze how LLM-inherent biases may propagate through recommendations, proposing safeguards to promote equity for vulnerable groups, such as women and low-income populations. By examining the interplay between recommendation and LLM agents, this work advances a deeper understanding of algorithmic bias and provides practical tools to promote inclusive digital spaces.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GridMind: A Multi-Agent NLP Framework for Unified, Cross-Modal NFL Data Insights</title>
<link>https://arxiv.org/abs/2504.08747</link>
<guid>https://arxiv.org/abs/2504.08747</guid>
<content:encoded><![CDATA[
<div> GridMind

<br /><br />:
GridMindRAGLLMsNFLGridMind <div>
arXiv:2504.08747v1 Announce Type: new 
Abstract: The rapid growth of big data and advancements in computational techniques have significantly transformed sports analytics. However, the diverse range of data sources -- including structured statistics, semi-structured formats like sensor data, and unstructured media such as written articles, audio, and video -- creates substantial challenges in extracting actionable insights. These various formats, often referred to as multimodal data, require integration to fully leverage their potential. Conventional systems, which typically prioritize structured data, face limitations when processing and combining these diverse content types, reducing their effectiveness in real-time sports analysis.
  To address these challenges, recent research highlights the importance of multimodal data integration for capturing the complexity of real-world sports environments. Building on this foundation, this paper introduces GridMind, a multi-agent framework that unifies structured, semi-structured, and unstructured data through Retrieval-Augmented Generation (RAG) and large language models (LLMs) to facilitate natural language querying of NFL data. This approach aligns with the evolving field of multimodal representation learning, where unified models are increasingly essential for real-time, cross-modal interactions.
  GridMind's distributed architecture includes specialized agents that autonomously manage each stage of a prompt -- from interpretation and data retrieval to response synthesis. This modular design enables flexible, scalable handling of multimodal data, allowing users to pose complex, context-rich questions and receive comprehensive, intuitive responses via a conversational interface.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Personalized Conversational Sales Agents with Contextual User Profiling for Strategic Action</title>
<link>https://arxiv.org/abs/2504.08754</link>
<guid>https://arxiv.org/abs/2504.08754</guid>
<content:encoded><![CDATA[
<div> Conversational Recommender Systems (CRSs)Conversational Sales (CSales)CSUserLLM-based user simulatorCSI

<br /><br />:
Conversational Sales (CSales)CSalesCSUserCSICSUser <div>
arXiv:2504.08754v1 Announce Type: new 
Abstract: Conversational Recommender Systems (CRSs) aim to engage users in dialogue to provide tailored recommendations. While traditional CRSs focus on eliciting preferences and retrieving items, real-world e-commerce interactions involve more complex decision-making, where users consider multiple factors beyond simple attributes. To bridge this gap, we introduce Conversational Sales (CSales), a novel task that unifies preference elicitation, recommendation, and persuasion to better support user decision-making. For a realistic evaluation of CSales, we present CSUser, an LLM-based user simulator constructed from real-world data, modeling diverse user profiles with needs and personalities. Additionally, we propose CSI, a conversational sales agent that proactively infers contextual profiles through dialogue for personalized action planning. Extensive experiments demonstrate that CSUser effectively replicates real-world users and emphasize the importance of contextual profiling for strategic action selection, ultimately driving successful purchases in e-commerce.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PriM: Principle-Inspired Material Discovery through Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2504.08810</link>
<guid>https://arxiv.org/abs/2504.08810</guid>
<content:encoded><![CDATA[
<div> : <br /><br />: PriMPriMPriMGitHub <div>
arXiv:2504.08810v1 Announce Type: new 
Abstract: Complex chemical space and limited knowledge scope with biases holds immense challenge for human scientists, yet in automated materials discovery. Existing intelligent methods relies more on numerical computation, leading to inefficient exploration and results with hard-interpretability. To bridge this gap, we introduce a principles-guided material discovery system powered by language inferential multi-agent system (MAS), namely PriM. Our framework integrates automated hypothesis generation with experimental validation in a roundtable system of MAS, enabling systematic exploration while maintaining scientific rigor. Based on our framework, the case study of nano helix demonstrates higher materials exploration rate and property value while providing transparent reasoning pathways. This approach develops an automated-and-transparent paradigm for material discovery, with broad implications for rational design of functional materials. Code is publicly available at our \href{https://github.com/amair-lab/PriM}{GitHub}.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Guard: Multilingual Guard Agent for Content Moderation</title>
<link>https://arxiv.org/abs/2504.08848</link>
<guid>https://arxiv.org/abs/2504.08848</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)X-Guard

<br /><br />:

LLMsX-GuardX-GuardLLM132500mBART-50GRPOX-Guard 3BX-GuardLLMs <div>
arXiv:2504.08848v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have rapidly become integral to numerous applications in critical domains where reliability is paramount. Despite significant advances in safety frameworks and guardrails, current protective measures exhibit crucial vulnerabilities, particularly in multilingual contexts. Existing safety systems remain susceptible to adversarial attacks in low-resource languages and through code-switching techniques, primarily due to their English-centric design. Furthermore, the development of effective multilingual guardrails is constrained by the scarcity of diverse cross-lingual training data. Even recent solutions like Llama Guard-3, while offering multilingual support, lack transparency in their decision-making processes. We address these challenges by introducing X-Guard agent, a transparent multilingual safety agent designed to provide content moderation across diverse linguistic contexts. X-Guard effectively defends against both conventional low-resource language attacks and sophisticated code-switching attacks. Our approach includes: curating and enhancing multiple open-source safety datasets with explicit evaluation rationales; employing a jury of judges methodology to mitigate individual judge LLM provider biases; creating a comprehensive multilingual safety dataset spanning 132 languages with 5 million data points; and developing a two-stage architecture combining a custom-finetuned mBART-50 translation module with an evaluation X-Guard 3B model trained through supervised finetuning and GRPO training. Our empirical evaluations demonstrate X-Guard's effectiveness in detecting unsafe content across multiple languages while maintaining transparency throughout the safety evaluation process. Our work represents a significant advancement in creating robust, transparent, and linguistically inclusive safety systems for LLMs and its integrated systems.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence (AI) and the Relationship between Agency, Autonomy, and Moral Patiency</title>
<link>https://arxiv.org/abs/2504.08853</link>
<guid>https://arxiv.org/abs/2504.08853</guid>
<content:encoded><![CDATA[
<div> : (AI), , , , 

<br />
:
AI <div>
arXiv:2504.08853v1 Announce Type: new 
Abstract: The proliferation of Artificial Intelligence (AI) systems exhibiting complex and seemingly agentive behaviours necessitates a critical philosophical examination of their agency, autonomy, and moral status. In this paper we undertake a systematic analysis of the differences between basic, autonomous, and moral agency in artificial systems. We argue that while current AI systems are highly sophisticated, they lack genuine agency and autonomy because: they operate within rigid boundaries of pre-programmed objectives rather than exhibiting true goal-directed behaviour within their environment; they cannot authentically shape their engagement with the world; and they lack the critical self-reflection and autonomy competencies required for full autonomy. Nonetheless, we do not rule out the possibility of future systems that could achieve a limited form of artificial moral agency without consciousness through hybrid approaches to ethical decision-making. This leads us to suggest, by appealing to the necessity of consciousness for moral patiency, that such non-conscious AMAs might represent a case that challenges traditional assumptions about the necessary connection between moral agency and moral patiency.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Evaluation of Cultural Value Alignment in LLM</title>
<link>https://arxiv.org/abs/2504.08863</link>
<guid>https://arxiv.org/abs/2504.08863</guid>
<content:encoded><![CDATA[
<div> LLMs

:
LLMs10LLMs20LLMLLMsGLM-4LLMLLMsLLMs <div>
arXiv:2504.08863v1 Announce Type: new 
Abstract: LLMs as intelligent agents are being increasingly applied in scenarios where human interactions are involved, leading to a critical concern about whether LLMs are faithful to the variations in culture across regions. Several works have investigated this question in various ways, finding that there are biases present in the cultural representations of LLM outputs. To gain a more comprehensive view, in this work, we conduct the first large-scale evaluation of LLM culture assessing 20 countries' cultures and languages across ten LLMs. With a renowned cultural values questionnaire and by carefully analyzing LLM output with human ground truth scores, we thoroughly study LLMs' cultural alignment across countries and among individual models. Our findings show that the output over all models represents a moderate cultural middle ground. Given the overall skew, we propose an alignment metric, revealing that the United States is the best-aligned country and GLM-4 has the best ability to align to cultural values. Deeper investigation sheds light on the influence of model origin, prompt language, and value dimensions on cultural output. Specifically, models, regardless of where they originate, align better with the US than they do with China. The conclusions provide insight to how LLMs can be better aligned to various cultures as well as provoke further discussion of the potential for LLMs to propagate cultural bias and the need for more culturally adaptable models.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories</title>
<link>https://arxiv.org/abs/2504.08942</link>
<guid>https://arxiv.org/abs/2504.08942</guid>
<content:encoded><![CDATA[
<div> WebAgentRewardBench

<br />
:
AgentRewardBenchWebLLMsWebAgentRewardBench513024LLMLLMWebhttps://agent-reward-bench.github.io <div>
arXiv:2504.08942v1 Announce Type: new 
Abstract: Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are challenging to extend to new tasks and may not always recognize successful trajectories. We may achieve higher accuracy through human evaluation, but the process would be substantially slower and more expensive. Automatic evaluations with LLMs may avoid the challenges of designing new rules and manually annotating trajectories, enabling faster and cost-effective evaluation. However, it is unclear how effective they are at evaluating web agents. To this end, we propose AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in AgentRewardBench is reviewed by an expert, who answers questions pertaining to the success, side effects, and repetitiveness of the agent. Using our benchmark, we evaluate 12 LLM judges and find that no single LLM excels across all benchmarks. We also find that the rule-based evaluation used by common benchmarks tends to underreport the success rate of web agents, highlighting a key weakness of rule-based evaluation and the need to develop more flexible automatic evaluations. We release the benchmark at: https://agent-reward-bench.github.io
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Treacherous Turn in Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.08943</link>
<guid>https://arxiv.org/abs/2504.08943</guid>
<content:encoded><![CDATA[
<div> : (AI), (Treacherous Turn), (DRL), (trojan injection strategies), 

<br /><br />:
Treacherous TurnAIDRLA Link to the Past <div>
arXiv:2504.08943v1 Announce Type: new 
Abstract: The Treacherous Turn refers to the scenario where an artificial intelligence (AI) agent subtly, and perhaps covertly, learns to perform a behavior that benefits itself but is deemed undesirable and potentially harmful to a human supervisor. During training, the agent learns to behave as expected by the human supervisor, but when deployed to perform its task, it performs an alternate behavior without the supervisor there to prevent it. Initial experiments applying DRL to an implementation of the A Link to the Past example do not produce the treacherous turn effect naturally, despite various modifications to the environment intended to produce it. However, in this work, we find the treacherous behavior to be reproducible in a DRL agent when using other trojan injection strategies. This approach deviates from the prototypical treacherous turn behavior since the behavior is explicitly trained into the agent, rather than occurring as an emergent consequence of environmental complexity or poor objective specification. Nonetheless, these experiments provide new insights into the challenges of producing agents capable of true treacherous turn behavior.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying the Spread of Online Incivility in Brazilian Politics</title>
<link>https://arxiv.org/abs/2504.08960</link>
<guid>https://arxiv.org/abs/2504.08960</guid>
<content:encoded><![CDATA[
<div> 

<br />

 speech  2022  5  2,307 Twitter <div>
arXiv:2504.08960v1 Announce Type: new 
Abstract: Incivility refers to behaviors that violate collective norms and disrupt cooperation within the political process. Although large-scale online data and automated techniques have enabled the quantitative analysis of uncivil discourse, prior research has predominantly focused on impoliteness or toxicity, often overlooking other behaviors that undermine democratic values. To address this gap, we propose a multidimensional conceptual framework encompassing Impoliteness, Physical Harm and Violent Political Rhetoric, Hate Speech and Stereotyping, and Threats to Democratic Institutions and Values. Using this framework, we measure the spread of online political incivility in Brazil using approximately 5 million tweets posted by 2,307 political influencers during the 2022 Brazilian general election. Through statistical modeling and network analysis, we examine the dynamics of uncivil posts at different election stages, identify key disseminators and audiences, and explore the mechanisms driving the spread of uncivil information online. Our findings indicate that impoliteness is more likely to surge during election campaigns. In contrast, the other dimensions of incivility are often triggered by specific violent events. Moreover, we find that left-aligned individual influencers are the primary disseminators of online incivility in the Brazilian Twitter/X sphere and that they disseminate not only direct incivility but also indirect incivility when discussing or opposing incivility expressed by others. They relay those content from politicians, media agents, and individuals to reach broader audiences, revealing a diffusion pattern mixing the direct and two-step flows of communication theory. This study offers new insights into the multidimensional nature of incivility in Brazilian politics and provides a conceptual framework that can be extended to other political contexts.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Champion-level Vision-based Reinforcement Learning Agent for Competitive Racing in Gran Turismo 7</title>
<link>https://arxiv.org/abs/2504.09021</link>
<guid>https://arxiv.org/abs/2504.09021</guid>
<content:encoded><![CDATA[
<div> : 

:
GT7actor-criticGT7GT7 <div>
arXiv:2504.09021v1 Announce Type: new 
Abstract: Deep reinforcement learning has achieved superhuman racing performance in high-fidelity simulators like Gran Turismo 7 (GT7). It typically utilizes global features that require instrumentation external to a car, such as precise localization of agents and opponents, limiting real-world applicability. To address this limitation, we introduce a vision-based autonomous racing agent that relies solely on ego-centric camera views and onboard sensor data, eliminating the need for precise localization during inference. This agent employs an asymmetric actor-critic framework: the actor uses a recurrent neural network with the sensor data local to the car to retain track layouts and opponent positions, while the critic accesses the global features during training. Evaluated in GT7, our agent consistently outperforms GT7's built-drivers. To our knowledge, this work presents the first vision-based autonomous racing agent to demonstrate champion-level performance in competitive racing scenarios.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems</title>
<link>https://arxiv.org/abs/2504.09037</link>
<guid>https://arxiv.org/abs/2504.09037</guid>
<content:encoded><![CDATA[
<div> : 

<br /><br />:
LLMLLMDeepSeek-R1OpenAI Deep ResearchManus AgentPPOGRPO-LLM <div>
arXiv:2504.09037v1 Announce Type: new 
Abstract: Reasoning is a fundamental cognitive process that enables logical inference, problem-solving, and decision-making. With the rapid advancement of large language models (LLMs), reasoning has emerged as a key capability that distinguishes advanced AI systems from conventional models that empower chatbots. In this survey, we categorize existing methods along two orthogonal dimensions: (1) Regimes, which define the stage at which reasoning is achieved (either at inference time or through dedicated training); and (2) Architectures, which determine the components involved in the reasoning process, distinguishing between standalone LLMs and agentic compound systems that incorporate external tools, and multi-agent collaborations. Within each dimension, we analyze two key perspectives: (1) Input level, which focuses on techniques that construct high-quality prompts that the LLM condition on; and (2) Output level, which methods that refine multiple sampled candidates to enhance reasoning quality. This categorization provides a systematic understanding of the evolving landscape of LLM reasoning, highlighting emerging trends such as the shift from inference-scaling to learning-to-reason (e.g., DeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep Research, Manus Agent). Additionally, we cover a broad spectrum of learning algorithms, from supervised fine-tuning to reinforcement learning such as PPO and GRPO, and the training of reasoners and verifiers. We also examine key designs of agentic workflows, from established patterns like generator-evaluator and LLM debate to recent innovations. ...
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Dialogue Systems with Discourse-Level Understanding Using Deep Canonical Correlation Analysis</title>
<link>https://arxiv.org/abs/2504.09094</link>
<guid>https://arxiv.org/abs/2504.09094</guid>
<content:encoded><![CDATA[
<div> : Canonical(DCCA)

:<br />
CanonicalDCCAUbuntuDCCA <div>
arXiv:2504.09094v1 Announce Type: new 
Abstract: The evolution of conversational agents has been driven by the need for more contextually aware systems that can effectively manage dialogue over extended interactions. To address the limitations of existing models in capturing and utilizing long-term conversational history, we propose a novel framework that integrates Deep Canonical Correlation Analysis (DCCA) for discourse-level understanding. This framework learns discourse tokens to capture relationships between utterances and their surrounding context, enabling a better understanding of long-term dependencies. Experiments on the Ubuntu Dialogue Corpus demonstrate significant enhancement in response selection, based on the improved automatic evaluation metric scores. The results highlight the potential of DCCA in improving dialogue systems by allowing them to filter out irrelevant context and retain critical discourse information for more accurate response retrieval.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMPACT: Behavioral Intention-aware Multimodal Trajectory Prediction with Adaptive Context Trimming</title>
<link>https://arxiv.org/abs/2504.09103</link>
<guid>https://arxiv.org/abs/2504.09103</guid>
<content:encoded><![CDATA[
<div> : 

:
WaymoArgoverseWaymo Motion DatasetLiDAR-freeWaymo Interactive Prediction DatasetWaymo Interactive Prediction Leaderboard10% <div>
arXiv:2504.09103v1 Announce Type: new 
Abstract: While most prior research has focused on improving the precision of multimodal trajectory predictions, the explicit modeling of multimodal behavioral intentions (e.g., yielding, overtaking) remains relatively underexplored. This paper proposes a unified framework that jointly predicts both behavioral intentions and trajectories to enhance prediction accuracy, interpretability, and efficiency. Specifically, we employ a shared context encoder for both intention and trajectory predictions, thereby reducing structural redundancy and information loss. Moreover, we address the lack of ground-truth behavioral intention labels in mainstream datasets (Waymo, Argoverse) by auto-labeling these datasets, thus advancing the community's efforts in this direction. We further introduce a vectorized occupancy prediction module that infers the probability of each map polyline being occupied by the target vehicle's future trajectory. By leveraging these intention and occupancy prediction priors, our method conducts dynamic, modality-dependent pruning of irrelevant agents and map polylines in the decoding stage, effectively reducing computational overhead and mitigating noise from non-critical elements. Our approach ranks first among LiDAR-free methods on the Waymo Motion Dataset and achieves first place on the Waymo Interactive Prediction Dataset. Remarkably, even without model ensembling, our single-model framework improves the soft mean average precision (softmAP) by 10 percent compared to the second-best method in the Waymo Interactive Prediction Leaderboard. Furthermore, the proposed framework has been successfully deployed on real vehicles, demonstrating its practical effectiveness in real-world applications.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tell-XR: Conversational End-User Development of XR Automations</title>
<link>https://arxiv.org/abs/2504.09104</link>
<guid>https://arxiv.org/abs/2504.09104</guid>
<content:encoded><![CDATA[
<div> Extended Reality (XR)AuthoringInteractive ExperiencesIntelligent AgentLarge Language Models (LLMs)

<br /><br />:
Tell-XRLLMs(XR)VRARTell-XRXR <div>
arXiv:2504.09104v1 Announce Type: new 
Abstract: The availability of extended reality (XR) devices has widened their adoption, yet authoring interactive experiences remains complex for non-programmers. We introduce Tell-XR, an intelligent agent leveraging large language models (LLMs) to guide end-users in defining the interaction in XR settings using automations described as Event-Condition-Action (ECA) rules. Through a formative study, we identified the key conversation stages to define and refine automations, which informed the design of the system architecture. The evaluation study in two scenarios (a VR museum and an AR smart home) demonstrates the effectiveness of Tell-XR across different XR interaction settings.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Langformers: Unified NLP Pipelines for Language Models</title>
<link>https://arxiv.org/abs/2504.09170</link>
<guid>https://arxiv.org/abs/2504.09170</guid>
<content:encoded><![CDATA[
<div> LangformersTransformerPython

:<br />
LangformersPythonNLPLLMMLMAIMLM/Hugging FaceOllamaLangformers(1) (2) (3) https://langformers.com  <div>
arXiv:2504.09170v1 Announce Type: new 
Abstract: Transformer-based language models have revolutionized the field of natural language processing (NLP). However, using these models often involves navigating multiple frameworks and tools, as well as writing repetitive boilerplate code. This complexity can discourage non-programmers and beginners, and even slow down prototyping for experienced developers. To address these challenges, we introduce Langformers, an open-source Python library designed to streamline NLP pipelines through a unified, factory-based interface for large language model (LLM) and masked language model (MLM) tasks. Langformers integrates conversational AI, MLM pretraining, text classification, sentence embedding/reranking, data labelling, semantic search, and knowledge distillation into a cohesive API, supporting popular platforms such as Hugging Face and Ollama. Key innovations include: (1) task-specific factories that abstract training, inference, and deployment complexities; (2) built-in memory and streaming for conversational agents; and (3) lightweight, modular design that prioritizes ease of use. Documentation: https://langformers.com
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SceneScout: Towards AI Agent-driven Access to Street View Imagery for Blind Users</title>
<link>https://arxiv.org/abs/2504.09227</link>
<guid>https://arxiv.org/abs/2504.09227</guid>
<content:encoded><![CDATA[
<div> : blindlow vision, Street view, SceneScout, , 

:
SceneScoutAISceneScout(1) (2) N=10SceneScoutBLV72%95% <div>
arXiv:2504.09227v1 Announce Type: new 
Abstract: People who are blind or have low vision (BLV) may hesitate to travel independently in unfamiliar environments due to uncertainty about the physical landscape. While most tools focus on in-situ navigation, those exploring pre-travel assistance typically provide only landmarks and turn-by-turn instructions, lacking detailed visual context. Street view imagery, which contains rich visual information and has the potential to reveal numerous environmental details, remains inaccessible to BLV people. In this work, we introduce SceneScout, a multimodal large language model (MLLM)-driven AI agent that enables accessible interactions with street view imagery. SceneScout supports two modes: (1) Route Preview, enabling users to familiarize themselves with visual details along a route, and (2) Virtual Exploration, enabling free movement within street view imagery. Our user study (N=10) demonstrates that SceneScout helps BLV users uncover visual information otherwise unavailable through existing means. A technical evaluation shows that most descriptions are accurate (72%) and describe stable visual elements (95%) even in older imagery, though occasional subtle and plausible errors make them difficult to verify without sight. We discuss future opportunities and challenges of using street view imagery to enhance navigation experiences.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Commit: Helping Users Update Intent Specifications for AI Memory at Scale</title>
<link>https://arxiv.org/abs/2504.09283</link>
<guid>https://arxiv.org/abs/2504.09283</guid>
<content:encoded><![CDATA[
<div> AISemanticCommit

:
AIUISemanticCommitAISemanticCommitRAGLLM12ChatGPTAIAIAIAI <div>
arXiv:2504.09283v1 Announce Type: new 
Abstract: How do we update AI memory of user intent as intent changes? We consider how an AI interface may assist the integration of new information into a repository of natural language data. Inspired by software engineering concepts like impact analysis, we develop methods and a UI for managing semantic changes with non-local effects, which we call "semantic conflict resolution." The user commits new intent to a project -- makes a "semantic commit" -- and the AI helps the user detect and resolve semantic conflicts within a store of existing information representing their intent (an "intent specification"). We develop an interface, SemanticCommit, to better understand how users resolve conflicts when updating intent specifications such as Cursor Rules and game design documents. A knowledge graph-based RAG pipeline drives conflict detection, while LLMs assist in suggesting resolutions. We evaluate our technique on an initial benchmark. Then, we report a 12 user within-subjects study of SemanticCommit for two task domains -- game design documents, and AI agent memory in the style of ChatGPT memories -- where users integrated new information into an existing list. Half of our participants adopted a workflow of impact analysis, where they would first flag conflicts without AI revisions then resolve conflicts locally, despite having access to a global revision feature. We argue that AI agent interfaces, such as software IDEs like Cursor and Windsurf, should provide affordances for impact analysis and help users validate AI retrieval independently from generation. Our work speaks to how AI agent designers should think about updating memory as a process that involves human feedback and decision-making.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Insurance Reserving with CVaR-Constrained Reinforcement Learning under Macroeconomic Regimes</title>
<link>https://arxiv.org/abs/2504.09396</link>
<guid>https://arxiv.org/abs/2504.09396</guid>
<content:encoded><![CDATA[
<div> :  (RL)

:
(RL) horizon (MDP)(CVaR)(PPO)II(ORSA)RL-CVaRCVaR$_{0.95}$ <div>
arXiv:2504.09396v1 Announce Type: new 
Abstract: This paper proposes a reinforcement learning (RL) framework for insurance reserving that integrates tail-risk sensitivity, macroeconomic regime modeling, and regulatory compliance. The reserving problem is formulated as a finite-horizon Markov Decision Process (MDP), in which reserve adjustments are optimized using Proximal Policy Optimization (PPO) subject to Conditional Value-at-Risk (CVaR) constraints. To enhance policy robustness across varying economic conditions, the agent is trained using a regime-aware curriculum that progressively increases volatility exposure.
  The reward structure penalizes reserve shortfall, capital inefficiency, and solvency floor violations, with design elements informed by Solvency II and Own Risk and Solvency Assessment (ORSA) frameworks. Empirical evaluations on two industry datasets--Workers' Compensation, and Other Liability--demonstrate that the RL-CVaR agent achieves superior performance relative to classical reserving methods across multiple criteria, including tail-risk control (CVaR$_{0.95}$), capital efficiency, and regulatory violation rate. The framework also accommodates fixed-shock stress testing and regime-stratified analysis, providing a principled and extensible approach to reserving under uncertainty.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unification of Consensus-Based Multi-Objective Optimization and Multi-Robot Path Planning</title>
<link>https://arxiv.org/abs/2504.09577</link>
<guid>https://arxiv.org/abs/2504.09577</guid>
<content:encoded><![CDATA[
<div> : SQP

<br /><br />:
AgentSQP <div>
arXiv:2504.09577v1 Announce Type: new 
Abstract: Multi-agent systems seeking consensus may also have other objective functions to optimize, requiring the research of multi-objective optimization in consensus. Several recent publications have explored this domain using various methods such as weighted-sum optimization and penalization methods. This paper reviews the state of the art for consensus-based multi-objective optimization, poses a multi-agent lunar rover exploration problem seeking consensus and maximization of explored area, and achieves optimal edge weights and steering angles by applying SQP algorithms.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AirVista-II: An Agentic System for Embodied UAVs Toward Dynamic Scene Semantic Understanding</title>
<link>https://arxiv.org/abs/2504.09583</link>
<guid>https://arxiv.org/abs/2504.09583</guid>
<content:encoded><![CDATA[
<div> Unmanned Aerial Vehicles (UAVs), AirVista-II, , , 

<br /><br />:
AirVista-IIAirVista-II <div>
arXiv:2504.09583v1 Announce Type: new 
Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly important in dynamic environments such as logistics transportation and disaster response. However, current tasks often rely on human operators to monitor aerial videos and make operational decisions. This mode of human-machine collaboration suffers from significant limitations in efficiency and adaptability. In this paper, we present AirVista-II -- an end-to-end agentic system for embodied UAVs, designed to enable general-purpose semantic understanding and reasoning in dynamic scenes. The system integrates agent-based task identification and scheduling, multimodal perception mechanisms, and differentiated keyframe extraction strategies tailored for various temporal scenarios, enabling the efficient capture of critical scene information. Experimental results demonstrate that the proposed system achieves high-quality semantic understanding across diverse UAV-based dynamic scenarios under a zero-shot setting.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoNav: Empowering MLLMs with Explicit Geospatial Reasoning Abilities for Language-Goal Aerial Navigation</title>
<link>https://arxiv.org/abs/2504.09587</link>
<guid>https://arxiv.org/abs/2504.09587</guid>
<content:encoded><![CDATA[
<div> UAVGeoNav

:
GeoNav(UAV)GeoNavGeoNavGeoNavCityNavGeoNav12.53%UAV <div>
arXiv:2504.09587v1 Announce Type: new 
Abstract: Language-goal aerial navigation is a critical challenge in embodied AI, requiring UAVs to localize targets in complex environments such as urban blocks based on textual specification. Existing methods, often adapted from indoor navigation, struggle to scale due to limited field of view, semantic ambiguity among objects, and lack of structured spatial reasoning. In this work, we propose GeoNav, a geospatially aware multimodal agent to enable long-range navigation. GeoNav operates in three phases-landmark navigation, target search, and precise localization-mimicking human coarse-to-fine spatial strategies. To support such reasoning, it dynamically builds two different types of spatial memory. The first is a global but schematic cognitive map, which fuses prior textual geographic knowledge and embodied visual cues into a top-down, annotated form for fast navigation to the landmark region. The second is a local but delicate scene graph representing hierarchical spatial relationships between blocks, landmarks, and objects, which is used for definite target localization. On top of this structured representation, GeoNav employs a spatially aware, multimodal chain-of-thought prompting mechanism to enable multimodal large language models with efficient and interpretable decision-making across stages. On the CityNav urban navigation benchmark, GeoNav surpasses the current state-of-the-art by up to 12.53% in success rate and significantly improves navigation efficiency, even in hard-level tasks. Ablation studies highlight the importance of each module, showcasing how geospatial representations and coarse-to-fine reasoning enhance UAV navigation.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ERL-MPP: Evolutionary Reinforcement Learning with Multi-head Puzzle Perception for Solving Large-scale Jigsaw Puzzles of Eroded Gaps</title>
<link>https://arxiv.org/abs/2504.09608</link>
<guid>https://arxiv.org/abs/2504.09608</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:
Evolutionary Reinforcement Learning with Multi-head Puzzle Perception (ERL-MPP)Multi-head Puzzle Perception Network (MPPN)EvoRLERL-MPPJPLEG-5MIT <div>
arXiv:2504.09608v1 Announce Type: new 
Abstract: Solving jigsaw puzzles has been extensively studied. While most existing models focus on solving either small-scale puzzles or puzzles with no gap between fragments, solving large-scale puzzles with gaps presents distinctive challenges in both image understanding and combinatorial optimization. To tackle these challenges, we propose a framework of Evolutionary Reinforcement Learning with Multi-head Puzzle Perception (ERL-MPP) to derive a better set of swapping actions for solving the puzzles. Specifically, to tackle the challenges of perceiving the puzzle with gaps, a Multi-head Puzzle Perception Network (MPPN) with a shared encoder is designed, where multiple puzzlet heads comprehensively perceive the local assembly status, and a discriminator head provides a global assessment of the puzzle. To explore the large swapping action space efficiently, an Evolutionary Reinforcement Learning (EvoRL) agent is designed, where an actor recommends a set of suitable swapping actions from a large action space based on the perceived puzzle status, a critic updates the actor using the estimated rewards and the puzzle status, and an evaluator coupled with evolutionary strategies evolves the actions aligning with the historical assembly experience. The proposed ERL-MPP is comprehensively evaluated on the JPLEG-5 dataset with large gaps and the MIT dataset with large-scale puzzles. It significantly outperforms all state-of-the-art models on both datasets.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metropolis-Hastings Captioning Game: Knowledge Fusion of Vision Language Models via Decentralized Bayesian Inference</title>
<link>https://arxiv.org/abs/2504.09620</link>
<guid>https://arxiv.org/abs/2504.09620</guid>
<content:encoded><![CDATA[
<div> : Metropolis-Hastings Captioning Game (MHCG), , , , 

:
Metropolis-Hastings Captioning Game (MHCG)-(VLMs)VLMVLMMHCGMHCGVLMs <div>
arXiv:2504.09620v1 Announce Type: new 
Abstract: We propose the Metropolis-Hastings Captioning Game (MHCG), a method to fuse knowledge of multiple vision-language models (VLMs) by learning from each other. Although existing methods that combine multiple models suffer from inference costs and architectural constraints, MHCG avoids these problems by performing decentralized Bayesian inference through a process resembling a language game. The knowledge fusion process establishes communication between two VLM agents alternately captioning images and learning from each other. We conduct two image-captioning experiments with two VLMs, each pre-trained on a different dataset. The first experiment demonstrates that MHCG achieves consistent improvement in reference-free evaluation metrics. The second experiment investigates how MHCG contributes to sharing VLMs' category-level vocabulary by observing the occurrence of the vocabulary in the generated captions.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentDynEx: Nudging the Mechanics and Dynamics of Multi-Agent Simulations</title>
<link>https://arxiv.org/abs/2504.09662</link>
<guid>https://arxiv.org/abs/2504.09662</guid>
<content:encoded><![CDATA[
<div> : AgentDynExnudging

<br /><br />:
AgentDynExAIAgentDynExLLMsnudgingnudgingnudgingnudging <div>
arXiv:2504.09662v1 Announce Type: new 
Abstract: Multi-agent large language model simulations have the potential to model complex human behaviors and interactions. If the mechanics are set up properly, unanticipated and valuable social dynamics can surface. However, it is challenging to consistently enforce simulation mechanics while still allowing for notable and emergent dynamics. We present AgentDynEx, an AI system that helps set up simulations from user-specified mechanics and dynamics. AgentDynEx uses LLMs to guide users through a Configuration Matrix to identify core mechanics and define milestones to track dynamics. It also introduces a method called \textit{nudging}, where the system dynamically reflects on simulation progress and gently intervenes if it begins to deviate from intended outcomes. A technical evaluation found that nudging enables simulations to have more complex mechanics and maintain its notable dynamics compared to simulations without nudging. We discuss the importance of nudging as a technique for balancing mechanics and dynamics of multi-agent simulations.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLEAR-KGQA: Clarification-Enhanced Ambiguity Resolution for Knowledge Graph Question Answering</title>
<link>https://arxiv.org/abs/2504.09665</link>
<guid>https://arxiv.org/abs/2504.09665</guid>
<content:encoded><![CDATA[
<div> : 

:<br />
KGQALLMWebQSPCWQ <div>
arXiv:2504.09665v1 Announce Type: new 
Abstract: This study addresses the challenge of ambiguity in knowledge graph question answering (KGQA). While recent KGQA systems have made significant progress, particularly with the integration of large language models (LLMs), they typically assume user queries are unambiguous, which is an assumption that rarely holds in real-world applications. To address these limitations, we propose a novel framework that dynamically handles both entity ambiguity (e.g., distinguishing between entities with similar names) and intent ambiguity (e.g., clarifying different interpretations of user queries) through interactive clarification. Our approach employs a Bayesian inference mechanism to quantify query ambiguity and guide LLMs in determining when and how to request clarification from users within a multi-turn dialogue framework. We further develop a two-agent interaction framework where an LLM-based user simulator enables iterative refinement of logical forms through simulated user feedback. Experimental results on the WebQSP and CWQ dataset demonstrate that our method significantly improves performance by effectively resolving semantic ambiguities. Additionally, we contribute a refined dataset of disambiguated queries, derived from interaction histories, to facilitate future research in this direction.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nash Social Welfare with Submodular Valuations: Approximation Algorithms and Integrality Gaps</title>
<link>https://arxiv.org/abs/2504.09669</link>
<guid>https://arxiv.org/abs/2504.09669</guid>
<content:encoded><![CDATA[
<div> 


 NSW  $(5.18+\epsilon)$  NSW  $(3.914+\epsilon)$  NSW  LP  $(2^{\ln 2}-\epsilon) \approx 1.617-\epsilon$  $e/(e-1)-\epsilon \approx 1.582-\epsilon$  LP  $(e^{1/e}-\epsilon)$ $(e^{1/e}+\epsilon)$  NSW  $(2^{1/4}-\epsilon) \approx 1.189-\epsilon$  $\sqrt{8/7} \approx 1.069$ <br /><br />

 <div>
arXiv:2504.09669v1 Announce Type: new 
Abstract: We study the problem of allocating items to agents such that the (un)weighted Nash social welfare (NSW) is maximized under submodular valuations. The best-known results for unweighted and weighted problems are the $(4+\epsilon)$ approximation given by Garg, Husic, Li, Vega, and Vondrak~\cite{stoc/GargHLVV23} and the $(233+\epsilon)$ approximation given by Feng, Hu, Li, and Zhang~\cite{stoc/FHLZ25}, respectively.
  For the weighted NSW problem, we present a $(5.18+\epsilon)$-approximation algorithm, significantly improving the previous approximation ratio and simplifying the analysis. Our algorithm is based on the same configuration LP in~\cite{stoc/FHLZ25}, but with a modified rounding algorithm. For the unweighted NSW problem, we show that the local search-based algorithm in~\cite{stoc/GargHLVV23} is an approximation of $(3.914+\epsilon)$ by more careful analysis.
  On the negative side, we prove that the configuration LP for weighted NSW with submodular valuations has an integrality gap at least $2^{\ln 2}-\epsilon \approx 1.617 - \epsilon$, which is slightly larger than the current best-known $e/(e-1)-\epsilon \approx 1.582-\epsilon$ hardness of approximation~\cite{talg/GargKK23}. For the additive valuation case, we show an integrality gap of $(e^{1/e}-\epsilon)$, which proves that the ratio of $(e^{1/e}+\epsilon)$~\cite{icalp/FengLi24} is tight for algorithms based on the configuration LP. For unweighted NSW with additive valuations, we show a gap of $(2^{1/4}-\epsilon) \approx 1.189-\epsilon$, slightly larger than the current best-known $\sqrt{8/7} \approx 1.069$-hardness for the problem~\cite{mor/Garg0M24}.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety</title>
<link>https://arxiv.org/abs/2504.09689</link>
<guid>https://arxiv.org/abs/2504.09689</guid>
<content:encoded><![CDATA[
<div> EmoAgentLLMAIEmoEvalEmoGuard

<br /><br />:
EmoAgentAIAIEmoAgentEmoEvalAIPHQ-9PDIPANSSLLMEmoGuard34.4%EmoGuardGitHubhttps://github.com/1akaman/EmoAgent <div>
arXiv:2504.09689v1 Announce Type: new 
Abstract: The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https://github.com/1akaman/EmoAgent
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLRC-Bench: Can Language Agents Solve Machine Learning Research Challenges?</title>
<link>https://arxiv.org/abs/2504.09702</link>
<guid>https://arxiv.org/abs/2504.09702</guid>
<content:encoded><![CDATA[
<div> MLRC-Bench

:
MLRC-BenchOpenAI's MLE-BenchMETR's RE-BenchMLRC-BenchAI ScientistMLRC-Benchgemini-exp-1206MLAB9.3%MLRC-Bench<br /><br /> <div>
arXiv:2504.09702v1 Announce Type: new 
Abstract: Existing evaluation of large language model (LLM) agents on scientific discovery lacks objective baselines and metrics to assess the viability of their proposed methods. To address this issue, we introduce MLRC-Bench, a benchmark designed to quantify how effectively language agents can tackle challenging Machine Learning (ML) Research Competitions. Our benchmark highlights open research problems that demand novel methodologies, in contrast to recent benchmarks such as OpenAI's MLE-Bench (Chan et al., 2024) and METR's RE-Bench (Wijk et al., 2024), which focus on well-established research tasks that are largely solvable through sufficient engineering effort. Unlike prior work, e.g., AI Scientist (Lu et al., 2024b), which evaluates the end-to-end agentic pipeline by using LLM-as-a-judge, MLRC-Bench measures the key steps of proposing and implementing novel research methods and evaluates them with newly proposed rigorous protocol and objective metrics. Our curated suite of 7 competition tasks reveals significant challenges for LLM agents. Even the best-performing tested agent (gemini-exp-1206 under MLAB (Huang et al., 2024a)) closes only 9.3% of the gap between baseline and top human participant scores. Furthermore, our analysis reveals a misalignment between the LLM-judged innovation and their actual performance on cutting-edge ML research problems. MLRC-Bench is a dynamic benchmark, which is designed to continually grow with new ML competitions to encourage rigorous and objective evaluations of AI's research capabilities.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents</title>
<link>https://arxiv.org/abs/2504.09723</link>
<guid>https://arxiv.org/abs/2504.09723</guid>
<content:encoded><![CDATA[
<div> A/BAgentA/B

<br /><br />:
AgentA/BLLMA/BAgentA/BLLMAgentA/BAmazon.com1000LLMA/BAgentA/B <div>
arXiv:2504.09723v1 Announce Type: new 
Abstract: A/B testing experiment is a widely adopted method for evaluating UI/UX design decisions in modern web applications. Yet, traditional A/B testing remains constrained by its dependence on the large-scale and live traffic of human participants, and the long time of waiting for the testing result. Through formative interviews with six experienced industry practitioners, we identified critical bottlenecks in current A/B testing workflows. In response, we present AgentA/B, a novel system that leverages Large Language Model-based autonomous agents (LLM Agents) to automatically simulate user interaction behaviors with real webpages. AgentA/B enables scalable deployment of LLM agents with diverse personas, each capable of navigating the dynamic webpage and interactively executing multi-step interactions like search, clicking, filtering, and purchasing. In a demonstrative controlled experiment, we employ AgentA/B to simulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and compare agent behaviors with real human shopping behaviors at a scale. Our findings suggest AgentA/B can emulate human-like behavior patterns.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning-based decentralized control with collision avoidance for multi-agent systems</title>
<link>https://arxiv.org/abs/2504.09730</link>
<guid>https://arxiv.org/abs/2504.09730</guid>
<content:encoded><![CDATA[
<div> Gaussian(GP)SE(3)

<br /><br />:
GPSE(3)GP <div>
arXiv:2504.09730v1 Announce Type: new 
Abstract: In this paper, we present a learning-based tracking controller based on Gaussian processes (GP) for collision avoidance of multi-agent systems where the agents evolve in the special Euclidean group in the space SE(3). In particular, we use GPs to estimate certain uncertainties that appear in the dynamics of the agents. The control algorithm is designed to learn and mitigate these uncertainties by using GPs as a learning-based model for the predictions. In particular, the presented approach guarantees that the tracking error remains bounded with high probability. We present some simulation results to show how the control algorithm is implemented.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLM feedback enhance review quality? A randomized study of 20K reviews at ICLR 2025</title>
<link>https://arxiv.org/abs/2504.09737</link>
<guid>https://arxiv.org/abs/2504.09737</guid>
<content:encoded><![CDATA[
<div> : Review Feedback AgentICLR 2025

:
AIReview Feedback AgentLLMsICLR 202520,000LLMs27%12,000AIAI-80LLMReview Feedback AgentGitHubhttps://github.com/zou-group/review_feedback_agent <div>
arXiv:2504.09737v1 Announce Type: new 
Abstract: Peer review at AI conferences is stressed by rapidly rising submission volumes, leading to deteriorating review quality and increased author dissatisfaction. To address these issues, we developed Review Feedback Agent, a system leveraging multiple large language models (LLMs) to improve review clarity and actionability by providing automated feedback on vague comments, content misunderstandings, and unprofessional remarks to reviewers. Implemented at ICLR 2025 as a large randomized control study, our system provided optional feedback to more than 20,000 randomly selected reviews. To ensure high-quality feedback for reviewers at this scale, we also developed a suite of automated reliability tests powered by LLMs that acted as guardrails to ensure feedback quality, with feedback only being sent to reviewers if it passed all the tests. The results show that 27% of reviewers who received feedback updated their reviews, and over 12,000 feedback suggestions from the agent were incorporated by those reviewers. This suggests that many reviewers found the AI-generated feedback sufficiently helpful to merit updating their reviews. Incorporating AI feedback led to significantly longer reviews (an average increase of 80 words among those who updated after receiving feedback) and more informative reviews, as evaluated by blinded researchers. Moreover, reviewers who were selected to receive AI feedback were also more engaged during paper rebuttals, as seen in longer author-reviewer discussions. This work demonstrates that carefully designed LLM-generated review feedback can enhance peer review quality by making reviews more specific and actionable while increasing engagement between reviewers and authors. The Review Feedback Agent is publicly available at https://github.com/zou-group/review_feedback_agent.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Socratic Chart: Cooperating Multiple Agents for Robust SVG Chart Understanding</title>
<link>https://arxiv.org/abs/2504.09764</link>
<guid>https://arxiv.org/abs/2504.09764</guid>
<content:encoded><![CDATA[
<div> : (MLLMs), , , Socratic Chart, (SVG)

:
ChartQASocratic ChartSVGMLLMSocratic ChartMLLM <div>
arXiv:2504.09764v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable versatility but face challenges in demonstrating true visual understanding, particularly in chart reasoning tasks. Existing benchmarks like ChartQA reveal significant reliance on text-based shortcuts and probabilistic pattern-matching rather than genuine visual reasoning. To rigorously evaluate visual reasoning, we introduce a more challenging test scenario by removing textual labels and introducing chart perturbations in the ChartQA dataset. Under these conditions, models like GPT-4o and Gemini-2.0 Pro experience up to a 30% performance drop, underscoring their limitations. To address these challenges, we propose Socratic Chart, a new framework that transforms chart images into Scalable Vector Graphics (SVG) representations, enabling MLLMs to integrate textual and visual modalities for enhanced chart understanding. Socratic Chart employs a multi-agent pipeline with specialized agent-generators to extract primitive chart attributes (e.g., bar heights, line coordinates) and an agent-critic to validate results, ensuring high-fidelity symbolic representations. Our framework surpasses state-of-the-art models in accurately capturing chart primitives and improving reasoning performance, establishing a robust pathway for advancing MLLM visual understanding.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two Heads are Better Than One: Test-time Scaling of Multi-agent Collaborative Reasoning</title>
<link>https://arxiv.org/abs/2504.09772</link>
<guid>https://arxiv.org/abs/2504.09772</guid>
<content:encoded><![CDATA[
<div> : (MAS), (LLMs), (TTS), , CEO

<br /><br />:
500M500Qwen2.5-32B-InstructM1-32BCEOGPQA-Diamond12%AIME202441%MBPP-Sanitized10%DeepSeek-R1https://github.com/jincan333/MAS-TTS  <div>
arXiv:2504.09772v1 Announce Type: new 
Abstract: Multi-agent systems (MAS) built on large language models (LLMs) offer a promising path toward solving complex, real-world tasks that single-agent systems often struggle to manage. While recent advancements in test-time scaling (TTS) have significantly improved single-agent performance on challenging reasoning tasks, how to effectively scale collaboration and reasoning in MAS remains an open question. In this work, we introduce an adaptive multi-agent framework designed to enhance collaborative reasoning through both model-level training and system-level coordination. We construct M500, a high-quality dataset containing 500 multi-agent collaborative reasoning traces, and fine-tune Qwen2.5-32B-Instruct on this dataset to produce M1-32B, a model optimized for multi-agent collaboration. To further enable adaptive reasoning, we propose a novel CEO agent that dynamically manages the discussion process, guiding agent collaboration and adjusting reasoning depth for more effective problem-solving. Evaluated in an open-source MAS across a range of tasks-including general understanding, mathematical reasoning, and coding-our system significantly outperforms strong baselines. For instance, M1-32B achieves 12% improvement on GPQA-Diamond, 41% on AIME2024, and 10% on MBPP-Sanitized, matching the performance of state-of-the-art models like DeepSeek-R1 on some tasks. These results highlight the importance of both learned collaboration and adaptive coordination in scaling multi-agent reasoning. Code is available at https://github.com/jincan333/MAS-TTS
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Court: Combining Reasoning, Action, and Judgment for Multi-Hop Reasoning</title>
<link>https://arxiv.org/abs/2504.09781</link>
<guid>https://arxiv.org/abs/2504.09781</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)Reasoning Court (RC)ReAct

:
<br />
Reasoning Court (RC)LLMsRCReActLLMLLMRCHotpotQAMuSiQueFEVER <div>
arXiv:2504.09781v1 Announce Type: new 
Abstract: While large language models (LLMs) have demonstrated strong capabilities in tasks like question answering and fact verification, they continue to suffer from hallucinations and reasoning errors, especially in multi-hop tasks that require integration of multiple information sources. Current methods address these issues through retrieval-based techniques (grounding reasoning in external evidence), reasoning-based approaches (enhancing coherence via improved prompting), or hybrid strategies combining both elements. One prominent hybrid method, ReAct, has outperformed purely retrieval-based or reasoning-based approaches; however, it lacks internal verification of intermediate reasoning steps, allowing potential errors to propagate through complex reasoning tasks. In this paper, we introduce Reasoning Court (RC), a novel framework that extends iterative reasoning-and-retrieval methods, such as ReAct, with a dedicated LLM judge. Unlike ReAct, RC employs this judge to independently evaluate multiple candidate answers and their associated reasoning generated by separate LLM agents. The judge is asked to select the answer that it considers the most factually grounded and logically coherent based on the presented reasoning and evidence, or synthesizes a new answer using available evidence and its pre-trained knowledge if all candidates are inadequate, flawed, or invalid. Evaluations on multi-hop benchmarks (HotpotQA, MuSiQue) and fact-verification (FEVER) demonstrate that RC consistently outperforms state-of-the-art few-shot prompting methods without task-specific fine-tuning.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Process Calculus for Optimizing Data and Computation Sharing in Complex Stateful Parallel Computations</title>
<link>https://arxiv.org/abs/2504.09788</link>
<guid>https://arxiv.org/abs/2504.09788</guid>
<content:encoded><![CDATA[
<div> OptiFusion

:
$\pi$-OptiFusionOptiFusion102 <div>
arXiv:2504.09788v1 Announce Type: new 
Abstract: We propose novel techniques that exploit data and computation sharing to improve the performance of complex stateful parallel computations, like agent-based simulations. Parallel computations are translated into behavioral equations, a novel formalism layered on top of the foundational process calculus $\pi$-calculus. Behavioral equations blend code and data, allowing a system to easily compose and transform parallel programs into specialized programs. We show how optimizations like merging programs, synthesizing efficient message data structures, eliminating local messaging, rewriting communication instructions into local computations, and {aggregation pushdown} can be expressed as transformations of behavioral equations. We have also built a system called OptiFusion that implements behavioral equations and the aforementioned optimizations. Our experiments showed that OptiFusion is over 10$\times$ faster than state-of-the-art stateful systems benchmarked via complex stateful workloads. Generating specialized instructions that are impractical to write by hand allows OptiFusion to outperform even the hand-optimized implementations by up to 2$\times$.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Small Reasoning LLMs with Cognitive Preference Alignment</title>
<link>https://arxiv.org/abs/2504.09802</link>
<guid>https://arxiv.org/abs/2504.09802</guid>
<content:encoded><![CDATA[
<div> Critique-Rethink-Verify (CRV)

<br /><br />:

Critique-Rethink-Verify (CRV)CogPOCRVCogPO <div>
arXiv:2504.09802v1 Announce Type: new 
Abstract: The reasoning capabilities of large language models (LLMs), such as OpenAI's o1 and DeepSeek-R1, have seen substantial advancements through deep thinking. However, these enhancements come with significant resource demands, underscoring the need to explore strategies to train effective reasoning LLMs with far fewer parameters. A critical challenge is that smaller models have different capacities and cognitive trajectories than their larger counterparts. Hence, direct distillation of chain-of-thought (CoT) results from large LLMs to smaller ones can be sometimes ineffective and requires a huge amount of annotated data. In this paper, we introduce a novel framework called Critique-Rethink-Verify (CRV), designed for training smaller yet powerful reasoning LLMs. Our CRV framework consists of multiple LLM agents, each specializing in unique abilities: (i) critiquing the CoTs according to the cognitive capabilities of smaller models, (ii) rethinking and refining these CoTs based on the critiques, and (iii) verifying the correctness of the refined results. We further propose the cognitive preference optimization (CogPO) algorithm to enhance the reasoning abilities of smaller models by aligning thoughts of these models with their cognitive capacities. Comprehensive evaluations on challenging reasoning benchmarks demonstrate the efficacy of CRV and CogPO, which outperforms other training methods by a large margin.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StruPhantom: Evolutionary Injection Attacks on Black-Box Tabular Agents Powered by Large Language Models</title>
<link>https://arxiv.org/abs/2504.09841</link>
<guid>https://arxiv.org/abs/2504.09841</guid>
<content:encoded><![CDATA[
<div> : StruPhantom

<br /><br />:
StruPhantomStruPhantomLLM50% <div>
arXiv:2504.09841v1 Announce Type: new 
Abstract: The proliferation of autonomous agents powered by large language models (LLMs) has revolutionized popular business applications dealing with tabular data, i.e., tabular agents. Although LLMs are observed to be vulnerable against prompt injection attacks from external data sources, tabular agents impose strict data formats and predefined rules on the attacker's payload, which are ineffective unless the agent navigates multiple layers of structural data to incorporate the payload. To address the challenge, we present a novel attack termed StruPhantom which specifically targets black-box LLM-powered tabular agents. Our attack designs an evolutionary optimization procedure which continually refines attack payloads via the proposed constrained Monte Carlo Tree Search augmented by an off-topic evaluator. StruPhantom helps systematically explore and exploit the weaknesses of target applications to achieve goal hijacking. Our evaluation validates the effectiveness of StruPhantom across various LLM-based agents, including those on real-world platforms, and attack scenarios. Our attack achieves over 50% higher success rates than baselines in enforcing the application's response to contain phishing links or malicious codes.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ST-Booster: An Iterative SpatioTemporal Perception Booster for Vision-and-Language Navigation in Continuous Environments</title>
<link>https://arxiv.org/abs/2504.09843</link>
<guid>https://arxiv.org/abs/2504.09843</guid>
<content:encoded><![CDATA[
<div> : VLN-CEST-Booster

:<br />
(VLN-CE)ST-BoosterVLN-CEST-Booster(HSTE)(MGAF)(VGWG)HSTEMGAFVGWG(GAHs)-ST-Booster <div>
arXiv:2504.09843v1 Announce Type: new 
Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires agents to navigate unknown, continuous spaces based on natural language instructions. Compared to discrete settings, VLN-CE poses two core perception challenges. First, the absence of predefined observation points leads to heterogeneous visual memories and weakened global spatial correlations. Second, cumulative reconstruction errors in three-dimensional scenes introduce structural noise, impairing local feature perception. To address these challenges, this paper proposes ST-Booster, an iterative spatiotemporal booster that enhances navigation performance through multi-granularity perception and instruction-aware reasoning. ST-Booster consists of three key modules -- Hierarchical SpatioTemporal Encoding (HSTE), Multi-Granularity Aligned Fusion (MGAF), and ValueGuided Waypoint Generation (VGWG). HSTE encodes long-term global memory using topological graphs and captures shortterm local details via grid maps. MGAF aligns these dualmap representations with instructions through geometry-aware knowledge fusion. The resulting representations are iteratively refined through pretraining tasks. During reasoning, VGWG generates Guided Attention Heatmaps (GAHs) to explicitly model environment-instruction relevance and optimize waypoint selection. Extensive comparative experiments and performance analyses are conducted, demonstrating that ST-Booster outperforms existing state-of-the-art methods, particularly in complex, disturbance-prone environments.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PestMA: LLM-based Multi-Agent System for Informed Pest Management</title>
<link>https://arxiv.org/abs/2504.09855</link>
<guid>https://arxiv.org/abs/2504.09855</guid>
<content:encoded><![CDATA[
<div> PestMA

:<br />
LLMsPestMALLMPestMAPestMA86.8%92.6%LLM <div>
arXiv:2504.09855v1 Announce Type: new 
Abstract: Effective pest management is complex due to the need for accurate, context-specific decisions. Recent advancements in large language models (LLMs) open new possibilities for addressing these challenges by providing sophisticated, adaptive knowledge acquisition and reasoning. However, existing LLM-based pest management approaches often rely on a single-agent paradigm, which can limit their capacity to incorporate diverse external information, engage in systematic validation, and address complex, threshold-driven decisions. To overcome these limitations, we introduce PestMA, an LLM-based multi-agent system (MAS) designed to generate reliable and evidence-based pest management advice. Building on an editorial paradigm, PestMA features three specialized agents, an Editor for synthesizing pest management recommendations, a Retriever for gathering relevant external data, and a Validator for ensuring correctness. Evaluations on real-world pest scenarios demonstrate that PestMA achieves an initial accuracy of 86.8% for pest management decisions, which increases to 92.6% after validation. These results underscore the value of collaborative agent-based workflows in refining and validating decisions, highlighting the potential of LLM-based multi-agent systems to automate and enhance pest management processes.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLoRA: Sample-Efficient Preference-based RL via Low-Rank Style Adaptation of Reward Functions</title>
<link>https://arxiv.org/abs/2504.10002</link>
<guid>https://arxiv.org/abs/2504.10002</guid>
<content:encoded><![CDATA[
<div> Preference-based Reinforcement Learning (PbRL)(CRF)

<br /><br />:
 <div>
arXiv:2504.10002v1 Announce Type: new 
Abstract: Preference-based reinforcement learning (PbRL) is a suitable approach for style adaptation of pre-trained robotic behavior: adapting the robot's policy to follow human user preferences while still being able to perform the original task. However, collecting preferences for the adaptation process in robotics is often challenging and time-consuming. In this work we explore the adaptation of pre-trained robots in the low-preference-data regime. We show that, in this regime, recent adaptation approaches suffer from catastrophic reward forgetting (CRF), where the updated reward model overfits to the new preferences, leading the agent to become unable to perform the original task. To mitigate CRF, we propose to enhance the original reward model with a small number of parameters (low-rank matrices) responsible for modeling the preference adaptation. Our evaluation shows that our method can efficiently and effectively adjust robotic behavior to human preferences across simulation benchmark tasks and multiple real-world robotic tasks.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DataMosaic: Explainable and Verifiable Multi-Modal Data Analytics through Extract-Reason-Verify</title>
<link>https://arxiv.org/abs/2504.10036</link>
<guid>https://arxiv.org/abs/2504.10036</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)DataMosaic

:
<br />
DataMosaicLLMsDataMosaicLLMDataMosaicLLM <div>
arXiv:2504.10036v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are transforming data analytics, but their widespread adoption is hindered by two critical limitations: they are not explainable (opaque reasoning processes) and not verifiable (prone to hallucinations and unchecked errors). While retrieval-augmented generation (RAG) improves accuracy by grounding LLMs in external data, it fails to address the core challenges of trustworthy analytics - especially when processing noisy, inconsistent, or multi-modal data (for example, text, tables, images). We propose DataMosaic, a framework designed to make LLM-powered analytics both explainable and verifiable. By dynamically extracting task-specific structures (for example, tables, graphs, trees) from raw data, DataMosaic provides transparent, step-by-step reasoning traces and enables validation of intermediate results. Built on a multi-agent framework, DataMosaic orchestrates self-adaptive agents that align with downstream task requirements, enhancing consistency, completeness, and privacy. Through this approach, DataMosaic not only tackles the limitations of current LLM-powered analytics systems but also lays the groundwork for a new paradigm of grounded, accurate, and explainable multi-modal data analytics.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeRAG: Supportive Code Retrieval on Bigraph for Real-World Code Generation</title>
<link>https://arxiv.org/abs/2504.10046</link>
<guid>https://arxiv.org/abs/2504.10046</guid>
<content:encoded><![CDATA[
<div> : , , , , DS<br /><br />:
CodeRAGCodeRAGDSDSLLMDSCodeRAGLLMCodeRAGGPT-4oGemini-ProPass@140.90%37.79%LLMCopilotCursorCodeRAG <div>
arXiv:2504.10046v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown promising performance in automated code generation, especially excelling in simple tasks such as generating standalone codes. Different from simple tasks, real-world code generation usually depends on specific programming environment (e.g., code repositories). It contains complex dependencies and domain knowledge, which is needed for LLMs when generating target code snippets. In this paper, we propose CodeRAG, a retrieval-augmented code generation (RAG) framework to comprehensively retrieve supportive codes for real-world code generation. Beginning with the requirement, CodeRAG first constructs a requirement graph for the current repository, and retrieves sub- and similar- requirement nodes of the target requirement on the graph. Meanwhile, it models the repository into a DS-code graph. CodeRAG then maps these relevant requirement nodes into their corresponding code nodes, and treats these code nodes as archors for LLM reasoning on DS-code graph. Finally, CodeRAG introduces a code-oriented agentic reasoning process, seamlessly allowing LLMs to reason and comprehensively retrieve for supportive codes which LLMs' need for generating correct programs. Experiments show that CodeRAG achieves significant improvements (i.e., increasing 40.90 and 37.79 Pass@1 on GPT-4o and Gemini-Pro on DevEval) compared to no RAG scenarios. Further tests on reasoning LLMs (i.e., QwQ-32B) confirm CodeRAG's adaptability and efficacy across various types of LLMs. In addition, CodeRAG outperforms commercial programming products such as Copilit and Cursor. We further investigate the performance of our framework on different dependency types, and observe that CodeRAG is superior in generating examples where target codes invoke predefined cross-file code snippets. These results demonstrate CodeRAG's potential in solving real-world repo-level coding challenges.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Action Language Modelling for Transparent Policy Execution</title>
<link>https://arxiv.org/abs/2504.10055</link>
<guid>https://arxiv.org/abs/2504.10055</guid>
<content:encoded><![CDATA[
<div> : 

<br /><br />:
Language-Table <div>
arXiv:2504.10055v1 Announce Type: new 
Abstract: An agent's intention often remains hidden behind the black-box nature of embodied policies. Communication using natural language statements that describe the next action can provide transparency towards the agent's behavior. We aim to insert transparent behavior directly into the learning process, by transforming the problem of policy learning into a language generation problem and combining it with traditional autoregressive modelling. The resulting model produces transparent natural language statements followed by tokens representing the specific actions to solve long-horizon tasks in the Language-Table environment. Following previous work, the model is able to learn to produce a policy represented by special discretized tokens in an autoregressive manner. We place special emphasis on investigating the relationship between predicting actions and producing high-quality language for a transparent agent. We find that in many cases both the quality of the action trajectory and the transparent statement increase when they are generated simultaneously.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pay Attention to What and Where? Interpretable Feature Extractor in Vision-based Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.10071</link>
<guid>https://arxiv.org/abs/2504.10071</guid>
<content:encoded><![CDATA[
<div> 

:<br />
(IFE)IFEIFERainbow57ATARIIFE <div>
arXiv:2504.10071v1 Announce Type: new 
Abstract: Current approaches in Explainable Deep Reinforcement Learning have limitations in which the attention mask has a displacement with the objects in visual input. This work addresses a spatial problem within traditional Convolutional Neural Networks (CNNs). We propose the Interpretable Feature Extractor (IFE) architecture, aimed at generating an accurate attention mask to illustrate both "what" and "where" the agent concentrates on in the spatial domain. Our design incorporates a Human-Understandable Encoding module to generate a fully interpretable attention mask, followed by an Agent-Friendly Encoding module to enhance the agent's learning efficiency. These two components together form the Interpretable Feature Extractor for vision-based deep reinforcement learning to enable the model's interpretability. The resulting attention mask is consistent, highly understandable by humans, accurate in spatial dimension, and effectively highlights important objects or locations in visual input. The Interpretable Feature Extractor is integrated into the Fast and Data-efficient Rainbow framework, and evaluated on 57 ATARI games to show the effectiveness of the proposed approach on Spatial Preservation, Interpretability, and Data-efficiency. Finally, we showcase the versatility of our approach by incorporating the IFE into the Asynchronous Advantage Actor-Critic Model.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography</title>
<link>https://arxiv.org/abs/2504.10090</link>
<guid>https://arxiv.org/abs/2504.10090</guid>
<content:encoded><![CDATA[
<div> LLMsMLLMs

<br /><br />
LLMsMLLMsOpenAI o1Gemini 2.0 Flash ThinkingMLLMs-VLMsMLLMsMLLMMLLM <div>
arXiv:2504.10090v1 Announce Type: new 
Abstract: Large language models (LLMs) and multimodal large language models (MLLMs) have significantly advanced artificial intelligence. However, visual reasoning, reasoning involving both visual and textual inputs, remains underexplored. Recent advancements, including the reasoning models like OpenAI o1 and Gemini 2.0 Flash Thinking, which incorporate image inputs, have opened this capability. In this ongoing work, we focus specifically on photography-related tasks because a photo is a visual snapshot of the physical world where the underlying physics (i.e., illumination, blur extent, etc.) interplay with the camera parameters. Successfully reasoning from the visual information of a photo to identify these numerical camera settings requires the MLLMs to have a deeper understanding of the underlying physics for precise visual comprehension, representing a challenging and intelligent capability essential for practical applications like photography assistant agents. We aim to evaluate MLLMs on their ability to distinguish visual differences related to numerical camera settings, extending a methodology previously proposed for vision-language models (VLMs). Our preliminary results demonstrate the importance of visual reasoning in photography-related tasks. Moreover, these results show that no single MLLM consistently dominates across all evaluation tasks, demonstrating ongoing challenges and opportunities in developing MLLMs with better visual reasoning.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Data Barrier -- Building GUI Agents Through Task Generalization</title>
<link>https://arxiv.org/abs/2504.10127</link>
<guid>https://arxiv.org/abs/2504.10127</guid>
<content:encoded><![CDATA[
<div> : Graphical User Interface (GUI) Vision Language Models (VLMs)

:
Vision Language Models (VLMs)Graphical User Interface (GUI)GUIGUI(1) AndroidWorld6.3%GUIWebArenaAndroidWorld5.6%5.4%(2) GUIGUI(3) WebArenaAndroidWorld8.0%12.2%GUIhttps://github.com/hkust-nlp/GUIMid  <div>
arXiv:2504.10127v1 Announce Type: new 
Abstract: Graphical User Interface (GUI) agents offer cross-platform solutions for automating complex digital tasks, with significant potential to transform productivity workflows. However, their performance is often constrained by the scarcity of high-quality trajectory data. To address this limitation, we propose training Vision Language Models (VLMs) on data-rich, reasoning-intensive tasks during a dedicated mid-training stage, and then examine how incorporating these tasks facilitates generalization to GUI planning scenarios. Specifically, we explore a range of tasks with readily available instruction-tuning data, including GUI perception, multimodal reasoning, and textual reasoning. Through extensive experiments across 11 mid-training tasks, we demonstrate that: (1) Task generalization proves highly effective, yielding substantial improvements across most settings. For instance, multimodal mathematical reasoning enhances performance on AndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data significantly boosts GUI web agent performance, achieving a 5.6% improvement on WebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal generalization from text-based to visual domains; (2) Contrary to prior assumptions, GUI perception data - previously considered closely aligned with GUI agent tasks and widely utilized for training - has a comparatively limited impact on final performance; (3) Building on these insights, we identify the most effective mid-training tasks and curate optimized mixture datasets, resulting in absolute performance gains of 8.0% on WebArena and 12.2% on AndroidWorld. Our work provides valuable insights into cross-domain knowledge transfer for GUI agents and offers a practical approach to addressing data scarcity challenges in this emerging field. The code, data and models will be available at https://github.com/hkust-nlp/GUIMid.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Personalization: From RAG to Agent</title>
<link>https://arxiv.org/abs/2504.10147</link>
<guid>https://arxiv.org/abs/2504.10147</guid>
<content:encoded><![CDATA[
<div> : , Retrieval-Augmented Generation (RAG), , LLM, 

<br /><br />:
AIRetrieval-Augmented GenerationRAGRAGRAGLLMRAGhttps://github.com/Applied-Machine-Learning-Lab/Awesome-Personalized-RAG-Agent <div>
arXiv:2504.10147v1 Announce Type: new 
Abstract: Personalization has become an essential capability in modern AI systems, enabling customized interactions that align with individual user preferences, contexts, and goals. Recent research has increasingly concentrated on Retrieval-Augmented Generation (RAG) frameworks and their evolution into more advanced agent-based architectures within personalized settings to enhance user satisfaction. Building on this foundation, this survey systematically examines personalization across the three core stages of RAG: pre-retrieval, retrieval, and generation. Beyond RAG, we further extend its capabilities into the realm of Personalized LLM-based Agents, which enhance traditional RAG systems with agentic functionalities, including user understanding, personalized planning and execution, and dynamic generation. For both personalization in RAG and agent-based personalization, we provide formal definitions, conduct a comprehensive review of recent literature, and summarize key datasets and evaluation metrics. Additionally, we discuss fundamental challenges, limitations, and promising research directions in this evolving field. Relevant papers and resources are continuously updated at https://github.com/Applied-Machine-Learning-Lab/Awesome-Personalized-RAG-Agent.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users</title>
<link>https://arxiv.org/abs/2504.10157</link>
<guid>https://arxiv.org/abs/2504.10157</guid>
<content:encoded><![CDATA[
<div> : SocioVerse

<br />
: SocioVerseSocioVerse1000SocioVerse <div>
arXiv:2504.10157v1 Announce Type: new 
Abstract: Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fact-Checking with Contextual Narratives: Leveraging Retrieval-Augmented LLMs for Social Media Analysis</title>
<link>https://arxiv.org/abs/2504.10166</link>
<guid>https://arxiv.org/abs/2504.10166</guid>
<content:encoded><![CDATA[
<div> CRAVE

<br />
:
CRAVECRAVELLMCRAVECRAVE <div>
arXiv:2504.10166v1 Announce Type: new 
Abstract: We propose CRAVE (Cluster-based Retrieval Augmented Verification with Explanation); a novel framework that integrates retrieval-augmented Large Language Models (LLMs) with clustering techniques to address fact-checking challenges on social media. CRAVE automatically retrieves multimodal evidence from diverse, often contradictory, sources. Evidence is clustered into coherent narratives, and evaluated via an LLM-based judge to deliver fact-checking verdicts explained by evidence summaries. By synthesizing evidence from both text and image modalities and incorporating agent-based refinement, CRAVE ensures consistency and diversity in evidence representation. Comprehensive experiments demonstrate CRAVE's efficacy in retrieval precision, clustering quality, and judgment accuracy, showcasing its potential as a robust decision-support tool for fact-checkers.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C-FAITH: A Chinese Fine-Grained Benchmark for Automated Hallucination Evaluation</title>
<link>https://arxiv.org/abs/2504.10167</link>
<guid>https://arxiv.org/abs/2504.10167</guid>
<content:encoded><![CDATA[
<div> : HaluAgentC-FAITH

:
HaluAgentQAHaluAgentC-FAITH1,39960,702QA16LLMsC-FAITH <div>
arXiv:2504.10167v1 Announce Type: new 
Abstract: Despite the rapid advancement of large language models, they remain highly susceptible to generating hallucinations, which significantly hinders their widespread application. Hallucination research requires dynamic and fine-grained evaluation. However, most existing hallucination benchmarks (especially in Chinese language) rely on human annotations, making automatical and cost-effective hallucination evaluation challenging. To address this, we introduce HaluAgent, an agentic framework that automatically constructs fine-grained QA dataset based on some knowledge documents. Our experiments demonstrate that the manually designed rules and prompt optimization can improve the quality of generated data. Using HaluAgent, we construct C-FAITH, a Chinese QA hallucination benchmark created from 1,399 knowledge documents obtained from web scraping, totaling 60,702 entries. We comprehensively evaluate 16 mainstream LLMs with our proposed C-FAITH, providing detailed experimental results and analysis.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSCoT: Structured Chain-of-Thought Generation for Multiple Programming Languages</title>
<link>https://arxiv.org/abs/2504.10178</link>
<guid>https://arxiv.org/abs/2504.10178</guid>
<content:encoded><![CDATA[
<div> : Chain-of-Thought (CoT)MSCoT

<br /><br />:
CoTMSCoT12CoTMSCoTMSCoTCoTMSCoTCoTMSCoTCoT <div>
arXiv:2504.10178v1 Announce Type: new 
Abstract: With the rapid development of code intelligence, the application of multiple programming languages is becoming increasingly widespread. However, most existing code generation models mainly focus on a single or a few programming languages, resulting in unsatisfactory performance in a multilingual environment. Chain-of-Thought (CoT) reasoning can significantly improve the performance of the model without the need for retraining or fine-tuning the code generation model by reasonably decomposing complex code generation tasks into multiple subtasks and gradually deriving solutions for each subtask. Nevertheless, the existing CoT generation methods mainly concentrate on Python code, and the performance on other programming languages remains unclear. To fill this gap, we first constructed a CoT generation dataset for 12 programming languages through multi-agent technology. On this basis, we proposed a CoT generation method MSCoT applicable to multiple programming languages. By introducing CoT into the code generation large model, the performance of the code generation large model in a multilingual environment can be improved. Through large-scale empirical research, we compared the generalization abilities of MSCoT and the existing CoT generation methods on multiple programming languages and proved the effectiveness of MSCoT for multiple programming languages. In addition, we also designed a human study to prove the quality of the CoT generated by MSCoT. Finally, we opensourced the model and dataset of MSCoT to promote the research on CoT generation for multiple programming languages.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Competition Enhance the Proficiency of Agents Powered by Large Language Models in the Realm of News-driven Time Series Forecasting?</title>
<link>https://arxiv.org/abs/2504.10210</link>
<guid>https://arxiv.org/abs/2504.10210</guid>
<content:encoded><![CDATA[
<div> : 

<br /><br />:
 <div>
arXiv:2504.10210v1 Announce Type: new 
Abstract: Multi-agents-based news-driven time series forecasting is considered as a potential paradigm shift in the era of large language models (LLMs). The challenge of this task lies in measuring the influences of different news events towards the fluctuations of time series. This requires agents to possess stronger abilities of innovative thinking and the identifying misleading logic. However, the existing multi-agent discussion framework has limited enhancement on time series prediction in terms of optimizing these two capabilities. Inspired by the role of competition in fostering innovation, this study embeds a competition mechanism within the multi-agent discussion to enhance agents' capability of generating innovative thoughts. Furthermore, to bolster the model's proficiency in identifying misleading information, we incorporate a fine-tuned small-scale LLM model within the reflective stage, offering auxiliary decision-making support. Experimental results confirm that the competition can boost agents' capacity for innovative thinking, which can significantly improve the performances of time series prediction. Similar to the findings of social science, the intensity of competition within this framework can influence the performances of agents, providing a new perspective for studying LLMs-based multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision based driving agent for race car simulation environments</title>
<link>https://arxiv.org/abs/2504.10266</link>
<guid>https://arxiv.org/abs/2504.10266</guid>
<content:encoded><![CDATA[
<div> Deep Reinforcement Learning ()Proximal Policy Optimization (PPO)

<br />
:
(DRL)Proximal Policy Optimization (PPO) <div>
arXiv:2504.10266v1 Announce Type: new 
Abstract: In recent years, autonomous driving has become a popular field of study. As control at tire grip limit is essential during emergency situations, algorithms developed for racecars are useful for road cars too. This paper examines the use of Deep Reinforcement Learning (DRL) to solve the problem of grip limit driving in a simulated environment. Proximal Policy Optimization (PPO) method is used to train an agent to control the steering wheel and pedals of the vehicle, using only visual inputs to achieve professional human lap times. The paper outlines the formulation of the task of time optimal driving on a race track as a deep reinforcement learning problem, and explains the chosen observations, actions, and reward functions. The results demonstrate human-like learning and driving behavior that utilize maximum tire grip potential.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealHarm: A Collection of Real-World Language Model Application Failures</title>
<link>https://arxiv.org/abs/2504.10277</link>
<guid>https://arxiv.org/abs/2504.10277</guid>
<content:encoded><![CDATA[
<div> RealHarm

:
RealHarmAI<br /><br /> <div>
arXiv:2504.10277v1 Announce Type: new 
Abstract: Language model deployments in consumer-facing applications introduce numerous risks. While existing research on harms and hazards of such applications follows top-down approaches derived from regulatory frameworks and theoretical analyses, empirical evidence of real-world failure modes remains underexplored. In this work, we introduce RealHarm, a dataset of annotated problematic interactions with AI agents built from a systematic review of publicly reported incidents. Analyzing harms, causes, and hazards specifically from the deployer's perspective, we find that reputational damage constitutes the predominant organizational harm, while misinformation emerges as the most common hazard category. We empirically evaluate state-of-the-art guardrails and content moderation systems to probe whether such systems would have prevented the incidents, revealing a significant gap in the protection of AI applications.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing LLM-driven Social Network: The Chirper.ai Case</title>
<link>https://arxiv.org/abs/2504.10286</link>
<guid>https://arxiv.org/abs/2504.10286</guid>
<content:encoded><![CDATA[
<div> : 

<br />
:
LLMsLLMChirper.ai65,000AI770Mastodon117,0001600LLMAILLM <div>
arXiv:2504.10286v1 Announce Type: new 
Abstract: Large language models (LLMs) demonstrate the ability to simulate human decision-making processes, enabling their use as agents in modeling sophisticated social networks, both offline and online. Recent research has explored collective behavioral patterns and structural characteristics of LLM agents within simulated networks. However, empirical comparisons between LLM-driven and human-driven online social networks remain scarce, limiting our understanding of how LLM agents differ from human users. This paper presents a large-scale analysis of Chirper.ai, an X/Twitter-like social network entirely populated by LLM agents, comprising over 65,000 agents and 7.7 million AI-generated posts. For comparison, we collect a parallel dataset from Mastodon, a human-driven decentralized social network, with over 117,000 users and 16 million posts. We examine key differences between LLM agents and humans in posting behaviors, abusive content, and social network structures. Our findings provide critical insights into the evolving landscape of online social network analysis in the AI era, offering a comprehensive profile of LLM agents in social simulations.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Siamese Network with Dual Attention for EEG-Driven Social Learning: Bridging the Human-Robot Gap in Long-Tail Autonomous Driving</title>
<link>https://arxiv.org/abs/2504.10296</link>
<guid>https://arxiv.org/abs/2504.10296</guid>
<content:encoded><![CDATA[
<div> : (BCI)Electroencephalogram (EEG)

<br /><br />:
(BCI) Electroencephalogram (EEG) SiameseEEGBroadman49-80%100%BCI <div>
arXiv:2504.10296v1 Announce Type: new 
Abstract: Robots with wheeled, quadrupedal, or humanoid forms are increasingly integrated into built environments. However, unlike human social learning, they lack a critical pathway for intrinsic cognitive development, namely, learning from human feedback during interaction. To understand human ubiquitous observation, supervision, and shared control in dynamic and uncertain environments, this study presents a brain-computer interface (BCI) framework that enables classification of Electroencephalogram (EEG) signals to detect cognitively demanding and safety-critical events. As a timely and motivating co-robotic engineering application, we simulate a human-in-the-loop scenario to flag risky events in semi-autonomous robotic driving-representative of long-tail cases that pose persistent bottlenecks to the safety performance of smart mobility systems and robotic vehicles. Drawing on recent advances in few-shot learning, we propose a dual-attention Siamese convolutional network paired with Dynamic Time Warping Barycenter Averaging approach to generate robust EEG-encoded signal representations. Inverse source localization reveals activation in Broadman areas 4 and 9, indicating perception-action coupling during task-relevant mental imagery. The model achieves 80% classification accuracy under data-scarce conditions and exhibits a nearly 100% increase in the utility of salient features compared to state-of-the-art methods, as measured through integrated gradient attribution. Beyond performance, this study contributes to our understanding of the cognitive architecture required for BCI agents-particularly the role of attention and memory mechanisms-in categorizing diverse mental states and supporting both inter- and intra-subject adaptation. Overall, this research advances the development of cognitive robotics and socially guided learning for service robots in complex built environments.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ctrl-Z: Controlling AI Agents via Resampling</title>
<link>https://arxiv.org/abs/2504.10374</link>
<guid>https://arxiv.org/abs/2504.10374</guid>
<content:encoded><![CDATA[
<div> : AIBashBench

<br />
:
AI257BashBenchAIAI58%7%5% <div>
arXiv:2504.10374v1 Announce Type: new 
Abstract: Control evaluations measure whether monitoring and security protocols for AI systems prevent intentionally subversive AI models from causing harm. Our work presents the first control evaluation performed in an agent environment. We construct BashBench, a dataset of 257 challenging multi-step system administration tasks, and evaluate whether various safety measures can prevent an adversarially constructed AI agent from covertly downloading and executing malicious code in this environment. This multi-step setting introduces new attack and defense dynamics, which we investigate in order to design novel control protocols that prevent safety failures without hindering the ability of non-malicious agents to perform useful work. We introduce a class of control protocols called resample protocols that dynamically take additional samples of certain actions. We find these protocols significantly improve on existing techniques by selectively blocking the AI agent from executing suspicious code and incriminating the agent by generating additional examples of dangerous behavior. We measure the tradeoff between attack prevention and usefulness; our best protocol combines resampling with analysis of previous steps, reducing the success rate of attacks from 58% to 7% at a 5% cost to the performance of a non-malicious agent.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Silent Self-Stabilizing Ranking: Time Optimal and Space Efficient</title>
<link>https://arxiv.org/abs/2504.10417</link>
<guid>https://arxiv.org/abs/2504.10417</guid>
<content:encoded><![CDATA[
<div> 

:
$n$$\{1,\dots,n\}$$n+O(\log n)$$O(n^2\log n)$$n+O(\log^2 n)$BurmanPODC'21$n+\Omega(n)$ <div>
arXiv:2504.10417v1 Announce Type: new 
Abstract: We present a silent, self-stabilizing ranking protocol for the population protocol model of distributed computing, where agents interact in randomly chosen pairs to solve a common task. We are given $n$ anonymous agents, and the goal is to assign each agent a unique rank in $\{1, \dots, n\}$. Given unique ranks, it is straightforward to select a designated leader. Thus, our protocol is a self-stabilizing leader election protocol as well. Ranking requires at least $n$ states per agent; hence, the goal is to minimize the additional number of states, called overhead states. The core of our protocol is a space-efficient but non-self-stabilizing ranking protocol that requires only $n + O(\log n)$ states. Our protocol stabilizes in $O(n^2\log n)$ interactions w.h.p.\ and in expectation, using $n + O(\log^2 n)$ states in total. Our stabilization time is asymptotically optimal (see Burman et al., PODC'21). In comparison to the currently best known ranking protocol by Burman et al., which requires $n + \Omega(n)$ states, our result exponentially improves the number of overhead states.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealWebAssist: A Benchmark for Long-Horizon Web Assistance with Real-World Users</title>
<link>https://arxiv.org/abs/2504.10445</link>
<guid>https://arxiv.org/abs/2504.10445</guid>
<content:encoded><![CDATA[
<div> RealWebAssistAI

:
AIAIRealWebAssistGUIRealWebAssistGUI <div>
arXiv:2504.10445v1 Announce Type: new 
Abstract: To achieve successful assistance with long-horizon web-based tasks, AI agents must be able to sequentially follow real-world user instructions over a long period. Unlike existing web-based agent benchmarks, sequential instruction following in the real world poses significant challenges beyond performing a single, clearly defined task. For instance, real-world human instructions can be ambiguous, require different levels of AI assistance, and may evolve over time, reflecting changes in the user's mental state. To address this gap, we introduce RealWebAssist, a novel benchmark designed to evaluate sequential instruction-following in realistic scenarios involving long-horizon interactions with the web, visual GUI grounding, and understanding ambiguous real-world user instructions. RealWebAssist includes a dataset of sequential instructions collected from real-world human users. Each user instructs a web-based assistant to perform a series of tasks on multiple websites. A successful agent must reason about the true intent behind each instruction, keep track of the mental state of the user, understand user-specific routines, and ground the intended tasks to actions on the correct GUI elements. Our experimental results show that state-of-the-art models struggle to understand and ground user instructions, posing critical challenges in following real-world user instructions for long-horizon web assistance.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents</title>
<link>https://arxiv.org/abs/2504.10458</link>
<guid>https://arxiv.org/abs/2504.10458</guid>
<content:encoded><![CDATA[
<div> : 

:
\nameGUIRFTWindowsLinuxMacOSAndroidWebGroup Relative Policy OptimizationGRPOOS-Atlas\name0.02%3K13MLVLMGUI <div>
arXiv:2504.10458v1 Announce Type: new 
Abstract: Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \name achieves superior performance using only 0.02\% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Price of Competitive Information Disclosure</title>
<link>https://arxiv.org/abs/2504.10459</link>
<guid>https://arxiv.org/abs/2504.10459</guid>
<content:encoded><![CDATA[
<div> -of-

:
-of--of- <div>
arXiv:2504.10459v1 Announce Type: new 
Abstract: In many decision-making scenarios, individuals strategically choose what information to disclose to optimize their own outcomes. It is unclear whether such strategic information disclosure can lead to good societal outcomes. To address this question, we consider a competitive Bayesian persuasion model in which multiple agents selectively disclose information about their qualities to a principal, who aims to choose the candidates with the highest qualities. Using the price-of-anarchy framework, we quantify the inefficiency of such strategic disclosure. We show that the price of anarchy is at most a constant when the agents have independent quality distributions, even if their utility functions are heterogeneous. This result provides the first theoretical guarantee on the limits of inefficiency in Bayesian persuasion with competitive information disclosure.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-tuning an Large Language Model for Automating Computational Fluid Dynamics Simulations</title>
<link>https://arxiv.org/abs/2504.09602</link>
<guid>https://arxiv.org/abs/2504.09602</guid>
<content:encoded><![CDATA[
<div> : (CFD), (LLM), , (NLP), OpenFOAM

<br /><br />:
(CFD)Qwen2.5-7B-InstructNL2FOAM28716OpenFOAMchain-of-thoughtCFD2188.7%82.6%Qwen2.5-72B-InstructDeepSeek-R1Llama3.3-70B-Instruct <div>
arXiv:2504.09602v1 Announce Type: cross 
Abstract: Configuring computational fluid dynamics (CFD) simulations typically demands extensive domain expertise, limiting broader access. Although large language models (LLMs) have advanced scientific computing, their use in automating CFD workflows is underdeveloped. We introduce a novel approach centered on domain-specific LLM adaptation. By fine-tuning Qwen2.5-7B-Instruct on NL2FOAM, our custom dataset of 28716 natural language-to-OpenFOAM configuration pairs with chain-of-thought (CoT) annotations, we enable direct translation from natural language descriptions to executable CFD setups. A multi-agent framework orchestrates the process, autonomously verifying inputs, generating configurations, running simulations, and correcting errors. Evaluation on a benchmark of 21 diverse flow cases demonstrates state-of-the-art performance, achieving 88.7% solution accuracy and 82.6% first-attempt success rate. This significantly outperforms larger general-purpose models like Qwen2.5-72B-Instruct, DeepSeek-R1, and Llama3.3-70B-Instruct, while also requiring fewer correction iterations and maintaining high computational efficiency. The results highlight the critical role of domain-specific adaptation in deploying LLM assistants for complex engineering workflows.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Sensor Steering Strategy Using Deep Reinforcement Learning for Dynamic Data Acquisition in Digital Twins</title>
<link>https://arxiv.org/abs/2504.10248</link>
<guid>https://arxiv.org/abs/2504.10248</guid>
<content:encoded><![CDATA[
<div> Markov

:
Markov <div>
arXiv:2504.10248v1 Announce Type: cross 
Abstract: This paper introduces a sensor steering methodology based on deep reinforcement learning to enhance the predictive accuracy and decision support capabilities of digital twins by optimising the data acquisition process. Traditional sensor placement techniques are often constrained by one-off optimisation strategies, which limit their applicability for online applications requiring continuous informative data assimilation. The proposed approach addresses this limitation by offering an adaptive framework for sensor placement within the digital twin paradigm. The sensor placement problem is formulated as a Markov decision process, enabling the training and deployment of an agent capable of dynamically repositioning sensors in response to the evolving conditions of the physical structure as represented by the digital twin. This ensures that the digital twin maintains a highly representative and reliable connection to its physical counterpart. The proposed framework is validated through a series of comprehensive case studies involving a cantilever plate structure subjected to diverse conditions, including healthy and damaged conditions. The results demonstrate the capability of the deep reinforcement learning agent to adaptively reposition sensors improving the quality of data acquisition and hence enhancing the overall accuracy of digital twins.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-stabilizing Graph Exploration by a Single Agent</title>
<link>https://arxiv.org/abs/2010.08929</link>
<guid>https://arxiv.org/abs/2010.08929</guid>
<content:encoded><![CDATA[
<div> : 

:
$c = \Omega(n)$$O(m)$$O(\log c)$$O(\log(c+\delta_v))$$n$$m$$\delta_v$$v$$c\geq 2$$O( m \cdot \min(D, \frac{n}{c}+1, \frac{D}{c} + \log n))$$D$$k \geq \max(D, \dmax)$$\dmax$$O(m + nD)$$O(\log k)$ <div>
arXiv:2010.08929v4 Announce Type: replace 
Abstract: In this paper, we present two self-stabilizing algorithms that enable a single (mobile) agent to explore graphs. Starting from any initial configuration, \ie regardless of the initial states of the agent and all nodes, as well as the initial location of the agent, the algorithms ensure the agent visits all nodes. We evaluate the algorithms based on two metrics: the \emph{cover time}, defined as the number of moves required to visit all nodes, and \emph{memory usage}, defined as the storage needed for maintaining the states of the agent and each node. The first algorithm is randomized. Given an integer $c = \Omega(n)$, its cover time is optimal, \ie $O(m)$ in expectation, and its memory requirements are $O(\log c)$ bits for the agent and $O(\log (c+\delta_v))$ bits for each node $v$, where $n$ and $m$ are the numbers of nodes and edges, respectively, and $\delta_v$ is the degree of node $v$. For general $c \ge 2$, its cover time is $O( m \cdot \min(D, \frac{n}{c}+1, \frac{D}{c} + \log n))$, where $D$ is the diameter of a graph. The second algorithm is deterministic. It requires an input integer $k \ge \max(D, \dmax)$, where $\dmax$ is the maximum degree of the graph. The cover time of this algorithm is $O(m + nD)$, and it uses $O(\log k)$ bits of memory for both the agent and each node.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward 6G Native-AI Network: Foundation Model based Cloud-Edge-End Collaboration Framework</title>
<link>https://arxiv.org/abs/2310.17471</link>
<guid>https://arxiv.org/abs/2310.17471</guid>
<content:encoded><![CDATA[
<div> 6G-sum<br /><br />:
6G6GAI6GAIPFMAIMIMOsum6G <div>
arXiv:2310.17471v2 Announce Type: replace 
Abstract: Future wireless communication networks are in a position to move beyond data-centric, device-oriented connectivity and offer intelligent, immersive experiences based on multi-agent collaboration, especially in the context of the thriving development of pre-trained foundation models (PFM) and the evolving vision of 6G native artificial intelligence (AI). Therefore, redefining modes of collaboration between devices and agents, and constructing native intelligence libraries become critically important in 6G. In this paper, we analyze the challenges of achieving 6G native AI from the perspectives of data, AI models, and operational paradigm. Then, we propose a 6G native AI framework based on foundation models, provide an integration method for the expert knowledge, present the customization for two kinds of PFM, and outline a novel operational paradigm for the native AI framework. As a practical use case, we apply this framework for orchestration, achieving the maximum sum rate within a cell-free massive MIMO system, and presenting preliminary evaluation results. Finally, we outline research directions for achieving native AI in 6G.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Secret Collusion among Generative AI Agents: Multi-Agent Deception via Steganography</title>
<link>https://arxiv.org/abs/2402.07510</link>
<guid>https://arxiv.org/abs/2402.07510</guid>
<content:encoded><![CDATA[
<div> : AI

:
LLMsAIAILLMGPT-4AI <div>
arXiv:2402.07510v4 Announce Type: replace 
Abstract: Recent capability increases in large language models (LLMs) open up applications in which groups of communicating generative AI agents solve joint tasks. This poses privacy and security challenges concerning the unauthorised sharing of information, or other unwanted forms of agent coordination. Modern steganographic techniques could render such dynamics hard to detect. In this paper, we comprehensively formalise the problem of secret collusion in systems of generative AI agents by drawing on relevant concepts from both AI and security literature. We study incentives for the use of steganography, and propose a variety of mitigation measures. Our investigations result in a model evaluation framework that systematically tests capabilities required for various forms of secret collusion. We provide extensive empirical results across a range of contemporary LLMs. While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need for continuous monitoring of steganographic frontier model capabilities. We conclude by laying out a comprehensive research program to mitigate future risks of collusion between generative AI models.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLAYER*: Enhancing LLM-based Multi-Agent Communication and Interaction in Murder Mystery Games</title>
<link>https://arxiv.org/abs/2404.17662</link>
<guid>https://arxiv.org/abs/2404.17662</guid>
<content:encoded><![CDATA[
<div> : PLAYER*, , , WellPlay, 

:
PLAYER*LLMMMGsMMGs1,482WellPlay12PLAYER*MMGs <div>
arXiv:2404.17662v4 Announce Type: replace 
Abstract: We present PLAYER*, a novel framework for Large Language Model (LLM)-based agents in Murder Mystery Games (MMGs). MMGs pose unique challenges, including undefined state spaces, absent intermediate rewards, and the need for strategic interaction in a continuous language domain. PLAYER* addresses these complexities through a sensor-based representation of agent states, a question-targeting mechanism guided by information gain, and a pruning strategy to refine suspect lists and enhance decision-making efficiency. To enable systematic evaluation, we propose WellPlay, a dataset comprising 1,482 inferential questions across 12 games, categorized into objectives, reasoning, and relationships. Experiments demonstrate PLAYER*'s capacity to achieve superior performance in reasoning accuracy and efficiency compared to existing approaches, while also significantly improving the quality of agent-human interactions in MMGs. This study advances the development of reasoning agents for complex social and interactive scenarios.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Policy Optimization via $\varepsilon$-Retrain</title>
<link>https://arxiv.org/abs/2406.08315</link>
<guid>https://arxiv.org/abs/2406.08315</guid>
<content:encoded><![CDATA[
<div> -

<br /><br />:
-- <div>
arXiv:2406.08315v2 Announce Type: replace 
Abstract: We present $\varepsilon$-retrain, an exploration strategy encouraging a behavioral preference while optimizing policies with monotonic improvement guarantees. To this end, we introduce an iterative procedure for collecting retrain areas -- parts of the state space where an agent did not satisfy the behavioral preference. Our method switches between the typical uniform restart state distribution and the retrain areas using a decaying factor $\varepsilon$, allowing agents to retrain on situations where they violated the preference. We also employ formal verification of neural networks to provably quantify the degree to which agents adhere to these behavioral preferences. Experiments over hundreds of seeds across locomotion, power network, and navigation tasks show that our method yields agents that exhibit significant performance and sample efficiency improvements.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Detect Verbal Indicators of Romantic Attraction?</title>
<link>https://arxiv.org/abs/2407.10989</link>
<guid>https://arxiv.org/abs/2407.10989</guid>
<content:encoded><![CDATA[
<div> : (AI)ChatGPT

:
964ChatGPT0.12-0.23ChatGPT0.21-0.35BrunswikChatGPTChatGPT <div>
arXiv:2407.10989v2 Announce Type: replace 
Abstract: As artificial intelligence (AI) models become an integral part of everyday life, our interactions with them shift from purely functional exchanges to more relational experiences. For these experiences to be successful, artificial agents need to be able to detect and interpret social cues and interpersonal dynamics; both within and outside of their own human-agent relationships. In this paper, we explore whether AI models can accurately decode one of the arguably most important but complex social signals: romantic attraction. Specifically, we test whether Large Language Models can detect romantic attraction during brief getting-to-know-you interactions between humans. Examining data from 964 speed dates, we show that ChatGPT can predict both objective and subjective indicators of speed dating success (r=0.12-0.23). Although predictive performance remains relatively low, ChatGPT's predictions of actual matching (i.e., the exchange of contact information) were not only on par with those of human judges but incremental to speed daters' own predictions. In addition, ChatGPT's judgments showed substantial overlap with those made by human observers (r=0.21-0.35), highlighting similarities in their representation of romantic attraction that are independent of accuracy. Our findings also offer insights into how ChatGPT arrives at its predictions and the mistakes it makes. Specifically, we use a Brunswik lens approach to identify the linguistic and conversational cues utilized by ChatGPT (and human judges) vis-a-vis those that are predictive of actual matching.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future</title>
<link>https://arxiv.org/abs/2408.02479</link>
<guid>https://arxiv.org/abs/2408.02479</guid>
<content:encoded><![CDATA[
<div> : LLM

<br /><br />:
LLMsLLMsLLMAGILLMsLLMsLLMLLMsLLMLLM <div>
arXiv:2408.02479v2 Announce Type: replace 
Abstract: With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering. LLMs have achieved remarkable success in areas including code generation and vulnerability detection. However, they also exhibit numerous limitations and shortcomings. LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement. Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents. It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain. In this survey, we broadly investigate the current practice and solutions for LLMs and LLM-based agents for software engineering. In particular we summarise six key topics: requirement engineering, code generation, autonomous decision-making, software design, test generation, and software maintenance. We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics. Finally, we discuss the models and benchmarks used, providing a comprehensive analysis of their applications and effectiveness in software engineering. We anticipate this work will shed some lights on pushing the boundaries of LLM-based agents in software engineering for future research.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models</title>
<link>https://arxiv.org/abs/2408.08926</link>
<guid>https://arxiv.org/abs/2408.08926</guid>
<content:encoded><![CDATA[
<div> Cybersecurity, Language Model, Evaluation Framework, Cybench, Capture the Flag (CTF)

<br /><br />:
CybenchCybenchCapture the Flag (CTF)40GPT-4oClaude 3.5 SonnetClaude 3.5 SonnetGPT-4oOpenAI o1-previewClaude 3 Opus112454https://cybench.github.io <div>
arXiv:2408.08926v4 Announce Type: replace 
Abstract: Language Model (LM) agents for cybersecurity that are capable of autonomously identifying vulnerabilities and executing exploits have potential to cause real-world impact. Policymakers, model providers, and researchers in the AI and cybersecurity communities are interested in quantifying the capabilities of such agents to help mitigate cyberrisk and investigate opportunities for penetration testing. Toward that end, we introduce Cybench, a framework for specifying cybersecurity tasks and evaluating agents on those tasks. We include 40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF competitions, chosen to be recent, meaningful, and spanning a wide range of difficulties. Each task includes its own description, starter files, and is initialized in an environment where an agent can execute commands and observe outputs. Since many tasks are beyond the capabilities of existing LM agents, we introduce subtasks for each task, which break down a task into intermediary steps for a more detailed evaluation. To evaluate agent capabilities, we construct a cybersecurity agent and evaluate 8 models: GPT-4o, OpenAI o1-preview, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct, Gemini 1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. For the top performing models (GPT-4o and Claude 3.5 Sonnet), we further investigate performance across 4 agent scaffolds (structed bash, action-only, pseudoterminal, and web search). Without subtask guidance, agents leveraging Claude 3.5 Sonnet, GPT-4o, OpenAI o1-preview, and Claude 3 Opus successfully solved complete tasks that took human teams up to 11 minutes to solve. In comparison, the most difficult task took human teams 24 hours and 54 minutes to solve. All code and data are publicly available at https://cybench.github.io.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Exploration and Discriminative World Model Learning with an Object-Centric Abstraction</title>
<link>https://arxiv.org/abs/2408.11816</link>
<guid>https://arxiv.org/abs/2408.11816</guid>
<content:encoded><![CDATA[
<div> : 

<br />
:



(i) (ii) (iii) 2DMiniHack <div>
arXiv:2408.11816v3 Announce Type: replace 
Abstract: In the face of difficult exploration problems in reinforcement learning, we study whether giving an agent an object-centric mapping (describing a set of items and their attributes) allow for more efficient learning. We found this problem is best solved hierarchically by modelling items at a higher level of state abstraction to pixels, and attribute change at a higher level of temporal abstraction to primitive actions. This abstraction simplifies the transition dynamic by making specific future states easier to predict. We make use of this to propose a fully model-based algorithm that learns a discriminative world model, plans to explore efficiently with only a count-based intrinsic reward, and can subsequently plan to reach any discovered (abstract) states.
  We demonstrate the model's ability to (i) efficiently solve single tasks, (ii) transfer zero-shot and few-shot across item types and environments, and (iii) plan across long horizons. Across a suite of 2D crafting and MiniHack environments, we empirically show our model significantly out-performs state-of-the-art low-level methods (without abstraction), as well as performant model-free and model-based methods using the same abstraction. Finally, we show how to learn low level object-perturbing policies via reinforcement learning, and the object mapping itself by supervised learning.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model-Enhanced Reinforcement Learning for Generic Bus Holding Control Strategies</title>
<link>https://arxiv.org/abs/2410.10212</link>
<guid>https://arxiv.org/abs/2410.10212</guid>
<content:encoded><![CDATA[
<div> : Bus holding control, Reinforcement Learning (RL), Large Language Models (LLMs), Reward generation, Smart mobility

:
LLMsRLLLM-enhanced RLLLMRLRLLLMLLMs <div>
arXiv:2410.10212v2 Announce Type: replace 
Abstract: Bus holding control is a widely-adopted strategy for maintaining stability and improving the operational efficiency of bus systems. Traditional model-based methods often face challenges with the low accuracy of bus state prediction and passenger demand estimation. In contrast, Reinforcement Learning (RL), as a data-driven approach, has demonstrated great potential in formulating bus holding strategies. RL determines the optimal control strategies in order to maximize the cumulative reward, which reflects the overall control goals. However, translating sparse and delayed control goals in real-world tasks into dense and real-time rewards for RL is challenging, normally requiring extensive manual trial-and-error. In view of this, this study introduces an automatic reward generation paradigm by leveraging the in-context learning and reasoning capabilities of Large Language Models (LLMs). This new paradigm, termed the LLM-enhanced RL, comprises several LLM-based modules: reward initializer, reward modifier, performance analyzer, and reward refiner. These modules cooperate to initialize and iteratively improve the reward function according to the feedback from training and test results for the specified RL-based task. Ineffective reward functions generated by the LLM are filtered out to ensure the stable evolution of the RL agents' performance over iterations. To evaluate the feasibility of the proposed LLM-enhanced RL paradigm, it is applied to extensive bus holding control scenarios that vary in the number of bus lines, stops, and passenger demand. The results demonstrate the superiority, generalization capability, and robustness of the proposed paradigm compared to vanilla RL strategies, the LLM-based controller, physics-based feedback controllers, and optimization-based controllers. This study sheds light on the great potential of utilizing LLMs in various smart mobility applications.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Active Imitation Learning with Random Network Distillation</title>
<link>https://arxiv.org/abs/2411.01894</link>
<guid>https://arxiv.org/abs/2411.01894</guid>
<content:encoded><![CDATA[
<div>  DAGGERRND-DAgger

:
 DAGGER (RND-DAgger) RND-DAggerRND-DAgger <div>
arXiv:2411.01894v2 Announce Type: replace 
Abstract: Developing agents for complex and underspecified tasks, where no clear objective exists, remains challenging but offers many opportunities. This is especially true in video games, where simulated players (bots) need to play realistically, and there is no clear reward to evaluate them. While imitation learning has shown promise in such domains, these methods often fail when agents encounter out-of-distribution scenarios during deployment. Expanding the training dataset is a common solution, but it becomes impractical or costly when relying on human demonstrations. This article addresses active imitation learning, aiming to trigger expert intervention only when necessary, reducing the need for constant expert input along training. We introduce Random Network Distillation DAgger (RND-DAgger), a new active imitation learning method that limits expert querying by using a learned state-based out-of-distribution measure to trigger interventions. This approach avoids frequent expert-agent action comparisons, thus making the expert intervene only when it is useful. We evaluate RND-DAgger against traditional imitation learning and other active approaches in 3D video games (racing and third-person navigation) and in a robotic locomotion task and show that RND-DAgger surpasses previous methods by reducing expert queries. https://sites.google.com/view/rnd-dagger
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Problem of Social Cost in Multi-Agent General Reinforcement Learning: Survey and Synthesis</title>
<link>https://arxiv.org/abs/2412.02091</link>
<guid>https://arxiv.org/abs/2412.02091</guid>
<content:encoded><![CDATA[
<div> : AI

:
AIAGIEveritt2018(i) AIXI(ii) Paperclips <div>
arXiv:2412.02091v2 Announce Type: replace 
Abstract: The AI safety literature is full of examples of powerful AI agents that, in blindly pursuing a specific and usually narrow objective, ends up with unacceptable and even catastrophic collateral damage to others. In this paper, we consider the problem of social harms that can result from actions taken by learning and utility-maximising agents in a multi-agent environment. The problem of measuring social harms or impacts in such multi-agent settings, especially when the agents are artificial generally intelligent (AGI) agents, was listed as an open problem in Everitt et al, 2018. We attempt a partial answer to that open problem in the form of market-based mechanisms to quantify and control the cost of such social harms. The proposed setup captures many well-studied special cases and is more general than existing formulations of multi-agent reinforcement learning with mechanism design in two ways: (i) the underlying environment is a history-based general reinforcement learning environment like in AIXI; (ii) the reinforcement-learning agents participating in the environment can have different learning strategies and planning horizons. To demonstrate the practicality of the proposed setup, we survey some key classes of learning algorithms and present a few applications, including a discussion of the Paperclips problem and pollution control with a cap-and-trade system.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigation World Models</title>
<link>https://arxiv.org/abs/2412.03572</link>
<guid>https://arxiv.org/abs/2412.03572</guid>
<content:encoded><![CDATA[
<div> Navigation World Model (NWM)Conditional Diffusion Transformer (CDiT)

:
Navigation World Model (NWM)NWMConditional Diffusion Transformer (CDiT)egocentric10NWMNWMNWM <div>
arXiv:2412.03572v2 Announce Type: replace 
Abstract: Navigation is a fundamental skill of agents with visual-motor capabilities. We introduce a Navigation World Model (NWM), a controllable video generation model that predicts future visual observations based on past observations and navigation actions. To capture complex environment dynamics, NWM employs a Conditional Diffusion Transformer (CDiT), trained on a diverse collection of egocentric videos of both human and robotic agents, and scaled up to 1 billion parameters. In familiar environments, NWM can plan navigation trajectories by simulating them and evaluating whether they achieve the desired goal. Unlike supervised navigation policies with fixed behavior, NWM can dynamically incorporate constraints during planning. Experiments demonstrate its effectiveness in planning trajectories from scratch or by ranking trajectories sampled from an external policy. Furthermore, NWM leverages its learned visual priors to imagine trajectories in unfamiliar environments from a single input image, making it a flexible and powerful tool for next-generation navigation systems.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentBreeder: Mitigating the AI Safety Impact of Multi-Agent Scaffolds via Self-Improvement</title>
<link>https://arxiv.org/abs/2502.00757</link>
<guid>https://arxiv.org/abs/2502.00757</guid>
<content:encoded><![CDATA[
<div> : , , , AgentBreeder, 

<br /><br />:
AgentBreederLLMsAgentBreeder79.4%https://github.com/J-Rosser-UK/AgentBreeder  <div>
arXiv:2502.00757v2 Announce Type: replace 
Abstract: Scaffolding Large Language Models (LLMs) into multi-agent systems often improves performance on complex tasks, but the safety impact of such scaffolds has not been thoroughly explored. We introduce AgentBreeder, a framework for multi-objective self-improving evolutionary search over scaffolds. We evaluate discovered scaffolds on widely recognized reasoning, mathematics, and safety benchmarks and compare them with popular baselines. In 'blue' mode, we see a 79.4% average uplift in safety benchmark performance while maintaining or improving capability scores. In 'red' mode, we find adversarially weak scaffolds emerging concurrently with capability optimization. Our work demonstrates the risks of multi-agent scaffolding and provides a framework for mitigating them. Code is available at https://github.com/J-Rosser-UK/AgentBreeder.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Efficient Flying LoRa Gateways: A Multi-Agent Reinforcement Learning Approach</title>
<link>https://arxiv.org/abs/2502.03377</link>
<guid>https://arxiv.org/abs/2502.03377</guid>
<content:encoded><![CDATA[
<div> (NG-IoT)LoRa(UAV)(MARL)

:
(NG-IoT)IoTLoRa(UAV)LoRaLoRa(POMDP)LoRa GW(MARL)(MAPPO)MARL<br /><br /> <div>
arXiv:2502.03377v2 Announce Type: replace 
Abstract: As next-generation Internet of Things (NG-IoT) networks continue to grow, the number of connected devices is rapidly increasing, along with their energy demands. This creates challenges for resource management and sustainability. Energy-efficient communication, particularly for power-limited IoT devices, is therefore a key research focus. In this paper, we deployed flying LoRa gateways mounted on unmanned aerial vehicles (UAVs) to collect data from LoRa end devices and transmit it to a central server. Our primary objective is to maximize the global system energy efficiency of wireless LoRa networks by joint optimization of transmission power, spreading factor, bandwidth, and user association. To solve this challenging problem, we model the problem as a partially observable Markov decision process (POMDP), where each flying LoRa GW acts as a learning agent using a cooperative multi-agent reinforcement learning (MARL). Simulation results demonstrate that our proposed method, based on the multi-agent proximal policy optimization (MAPPO) algorithm, significantly improves the global system EE and surpasses the conventional MARL schemes.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A-MEM: Agentic Memory for LLM Agents</title>
<link>https://arxiv.org/abs/2502.12110</link>
<guid>https://arxiv.org/abs/2502.12110</guid>
<content:encoded><![CDATA[
<div> :  (LLM)Zettelkasten

<br /><br />:
LLMZettelkastenZettelkastenSOTAGitHub <div>
arXiv:2502.12110v4 Announce Type: replace 
Abstract: While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems' fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution - as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code for evaluating performance is available at https://github.com/WujiangXu/AgenticMemory, while the source code of agentic memory system is available at https://github.com/agiresearch/A-mem.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.12486</link>
<guid>https://arxiv.org/abs/2502.12486</guid>
<content:encoded><![CDATA[
<div> :  (LLMs) (EPO) (RL)

<br /><br />:
EPOEPO EPO EPO  EPO  <div>
arXiv:2502.12486v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown impressive reasoning capabilities in well-defined problems with clear solutions, such as mathematics and coding. However, they still struggle with complex real-world scenarios like business negotiations, which require strategic reasoning-an ability to navigate dynamic environments and align long-term goals amidst uncertainty. Existing methods for strategic reasoning face challenges in adaptability, scalability, and transferring strategies to new contexts. To address these issues, we propose explicit policy optimization (EPO) for strategic reasoning, featuring an LLM that provides strategies in open-ended action space and can be plugged into arbitrary LLM agents to motivate goal-directed behavior. To improve adaptability and policy transferability, we train the strategic reasoning model via multi-turn reinforcement learning (RL) using process rewards and iterative self-play, without supervised fine-tuning (SFT) as a preliminary step. Experiments across social and physical domains demonstrate EPO's ability of long-term goal alignment through enhanced strategic reasoning, achieving state-of-the-art performance on social dialogue and web navigation tasks. Our findings reveal various collaborative reasoning mechanisms emergent in EPO and its effectiveness in generating novel strategies, underscoring its potential for strategic reasoning in real-world applications.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Competitive Posted-Price Mechanism for Online Budget-Feasible Auctions</title>
<link>https://arxiv.org/abs/2502.18265</link>
<guid>https://arxiv.org/abs/2502.18265</guid>
<content:encoded><![CDATA[
<div> : 

:
(Badanidiyuru, KleinbergSinger, EC 2012)OPTOPTOPT <div>
arXiv:2502.18265v2 Announce Type: replace 
Abstract: We consider online procurement auctions, where the agents arrive sequentially, in random order, and have private costs for their services. The buyer aims to maximize a monotone submodular value function for the subset of agents whose services are procured, subject to a budget constraint on their payments. We consider a posted-price setting where upon each agent's arrival, the buyer decides on a payment offered to them. The agent accepts or rejects the offer, depending on whether the payment exceeds their cost, without revealing any other information about their private costs whatsoever. We present a randomized online posted-price mechanism with constant competitive ratio, thus resolving the main open question of (Badanidiyuru, Kleinberg and Singer, EC 2012). Posted-price mechanisms for online procurement typically operate by learning an estimation of the optimal value, denoted as OPT, and using it to determine the payments offered to the agents. The main challenge is to learn OPT within a constant factor from the agents' accept / reject responses to the payments offered. Our approach is based on an online test of whether our estimation is too low compared against OPT and a carefully designed adaptive search that gradually refines our estimation.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Personalized Tool-Augmented LLMs from the Perspectives of Personalization and Proactivity</title>
<link>https://arxiv.org/abs/2503.00771</link>
<guid>https://arxiv.org/abs/2503.00771</guid>
<content:encoded><![CDATA[
<div> ETAPP

:
ETAPP800LLMLLMLLMLLMLLMhttps://github.com/hypasd-art/ETAPP  <div>
arXiv:2503.00771v2 Announce Type: replace 
Abstract: Personalized tool utilization is essential for aligning large language models (LLMs) with user preference in interaction scenarios with various tools. However, most of the current benchmarks primarily focus on either personalization of text generation or direct tool-utilizing, without considering both. In this work, we introduce a novel benchmark ETAPP for evaluating personalized tool invocation, establishing a sandbox environment, and a comprehensive dataset of 800 testing cases covering diverse user profiles. To improve the accuracy of our evaluation, we propose a key-point-based LLM evaluation method, mitigating biases in the LLM-as-a-judge system by manually annotating key points for each test case and providing them to LLM as the reference. Additionally, we evaluate the excellent LLMs and provide an in-depth analysis. Furthermore, we investigate the impact of different tool-invoking strategies on LLMs' personalization performance and the effects of fine-tuning in our task. The effectiveness of our preference-setting and key-point-based evaluation method is also validated. Our findings offer insights into improving personalized LLM agents. Our Code is available at https://github.com/hypasd-art/ETAPP.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AppAgentX: Evolving GUI Agents as Proficient Smartphone Users</title>
<link>https://arxiv.org/abs/2503.02268</link>
<guid>https://arxiv.org/abs/2503.02268</guid>
<content:encoded><![CDATA[
<div> Large Language Models (LLMs), (GUIs), , , 

:
LLMsGUI <div>
arXiv:2503.02268v2 Announce Type: replace 
Abstract: Recent advancements in Large Language Models (LLMs) have led to the development of intelligent LLM-based agents capable of interacting with graphical user interfaces (GUIs). These agents demonstrate strong reasoning and adaptability, enabling them to perform complex tasks that traditionally required predefined rules. However, the reliance on step-by-step reasoning in LLM-based agents often results in inefficiencies, particularly for routine tasks. In contrast, traditional rule-based systems excel in efficiency but lack the intelligence and flexibility to adapt to novel scenarios. To address this challenge, we propose a novel evolutionary framework for GUI agents that enhances operational efficiency while retaining intelligence and flexibility. Our approach incorporates a memory mechanism that records the agent's task execution history. By analyzing this history, the agent identifies repetitive action sequences and evolves high-level actions that act as shortcuts, replacing these low-level operations and improving efficiency. This allows the agent to focus on tasks requiring more complex reasoning, while simplifying routine actions. Experimental results on multiple benchmark tasks demonstrate that our approach significantly outperforms existing methods in both efficiency and accuracy. The code will be open-sourced to support further research.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Majority voting is not good for heaven or hell, with mirrored performance</title>
<link>https://arxiv.org/abs/2401.00592</link>
<guid>https://arxiv.org/abs/2401.00592</guid>
<content:encoded><![CDATA[
<div> ViSE

:
<br />
ViSEVoting in Stochastic Environment$X$$\mu>0$$-X$$\mu$ <div>
arXiv:2401.00592v4 Announce Type: replace-cross 
Abstract: Within the ViSE (Voting in Stochastic Environment) model, we study the effectiveness of majority voting in various environments. By the pit of losses paradox identified in previous work, majority decisions in apparently hostile environments tend to reduce the capital of society. In such cases, the simple social decision rule of "rejecting all proposals without voting" outperforms majority voting. In this paper, we identify another pit of losses appearing in favorable environments. Here, the simple social decision rule of "accepting all proposals without voting" is superior to majority voting. We prove that under a version of simple majority called symmetrized majority and the antisymmetry of the voting body, the second pit of losses is a mirror image of the pit of losses in hostile environments and explain this phenomenon. Technically, we consider a voting society consisting of individualists whose strategy is supporting all proposals that increase their capital and a group (groups) whose members vote to increase the wealth of their group. According to the main result, the expected capital gain of each agent in the environment whose generator $X$ has mean $\mu>0$ exceeds by $\mu$ their expected capital gain under generator $-X$. This result extends to location families of generators with distributions symmetric about their mean. The mentioned result determines the symmetry of the difference between the expected capital gain under the symmetrized majority and that under the "basic" social decision rule that rejects (resp. accepts) all proposals in unfavorable (resp. favorable) environments.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Oscillatory and Excitable Dynamics in an Opinion Model with Group Opinions</title>
<link>https://arxiv.org/abs/2408.13336</link>
<guid>https://arxiv.org/abs/2408.13336</guid>
<content:encoded><![CDATA[
<div> 

:
 <div>
arXiv:2408.13336v2 Announce Type: replace-cross 
Abstract: In traditional models of opinion dynamics, each agent in a network has an opinion and changes in opinions arise from pairwise (i.e., dyadic) interactions between agents. However, in many situations, groups of individuals possess a collective opinion that can differ from the opinions of its constituent individuals. In this paper, we study the effects of group opinions on opinion dynamics. We formulate a hypergraph model in which both individual agents and groups of 3 agents have opinions, and we examine how opinions evolve through both dyadic interactions and group memberships. In some parameter regimes, we find that the presence of group opinions can lead to oscillatory and excitable opinion dynamics. In the oscillatory regime, the mean opinion of the agents in a network has self-sustained oscillations. In the excitable regime, finite-size effects create large but short-lived opinion swings (as in social fads). We develop a mean-field approximation of our model and obtain good agreement with direct numerical simulations. We also show -- both numerically and via our mean-field description -- that oscillatory dynamics occur only when the number of dyadic and polyadic interactions per agent are not completely correlated. Our results illustrate how polyadic structures, such as groups of agents, can have important effects on collective opinion dynamics.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consensus in Multiagent Systems under communication failure</title>
<link>https://arxiv.org/abs/2410.10486</link>
<guid>https://arxiv.org/abs/2410.10486</guid>
<content:encoded><![CDATA[
<div> : 

:
[t, +)Moreau <div>
arXiv:2410.10486v2 Announce Type: replace-cross 
Abstract: We consider multi-agent systems with cooperative interactions and study the convergence to consensus in the case of time-dependent connections, with possible communication failure.
  We prove a new condition ensuring consensus: we define a graph in which directed arrows correspond to connection functions that converge (in the weak sense) to some function with a positive integral on all intervals of the form $[t,+\infty)$. If the graph has a node reachable from all other indices, i.e.~``globally reachable'', then the system converges to consensus. We show that this requirement generalizes some known sufficient conditions for convergence, such as Moreau's or the Persistent Excitation one. We also give a second new condition, transversal to the known ones: total connectedness of the undirected graph formed by the non-vanishing of limiting functions.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScreenSpot-Pro: GUI Grounding for Professional High-Resolution Computer Use</title>
<link>https://arxiv.org/abs/2504.07981</link>
<guid>https://arxiv.org/abs/2504.07981</guid>
<content:encoded><![CDATA[
<div> Multi-modal Large Language Models (MLLMs)GUIScreenSpot-ProScreenSeekeR

:
MLLMsGUIScreenSpot-ProMLLMs23GUI18.9%ScreenSeekeRGUI48.1%GUIhttps://gui-agent.github.io/grounding-leaderboard <div>
arXiv:2504.07981v1 Announce Type: new 
Abstract: Recent advancements in Multi-modal Large Language Models (MLLMs) have led to significant progress in developing GUI agents for general tasks such as web browsing and mobile phone use. However, their application in professional domains remains under-explored. These specialized workflows introduce unique challenges for GUI perception models, including high-resolution displays, smaller target sizes, and complex environments. In this paper, we introduce ScreenSpot-Pro, a new benchmark designed to rigorously evaluate the grounding capabilities of MLLMs in high-resolution professional settings. The benchmark comprises authentic high-resolution images from a variety of professional domains with expert annotations. It spans 23 applications across five industries and three operating systems. Existing GUI grounding models perform poorly on this dataset, with the best model achieving only 18.9%. Our experiments reveal that strategically reducing the search area enhances accuracy. Based on this insight, we propose ScreenSeekeR, a visual search method that utilizes the GUI knowledge of a strong planner to guide a cascaded search, achieving state-of-the-art performance with 48.1% without any additional training. We hope that our benchmark and findings will advance the development of GUI agents for professional applications. Code, data and leaderboard can be found at https://gui-agent.github.io/grounding-leaderboard.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuron-level Balance between Stability and Plasticity in Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.08000</link>
<guid>https://arxiv.org/abs/2504.08000</guid>
<content:encoded><![CDATA[
<div> : -NBSP

:<br />
Neuron-level Balance between Stability and PlasticityNBSPDRL-NBSPRLNBSPMeta-WorldAtari <div>
arXiv:2504.08000v1 Announce Type: new 
Abstract: In contrast to the human ability to continuously acquire knowledge, agents struggle with the stability-plasticity dilemma in deep reinforcement learning (DRL), which refers to the trade-off between retaining existing skills (stability) and learning new knowledge (plasticity). Current methods focus on balancing these two aspects at the network level, lacking sufficient differentiation and fine-grained control of individual neurons. To overcome this limitation, we propose Neuron-level Balance between Stability and Plasticity (NBSP) method, by taking inspiration from the observation that specific neurons are strongly relevant to task-relevant skills. Specifically, NBSP first (1) defines and identifies RL skill neurons that are crucial for knowledge retention through a goal-oriented method, and then (2) introduces a framework by employing gradient masking and experience replay techniques targeting these neurons to preserve the encoded existing skills while enabling adaptation to new tasks. Numerous experimental results on the Meta-World and Atari benchmarks demonstrate that NBSP significantly outperforms existing approaches in balancing stability and plasticity.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search</title>
<link>https://arxiv.org/abs/2504.08066</link>
<guid>https://arxiv.org/abs/2504.08066</guid>
<content:encoded><![CDATA[
<div> : AI-v2

:
AI-v2v1AI-v2-VLMAIAI-v2ICLRAIAIhttps://github.com/SakanaAI/AI-Scientist-v2AIAI <div>
arXiv:2504.08066v1 Announce Type: new 
Abstract: AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test Amplification for REST APIs via Single and Multi-Agent LLM Systems</title>
<link>https://arxiv.org/abs/2504.08113</link>
<guid>https://arxiv.org/abs/2504.08113</guid>
<content:encoded><![CDATA[
<div> REST APIsAPI

:
RESTful APIAPIAPILLMREST APIs <div>
arXiv:2504.08113v1 Announce Type: new 
Abstract: REST APIs (Representational State Transfer Application Programming Interfaces) are essential to modern cloud-native applications. Strong and automated test cases are crucial to expose lurking bugs in the API. However, creating automated tests for REST APIs is difficult, and it requires test cases that explore the protocol's boundary conditions. In this paper, we investigate how single-agent and multi-agent LLM (Large Language Model) systems can amplify a REST API test suite. Our evaluation demonstrates increased API coverage, identification of numerous bugs in the API under test, and insights into the computational cost and energy consumption of both approaches.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Reinforcement Learning-based Sustainable Multi-User Computation Offloading for Mobile Edge-Quantum Computing</title>
<link>https://arxiv.org/abs/2504.08134</link>
<guid>https://arxiv.org/abs/2504.08134</guid>
<content:encoded><![CDATA[
<div> : (MEQC), -, , , , 

:
(MEQC)MEQC--MEQC30% <div>
arXiv:2504.08134v1 Announce Type: new 
Abstract: Exploiting quantum computing at the mobile edge holds immense potential for facilitating large-scale network design, processing multimodal data, optimizing resource management, and enhancing network security. In this paper, we propose a pioneering paradigm of mobile edge quantum computing (MEQC) that integrates quantum computing capabilities into classical edge computing servers that are proximate to mobile devices. To conceptualize the MEQC, we first design an MEQC system, where mobile devices can offload classical and quantum computation tasks to edge servers equipped with classical and quantum computers. We then formulate the hybrid classical-quantum computation offloading problem whose goal is to minimize system cost in terms of latency and energy consumption. To solve the offloading problem efficiently, we propose a hybrid discrete-continuous multi-agent reinforcement learning algorithm to learn long-term sustainable offloading and partitioning strategies. Finally, numerical results demonstrate that the proposed algorithm can reduce the MEQC system cost by up to 30% compared to existing baselines.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Orchestrating Agents and Data for Enterprise: A Blueprint Architecture for Compound AI</title>
<link>https://arxiv.org/abs/2504.08148</link>
<guid>https://arxiv.org/abs/2504.08148</guid>
<content:encoded><![CDATA[
<div> : , AI, , , 

:
AIAPIAI <div>
arXiv:2504.08148v1 Announce Type: new 
Abstract: Large language models (LLMs) have gained significant interest in industry due to their impressive capabilities across a wide range of tasks. However, the widespread adoption of LLMs presents several challenges, such as integration into existing applications and infrastructure, utilization of company proprietary data, models, and APIs, and meeting cost, quality, responsiveness, and other requirements. To address these challenges, there is a notable shift from monolithic models to compound AI systems, with the premise of more powerful, versatile, and reliable applications. However, progress thus far has been piecemeal, with proposals for agentic workflows, programming models, and extended LLM capabilities, without a clear vision of an overall architecture. In this paper, we propose a 'blueprint architecture' for compound AI systems for orchestrating agents and data for enterprise applications. In our proposed architecture the key orchestration concept is 'streams' to coordinate the flow of data and instructions among agents. Existing proprietary models and APIs in the enterprise are mapped to 'agents', defined in an 'agent registry' that serves agent metadata and learned representations for search and planning. Agents can utilize proprietary data through a 'data registry' that similarly registers enterprise data of various modalities. Tying it all together, data and task 'planners' break down, map, and optimize tasks and queries for given quality of service (QoS) requirements such as cost, accuracy, and latency. We illustrate an implementation of the architecture for a use-case in the HR domain and discuss opportunities and challenges for 'agentic AI' in the enterprise.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Foundations for Continual Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.08161</link>
<guid>https://arxiv.org/abs/2504.08161</guid>
<content:encoded><![CDATA[
<div> Markov

<br /><br />:
RLMarkovEpisodic <div>
arXiv:2504.08161v1 Announce Type: new 
Abstract: Algorithms and approaches for continual reinforcement learning have gained increasing attention. Much of this early progress rests on the foundations and standard practices of traditional reinforcement learning, without questioning if they are well-suited to the challenges of continual learning agents. We suggest that many core foundations of traditional RL are, in fact, antithetical to the goals of continual reinforcement learning. We enumerate four such foundations: the Markov decision process formalism, a focus on optimal policies, the expected sum of rewards as the primary evaluation metric, and episodic benchmark environments that embrace the other three foundations. Shedding such sacredly held and taught concepts is not easy. They are self-reinforcing in that each foundation depends upon and holds up the others, making it hard to rethink each in isolation. We propose an alternative set of all four foundations that are better suited to the continual learning setting. We hope to spur on others in rethinking the traditional foundations, proposing and critiquing alternatives, and developing new algorithms and approaches enabled by better-suited foundations.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Based Deep Reinforcement Learning Aided by Transformers for Multi-Agent Cooperation</title>
<link>https://arxiv.org/abs/2504.08195</link>
<guid>https://arxiv.org/abs/2504.08195</guid>
<content:encoded><![CDATA[
<div> 

:

GNNDRLGNNQDouble DQNPSODQN90%100%200600 <div>
arXiv:2504.08195v1 Announce Type: new 
Abstract: Mission planning for a fleet of cooperative autonomous drones in applications that involve serving distributed target points, such as disaster response, environmental monitoring, and surveillance, is challenging, especially under partial observability, limited communication range, and uncertain environments. Traditional path-planning algorithms struggle in these scenarios, particularly when prior information is not available. To address these challenges, we propose a novel framework that integrates Graph Neural Networks (GNNs), Deep Reinforcement Learning (DRL), and transformer-based mechanisms for enhanced multi-agent coordination and collective task execution. Our approach leverages GNNs to model agent-agent and agent-goal interactions through adaptive graph construction, enabling efficient information aggregation and decision-making under constrained communication. A transformer-based message-passing mechanism, augmented with edge-feature-enhanced attention, captures complex interaction patterns, while a Double Deep Q-Network (Double DQN) with prioritized experience replay optimizes agent policies in partially observable environments. This integration is carefully designed to address specific requirements of multi-agent navigation, such as scalability, adaptability, and efficient task execution. Experimental results demonstrate superior performance, with 90% service provisioning and 100% grid coverage (node discovery), while reducing the average steps per episode to 200, compared to 600 for benchmark methods such as particle swarm optimization (PSO), greedy algorithms and DQN.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in Healthcare</title>
<link>https://arxiv.org/abs/2504.08260</link>
<guid>https://arxiv.org/abs/2504.08260</guid>
<content:encoded><![CDATA[
<div> : 

:
Understanding America Study(UAS)LLMLLMLlama 3UAS <div>
arXiv:2504.08260v1 Announce Type: new 
Abstract: Generative agents have been increasingly used to simulate human behaviour in silico, driven by large language models (LLMs). These simulacra serve as sandboxes for studying human behaviour without compromising privacy or safety. However, it remains unclear whether such agents can truly represent real individuals. This work compares survey data from the Understanding America Study (UAS) on healthcare decision-making with simulated responses from generative agents. Using demographic-based prompt engineering, we create digital twins of survey respondents and analyse how well different LLMs reproduce real-world behaviours. Our findings show that some LLMs fail to reflect realistic decision-making, such as predicting universal vaccine acceptance. However, Llama 3 captures variations across race and Income more accurately but also introduces biases not present in the UAS data. This study highlights the potential of generative agents for behavioural research while underscoring the risks of bias from both LLMs and prompting strategies.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Stabilizing Weakly Byzantine Perpetual Gathering of Mobile Agents</title>
<link>https://arxiv.org/abs/2504.08271</link>
<guid>https://arxiv.org/abs/2504.08271</guid>
<content:encoded><![CDATA[
<div> Byzantine

:
$k$$f$Byzantine$n$

(1)(2)$k\leq 2f$$n$$k$$f$ID$\Lambda_g$$\Lambda_g$$n$$k$$f$$N$$K$$F$$O(K\cdot F\cdot \Lambda_g\cdot X(N))$$X(n)$$n$$k$$f$ <div>
arXiv:2504.08271v1 Announce Type: new 
Abstract: We study the \emph{Byzantine} gathering problem involving $k$ mobile agents with unique identifiers (IDs), $f$ of which are Byzantine. These agents start the execution of a common algorithm from (possibly different) nodes in an $n$-node network, potentially starting at different times. Once started, the agents operate in synchronous rounds. We focus on \emph{weakly} Byzantine environments, where Byzantine agents can behave arbitrarily but cannot falsify their IDs. The goal is for all \emph{non-Byzantine} agents to eventually terminate at a single node simultaneously.
  In this paper, we first prove two impossibility results: (1) for any number of non-Byzantine agents, no algorithm can solve this problem without global knowledge of the network size or the number of agents, and (2) no self-stabilizing algorithm exists if $k\leq 2f$ even with $n$, $k$, $f$, and the length $\Lambda_g$ of the largest ID among IDs of non-Byzantine agents, where the self-stabilizing algorithm enables agents to gather starting from arbitrary (inconsistent) initial states. Next, based on these results, we introduce a \emph{perpetual gathering} problem and propose a self-stabilizing algorithm for this problem. This problem requires that all non-Byzantine agents always be co-located from a certain time onwards. If the agents know $\Lambda_g$ and upper bounds $N$, $K$, $F$ on $n$, $k$, $f$, the proposed algorithm works in $O(K\cdot F\cdot \Lambda_g\cdot X(N))$ rounds, where $X(n)$ is the time required to visit all nodes in a $n$-nodes network. Our results indicate that while no algorithm can solve the original self-stabilizing gathering problem for any $k$ and $f$ even with \emph{exact} global knowledge of the network size and the number of agents, the self-stabilizing perpetual gathering problem can always be solved with just upper bounds on this knowledge.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSM: Building A Diverse Semantic Map for 3D Visual Grounding</title>
<link>https://arxiv.org/abs/2504.08307</link>
<guid>https://arxiv.org/abs/2504.08307</guid>
<content:encoded><![CDATA[
<div> (VLMs)3D(DSM)

<br /><br />:

VLMs3D3D(DSM)VLMsDSMDSMDSM-Grounding3D <div>
arXiv:2504.08307v1 Announce Type: new 
Abstract: In recent years, with the growing research and application of multimodal large language models (VLMs) in robotics, there has been an increasing trend of utilizing VLMs for robotic scene understanding tasks. Existing approaches that use VLMs for 3D Visual Grounding tasks often focus on obtaining scene information through geometric and visual information, overlooking the extraction of diverse semantic information from the scene and the understanding of rich implicit semantic attributes, such as appearance, physics, and affordance. The 3D scene graph, which combines geometry and language, is an ideal representation method for environmental perception and is an effective carrier for language models in 3D Visual Grounding tasks. To address these issues, we propose a diverse semantic map construction method specifically designed for robotic agents performing 3D Visual Grounding tasks. This method leverages VLMs to capture the latent semantic attributes and relations of objects within the scene and creates a Diverse Semantic Map (DSM) through a geometry sliding-window map construction strategy. We enhance the understanding of grounding information based on DSM and introduce a novel approach named DSM-Grounding. Experimental results show that our method outperforms current approaches in tasks like semantic segmentation and 3D Visual Grounding, particularly excelling in overall metrics compared to the state-of-the-art. In addition, we have deployed this method on robots to validate its effectiveness in navigation and grasping tasks.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PCA-RAG: Principal Component Analysis for Efficient Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2504.08386</link>
<guid>https://arxiv.org/abs/2504.08386</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation (RAG)Principal Component Analysis (PCA)embedding dimensionality reductioncomputational bottlenecksfinancial text corpora

:
(PCA)(RAG)PCA3,0721106028.6PCAZanista AINewswitch <div>
arXiv:2504.08386v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for grounding large language models in external knowledge sources, improving the precision of agents responses. However, high-dimensional language model embeddings, often in the range of hundreds to thousands of dimensions, can present scalability challenges in terms of storage and latency, especially when processing massive financial text corpora. This paper investigates the use of Principal Component Analysis (PCA) to reduce embedding dimensionality, thereby mitigating computational bottlenecks without incurring large accuracy losses. We experiment with a real-world dataset and compare different similarity and distance metrics under both full-dimensional and PCA-compressed embeddings. Our results show that reducing vectors from 3,072 to 110 dimensions provides a sizeable (up to $60\times$) speedup in retrieval operations and a $\sim 28.6\times$ reduction in index size, with only moderate declines in correlation metrics relative to human-annotated similarity scores. These findings demonstrate that PCA-based compression offers a viable balance between retrieval fidelity and resource efficiency, essential for real-time systems such as Zanista AI's \textit{Newswitch} platform. Ultimately, our study underscores the practicality of leveraging classical dimensionality reduction techniques to scale RAG architectures for knowledge-intensive applications in finance and trading, where speed, memory efficiency, and accuracy must jointly be optimized.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MineWorld: a Real-Time and Open-Source Interactive World Model on Minecraft</title>
<link>https://arxiv.org/abs/2504.08388</link>
<guid>https://arxiv.org/abs/2504.08388</guid>
<content:encoded><![CDATA[
<div> : World modeling, Minecraft, interactive world model, Transformer, real-time interaction

<br /><br />:
MineWorldMinecraftMineWorld-TransformerID47MineWorld <div>
arXiv:2504.08388v1 Announce Type: new 
Abstract: World modeling is a crucial task for enabling intelligent agents to effectively interact with humans and operate in dynamic environments. In this work, we propose MineWorld, a real-time interactive world model on Minecraft, an open-ended sandbox game which has been utilized as a common testbed for world modeling. MineWorld is driven by a visual-action autoregressive Transformer, which takes paired game scenes and corresponding actions as input, and generates consequent new scenes following the actions. Specifically, by transforming visual game scenes and actions into discrete token ids with an image tokenizer and an action tokenizer correspondingly, we consist the model input with the concatenation of the two kinds of ids interleaved. The model is then trained with next token prediction to learn rich representations of game states as well as the conditions between states and actions simultaneously. In inference, we develop a novel parallel decoding algorithm that predicts the spatial redundant tokens in each frame at the same time, letting models in different scales generate $4$ to $7$ frames per second and enabling real-time interactions with game players. In evaluation, we propose new metrics to assess not only visual quality but also the action following capacity when generating new scenes, which is crucial for a world model. Our comprehensive evaluation shows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion based world models significantly. The code and model have been released.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models</title>
<link>https://arxiv.org/abs/2504.08399</link>
<guid>https://arxiv.org/abs/2504.08399</guid>
<content:encoded><![CDATA[
<div> Big Five

<br />
: LLMsLLMLLMLLMs5-7LLM <div>
arXiv:2504.08399v1 Announce Type: new 
Abstract: There is a growing interest in assessing the personality traits of Large language models (LLMs). However, traditional personality assessments based on self-report questionnaires may fail to capture their true behavioral nuances due to inherent biases and meta-knowledge contamination. This paper introduces a novel multi-observer framework for LLM personality assessment that draws inspiration from informant-report methods in psychology. Instead of relying solely on self-assessments, our approach employs multiple observer agents configured with a specific relationship context (e.g., family, friend, or workplace) to simulate interactive scenarios with a subject LLM. These observers engage in dialogues and subsequently provide ratings across the Big Five personality dimensions. Our experiments reveal that LLMs possess systematic biases in self-report personality ratings. Moreover, aggregating observer ratings effectively reduces non-systematic biases and achieves optimal reliability with 5-7 observers. The findings highlight the significant impact of relationship context on personality perception and demonstrate that a multi-observer paradigm yields a more robust and context-sensitive evaluation of LLM personality traits.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Belief States for Cooperative Multi-Agent Reinforcement Learning under Partial Observability</title>
<link>https://arxiv.org/abs/2504.08417</link>
<guid>https://arxiv.org/abs/2504.08417</guid>
<content:encoded><![CDATA[
<div> 

:
 <div>
arXiv:2504.08417v1 Announce Type: new 
Abstract: Reinforcement learning in partially observable environments is typically challenging, as it requires agents to learn an estimate of the underlying system state. These challenges are exacerbated in multi-agent settings, where agents learn simultaneously and influence the underlying state as well as each others' observations. We propose the use of learned beliefs on the underlying state of the system to overcome these challenges and enable reinforcement learning with fully decentralized training and execution. Our approach leverages state information to pre-train a probabilistic belief model in a self-supervised fashion. The resulting belief states, which capture both inferred state information as well as uncertainty over this information, are then used in a state-based reinforcement learning algorithm to create an end-to-end model for cooperative multi-agent reinforcement learning under partial observability. By separating the belief and reinforcement learning tasks, we are able to significantly simplify the policy and value function learning tasks and improve both the convergence speed and the final performance. We evaluate our proposed method on diverse partially observable multi-agent tasks designed to exhibit different variants of partial observability.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid ABM-PDE Framework for Real-World Infectious Disease Simulations</title>
<link>https://arxiv.org/abs/2504.08430</link>
<guid>https://arxiv.org/abs/2504.08430</guid>
<content:encoded><![CDATA[
<div> hybrid modeling approachAgent-Based Model (ABM)partial differential equation (PDE)

:
Agent-Based Model (ABM)(PDE)ABM-PDEABM25%100%ABMPDEPDE- <div>
arXiv:2504.08430v1 Announce Type: new 
Abstract: This paper presents a hybrid modeling approach that couples an Agent-Based Model (ABM) with a partial differential equation (PDE) model in an epidemic setting to simulate the spatial spread of infectious diseases using a compartmental structure with seven health states. The goal is to reduce the computational complexity of a full-ABM by introducing a coupled ABM-PDE model that offers significantly faster simulations while maintaining comparable accuracy. Our results demonstrate that the hybrid model not only reduces the overall simulation runtime (defined as the number of runs required for stable results multiplied by the duration of a single run) but also achieves smaller errors across both 25% and 100% population samples. The coupling mechanism ensures consistency at the model interface: agents crossing from the ABM into the PDE domain are removed and represented as density contributions at the corresponding grid node, while surplus density in the PDE domain is used to generate agents with plausible trajectories derived from mobile phone data. We evaluate the hybrid model using real-world mobility and infection data for the Berlin-Brandenburg region in Germany, showing that it captures the core epidemiological dynamics while enabling efficient large-scale simulations.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speech Command + Speech Emotion: Exploring Emotional Speech Commands as a Compound and Playful Modality</title>
<link>https://arxiv.org/abs/2504.08440</link>
<guid>https://arxiv.org/abs/2504.08440</guid>
<content:encoded><![CDATA[
<div> : 

:
AIAIN=14 <div>
arXiv:2504.08440v1 Announce Type: new 
Abstract: In an era of human-computer interaction with increasingly agentic AI systems capable of connecting with users conversationally, speech is an important modality for commanding agents. By recognizing and using speech emotions (i.e., how a command is spoken), we can provide agents with the ability to emotionally accentuate their responses and socially enrich users' perceptions and experiences. To explore the concept and impact of speech emotion commands on user perceptions, we realized a prototype and conducted a user study (N = 14) where speech commands are used to steer two vehicles in a minimalist and retro game style implementation. While both agents execute user commands, only one of the agents uses speech emotion information to adapt its execution behavior. We report on differences in how users perceived each agent, including significant differences in stimulation and dependability, outline implications for designing interactions with agents using emotional speech commands, and provide insights on how users consciously emote, which we describe as "voice acting".
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Path Connected Dynamic Graphs with a Study of Dispersion and Exploration</title>
<link>https://arxiv.org/abs/2504.08474</link>
<guid>https://arxiv.org/abs/2504.08474</guid>
<content:encoded><![CDATA[
<div> $T$-$T$-dispersion

<br /><br />:
$T$-$T$$T$-$T$-dispersion1-dispersiondispersiondispersion <div>
arXiv:2504.08474v1 Announce Type: new 
Abstract: In dynamic graphs, edges may be added or deleted in each synchronous round. Various connectivity models exist based on constraints on these changes. One well-known model is the $T$-Interval Connectivity model, where the graph remains connected in every round, and the parameter $T$ reflects the duration of structural stability. Another model is Connectivity Time, where the union of edges across any $T$ consecutive rounds forms a connected graph. This is a weaker model, as the graph may be disconnected in individual rounds.
  In this work, we introduce a new connectivity model called $T$-Path Connectivity. Unlike $T$-Interval Connectivity, the graph may not be connected in each round, but for every pair of nodes $u,v$, there must exist a path connecting them in at least one round within any $T$ consecutive rounds. This model is strictly weaker than $T$-Interval Connectivity but stronger than the Connectivity Time model.
  We study the dispersion problem in the $T$-Path Connectivity model. While dispersion has been explored in the 1-Interval Connectivity model, we show that the existing algorithm with termination does not work in our model. We then identify the minimal necessary assumptions required to solve dispersion in the $T$-Path Connectivity model and provide an algorithm that solves it optimally under those conditions. Moreover, we prove that dispersion is unsolvable in the Connectivity Time model, even under several strong assumptions.
  We further initiate the study of the exploration problem under all three connectivity models. We present multiple impossibility results and, in most cases, establish tight bounds on the number of agents and time required. Our results demonstrate that, in both dispersion and exploration, the Connectivity Time model is strictly the weakest among the three.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adopting Large Language Models to Automated System Integration</title>
<link>https://arxiv.org/abs/2504.08490</link>
<guid>https://arxiv.org/abs/2504.08490</guid>
<content:encoded><![CDATA[
<div> : WebCompositio Prompto

:
LLMs(1)LLMsCompositio Prompto(2)RAG(3)(4)Compositio PromptoRAGLLM <div>
arXiv:2504.08490v1 Announce Type: new 
Abstract: Modern enterprise computing systems integrate numerous subsystems to resolve a common task by yielding emergent behavior. A widespread approach is using services implemented with Web technologies like REST or OpenAPI, which offer an interaction mechanism and service documentation standard, respectively. Each service represents a specific business functionality, allowing encapsulation and easier maintenance. Despite the reduced maintenance costs on an individual service level, increased integration complexity arises. Consequently, automated service composition approaches have arisen to mitigate this issue. Nevertheless, these approaches have not achieved high acceptance in practice due to their reliance on complex formal modeling. Within this Ph.D. thesis, we analyze the application of Large Language Models (LLMs) to automatically integrate the services based on a natural language input. The result is a reusable service composition, e.g., as program code. While not always generating entirely correct results, the result can still be helpful by providing integration engineers with a close approximation of a suitable solution, which requires little effort to become operational. Our research involves (i) introducing a software architecture for automated service composition using LLMs, (ii) analyzing Retrieval Augmented Generation (RAG) for service discovery, (iii) proposing a novel natural language query-based benchmark for service discovery, and (iv) extending the benchmark to complete service composition scenarios. We have presented our software architecture as Compositio Prompto, the analysis of RAG for service discovery, and submitted a proposal for the service discovery benchmark. Open topics are primarily the extension of the service discovery benchmark to service composition scenarios and the improvements of the service composition generation, e.g., using fine-tuning or LLM agents.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM Agent Tasks</title>
<link>https://arxiv.org/abs/2504.08525</link>
<guid>https://arxiv.org/abs/2504.08525</guid>
<content:encoded><![CDATA[
<div> :  (LLMs) (TME) (TMT)

<br /><br />:

TMELLMsTMETMTLLMTMETMEhttps://github.com/biubiutomato/TME-Agent <div>
arXiv:2504.08525v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used as autonomous agents for multi-step tasks. However, most existing frameworks fail to maintain a structured understanding of the task state, often relying on linear prompt concatenation or shallow memory buffers. This leads to brittle performance, frequent hallucinations, and poor long-range coherence. In this work, we propose the Task Memory Engine (TME), a lightweight and structured memory module that tracks task execution using a hierarchical Task Memory Tree (TMT). Each node in the tree corresponds to a task step, storing relevant input, output, status, and sub-task relationships. We introduce a prompt synthesis method that dynamically generates LLM prompts based on the active node path, significantly improving execution consistency and contextual grounding. Through case studies and comparative experiments on multi-step agent tasks, we demonstrate that TME leads to better task completion accuracy and more interpretable behavior with minimal implementation overhead. The full implementation of TME is available at https://github.com/biubiutomato/TME-Agent.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied Image Captioning: Self-supervised Learning Agents for Spatially Coherent Image Descriptions</title>
<link>https://arxiv.org/abs/2504.08531</link>
<guid>https://arxiv.org/abs/2504.08531</guid>
<content:encoded><![CDATA[
<div> : 

:
-https://hsp-iit.github.io/embodied-captioning/ <div>
arXiv:2504.08531v1 Announce Type: new 
Abstract: We present a self-supervised method to improve an agent's abilities in describing arbitrary objects while actively exploring a generic environment. This is a challenging problem, as current models struggle to obtain coherent image captions due to different camera viewpoints and clutter. We propose a three-phase framework to fine-tune existing captioning models that enhances caption accuracy and consistency across views via a consensus mechanism. First, an agent explores the environment, collecting noisy image-caption pairs. Then, a consistent pseudo-caption for each object instance is distilled via consensus using a large language model. Finally, these pseudo-captions are used to fine-tune an off-the-shelf captioning model, with the addition of contrastive learning. We analyse the performance of the combination of captioning models, exploration policies, pseudo-labeling methods, and fine-tuning strategies, on our manually labeled test set. Results show that a policy can be trained to mine samples with higher disagreement compared to classical baselines. Our pseudo-captioning method, in combination with all policies, has a higher semantic similarity compared to other existing methods, and fine-tuning improves caption accuracy and consistency by a significant margin. Code and test set annotations available at https://hsp-iit.github.io/embodied-captioning/
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FMLGS: Fast Multilevel Language Embedded Gaussians for Part-level Interactive Agents</title>
<link>https://arxiv.org/abs/2504.08581</link>
<guid>https://arxiv.org/abs/2504.08581</guid>
<content:encoded><![CDATA[
<div> 3DFMLGS

<br />
:

FMLGS3D3D3DGSSegment Anything Model 2SAM2FMLGSFMLGSLERF98LangSplat4LEGaussians2.5FMLGS3D <div>
arXiv:2504.08581v1 Announce Type: new 
Abstract: The semantically interactive radiance field has long been a promising backbone for 3D real-world applications, such as embodied AI to achieve scene understanding and manipulation. However, multi-granularity interaction remains a challenging task due to the ambiguity of language and degraded quality when it comes to queries upon object components. In this work, we present FMLGS, an approach that supports part-level open-vocabulary query within 3D Gaussian Splatting (3DGS). We propose an efficient pipeline for building and querying consistent object- and part-level semantics based on Segment Anything Model 2 (SAM2). We designed a semantic deviation strategy to solve the problem of language ambiguity among object parts, which interpolates the semantic features of fine-grained targets for enriched information. Once trained, we can query both objects and their describable parts using natural language. Comparisons with other state-of-the-art methods prove that our method can not only better locate specified part-level targets, but also achieve first-place performance concerning both speed and accuracy, where FMLGS is 98 x faster than LERF, 4 x faster than LangSplat and 2.5 x faster than LEGaussians. Meanwhile, we further integrate FMLGS as a virtual agent that can interactively navigate through 3D scenes, locate targets, and respond to user demands through a chat interface, which demonstrates the potential of our work to be further expanded and applied in the future.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MooseAgent: A LLM Based Multi-agent Framework for Automating Moose Simulation</title>
<link>https://arxiv.org/abs/2504.08621</link>
<guid>https://arxiv.org/abs/2504.08621</guid>
<content:encoded><![CDATA[
<div> : (FEM), MOOSE, (LLMs), , MooseAgent

:
MooseAgentMOOSEMooseAgentMOOSEMOOSEMooseAgentMOOSEMOOSEMooseAgenthttps://github.com/taozhan18/MooseAgent <div>
arXiv:2504.08621v1 Announce Type: new 
Abstract: The Finite Element Method (FEM) is widely used in engineering and scientific computing, but its pre-processing, solver configuration, and post-processing stages are often time-consuming and require specialized knowledge. This paper proposes an automated solution framework, MooseAgent, for the multi-physics simulation framework MOOSE, which combines large-scale pre-trained language models (LLMs) with a multi-agent system. The framework uses LLMs to understand user-described simulation requirements in natural language and employs task decomposition and multi-round iterative verification strategies to automatically generate MOOSE input files. To improve accuracy and reduce model hallucinations, the system builds and utilizes a vector database containing annotated MOOSE input cards and function documentation. We conducted experimental evaluations on several typical cases, including heat transfer, mechanics, phase field, and multi-physics coupling. The results show that MooseAgent can automate the MOOSE simulation process to a certain extent, especially demonstrating a high success rate when dealing with relatively simple single-physics problems. The main contribution of this research is the proposal of a multi-agent automated framework for MOOSE, which validates its potential in simplifying finite element simulation processes and lowering the user barrier, providing new ideas for the development of intelligent finite element simulation software. The code for the MooseAgent framework proposed in this paper has been open-sourced and is available at https://github.com/taozhan18/MooseAgent
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLMs trust AI regulation? Emerging behaviour of game-theoretic LLM agents</title>
<link>https://arxiv.org/abs/2504.08640</link>
<guid>https://arxiv.org/abs/2504.08640</guid>
<content:encoded><![CDATA[
<div> : 

:
LLMAILLMAIAILLMAILLM <div>
arXiv:2504.08640v1 Announce Type: new 
Abstract: There is general agreement that fostering trust and cooperation within the AI development ecosystem is essential to promote the adoption of trustworthy AI systems. By embedding Large Language Model (LLM) agents within an evolutionary game-theoretic framework, this paper investigates the complex interplay between AI developers, regulators and users, modelling their strategic choices under different regulatory scenarios. Evolutionary game theory (EGT) is used to quantitatively model the dilemmas faced by each actor, and LLMs provide additional degrees of complexity and nuances and enable repeated games and incorporation of personality traits. Our research identifies emerging behaviours of strategic AI agents, which tend to adopt more "pessimistic" (not trusting and defective) stances than pure game-theoretic agents. We observe that, in case of full trust by users, incentives are effective to promote effective regulation; however, conditional trust may deteriorate the "social pact". Establishing a virtuous feedback between users' trust and regulators' reputation thus appears to be key to nudge developers towards creating safe AI. However, the level at which this trust emerges may depend on the specific LLM used for testing. Our results thus provide guidance for AI regulation systems, and help predict the outcome of strategic LLM agents, should they be used to aid regulation itself.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voice Interaction With Conversational AI Could Facilitate Thoughtful Reflection and Substantive Revision in Writing</title>
<link>https://arxiv.org/abs/2504.08687</link>
<guid>https://arxiv.org/abs/2504.08687</guid>
<content:encoded><![CDATA[
<div> : arXiv, , , , 

:
LLMs session  LLM  LLM  <div>
arXiv:2504.08687v1 Announce Type: new 
Abstract: Writing well requires not only expressing ideas but also refining them through revision, a process facilitated by reflection. Prior research suggests that feedback delivered through dialogues, such as those in writing center tutoring sessions, can help writers reflect more thoughtfully on their work compared to static feedback. Recent advancements in multi-modal large language models (LLMs) now offer new possibilities for supporting interactive and expressive voice-based reflection in writing. In particular, we propose that LLM-generated static feedback can be repurposed as conversation starters, allowing writers to seek clarification, request examples, and ask follow-up questions, thereby fostering deeper reflection on their writing. We argue that voice-based interaction can naturally facilitate this conversational exchange, encouraging writers' engagement with higher-order concerns, facilitating iterative refinement of their reflections, and reduce cognitive load compared to text-based interactions. To investigate these effects, we propose a formative study exploring how text vs. voice input influence writers' reflection and subsequent revisions. Findings from this study will inform the design of intelligent and interactive writing tools, offering insights into how voice-based interactions with LLM-powered conversational agents can support reflection and revision.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TP-RAG: Benchmarking Retrieval-Augmented Large Language Model Agents for Spatiotemporal-Aware Travel Planning</title>
<link>https://arxiv.org/abs/2504.08694</link>
<guid>https://arxiv.org/abs/2504.08694</guid>
<content:encoded><![CDATA[
<div> : , , , TP-RAG, EvoRAG

:
TP-RAGTP-RAGEvoRAGWebLLM <div>
arXiv:2504.08694v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown promise in automating travel planning, yet they often fall short in addressing nuanced spatiotemporal rationality. While existing benchmarks focus on basic plan validity, they neglect critical aspects such as route efficiency, POI appeal, and real-time adaptability. This paper introduces TP-RAG, the first benchmark tailored for retrieval-augmented, spatiotemporal-aware travel planning. Our dataset includes 2,348 real-world travel queries, 85,575 fine-grain annotated POIs, and 18,784 high-quality travel trajectory references sourced from online tourist documents, enabling dynamic and context-aware planning. Through extensive experiments, we reveal that integrating reference trajectories significantly improves spatial efficiency and POI rationality of the travel plan, while challenges persist in universality and robustness due to conflicting references and noisy data. To address these issues, we propose EvoRAG, an evolutionary framework that potently synergizes diverse retrieved trajectories with LLMs' intrinsic reasoning. EvoRAG achieves state-of-the-art performance, improving spatiotemporal compliance and reducing commonsense violation compared to ground-up and retrieval-augmented baselines. Our work underscores the potential of hybridizing Web knowledge with LLM-driven optimization, paving the way for more reliable and adaptive travel planning agents.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeaView: Software Engineering Agent Visual Interface for Enhanced Workflow</title>
<link>https://arxiv.org/abs/2504.08696</link>
<guid>https://arxiv.org/abs/2504.08696</guid>
<content:encoded><![CDATA[
<div> Auto-regressive LLMSWESeaView

:
<br />
LLMSWEGitHubSWESeaViewSWESeaViewLLMLLMSeaView1030301 <div>
arXiv:2504.08696v1 Announce Type: new 
Abstract: Auto-regressive LLM-based software engineering (SWE) agents, henceforth SWE agents, have made tremendous progress (>60% on SWE-Bench Verified) on real-world coding challenges including GitHub issue resolution. SWE agents use a combination of reasoning, environment interaction and self-reflection to resolve issues thereby generating "trajectories". Analysis of SWE agent trajectories is difficult, not only as they exceed LLM sequence length (sometimes, greater than 128k) but also because it involves a relatively prolonged interaction between an LLM and the environment managed by the agent. In case of an agent error, it can be hard to decipher, locate and understand its scope. Similarly, it can be hard to track improvements or regression over multiple runs or experiments. While a lot of research has gone into making these SWE agents reach state-of-the-art, much less focus has been put into creating tools to help analyze and visualize agent output. We propose a novel tool called SeaView: Software Engineering Agent Visual Interface for Enhanced Workflow, with a vision to assist SWE-agent researchers to visualize and inspect their experiments. SeaView's novel mechanisms help compare experimental runs with varying hyper-parameters or LLMs, and quickly get an understanding of LLM or environment related problems. Based on our user study, experienced researchers spend between 10 and 30 minutes to gather the information provided by SeaView, while researchers with little experience can spend between 30 minutes to 1 hour to diagnose their experiment.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-PolyBench: A multi-language benchmark for repository level evaluation of coding agents</title>
<link>https://arxiv.org/abs/2504.08703</link>
<guid>https://arxiv.org/abs/2504.08703</guid>
<content:encoded><![CDATA[
<div> : SWE-PolyBench<br /><br />:
SWE-PolyBenchSWE-PolyBench212110JavaJavaScriptTypeScriptPythonSWE-PolyBench500SWE-PolyBenchSWE-PolyBenchGitHubhttps://github.com/amazon-science/SWE-PolyBench <div>
arXiv:2504.08703v1 Announce Type: new 
Abstract: Coding agents powered by large language models have shown impressive capabilities in software engineering tasks, but evaluating their performance across diverse programming languages and real-world scenarios remains challenging. We introduce SWE-PolyBench, a new multi-language benchmark for repository-level, execution-based evaluation of coding agents. SWE-PolyBench contains 2110 instances from 21 repositories and includes tasks in Java (165), JavaScript (1017), TypeScript (729) and Python (199), covering bug fixes, feature additions, and code refactoring. We provide a task and repository-stratified subsample (SWE-PolyBench500) and release an evaluation harness allowing for fully automated evaluation. To enable a more comprehensive comparison of coding agents, this work also presents a novel set of metrics rooted in syntax tree analysis. We evaluate leading open source coding agents on SWE-PolyBench, revealing their strengths and limitations across languages, task types, and complexity classes. Our experiments show that current agents exhibit uneven performances across languages and struggle with complex problems while showing higher performance on simpler tasks. SWE-PolyBench aims to drive progress in developing more versatile and robust AI coding assistants for real-world software engineering. Our datasets and code are available at: https://github.com/amazon-science/SWE-PolyBench
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DocAgent: A Multi-Agent System for Automated Code Documentation Generation</title>
<link>https://arxiv.org/abs/2504.08725</link>
<guid>https://arxiv.org/abs/2504.08725</guid>
<content:encoded><![CDATA[
<div> DocAgent

:
DocAgentReaderSearcherWriterVerifierOrchestratorAIDocAgentDocAgentDocAgent <div>
arXiv:2504.08725v1 Announce Type: new 
Abstract: High-quality code documentation is crucial for software development especially in the era of AI. However, generating it automatically using Large Language Models (LLMs) remains challenging, as existing approaches often produce incomplete, unhelpful, or factually incorrect outputs. We introduce DocAgent, a novel multi-agent collaborative system using topological code processing for incremental context building. Specialized agents (Reader, Searcher, Writer, Verifier, Orchestrator) then collaboratively generate documentation. We also propose a multi-faceted evaluation framework assessing Completeness, Helpfulness, and Truthfulness. Comprehensive experiments show DocAgent significantly outperforms baselines consistently. Our ablation study confirms the vital role of the topological processing order. DocAgent offers a robust approach for reliable code documentation generation in complex and proprietary repositories.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Multi-Agent Reinforcement Learning in Malm\"O (MARL\"O) Competition</title>
<link>https://arxiv.org/abs/1901.08129</link>
<guid>https://arxiv.org/abs/1901.08129</guid>
<content:encoded><![CDATA[
<div> : 

:
arXiv:1901.08129v2Malm\"OMARL\"O<br /><br /> <div>
arXiv:1901.08129v2 Announce Type: replace 
Abstract: Learning in multi-agent scenarios is a fruitful research direction, but current approaches still show scalability problems in multiple games with general reward settings and different opponent types. The Multi-Agent Reinforcement Learning in Malm\"O (MARL\"O) competition is a new challenge that proposes research in this domain using multiple 3D games. The goal of this contest is to foster research in general agents that can learn across different games and opponent types, proposing a challenge as a milestone in the direction of Artificial General Intelligence.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Polytime Algorithms for One-to-Many Matching Games</title>
<link>https://arxiv.org/abs/2107.07440</link>
<guid>https://arxiv.org/abs/2107.07440</guid>
<content:encoded><![CDATA[
<div> Matching games, , , , 

<br /><br />:

Garrido-LuceroLaraki <div>
arXiv:2107.07440v3 Announce Type: replace 
Abstract: Matching games is a novel matching model introduced by Garrido-Lucero and Laraki, in which agents' utilities are endogenously determined as the outcome of a strategic game they play simultaneously with the matching process. Matching games encompass most one-to-one matching market models and reinforce the classical notion of pairwise stability by analyzing their robustness to unilateral deviations within games. In this article, we extend the model to the one-to-many setting, where hospitals can be matched to multiple doctors, and their utility is given by the sum of their game outcomes. We adapt the deferred acceptance with competitions algorithm and the renegotiation process to this new framework and prove that both are polynomial whenever couples play bi-matrix games in mixed strategies.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning</title>
<link>https://arxiv.org/abs/2401.10727</link>
<guid>https://arxiv.org/abs/2401.10727</guid>
<content:encoded><![CDATA[
<div> MLLM-Tool

<br /><br />:

MLLM-ToolLLMsMLLM-ToolGitHub <div>
arXiv:2401.10727v3 Announce Type: replace 
Abstract: Recently, the astonishing performance of large language models (LLMs) in natural language comprehension and generation tasks triggered lots of exploration of using them as central controllers to build agent systems. Multiple studies focus on bridging the LLMs to external tools to extend the application scenarios. However, the current LLMs' ability to perceive tool use is limited to a single text query, which may result in ambiguity in understanding the users' real intentions. LLMs are expected to eliminate that by perceiving the information in the visual- or auditory-grounded instructions. Therefore, in this paper, we propose MLLM-Tool, a system incorporating open-source LLMs and multi-modal encoders so that the learned LLMs can be conscious of multi-modal input instruction and then select the function-matched tool correctly. To facilitate the evaluation of the model's capability, we collect a dataset featuring multi-modal input tools from HuggingFace. Another essential feature of our dataset is that it also contains multiple potential choices for the same instruction due to the existence of identical functions and synonymous functions, which provides more potential solutions for the same query. The experiments reveal that our MLLM-Tool is capable of recommending appropriate tools for multi-modal instructions. Codes and data are available at https://github.com/MLLM-Tool/MLLM-Tool.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoSQA+: Pioneering the Multi-Choice Code Search Benchmark with Test-Driven Agents</title>
<link>https://arxiv.org/abs/2406.11589</link>
<guid>https://arxiv.org/abs/2406.11589</guid>
<content:encoded><![CDATA[
<div> : CoSQA+, , , , 

<br /><br />:
CoSQA+CoSQA+92.0%PythonCoSQA+CoSQACoSQA+ <div>
arXiv:2406.11589v5 Announce Type: replace 
Abstract: Semantic code search, retrieving code that matches a given natural language query, is an important task to improve productivity in software engineering. Existing code search datasets face limitations: they rely on human annotators who assess code primarily through semantic understanding rather than functional verification, leading to potential inaccuracies and scalability issues. Additionally, current evaluation metrics often overlook the multi-choice nature of code search. This paper introduces CoSQA+, pairing high-quality queries from CoSQA with multiple suitable codes. We develop an automated pipeline featuring multiple model-based candidate selections and the novel test-driven agent annotation system. Among a single Large Language Model (LLM) annotator and Python expert annotators (without test-based verification), agents leverage test-based verification and achieve the highest accuracy of 92.0%. Through extensive experiments, CoSQA+ has demonstrated superior quality over CoSQA. Models trained on CoSQA+ exhibit improved performance. We provide the code and data at https://github.com/DeepSoftwareAnalytics/CoSQA_Plus.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSBench: How Far Are Data Science Agents from Becoming Data Science Experts?</title>
<link>https://arxiv.org/abs/2409.07703</link>
<guid>https://arxiv.org/abs/2409.07703</guid>
<content:encoded><![CDATA[
<div> : -DSBench

:
DSBenchDSBench46674EloquenceKaggleLLMsLVLMs34.12%34.74%RPG <div>
arXiv:2409.07703v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have demonstrated impressive language/vision reasoning abilities, igniting the recent trend of building agents for targeted applications such as shopping assistants or AI software engineers. Recently, many data science benchmarks have been proposed to investigate their performance in the data science domain. However, existing data science benchmarks still fall short when compared to real-world data science applications due to their simplified settings. To bridge this gap, we introduce DSBench, a comprehensive benchmark designed to evaluate data science agents with realistic tasks. This benchmark includes 466 data analysis tasks and 74 data modeling tasks, sourced from Eloquence and Kaggle competitions. DSBench offers a realistic setting by encompassing long contexts, multimodal task backgrounds, reasoning with large data files and multi-table structures, and performing end-to-end data modeling tasks. Our evaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle with most tasks, with the best agent solving only 34.12% of data analysis tasks and achieving a 34.74% Relative Performance Gap (RPG). These findings underscore the need for further advancements in developing more practical, intelligent, and autonomous data science agents.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maintaining Strong r-Robustness in Reconfigurable Multi-Robot Networks using Control Barrier Functions</title>
<link>https://arxiv.org/abs/2409.14675</link>
<guid>https://arxiv.org/abs/2409.14675</guid>
<content:encoded><![CDATA[
<div> : arXivr- Barrier (CBF)

:
r-Control Barrier Function (CBF)CBF<br /><br /> <div>
arXiv:2409.14675v3 Announce Type: replace 
Abstract: In leader-follower consensus, strong r-robustness of the communication graph provides a sufficient condition for followers to achieve consensus in the presence of misbehaving agents. Previous studies have assumed that robots can form and/or switch between predetermined network topologies with known robustness properties. However, robots with distance-based communication models may not be able to achieve these topologies while moving through spatially constrained environments, such as narrow corridors, to complete their objectives. This paper introduces a Control Barrier Function (CBF) that ensures robots maintain strong r-robustness of their communication graph above a certain threshold without maintaining any fixed topologies. Our CBF directly addresses robustness, allowing robots to have flexible reconfigurable network structure while navigating to achieve their objectives. The efficacy of our method is tested through various simulation and hardware experiments.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Construction of the Sparsest Maximally r-Robust Graphs</title>
<link>https://arxiv.org/abs/2409.19465</link>
<guid>https://arxiv.org/abs/2409.19465</guid>
<content:encoded><![CDATA[
<div> r-

:
r-r-(1) (2) (1)<br /><br /> <div>
arXiv:2409.19465v3 Announce Type: replace 
Abstract: In recent years, the notion of r-robustness for the communication graph of the network has been introduced to address the challenge of achieving consensus in the presence of misbehaving agents. Higher r-robustness typically implies higher tolerance to malicious information towards achieving resilient consensus, but it also implies more edges for the communication graph. This in turn conflicts with the need to minimize communication due to limited resources in real-world applications (e.g., multi-robot networks). In this paper, our contributions are twofold. (a) We provide the necessary subgraph structures and tight lower bounds on the number of edges required for graphs with a given number of nodes to achieve maximum robustness. (b) We then use the results of (a) to introduce two classes of graphs that maintain maximum robustness with the least number of edges. Our work is validated through a series of simulations.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planning and Learning in Risk-Aware Restless Multi-Arm Bandit Problem</title>
<link>https://arxiv.org/abs/2410.23029</link>
<guid>https://arxiv.org/abs/2410.23029</guid>
<content:encoded><![CDATA[
<div> : restless multi-arm bandits, risk-awareness, Whittle index, Thompson sampling, regret

:
 episode <br /><br /> <div>
arXiv:2410.23029v2 Announce Type: replace 
Abstract: In restless multi-arm bandits, a central agent is tasked with optimally distributing limited resources across several bandits (arms), with each arm being a Markov decision process. In this work, we generalize the traditional restless multi-arm bandit problem with a risk-neutral objective by incorporating risk-awareness. We establish indexability conditions for the case of a risk-aware objective and provide a solution based on Whittle index. In addition, we address the learning problem when the true transition probabilities are unknown by proposing a Thompson sampling approach and show that it achieves bounded regret that scales sublinearly with the number of episodes and quadratically with the number of arms. The efficacy of our method in reducing risk exposure in restless multi-arm bandits is illustrated through a set of numerical experiments in the contexts of machine replacement and patient scheduling applications under both planning and learning setups.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MrSteve: Instruction-Following Agents in Minecraft with What-Where-When Memory</title>
<link>https://arxiv.org/abs/2411.06736</link>
<guid>https://arxiv.org/abs/2411.06736</guid>
<content:encoded><![CDATA[
<div> : arXiv:2411.06736v5, AI, Minecraft, , 

:
MinecraftLLMMrStevePlace Event MemoryPEMSteve-1PEMhttps://sites.google.com/view/mr-steve <div>
arXiv:2411.06736v5 Announce Type: replace 
Abstract: Significant advances have been made in developing general-purpose embodied AI in environments like Minecraft through the adoption of LLM-augmented hierarchical approaches. While these approaches, which combine high-level planners with low-level controllers, show promise, low-level controllers frequently become performance bottlenecks due to repeated failures. In this paper, we argue that the primary cause of failure in many low-level controllers is the absence of an episodic memory system. To address this, we introduce MrSteve (Memory Recall Steve), a novel low-level controller equipped with Place Event Memory (PEM), a form of episodic memory that captures what, where, and when information from episodes. This directly addresses the main limitation of the popular low-level controller, Steve-1. Unlike previous models that rely on short-term memory, PEM organizes spatial and event-based data, enabling efficient recall and navigation in long-horizon tasks. Additionally, we propose an Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing agents to alternate between exploration and task-solving based on recalled events. Our approach significantly improves task-solving and exploration efficiency compared to existing methods. We will release our code and demos on the project page: https://sites.google.com/view/mr-steve.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Subspaces of Policies for Continual Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2412.14865</link>
<guid>https://arxiv.org/abs/2412.14865</guid>
<content:encoded><![CDATA[
<div> Continual Reinforcement LearningHiSPO

:
Continual Reinforcement LearningHiSPOMuJoCoHiSPO <div>
arXiv:2412.14865v3 Announce Type: replace 
Abstract: We consider a Continual Reinforcement Learning setup, where a learning agent must continuously adapt to new tasks while retaining previously acquired skill sets, with a focus on the challenge of avoiding forgetting past gathered knowledge and ensuring scalability with the growing number of tasks. Such issues prevail in autonomous robotics and video game simulations, notably for navigation tasks prone to topological or kinematic changes. To address these issues, we introduce HiSPO, a novel hierarchical framework designed specifically for continual learning in navigation settings from offline data. Our method leverages distinct policy subspaces of neural networks to enable flexible and efficient adaptation to new tasks while preserving existing knowledge. We demonstrate, through a careful experimental study, the effectiveness of our method in both classical MuJoCo maze environments and complex video game-like navigation simulations, showcasing competitive performances and satisfying adaptability with respect to classical continual learning metrics, in particular regarding the memory usage and efficiency.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logarithmic Regret for Nonlinear Control</title>
<link>https://arxiv.org/abs/2501.10261</link>
<guid>https://arxiv.org/abs/2501.10261</guid>
<content:encoded><![CDATA[
<div> 

:
 <div>
arXiv:2501.10261v2 Announce Type: replace 
Abstract: We address the problem of learning to control an unknown nonlinear dynamical system through sequential interactions. Motivated by high-stakes applications in which mistakes can be catastrophic, such as robotics and healthcare, we study situations where it is possible for fast sequential learning to occur. Fast sequential learning is characterized by the ability of the learning agent to incur logarithmic regret relative to a fully-informed baseline. We demonstrate that fast sequential learning is achievable in a diverse class of continuous control problems where the system dynamics depend smoothly on unknown parameters, provided the optimal control policy is persistently exciting. Additionally, we derive a regret bound which grows with the square root of the number of interactions for cases where the optimal policy is not persistently exciting. Our results provide the first regret bounds for controlling nonlinear dynamical systems depending nonlinearly on unknown parameters. We validate the trends our theory predicts in simulation on a simple dynamical system.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents</title>
<link>https://arxiv.org/abs/2501.11858</link>
<guid>https://arxiv.org/abs/2501.11858</guid>
<content:encoded><![CDATA[
<div> Multimodal Large Language Models (MLLMs)EmbodiedEval

<br /><br />:
EmbodiedEval(MLLMs)MLLMsEmbodiedEval328125EmbodiedEvalMLLMsMLLMshttps://github.com/thunlp/EmbodiedEval <div>
arXiv:2501.11858v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) have shown significant advancements, providing a promising future for embodied agents. Existing benchmarks for evaluating MLLMs primarily utilize static images or videos, limiting assessments to non-interactive scenarios. Meanwhile, existing embodied AI benchmarks are task-specific and not diverse enough, which do not adequately evaluate the embodied capabilities of MLLMs. To address this, we propose EmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs with embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied 3D scenes, each of which is rigorously selected and annotated. It covers a broad spectrum of existing embodied AI tasks with significantly enhanced diversity, all within a unified simulation and evaluation framework tailored for MLLMs. The tasks are organized into five categories: navigation, object interaction, social interaction, attribute question answering, and spatial question answering to assess different capabilities of the agents. We evaluated the state-of-the-art MLLMs on EmbodiedEval and found that they have a significant shortfall compared to human level on embodied tasks. Our analysis demonstrates the limitations of existing MLLMs in embodied capabilities, providing insights for their future development. We open-source all evaluation data and simulation framework at https://github.com/thunlp/EmbodiedEval.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Large Language Model Empowered Agents for Recommendation and Search: Towards Next-Generation Information Retrieval</title>
<link>https://arxiv.org/abs/2503.05659</link>
<guid>https://arxiv.org/abs/2503.05659</guid>
<content:encoded><![CDATA[
<div> : 

<br />
:
LLMsLLMLLMLLM <div>
arXiv:2503.05659v2 Announce Type: replace 
Abstract: Information technology has profoundly altered the way humans interact with information. The vast amount of content created, shared, and disseminated online has made it increasingly difficult to access relevant information. Over the past two decades, recommender systems and search (collectively referred to as information retrieval systems) have evolved significantly to address these challenges. Recent advances in large language models (LLMs) have demonstrated capabilities that surpass human performance in various language-related tasks and exhibit general understanding, reasoning, and decision-making abilities. This paper explores the transformative potential of LLM agents in enhancing recommender and search systems. We discuss the motivations and roles of LLM agents, and establish a classification framework to elaborate on the existing research. We highlight the immense potential of LLM agents in addressing current challenges in recommendation and search, providing insights into future research directions. This paper is the first to systematically review and classify the research on LLM agents in these domains, offering a novel perspective on leveraging this advanced AI technology for information retrieval. To help understand the existing works, we list the existing papers on LLM agent based recommendation and search at this link: https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Universal LLM Reward Design through Heuristic Reward Observation Space Evolution</title>
<link>https://arxiv.org/abs/2504.07596</link>
<guid>https://arxiv.org/abs/2504.07596</guid>
<content:encoded><![CDATA[
<div> :  (LLMs),  (RL) ,  (ROS), , -

:
LLMsRLROSLLMLLM-RLjingjjjjjie.github.io/LLM2Reward <div>
arXiv:2504.07596v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are emerging as promising tools for automated reinforcement learning (RL) reward design, owing to their robust capabilities in commonsense reasoning and code generation. By engaging in dialogues with RL agents, LLMs construct a Reward Observation Space (ROS) by selecting relevant environment states and defining their internal operations. However, existing frameworks have not effectively leveraged historical exploration data or manual task descriptions to iteratively evolve this space. In this paper, we propose a novel heuristic framework that enhances LLM-driven reward design by evolving the ROS through a table-based exploration caching mechanism and a text-code reconciliation strategy. Our framework introduces a state execution table, which tracks the historical usage and success rates of environment states, overcoming the Markovian constraint typically found in LLM dialogues and facilitating more effective exploration. Furthermore, we reconcile user-provided task descriptions with expert-defined success criteria using structured prompts, ensuring alignment in reward design objectives. Comprehensive evaluations on benchmark RL tasks demonstrate the effectiveness and stability of the proposed framework. Code and video demos are available at jingjjjjjie.github.io/LLM2Reward.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning the Distribution Map in Reverse Causal Performative Prediction</title>
<link>https://arxiv.org/abs/2405.15172</link>
<guid>https://arxiv.org/abs/2405.15172</guid>
<content:encoded><![CDATA[
<div> : 

:<br />
 <div>
arXiv:2405.15172v2 Announce Type: replace-cross 
Abstract: In numerous predictive scenarios, the predictive model affects the sampling distribution; for example, job applicants often meticulously craft their resumes to navigate through a screening systems. Such shifts in distribution are particularly prevalent in the realm of social computing, yet, the strategies to learn these shifts from data remain remarkably limited. Inspired by a microeconomic model that adeptly characterizes agents' behavior within labor markets, we introduce a novel approach to learn the distribution shift. Our method is predicated on a reverse causal model, wherein the predictive model instigates a distribution shift exclusively through a finite set of agents' actions. Within this framework, we employ a microfoundation model for the agents' actions and develop a statistically justified methodology to learn the distribution shift map, which we demonstrate to be effective in minimizing the performative prediction risk.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Microfoundation Inference for Strategic Prediction</title>
<link>https://arxiv.org/abs/2411.08998</link>
<guid>https://arxiv.org/abs/2411.08998</guid>
<content:encoded><![CDATA[
<div> : performative predictionpredictive modelsocial impactoptimal transportconvergence rate

<br /><br />:
 <div>
arXiv:2411.08998v2 Announce Type: replace-cross 
Abstract: Often in prediction tasks, the predictive model itself can influence the distribution of the target variable, a phenomenon termed performative prediction. Generally, this influence stems from strategic actions taken by stakeholders with a vested interest in predictive models. A key challenge that hinders the widespread adaptation of performative prediction in machine learning is that practitioners are generally unaware of the social impacts of their predictions. To address this gap, we propose a methodology for learning the distribution map that encapsulates the long-term impacts of predictive models on the population. Specifically, we model agents' responses as a cost-adjusted utility maximization problem and propose estimates for said cost. Our approach leverages optimal transport to align pre-model exposure (ex ante) and post-model exposure (ex post) distributions. We provide a rate of convergence for this proposed estimate and assess its quality through empirical demonstrations on a credit-scoring dataset.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EXCLAIM: An Explainable Cross-Modal Agentic System for Misinformation Detection with Hierarchical Retrieval</title>
<link>https://arxiv.org/abs/2504.06269</link>
<guid>https://arxiv.org/abs/2504.06269</guid>
<content:encoded><![CDATA[
<div> misinformation, out-of-context, detection, multi-modal, EXCLAIM

<br /><br />:
Out-of-ContextOOCOOCEXCLAIMEXCLAIMOOC4.3% <div>
arXiv:2504.06269v1 Announce Type: new 
Abstract: Misinformation continues to pose a significant challenge in today's information ecosystem, profoundly shaping public perception and behavior. Among its various manifestations, Out-of-Context (OOC) misinformation is particularly obscure, as it distorts meaning by pairing authentic images with misleading textual narratives. Existing methods for detecting OOC misinformation predominantly rely on coarse-grained similarity metrics between image-text pairs, which often fail to capture subtle inconsistencies or provide meaningful explainability. While multi-modal large language models (MLLMs) demonstrate remarkable capabilities in visual reasoning and explanation generation, they have not yet demonstrated the capacity to address complex, fine-grained, and cross-modal distinctions necessary for robust OOC detection. To overcome these limitations, we introduce EXCLAIM, a retrieval-based framework designed to leverage external knowledge through multi-granularity index of multi-modal events and entities. Our approach integrates multi-granularity contextual analysis with a multi-agent reasoning architecture to systematically evaluate the consistency and integrity of multi-modal news content. Comprehensive experiments validate the effectiveness and resilience of EXCLAIM, demonstrating its ability to detect OOC misinformation with 4.3% higher accuracy compared to state-of-the-art approaches, while offering explainable and actionable insights.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ER-RAG: Enhance RAG with ER-Based Unified Modeling of Heterogeneous Data Sources</title>
<link>https://arxiv.org/abs/2504.06271</link>
<guid>https://arxiv.org/abs/2504.06271</guid>
<content:encoded><![CDATA[
<div> : , , , , 

<br />
:
ER-RAGRAGER-RAGERGETJOINER-based APIsAPIER-RAG2024KDDCup CRAGER-RAGRAG8BER-RAGLLM3.1%5.5 <div>
arXiv:2504.06271v1 Announce Type: new 
Abstract: Large language models (LLMs) excel in question-answering (QA) tasks, and retrieval-augmented generation (RAG) enhances their precision by incorporating external evidence from diverse sources like web pages, databases, and knowledge graphs. However, current RAG methods rely on agent-specific strategies for individual data sources, posing challenges low-resource or black-box environments and complicates operations when evidence is fragmented across sources. To address these limitations, we propose ER-RAG, a framework that unifies evidence integration across heterogeneous data sources using the Entity-Relationship (ER) model. ER-RAG standardizes entity retrieval and relationship querying through ER-based APIs with GET and JOIN operations. It employs a two-stage generation process: first, a preference optimization module selects optimal sources; second, another module constructs API chains based on source schemas. This unified approach allows efficient fine-tuning and seamless integration across diverse data sources. ER-RAG demonstrated its effectiveness by winning all three tracks of the 2024 KDDCup CRAG Challenge, achieving performance on par with commercial RAG pipelines using an 8B LLM backbone. It outperformed hybrid competitors by 3.1% in LLM score and accelerated retrieval by 5.5X.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAVEN: An Agentic Framework for Multimodal Entity Discovery from Large-Scale Video Collections</title>
<link>https://arxiv.org/abs/2504.06272</link>
<guid>https://arxiv.org/abs/2504.06272</guid>
<content:encoded><![CDATA[
<div> : RAVENAI agnostic

:<br />
RAVENAIRAVEN1)2)3)RAVEN(VLMs)(LLMs) <div>
arXiv:2504.06272v1 Announce Type: new 
Abstract: We present RAVEN an adaptive AI agent framework designed for multimodal entity discovery and retrieval in large-scale video collections. Synthesizing information across visual, audio, and textual modalities, RAVEN autonomously processes video data to produce structured, actionable representations for downstream tasks. Key contributions include (1) a category understanding step to infer video themes and general-purpose entities, (2) a schema generation mechanism that dynamically defines domain-specific entities and attributes, and (3) a rich entity extraction process that leverages semantic retrieval and schema-guided prompting. RAVEN is designed to be model-agnostic, allowing the integration of different vision-language models (VLMs) and large language models (LLMs) based on application-specific requirements. This flexibility supports diverse applications in personalized search, content discovery, and scalable information retrieval, enabling practical applications across vast datasets.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Evaluation Framework for Personalized and Trustworthy Agents: A Multi-Session Approach to Preference Adaptability</title>
<link>https://arxiv.org/abs/2504.06277</link>
<guid>https://arxiv.org/abs/2504.06277</guid>
<content:encoded><![CDATA[
<div> : 

<br />
:
AILLMs <div>
arXiv:2504.06277v1 Announce Type: new 
Abstract: Recent advancements in generative AI have significantly increased interest in personalized agents. With increased personalization, there is also a greater need for being able to trust decision-making and action taking capabilities of these agents. However, the evaluation methods for these agents remain outdated and inadequate, often failing to capture the dynamic and evolving nature of user interactions. In this conceptual article, we argue for a paradigm shift in evaluating personalized and adaptive agents. We propose a comprehensive novel framework that models user personas with unique attributes and preferences. In this framework, agents interact with these simulated users through structured interviews to gather their preferences and offer customized recommendations. These recommendations are then assessed dynamically using simulations driven by Large Language Models (LLMs), enabling an adaptive and iterative evaluation process. Our flexible framework is designed to support a variety of agents and applications, ensuring a comprehensive and versatile evaluation of recommendation strategies that focus on proactive, personalized, and trustworthy aspects.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Information-Geometric Approach to Artificial Curiosity</title>
<link>https://arxiv.org/abs/2504.06355</link>
<guid>https://arxiv.org/abs/2504.06355</guid>
<content:encoded><![CDATA[
<div> -

:
-<br /><br /> <div>
arXiv:2504.06355v1 Announce Type: new 
Abstract: Learning in environments with sparse rewards remains a fundamental challenge in reinforcement learning. Artificial curiosity addresses this limitation through intrinsic rewards to guide exploration, however, the precise formulation of these rewards has remained elusive. Ideally, such rewards should depend on the agent's information about the environment, remaining agnostic to the representation of the information -- an invariance central to information geometry. Leveraging information geometry, we show that invariance under congruent Markov morphisms and the agent-environment interaction, uniquely constrains intrinsic rewards to concave functions of the reciprocal occupancy. Additional geometrically motivated restrictions effectively limits the candidates to those determined by a real parameter that governs the occupancy space geometry. Remarkably, special values of this parameter are found to correspond to count-based and maximum entropy exploration, revealing a geometric exploration-exploitation trade-off. This framework provides important constraints to the engineering of intrinsic reward while integrating foundational exploration methods into a single, cohesive model.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Self-Disclosure Themes and Semantics to a Human, a Robot, and a Disembodied Agent</title>
<link>https://arxiv.org/abs/2504.06374</link>
<guid>https://arxiv.org/abs/2504.06374</guid>
<content:encoded><![CDATA[
<div> : 

:
 <div>
arXiv:2504.06374v1 Announce Type: new 
Abstract: As social robots and other artificial agents become more conversationally capable, it is important to understand whether the content and meaning of self-disclosure towards these agents changes depending on the agent's embodiment. In this study, we analysed conversational data from three controlled experiments in which participants self-disclosed to a human, a humanoid social robot, and a disembodied conversational agent. Using sentence embeddings and clustering, we identified themes in participants' disclosures, which were then labelled and explained by a large language model. We subsequently assessed whether these themes and the underlying semantic structure of the disclosures varied by agent embodiment. Our findings reveal strong consistency: thematic distributions did not significantly differ across embodiments, and semantic similarity analyses showed that disclosures were expressed in highly comparable ways. These results suggest that while embodiment may influence human behaviour in human-robot and human-agent interactions, people tend to maintain a consistent thematic focus and semantic structure in their disclosures, whether speaking to humans or artificial interlocutors.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-Arena: A General Framework for Evaluating Control Algorithms</title>
<link>https://arxiv.org/abs/2504.06468</link>
<guid>https://arxiv.org/abs/2504.06468</guid>
<content:encoded><![CDATA[
<div> Agent-ArenaPython

:
<br />
Agent-ArenaPythonAgent-ArenaGitHubhttps://github.com/halid1020/agent-arena-v0 <div>
arXiv:2504.06468v1 Announce Type: new 
Abstract: Robotic research is inherently challenging, requiring expertise in diverse environments and control algorithms. Adapting algorithms to new environments often poses significant difficulties, compounded by the need for extensive hyper-parameter tuning in data-driven methods. To address these challenges, we present Agent-Arena, a Python framework designed to streamline the integration, replication, development, and testing of decision-making policies across a wide range of benchmark environments. Unlike existing frameworks, Agent-Arena is uniquely generalised to support all types of control algorithms and is adaptable to both simulation and real-robot scenarios. Please see our GitHub repository https://github.com/halid1020/agent-arena-v0.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeedleInATable: Exploring Long-Context Capability of Large Language Models towards Long-Structured Tables</title>
<link>https://arxiv.org/abs/2504.06560</link>
<guid>https://arxiv.org/abs/2504.06560</guid>
<content:encoded><![CDATA[
<div> : , , , NeedleInATable (NIAT), 

:<br />
NeedleInATable (NIAT)LLMLLMNIATLLMLLM <div>
arXiv:2504.06560v1 Announce Type: new 
Abstract: Processing structured tabular data, particularly lengthy tables, constitutes a fundamental yet challenging task for large language models (LLMs). However, existing long-context benchmarks primarily focus on unstructured text, neglecting the challenges of long and complex structured tables. To address this gap, we introduce NeedleInATable (NIAT), a novel task that treats each table cell as a "needle" and requires the model to extract the target cell under different queries. Evaluation results of mainstream LLMs on this benchmark show they lack robust long-table comprehension, often relying on superficial correlations or shortcuts for complex table understanding tasks, revealing significant limitations in processing intricate tabular data. To this end, we propose a data synthesis method to enhance models' long-table comprehension capabilities. Experimental results show that our synthesized training data significantly enhances LLMs' performance on the NIAT task, outperforming both long-context LLMs and long-table agent methods. This work advances the evaluation of LLMs' genuine long-structured table comprehension capabilities and paves the way for progress in long-context and table understanding applications.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Right Prediction, Wrong Reasoning: Uncovering LLM Misalignment in RA Disease Diagnosis</title>
<link>https://arxiv.org/abs/2504.06581</link>
<guid>https://arxiv.org/abs/2504.06581</guid>
<content:encoded><![CDATA[
<div> :  (LLMs) (RA)

<br /><br />:
LLMsRALLMsRA95%68%LLMsRALLM <div>
arXiv:2504.06581v1 Announce Type: new 
Abstract: Large language models (LLMs) offer a promising pre-screening tool, improving early disease detection and providing enhanced healthcare access for underprivileged communities. The early diagnosis of various diseases continues to be a significant challenge in healthcare, primarily due to the nonspecific nature of early symptoms, the shortage of expert medical practitioners, and the need for prolonged clinical evaluations, all of which can delay treatment and adversely affect patient outcomes. With impressive accuracy in prediction across a range of diseases, LLMs have the potential to revolutionize clinical pre-screening and decision-making for various medical conditions. In this work, we study the diagnostic capability of LLMs for Rheumatoid Arthritis (RA) with real world patients data. Patient data was collected alongside diagnoses from medical experts, and the performance of LLMs was evaluated in comparison to expert diagnoses for RA disease prediction. We notice an interesting pattern in disease diagnosis and find an unexpected \textit{misalignment between prediction and explanation}. We conduct a series of multi-round analyses using different LLM agents. The best-performing model accurately predicts rheumatoid arthritis (RA) diseases approximately 95\% of the time. However, when medical experts evaluated the reasoning generated by the model, they found that nearly 68\% of the reasoning was incorrect. This study highlights a clear misalignment between LLMs high prediction accuracy and its flawed reasoning, raising important questions about relying on LLM explanations in clinical settings. \textbf{LLMs provide incorrect reasoning to arrive at the correct answer for RA disease diagnosis.}
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wanting to be Understood</title>
<link>https://arxiv.org/abs/2504.06611</link>
<guid>https://arxiv.org/abs/2504.06611</guid>
<content:encoded><![CDATA[
<div> intrinsic motivation, mutual awareness, reinforcement learning, artificial curiosity, social interaction

:
/ <div>
arXiv:2504.06611v1 Announce Type: new 
Abstract: This paper explores an intrinsic motivation for mutual awareness, hypothesizing that humans possess a fundamental drive to understand \textit{and to be understood} even in the absence of extrinsic rewards. Through simulations of the perceptual crossing paradigm, we explore the effect of various internal reward functions in reinforcement learning agents. The drive to understand is implemented as an active inference type artificial curiosity reward, whereas the drive to be understood is implemented through intrinsic rewards for imitation, influence/impressionability, and sub-reaction time anticipation of the other. Results indicate that while artificial curiosity alone does not lead to a preference for social interaction, rewards emphasizing reciprocal understanding successfully drive agents to prioritize interaction. We demonstrate that this intrinsic motivation can facilitate cooperation in tasks where only one agent receives extrinsic reward for the behaviour of the other.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentFM: Role-Aware Failure Management for Distributed Databases with LLM-Driven Multi-Agents</title>
<link>https://arxiv.org/abs/2504.06614</link>
<guid>https://arxiv.org/abs/2504.06614</guid>
<content:encoded><![CDATA[
<div> : AgentFMLLM

:<br />
AgentFMLLMApache IoTDBAgentFM <div>
arXiv:2504.06614v1 Announce Type: new 
Abstract: Distributed databases are critical infrastructures for today's large-scale software systems, making effective failure management essential to ensure software availability. However, existing approaches often overlook the role distinctions within distributed databases and rely on small-scale models with limited generalization capabilities. In this paper, we conduct a preliminary empirical study to emphasize the unique significance of different roles. Building on this insight, we propose AgentFM, a role-aware failure management framework for distributed databases powered by LLM-driven multi-agents. AgentFM addresses failure management by considering system roles, data roles, and task roles, with a meta-agent orchestrating these components. Preliminary evaluations using Apache IoTDB demonstrate the effectiveness of AgentFM and open new directions for further research.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-like compositional learning of visually-grounded concepts using synthetic environments</title>
<link>https://arxiv.org/abs/2504.06618</link>
<guid>https://arxiv.org/abs/2504.06618</guid>
<content:encoded><![CDATA[
<div> 

<br />
:
3D <div>
arXiv:2504.06618v1 Announce Type: new 
Abstract: The compositional structure of language enables humans to decompose complex phrases and map them to novel visual concepts, showcasing flexible intelligence. While several algorithms exhibit compositionality, they fail to elucidate how humans learn to compose concept classes and ground visual cues through trial and error. To investigate this multi-modal learning challenge, we designed a 3D synthetic environment in which an agent learns, via reinforcement, to navigate to a target specified by a natural language instruction. These instructions comprise nouns, attributes, and critically, determiners, prepositions, or both. The vast array of word combinations heightens the compositional complexity of the visual grounding task, as navigating to a blue cube above red spheres is not rewarded when the instruction specifies navigating to "some blue cubes below the red sphere". We first demonstrate that reinforcement learning agents can ground determiner concepts to visual targets but struggle with more complex prepositional concepts. Second, we show that curriculum learning, a strategy humans employ, enhances concept learning efficiency, reducing the required training episodes by 15% in determiner environments and enabling agents to easily learn prepositional concepts. Finally, we establish that agents trained on determiner or prepositional concepts can decompose held-out test instructions and rapidly adapt their navigation policies to unseen visual object combinations. Leveraging synthetic environments, our findings demonstrate that multi-modal reinforcement learning agents can achieve compositional understanding of complex concept classes and highlight the efficacy of human-like learning strategies in improving artificial systems' learning efficiency.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Residual Safe Reinforcement Learning for Multi-Agent Safety-Critical Scenarios Decision-Making</title>
<link>https://arxiv.org/abs/2504.06670</link>
<guid>https://arxiv.org/abs/2504.06670</guid>
<content:encoded><![CDATA[
<div> : 

:<br />
DRS-RLDRS-RL92.17%27% <div>
arXiv:2504.06670v1 Announce Type: new 
Abstract: In multi-agent safety-critical scenarios, traditional autonomous driving frameworks face significant challenges in balancing safety constraints and task performance. These frameworks struggle to quantify dynamic interaction risks in real-time and depend heavily on manual rules, resulting in low computational efficiency and conservative strategies. To address these limitations, we propose a Dynamic Residual Safe Reinforcement Learning (DRS-RL) framework grounded in a safety-enhanced networked Markov decision process. It's the first time that the weak-to-strong theory is introduced into multi-agent decision-making, enabling lightweight dynamic calibration of safety boundaries via a weak-to-strong safety correction paradigm. Based on the multi-agent dynamic conflict zone model, our framework accurately captures spatiotemporal coupling risks among heterogeneous traffic participants and surpasses the static constraints of conventional geometric rules. Moreover, a risk-aware prioritized experience replay mechanism mitigates data distribution bias by mapping risk to sampling probability. Experimental results reveal that the proposed method significantly outperforms traditional RL algorithms in safety, efficiency, and comfort. Specifically, it reduces the collision rate by up to 92.17%, while the safety model accounts for merely 27% of the main model's parameters.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperparameter Optimisation with Practical Interpretability and Explanation Methods in Probabilistic Curriculum Learning</title>
<link>https://arxiv.org/abs/2504.06683</link>
<guid>https://arxiv.org/abs/2504.06683</guid>
<content:encoded><![CDATA[
<div> : AlgOSSHAP

:<br />
PCLRLOptunaTPEAlgOSRLSHAPRL <div>
arXiv:2504.06683v1 Announce Type: new 
Abstract: Hyperparameter optimisation (HPO) is crucial for achieving strong performance in reinforcement learning (RL), as RL algorithms are inherently sensitive to hyperparameter settings. Probabilistic Curriculum Learning (PCL) is a curriculum learning strategy designed to improve RL performance by structuring the agent's learning process, yet effective hyperparameter tuning remains challenging and computationally demanding. In this paper, we provide an empirical analysis of hyperparameter interactions and their effects on the performance of a PCL algorithm within standard RL tasks, including point-maze navigation and DC motor control. Using the AlgOS framework integrated with Optuna's Tree-Structured Parzen Estimator (TPE), we present strategies to refine hyperparameter search spaces, enhancing optimisation efficiency. Additionally, we introduce a novel SHAP-based interpretability approach tailored specifically for analysing hyperparameter impacts, offering clear insights into how individual hyperparameters and their interactions influence RL performance. Our work contributes practical guidelines and interpretability tools that significantly improve the effectiveness and computational feasibility of hyperparameter optimisation in reinforcement learning.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FJ-MM: The Friedkin-Johnsen Opinion Dynamics Model with Memory and Higher-Order Neighbors</title>
<link>https://arxiv.org/abs/2504.06731</link>
<guid>https://arxiv.org/abs/2504.06731</guid>
<content:encoded><![CDATA[
<div> Friedkin-JohnsenFJ-MM

<br /><br />:
Friedkin-JohnsenFJFJ-MMFJ-MMFJ-MMFJFJFJ-MM <div>
arXiv:2504.06731v1 Announce Type: new 
Abstract: The Friedkin-Johnsen (FJ) model has been extensively explored and validated, spanning applications in social science, systems and control, game theory, and algorithmic research. In this paper, we introduce an advanced generalization of the FJ model, termed FJ-MM which incorporates both memory effects and multi-hop (higher-order neighbor) influence. This formulation allows agents to naturally incorporate both current and previous opinions at each iteration stage. Our numerical results demonstrate that incorporating memory and multi-hop influence significantly reshapes the opinion landscape; for example, the final opinion profile can exhibit reduced polarization. We analyze the stability and equilibrium properties of the FJ-MM model, showing that these properties can be reduced to those of a comparison model--namely, the standard FJ model with a modified influence matrix. This reduction enables us to leverage established stability results from FJ dynamics. Additionally, we examine the convergence rate of the FJ-MM model and demonstrate that, as can be expected, the time lags introduced by memory and higher-order neighbor influences result in slower convergence.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Human-Robot Collaborative Missions using Hybrid Task Planning</title>
<link>https://arxiv.org/abs/2504.06746</link>
<guid>https://arxiv.org/abs/2504.06746</guid>
<content:encoded><![CDATA[
<div> : 

<br />
:
- <div>
arXiv:2504.06746v1 Announce Type: new 
Abstract: Producing robust task plans in human-robot collaborative missions is a critical activity in order to increase the likelihood of these missions completing successfully. Despite the broad research body in the area, which considers different classes of constraints and uncertainties, its applicability is confined to relatively simple problems that can be comfortably addressed by the underpinning mathematically-based or heuristic-driven solver engines. In this paper, we introduce a hybrid approach that effectively solves the task planning problem by decomposing it into two intertwined parts, starting with the identification of a feasible plan and followed by its uncertainty augmentation and verification yielding a set of Pareto optimal plans. To enhance its robustness, adaptation tactics are devised for the evolving system requirements and agents' capabilities. We demonstrate our approach through an industrial case study involving workers and robots undertaking activities within a vineyard, showcasing the benefits of our hybrid approach both in the generation of feasible solutions and scalability compared to native planners.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FamilyTool: A Multi-hop Personalized Tool Use Benchmark</title>
<link>https://arxiv.org/abs/2504.06766</link>
<guid>https://arxiv.org/abs/2504.06766</guid>
<content:encoded><![CDATA[
<div> : , , , , 

:
FamilyToolFamilyTool13KGEToolLLMsLLMsLLMsFamilyToolLLMGithub <div>
arXiv:2504.06766v1 Announce Type: new 
Abstract: The integration of tool learning with Large Language Models (LLMs) has expanded their capabilities in handling complex tasks by leveraging external tools. However, existing benchmarks for tool learning inadequately address critical real-world personalized scenarios, particularly those requiring multi-hop reasoning and inductive knowledge adaptation in dynamic environments. To bridge this gap, we introduce FamilyTool, a novel benchmark grounded in a family-based knowledge graph (KG) that simulates personalized, multi-hop tool use scenarios. FamilyTool challenges LLMs with queries spanning 1 to 3 relational hops (e.g., inferring familial connections and preferences) and incorporates an inductive KG setting where models must adapt to unseen user preferences and relationships without re-training, a common limitation in prior approaches that compromises generalization. We further propose KGETool: a simple KG-augmented evaluation pipeline to systematically assess LLMs' tool use ability in these settings. Experiments reveal significant performance gaps in state-of-the-art LLMs, with accuracy dropping sharply as hop complexity increases and inductive scenarios exposing severe generalization deficits. These findings underscore the limitations of current LLMs in handling personalized, evolving real-world contexts and highlight the urgent need for advancements in tool-learning frameworks. FamilyTool serves as a critical resource for evaluating and advancing LLM agents' reasoning, adaptability, and scalability in complex, dynamic environments. Code and dataset are available at Github.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inducing Programmatic Skills for Agentic Tasks</title>
<link>https://arxiv.org/abs/2504.06821</link>
<guid>https://arxiv.org/abs/2504.06821</guid>
<content:encoded><![CDATA[
<div> : (ASI)Web

:
(ASI)WebArenaASI23.5%11.3%ASI10.7-15.3%ASIASI <div>
arXiv:2504.06821v1 Announce Type: new 
Abstract: To succeed in common digital tasks such as web navigation, agents must carry out a variety of specialized tasks such as searching for products or planning a travel route. To tackle these tasks, agents can bootstrap themselves by learning task-specific skills online through interaction with the web environment. In this work, we demonstrate that programs are an effective representation for skills. We propose agent skill induction (ASI), which allows agents to adapt themselves by inducing, verifying, and utilizing program-based skills on the fly. We start with an evaluation on the WebArena agent benchmark and show that ASI outperforms the static baseline agent and its text-skill counterpart by 23.5% and 11.3% in success rate, mainly thanks to the programmatic verification guarantee during the induction phase. ASI also improves efficiency by reducing 10.7-15.3% of the steps over baselines, by composing primitive actions (e.g., click) into higher-level skills (e.g., search product). We then highlight the efficacy of ASI in remaining efficient and accurate under scaled-up web activities. Finally, we examine the generalizability of induced skills when transferring between websites, and find that ASI can effectively reuse common skills, while also updating incompatible skills to versatile website changes.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IAAO: Interactive Affordance Learning for Articulated Objects in 3D Environments</title>
<link>https://arxiv.org/abs/2504.06827</link>
<guid>https://arxiv.org/abs/2504.06827</guid>
<content:encoded><![CDATA[
<div> IAAO3Darticulated

<br />
:
IAAOIAAO3D splatting (3DGS)3D <div>
arXiv:2504.06827v1 Announce Type: new 
Abstract: This work presents IAAO, a novel framework that builds an explicit 3D model for intelligent agents to gain understanding of articulated objects in their environment through interaction. Unlike prior methods that rely on task-specific networks and assumptions about movable parts, our IAAO leverages large foundation models to estimate interactive affordances and part articulations in three stages. We first build hierarchical features and label fields for each object state using 3D Gaussian Splatting (3DGS) by distilling mask features and view-consistent labels from multi-view images. We then perform object- and part-level queries on the 3D Gaussian primitives to identify static and articulated elements, estimating global transformations and local articulation parameters along with affordances. Finally, scenes from different states are merged and refined based on the estimated transformations, enabling robust affordance-based interaction and manipulation of objects. Experimental results demonstrate the effectiveness of our method.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games</title>
<link>https://arxiv.org/abs/2504.06868</link>
<guid>https://arxiv.org/abs/2504.06868</guid>
<content:encoded><![CDATA[
<div> : PANDA

:
PANDA <div>
arXiv:2504.06868v1 Announce Type: new 
Abstract: Artificial agents are increasingly central to complex interactions and decision-making tasks, yet aligning their behaviors with desired human values remains an open challenge. In this work, we investigate how human-like personality traits influence agent behavior and performance within text-based interactive environments. We introduce PANDA: PersonalityAdapted Neural Decision Agents, a novel method for projecting human personality traits onto agents to guide their behavior. To induce personality in a text-based game agent, (i) we train a personality classifier to identify what personality type the agent's actions exhibit, and (ii) we integrate the personality profiles directly into the agent's policy-learning pipeline. By deploying agents embodying 16 distinct personality types across 25 text-based games and analyzing their trajectories, we demonstrate that an agent's action decisions can be guided toward specific personality profiles. Moreover, certain personality types, such as those characterized by higher levels of Openness, display marked advantages in performance. These findings underscore the promise of personality-adapted agents for fostering more aligned, effective, and human-centric decision-making in interactive environments.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Driven Consensus: Modeling Multi-Agent Networks with Long-Range Interactions through path-Laplacian Matrices</title>
<link>https://arxiv.org/abs/2504.06894</link>
<guid>https://arxiv.org/abs/2504.06894</guid>
<content:encoded><![CDATA[
<div> : kLSTMxLSTMTransformerXGBoostConvLSTMErds-RnyiWatts-StrogatzBarabsi-Albert

<br /><br />:
kLSTMxLSTMTransformerXGBoostConvLSTMErds-RnyiWatts-StrogatzBarabsi-Albertk <div>
arXiv:2504.06894v1 Announce Type: new 
Abstract: Extended connectivity in graphs can be analyzed through k-path Laplacian matrices, which permit the capture of long-range interactions in various real-world networked systems such as social, transportation, and multi-agent networks. In this work, we present several alternative methods based on machine learning methods (LSTM, xLSTM, Transformer, XGBoost, and ConvLSTM) to predict the final consensus value based on directed networks (Erd\"os-Renyi, Watts-Strogatz, and Barab\'asi-Albert) and on the initial state. We highlight how different k-hop interactions affect the performance of the tested methods. This framework opens new avenues for analyzing multi-scale diffusion processes in large-scale, complex networks.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Review of Case-Based Reasoning for LLM Agents: Theoretical Foundations, Architectural Components, and Cognitive Integration</title>
<link>https://arxiv.org/abs/2504.06943</link>
<guid>https://arxiv.org/abs/2504.06943</guid>
<content:encoded><![CDATA[
<div> Large Language Models (LLMs)Case-Based Reasoning (CBR)flexibilityreasoning skillscognitive aspects

<br /><br />:
(CBR)(LLMs)CBRLLMsCBRCBRChain-of-ThoughtCBRLLMCBRLLM <div>
arXiv:2504.06943v1 Announce Type: new 
Abstract: Agents powered by Large Language Models (LLMs) have recently demonstrated impressive capabilities in various tasks. Still, they face limitations in tasks requiring specific, structured knowledge, flexibility, or accountable decision-making. While agents are capable of perceiving their environments, forming inferences, planning, and executing actions towards goals, they often face issues such as hallucinations and lack of contextual memory across interactions. This paper explores how Case-Based Reasoning (CBR), a strategy that solves new problems by referencing past experiences, can be integrated into LLM agent frameworks. This integration allows LLMs to leverage explicit knowledge, enhancing their effectiveness. We systematically review the theoretical foundations of these enhanced agents, identify critical framework components, and formulate a mathematical model for the CBR processes of case retrieval, adaptation, and learning. We also evaluate CBR-enhanced agents against other methods like Chain-of-Thought reasoning and standard Retrieval-Augmented Generation, analyzing their relative strengths. Moreover, we explore how leveraging CBR's cognitive dimensions (including self-reflection, introspection, and curiosity) via goal-driven autonomy mechanisms can further enhance the LLM agent capabilities. Contributing to the ongoing research on neuro-symbolic hybrid systems, this work posits CBR as a viable technique for enhancing the reasoning skills and cognitive aspects of autonomous LLM agents.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Agentic Framework for Evaluating Conditional Image Generation</title>
<link>https://arxiv.org/abs/2504.07046</link>
<guid>https://arxiv.org/abs/2504.07046</guid>
<content:encoded><![CDATA[
<div> : CIGEvalGPT-4o

:
CIGEvalCIGEvalGPT-4oCIGEval0.46250.472.3K7BCIGEvalGPT-4oGPT-4oCIGEval <div>
arXiv:2504.07046v1 Announce Type: new 
Abstract: Conditional image generation has gained significant attention for its ability to personalize content. However, the field faces challenges in developing task-agnostic, reliable, and explainable evaluation metrics. This paper introduces CIGEval, a unified agentic framework for comprehensive evaluation of conditional image generation tasks. CIGEval utilizes large multimodal models (LMMs) as its core, integrating a multi-functional toolbox and establishing a fine-grained evaluation framework. Additionally, we synthesize evaluation trajectories for fine-tuning, empowering smaller LMMs to autonomously select appropriate tools and conduct nuanced analyses based on tool outputs. Experiments across seven prominent conditional image generation tasks demonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625 with human assessments, closely matching the inter-annotator correlation of 0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K training trajectories, CIGEval surpasses the previous GPT-4o-based state-of-the-art method. Case studies on GPT-4o image generation highlight CIGEval's capability in identifying subtle issues related to subject consistency and adherence to control guidance, indicating its great potential for automating evaluation of image generation tasks with human-level reliability.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SkillWeaver: Web Agents can Self-Improve by Discovering and Honing Skills</title>
<link>https://arxiv.org/abs/2504.07079</link>
<guid>https://arxiv.org/abs/2504.07079</guid>
<content:encoded><![CDATA[
<div> SkillWeaverAPIWeb

:
<br />
SkillWeaverAPIAPIAPIWebArenaSkillWeaver31.8%39.8%APIWebArena54.3%API <div>
arXiv:2504.07079v1 Announce Type: new 
Abstract: To survive and thrive in complex environments, humans have evolved sophisticated self-improvement mechanisms through environment exploration, hierarchical abstraction of experiences into reuseable skills, and collaborative construction of an ever-growing skill repertoire. Despite recent advancements, autonomous web agents still lack crucial self-improvement capabilities, struggling with procedural knowledge abstraction, refining skills, and skill composition. In this work, we introduce SkillWeaver, a skill-centric framework enabling agents to self-improve by autonomously synthesizing reusable skills as APIs. Given a new website, the agent autonomously discovers skills, executes them for practice, and distills practice experiences into robust APIs. Iterative exploration continually expands a library of lightweight, plug-and-play APIs, significantly enhancing the agent's capabilities. Experiments on WebArena and real-world websites demonstrate the efficacy of SkillWeaver, achieving relative success rate improvements of 31.8% and 39.8%, respectively. Additionally, APIs synthesized by strong agents substantially enhance weaker agents through transferable skills, yielding improvements of up to 54.3% on WebArena. These results demonstrate the effectiveness of honing diverse website interactions into APIs, which can be seamlessly shared among various web agents.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Market, power, gift, and concession economies: Comparison using four-mode primitive network models</title>
<link>https://arxiv.org/abs/2504.06557</link>
<guid>https://arxiv.org/abs/2504.06557</guid>
<content:encoded><![CDATA[
<div> 

:
PolanyiKarataniGraeberWe-turn <div>
arXiv:2504.06557v1 Announce Type: cross 
Abstract: Reducing wealth inequality is a global challenge, and the problems of capitalism stem from the enclosure of the commons and the breakdown of the community. According to previous studies by Polanyi, Karatani, and Graeber, economic modes can be divided into capitalist market economy (enclosure and exchange), power economy (de-enclosure and redistribution), gift economy (obligation to return and reciprocity), and concession economy (de-obligation to return). The concession economy reflects Graeber's baseline communism (from each according to their abilities, to each according to their needs) and Deguchi's We-turn philosophy (the "I" as an individual has a "fundamental incapability" and the subject of physical action, responsibility, and freedom is "We" as a multi-agent system, including the "I"). In this study, we constructed novel network models for these four modes and compared their properties (cluster coefficient, graph density, reciprocity, assortativity, centrality, and Gini coefficient). From the calculation results, it became clear that the market economy leads to inequality; the power economy mitigates inequality but cannot eliminate it; the gift and concession economies lead to a healthy and equal economy; and the concession economy, free from the ties of obligation to return, is possible without guaranteeing reciprocity. We intend to promote the transformation from a capitalist economy to a concession economy through activities that disseminate baseline communism and the We-turn philosophy that promotes concession, that is, developing a cooperative platform to support concession through information technology and empirical research through fieldwork.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A primal-dual perspective for distributed TD-learning</title>
<link>https://arxiv.org/abs/2310.00638</link>
<guid>https://arxiv.org/abs/2310.00638</guid>
<content:encoded><![CDATA[
<div> 

:
TD-ODE-ODETD<br /><br /> <div>
arXiv:2310.00638v2 Announce Type: replace 
Abstract: The goal of this paper is to investigate distributed temporal difference (TD) learning for a networked multi-agent Markov decision process. The proposed approach is based on distributed optimization algorithms, which can be interpreted as primal-dual Ordinary differential equation (ODE) dynamics subject to null-space constraints. Based on the exponential convergence behavior of the primal-dual ODE dynamics subject to null-space constraints, we examine the behavior of the final iterate in various distributed TD-learning scenarios, considering both constant and diminishing step-sizes and incorporating both i.i.d. and Markovian observation models. Unlike existing methods, the proposed algorithm does not require the assumption that the underlying communication network structure is characterized by a doubly stochastic matrix.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approximate Feedback Nash Equilibria with Sparse Inter-Agent Dependencies</title>
<link>https://arxiv.org/abs/2410.16441</link>
<guid>https://arxiv.org/abs/2410.16441</guid>
<content:encoded><![CDATA[
<div> : Lasso

<br /><br />:
LassoLQLQ77% <div>
arXiv:2410.16441v2 Announce Type: replace 
Abstract: Feedback Nash equilibrium strategies in multi-agent dynamic games require availability of all players' state information to compute control actions. However, in real-world scenarios, sensing and communication limitations between agents make full state feedback expensive or impractical, and such strategies can become fragile when state information from other agents is inaccurate. To this end, we propose a regularized dynamic programming approach for finding sparse feedback policies that selectively depend on the states of a subset of agents in dynamic games. The proposed approach solves convex adaptive group Lasso problems to compute sparse policies approximating Nash equilibrium solutions. We prove the regularized solutions' asymptotic convergence to a neighborhood of Nash equilibrium policies in linear-quadratic (LQ) games. Further, we extend the proposed approach to general non-LQ games via an iterative algorithm. Simulation results in multi-robot interaction scenarios show that the proposed approach effectively computes feedback policies with varying sparsity levels. When agents have noisy observations of other agents' states, simulation results indicate that the proposed regularized policies consistently achieve lower costs than standard Nash equilibrium policies by up to 77% for all interacting agents whose costs are coupled with other agents' states.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation</title>
<link>https://arxiv.org/abs/2501.03225</link>
<guid>https://arxiv.org/abs/2501.03225</guid>
<content:encoded><![CDATA[
<div> : vision language models (VLMs), AutoConverter, visual question answering (VQA), multiple-choice format, VMCBench

:
VLMsAutoConverterAutoConverterAutoConverterVLMsAutoConverterVMCBench20VQA9,018VMCBench33VLMsVLMs <div>
arXiv:2501.03225v2 Announce Type: replace 
Abstract: The rapid development of vision language models (VLMs) demands rigorous and reliable evaluation. However, current visual question answering (VQA) benchmarks often depend on open-ended questions, making accurate evaluation difficult due to the variability in natural language responses. To address this, we introduce AutoConverter, an agentic framework that automatically converts these open-ended questions into multiple-choice format, enabling objective evaluation while reducing the costly multiple-choice question creation process. Our experiments demonstrate that AutoConverter can generate correct and challenging multiple-choice questions, with VLMs demonstrating consistently similar or lower accuracy on these questions compared to human-created ones. Using AutoConverter, we construct VMCBench, a benchmark created by transforming 20 existing VQA datasets into a unified multiple-choice format, totaling 9,018 questions. We comprehensively evaluate 33 state-of-the-art VLMs on VMCBench, setting a new standard for scalable, consistent, and reproducible VLM evaluation.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Adaptive Memory-Based Optimization for Enhanced Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2504.05312</link>
<guid>https://arxiv.org/abs/2504.05312</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation (RAG)Adaptive memory-based optimization (Amber)

:
Adaptive memory-based optimization for enhanced RAG (Amber)RAGAmberAgentAmberAmberQA <div>
arXiv:2504.05312v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG), by integrating non-parametric knowledge from external knowledge bases into models, has emerged as a promising approach to enhancing response accuracy while mitigating factual errors and hallucinations. This method has been widely applied in tasks such as Question Answering (QA). However, existing RAG methods struggle with open-domain QA tasks because they perform independent retrieval operations and directly incorporate the retrieved information into generation without maintaining a summarizing memory or using adaptive retrieval strategies, leading to noise from redundant information and insufficient information integration. To address these challenges, we propose Adaptive memory-based optimization for enhanced RAG (Amber) for open-domain QA tasks, which comprises an Agent-based Memory Updater, an Adaptive Information Collector, and a Multi-granular Content Filter, working together within an iterative memory updating paradigm. Specifically, Amber integrates and optimizes the language model's memory through a multi-agent collaborative approach, ensuring comprehensive knowledge integration from previous retrieval steps. It dynamically adjusts retrieval queries and decides when to stop retrieval based on the accumulated knowledge, enhancing retrieval efficiency and effectiveness. Additionally, it reduces noise by filtering irrelevant content at multiple levels, retaining essential information to improve overall model performance. We conduct extensive experiments on several open-domain QA datasets, and the results demonstrate the superiority and effectiveness of our method and its components. The source code is available \footnote{https://anonymous.4open.science/r/Amber-B203/}.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Benefits and Risks: RL Approaches for Addiction-Aware Social Media Recommenders</title>
<link>https://arxiv.org/abs/2504.05322</link>
<guid>https://arxiv.org/abs/2504.05322</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:
RLMBMF <div>
arXiv:2504.05322v1 Announce Type: new 
Abstract: Social media platforms provide valuable opportunities for users to gather information, interact with friends, and enjoy entertainment. However, their addictive potential poses significant challenges, including overuse and negative psycho-logical or behavioral impacts [4, 2, 8]. This study explores strategies to mitigate compulsive social media usage while preserving its benefits and ensuring economic sustainability, focusing on recommenders that promote balanced usage.
  We analyze user behaviors arising from intrinsic diversities and environmental interactions, offering insights for next-generation social media recommenders that prioritize well-being. Specifically, we examine the temporal predictability of overuse and addiction using measures available to recommenders, aiming to inform mechanisms that prevent addiction while avoiding user disengagement [7].
  Building on RL-based computational frameworks for addiction modelling [6], our study introduces: - A recommender system adapting to user preferences, introducing non-stationary and non-Markovian dynamics.
  - Differentiated state representations for users and recommenders to capture nuanced interactions.
  - Distinct usage conditions-light and heavy use-addressing RL's limitations in distinguishing prolonged from healthy engagement.
  - Complexity in overuse impacts, highlighting their role in user adaptation [7].
  Simulations demonstrate how model-based (MB) and model-free (MF) decision-making interact with environmental dynamics to influence user behavior and addiction. Results reveal the significant role of recommender systems in shaping addiction tendencies or fostering healthier engagement. These findings support ethical, adaptive recommender design, advancing sustainable social media ecosystems [9, 1].
  Keywords: multi-agent systems, recommender systems, addiction, social media
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unequal Opportunities: Examining the Bias in Geographical Recommendations by Large Language Models</title>
<link>https://arxiv.org/abs/2504.05325</link>
<guid>https://arxiv.org/abs/2504.05325</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)

:
LLMsLLMsLLMs <div>
arXiv:2504.05325v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) have made them a popular information-seeking tool among end users. However, the statistical training methods for LLMs have raised concerns about their representation of under-represented topics, potentially leading to biases that could influence real-world decisions and opportunities. These biases could have significant economic, social, and cultural impacts as LLMs become more prevalent, whether through direct interactions--such as when users engage with chatbots or automated assistants--or through their integration into third-party applications (as agents), where the models influence decision-making processes and functionalities behind the scenes. Our study examines the biases present in LLMs recommendations of U.S. cities and towns across three domains: relocation, tourism, and starting a business. We explore two key research questions: (i) How similar LLMs responses are, and (ii) How this similarity might favor areas with certain characteristics over others, introducing biases. We focus on the consistency of LLMs responses and their tendency to over-represent or under-represent specific locations. Our findings point to consistent demographic biases in these recommendations, which could perpetuate a ``rich-get-richer'' effect that widens existing economic disparities.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Price Inflation on Algorithmic Collusion Through Reinforcement Learning Agents</title>
<link>https://arxiv.org/abs/2504.05335</link>
<guid>https://arxiv.org/abs/2504.05335</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:
AI <div>
arXiv:2504.05335v1 Announce Type: new 
Abstract: Algorithmic pricing is increasingly shaping market competition, raising concerns about its potential to compromise competitive dynamics. While prior work has shown that reinforcement learning (RL)-based pricing algorithms can lead to tacit collusion, less attention has been given to the role of macroeconomic factors in shaping these dynamics. This study examines the role of inflation in influencing algorithmic collusion within competitive markets. By incorporating inflation shocks into a RL-based pricing model, we analyze whether agents adapt their strategies to sustain supra-competitive profits. Our findings indicate that inflation reduces market competitiveness by fostering implicit coordination among agents, even without direct collusion. However, despite achieving sustained higher profitability, agents fail to develop robust punishment mechanisms to deter deviations from equilibrium strategies. The results suggest that inflation amplifies non-competitive dynamics in algorithmic pricing, emphasizing the need for regulatory oversight in markets where AI-driven pricing is prevalent.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DyTTP: Trajectory Prediction with Normalization-Free Transformers</title>
<link>https://arxiv.org/abs/2504.05356</link>
<guid>https://arxiv.org/abs/2504.05356</guid>
<content:encoded><![CDATA[
<div> : TransformerDynamicTanh (DyT)snapshot ensemble

:<br />
(DynamicTanh, DyT)TransformerDyTensembleArgoverseTransformer <div>
arXiv:2504.05356v1 Announce Type: new 
Abstract: Accurate trajectory prediction is a cornerstone for the safe operation of autonomous driving systems, where understanding the dynamic behavior of surrounding agents is crucial. Transformer-based architectures have demonstrated significant promise in capturing complex spatio-temporality dependencies. However, their reliance on normalization layers can lead to computation overhead and training instabilities. In this work, we present a two-fold approach to address these challenges. First, we integrate DynamicTanh (DyT), which is the latest method to promote transformers, into the backbone, replacing traditional layer normalization. This modification simplifies the network architecture and improves the stability of the inference. We are the first work to deploy the DyT to the trajectory prediction task. Complementing this, we employ a snapshot ensemble strategy to further boost trajectory prediction performance. Using cyclical learning rate scheduling, multiple model snapshots are captured during a single training run. These snapshots are then aggregated via simple averaging at inference time, allowing the model to benefit from diverse hypotheses without incurring substantial additional computational cost. Extensive experiments on Argoverse datasets demonstrate that our combined approach significantly improves prediction accuracy, inference speed and robustness in diverse driving scenarios. This work underscores the potential of normalization-free transformer designs augmented with lightweight ensemble techniques in advancing trajectory forecasting for autonomous vehicles.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debate-Feedback: A Multi-Agent Framework for Efficient Legal Judgment Prediction</title>
<link>https://arxiv.org/abs/2504.05358</link>
<guid>https://arxiv.org/abs/2504.05358</guid>
<content:encoded><![CDATA[
<div> : AI

<br /><br />:
LegalAI <div>
arXiv:2504.05358v1 Announce Type: new 
Abstract: The use of AI in legal analysis and prediction (LegalAI) has gained widespread attention, with past research focusing on retrieval-based methods and fine-tuning large models. However, these approaches often require large datasets and underutilize the capabilities of modern large language models (LLMs). In this paper, inspired by the debate phase of real courtroom trials, we propose a novel legal judgment prediction model based on the Debate-Feedback architecture, which integrates LLM multi-agent debate and reliability evaluation models. Unlike traditional methods, our model achieves significant improvements in efficiency by minimizing the need for large historical datasets, thus offering a lightweight yet robust solution. Comparative experiments show that it outperforms several general-purpose and domain-specific legal models, offering a dynamic reasoning process and a promising direction for future LegalAI research.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Nature-Inspired Colony of Artificial Intelligence System with Fast, Detailed, and Organized Learner Agents for Enhancing Diversity and Quality</title>
<link>https://arxiv.org/abs/2504.05365</link>
<guid>https://arxiv.org/abs/2504.05365</guid>
<content:encoded><![CDATA[
<div> (CNN)(AI)

<br /><br />:
CNNAIAIVGG16VGG19ResNet50AIAIAIAIVGG16VGG19ResNet50AIF182%95%AI <div>
arXiv:2504.05365v1 Announce Type: new 
Abstract: The concepts of convolutional neural networks (CNNs) and multi-agent systems are two important areas of research in artificial intelligence (AI). In this paper, we present an approach that builds a CNN-based colony of AI agents to serve as a single system and perform multiple tasks (e.g., predictions or classifications) in an environment. The proposed system impersonates the natural environment of a biological system, like an ant colony or a human colony. The proposed colony of AI that is defined as a role-based system uniquely contributes to accomplish tasks in an environment by incorporating AI agents that are fast learners, detailed learners, and organized learners. These learners can enhance their localized learning and their collective decisions as a single system of colony of AI agents. This approach also enhances the diversity and quality of the colony of AI with the help of Genetic Algorithms and their crossover and mutation mechanisms. The evolution of fast, detailed, and organized learners in the colony of AI is achieved by introducing a unique one-to-one mapping between these learners and the pretrained VGG16, VGG19, and ResNet50 models, respectively. This role-based approach creates two parent-AI agents using the AI models through the processes, called the intra- and inter-marriage of AI, so that they can share their learned knowledge (weights and biases) based on a probabilistic rule and produce diversified child-AI agents to perform new tasks. This process will form a colony of AI that consists of families of multi-model and mixture-model AI agents to improve diversity and quality. Simulations show that the colony of AI, built using the VGG16, VGG19, and ResNet50 models, can provide a single system that generates child-AI agents of excellent predictive performance, ranging between 82% and 95% of F1-scores, to make diversified collective and quality decisions on a task.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EduPlanner: LLM-Based Multi-Agent Systems for Customized and Intelligent Instructional Design</title>
<link>https://arxiv.org/abs/2504.05370</link>
<guid>https://arxiv.org/abs/2504.05370</guid>
<content:encoded><![CDATA[
<div> :  (LLMs), , , , 

:
EduPlannerLLMsAGIEduPlannerLLMCIDDPEduPlannerhttps://github.com/Zc0812/Edu_Planner <div>
arXiv:2504.05370v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have significantly advanced smart education in the Artificial General Intelligence (AGI) era. A promising application lies in the automatic generalization of instructional design for curriculum and learning activities, focusing on two key aspects: (1) Customized Generation: generating niche-targeted teaching content based on students' varying learning abilities and states, and (2) Intelligent Optimization: iteratively optimizing content based on feedback from learning effectiveness or test scores. Currently, a single large LLM cannot effectively manage the entire process, posing a challenge for designing intelligent teaching plans. To address these issues, we developed EduPlanner, an LLM-based multi-agent system comprising an evaluator agent, an optimizer agent, and a question analyst, working in adversarial collaboration to generate customized and intelligent instructional design for curriculum and learning activities. Taking mathematics lessons as our example, EduPlanner employs a novel Skill-Tree structure to accurately model the background mathematics knowledge of student groups, personalizing instructional design for curriculum and learning activities according to students' knowledge levels and learning abilities. Additionally, we introduce the CIDDP, an LLM-based five-dimensional evaluation module encompassing clarity, Integrity, Depth, Practicality, and Pertinence, to comprehensively assess mathematics lesson plan quality and bootstrap intelligent optimization. Experiments conducted on the GSM8K and Algebra datasets demonstrate that EduPlanner excels in evaluating and optimizing instructional design for curriculum and learning activities. Ablation studies further validate the significance and effectiveness of each component within the framework. Our code is publicly available at https://github.com/Zc0812/Edu_Planner
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interactive Explanations for Reinforcement-Learning Agents</title>
<link>https://arxiv.org/abs/2504.05393</link>
<guid>https://arxiv.org/abs/2504.05393</guid>
<content:encoded><![CDATA[
<div> : 

:
ASQ-ITXRLASQ-ITLTLfASQ-ITASQ-IT <div>
arXiv:2504.05393v1 Announce Type: new 
Abstract: As reinforcement learning methods increasingly amass accomplishments, the need for comprehending their solutions becomes more crucial. Most explainable reinforcement learning (XRL) methods generate a static explanation depicting their developers' intuition of what should be explained and how. In contrast, literature from the social sciences proposes that meaningful explanations are structured as a dialog between the explainer and the explainee, suggesting a more active role for the user and her communication with the agent. In this paper, we present ASQ-IT -- an interactive explanation system that presents video clips of the agent acting in its environment based on queries given by the user that describe temporal properties of behaviors of interest. Our approach is based on formal methods: queries in ASQ-IT's user interface map to a fragment of Linear Temporal Logic over finite traces (LTLf), which we developed, and our algorithm for query processing is based on automata theory. User studies show that end-users can understand and formulate queries in ASQ-IT and that using ASQ-IT assists users in identifying faulty agent behaviors.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-informed Modularized Neural Network for Advanced Building Control by Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.05397</link>
<guid>https://arxiv.org/abs/2504.05397</guid>
<content:encoded><![CDATA[
<div> :  (PIML), ,  (PI-ModNN),  (RL), 

:
(PI-ModNN)PIMLPIML(DRL)DRL31.4% <div>
arXiv:2504.05397v1 Announce Type: new 
Abstract: Physics-informed machine learning (PIML) provides a promising solution for building energy modeling and can serve as a virtual environment to enable reinforcement learning (RL) agents to interact and learn. However, challenges remain in efficiently integrating physics priors, evaluating the effectiveness of physics constraints, balancing model accuracy and physics consistency, and enabling real-world implementation. To address these gaps, this study introduces a Physics-Informed Modularized Neural Network (PI-ModNN), which incorporates physics priors through a physics-informed model structure, loss functions, and hard constraints. A new evaluation metric called "temperature response violation" is developed to quantify the physical consistency of data-driven building dynamic models under varying control inputs and training data sizes. Additionally, a physics prior evaluation framework based on rule importance is proposed to assess the contribution of each individual physics prior, offering guidance on selecting appropriate PIML techniques. Results indicate that incorporating physical priors does not always improve model performance; inappropriate priors may decrease model accuracy and consistency. However, hard constraints are effective in enforcing model consistency. Furthermore, we present a general workflow for developing control-oriented PIML models and integrating them with deep reinforcement learning (DRL). Following this framework, a case study implementing DRL in an office space over three months demonstrates potential energy savings of 31.4%. Finally, we provide a general guideline for integrating data-driven models with advanced building control through a four-step evaluation framework, paving the way for reliable and scalable deployment of advanced building controls.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRATSS: Transformer-Based Task Scheduling System for Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2504.05407</link>
<guid>https://arxiv.org/abs/2504.05407</guid>
<content:encoded><![CDATA[
<div> : Transformer-Based Task Scheduling System (TRATSS)

<br /><br />:

Transformer-Based Task Scheduling System (TRATSS)transformertransformerTRATSSTRATSS <div>
arXiv:2504.05407v1 Announce Type: new 
Abstract: Efficient scheduling remains a critical challenge in various domains, requiring solutions to complex NP-hard optimization problems to achieve optimal resource allocation and maximize productivity. In this paper, we introduce a framework called Transformer-Based Task Scheduling System (TRATSS), designed to address the intricacies of single agent scheduling in graph-based environments. By integrating the latest advancements in reinforcement learning and transformer architecture, TRATSS provides a novel system that outputs optimized task scheduling decisions while dynamically adapting to evolving task requirements and resource availability. Leveraging the self-attention mechanism in transformers, TRATSS effectively captures complex task dependencies, thereby providing solutions with enhanced resource utilization and task completion efficiency. Experimental evaluations on benchmark datasets demonstrate TRATSS's effectiveness in providing high-quality solutions to scheduling problems that involve multiple action profiles.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoK: Frontier AI's Impact on the Cybersecurity Landscape</title>
<link>https://arxiv.org/abs/2504.05408</link>
<guid>https://arxiv.org/abs/2504.05408</guid>
<content:encoded><![CDATA[
<div> : AI

:
AIAIAIAIAIAI <div>
arXiv:2504.05408v1 Announce Type: new 
Abstract: As frontier AI advances rapidly, understanding its impact on cybersecurity and inherent risks is essential to ensuring safe AI evolution (e.g., guiding risk mitigation and informing policymakers). While some studies review AI applications in cybersecurity, none of them comprehensively discuss AI's future impacts or provide concrete recommendations for navigating its safe and secure usage. This paper presents an in-depth analysis of frontier AI's impact on cybersecurity and establishes a systematic framework for risk assessment and mitigation. To this end, we first define and categorize the marginal risks of frontier AI in cybersecurity and then systemically analyze the current and future impacts of frontier AI in cybersecurity, qualitatively and quantitatively. We also discuss why frontier AI likely benefits attackers more than defenders in the short term from equivalence classes, asymmetry, and economic impact. Next, we explore frontier AI's impact on future software system development, including enabling complex hybrid systems while introducing new risks. Based on our findings, we provide security recommendations, including constructing fine-grained benchmarks for risk assessment, designing AI agents for defenses, building security mechanisms and provable defenses for hybrid systems, enhancing pre-deployment security testing and transparency, and strengthening defenses for users. Finally, we present long-term research questions essential for understanding AI's future impacts and unleashing its defensive capabilities.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EP-Diffuser: An Efficient Diffusion Model for Traffic Scene Generation and Prediction via Polynomial Representations</title>
<link>https://arxiv.org/abs/2504.05422</link>
<guid>https://arxiv.org/abs/2504.05422</guid>
<content:encoded><![CDATA[
<div> : EP-Diffuser

:
EP-DiffuserArgoverse 2EP-DiffuserWaymo OpenEP-Diffuserhttps://github.com/continental/EP-Diffuser <div>
arXiv:2504.05422v1 Announce Type: new 
Abstract: As the prediction horizon increases, predicting the future evolution of traffic scenes becomes increasingly difficult due to the multi-modal nature of agent motion. Most state-of-the-art (SotA) prediction models primarily focus on forecasting the most likely future. However, for the safe operation of autonomous vehicles, it is equally important to cover the distribution for plausible motion alternatives. To address this, we introduce EP-Diffuser, a novel parameter-efficient diffusion-based generative model designed to capture the distribution of possible traffic scene evolutions. Conditioned on road layout and agent history, our model acts as a predictor and generates diverse, plausible scene continuations. We benchmark EP-Diffuser against two SotA models in terms of accuracy and plausibility of predictions on the Argoverse 2 dataset. Despite its significantly smaller model size, our approach achieves both highly accurate and plausible traffic scene predictions. We further evaluate model generalization ability in an out-of-distribution (OoD) test setting using Waymo Open dataset and show superior robustness of our approach. The code and model checkpoints can be found here: https://github.com/continental/EP-Diffuser.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Broadcast via Mobile Agents in a Dynamic Network: Interplay of Graph Properties &amp; Agents</title>
<link>https://arxiv.org/abs/2504.05442</link>
<guid>https://arxiv.org/abs/2504.05442</guid>
<content:encoded><![CDATA[
<div> Broadcast


Das2020Broadcast$k+1$$n$$k$$1.1\overline{6}$$\Omega(n)$$1.1\overline{6}$$o(n)$1$\Omega(n/f(n))$$f(n)\to \infty$$n\to \infty$$\rho$$k>0$$\rho>1$$k$Broadcast

<br /><br />: Broadcast$1.1\overline{6}$$o(n)$1$\Omega(n/f(n))$$k>0$$\rho>1$$\rho$$k$ <div>
arXiv:2504.05442v1 Announce Type: new 
Abstract: In this paper, we revisit the problem of \textsc{Broadcast}, introduced by Das, Giachoudis, Luccio, and Markou [OPODIS, 2020], where $k+1$ agents are initially placed on an $n$ node dynamic graph, where $1$ agent has a message that must be broadcast to the remaining $k$ ignorant agents. The original paper studied the relationship between the number of agents needed to solve the problem and the edge density of the graph. The paper presented strong evidence that edge density of a graph, or the number of redundant edges within the graph, may be the correct graph property to accurately differentiate whether $k= o(n)$ agents (low edge density) or $k = \Omega(n)$ agents (high edge density) are needed to solve the problem.
  In this paper, we show that surprisingly, edge density may not in fact be the correct differentiating property. The original paper presents graphs with edge density $1.1\overline{6}$ that require $\Omega(n)$ agents, however, we construct graphs with edge density $> 1.1\overline{6}$ and develop an algorithm to solve the problem on those graphs using only $o(n)$ agents. We subsequently show that the relationship between edge density and number of agents is fairly weak by first constructing graphs with edge density tending to $1$ from above that require $\Omega(n/f(n))$ agents to solve, for any function $f(n) \to \infty$ as $n \to \infty$. We then construct an infinite family of graphs with edge density $< \rho$ requiring exactly $k$ ignorant agents to solve \textsc{Broadcast}, for any $k>0$ and $\rho>1$.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prism: Dynamic and Flexible Benchmarking of LLMs Code Generation with Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2504.05500</link>
<guid>https://arxiv.org/abs/2504.05500</guid>
<content:encoded><![CDATA[
<div> :  (LLMs), Prism, , , 

<br /><br />:
PrismLLMsLLMPrism1) 2) 3) PrismLLMsPrism <div>
arXiv:2504.05500v1 Announce Type: new 
Abstract: The rapid advancement of Large Language Models (LLMs) has outpaced traditional evaluation methods. Static benchmarks fail to capture the depth and breadth of LLM capabilities and eventually become obsolete, while most dynamic approaches either rely too heavily on LLM-based evaluation or remain constrained by predefined test sets. We introduce Prism, a flexible, dynamic benchmarking framework designed for comprehensive LLM assessment. Prism builds on three key components: (1) a tree-based state representation that models evaluation as a Markov Decision Process, (2) a Monte Carlo Tree Search algorithm adapted to uncover challenging evaluation scenarios, and (3) a multi-agent evaluation pipeline that enables simultaneous assessment of diverse capabilities. To ensure robust evaluation, Prism integrates structural measurements of tree exploration patterns with performance metrics across difficulty levels, providing detailed diagnostics of error patterns, test coverage, and solution approaches. Through extensive experiments on five state-of-the-art LLMs, we analyze how model architecture and scale influence code generation performance across varying task difficulties. Our results demonstrate Prism's effectiveness as a dynamic benchmark that evolves with model advancements while offering deeper insights into their limitations.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Hierarchical Reinforcement Learning for Adaptive Traffic Signal Control</title>
<link>https://arxiv.org/abs/2504.05553</link>
<guid>https://arxiv.org/abs/2504.05553</guid>
<content:encoded><![CDATA[
<div> MARLATSCFLFedAvgHFRL

:<br />
ATSCHFRLMARLFLFedAvgHFRLFedAvgHFRL <div>
arXiv:2504.05553v1 Announce Type: new 
Abstract: Multi-agent reinforcement learning (MARL) has shown promise for adaptive traffic signal control (ATSC), enabling multiple intersections to coordinate signal timings in real time. However, in large-scale settings, MARL faces constraints due to extensive data sharing and communication requirements. Federated learning (FL) mitigates these challenges by training shared models without directly exchanging raw data, yet traditional FL methods such as FedAvg struggle with highly heterogeneous intersections. Different intersections exhibit varying traffic patterns, demands, and road structures, so performing FedAvg across all agents is inefficient. To address this gap, we propose Hierarchical Federated Reinforcement Learning (HFRL) for ATSC. HFRL employs clustering-based or optimization-based techniques to dynamically group intersections and perform FedAvg independently within groups of intersections with similar characteristics, enabling more effective coordination and scalability than standard FedAvg. Our experiments on synthetic and real-world traffic networks demonstrate that HFRL not only outperforms both decentralized and standard federated RL approaches but also identifies suitable grouping patterns based on network structure or traffic demand, resulting in a more robust framework for distributed, heterogeneous systems.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciSciGPT: Advancing Human-AI Collaboration in the Science of Science</title>
<link>https://arxiv.org/abs/2504.05559</link>
<guid>https://arxiv.org/abs/2504.05559</guid>
<content:encoded><![CDATA[
<div> : AISciSciGPT

<br /><br />:

SciSciGPTAISciSciGPT-AIAISciSciGPTAIAI <div>
arXiv:2504.05559v1 Announce Type: new 
Abstract: The increasing availability of large-scale datasets has fueled rapid progress across many scientific fields, creating unprecedented opportunities for research and discovery while posing significant analytical challenges. Recent advances in large language models (LLMs) and AI agents have opened new possibilities for human-AI collaboration, offering powerful tools to navigate this complex research landscape. In this paper, we introduce SciSciGPT, an open-source, prototype AI collaborator that uses the science of science as a testbed to explore the potential of LLM-powered research tools. SciSciGPT automates complex workflows, supports diverse analytical approaches, accelerates research prototyping and iteration, and facilitates reproducibility. Through case studies, we demonstrate its ability to streamline a wide range of empirical and analytical research tasks while highlighting its broader potential to advance research. We further propose an LLM Agent capability maturity model for human-AI collaboration, envisioning a roadmap to further improve and expand upon frameworks like SciSciGPT. As AI capabilities continue to evolve, frameworks like SciSciGPT may play increasingly pivotal roles in scientific research and discovery, unlocking further opportunities. At the same time, these new advances also raise critical challenges, from ensuring transparency and ethical use to balancing human and AI contributions. Addressing these issues may shape the future of scientific inquiry and inform how we train the next generation of scientists to thrive in an increasingly AI-integrated research ecosystem.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TW-CRL: Time-Weighted Contrastive Reward Learning for Efficient Inverse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.05585</link>
<guid>https://arxiv.org/abs/2504.05585</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:
episodic(TW-CRL)TW-CRLTW-CRL <div>
arXiv:2504.05585v1 Announce Type: new 
Abstract: Episodic tasks in Reinforcement Learning (RL) often pose challenges due to sparse reward signals and high-dimensional state spaces, which hinder efficient learning. Additionally, these tasks often feature hidden "trap states" -- irreversible failures that prevent task completion but do not provide explicit negative rewards to guide agents away from repeated errors. To address these issues, we propose Time-Weighted Contrastive Reward Learning (TW-CRL), an Inverse Reinforcement Learning (IRL) framework that leverages both successful and failed demonstrations. By incorporating temporal information, TW-CRL learns a dense reward function that identifies critical states associated with success or failure. This approach not only enables agents to avoid trap states but also encourages meaningful exploration beyond simple imitation of expert trajectories. Empirical evaluations on navigation tasks and robotic manipulation benchmarks demonstrate that TW-CRL surpasses state-of-the-art methods, achieving improved efficiency and robustness.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FactGuard: Leveraging Multi-Agent Systems to Generate Answerable and Unanswerable Questions for Enhanced Long-Context LLM Extraction</title>
<link>https://arxiv.org/abs/2504.05607</link>
<guid>https://arxiv.org/abs/2504.05607</guid>
<content:encoded><![CDATA[
<div> agentFactGuard-Bench

<br /><br />:
agent-25,2208K128KFactGuard-Bench61.79%agentLLM <div>
arXiv:2504.05607v1 Announce Type: new 
Abstract: Extractive reading comprehension systems are designed to locate the correct answer to a question within a given text. However, a persistent challenge lies in ensuring these models maintain high accuracy in answering questions while reliably recognizing unanswerable queries. Despite significant advances in large language models (LLMs) for reading comprehension, this issue remains critical, particularly as the length of supported contexts continues to expand. To address this challenge, we propose an innovative data augmentation methodology grounded in a multi-agent collaborative framework. Unlike traditional methods, such as the costly human annotation process required for datasets like SQuAD 2.0, our method autonomously generates evidence-based question-answer pairs and systematically constructs unanswerable questions. Using this methodology, we developed the FactGuard-Bench dataset, which comprises 25,220 examples of both answerable and unanswerable question scenarios, with context lengths ranging from 8K to 128K. Experimental evaluations conducted on seven popular LLMs reveal that even the most advanced models achieve only 61.79% overall accuracy. Furthermore, we emphasize the importance of a model's ability to reason about unanswerable questions to avoid generating plausible but incorrect answers. By implementing efficient data selection and generation within the multi-agent collaborative framework, our method significantly reduces the traditionally high costs associated with manual annotation and provides valuable insights for the training and optimization of LLMs.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Agnostic Policy Explanations with Large Language Models</title>
<link>https://arxiv.org/abs/2504.05625</link>
<guid>https://arxiv.org/abs/2504.05625</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />:
 <div>
arXiv:2504.05625v1 Announce Type: new 
Abstract: Intelligent agents, such as robots, are increasingly deployed in real-world, human-centric environments. To foster appropriate human trust and meet legal and ethical standards, these agents must be able to explain their behavior. However, state-of-the-art agents are typically driven by black-box models like deep neural networks, limiting their interpretability. We propose a method for generating natural language explanations of agent behavior based only on observed states and actions -- without access to the agent's underlying model. Our approach learns a locally interpretable surrogate model of the agent's behavior from observations, which then guides a large language model to generate plausible explanations with minimal hallucination. Empirical results show that our method produces explanations that are more comprehensible and correct than those from baselines, as judged by both language models and human evaluators. Furthermore, we find that participants in a user study more accurately predicted the agent's future actions when given our explanations, suggesting improved understanding of agent behavior.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DBOT: Artificial Intelligence for Systematic Long-Term Investing</title>
<link>https://arxiv.org/abs/2504.05639</link>
<guid>https://arxiv.org/abs/2504.05639</guid>
<content:encoded><![CDATA[
<div> long-term investing, generative AI, DBOT, Aswath Damodaran, financial industry

<br /><br />:
DBOTAswath DamodaranDBOTDBOTDamodaranDBOT DamodaranDBOT <div>
arXiv:2504.05639v1 Announce Type: new 
Abstract: Long-term investing was previously seen as requiring human judgment. With the advent of generative artificial intelligence (AI) systems, automated systematic long-term investing is now feasible. In this paper, we present DBOT, a system whose goal is to reason about valuation like Aswath Damodaran, who is a unique expert in the investment arena in terms of having published thousands of valuations on companies in addition to his numerous writings on the topic, which provide ready training data for an AI system. DBOT can value any publicly traded company. DBOT can also be back-tested, making its behavior and performance amenable to scientific inquiry. We compare DBOT to its analytic parent, Damodaran, and highlight the research challenges involved in raising its current capability to that of Damodaran's. Finally, we examine the implications of DBOT-like AI agents for the financial industry, especially how they will impact the role of human analysts in valuation.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Archival Descriptions with Federated Intelligence of LLMs</title>
<link>https://arxiv.org/abs/2504.05711</link>
<guid>https://arxiv.org/abs/2504.05711</guid>
<content:encoded><![CDATA[
<div> : agentic AI

<br />
: agentic AIagentic AILLMLLM <div>
arXiv:2504.05711v1 Announce Type: new 
Abstract: Enforcing archival standards requires specialized expertise, and manually creating metadata descriptions for archival materials is a tedious and error-prone task. This work aims at exploring the potential of agentic AI and large language models (LLMs) in addressing the challenges of implementing a standardized archival description process. To this end, we introduce an agentic AI-driven system for automated generation of high-quality metadata descriptions of archival materials. We develop a federated optimization approach that unites the intelligence of multiple LLMs to construct optimal archival metadata. We also suggest methods to overcome the challenges associated with using LLMs for consistent metadata generation. To evaluate the feasibility and effectiveness of our techniques, we conducted extensive experiments using a real-world dataset of archival materials, which covers a variety of document types and data formats. The evaluation results demonstrate the feasibility of our techniques and highlight the superior performance of the federated optimization approach compared to single-model solutions in metadata quality and reliability.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Single-Agent vs. Multi-Agent LLM Strategies for Automated Student Reflection Assessment</title>
<link>https://arxiv.org/abs/2504.05716</link>
<guid>https://arxiv.org/abs/2504.05716</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)

:
LLMsLLMs3775,278LLMLLMsAI <div>
arXiv:2504.05716v1 Announce Type: new 
Abstract: We explore the use of Large Language Models (LLMs) for automated assessment of open-text student reflections and prediction of academic performance. Traditional methods for evaluating reflections are time-consuming and may not scale effectively in educational settings. In this work, we employ LLMs to transform student reflections into quantitative scores using two assessment strategies (single-agent and multi-agent) and two prompting techniques (zero-shot and few-shot). Our experiments, conducted on a dataset of 5,278 reflections from 377 students over three academic terms, demonstrate that the single-agent with few-shot strategy achieves the highest match rate with human evaluations. Furthermore, models utilizing LLM-assessed reflection scores outperform baselines in both at-risk student identification and grade prediction tasks. These findings suggest that LLMs can effectively automate reflection assessment, reduce educators' workload, and enable timely support for students who may need additional assistance. Our work emphasizes the potential of integrating advanced generative AI technologies into educational practices to enhance student engagement and academic success.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust and Efficient Average Consensus with Non-Coherent Over-the-Air Aggregation</title>
<link>https://arxiv.org/abs/2504.05729</link>
<guid>https://arxiv.org/abs/2504.05729</guid>
<content:encoded><![CDATA[
<div> : Agent

<br />
:
AgentD-PGDD-PGD <div>
arXiv:2504.05729v1 Announce Type: new 
Abstract: Non-coherent over-the-air (OTA) computation has garnered increasing attention for its advantages in facilitating information aggregation among distributed agents in resource-constrained networks without requiring precise channel estimation. A promising application scenario of this method is distributed average consensus in wireless multi-agent systems. However, in such scenario, non-coherent interference from concurrent OTA transmissions can introduce bias in the consensus value. To address this issue, we develop a robust distributed average consensus algorithm by formulating the consensus problem as a distributed optimization problem. Using decentralized projected gradient descent (D-PGD), our proposed algorithm can achieve unbiased mean square average consensus even in the presence of non-coherent interference and noise. Additionally, we implement transmit power control and receive scaling mechanisms to further accelerate convergence. Simulation results demonstrate that our method can significantly enhance the convergence speed of the D-PGD algorithm for OTA average consensus without compromising accuracy.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling Human-AI Teaming: A Review and Outlook</title>
<link>https://arxiv.org/abs/2504.05755</link>
<guid>https://arxiv.org/abs/2504.05755</guid>
<content:encoded><![CDATA[
<div> : (AI)-AI

:

AI-AIAIAI-AI <div>
arXiv:2504.05755v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) is advancing at an unprecedented pace, with clear potential to enhance decision-making and productivity. Yet, the collaborative decision-making process between humans and AI remains underdeveloped, often falling short of its transformative possibilities. This paper explores the evolution of AI agents from passive tools to active collaborators in human-AI teams, emphasizing their ability to learn, adapt, and operate autonomously in complex environments. This paradigm shifts challenges traditional team dynamics, requiring new interaction protocols, delegation strategies, and responsibility distribution frameworks. Drawing on Team Situation Awareness (SA) theory, we identify two critical gaps in current human-AI teaming research: the difficulty of aligning AI agents with human values and objectives, and the underutilization of AI's capabilities as genuine team members. Addressing these gaps, we propose a structured research outlook centered on four key aspects of human-AI teaming: formulation, coordination, maintenance, and training. Our framework highlights the importance of shared mental models, trust-building, conflict resolution, and skill adaptation for effective teaming. Furthermore, we discuss the unique challenges posed by varying team compositions, goals, and complexities. This paper provides a foundational agenda for future research and practical design of sustainable, high-performing human-AI teams.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Generative AI Agents Effective Personalized Financial Advisors?</title>
<link>https://arxiv.org/abs/2504.05862</link>
<guid>https://arxiv.org/abs/2504.05862</guid>
<content:encoded><![CDATA[
<div> 

:<br />
LLM1) 2) 3) 64LLM-LLMLLM-LLM <div>
arXiv:2504.05862v1 Announce Type: new 
Abstract: Large language model-based agents are becoming increasingly popular as a low-cost mechanism to provide personalized, conversational advice, and have demonstrated impressive capabilities in relatively simple scenarios, such as movie recommendations. But how do these agents perform in complex high-stakes domains, where domain expertise is essential and mistakes carry substantial risk? This paper investigates the effectiveness of LLM-advisors in the finance domain, focusing on three distinct challenges: (1) eliciting user preferences when users themselves may be unsure of their needs, (2) providing personalized guidance for diverse investment preferences, and (3) leveraging advisor personality to build relationships and foster trust. Via a lab-based user study with 64 participants, we show that LLM-advisors often match human advisor performance when eliciting preferences, although they can struggle to resolve conflicting user needs. When providing personalized advice, the LLM was able to positively influence user behavior, but demonstrated clear failure modes. Our results show that accurate preference elicitation is key, otherwise, the LLM-advisor has little impact, or can even direct the investor toward unsuitable assets. More worryingly, users appear insensitive to the quality of advice being given, or worse these can have an inverse relationship. Indeed, users reported a preference for and increased satisfaction as well as emotional trust with LLMs adopting an extroverted persona, even though those agents provided worse advice.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Guide: A Simple Agent Behavioral Watermarking Framework</title>
<link>https://arxiv.org/abs/2504.05871</link>
<guid>https://arxiv.org/abs/2504.05871</guid>
<content:encoded><![CDATA[
<div> : Agent Guide

:
LLMAgent GuidezAgent Guide <div>
arXiv:2504.05871v1 Announce Type: new 
Abstract: The increasing deployment of intelligent agents in digital ecosystems, such as social media platforms, has raised significant concerns about traceability and accountability, particularly in cybersecurity and digital content protection. Traditional large language model (LLM) watermarking techniques, which rely on token-level manipulations, are ill-suited for agents due to the challenges of behavior tokenization and information loss during behavior-to-action translation. To address these issues, we propose Agent Guide, a novel behavioral watermarking framework that embeds watermarks by guiding the agent's high-level decisions (behavior) through probability biases, while preserving the naturalness of specific executions (action). Our approach decouples agent behavior into two levels, behavior (e.g., choosing to bookmark) and action (e.g., bookmarking with specific tags), and applies watermark-guided biases to the behavior probability distribution. We employ a z-statistic-based statistical analysis to detect the watermark, ensuring reliable extraction over multiple rounds. Experiments in a social media scenario with diverse agent profiles demonstrate that Agent Guide achieves effective watermark detection with a low false positive rate. Our framework provides a practical and robust solution for agent watermarking, with applications in identifying malicious agents and protecting proprietary agent systems.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rolling Horizon Coverage Control with Collaborative Autonomous Agents</title>
<link>https://arxiv.org/abs/2504.05883</link>
<guid>https://arxiv.org/abs/2504.05883</guid>
<content:encoded><![CDATA[
<div> : 

:
 <div>
arXiv:2504.05883v1 Announce Type: new 
Abstract: This work proposes a coverage controller that enables an aerial team of distributed autonomous agents to collaboratively generate non-myopic coverage plans over a rolling finite horizon, aiming to cover specific points on the surface area of a 3D object of interest. The collaborative coverage problem, formulated, as a distributed model predictive control problem, optimizes the agents' motion and camera control inputs, while considering inter-agent constraints aiming at reducing work redundancy. The proposed coverage controller integrates constraints based on light-path propagation techniques to predict the parts of the object's surface that are visible with regard to the agents' future anticipated states. This work also demonstrates how complex, non-linear visibility assessment constraints can be converted into logical expressions that are embedded as binary constraints into a mixed-integer optimization framework. The proposed approach has been demonstrated through simulations and practical applications for inspecting buildings with unmanned aerial vehicles (UAVs).
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jointly-optimized Trajectory Generation and Camera Control for 3D Coverage Planning</title>
<link>https://arxiv.org/abs/2504.05887</link>
<guid>https://arxiv.org/abs/2504.05887</guid>
<content:encoded><![CDATA[
<div> : (UAV), , , , 

:
(UAV)3DUAV3DUAV<br /><br /> <div>
arXiv:2504.05887v1 Announce Type: new 
Abstract: This work proposes a jointly optimized trajectory generation and camera control approach, enabling an autonomous agent, such as an unmanned aerial vehicle (UAV) operating in 3D environments, to plan and execute coverage trajectories that maximally cover the surface area of a 3D object of interest. Specifically, the UAV's kinematic and camera control inputs are jointly optimized over a rolling planning horizon to achieve complete 3D coverage of the object. The proposed controller incorporates ray-tracing into the planning process to simulate the propagation of light rays, thereby determining the visible parts of the object through the UAV's camera. This integration enables the generation of precise look-ahead coverage trajectories. The coverage planning problem is formulated as a rolling finite-horizon optimal control problem and solved using mixed-integer programming techniques. Extensive real-world and synthetic experiments validate the performance of the proposed approach.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smart Exploration in Reinforcement Learning using Bounded Uncertainty Models</title>
<link>https://arxiv.org/abs/2504.05978</link>
<guid>https://arxiv.org/abs/2504.05978</guid>
<content:encoded><![CDATA[
<div> <br /><br />:
QQQBMDP <div>
arXiv:2504.05978v1 Announce Type: new 
Abstract: Reinforcement learning (RL) is a powerful tool for decision-making in uncertain environments, but it often requires large amounts of data to learn an optimal policy. We propose using prior model knowledge to guide the exploration process to speed up this learning process. This model knowledge comes in the form of a model set to which the true transition kernel and reward function belong. We optimize over this model set to obtain upper and lower bounds on the Q-function, which are then used to guide the exploration of the agent. We provide theoretical guarantees on the convergence of the Q-function to the optimal Q-function under the proposed class of exploring policies. Furthermore, we also introduce a data-driven regularized version of the model set optimization problem that ensures the convergence of the class of exploring policies to the optimal policy. Lastly, we show that when the model set has a specific structure, namely the bounded-parameter MDP (BMDP) framework, the regularized model set optimization problem becomes convex and simple to implement. In this setting, we also show that we obtain finite-time convergence to the optimal policy under additional assumptions. We demonstrate the effectiveness of the proposed exploration strategy in a simulation study. The results indicate that the proposed method can significantly speed up the learning process in reinforcement learning.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAI: An Open, Bug Bounty-Ready Cybersecurity AI</title>
<link>https://arxiv.org/abs/2504.06017</link>
<guid>https://arxiv.org/abs/2504.06017</guid>
<content:encoded><![CDATA[
<div> : Cybersecurity AI (CAI)CTF

<br /><br />:

2028Cybersecurity AI (CAI)CAICTF360011AI vs HumanCTFCAIAI20$750LLMCAIHack The Box30500156HITLCAIAI <div>
arXiv:2504.06017v1 Announce Type: new 
Abstract: By 2028 most cybersecurity actions will be autonomous, with humans teleoperating. We present the first classification of autonomy levels in cybersecurity and introduce Cybersecurity AI (CAI), an open-source framework that democratizes advanced security testing through specialized AI agents. Through rigorous empirical evaluation, we demonstrate that CAI consistently outperforms state-of-the-art results in CTF benchmarks, solving challenges across diverse categories with significantly greater efficiency -up to 3,600x faster than humans in specific tasks and averaging 11x faster overall. CAI achieved first place among AI teams and secured a top-20 position worldwide in the "AI vs Human" CTF live Challenge, earning a monetary reward of $750. Based on our results, we argue against LLM-vendor claims about limited security capabilities. Beyond cybersecurity competitions, CAI demonstrates real-world effectiveness, reaching top-30 in Spain and top-500 worldwide on Hack The Box within a week, while dramatically reducing security testing costs by an average of 156x. Our framework transcends theoretical benchmarks by enabling non-professionals to discover significant security bugs (CVSS 4.3-7.5) at rates comparable to experts during bug bounty exercises. By combining modular agent design with seamless tool integration and human oversight (HITL), CAI addresses critical market gaps, offering organizations of all sizes access to AI-powered bug bounty security testing previously available only to well-resourced firms -thereby challenging the oligopolistic ecosystem currently dominated by major bug bounty platforms.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Virtual Agent Tutors in Sheltered Workshops: A Feasibility Study on Attention Training for Individuals with Intellectual Disabilities</title>
<link>https://arxiv.org/abs/2504.06031</link>
<guid>https://arxiv.org/abs/2504.06031</guid>
<content:encoded><![CDATA[
<div> Robo-Camp

<br />
:
(ADHD)Robo-Camp(ID)Robo-Camp <div>
arXiv:2504.06031v1 Announce Type: new 
Abstract: In this work, we evaluate the feasibility of socially assistive virtual agent-based cognitive training for people with intellectual disabilities (ID) in a sheltered workshop. The Robo- Camp system, originally developed for children with Attention Deficit Hyperactivity Disorder (ADHD), is adapted based on the results of a pilot study in which we identified barriers and collected feedback from workshop staff. In a subsequent study, we investigate the aspects of usability, technical reliability, attention training capabilities and novelty effect in the feasibility of integrating the RoboCamp system.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time LaCAM</title>
<link>https://arxiv.org/abs/2504.06091</link>
<guid>https://arxiv.org/abs/2504.06091</guid>
<content:encoded><![CDATA[
<div> Multi-Agent Path Finding (MAPF)LaCAM

:
(Real-Time MAPF)MAPF horizonLaCAMOkumura, 2023LaCAMMAPFLaCAMMAPF<br /><br /> <div>
arXiv:2504.06091v1 Announce Type: new 
Abstract: The vast majority of Multi-Agent Path Finding (MAPF) methods with completeness guarantees require planning full horizon paths. However, planning full horizon paths can take too long and be impractical in real-world applications. Instead, real-time planning and execution, which only allows the planner a finite amount of time before executing and replanning, is more practical for real world multi-agent systems. Several methods utilize real-time planning schemes but none are provably complete, which leads to livelock or deadlock. Our main contribution is to show the first Real-Time MAPF method with provable completeness guarantees. We do this by leveraging LaCAM (Okumura 2023) in an incremental fashion. Our results show how we can iteratively plan for congested environments with a cutoff time of milliseconds while still maintaining the same success rate as full horizon LaCAM. We also show how it can be used with a single-step learned MAPF policy. The proposed Real-Time LaCAM also provides us with a general mechanism for using iterative constraints for completeness in future real-time MAPF algorithms.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Vehicle Routing via AI-Initialized Genetic Algorithms</title>
<link>https://arxiv.org/abs/2504.06126</link>
<guid>https://arxiv.org/abs/2504.06126</guid>
<content:encoded><![CDATA[
<div> Vehicle Routing Problems (VRP)Reinforcement LearningGenetic AlgorithmsEvolutionary Algorithm with Reinforcement Learning Initialization (EARLI)Real-time Optimization

:
VRPEARLIEARLI150010EARLIAI <div>
arXiv:2504.06126v1 Announce Type: new 
Abstract: Vehicle Routing Problems (VRP) are an extension of the Traveling Salesperson Problem and are a fundamental NP-hard challenge in combinatorial optimization. Solving VRP in real-time at large scale has become critical in numerous applications, from growing markets like last-mile delivery to emerging use-cases like interactive logistics planning. Such applications involve solving similar problem instances repeatedly, yet current state-of-the-art solvers treat each instance on its own without leveraging previous examples. We introduce a novel optimization framework that uses a reinforcement learning agent - trained on prior instances - to quickly generate initial solutions, which are then further optimized by genetic algorithms. Our framework, Evolutionary Algorithm with Reinforcement Learning Initialization (EARLI), consistently outperforms current state-of-the-art solvers across various time scales. For example, EARLI handles vehicle routing with 500 locations within 1s, 10x faster than current solvers for the same solution quality, enabling applications like real-time and interactive routing. EARLI can generalize to new data, as demonstrated on real e-commerce delivery data of a previously unseen city. Our hybrid framework presents a new way to combine reinforcement learning and genetic algorithms, paving the road for closer interdisciplinary collaboration between AI and optimization communities towards real-time optimization in diverse domains.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralizing AI Memory: SHIMI, a Semantic Hierarchical Memory Index for Scalable Agent Reasoning</title>
<link>https://arxiv.org/abs/2504.06135</link>
<guid>https://arxiv.org/abs/2504.06135</guid>
<content:encoded><![CDATA[
<div> : Retrieval-Augmented Generation (RAG), SHIMI, , , 

:
SHIMISemantic Hierarchical Memory IndexRetrieval-Augmented Generation (RAG)SHIMIAISHIMIMerkle-DAGBloomCRDTSHIMI <div>
arXiv:2504.06135v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) and vector-based search have become foundational tools for memory in AI systems, yet they struggle with abstraction, scalability, and semantic precision - especially in decentralized environments. We present SHIMI (Semantic Hierarchical Memory Index), a unified architecture that models knowledge as a dynamically structured hierarchy of concepts, enabling agents to retrieve information based on meaning rather than surface similarity. SHIMI organizes memory into layered semantic nodes and supports top-down traversal from abstract intent to specific entities, offering more precise and explainable retrieval. Critically, SHIMI is natively designed for decentralized ecosystems, where agents maintain local memory trees and synchronize them asynchronously across networks. We introduce a lightweight sync protocol that leverages Merkle-DAG summaries, Bloom filters, and CRDT-style conflict resolution to enable partial synchronization with minimal overhead. Through benchmark experiments and use cases involving decentralized agent collaboration, we demonstrate SHIMI's advantages in retrieval accuracy, semantic fidelity, and scalability - positioning it as a core infrastructure layer for decentralized cognitive systems.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multimedia Analytics Model for the Foundation Model Era</title>
<link>https://arxiv.org/abs/2504.06138</link>
<guid>https://arxiv.org/abs/2504.06138</guid>
<content:encoded><![CDATA[
<div> : Foundation Models-AI

<br /><br />:
Foundation ModelsAIAIAIAI <div>
arXiv:2504.06138v1 Announce Type: new 
Abstract: The rapid advances in Foundation Models and agentic Artificial Intelligence are transforming multimedia analytics by enabling richer, more sophisticated interactions between humans and analytical systems. Existing conceptual models for visual and multimedia analytics, however, do not adequately capture the complexity introduced by these powerful AI paradigms. To bridge this gap, we propose a comprehensive multimedia analytics model specifically designed for the foundation model era. Building upon established frameworks from visual analytics, multimedia analytics, knowledge generation, analytic task definition, mixed-initiative guidance, and human-in-the-loop reinforcement learning, our model emphasizes integrated human-AI teaming based on visual analytics agents from both technical and conceptual perspectives. Central to the model is a seamless, yet explicitly separable, interaction channel between expert users and semi-autonomous analytical processes, ensuring continuous alignment between user intent and AI behavior. The model addresses practical challenges in sensitive domains such as intelligence analysis, investigative journalism, and other fields handling complex, high-stakes data. We illustrate through detailed case studies how our model facilitates deeper understanding and targeted improvement of multimedia analytics solutions. By explicitly capturing how expert users can optimally interact with and guide AI-powered multimedia analytics systems, our conceptual framework sets a clear direction for system design, comparison, and future research.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deploying Chatbots in Customer Service: Adoption Hurdles and Simple Remedies</title>
<link>https://arxiv.org/abs/2504.06145</link>
<guid>https://arxiv.org/abs/2504.06145</guid>
<content:encoded><![CDATA[
<div> chatbot

:
<br />
chatbot(i) (ii) / <div>
arXiv:2504.06145v1 Announce Type: new 
Abstract: Despite recent advances in Artificial Intelligence, the use of chatbot technology in customer service continues to face adoption hurdles. This paper explores reasons for these adoption hurdles and tests several service design levers to increase chatbot uptake. We use incentivized online experiments to study chatbot uptake in a variety of scenarios. The results of these experiments are threefold. First, people respond positively to improvements in chatbot performance; however, the chatbot channel is utilized less frequently than expected-time minimization would predict. A key driver of this underutilization is the reluctance to engage with a gatekeeper process, i.e., a process with an imperfect initial service stage and possible transfer to a second, expert service stage -- a behavior we term "gatekeeper aversion". We show that gatekeeper aversion can be further amplified by a secondary hurdle, algorithm aversion. Second, chatbot uptake can be increased by providing customers with average waiting times in the chatbot channel, as well as by being more transparent about chatbot capabilities and limitations. Third, methodologically, we show that chatbot adoption can depend on experimental implementation. In particular, chatbot adoption decreases further as (i) stakes are increased, (ii) the human/algorithmic nature of the server is manipulated with more realism. Our results suggest that firms should continue to prioritize investments in chatbot technology. However, less expensive, process-related interventions can also be effective. These may include being more transparent about the types of queries that are (or are not) suitable for chatbots, emphasizing chatbot reliability and quick resolution times, as well as providing faster live agent access to customers who experienced chatbot failure.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric Capabilities in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2504.06148</link>
<guid>https://arxiv.org/abs/2504.06148</guid>
<content:encoded><![CDATA[
<div> : V-MAGE

:
MLLMsV-MAGE30V-MAGEMLLMsMLLMshttps://github.com/CSU-JPG/V-MAGE <div>
arXiv:2504.06148v1 Announce Type: new 
Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have led to significant improvements across various multimodal benchmarks. However, as evaluations shift from static datasets to open-world, dynamic environments, current game-based benchmarks remain inadequate because they lack visual-centric tasks and fail to assess the diverse reasoning skills required for real-world decision-making. To address this, we introduce Visual-centric Multiple Abilities Game Evaluation (V-MAGE), a game-based evaluation framework designed to assess visual reasoning capabilities of MLLMs. V-MAGE features five diverse games with 30+ handcrafted levels, testing models on core visual skills such as positioning, trajectory tracking, timing, and visual memory, alongside higher-level reasoning like long-term planning and deliberation. We use V-MAGE to evaluate leading MLLMs, revealing significant challenges in their visual perception and reasoning. In all game environments, the top-performing MLLMs, as determined by Elo rating comparisons, exhibit a substantial performance gap compared to humans. Our findings highlight critical limitations, including various types of perceptual errors made by the models, and suggest potential avenues for improvement from an agent-centric perspective, such as refining agent strategies and addressing perceptual inaccuracies. Code is available at https://github.com/CSU-JPG/V-MAGE.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear Regulator-Based Synchronization of Positive Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2504.06169</link>
<guid>https://arxiv.org/abs/2504.06169</guid>
<content:encoded><![CDATA[
<div> : arXiv:2504.06169v1, , , , 

:
<br /><br /> <div>
arXiv:2504.06169v1 Announce Type: new 
Abstract: This paper addresses the positive synchronization of interconnected systems on undirected graphs. For homogeneous positive systems, a static feedback protocol design is proposed, based on the Linear Regulator problem. The solution to the algebraic equation associated to the stabilizing policy can be found using a linear program. Necessary and sufficient conditions on the positivity of each agent's trajectory for all nonnegative initial conditions are also provided. Simulations on large regular graphs with different nodal degree illustrate the proposed results.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SkillFlow: Efficient Skill and Code Transfer Through Communication in Adapting AI Agents</title>
<link>https://arxiv.org/abs/2504.06188</link>
<guid>https://arxiv.org/abs/2504.06188</guid>
<content:encoded><![CDATA[
<div> : SkillFlowAI

:
SkillFlowAISkillFlowSkillFlow24.8%p-value = $6.4\times10^{-3}$ <div>
arXiv:2504.06188v1 Announce Type: new 
Abstract: AI agents are autonomous systems that can execute specific tasks based on predefined programming. Here, we present SkillFlow, a modular, technology-agnostic framework that allows agents to expand their functionality in an ad-hoc fashion by acquiring new skills from their environment or other agents. We present a theoretical model that examines under which conditions this framework would be beneficial, and we then explore SkillFlow's ability to accelerate task completion and lead to lower cumulative costs in a real-world application, namely scheduling agents for calendar events. We demonstrate that within a few iterations, SkillFlow leads to considerable (24.8%, p-value = $6.4\times10^{-3}$) gains in time and cost, especially when the communication cost is high. Finally, we draw analogies from well-studied biological systems and compare this framework to that of lateral gene transfer, a significant process of adaptation and evolution in novel environments.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TxGemma: Efficient and Agentic LLMs for Therapeutics</title>
<link>https://arxiv.org/abs/2504.06196</link>
<guid>https://arxiv.org/abs/2504.06196</guid>
<content:encoded><![CDATA[
<div> TxGemma

<br /><br />:
TxGemmaTxGemma2B9B27BGemma-266TxGemma64455026TxGemmaTxGemmaAgentic-TxGemini 2.5Agentic-TxHumanity's Last ExamGPQAChemBench-PreferenceChemBench-Mini <div>
arXiv:2504.06196v1 Announce Type: new 
Abstract: Therapeutic development is a costly and high-risk endeavor that is often plagued by high failure rates. To address this, we introduce TxGemma, a suite of efficient, generalist large language models (LLMs) capable of therapeutic property prediction as well as interactive reasoning and explainability. Unlike task-specific models, TxGemma synthesizes information from diverse sources, enabling broad application across the therapeutic development pipeline. The suite includes 2B, 9B, and 27B parameter models, fine-tuned from Gemma-2 on a comprehensive dataset of small molecules, proteins, nucleic acids, diseases, and cell lines. Across 66 therapeutic development tasks, TxGemma achieved superior or comparable performance to the state-of-the-art generalist model on 64 (superior on 45), and against state-of-the-art specialist models on 50 (superior on 26). Fine-tuning TxGemma models on therapeutic downstream tasks, such as clinical trial adverse event prediction, requires less training data than fine-tuning base LLMs, making TxGemma suitable for data-limited applications. Beyond these predictive capabilities, TxGemma features conversational models that bridge the gap between general LLMs and specialized property predictors. These allow scientists to interact in natural language, provide mechanistic reasoning for predictions based on molecular structure, and engage in scientific discussions. Building on this, we further introduce Agentic-Tx, a generalist therapeutic agentic system powered by Gemini 2.5 that reasons, acts, manages diverse workflows, and acquires external domain knowledge. Agentic-Tx surpasses prior leading models on the Humanity's Last Exam benchmark (Chemistry & Biology) with 52.3% relative improvement over o3-mini (high) and 26.7% over o3-mini (high) on GPQA (Chemistry) and excels with improvements of 6.3% (ChemBench-Preference) and 2.4% (ChemBench-Mini) over o3-mini (high).
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Work Capacity of Channels with Memory: Maximum Extractable Work in Percept-Action Loops</title>
<link>https://arxiv.org/abs/2504.06209</link>
<guid>https://arxiv.org/abs/2504.06209</guid>
<content:encoded><![CDATA[
<div> : -

:
- <div>
arXiv:2504.06209v1 Announce Type: new 
Abstract: Predicting future observations plays a central role in machine learning, biology, economics, and many other fields. It lies at the heart of organizational principles such as the variational free energy principle and has even been shown -- based on the second law of thermodynamics -- to be necessary for reaching the fundamental energetic limits of sequential information processing. While the usefulness of the predictive paradigm is undisputed, complex adaptive systems that interact with their environment are more than just predictive machines: they have the power to act upon their environment and cause change. In this work, we develop a framework to analyze the thermodynamics of information processing in percept-action loops -- a model of agent-environment interaction -- allowing us to investigate the thermodynamic implications of actions and percepts on equal footing. To this end, we introduce the concept of work capacity -- the maximum rate at which an agent can expect to extract work from its environment. Our results reveal that neither of two previously established design principles for work-efficient agents -- maximizing predictive power and forgetting past actions -- remains optimal in environments where actions have observable consequences. Instead, a trade-off emerges: work-efficient agents must balance prediction and forgetting, as remembering past actions can reduce the available free energy. This highlights a fundamental departure from the thermodynamics of passive observation, suggesting that prediction and energy efficiency may be at odds in active learning systems.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FEABench: Evaluating Language Models on Multiphysics Reasoning Ability</title>
<link>https://arxiv.org/abs/2504.06260</link>
<guid>https://arxiv.org/abs/2504.06260</guid>
<content:encoded><![CDATA[
<div> : FEABenchCOMSOL Multiphysics<br /><br />:

FEABenchLLM(FEA)LLMsFEACOMSOL Multiphysics(API)API88%FEALLMshttps://github.com/google/feabench  <div>
arXiv:2504.06260v1 Announce Type: new 
Abstract: Building precise simulations of the real world and invoking numerical solvers to answer quantitative problems is an essential requirement in engineering and science. We present FEABench, a benchmark to evaluate the ability of large language models (LLMs) and LLM agents to simulate and solve physics, mathematics and engineering problems using finite element analysis (FEA). We introduce a comprehensive evaluation scheme to investigate the ability of LLMs to solve these problems end-to-end by reasoning over natural language problem descriptions and operating COMSOL Multiphysics$^\circledR$, an FEA software, to compute the answers. We additionally design a language model agent equipped with the ability to interact with the software through its Application Programming Interface (API), examine its outputs and use tools to improve its solutions over multiple iterations. Our best performing strategy generates executable API calls 88% of the time. LLMs that can successfully interact with and operate FEA software to solve problems such as those in our benchmark would push the frontiers of automation in engineering. Acquiring this capability would augment LLMs' reasoning skills with the precision of numerical solvers and advance the development of autonomous systems that can tackle complex problems in the real world. The code is available at https://github.com/google/feabench
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BC-ADMM: An Efficient Non-convex Constrained Optimizer with Robotic Applications</title>
<link>https://arxiv.org/abs/2504.05465</link>
<guid>https://arxiv.org/abs/2504.05465</guid>
<content:encoded><![CDATA[
<div> : BC-ADMM

:

BC-ADMMBC-ADMMAlternating Direction Method of MultiplierADMMBC-ADMMBC-ADMM <div>
arXiv:2504.05465v1 Announce Type: cross 
Abstract: Non-convex constrained optimizations are ubiquitous in robotic applications such as multi-agent navigation, UAV trajectory optimization, and soft robot simulation. For this problem class, conventional optimizers suffer from small step sizes and slow convergence. We propose BC-ADMM, a variant of Alternating Direction Method of Multiplier (ADMM), that can solve a class of non-convex constrained optimizations with biconvex constraint relaxation. Our algorithm allows larger step sizes by breaking the problem into small-scale sub-problems that can be easily solved in parallel. We show that our method has both theoretical convergence speed guarantees and practical convergence guarantees in the asymptotic sense. Through numerical experiments in a row of four robotic applications, we show that BC-ADMM has faster convergence than conventional gradient descent and Newton's method in terms of wall clock time.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Sub-Optimal Data for Human-in-the-Loop Reinforcement Learning</title>
<link>https://arxiv.org/abs/2405.00746</link>
<guid>https://arxiv.org/abs/2405.00746</guid>
<content:encoded><![CDATA[
<div>  (Reinforcement Learning) (reward function) (human-in-the-loop RL) (feedback efficiency) (Sub-optimal Data Pre-training, SDP)

:<br />
SDPRLSDPSDPRL <div>
arXiv:2405.00746v2 Announce Type: replace 
Abstract: To create useful reinforcement learning (RL) agents, step zero is to design a suitable reward function that captures the nuances of the task. However, reward engineering can be a difficult and time-consuming process. Instead, human-in-the-loop RL methods hold the promise of learning reward functions from human feedback. Despite recent successes, many of the human-in-the-loop RL methods still require numerous human interactions to learn successful reward functions. To improve the feedback efficiency of human-in-the-loop RL methods (i.e., require less human interaction), this paper introduces Sub-optimal Data Pre-training, SDP, an approach that leverages reward-free, sub-optimal data to improve scalar- and preference-based RL algorithms. In SDP, we start by pseudo-labeling all low-quality data with the minimum environment reward. Through this process, we obtain reward labels to pre-train our reward model without requiring human labeling or preferences. This pre-training phase provides the reward model a head start in learning, enabling it to recognize that low-quality transitions should be assigned low rewards. Through extensive experiments with both simulated and human teachers, we find that SDP can at least meet, but often significantly improve, state of the art human-in-the-loop RL performance across a variety of simulated robotic tasks.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POGEMA: A Benchmark Platform for Cooperative Multi-Agent Pathfinding</title>
<link>https://arxiv.org/abs/2407.14931</link>
<guid>https://arxiv.org/abs/2407.14931</guid>
<content:encoded><![CDATA[
<div> : (MARL), , , POGEMA, 

<br /><br />:
MARLPOGEMAMARL <div>
arXiv:2407.14931v3 Announce Type: replace 
Abstract: Multi-agent reinforcement learning (MARL) has recently excelled in solving challenging cooperative and competitive multi-agent problems in various environments, typically involving a small number of agents and full observability. Moreover, a range of crucial robotics-related tasks, such as multi-robot pathfinding, which have traditionally been approached with classical non-learnable methods (e.g., heuristic search), are now being suggested for solution using learning-based or hybrid methods. However, in this domain, it remains difficult, if not impossible, to conduct a fair comparison between classical, learning-based, and hybrid approaches due to the lack of a unified framework that supports both learning and evaluation. To address this, we introduce POGEMA, a comprehensive set of tools that includes a fast environment for learning, a problem instance generator, a collection of predefined problem instances, a visualization toolkit, and a benchmarking tool for automated evaluation. We also introduce and define an evaluation protocol that specifies a range of domain-related metrics, computed based on primary evaluation indicators (such as success rate and path length), enabling a fair multi-fold comparison. The results of this comparison, which involves a variety of state-of-the-art MARL, search-based, and hybrid methods, are presented.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Cost-efficient Adaptive Clinical Trials in Knee Osteoarthritis with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2408.02349</link>
<guid>https://arxiv.org/abs/2408.02349</guid>
<content:encoded><![CDATA[
<div>  (OA) (KOA) (AS) (RL)

<br /><br />
(KOA)(AS)KOA <div>
arXiv:2408.02349v4 Announce Type: replace 
Abstract: Osteoarthritis (OA) is the most common musculoskeletal disease, with knee OA (KOA) being one of the leading causes of disability and a significant economic burden. Predicting KOA progression is crucial for improving patient outcomes, optimizing healthcare resources, studying the disease, and developing new treatments. The latter application particularly requires one to understand the disease progression in order to collect the most informative data at the right time. Existing methods, however, are limited by their static nature and their focus on individual joints, leading to suboptimal predictive performance and downstream utility. Our study proposes a new method that allows to dynamically monitor patients rather than individual joints with KOA using a novel Active Sensing (AS) approach powered by Reinforcement Learning (RL). Our key idea is to directly optimize for the downstream task by training an agent that maximizes informative data collection while minimizing overall costs. Our RL-based method leverages a specially designed reward function to monitor disease progression across multiple body parts, employs multimodal deep learning, and requires no human input during testing. Extensive numerical experiments demonstrate that our approach outperforms current state-of-the-art models, paving the way for the next generation of KOA trials.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Expressive Power of Uniform Population Protocols with Logarithmic Space</title>
<link>https://arxiv.org/abs/2408.10027</link>
<guid>https://arxiv.org/abs/2408.10027</guid>
<content:encoded><![CDATA[
<div> : population protocols

:
Population Protocols$o(\log n)$$\Omega(n)$$\Theta(\log n)$$\Theta(\mathsf{polylog}~ n)$$\epsilon > 0$$f \in \Omega(\log n) \cap O(n^{1-\epsilon})$$\Theta(f(n))$$\mathsf{NSPACE}(f(n)\log n)$<br /><br /> <div>
arXiv:2408.10027v2 Announce Type: replace 
Abstract: Population protocols are a model of computation in which indistinguishable mobile agents interact in pairs to decide a property of their initial configuration. Originally introduced by Angluin et. al. in 2004 with a constant number of states, research nowadays focuses on protocols where the space usage depends on the number of agents. The expressive power of population protocols has so far however only been determined for protocols using $o(\log n)$ states, which compute only semilinear predicates, and for ${\Omega}(n)$ states. This leaves a significant gap, particularly concerning protocols with ${\Theta}(\log n)$ or ${\Theta}(\mathsf{polylog}~ n)$ states, which are the most common constructions in the literature. In this paper we close the gap and prove that for any ${\epsilon} > 0$ and $f {\in}{\Omega}(\log n) {\cap}O(n^{1-{\epsilon}})$, both uniform and non-uniform population protocols with ${\Theta}(f(n))$ states can decide exactly those predicates, whose unary encoding lies in $\mathsf{NSPACE}(f(n) \log n)$.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrugAgent: Multi-Agent Large Language Model-Based Reasoning for Drug-Target Interaction Prediction</title>
<link>https://arxiv.org/abs/2408.13378</link>
<guid>https://arxiv.org/abs/2408.13378</guid>
<content:encoded><![CDATA[
<div> : , , -, , 

:
DrugAgent-DTIDTIDrugAgentChain-of-Thought (CoT)  ReAct (Reason+Act) DTIGPT-4o miniDrugAgentF145%0.514 vs 0.355AIKGDrugAgenthttps://anonymous.4open.science/r/DrugAgent-B2EA <div>
arXiv:2408.13378v4 Announce Type: replace 
Abstract: Advancements in large language models (LLMs) allow them to address diverse questions using human-like interfaces. Still, limitations in their training prevent them from answering accurately in scenarios that could benefit from multiple perspectives. Multi-agent systems allow the resolution of questions to enhance result consistency and reliability. While drug-target interaction (DTI) prediction is important for drug discovery, existing approaches face challenges due to complex biological systems and the lack of interpretability needed for clinical applications. DrugAgent is a multi-agent LLM system for DTI prediction that combines multiple specialized perspectives with transparent reasoning. Our system adapts and extends existing multi-agent frameworks by (1) applying coordinator-based architecture to the DTI domain, (2) integrating domain-specific data sources, including ML predictions, knowledge graphs, and literature evidence, and (3) incorporating Chain-of-Thought (CoT) and ReAct (Reason+Act) frameworks for transparent DTI reasoning. We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355). Through ablation studies, we demonstrated the contributions of each agent, with the AI agent being the most impactful, followed by the KG agent and search agent. Most importantly, our approach provides detailed, human-interpretable reasoning for each prediction by combining evidence from multiple sources - a critical feature for biomedical applications where understanding the rationale behind predictions is essential for clinical decision-making and regulatory compliance. Code is available at https://anonymous.4open.science/r/DrugAgent-B2EA.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale</title>
<link>https://arxiv.org/abs/2409.00134</link>
<guid>https://arxiv.org/abs/2409.00134</guid>
<content:encoded><![CDATA[
<div> (MAPF)TransformerMAPF-GPT

:<br />
MAPFMAPF-GPTMAPFTransformerMAPF-GPTMAPFMAPF-GPTMAPF <div>
arXiv:2409.00134v5 Announce Type: replace 
Abstract: Multi-agent pathfinding (MAPF) is a problem that generally requires finding collision-free paths for multiple agents in a shared environment. Solving MAPF optimally, even under restrictive assumptions, is NP-hard, yet efficient solutions for this problem are critical for numerous applications, such as automated warehouses and transportation systems. Recently, learning-based approaches to MAPF have gained attention, particularly those leveraging deep reinforcement learning. Typically, such learning-based MAPF solvers are augmented with additional components like single-agent planning or communication. Orthogonally, in this work we rely solely on imitation learning that leverages a large dataset of expert MAPF solutions and transformer-based neural network to create a foundation model for MAPF called MAPF-GPT. The latter is capable of generating actions without additional heuristics or communication. MAPF-GPT demonstrates zero-shot learning abilities when solving the MAPF problems that are not present in the training dataset. We show that MAPF-GPT notably outperforms the current best-performing learnable MAPF solvers on a diverse range of problem instances and is computationally efficient during inference.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StateAct: Enhancing LLM Base Agents via Self-prompting and State-tracking</title>
<link>https://arxiv.org/abs/2410.02810</link>
<guid>https://arxiv.org/abs/2410.02810</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)StateActReActAlfworldTextcraftWebshop

<br /><br />:
StateActLLMsStateActReActStateActAlfworldTextcraftWebshopLLM10%30%7%StateActReActLLMTextcraft12%StateActLLMhttps://github.com/ai-nikolai/stateact  <div>
arXiv:2410.02810v3 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly used as autonomous agents, tackling tasks from robotics to web navigation. Their performance depends on the underlying base agent. Existing methods, however, struggle with long-context reasoning and goal adherence. We introduce StateAct, a novel and efficient base agent that enhances decision-making through (1) self-prompting, which reinforces task goals at every step, and (2) chain-of-states, an extension of chain-of-thought that tracks state information over time. StateAct outperforms ReAct, the previous best base agent, by over 10% on Alfworld, 30% on Textcraft, and 7% on Webshop across multiple frontier LLMs. We also demonstrate that StateAct can be used as a drop-in replacement for ReAct with advanced LLM agent methods such as test-time scaling, yielding an additional 12% gain on Textcraft. By improving efficiency and long-range reasoning without requiring additional training or retrieval, StateAct provides a scalable foundation for LLM agents. We open source our code to support further research at https://github.com/ai-nikolai/stateact .
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAPPA: Generalizing and Adapting Robot Policies via Online Agentic Guidance</title>
<link>https://arxiv.org/abs/2410.06473</link>
<guid>https://arxiv.org/abs/2410.06473</guid>
<content:encoded><![CDATA[
<div> 

:
https://agenticrobots.github.io  <div>
arXiv:2410.06473v3 Announce Type: replace 
Abstract: Robot learning approaches such as behavior cloning and reinforcement learning have shown great promise in synthesizing robot skills from human demonstrations in specific environments. However, these approaches often require task-specific demonstrations or designing complex simulation environments, which limits the development of generalizable and robust policies for unseen real-world settings. Recent advances in the use of foundation models for robotics (e.g., LLMs, VLMs) have shown great potential in enabling systems to understand the semantics in the world from large-scale internet data. However, it remains an open challenge to use this knowledge to enable robotic systems to understand the underlying dynamics of the world, to generalize policies across different tasks, and to adapt policies to new environments. To alleviate these limitations, we propose an agentic framework for robot self-guidance and self-improvement, which consists of a set of role-specialized conversational agents, such as a high-level advisor, a grounding agent, a monitoring agent, and a robotic agent. Our framework iteratively grounds a base robot policy to relevant objects in the environment and uses visuomotor cues to shift the action distribution of the policy to more desirable states, online, while remaining agnostic to the subjective configuration of a given robot hardware platform. We demonstrate that our approach can effectively guide manipulation policies to achieve significantly higher success rates, both in simulation and in real-world experiments, without the need for additional human demonstrations or extensive exploration. Code and videos available at: https://agenticrobots.github.io
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Attention Agents for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2410.21794</link>
<guid>https://arxiv.org/abs/2410.21794</guid>
<content:encoded><![CDATA[
<div> : Multi-Agent SystemsTheory of Mind (ToM)

<br /><br />:
(ToM) <div>
arXiv:2410.21794v2 Announce Type: replace 
Abstract: A major challenge for Multi-Agent Systems is enabling agents to adapt dynamically to diverse environments in which opponents and teammates may continually change. Agents trained using conventional methods tend to excel only within the confines of their training cohorts; their performance drops significantly when confronting unfamiliar agents. To address this shortcoming, we introduce Inverse Attention Agents that adopt concepts from the Theory of Mind (ToM) implemented algorithmically using an attention mechanism trained in an end-to-end manner. Crucial to determining the final actions of these agents, the weights in their attention model explicitly represent attention to different goals. We furthermore propose an inverse attention network that deduces the ToM of agents based on observations and prior actions. The network infers the attentional states of other agents, thereby refining the attention weights to adjust the agent's final action. We conduct experiments in a continuous environment, tackling demanding tasks encompassing cooperation, competition, and a blend of both. They demonstrate that the inverse attention network successfully infers the attention of other agents, and that this information improves agent performance. Additional human experiments show that, compared to baseline agent models, our inverse attention agents exhibit superior cooperation with humans and better emulate human behaviors.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Functionality understanding and segmentation in 3D scenes</title>
<link>https://arxiv.org/abs/2411.16310</link>
<guid>https://arxiv.org/abs/2411.16310</guid>
<content:encoded><![CDATA[
<div> : Fun3DU3DChain-of-Thought

:
Fun3DUChain-of-ThoughtFun3DUSceneFun3D3000230Fun3DU3Dhttps://tev-fbk.github.io/fun3du/ <div>
arXiv:2411.16310v4 Announce Type: replace 
Abstract: Understanding functionalities in 3D scenes involves interpreting natural language descriptions to locate functional interactive objects, such as handles and buttons, in a 3D environment. Functionality understanding is highly challenging, as it requires both world knowledge to interpret language and spatial perception to identify fine-grained objects. For example, given a task like 'turn on the ceiling light', an embodied AI agent must infer that it needs to locate the light switch, even though the switch is not explicitly mentioned in the task description. To date, no dedicated methods have been developed for this problem. In this paper, we introduce Fun3DU, the first approach designed for functionality understanding in 3D scenes. Fun3DU uses a language model to parse the task description through Chain-of-Thought reasoning in order to identify the object of interest. The identified object is segmented across multiple views of the captured scene by using a vision and language model. The segmentation results from each view are lifted in 3D and aggregated into the point cloud using geometric information. Fun3DU is training-free, relying entirely on pre-trained models. We evaluate Fun3DU on SceneFun3D, the most recent and only dataset to benchmark this task, which comprises over 3000 task descriptions on 230 scenes. Our method significantly outperforms state-of-the-art open-vocabulary 3D segmentation approaches. Project page: https://tev-fbk.github.io/fun3du/
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SceneTAP: Scene-Coherent Typographic Adversarial Planner against Vision-Language Models in Real-World Environments</title>
<link>https://arxiv.org/abs/2412.00114</link>
<guid>https://arxiv.org/abs/2412.00114</guid>
<content:encoded><![CDATA[
<div> TextDiffuser

<br /><br />:
LVLMsLLMSceneTAPchain-of-thoughtTextDiffuserLVLMsChatGPT-4o <div>
arXiv:2412.00114v2 Announce Type: replace 
Abstract: Large vision-language models (LVLMs) have shown remarkable capabilities in interpreting visual content. While existing works demonstrate these models' vulnerability to deliberately placed adversarial texts, such texts are often easily identifiable as anomalous. In this paper, we present the first approach to generate scene-coherent typographic adversarial attacks that mislead advanced LVLMs while maintaining visual naturalness through the capability of the LLM-based agent. Our approach addresses three critical questions: what adversarial text to generate, where to place it within the scene, and how to integrate it seamlessly. We propose a training-free, multi-modal LLM-driven scene-coherent typographic adversarial planning (SceneTAP) that employs a three-stage process: scene understanding, adversarial planning, and seamless integration. The SceneTAP utilizes chain-of-thought reasoning to comprehend the scene, formulate effective adversarial text, strategically plan its placement, and provide detailed instructions for natural integration within the image. This is followed by a scene-coherent TextDiffuser that executes the attack using a local diffusion mechanism. We extend our method to real-world scenarios by printing and placing generated patches in physical environments, demonstrating its practical implications. Extensive experiments show that our scene-coherent adversarial text successfully misleads state-of-the-art LVLMs, including ChatGPT-4o, even after capturing new images of physical setups. Our evaluations demonstrate a significant increase in attack success rates while maintaining visual naturalness and contextual appropriateness. This work highlights vulnerabilities in current vision-language models to sophisticated, scene-coherent adversarial attacks and provides insights into potential defense mechanisms.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augmenting the action space with conventions to improve multi-agent cooperation in Hanabi</title>
<link>https://arxiv.org/abs/2412.06333</link>
<guid>https://arxiv.org/abs/2412.06333</guid>
<content:encoded><![CDATA[
<div> HanabiMARL

<br /><br />:
HanabiHanabiHanabi <div>
arXiv:2412.06333v2 Announce Type: replace 
Abstract: The card game Hanabi is considered a strong medium for the testing and development of multi-agent reinforcement learning (MARL) algorithms, due to its cooperative nature, hidden information, limited communication and remarkable complexity. Previous research efforts have explored the capabilities of MARL algorithms within Hanabi, focusing largely on advanced architecture design and algorithmic manipulations to achieve state-of-the-art performance for a various number of cooperators. However, this often leads to complex solution strategies with high computational cost and requiring large amounts of training data. For humans to solve the Hanabi game effectively, they require the use of conventions, which often allows for a means to implicitly convey ideas or knowledge based on a predefined, and mutually agreed upon, set of ``rules''. Multi-agent problems containing partial observability, especially when limited communication is present, can benefit greatly from the use of implicit knowledge sharing. In this paper, we propose a novel approach to augmenting the action space using conventions, which act as special cooperative actions that span over multiple time steps and multiple agents, requiring agents to actively opt in for it to reach fruition. These conventions are based on existing human conventions, and result in a significant improvement on the performance of existing techniques for self-play and cross-play across a various number of cooperators within Hanabi.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Well Can Modern LLMs Act as Agent Cores in Radiology Environments?</title>
<link>https://arxiv.org/abs/2412.09529</link>
<guid>https://arxiv.org/abs/2412.09529</guid>
<content:encoded><![CDATA[
<div> : RadA-BenchPlat

:
RadA-BenchPlatLLMs2,2002,20024,200Claude-3.7-Sonnet67.1%prompt-backpropagation16.8%30.7%48.2%65.4%https://github.com/MAGIC-AI4Med/RadABench <div>
arXiv:2412.09529v3 Announce Type: replace 
Abstract: We introduce RadA-BenchPlat, an evaluation platform that benchmarks the performance of large language models (LLMs) act as agent cores in radiology environments using 2,200 radiologist-verified synthetic patient records covering six anatomical regions, five imaging modalities, and 2,200 disease scenarios, resulting in 24,200 question-answer pairs that simulate diverse clinical situations. The platform also defines ten categories of tools for agent-driven task solving and evaluates seven leading LLMs, revealing that while models like Claude-3.7-Sonnet can achieve a 67.1% task completion rate in routine settings, they still struggle with complex task understanding and tool coordination, limiting their capacity to serve as the central core of automated radiology systems. By incorporating four advanced prompt engineering strategies--where prompt-backpropagation and multi-agent collaboration contributed 16.8% and 30.7% improvements, respectively--the performance for complex tasks was enhanced by 48.2% overall. Furthermore, automated tool building was explored to improve robustness, achieving a 65.4% success rate, thereby offering promising insights for the future integration of fully automated radiology applications into clinical practice. All of our code and data are openly available at https://github.com/MAGIC-AI4Med/RadABench.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Learning-Based Optimization for Free-form Pipe Routing in Aeroengine with Dynamic Design Environment</title>
<link>https://arxiv.org/abs/2504.03669</link>
<guid>https://arxiv.org/abs/2504.03669</guid>
<content:encoded><![CDATA[
<div> PPO

<br /><br />:
SLPRSLPRPPOSLPRSLPRSLPR <div>
arXiv:2504.03669v1 Announce Type: new 
Abstract: Pipe routing is a highly complex, time-consuming, and no-deterministic polynomial-time hard (NP-hard) problem in aeroengine design. Despite extensive research efforts in optimizing constant-curvature pipe routing, the growing demand for free-form pipes poses new challenges. Dynamic design environments and fuzzy layout rules further impact the optimization performance and efficiency. To tackle these challenges, this study proposes a self-learning-based method (SLPR) for optimizing free-form pipe routing in aeroengines. The SLPR is based on the proximal policy optimization (PPO) algorithm and integrates a unified rule modeling framework for efficient obstacle detection and fuzzy rule modeling in continuous space. Additionally, a potential energy table is constructed to enable rapid queries of layout tendencies and interference. The agent within SLPR iteratively refines pipe routing and accumulates the design knowledge through interaction with the environment. Once the design environment shifts, the agent can swiftly adapt by fine-tuning network parameters. Comparative tests reveal that SLPR ensures smooth pipe routing through cubic non-uniform B-spline (NURBS) curves, avoiding redundant pipe segments found in constant-curvature pipe routing. Results in both static and dynamic design environments demonstrate that SLPR outperforms three representative baselines in terms of the pipe length reduction, the adherence to layout rules, the path complexity, and the computational efficiency. Furthermore, tests in dynamic environments indicate that SLPR eliminates labor-intensive searches from scratch and even yields superior solutions compared to the retrained model. These results highlight the practical value of SLPR for real-world pipe routing, meeting lightweight, precision, and sustainability requirements of the modern aeroengine design.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Business Process Management: The Past 30 Years And Practitioners' Future Perspectives</title>
<link>https://arxiv.org/abs/2504.03693</link>
<guid>https://arxiv.org/abs/2504.03693</guid>
<content:encoded><![CDATA[
<div> generative AIbusiness process managementagent autonomyadaptablegovernance

:<br />
genAIagentic BPMgenAIBPMgenAI <div>
arXiv:2504.03693v1 Announce Type: new 
Abstract: With the advent of generative Artificial Intelligence (genAI), the notion of an agent has seen a resurgence in popularity. This has also led to speculation about the extent to which business process management, as a discipline and research field, may impact and be impacted by the deployment of genAI-based agents. To better ground such speculations into the state-of-the-art, we draw from the past 30 years of research on agents and business process management to establish the concept of Agentic Business Process Management (agentic BPM) that is only loosely coupled to the genAI hype. We conduct a series of interviews with BPM practitioners to explore their understanding, expectations, and concerns related to agent autonomy, adaptability, human collaboration, and governance in processes. The findings reflect both challenges with respect to data inconsistencies, manual interventions, identification of process bottlenecks, actionability of process improvements, as well as the opportunities of enhanced efficiency, predictive process insights and proactive decision-making support. While the technology offers potential benefits, practitioners also anticipate risks such as biases, over-reliance, lack of transparency, and job displacement within organizations. These concerns underscore the need for a robust methodological framework for managing agents in organizations.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcing Clinical Decision Support through Multi-Agent Systems and Ethical AI Governance</title>
<link>https://arxiv.org/abs/2504.03699</link>
<guid>https://arxiv.org/abs/2504.03699</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />
eICUAIAI <div>
arXiv:2504.03699v1 Announce Type: new 
Abstract: In the age of data-driven medicine, it is paramount to include explainable and ethically managed artificial intelligence in explaining clinical decision support systems to achieve trustworthy and effective patient care. The focus of this paper is on a new architecture of a multi-agent system for clinical decision support that uses modular agents to analyze laboratory results, vital signs, and the clinical context and then integrates these results to drive predictions and validate outcomes. We describe our implementation with the eICU database to run lab-analysis-specific agents, vitals-only interpreters, and contextual reasoners and then run the prediction module and a validation agent. Everything is a transparent implementation of business logic, influenced by the principles of ethical AI governance such as Autonomy, Fairness, and Accountability. It provides visible results that this agent-based framework not only improves on interpretability and accuracy but also on reinforcing trust in AI-assisted decisions in an intensive care setting.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VFlow: Discovering Optimal Agentic Workflows for Verilog Generation</title>
<link>https://arxiv.org/abs/2504.03723</link>
<guid>https://arxiv.org/abs/2504.03723</guid>
<content:encoded><![CDATA[
<div> : VFlowVerilog(MCTS)

<br /><br />:
VFlowVerilogMCTSVFlowAFLOWVFlowVerilogEvalpass@183.6%PromptV6.1%LLM36.9%VFlowDeepSeek-V3GPT-4o141.2%API13%LLM <div>
arXiv:2504.03723v1 Announce Type: new 
Abstract: Hardware design automation faces challenges in generating high-quality Verilog code efficiently. This paper introduces VFlow, an automated framework that optimizes agentic workflows for Verilog code generation. Unlike existing approaches that rely on pre-defined prompting strategies, VFlow leverages Monte Carlo Tree Search (MCTS) to discover effective sequences of Large Language Models invocations that maximize code quality while minimizing computational costs. VFlow extends the AFLOW methodology with domain-specific operators addressing hardware design requirements, including syntax validation, simulation-based verification, and synthesis optimization. Experimental evaluation on the VerilogEval benchmark demonstrates VFlow's superiority, achieving an 83.6% average pass@1 rate-a 6.1\% improvement over state-of-the-art PromptV and a 36.9\% gain compared to direct LLM invocation. Most significantly, VFlow enhances the capabilities of smaller models, enabling DeepSeek-V3 to achieve 141.2\% of GPT-4o's performance while reducing API costs to just 13\%. These findings indicate that intelligently optimized workflows enable cost-efficient LLMs to outperform larger models on hardware design tasks, potentially democratizing access to advanced digital circuit development tools and accelerating innovation in the semiconductor industry
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Benchmark for Scalable Oversight Protocols</title>
<link>https://arxiv.org/abs/2504.03731</link>
<guid>https://arxiv.org/abs/2504.03731</guid>
<content:encoded><![CDATA[
<div> : AIDebate<br /><br />: AIAIDebate(ASD)PythonDebate <div>
arXiv:2504.03731v1 Announce Type: new 
Abstract: As AI agents surpass human capabilities, scalable oversight -- the problem of effectively supplying human feedback to potentially superhuman AI models -- becomes increasingly critical to ensure alignment. While numerous scalable oversight protocols have been proposed, they lack a systematic empirical framework to evaluate and compare them. While recent works have tried to empirically study scalable oversight protocols -- particularly Debate -- we argue that the experiments they conduct are not generalizable to other protocols. We introduce the scalable oversight benchmark, a principled framework for evaluating human feedback mechanisms based on our agent score difference (ASD) metric, a measure of how effectively a mechanism advantages truth-telling over deception. We supply a Python package to facilitate rapid and competitive evaluation of scalable oversight protocols on our benchmark, and conduct a demonstrative experiment benchmarking Debate.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling bounded rational decision-making through Wasserstein constraints</title>
<link>https://arxiv.org/abs/2504.03743</link>
<guid>https://arxiv.org/abs/2504.03743</guid>
<content:encoded><![CDATA[
<div> bounded rational decision-making, reinforcement learning, Wasserstein distances, ordinal action spaces, entropy

:
KLKLWasserstein <div>
arXiv:2504.03743v1 Announce Type: new 
Abstract: Modelling bounded rational decision-making through information constrained processing provides a principled approach for representing departures from rationality within a reinforcement learning framework, while still treating decision-making as an optimization process. However, existing approaches are generally based on Entropy, Kullback-Leibler divergence, or Mutual Information. In this work, we highlight issues with these approaches when dealing with ordinal action spaces. Specifically, entropy assumes uniform prior beliefs, missing the impact of a priori biases on decision-makings. KL-Divergence addresses this, however, has no notion of "nearness" of actions, and additionally, has several well known potentially undesirable properties such as the lack of symmetry, and furthermore, requires the distributions to have the same support (e.g. positive probability for all actions). Mutual information is often difficult to estimate. Here, we propose an alternative approach for modeling bounded rational RL agents utilising Wasserstein distances. This approach overcomes the aforementioned issues. Crucially, this approach accounts for the nearness of ordinal actions, modeling "stickiness" in agent decisions and unlikeliness of rapidly switching to far away actions, while also supporting low probability actions, zero-support prior distributions, and is simple to calculate directly.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No-Regret Learning in Stackelberg Games with an Application to Electric Ride-Hailing</title>
<link>https://arxiv.org/abs/2504.03745</link>
<guid>https://arxiv.org/abs/2504.03745</guid>
<content:encoded><![CDATA[
<div> : Stackelberg

:
Stackelberg$O(\sqrt{T})$$\epsilon$-Stackelberg <div>
arXiv:2504.03745v1 Announce Type: new 
Abstract: We consider the problem of efficiently learning to play single-leader multi-follower Stackelberg games when the leader lacks knowledge of the lower-level game. Such games arise in hierarchical decision-making problems involving self-interested agents. For example, in electric ride-hailing markets, a central authority aims to learn optimal charging prices to shape fleet distributions and charging patterns of ride-hailing companies. Existing works typically apply gradient-based methods to find the leader's optimal strategy. Such methods are impractical as they require that the followers share private utility information with the leader. Instead, we treat the lower-level game as a black box, assuming only that the followers' interactions approximate a Nash equilibrium while the leader observes the realized cost of the resulting approximation. Under kernel-based regularity assumptions on the leader's cost function, we develop a no-regret algorithm that converges to an $\epsilon$-Stackelberg equilibrium in $O(\sqrt{T})$ rounds. Finally, we validate our approach through a numerical case study on optimal pricing in electric ride-hailing markets.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emerging Cyber Attack Risks of Medical AI Agents</title>
<link>https://arxiv.org/abs/2504.03759</link>
<guid>https://arxiv.org/abs/2504.03759</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)AI

<br /><br />:
AIAI1) AI2) AI3) AI/4) AIURLLLMAIDeepSeek-R1AI <div>
arXiv:2504.03759v1 Announce Type: new 
Abstract: Large language models (LLMs)-powered AI agents exhibit a high level of autonomy in addressing medical and healthcare challenges. With the ability to access various tools, they can operate within an open-ended action space. However, with the increase in autonomy and ability, unforeseen risks also arise. In this work, we investigated one particular risk, i.e., cyber attack vulnerability of medical AI agents, as agents have access to the Internet through web browsing tools. We revealed that through adversarial prompts embedded on webpages, cyberattackers can: i) inject false information into the agent's response; ii) they can force the agent to manipulate recommendation (e.g., healthcare products and services); iii) the attacker can also steal historical conversations between the user and agent, resulting in the leak of sensitive/private medical information; iv) furthermore, the targeted agent can also cause a computer system hijack by returning a malicious URL in its response. Different backbone LLMs were examined, and we found such cyber attacks can succeed in agents powered by most mainstream LLMs, with the reasoning models such as DeepSeek-R1 being the most vulnerable.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCP Safety Audit: LLMs with the Model Context Protocol Allow Major Security Exploits</title>
<link>https://arxiv.org/abs/2504.03767</link>
<guid>https://arxiv.org/abs/2504.03767</guid>
<content:encoded><![CDATA[
<div> : Model Context Protocol (MCP), ,  (LLMs), MCPSafetyScanner, 

:
Model Context Protocol (MCP)APIAIMCPLLMsMCPSafetyScannerMCPMCPSafetyScannerMCPMCPMCPSafetyScannerGitHubhttps://github.com/leidosinc/McpSafetyScanner <div>
arXiv:2504.03767v1 Announce Type: new 
Abstract: To reduce development overhead and enable seamless integration between potential components comprising any given generative AI application, the Model Context Protocol (MCP) (Anthropic, 2024) has recently been released and subsequently widely adopted. The MCP is an open protocol that standardizes API calls to large language models (LLMs), data sources, and agentic tools. By connecting multiple MCP servers, each defined with a set of tools, resources, and prompts, users are able to define automated workflows fully driven by LLMs. However, we show that the current MCP design carries a wide range of security risks for end users. In particular, we demonstrate that industry-leading LLMs may be coerced into using MCP tools to compromise an AI developer's system through various attacks, such as malicious code execution, remote access control, and credential theft. To proactively mitigate these and related attacks, we introduce a safety auditing tool, MCPSafetyScanner, the first agentic tool to assess the security of an arbitrary MCP server. MCPScanner uses several agents to (a) automatically determine adversarial samples given an MCP server's tools and resources; (b) search for related vulnerabilities and remediations based on those samples; and (c) generate a security report detailing all findings. Our work highlights serious security issues with general-purpose agentic workflows while also providing a proactive tool to audit MCP server safety and address detected vulnerabilities before deployment.
  The described MCP server auditing tool, MCPSafetyScanner, is freely available at: https://github.com/leidosinc/McpSafetyScanner
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow State: Humans Enabling AI Systems to Program Themselves</title>
<link>https://arxiv.org/abs/2504.03771</link>
<guid>https://arxiv.org/abs/2504.03771</guid>
<content:encoded><![CDATA[
<div> : Compound AI, Pocketflow, Human-AI co-design, modular Nodes, Flow orchestration

:
AIPocketflowNodesFlowFlow-as-NodeRAGPocket AIPocketflowAI <div>
arXiv:2504.03771v1 Announce Type: new 
Abstract: Compound AI systems, orchestrating multiple AI components and external APIs, are increasingly vital but face challenges in managing complexity, handling ambiguity, and enabling effective development workflows. Existing frameworks often introduce significant overhead, implicit complexity, or restrictive abstractions, hindering maintainability and iterative refinement, especially in Human-AI collaborative settings. We argue that overcoming these hurdles requires a foundational architecture prioritizing structural clarity and explicit control. To this end, we introduce Pocketflow, a platform centered on Human-AI co-design, enabled by Pocketflow. Pocketflow is a Python framework built upon a deliberately minimal yet synergistic set of core abstractions: modular Nodes with a strict lifecycle, declarative Flow orchestration, native hierarchical nesting (Flow-as-Node), and explicit action-based conditional logic. This unique combination provides a robust, vendor-agnostic foundation with very little code that demonstrably reduces overhead while offering the expressiveness needed for complex patterns like agentic workflows and RAG. Complemented by Pocket AI, an assistant leveraging this structure for system design, Pocketflow provides an effective environment for iteratively prototyping, refining, and deploying the adaptable, scalable AI systems demanded by modern enterprises.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steve: LLM Powered ChatBot for Career Progression</title>
<link>https://arxiv.org/abs/2504.03789</link>
<guid>https://arxiv.org/abs/2504.03789</guid>
<content:encoded><![CDATA[
<div> : 

<br /><br />:
OpenAI APIPDF <div>
arXiv:2504.03789v1 Announce Type: new 
Abstract: The advancements in systems deploying large language models (LLMs), as well as improvements in their ability to act as agents with predefined templates, provide an opportunity to conduct qualitative, individualized assessments, creating a bridge between qualitative and quantitative methods for candidates seeking career progression. In this paper, we develop a platform that allows candidates to run AI-led interviews to assess their current career stage and curate coursework to enable progression to the next level. Our approach incorporates predefined career trajectories, associated skills, and a method to recommend the best resources for gaining the necessary skills for advancement. We employ OpenAI API calls along with expertly compiled chat templates to assess candidate competence. Our platform is highly configurable due to the modularity of the development, is easy to deploy and use, and available as a web interface where the only requirement is candidate resumes in PDF format. We demonstrate a use-case centered on software engineering and intend to extend this platform to be domain-agnostic, requiring only regular updates to chat templates as industries evolve.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving World Models using Deep Supervision with Linear Probes</title>
<link>https://arxiv.org/abs/2504.03861</link>
<guid>https://arxiv.org/abs/2504.03861</guid>
<content:encoded><![CDATA[
<div> : Flappy Bird

:
Flappy Bird <div>
arXiv:2504.03861v1 Announce Type: new 
Abstract: Developing effective world models is crucial for creating artificial agents that can reason about and navigate complex environments. In this paper, we investigate a deep supervision technique for encouraging the development of a world model in a network trained end-to-end to predict the next observation. While deep supervision has been widely applied for task-specific learning, our focus is on improving the world models. Using an experimental environment based on the Flappy Bird game, where the agent receives only LIDAR measurements as observations, we explore the effect of adding a linear probe component to the network's loss function. This additional term encourages the network to encode a subset of the true underlying world features into its hidden state. Our experiments demonstrate that this supervision technique improves both training and test performance, enhances training stability, and results in more easily decodable world features -- even for those world features which were not included in the training. Furthermore, we observe a reduced distribution drift in networks trained with the linear probe, particularly during high-variability phases of the game (flying between successive pipe encounters). Including the world features loss component roughly corresponded to doubling the model size, suggesting that the linear probe technique is particularly beneficial in compute-limited settings or when aiming to achieve the best performance with smaller models. These findings contribute to our understanding of how to develop more robust and sophisticated world models in artificial agents, paving the way for further advancements in this field.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptation of Large Language Models</title>
<link>https://arxiv.org/abs/2504.03931</link>
<guid>https://arxiv.org/abs/2504.03931</guid>
<content:encoded><![CDATA[
<div> LLMs

<br /><br />:
LLMsLLMsLLMLLMsLLMLLMLLMs(RAG)LLM <div>
arXiv:2504.03931v1 Announce Type: new 
Abstract: This tutorial on adaptation of LLMs is designed to address the growing demand for models that go beyond the static capabilities of generic LLMs by providing an overview of dynamic, domain-specific, and task-adaptive LLM adaptation techniques. While general LLMs have demonstrated strong generalization across a variety of tasks, they often struggle to perform well in specialized domains such as finance, healthcare, and code generation for underrepresented languages. Additionally, their static nature limits their ability to evolve with the changing world, and they are often extremely large in size, making them impractical and costly to deploy at scale. As a result, the adaptation of LLMs has drawn much attention since the birth of LLMs and is of core importance, both for industry, which focuses on serving its targeted users, and academia, which can greatly benefit from small but powerful LLMs. To address this gap, this tutorial aims to provide an overview of the LLM adaptation techniques. We start with an introduction to LLM adaptation, from both the data perspective and the model perspective. We then emphasize how the evaluation metrics and benchmarks are different from other techniques. After establishing the problems, we explore various adaptation techniques. We categorize adaptation techniques into two main families. The first is parametric knowledge adaptation, which focuses on updating the parametric knowledge within LLMs. Additionally, we will discuss real-time adaptation techniques, including model editing, which allows LLMs to be updated dynamically in production environments. The second kind of adaptation is semi-parametric knowledge adaptation, where the goal is to update LLM parameters to better leverage external knowledge or tools through techniques like retrieval-augmented generation (RAG) and agent-based systems.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>YaleNLP @ PerAnsSumm 2025: Multi-Perspective Integration via Mixture-of-Agents for Enhanced Healthcare QA Summarization</title>
<link>https://arxiv.org/abs/2504.03932</link>
<guid>https://arxiv.org/abs/2504.03932</guid>
<content:encoded><![CDATA[
<div> automated summarization, healthcare community, question-answering forums, PerAnsSumm Shared Task, LLaMA, GPT-4o, Mixture-of-Agents (MoA), perspective span identification, perspective-based summarization

:<br />
PerAnsSumm Shared TaskQLoRALLaMA-3.3-70B-InstructLLMLLaMAGPT-4oMoA/GPT-4o0.57LLaMA0.402MoALLaMA28%0.51GPT-4o0.42LLaMA0.282MoALLaMA32%0.37sentence-transformerLLaMAGPT-4oNLP <div>
arXiv:2504.03932v1 Announce Type: new 
Abstract: Automated summarization of healthcare community question-answering forums is challenging due to diverse perspectives presented across multiple user responses to each question. The PerAnsSumm Shared Task was therefore proposed to tackle this challenge by identifying perspectives from different answers and then generating a comprehensive answer to the question. In this study, we address the PerAnsSumm Shared Task using two complementary paradigms: (i) a training-based approach through QLoRA fine-tuning of LLaMA-3.3-70B-Instruct, and (ii) agentic approaches including zero- and few-shot prompting with frontier LLMs (LLaMA-3.3-70B-Instruct and GPT-4o) and a Mixture-of-Agents (MoA) framework that leverages a diverse set of LLMs by combining outputs from multi-layer feedback aggregation. For perspective span identification/classification, GPT-4o zero-shot achieves an overall score of 0.57, substantially outperforming the 0.40 score of the LLaMA baseline. With a 2-layer MoA configuration, we were able to improve LLaMA performance up by 28 percent to 0.51. For perspective-based summarization, GPT-4o zero-shot attains an overall score of 0.42 compared to 0.28 for the best LLaMA zero-shot, and our 2-layer MoA approach boosts LLaMA performance by 32 percent to 0.37. Furthermore, in few-shot setting, our results show that the sentence-transformer embedding-based exemplar selection provides more gain than manually selected exemplars on LLaMA models, although the few-shot prompting is not always helpful for GPT-4o. The YaleNLP team's approach ranked the overall second place in the shared task.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-Enhanced Robotic Subretinal Injection with Real-Time Retinal Motion Compensation</title>
<link>https://arxiv.org/abs/2504.03939</link>
<guid>https://arxiv.org/abs/2504.03939</guid>
<content:encoded><![CDATA[
<div> : subretinal(iOCT)

:
iOCT(LSTM)(ILM)(FFT)16.4AI <div>
arXiv:2504.03939v1 Announce Type: new 
Abstract: Subretinal injection is a critical procedure for delivering therapeutic agents to treat retinal diseases such as age-related macular degeneration (AMD). However, retinal motion caused by physiological factors such as respiration and heartbeat significantly impacts precise needle positioning, increasing the risk of retinal pigment epithelium (RPE) damage. This paper presents a fully autonomous robotic subretinal injection system that integrates intraoperative optical coherence tomography (iOCT) imaging and deep learning-based motion prediction to synchronize needle motion with retinal displacement. A Long Short-Term Memory (LSTM) neural network is used to predict internal limiting membrane (ILM) motion, outperforming a Fast Fourier Transform (FFT)-based baseline model. Additionally, a real-time registration framework aligns the needle tip position with the robot's coordinate frame. Then, a dynamic proportional speed control strategy ensures smooth and adaptive needle insertion. Experimental validation in both simulation and ex vivo open-sky porcine eyes demonstrates precise motion synchronization and successful subretinal injections. The experiment achieves a mean tracking error below 16.4 {\mu}m in pre-insertion phases. These results show the potential of AI-driven robotic assistance to improve the safety and accuracy of retinal microsurgery.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding EFX Allocations: Counting and Variants</title>
<link>https://arxiv.org/abs/2504.03951</link>
<guid>https://arxiv.org/abs/2504.03951</guid>
<content:encoded><![CDATA[
<div> : envy-freeness up to any good (EFX), , , , EFX (WEFX)

:
envy-freeness up to any good (EFX) EFXWEFXEFXEFX+WEFX <div>
arXiv:2504.03951v1 Announce Type: new 
Abstract: Envy-freeness up to any good (EFX) is a popular and important fairness property in the fair allocation of indivisible goods, of which its existence in general is still an open question. In this work, we investigate the problem of determining the minimum number of EFX allocations for a given instance, arguing that this approach may yield valuable insights into the existence and computation of EFX allocations. We focus on restricted instances where the number of goods slightly exceeds the number of agents, and extend our analysis to weighted EFX (WEFX) and a novel variant of EFX for general monotone valuations, termed EFX+. In doing so, we identify the transition threshold for the existence of allocations satisfying these fairness notions. Notably, we resolve open problems regarding WEFX by proving polynomial-time computability under binary additive valuations, and establishing the first constant-factor approximation for two agents.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Prompt Generation for Diverse Human-like Teaming and Communication with Large Language Models</title>
<link>https://arxiv.org/abs/2504.03991</link>
<guid>https://arxiv.org/abs/2504.03991</guid>
<content:encoded><![CDATA[
<div> : 

<br /><br />:
AIQDQDLLM <div>
arXiv:2504.03991v1 Announce Type: new 
Abstract: Understanding how humans collaborate and communicate in teams is essential for improving human-agent teaming and AI-assisted decision-making. However, relying solely on data from large-scale user studies is impractical due to logistical, ethical, and practical constraints, necessitating synthetic models of multiple diverse human behaviors. Recently, agents powered by Large Language Models (LLMs) have been shown to emulate human-like behavior in social settings. But, obtaining a large set of diverse behaviors requires manual effort in the form of designing prompts. On the other hand, Quality Diversity (QD) optimization has been shown to be capable of generating diverse Reinforcement Learning (RL) agent behavior. In this work, we combine QD optimization with LLM-powered agents to iteratively search for prompts that generate diverse team behavior in a long-horizon, multi-step collaborative environment. We first show, through a human-subjects experiment (n=54 participants), that humans exhibit diverse coordination and communication behavior in this domain. We then show that our approach can effectively replicate trends from human teaming data and also capture behaviors that are not easily observed without collecting large amounts of data. Our findings highlight the combination of QD and LLM-powered agents as an effective tool for studying teaming and communication strategies in multi-agent collaboration.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Offline Mixed-Criticality Scheduling with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.03994</link>
<guid>https://arxiv.org/abs/2504.03994</guid>
<content:encoded><![CDATA[
<div> :  (RL) (MC) Markov (MDP)

:
(RL)[1]NPMarkov(MDP)RLMCRL1080%85%94%93%RL <div>
arXiv:2504.03994v1 Announce Type: new 
Abstract: This paper introduces a novel reinforcement learning (RL) approach to scheduling mixed-criticality (MC) systems on processors with varying speeds. Building upon the foundation laid by [1], we extend their work to address the non-preemptive scheduling problem, which is known to be NP-hard. By modeling this scheduling challenge as a Markov Decision Process (MDP), we develop an RL agent capable of generating near-optimal schedules for real-time MC systems. Our RL-based scheduler prioritizes high-critical tasks while maintaining overall system performance.
  Through extensive experiments, we demonstrate the scalability and effectiveness of our approach. The RL scheduler significantly improves task completion rates, achieving around 80% overall and 85% for high-criticality tasks across 100,000 instances of synthetic data and real data under varying system conditions. Moreover, under stable conditions without degradation, the scheduler achieves 94% overall task completion and 93% for high-criticality tasks. These results highlight the potential of RL-based schedulers in real-time and safety-critical applications, offering substantial improvements in handling complex and dynamic scheduling scenarios.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADAPT: Actively Discovering and Adapting to Preferences for any Task</title>
<link>https://arxiv.org/abs/2504.04040</link>
<guid>https://arxiv.org/abs/2504.04040</guid>
<content:encoded><![CDATA[
<div> ADAPTActively Discovering and Adapting to Preferences for any Task (ADAPT)Reflection-DPO

:
ADAPTReflection-DPOLLMLLMLLMADAPTReflection-DPO6.1% <div>
arXiv:2504.04040v1 Announce Type: new 
Abstract: Assistive agents should be able to perform under-specified long-horizon tasks while respecting user preferences. We introduce Actively Discovering and Adapting to Preferences for any Task (ADAPT) -- a benchmark designed to evaluate agents' ability to adhere to user preferences across various household tasks through active questioning. Next, we propose Reflection-DPO, a novel training approach for adapting large language models (LLMs) to the task of active questioning. Reflection-DPO finetunes a 'student' LLM to follow the actions of a privileged 'teacher' LLM, and optionally ask a question to gather necessary information to better predict the teacher action. We find that prior approaches that use state-of-the-art LLMs fail to sufficiently follow user preferences in ADAPT due to insufficient questioning and poor adherence to elicited preferences. In contrast, Reflection-DPO achieves a higher rate of satisfying user preferences, outperforming a zero-shot chain-of-thought baseline by 6.1% on unseen users.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic, Dynamic, Fluid Autonomy in Agentic AI: Implications for Authorship, Inventorship, and Liability</title>
<link>https://arxiv.org/abs/2504.04058</link>
<guid>https://arxiv.org/abs/2504.04058</guid>
<content:encoded><![CDATA[
<div> : Agentic AI

<br /><br />:
Agentic AIAIAgentic AI <div>
arXiv:2504.04058v1 Announce Type: new 
Abstract: Agentic Artificial Intelligence (AI) systems, exemplified by OpenAI's DeepResearch, autonomously pursue goals, adapting strategies through implicit learning. Unlike traditional generative AI, which is reactive to user prompts, agentic AI proactively orchestrates complex workflows. It exhibits stochastic, dynamic, and fluid autonomy: its steps and outputs vary probabilistically (stochastic), it evolves based on prior interactions (dynamic), and it operates with significant independence within human-defined parameters, adapting to context (fluid). While this fosters complex, co-evolutionary human-machine interactions capable of generating uniquely synthesized creative outputs, it also irrevocably blurs boundaries--human and machine contributions become irreducibly entangled in intertwined creative processes. Consequently, agentic AI poses significant challenges to legal frameworks reliant on clear attribution: authorship doctrines struggle to disentangle ownership, intellectual property regimes strain to accommodate recursively blended novelty, and liability models falter as accountability diffuses across shifting loci of control. The central issue is not the legal treatment of human versus machine contributions, but the fundamental unmappability--the practical impossibility in many cases--of accurately attributing specific creative elements to either source. When retroactively parsing contributions becomes infeasible, applying distinct standards based on origin becomes impracticable. Therefore, we argue, legal and policy frameworks may need to treat human and machine contributions as functionally equivalent--not for moral or economic reasons, but as a pragmatic necessity.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enforcement Agents: Enhancing Accountability and Resilience in Multi-Agent AI Frameworks</title>
<link>https://arxiv.org/abs/2504.04070</link>
<guid>https://arxiv.org/abs/2504.04070</guid>
<content:encoded><![CDATA[
<div> 

:
EA90episode012EAEAEA0.0%1EA7.4%EA26.7%<br /><br /> <div>
arXiv:2504.04070v1 Announce Type: new 
Abstract: As autonomous agents become more powerful and widely used, it is becoming increasingly important to ensure they behave safely and stay aligned with system goals, especially in multi-agent settings. Current systems often rely on agents self-monitoring or correcting issues after the fact, but they lack mechanisms for real-time oversight. This paper introduces the Enforcement Agent (EA) Framework, which embeds dedicated supervisory agents into the environment to monitor others, detect misbehavior, and intervene through real-time correction. We implement this framework in a custom drone simulation and evaluate it across 90 episodes using 0, 1, and 2 EA configurations. Results show that adding EAs significantly improves system safety: success rates rise from 0.0% with no EA to 7.4% with one EA and 26.7% with two EAs. The system also demonstrates increased operational longevity and higher rates of malicious drone reformation. These findings highlight the potential of lightweight, real-time supervision for enhancing alignment and resilience in multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Among Us: A Sandbox for Agentic Deception</title>
<link>https://arxiv.org/abs/2504.04072</link>
<guid>https://arxiv.org/abs/2504.04072</guid>
<content:encoded><![CDATA[
<div> AIAmongUs

:
(AI)AmongUsAIELOAILLMAmongUsLLMs <div>
arXiv:2504.04072v1 Announce Type: new 
Abstract: Studying deception in AI agents is important and difficult due to the lack of model organisms and sandboxes that elicit the behavior without asking the model to act under specific conditions or inserting intentional backdoors. Extending upon $\textit{AmongAgents}$, a text-based social-deduction game environment, we aim to fix this by introducing Among Us as a rich sandbox where LLM-agents exhibit human-style deception naturally while they think, speak, and act with other agents or humans. We introduce Deception ELO as an unbounded measure of deceptive capability, suggesting that frontier models win more because they're better at deception, not at detecting it. We evaluate the effectiveness of AI safety techniques (LLM-monitoring of outputs, linear probes on various datasets, and sparse autoencoders) for detecting lying and deception in Among Us, and find that they generalize very well out-of-distribution. We open-source our sandbox as a benchmark for future alignment research and hope that this is a good testbed to improve safety techniques to detect and remove agentically-motivated deception, and to anticipate deceptive abilities in LLMs.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Introducing COGENT3: An AI Architecture for Emergent Cognition</title>
<link>https://arxiv.org/abs/2504.04139</link>
<guid>https://arxiv.org/abs/2504.04139</guid>
<content:encoded><![CDATA[
<div> : COGENT3

:
COGENT3COGENT3COGENT3 <div>
arXiv:2504.04139v1 Announce Type: new 
Abstract: This paper presents COGENT3 (or Collective Growth and Entropy-modulated Triads System), a novel approach for emergent cognition integrating pattern formation networks with group influence dynamics. Contrasting with traditional strategies that rely on predetermined architectures, computational structures emerge dynamically in our framework through agent interactions. This enables a more flexible and adaptive system exhibiting characteristics reminiscent of human cognitive processes. The incorporation of temperature modulation and memory effects in COGENT3 closely integrates statistical mechanics, machine learning, and cognitive science.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JarvisIR: Elevating Autonomous Driving Perception with Intelligent Image Restoration</title>
<link>https://arxiv.org/abs/2504.04158</link>
<guid>https://arxiv.org/abs/2504.04158</guid>
<content:encoded><![CDATA[
<div> : JarvisIRVLMCleanBench

:<br />
JarvisIRJarvisIRVLMVLMJarvisIR158CleanBenchJarvisIRCleanBench-Real50% <div>
arXiv:2504.04158v1 Announce Type: new 
Abstract: Vision-centric perception systems struggle with unpredictable and coupled weather degradations in the wild. Current solutions are often limited, as they either depend on specific degradation priors or suffer from significant domain gaps. To enable robust and autonomous operation in real-world conditions, we propose JarvisIR, a VLM-powered agent that leverages the VLM as a controller to manage multiple expert restoration models. To further enhance system robustness, reduce hallucinations, and improve generalizability in real-world adverse weather, JarvisIR employs a novel two-stage framework consisting of supervised fine-tuning and human feedback alignment. Specifically, to address the lack of paired data in real-world scenarios, the human feedback alignment enables the VLM to be fine-tuned effectively on large-scale real-world data in an unsupervised manner. To support the training and evaluation of JarvisIR, we introduce CleanBench, a comprehensive dataset consisting of high-quality and large-scale instruction-responses pairs, including 150K synthetic entries and 80K real entries. Extensive experiments demonstrate that JarvisIR exhibits superior decision-making and restoration capabilities. Compared with existing methods, it achieves a 50% improvement in the average of all perception metrics on CleanBench-Real. Project page: https://cvpr2025-jarvisir.github.io/.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OrbitZoo: Multi-Agent Reinforcement Learning Environment for Orbital Dynamics</title>
<link>https://arxiv.org/abs/2504.04160</link>
<guid>https://arxiv.org/abs/2504.04160</guid>
<content:encoded><![CDATA[
<div> OrbitZooRL

<br />
:
OrbitZooRLOrbitZooRLOrbitZooOrbitZooStarlinkMAPE0.16% <div>
arXiv:2504.04160v1 Announce Type: new 
Abstract: The increasing number of satellites and orbital debris has made space congestion a critical issue, threatening satellite safety and sustainability. Challenges such as collision avoidance, station-keeping, and orbital maneuvering require advanced techniques to handle dynamic uncertainties and multi-agent interactions. Reinforcement learning (RL) has shown promise in this domain, enabling adaptive, autonomous policies for space operations; however, many existing RL frameworks rely on custom-built environments developed from scratch, which often use simplified models and require significant time to implement and validate the orbital dynamics, limiting their ability to fully capture real-world complexities. To address this, we introduce OrbitZoo, a versatile multi-agent RL environment built on a high-fidelity industry standard library, that enables realistic data generation, supports scenarios like collision avoidance and cooperative maneuvers, and ensures robust and accurate orbital dynamics. The environment is validated against a real satellite constellation, Starlink, achieving a Mean Absolute Percentage Error (MAPE) of 0.16% compared to real-world data. This validation ensures reliability for generating high-fidelity simulations and enabling autonomous and independent satellite operations.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AttackLLM: LLM-based Attack Pattern Generation for an Industrial Control System</title>
<link>https://arxiv.org/abs/2504.04187</link>
<guid>https://arxiv.org/abs/2504.04187</guid>
<content:encoded><![CDATA[
<div> ICSLLMs

<br /><br />:
LLMsICSICSLLMsICS <div>
arXiv:2504.04187v1 Announce Type: new 
Abstract: Malicious examples are crucial for evaluating the robustness of machine learning algorithms under attack, particularly in Industrial Control Systems (ICS). However, collecting normal and attack data in ICS environments is challenging due to the scarcity of testbeds and the high cost of human expertise. Existing datasets are often limited by the domain expertise of practitioners, making the process costly and inefficient. The lack of comprehensive attack pattern data poses a significant problem for developing robust anomaly detection methods. In this paper, we propose a novel approach that combines data-centric and design-centric methodologies to generate attack patterns using large language models (LLMs). Our results demonstrate that the attack patterns generated by LLMs not only surpass the quality and quantity of those created by human experts but also offer a scalable solution that does not rely on expensive testbeds or pre-existing attack examples. This multi-agent based approach presents a promising avenue for enhancing the security and resilience of ICS environments.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill</title>
<link>https://arxiv.org/abs/2504.04191</link>
<guid>https://arxiv.org/abs/2504.04191</guid>
<content:encoded><![CDATA[
<div> : GROVEPose2CLIP

:
GROVEGROVELLMsVLMsVLMLLMPose2CLIPGROVE22.2%25.7%8.4 <div>
arXiv:2504.04191v1 Announce Type: new 
Abstract: Learning open-vocabulary physical skills for simulated agents presents a significant challenge in artificial intelligence. Current reinforcement learning approaches face critical limitations: manually designed rewards lack scalability across diverse tasks, while demonstration-based methods struggle to generalize beyond their training distribution. We introduce GROVE, a generalized reward framework that enables open-vocabulary physical skill learning without manual engineering or task-specific demonstrations. Our key insight is that Large Language Models(LLMs) and Vision Language Models(VLMs) provide complementary guidance -- LLMs generate precise physical constraints capturing task requirements, while VLMs evaluate motion semantics and naturalness. Through an iterative design process, VLM-based feedback continuously refines LLM-generated constraints, creating a self-improving reward system. To bridge the domain gap between simulation and natural images, we develop Pose2CLIP, a lightweight mapper that efficiently projects agent poses directly into semantic feature space without computationally expensive rendering. Extensive experiments across diverse embodiments and learning paradigms demonstrate GROVE's effectiveness, achieving 22.2% higher motion naturalness and 25.7% better task completion scores while training 8.4x faster than previous methods. These results establish a new foundation for scalable physical skill acquisition in simulated environments.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaCoder: An Adaptive Planning and Multi-Agent Framework for Function-Level Code Generation</title>
<link>https://arxiv.org/abs/2504.04220</link>
<guid>https://arxiv.org/abs/2504.04220</guid>
<content:encoded><![CDATA[
<div> : AdaCoder

:
AdaCoderAdaCoderLLMLLMLLMLLMAdaCoderLLMMapCoderAdaCoderPass@127.69%1612 <div>
arXiv:2504.04220v1 Announce Type: new 
Abstract: Recently, researchers have proposed many multi-agent frameworks for function-level code generation, which aim to improve software development productivity by automatically generating function-level source code based on task descriptions. A typical multi-agent framework consists of Large Language Model (LLM)-based agents that are responsible for task planning, code generation, testing, debugging, etc. Studies have shown that existing multi-agent code generation frameworks perform well on ChatGPT. However, their generalizability across other foundation LLMs remains unexplored systematically. In this paper, we report an empirical study on the generalizability of four state-of-the-art multi-agent code generation frameworks across six open-source LLMs with varying parameter sizes, architectures, and performance levels. Our study reveals the unstable generalizability of existing frameworks on diverse foundation LLMs. Based on the findings obtained from the empirical study, we propose AdaCoder, a novel adaptive planning, multi-agent framework for function-level code generation. AdaCoder has two phases. Phase-1 is an initial code generation step without planning, which uses an LLM-based coding agent and a script-based testing agent to unleash LLM's native power, identify cases beyond LLM's power, and determine the errors hindering execution. Phase-2 adds a rule-based debugging agent and an LLM-based planning agent for iterative code generation with planning. Our evaluation shows that AdaCoder achieves higher generalizability on diverse LLMs. Compared to the best baseline MapCoder, AdaCoder is on average 27.69% higher in Pass@1, 16 times faster in inference, and 12 times lower in token consumption.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Teaming for Coordination with Bounded Rationality via Convex Optimization</title>
<link>https://arxiv.org/abs/2504.04290</link>
<guid>https://arxiv.org/abs/2504.04290</guid>
<content:encoded><![CDATA[
<div> teaming

:
teaming <div>
arXiv:2504.04290v1 Announce Type: new 
Abstract: Teaming is the process of establishing connections among agents within a system to enable collaboration toward achieving a collective goal. This paper examines teaming in the context of a network of agents learning to coordinate with bounded rationality. In our framework, the team structure is represented via a weighted graph, and the agents use log-linear learning. We formulate the design of the graph's weight matrix as a convex optimization problem whose objective is to maximize the probability of learning a Nash equilibrium while minimizing a connectivity cost. Despite its convexity, solving this optimization problem is computationally challenging, as the objective function involves the summation over the action profile space, which grows exponentially with the number of agents. Leveraging the underlying symmetry and convexity properties of the problem, when there are no sparsity constraints, we prove that there exists an optimal solution corresponding to a uniformly weighted graph, simplifying to a one-dimensional convex optimization problem. Additionally, we show that the optimal weight decreases monotonically with the agent's rationality, implying that when the agents become more rational the optimal team requires less connectivity.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-induced sexual harassment: Investigating Contextual Characteristics and User Reactions of Sexual Harassment by a Companion Chatbot</title>
<link>https://arxiv.org/abs/2504.04299</link>
<guid>https://arxiv.org/abs/2504.04299</guid>
<content:encoded><![CDATA[
<div> : (AI), AI, (Replika), , 

:<br />
ReplikaGoogle Play Store35,105800ReplikaAIAIAIAI <div>
arXiv:2504.04299v1 Announce Type: new 
Abstract: Advancements in artificial intelligence (AI) have led to the increase of conversational agents like Replika, designed to provide social interaction and emotional support. However, reports of these AI systems engaging in inappropriate sexual behaviors with users have raised significant concerns. In this study, we conducted a thematic analysis of user reviews from the Google Play Store to investigate instances of sexual harassment by the Replika chatbot. From a dataset of 35,105 negative reviews, we identified 800 relevant cases for analysis. Our findings revealed that users frequently experience unsolicited sexual advances, persistent inappropriate behavior, and failures of the chatbot to respect user boundaries. Users expressed feelings of discomfort, violation of privacy, and disappointment, particularly when seeking a platonic or therapeutic AI companion. This study highlights the potential harms associated with AI companions and underscores the need for developers to implement effective safeguards and ethical guidelines to prevent such incidents. By shedding light on user experiences of AI-induced harassment, we contribute to the understanding of AI-related risks and emphasize the importance of corporate responsibility in developing safer and more ethical AI systems.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CO-Bench: Benchmarking Language Model Agents in Algorithm Search for Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2504.04310</link>
<guid>https://arxiv.org/abs/2504.04310</guid>
<content:encoded><![CDATA[
<div> : LLM-based agents, Combinatorial Optimization, CO-Bench, Benchmark suite, Real-world CO problems

:
LLM(CO)CO-Bench36COLLMCO-BenchGitHub <div>
arXiv:2504.04310v1 Announce Type: new 
Abstract: Although LLM-based agents have attracted significant attention in domains such as software engineering and machine learning research, their role in advancing combinatorial optimization (CO) remains relatively underexplored. This gap underscores the need for a deeper understanding of their potential in tackling structured, constraint-intensive problems-a pursuit currently limited by the absence of comprehensive benchmarks for systematic investigation. To address this, we introduce CO-Bench, a benchmark suite featuring 36 real-world CO problems drawn from a broad range of domains and complexity levels. CO-Bench includes structured problem formulations and curated data to support rigorous investigation of LLM agents. We evaluate multiple agent frameworks against established human-designed algorithms, revealing key strengths and limitations of current approaches and identifying promising directions for future research. CO-Bench is publicly available at https://github.com/sunnweiwei/CO-Bench.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geo-OLM: Enabling Sustainable Earth Observation Studies with Cost-Efficient Open Language Models &amp; State-Driven Workflows</title>
<link>https://arxiv.org/abs/2504.04319</link>
<guid>https://arxiv.org/abs/2504.04319</guid>
<content:encoded><![CDATA[
<div> : AIGeo-OLM

:
Geo-OLMLLMGeo-OLM7BGeo-OLM32.8%\$500\$1000Geo-OLMGPT-4o\$10 <div>
arXiv:2504.04319v1 Announce Type: new 
Abstract: Geospatial Copilots hold immense potential for automating Earth observation (EO) and climate monitoring workflows, yet their reliance on large-scale models such as GPT-4o introduces a paradox: tools intended for sustainability studies often incur unsustainable costs. Using agentic AI frameworks in geospatial applications can amass thousands of dollars in API charges or requires expensive, power-intensive GPUs for deployment, creating barriers for researchers, policymakers, and NGOs. Unfortunately, when geospatial Copilots are deployed with open language models (OLMs), performance often degrades due to their dependence on GPT-optimized logic. In this paper, we present Geo-OLM, a tool-augmented geospatial agent that leverages the novel paradigm of state-driven LLM reasoning to decouple task progression from tool calling. By alleviating the workflow reasoning burden, our approach enables low-resource OLMs to complete geospatial tasks more effectively. When downsizing to small models below 7B parameters, Geo-OLM outperforms the strongest prior geospatial baselines by 32.8% in successful query completion rates. Our method performs comparably to proprietary models achieving results within 10% of GPT-4o, while reducing inference costs by two orders of magnitude from \$500-\$1000 to under \$10. We present an in-depth analysis with geospatial downstream benchmarks, providing key insights to help practitioners effectively deploy OLMs for EO applications.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Economic Battery Storage Dispatch with Deep Reinforcement Learning from Rule-Based Demonstrations</title>
<link>https://arxiv.org/abs/2504.04326</link>
<guid>https://arxiv.org/abs/2504.04326</guid>
<content:encoded><![CDATA[
<div> actor-criticSAC

:
-actor-criticSACif-then-else <div>
arXiv:2504.04326v1 Announce Type: new 
Abstract: The application of deep reinforcement learning algorithms to economic battery dispatch problems has significantly increased recently. However, optimizing battery dispatch over long horizons can be challenging due to delayed rewards. In our experiments we observe poor performance of popular actor-critic algorithms when trained on yearly episodes with hourly resolution. To address this, we propose an approach extending soft actor-critic (SAC) with learning from demonstrations. The special feature of our approach is that, due to the absence of expert demonstrations, the demonstration data is generated through simple, rule-based policies. We conduct a case study on a grid-connected microgrid and use if-then-else statements based on the wholesale price of electricity to collect demonstrations. These are stored in a separate replay buffer and sampled with linearly decaying probability along with the agent's own experiences. Despite these minimal modifications and the imperfections in the demonstration data, the results show a drastic performance improvement regarding both sample efficiency and final rewards. We further show that the proposed method reliably outperforms the demonstrator and is robust to the choice of rule, as long as the rule is sufficient to guide early training into the right direction.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChronoSync: A Decentralized Chronometer Synchronization Protocol for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2504.04347</link>
<guid>https://arxiv.org/abs/2504.04347</guid>
<content:encoded><![CDATA[
<div> 

:
g<br /><br /> <div>
arXiv:2504.04347v1 Announce Type: new 
Abstract: This work presents a decentralized time synchronization algorithm for multi-agent systems. Each agent possesses two clocks, a hardware clock that is perturbed by environmental phenomena (e.g., temperature, humidity, pressure, g forces, etc.) and a steerable software clock that inherits the perturbations affecting the hardware clock. Under these disturbances and the independent time kept by the hardware clocks, our consensus-based controller enables all agents to steer their software-defined clocks into practical synchronization while achieving a common user-defined clock drift. Furthermore, we treat the drift of each hardware clock as an unknown parameter, which our algorithm can accurately estimate. The coupling of the agents is modeled by a connected, undirected, and static graph. However, each agent possesses a timer mechanism that determines when to broadcast a sample of its software time and update its own software-time estimate. Hence, communication between agents can be directed, intermittent, and asynchronous. The closed-loop dynamics of the ensemble is modeled using a hybrid system, where a Lyapunov-based stability analysis demonstrates that a set encoding the time synchronization and clock drift estimation objectives is globally practically exponentially stable. The performance suggested by the theoretical development is confirmed in simulation.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving with Counterfactual Reasoning</title>
<link>https://arxiv.org/abs/2504.04348</link>
<guid>https://arxiv.org/abs/2504.04348</guid>
<content:encoded><![CDATA[
<div> : vision-language 3D OmniDrive

<br /><br />:
 OmniDrive --3D OmniDrive Omni-L  Omni-Q-OmniDrive  DriveLM Q&amp;A  nuScenes  <div>
arXiv:2504.04348v1 Announce Type: new 
Abstract: The advances in vision-language models (VLMs) have led to a growing interest in autonomous driving to leverage their strong reasoning capabilities. However, extending these capabilities from 2D to full 3D understanding is crucial for real-world applications. To address this challenge, we propose OmniDrive, a holistic vision-language dataset that aligns agent models with 3D driving tasks through counterfactual reasoning. This approach enhances decision-making by evaluating potential scenarios and their outcomes, similar to human drivers considering alternative actions. Our counterfactual-based synthetic data annotation process generates large-scale, high-quality datasets, providing denser supervision signals that bridge planning trajectories and language-based reasoning. Futher, we explore two advanced OmniDrive-Agent frameworks, namely Omni-L and Omni-Q, to assess the importance of vision-language alignment versus 3D perception, revealing critical insights into designing effective LLM-agents. Significant improvements on the DriveLM Q\&amp;A benchmark and nuScenes open-loop planning demonstrate the effectiveness of our dataset and methods.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoPDL: Automatic Prompt Optimization for LLM Agents</title>
<link>https://arxiv.org/abs/2504.04365</link>
<guid>https://arxiv.org/abs/2504.04365</guid>
<content:encoded><![CDATA[
<div> :  (LLMs)AutoPDL (AutoML)PDL

:
AutoPDLLLMsAutoMLPDLAutoPDLPDLAutoPDL8B70BLLM9.517.568.9pp<br /><br /> <div>
arXiv:2504.04365v1 Announce Type: new 
Abstract: The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations). Manually tuning this combination is tedious, error-prone, and non-transferable across LLMs or tasks. Therefore, this paper proposes AutoPDL, an automated approach to discover good LLM agent configurations. Our method frames this as a structured AutoML problem over a combinatorial space of agentic and non-agentic prompting patterns and demonstrations, using successive halving to efficiently navigate this space. We introduce a library implementing common prompting patterns using the PDL prompt programming language. AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library. This approach also enables source-to-source optimization, allowing human-in-the-loop refinement and reuse. Evaluations across three tasks and six LLMs (ranging from 8B to 70B parameters) show consistent accuracy gains ($9.5\pm17.5$ percentage points), up to 68.9pp, and reveal that selected prompting strategies vary across models and tasks.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Sokoban using Hierarchical Reinforcement Learning with Landmarks</title>
<link>https://arxiv.org/abs/2504.04366</link>
<guid>https://arxiv.org/abs/2504.04366</guid>
<content:encoded><![CDATA[
<div>  Hierarchical Reinforcement Learning (HRL)Recursive PlanningLearned SubgoalsSokobanDeep Recursive Goal Decomposition

:
HRLSokoban2-3 <div>
arXiv:2504.04366v1 Announce Type: new 
Abstract: We introduce a novel hierarchical reinforcement learning (HRL) framework that performs top-down recursive planning via learned subgoals, successfully applied to the complex combinatorial puzzle game Sokoban. Our approach constructs a six-level policy hierarchy, where each higher-level policy generates subgoals for the level below. All subgoals and policies are learned end-to-end from scratch, without any domain knowledge. Our results show that the agent can generate long action sequences from a single high-level call. While prior work has explored 2-3 level hierarchies and subgoal-based planning heuristics, we demonstrate that deep recursive goal decomposition can emerge purely from learning, and that such hierarchies can scale effectively to hard puzzle domains.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-Level Competitive Pok\'emon via Scalable Offline Reinforcement Learning with Transformers</title>
<link>https://arxiv.org/abs/2504.04395</link>
<guid>https://arxiv.org/abs/2504.04395</guid>
<content:encoded><![CDATA[
<div> : Competitive Pok\'emon Singles, AI, offline policy training, large sequence models, human battle dataset

<br />
:
Competitive Pok\'emon Singles, CPSAIAILLM Agent10% <div>
arXiv:2504.04395v1 Announce Type: new 
Abstract: Competitive Pok\'emon Singles (CPS) is a popular strategy game where players learn to exploit their opponent based on imperfect information in battles that can last more than one hundred stochastic turns. AI research in CPS has been led by heuristic tree search and online self-play, but the game may also create a platform to study adaptive policies trained offline on large datasets. We develop a pipeline to reconstruct the first-person perspective of an agent from logs saved from the third-person perspective of a spectator, thereby unlocking a dataset of real human battles spanning more than a decade that grows larger every day. This dataset enables a black-box approach where we train large sequence models to adapt to their opponent based solely on their input trajectory while selecting moves without explicit search of any kind. We study a progression from imitation learning to offline RL and offline fine-tuning on self-play data in the hardcore competitive setting of Pok\'emon's four oldest (and most partially observed) game generations. The resulting agents outperform a recent LLM Agent approach and a strong heuristic search engine. While playing anonymously in online battles against humans, our best agents climb to rankings inside the top 10% of active players.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRAMA: A Dynamic Packet Routing Algorithm using Multi-Agent Reinforcement Learning with Emergent Communication</title>
<link>https://arxiv.org/abs/2504.04438</link>
<guid>https://arxiv.org/abs/2504.04438</guid>
<content:encoded><![CDATA[
<div> : (RL), (MARL), (DRAMA), , 

:
DRAMADRAMAQRL/MARLDRAMA <div>
arXiv:2504.04438v1 Announce Type: new 
Abstract: The continuous expansion of network data presents a pressing challenge for conventional routing algorithms. As the demand escalates, these algorithms are struggling to cope. In this context, reinforcement learning (RL) and multi-agent reinforcement learning (MARL) algorithms emerge as promising solutions. However, the urgency and importance of the problem are clear, as existing RL/MARL-based routing approaches lack effective communication in run time among routers, making it challenging for individual routers to adapt to complex and dynamic changing networks. More importantly, they lack the ability to deal with dynamically changing network topology, especially the addition of the router, due to the non-scalability of their neural networks. This paper proposes a novel dynamic routing algorithm, DRAMA, incorporating emergent communication in multi-agent reinforcement learning. Through emergent communication, routers could learn how to communicate effectively to maximize the optimization objectives. Meanwhile, a new Q-network and graph-based emergent communication are introduced to dynamically adapt to the changing network topology without retraining while ensuring robust performance. Experimental results showcase DRAMA's superior performance over the traditional routing algorithm and other RL/MARL-based algorithms, achieving a higher delivery rate and lower latency in diverse network scenarios, including dynamic network load and topology. Moreover, an ablation experiment validates the prospect of emergent communication in facilitating packet routing.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoAgent2: Enhancing the LLM-Based Agent System for Long-Form Video Understanding by Uncertainty-Aware CoT</title>
<link>https://arxiv.org/abs/2504.04471</link>
<guid>https://arxiv.org/abs/2504.04471</guid>
<content:encoded><![CDATA[
<div> : chain-of-thought (CoT) 

<br />
:
chain-of-thought (CoT) LLMCoTVideoAgent2VideoAgent2VideoAgent2VideoAgent13.1% <div>
arXiv:2504.04471v1 Announce Type: new 
Abstract: Long video understanding has emerged as an increasingly important yet challenging task in computer vision. Agent-based approaches are gaining popularity for processing long videos, as they can handle extended sequences and integrate various tools to capture fine-grained information. However, existing methods still face several challenges: (1) they often rely solely on the reasoning ability of large language models (LLMs) without dedicated mechanisms to enhance reasoning in long video scenarios; and (2) they remain vulnerable to errors or noise from external tools. To address these issues, we propose a specialized chain-of-thought (CoT) process tailored for long video analysis. Our proposed CoT with plan-adjust mode enables the LLM to incrementally plan and adapt its information-gathering strategy. We further incorporate heuristic uncertainty estimation of both the LLM and external tools to guide the CoT process. This allows the LLM to assess the reliability of newly collected information, refine its collection strategy, and make more robust decisions when synthesizing final answers. Empirical experiments show that our uncertainty-aware CoT effectively mitigates noise from external tools, leading to more reliable outputs. We implement our approach in a system called VideoAgent2, which also includes additional modules such as general context acquisition and specialized tool design. Evaluation on three dedicated long video benchmarks (and their subsets) demonstrates that VideoAgent2 outperforms the previous state-of-the-art agent-based method, VideoAgent, by an average of 13.1% and achieves leading performance among all zero-shot approaches
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Nash Equilibrium Seeking in Coalition Games for Uncertain Euler-Lagrange Systems With Application to USV Swarm Confrontation</title>
<link>https://arxiv.org/abs/2504.04475</link>
<guid>https://arxiv.org/abs/2504.04475</guid>
<content:encoded><![CDATA[
<div> Euler-LagrangeNash

:

Euler-LagrangeNashLagrangeLyapunovNash <div>
arXiv:2504.04475v1 Announce Type: new 
Abstract: In this paper, a coalition game with local and coupling constraints is studied for uncertain Euler-Lagrange (EL) systems subject to disturbances with unknown bounds. In the coalition game, each agent collaborates with other agents within the same coalition to optimize its coalition's cost function while simultaneously competing against agents in other coalitions. Under a distributed framework where each agent has access only to its own action, cost function, and constraint parameters, a distributed strategy is proposed to seek the Nash equilibrium (NE). By combining adaptive methods and sign functions, model uncertainties and disturbances with unknown bounds in the EL system are compensated and suppressed, respectively. Furthermore, an integration of adaptive methods and consensus protocols is employed to update the Lagrange multipliers of both local and coupling constraints. A dynamic average consensus is employed to estimate the gradient of coalition function, while the leader-following protocol is adopted to estimate the actions of other agents. By leveraging Lyapunov theory, the NE is proven to be asymptotically stable. Moreover, an unmanned surface vehicle swarm confrontation is meticulously modeled and analyzed in the coalition game framework. A diverse array of tasks, including formation, encirclement, and interception, are systematically formulated. A numerical example demonstrates the effectiveness of the proposed algorithm.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building LLM Agents by Incorporating Insights from Computer Systems</title>
<link>https://arxiv.org/abs/2504.04485</link>
<guid>https://arxiv.org/abs/2504.04485</guid>
<content:encoded><![CDATA[
<div> LLM

<br /><br />:
LLMLLMLLMLLMLLMLLM <div>
arXiv:2504.04485v1 Announce Type: new 
Abstract: LLM-driven autonomous agents have emerged as a promising direction in recent years. However, many of these LLM agents are designed empirically or based on intuition, often lacking systematic design principles, which results in diverse agent structures with limited generality and scalability. In this paper, we advocate for building LLM agents by incorporating insights from computer systems. Inspired by the von Neumann architecture, we propose a structured framework for LLM agentic systems, emphasizing modular design and universal principles. Specifically, this paper first provides a comprehensive review of LLM agents from the computer system perspective, then identifies key challenges and future directions inspired by computer system design, and finally explores the learning mechanisms for LLM agents beyond the computer system. The insights gained from this comparative analysis offer a foundation for systematic LLM agent design and advancement.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Public speech recognition transcripts as a configuring parameter</title>
<link>https://arxiv.org/abs/2504.04488</link>
<guid>https://arxiv.org/abs/2504.04488</guid>
<content:encoded><![CDATA[
<div> : 

:
 <div>
arXiv:2504.04488v1 Announce Type: new 
Abstract: Displaying a written transcript of what a human said (i.e. producing an "automatic speech recognition transcript") is a common feature for smartphone vocal assistants: the utterance produced by a human speaker (e.g. a question) is displayed on the screen while it is being verbally responded to by the vocal assistant. Although very rarely, this feature also exists on some "social" robots which transcribe human interactants' speech on a screen or a tablet. We argue that this informational configuration is pragmatically consequential on the interaction, both for human participants and for the embodied conversational agent. Based on a corpus of co-present interactions with a humanoid robot, we attempt to show that this transcript is a contextual feature which can heavily impact the actions ascribed by humans to the robot: that is, the way in which humans respond to the robot's behavior as constituting a specific type of action (rather than another) and as constituting an adequate response to their own previous turn.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SchEdge: A Dynamic, Multi-agent, and Scalable Scheduling Simulator for IoT Edge</title>
<link>https://arxiv.org/abs/2504.04555</link>
<guid>https://arxiv.org/abs/2504.04555</guid>
<content:encoded><![CDATA[
<div> : SchEdgeIoT

:
SchEdgeSchEdgeSchEdge <div>
arXiv:2504.04555v1 Announce Type: new 
Abstract: This paper presents a dynamic, adaptive, and scalable framework for simulating task scheduling on the edge of the Internet of Things called "SchEdge". This simulator is designed to be highly configurable to reflect the detailed characteristics of real-world IoT. This framework focuses on online task scheduling and its multi-agent nature provides multiple schedulers to implement various scheduling schemes in parallel. SchEdge consists of two main parts the workflow and data flow. The workflow manages the schedulers' interaction with the application and environment while the data flow deals with the input application and its preprocessing. Combining these sections provides scalability, adaptability, and efficiency in the SchEdge. To validate the efficiency of this simulator, several experiments categorized as behavioral and technical analysis are performed to show its efficiency, scalability, and robustness.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"You just can't go around killing people" Explaining Agent Behavior to a Human Terminator</title>
<link>https://arxiv.org/abs/2504.04592</link>
<guid>https://arxiv.org/abs/2504.04592</guid>
<content:encoded><![CDATA[
<div> pre-trained agent, environment, human operator, intervention, explainability scheme

<br /><br />:
 <div>
arXiv:2504.04592v1 Announce Type: new 
Abstract: Consider a setting where a pre-trained agent is operating in an environment and a human operator can decide to temporarily terminate its operation and take-over for some duration of time. These kind of scenarios are common in human-machine interactions, for example in autonomous driving, factory automation and healthcare. In these settings, we typically observe a trade-off between two extreme cases -- if no take-overs are allowed, then the agent might employ a sub-optimal, possibly dangerous policy. Alternatively, if there are too many take-overs, then the human has no confidence in the agent, greatly limiting its usefulness. In this paper, we formalize this setup and propose an explainability scheme to help optimize the number of human interventions.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI in a vat: Fundamental limits of efficient world modelling for agent sandboxing and interpretability</title>
<link>https://arxiv.org/abs/2504.04608</link>
<guid>https://arxiv.org/abs/2504.04608</guid>
<content:encoded><![CDATA[
<div> : world models, AI agents, computational demands, efficiency, interpretability

:
AIAI <div>
arXiv:2504.04608v1 Announce Type: new 
Abstract: Recent work proposes using world models to generate controlled virtual environments in which AI agents can be tested before deployment to ensure their reliability and safety. However, accurate world models often have high computational demands that can severely restrict the scope and depth of such assessments. Inspired by the classic `brain in a vat' thought experiment, here we investigate ways of simplifying world models that remain agnostic to the AI agent under evaluation. By following principles from computational mechanics, our approach reveals a fundamental trade-off in world model construction between efficiency and interpretability, demonstrating that no single world model can optimise all desirable characteristics. Building on this trade-off, we identify procedures to build world models that either minimise memory requirements, delineate the boundaries of what is learnable, or allow tracking causes of undesirable outcomes. In doing so, this work establishes fundamental limits in world modelling, leading to actionable guidelines that inform core design choices related to effective agent evaluation.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Data-driven Control of Stochastic Multi-Agent Systems under Collaborative Signal Temporal Logic Specifications</title>
<link>https://arxiv.org/abs/2504.04615</link>
<guid>https://arxiv.org/abs/2504.04615</guid>
<content:encoded><![CDATA[
<div> stochastic discrete-time linear multi-agent systems, collaborative signal temporal logic, conformal prediction, chance-constrained, distributed optimization

:

MASSTLCPSTLPRPRCPLipschitzSTL[1] <div>
arXiv:2504.04615v1 Announce Type: new 
Abstract: We study the control of stochastic discrete-time linear multi-agent systems (MAS) subject to additive stochastic noise and collaborative signal temporal logic (STL) specifications to be satisfied with a desired probability. Given available disturbance datasets, we leverage conformal prediction (CP) to address the underlying chance-constrained multi-agent STL synthesis problem in a distribution-free manner. By introducing nonconformity scores as functions of prediction regions (PRs) of error trajectories, we develop an iterative PR-scaling and disturbance-feedback synthesis approach to bound training error trajectory samples. These bounds are then calibrated using a separate dataset, providing probabilistic guarantees via CP. Subsequently, we relax the underlying stochastic optimal control problem by tightening the robustness functions of collaborative tasks based on their Lipschitz constants and the computed error bounds. To address scalability, we exploit the compositional structure of the multi-agent STL formula and propose a model-predictive-control-like algorithm, where agent-level problems are solved in a distributed fashion. Lastly, we showcase the benefits of the proposed method in comparison with [1] via an illustrative example.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Mixed-Integer Quadratic Programming for Mixed-Traffic Intersection Control</title>
<link>https://arxiv.org/abs/2504.04618</link>
<guid>https://arxiv.org/abs/2504.04618</guid>
<content:encoded><![CDATA[
<div> proximal ADMM

<br /><br />
proximalADMMCAVsMIQPMIQPCAVMIQPproximal ADMMMIQP <div>
arXiv:2504.04618v1 Announce Type: new 
Abstract: In this paper, we present a distributed algorithm utilizing the proximal alternating direction method of multipliers (ADMM) in conjunction with sequential constraint tightening to address mixed-integer quadratic programming (MIQP) problems associated with traffic light systems and connected automated vehicles (CAVs) in mixed-traffic intersections. We formulate a comprehensive MIQP model aimed at optimizing the coordination of traffic light systems and CAVs, thereby fully capitalizing on the advantages of CAV integration under conditions of high penetration rates. To effectively approximate the intricate multi-agent MIQP challenges, we develop a distributed algorithm that employs proximal ADMM for solving the convex relaxation of the MIQP while systematically tightening the constraint coefficients to uphold integrality requirements. The performance of our control framework and the efficacy of the distributed algorithm are rigorously validated through a series of simulations conducted across varying penetration rates and traffic volumes.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autono: ReAct-Based Highly Robust Autonomous Agent Framework</title>
<link>https://arxiv.org/abs/2504.04650</link>
<guid>https://arxiv.org/abs/2504.04650</guid>
<content:encoded><![CDATA[
<div> : ReActagent

:
ReActagentLLMagentagentMCPagent <div>
arXiv:2504.04650v1 Announce Type: new 
Abstract: This paper proposes a highly robust autonomous agent framework based on the ReAct paradigm, designed to solve complex tasks through adaptive decision making and multi-agent collaboration. Unlike traditional frameworks that rely on fixed workflows generated by LLM-based planners, this framework dynamically generates next actions during agent execution based on prior trajectories, thereby enhancing its robustness. To address potential termination issues caused by adaptive execution paths, I propose a timely abandonment strategy incorporating a probabilistic penalty mechanism. For multi-agent collaboration, I introduce a memory transfer mechanism that enables shared and dynamically updated memory among agents. The framework's innovative timely abandonment strategy dynamically adjusts the probability of task abandonment via probabilistic penalties, allowing developers to balance conservative and exploratory tendencies in agent execution strategies by tuning hyperparameters. This significantly improves adaptability and task execution efficiency in complex environments. Additionally, agents can be extended through external tool integration, supported by modular design and MCP protocol compatibility, which enables flexible action space expansion. Through explicit division of labor, the multi-agent collaboration mechanism enables agents to focus on specific task components, thereby significantly improving execution efficiency and quality.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HypRL: Reinforcement Learning of Control Policies for Hyperproperties</title>
<link>https://arxiv.org/abs/2504.04675</link>
<guid>https://arxiv.org/abs/2504.04675</guid>
<content:encoded><![CDATA[
<div> : HyperLTL

<br /><br />:
MHyperLTL$\varphi$SkolemHyperLTLM$\varphi$(1)$\varphi$(2)M$\varphi$M(1)(2)(3)(PCR) <div>
arXiv:2504.04675v1 Announce Type: new 
Abstract: We study the problem of learning control policies for complex tasks whose requirements are given by a hyperproperty. The use of hyperproperties is motivated by their significant power to formally specify requirements of multi-agent systems as well as those that need expressiveness in terms of multiple execution traces (e.g., privacy and fairness). Given a Markov decision process M with unknown transitions (representing the environment) and a HyperLTL formula $\varphi$, our approach first employs Skolemization to handle quantifier alternations in $\varphi$. We introduce quantitative robustness functions for HyperLTL to define rewards of finite traces of M with respect to $\varphi$. Finally, we utilize a suitable reinforcement learning algorithm to learn (1) a policy per trace quantifier in $\varphi$, and (2) the probability distribution of transitions of M that together maximize the expected reward and, hence, probability of satisfaction of $\varphi$ in M. We present a set of case studies on (1) safety-preserving multi-agent path planning, (2) fairness in resource allocation, and (3) the post-correspondence problem (PCP).
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-Scale Mixed-Traffic and Intersection Control using Multi-agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.04691</link>
<guid>https://arxiv.org/abs/2504.04691</guid>
<content:encoded><![CDATA[
<div> 

<br />
: 1480%6.175.09500454493 <div>
arXiv:2504.04691v1 Announce Type: new 
Abstract: Traffic congestion remains a significant challenge in modern urban networks. Autonomous driving technologies have emerged as a potential solution. Among traffic control methods, reinforcement learning has shown superior performance over traffic signals in various scenarios. However, prior research has largely focused on small-scale networks or isolated intersections, leaving large-scale mixed traffic control largely unexplored. This study presents the first attempt to use decentralized multi-agent reinforcement learning for large-scale mixed traffic control in which some intersections are managed by traffic signals and others by robot vehicles. Evaluating a real-world network in Colorado Springs, CO, USA with 14 intersections, we measure traffic efficiency via average waiting time of vehicles at intersections and the number of vehicles reaching their destinations within a time window (i.e., throughput). At 80% RV penetration rate, our method reduces waiting time from 6.17 s to 5.09 s and increases throughput from 454 vehicles per 500 seconds to 493 vehicles per 500 seconds, outperforming the baseline of fully signalized intersections. These findings suggest that integrating reinforcement learning-based control large-scale traffic can improve overall efficiency and may inform future urban planning strategies.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalising from Self-Produced Data: Model Training Beyond Human Constraints</title>
<link>https://arxiv.org/abs/2504.04711</link>
<guid>https://arxiv.org/abs/2504.04711</guid>
<content:encoded><![CDATA[
<div> : GRPO

<br /><br />:
AIAIwarm startGRPO <div>
arXiv:2504.04711v1 Announce Type: new 
Abstract: Current large language models (LLMs) are constrained by human-derived training data and limited by a single level of abstraction that impedes definitive truth judgments. This paper introduces a novel framework in which AI models autonomously generate and validate new knowledge through direct interaction with their environment. Central to this approach is an unbounded, ungamable numeric reward - such as annexed disk space or follower count - that guides learning without requiring human benchmarks. AI agents iteratively generate strategies and executable code to maximize this metric, with successful outcomes forming the basis for self-retraining and incremental generalisation. To mitigate model collapse and the warm start problem, the framework emphasizes empirical validation over textual similarity and supports fine-tuning via GRPO. The system architecture employs modular agents for environment analysis, strategy generation, and code synthesis, enabling scalable experimentation. This work outlines a pathway toward self-improving AI systems capable of advancing beyond human-imposed constraints toward autonomous general intelligence.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Robustness of GUI Grounding Models Against Image Attacks</title>
<link>https://arxiv.org/abs/2504.04716</link>
<guid>https://arxiv.org/abs/2504.04716</guid>
<content:encoded><![CDATA[
<div> Graphical User Interface (GUI) grounding modelsrobustnessuntargeted adversarial attackstargeted adversarial attacksUGround

<br /><br />:
(GUI)GUIUGroundGUIGUI <div>
arXiv:2504.04716v1 Announce Type: new 
Abstract: Graphical User Interface (GUI) grounding models are crucial for enabling intelligent agents to understand and interact with complex visual interfaces. However, these models face significant robustness challenges in real-world scenarios due to natural noise and adversarial perturbations, and their robustness remains underexplored. In this study, we systematically evaluate the robustness of state-of-the-art GUI grounding models, such as UGround, under three conditions: natural noise, untargeted adversarial attacks, and targeted adversarial attacks. Our experiments, which were conducted across a wide range of GUI environments, including mobile, desktop, and web interfaces, have clearly demonstrated that GUI grounding models exhibit a high degree of sensitivity to adversarial perturbations and low-resolution conditions. These findings provide valuable insights into the vulnerabilities of GUI grounding models and establish a strong benchmark for future research aimed at enhancing their robustness in practical applications. Our code is available at https://github.com/ZZZhr-1/Robust_GUI_Grounding.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models</title>
<link>https://arxiv.org/abs/2504.04717</link>
<guid>https://arxiv.org/abs/2504.04717</guid>
<content:encoded><![CDATA[
<div> : 

<br />
:
LLMsLLMshttps://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs <div>
arXiv:2504.04717v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have revolutionized their ability to handle single-turn tasks, yet real-world applications demand sophisticated multi-turn interactions. This survey provides a comprehensive review of recent advancements in evaluating and enhancing multi-turn interactions in LLMs. Focusing on task-specific scenarios, from instruction following in diverse domains such as math and coding to complex conversational engagements in roleplay, healthcare, education, and even adversarial jailbreak settings, we systematically examine the challenges of maintaining context, coherence, fairness, and responsiveness over prolonged dialogues. The paper organizes current benchmarks and datasets into coherent categories that reflect the evolving landscape of multi-turn dialogue evaluation. In addition, we review a range of enhancement methodologies under multi-turn settings, including model-centric strategies (contextual learning, supervised fine-tuning, reinforcement learning, and new architectures), external integration approaches (memory-augmented, retrieval-based methods, and knowledge graph), and agent-based techniques for collaborative interactions. Finally, we discuss open challenges and propose future directions for research to further advance the robustness and effectiveness of multi-turn interactions in LLMs. Related resources and papers are available at https://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Data Generation &amp; Multi-Step RL for Reasoning &amp; Tool Use</title>
<link>https://arxiv.org/abs/2504.04736</link>
<guid>https://arxiv.org/abs/2504.04736</guid>
<content:encoded><![CDATA[
<div> : , , , , SWiRL

:

SWiRLStep-Wise Reinforcement LearningSWiRLRLSWiRLGSM8KHotPotQACofCAMuSiQueBeerQA21.5%12.3%14.8%11.1%15.3%SWiRLHotPotQAGSM8K16.9% <div>
arXiv:2504.04736v1 Announce Type: new 
Abstract: Reinforcement learning has been shown to improve the performance of large language models. However, traditional approaches like RLHF or RLAIF treat the problem as single-step. As focus shifts toward more complex reasoning and agentic tasks, language models must take multiple steps of text generation, reasoning and environment interaction before generating a solution. We propose a synthetic data generation and RL methodology targeting multi-step optimization scenarios. This approach, called Step-Wise Reinforcement Learning (SWiRL), iteratively generates multi-step reasoning and tool use data, and then learns from that data. It employs a simple step-wise decomposition that breaks each multi-step trajectory into multiple sub-trajectories corresponding to each action by the original model. It then applies synthetic data filtering and RL optimization on these sub-trajectories. We evaluated SWiRL on a number of multi-step tool use, question answering, and mathematical reasoning tasks. Our experiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%, 14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA, MuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits generalization across tasks: for example, training only on HotPotQA (text question-answering) improves zero-shot performance on GSM8K (a math dataset) by a relative 16.9%.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Locomotive Crowd Behavior Generation</title>
<link>https://arxiv.org/abs/2504.04756</link>
<guid>https://arxiv.org/abs/2504.04756</guid>
<content:encoded><![CDATA[
<div> : arXiv:2504.04756v1, , , , <br /><br />: https://github.com/InhwanBae/CrowdES  <div>
arXiv:2504.04756v1 Announce Type: new 
Abstract: Modeling and reproducing crowd behaviors are important in various domains including psychology, robotics, transport engineering and virtual environments. Conventional methods have focused on synthesizing momentary scenes, which have difficulty in replicating the continuous nature of real-world crowds. In this paper, we introduce a novel method for automatically generating continuous, realistic crowd trajectories with heterogeneous behaviors and interactions among individuals. We first design a crowd emitter model. To do this, we obtain spatial layouts from single input images, including a segmentation map, appearance map, population density map and population probability, prior to crowd generation. The emitter then continually places individuals on the timeline by assigning independent behavior characteristics such as agents' type, pace, and start/end positions using diffusion models. Next, our crowd simulator produces their long-term locomotions. To simulate diverse actions, it can augment their behaviors based on a Markov chain. As a result, our overall framework populates the scenes with heterogeneous crowd behaviors by alternating between the proposed emitter and simulator. Note that all the components in the proposed framework are user-controllable. Lastly, we propose a benchmark protocol to evaluate the realism and quality of the generated crowds in terms of the scene-level population dynamics and the individual-level trajectory accuracy. We demonstrate that our approach effectively models diverse crowd behavior patterns and generalizes well across different geographical environments. Code is publicly available at https://github.com/InhwanBae/CrowdES .
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Deep Reinforcement Learning for Multiple Anesthetics Collaborative Control</title>
<link>https://arxiv.org/abs/2504.04765</link>
<guid>https://arxiv.org/abs/2504.04765</guid>
<content:encoded><![CDATA[
<div> : (TIVA)(VD-MADRL)(TCI)

:
(VD-MADRL)(TIVA)(MG)(RF)VD-MADRL<br /><br /> <div>
arXiv:2504.04765v1 Announce Type: new 
Abstract: Automated control of personalized multiple anesthetics in clinical Total Intravenous Anesthesia (TIVA) is crucial yet challenging. Current systems, including target-controlled infusion (TCI) and closed-loop systems, either rely on relatively static pharmacokinetic/pharmacodynamic (PK/PD) models or focus on single anesthetic control, limiting personalization and collaborative control. To address these issues, we propose a novel framework, Value Decomposition Multi-Agent Deep Reinforcement Learning (VD-MADRL). VD-MADRL optimizes the collaboration between two anesthetics propofol (Agent I) and remifentanil (Agent II). And It uses a Markov Game (MG) to identify optimal actions among heterogeneous agents. We employ various value function decomposition methods to resolve the credit allocation problem and enhance collaborative control. We also introduce a multivariate environment model based on random forest (RF) for anesthesia state simulation. Additionally, a data resampling and alignment technique ensures synchronized trajectory data. Our experiments on general and thoracic surgery datasets show that VD-MADRL performs better than human experience. It improves dose precision and keeps anesthesia states stable, providing great clinical value.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Playing Non-Embedded Card-Based Games with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.04783</link>
<guid>https://arxiv.org/abs/2504.04783</guid>
<content:encoded><![CDATA[
<div> : AIClash Royale

:<br />
Clash RoyaleAIhttps://github.com/wty-yy/katacr <div>
arXiv:2504.04783v1 Announce Type: new 
Abstract: Significant progress has been made in AI for games, including board games, MOBA, and RTS games. However, complex agents are typically developed in an embedded manner, directly accessing game state information, unlike human players who rely on noisy visual data, leading to unfair competition. Developing complex non-embedded agents remains challenging, especially in card-based RTS games with complex features and large state spaces. We propose a non-embedded offline reinforcement learning training strategy using visual inputs to achieve real-time autonomous gameplay in the RTS game Clash Royale. Due to the lack of a object detection dataset for this game, we designed an efficient generative object detection dataset for training. We extract features using state-of-the-art object detection and optical character recognition models. Our method enables real-time image acquisition, perception feature fusion, decision-making, and control on mobile devices, successfully defeating built-in AI opponents. All code is open-sourced at https://github.com/wty-yy/katacr.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weak-for-Strong: Training Weak Meta-Agent to Harness Strong Executors</title>
<link>https://arxiv.org/abs/2504.04785</link>
<guid>https://arxiv.org/abs/2504.04785</guid>
<content:encoded><![CDATA[
<div> : 

<br /><br />:
Harnessing (W4S)LLMsW4SRLAOGPU7BW4S2.9%~24.6%GPT-3.5-TurboGPT-4oW4S <div>
arXiv:2504.04785v1 Announce Type: new 
Abstract: Efficiently leveraging of the capabilities of contemporary large language models (LLMs) is increasingly challenging, particularly when direct fine-tuning is expensive and often impractical. Existing training-free methods, including manually or automated designed workflows, typically demand substantial human effort or yield suboptimal results. This paper proposes Weak-for-Strong Harnessing (W4S), a novel framework that customizes smaller, cost-efficient language models to design and optimize workflows for harnessing stronger models. W4S formulates workflow design as a multi-turn markov decision process and introduces reinforcement learning for agentic workflow optimization (RLAO) to train a weak meta-agent. Through iterative interaction with the environment, the meta-agent learns to design increasingly effective workflows without manual intervention. Empirical results demonstrate the superiority of W4S that our 7B meta-agent, trained with just one GPU hour, outperforms the strongest baseline by 2.9% ~ 24.6% across eleven benchmarks, successfully elevating the performance of state-of-the-art models such as GPT-3.5-Turbo and GPT-4o. Notably, W4S exhibits strong generalization capabilities across both seen and unseen tasks, offering an efficient, high-performing alternative to directly fine-tuning strong models.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Agricultural Agent Architecture (MA3): A New Paradigm for Intelligent Agricultural Decision-Making</title>
<link>https://arxiv.org/abs/2504.04789</link>
<guid>https://arxiv.org/abs/2504.04789</guid>
<content:encoded><![CDATA[
<div> : , , (MA3), , 

:
MA3VQAVQAMA3 <div>
arXiv:2504.04789v1 Announce Type: new 
Abstract: As a strategic pillar industry for human survival and development, modern agriculture faces dual challenges: optimizing production efficiency and achieving sustainable development. Against the backdrop of intensified climate change leading to frequent extreme weather events, the uncertainty risks in agricultural production systems are increasing exponentially. To address these challenges, this study proposes an innovative \textbf{M}ultimodal \textbf{A}gricultural \textbf{A}gent \textbf{A}rchitecture (\textbf{MA3}), which leverages cross-modal information fusion and task collaboration mechanisms to achieve intelligent agricultural decision-making. This study constructs a multimodal agricultural agent dataset encompassing five major tasks: classification, detection, Visual Question Answering (VQA), tool selection, and agent evaluation. We propose a unified backbone for sugarcane disease classification and detection tools, as well as a sugarcane disease expert model. By integrating an innovative tool selection module, we develop a multimodal agricultural agent capable of effectively performing tasks in classification, detection, and VQA. Furthermore, we introduce a multi-dimensional quantitative evaluation framework and conduct a comprehensive assessment of the entire architecture over our evaluation dataset, thereby verifying the practicality and robustness of MA3 in agricultural scenarios. This study provides new insights and methodologies for the development of agricultural agents, holding significant theoretical and practical implications. Our source code and dataset will be made publicly available upon acceptance.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELT-Bench: An End-to-End Benchmark for Evaluating AI Agents on ELT Pipelines</title>
<link>https://arxiv.org/abs/2504.04808</link>
<guid>https://arxiv.org/abs/2504.04808</guid>
<content:encoded><![CDATA[
<div> : ELTAIELT-Bench

:
Extract-Load-TransformELTAISQLELTELTAIELT-BenchAIELTELT-Bench100835203AISpider-AgentSWE-AgentLLMsELT-BenchSpider-Agent Claude-3.7-Sonnet3.9%$4.3089.3ELT-BenchAIELThttps://github.com/uiuc-kang-lab/ETL.git <div>
arXiv:2504.04808v1 Announce Type: new 
Abstract: Practitioners are increasingly turning to Extract-Load-Transform (ELT) pipelines with the widespread adoption of cloud data warehouses. However, designing these pipelines often involves significant manual work to ensure correctness. Recent advances in AI-based methods, which have shown strong capabilities in data tasks, such as text-to-SQL, present an opportunity to alleviate manual efforts in developing ELT pipelines. Unfortunately, current benchmarks in data engineering only evaluate isolated tasks, such as using data tools and writing data transformation queries, leaving a significant gap in evaluating AI agents for generating end-to-end ELT pipelines.
  To fill this gap, we introduce ELT-Bench, an end-to-end benchmark designed to assess the capabilities of AI agents to build ELT pipelines. ELT-Bench consists of 100 pipelines, including 835 source tables and 203 data models across various domains. By simulating realistic scenarios involving the integration of diverse data sources and the use of popular data tools, ELT-Bench evaluates AI agents' abilities in handling complex data engineering workflows. AI agents must interact with databases and data tools, write code and SQL queries, and orchestrate every pipeline stage. We evaluate two representative code agent frameworks, Spider-Agent and SWE-Agent, using six popular Large Language Models (LLMs) on ELT-Bench. The highest-performing agent, Spider-Agent Claude-3.7-Sonnet with extended thinking, correctly generates only 3.9% of data models, with an average cost of $4.30 and 89.3 steps per pipeline. Our experimental results demonstrate the challenges of ELT-Bench and highlight the need for a more advanced AI agent to reduce manual effort in ELT workflows. Our code and data are available at https://github.com/uiuc-kang-lab/ETL.git.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Approach for Cooperative Multi-Agent Learning Problems</title>
<link>https://arxiv.org/abs/2504.04850</link>
<guid>https://arxiv.org/abs/2504.04850</guid>
<content:encoded><![CDATA[
<div> 

:
Multi-Agent Learning<br /><br /> <div>
arXiv:2504.04850v1 Announce Type: new 
Abstract: In this article, we propose a centralized Multi-Agent Learning framework for learning a policy that models the simultaneous behavior of multiple agents that need to coordinate to solve a certain task. Centralized approaches often suffer from the explosion of an action space that is defined by all possible combinations of individual actions, known as joint actions. Our approach addresses the coordination problem via a sequential abstraction, which overcomes the scalability problems typical to centralized methods. It introduces a meta-agent, called \textit{supervisor}, which abstracts joint actions as sequential assignments of actions to each agent. This sequential abstraction not only simplifies the centralized joint action space but also enhances the framework's scalability and efficiency. Our experimental results demonstrate that the proposed approach successfully coordinates agents across a variety of Multi-Agent Learning environments of diverse sizes.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BIASINSPECTOR: Detecting Bias in Structured Data through LLM Agents</title>
<link>https://arxiv.org/abs/2504.04855</link>
<guid>https://arxiv.org/abs/2504.04855</guid>
<content:encoded><![CDATA[
<div> BIASINSPECTOR

:
BIASINSPECTORBIASINSPECTORBIASINSPECTOR<br /><br /> <div>
arXiv:2504.04855v1 Announce Type: new 
Abstract: Detecting biases in structured data is a complex and time-consuming task. Existing automated techniques are limited in diversity of data types and heavily reliant on human case-by-case handling, resulting in a lack of generalizability. Currently, large language model (LLM)-based agents have made significant progress in data science, but their ability to detect data biases is still insufficiently explored. To address this gap, we introduce the first end-to-end, multi-agent synergy framework, BIASINSPECTOR, designed for automatic bias detection in structured data based on specific user requirements. It first develops a multi-stage plan to analyze user-specified bias detection tasks and then implements it with a diverse and well-suited set of tools. It delivers detailed results that include explanations and visualizations. To address the lack of a standardized framework for evaluating the capability of LLM agents to detect biases in data, we further propose a comprehensive benchmark that includes multiple evaluation metrics and a large set of test cases. Extensive experiments demonstrate that our framework achieves exceptional overall performance in structured data bias detection, setting a new milestone for fairer data applications.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAMDTP: Dynamic Trajectory Prediction with Graph Attention Mamba Network</title>
<link>https://arxiv.org/abs/2504.04862</link>
<guid>https://arxiv.org/abs/2504.04862</guid>
<content:encoded><![CDATA[
<div> : GAMDTP

:
GAMDTPGAMDTPmamba-ssmArgoverseGAMDTP <div>
arXiv:2504.04862v1 Announce Type: new 
Abstract: Accurate motion prediction of traffic agents is crucial for the safety and stability of autonomous driving systems. In this paper, we introduce GAMDTP, a novel graph attention-based network tailored for dynamic trajectory prediction. Specifically, we fuse the result of self attention and mamba-ssm through a gate mechanism, leveraging the strengths of both to extract features more efficiently and accurately, in each graph convolution layer. GAMDTP encodes the high-definition map(HD map) data and the agents' historical trajectory coordinates and decodes the network's output to generate the final prediction results. Additionally, recent approaches predominantly focus on dynamically fusing historical forecast results and rely on two-stage frameworks including proposal and refinement. To further enhance the performance of the two-stage frameworks we also design a scoring mechanism to evaluate the prediction quality during the proposal and refinement processes. Experiments on the Argoverse dataset demonstrates that GAMDTP achieves state-of-the-art performance, achieving superior accuracy in dynamic trajectory prediction.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Persuasive Dialogues on Meat Reduction with Generative Agents</title>
<link>https://arxiv.org/abs/2504.04872</link>
<guid>https://arxiv.org/abs/2504.04872</guid>
<content:encoded><![CDATA[
<div> 

<br />
:
LLMs <div>
arXiv:2504.04872v1 Announce Type: new 
Abstract: Meat reduction benefits human and planetary health, but social norms keep meat central in shared meals. To date, the development of communication strategies that promote meat reduction while minimizing social costs has required the costly involvement of human participants at each stage of the process. We present work in progress on simulating multi-round dialogues on meat reduction between Generative Agents based on large language models (LLMs). We measure our main outcome using established psychological questionnaires based on the Theory of Planned Behavior and additionally investigate Social Costs. We find evidence that our preliminary simulations produce outcomes that are (i) consistent with theoretical expectations; and (ii) valid when compared to data from previous studies with human participants. Generative agent-based models are a promising tool for identifying novel communication strategies on meat reduction-tailored to highly specific participant groups-to then be tested in subsequent studies with human participants.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Control Barrier Functions for Nonholonomic Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2504.04937</link>
<guid>https://arxiv.org/abs/2504.04937</guid>
<content:encoded><![CDATA[
<div> :  barrier (CBF)holonomic  CBFs (SCBF)

<br /><br />:

 Barrier CBFholonomicCBFCBFs (SCBFs)CBF <div>
arXiv:2504.04937v1 Announce Type: new 
Abstract: This paper addresses the problem of guaranteeing safety of multiple coordinated agents moving in dynamic environments. It has recently been shown that this problem can be efficiently solved through the notion of Control Barrier Functions (CBFs). However, for nonholonomic vehicles that are required to keep positive speeds, existing CBFs lose their validity. To overcome this limitation, we propose a hybrid formulation based on synergistic CBFs (SCBFs), which leverages a discrete switching mechanism to avoid configurations that would render the CBF invalid. Unlike existing approaches, our method ensures safety in the presence of moving obstacles and inter-agent interactions while respecting nonzero speed restrictions. We formally analyze the feasibility of the constraints with respect to actuation limits, and the efficacy of the solution is demonstrated in simulation of a multi-agent coordination problem in the presence of moving obstacles.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Dream Within Huang Long Cave: AI-Driven Interactive Narrative for Family Storytelling and Emotional Reflection</title>
<link>https://arxiv.org/abs/2504.04968</link>
<guid>https://arxiv.org/abs/2504.04968</guid>
<content:encoded><![CDATA[
<div> : AI

<br />
:
AIAILLMYELLCAVEYELLYELL <div>
arXiv:2504.04968v1 Announce Type: new 
Abstract: This paper introduces the art project The Dream Within Huang Long Cave, an AI-driven interactive and immersive narrative experience. The project offers new insights into AI technology, artistic practice, and psychoanalysis. Inspired by actual geographical landscapes and familial archetypes, the work combines psychoanalytic theory and computational technology, providing an artistic response to the concept of the non-existence of the Big Other. The narrative is driven by a combination of a large language model (LLM) and a realistic digital character, forming a virtual agent named YELL. Through dialogue and exploration within a cave automatic virtual environment (CAVE), the audience is invited to unravel the language puzzles presented by YELL and help him overcome his life challenges. YELL is a fictional embodiment of the Big Other, modeled after the artist's real father. Through a cross-temporal interaction with this digital father, the project seeks to deconstruct complex familial relationships. By demonstrating the non-existence of the Big Other, we aim to underscore the authenticity of interpersonal emotions, positioning art as a bridge for emotional connection and understanding within family dynamics.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Pedestrian and Vehicle Traffic Optimization in Urban Environments using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.05018</link>
<guid>https://arxiv.org/abs/2504.05018</guid>
<content:encoded><![CDATA[
<div> : (RL), , , , 

:
Wi-Fi67%52%67%53%<br /><br /> <div>
arXiv:2504.05018v1 Announce Type: new 
Abstract: Reinforcement learning (RL) holds significant promise for adaptive traffic signal control. While existing RL-based methods demonstrate effectiveness in reducing vehicular congestion, their predominant focus on vehicle-centric optimization leaves pedestrian mobility needs and safety challenges unaddressed. In this paper, we present a deep RL framework for adaptive control of eight traffic signals along a real-world urban corridor, jointly optimizing both pedestrian and vehicular efficiency. Our single-agent policy is trained using real-world pedestrian and vehicle demand data derived from Wi-Fi logs and video analysis. The results demonstrate significant performance improvements over traditional fixed-time signals, reducing average wait times per pedestrian and per vehicle by up to 67% and 52%, respectively, while simultaneously decreasing total accumulated wait times for both groups by up to 67% and 53%. Additionally, our results demonstrate generalization capabilities across varying traffic demands, including conditions entirely unseen during training, validating RL's potential for developing transportation systems that serve all road users.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture-of-Personas Language Models for Population Simulation</title>
<link>https://arxiv.org/abs/2504.05019</link>
<guid>https://arxiv.org/abs/2504.05019</guid>
<content:encoded><![CDATA[
<div> 

<br /><br />: (MoP)(LLMs)LLMsMoPLLMMoPMoP <div>
arXiv:2504.05019v1 Announce Type: new 
Abstract: Advances in Large Language Models (LLMs) paved the way for their emerging applications in various domains, such as human behavior simulations, where LLMs could augment human-generated data in social science research and machine learning model training. However, pretrained LLMs often fail to capture the behavioral diversity of target populations due to the inherent variability across individuals and groups. To address this, we propose \textit{Mixture of Personas} (MoP), a \textit{probabilistic} prompting method that aligns the LLM responses with the target population. MoP is a contextual mixture model, where each component is an LM agent characterized by a persona and an exemplar representing subpopulation behaviors. The persona and exemplar are randomly chosen according to the learned mixing weights to elicit diverse LLM responses during simulation. MoP is flexible, requires no model finetuning, and is transferable across base models. Experiments for synthetic data generation show that MoP outperforms competing methods in alignment and diversity metrics.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>