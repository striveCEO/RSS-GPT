<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>GitHub All Languages Weekly Trending</title>
<link>http://mshibanami.github.io/GitHubTrendingRSS</link>

<item>
<title>toss/es-toolkit</title>
<link>https://github.com/toss/es-toolkit</link>
<guid>https://github.com/toss/es-toolkit</guid>
<content:encoded><![CDATA[
<div> 关键词：es-toolkit, 高性能, 小型bundle, TypeScript支持, 性能优化

总结:

es-toolkit 是一款高性能、小型体积的现代JavaScript工具库。它提供了各种实用的函数，如debounce、chunk等，旨在提升开发效率。es-toolkit在现代JavaScript环境中实现了显著的性能提升，比其他库快了2-3倍，同时体积减少了97%，这使得它成为轻量级项目和追求高效执行的理想选择。

es-toolkit还具有内置的类型注解支持，与TypeScript兼容，提供清晰且强大的类型定义。它包括有用的类型守卫，帮助开发者更好地理解数据结构。此外，es-toolkit的代码经过全面测试，拥有100%的覆盖率，确保了其稳定性和可靠性。

对于那些寻求高性能、轻量级且类型安全的JavaScript库的开发者来说，es-toolkit是一个值得考虑的选择。无论是需要处理大量数据还是希望优化代码执行速度，es-toolkit都能提供有效的解决方案。同时，es-toolkit欢迎社区成员的贡献，共同推动其发展和改进。 <div>
<p>A modern JavaScript utility library that's 2-3 times faster and up to 97% smaller—a major upgrade to lodash.</p><hr /><p><img alt="" src="https://raw.githubusercontent.com/toss/es-toolkit/main/docs/public/og.png" /></p> 
<h1>es-toolkit · <a href="https://github.com/toss/slash/raw/main/LICENSE"><img alt="MIT License" src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" /></a> <a href="https://codecov.io/gh/toss/es-toolkit"><img alt="codecov" src="https://codecov.io/gh/toss/es-toolkit/graph/badge.svg?token=8N5S3AR3C7" /></a> <a href="https://www.npmjs.com/package/es-toolkit"><img alt="NPM badge" src="https://img.shields.io/npm/v/es-toolkit?logo=npm" /></a> <a href="https://jsr.io/@es-toolkit/es-toolkit"><img alt="JSR badge" src="https://jsr.io/badges/@es-toolkit/es-toolkit" /></a></h1> 
<p>English | <a href="https://github.com/toss/es-toolkit/raw/main/README-ko_kr.md">한국어</a> | <a href="https://github.com/toss/es-toolkit/raw/main/README-zh_hans.md">简体中文</a> | <a href="https://github.com/toss/es-toolkit/raw/main/README-ja_jp.md">日本語</a></p> 
<p>es-toolkit is a state-of-the-art, high-performance JavaScript utility library with a small bundle size and strong type annotations.</p> 
<ul> 
 <li>es-toolkit offers a variety of everyday utility functions with modern implementations, such as <a href="https://es-toolkit.slash.page/reference/function/debounce.html">debounce</a>, <a href="https://es-toolkit.slash.page/reference/promise/delay.html">delay</a>, <a href="https://es-toolkit.slash.page/reference/array/chunk.html">chunk</a>, <a href="https://es-toolkit.slash.page/reference/math/sum.html">sum</a>, and <a href="https://es-toolkit.slash.page/reference/object/pick.html">pick</a>.</li> 
 <li>Designed with performance in mind, es-toolkit achieves <a href="https://es-toolkit.slash.page/performance.html">2-3× better performance</a> in modern JavaScript environments.</li> 
 <li>es-toolkit supports tree shaking out of the box, and <a href="https://es-toolkit.slash.page/bundle-size.html">reduces JavaScript code by up to 97%</a> compared to other libraries.</li> 
 <li>es-toolkit includes built-in TypeScript support, with straightforward yet robust types. It also provides useful type guards such as <a href="https://es-toolkit.slash.page/reference/predicate/isNotNil.html">isNotNil</a>.</li> 
 <li>es-toolkit is battle-tested with 100% test coverage, ensuring reliability and robustness.</li> 
</ul> 
<h2>Examples</h2> 
<pre><code class="language-tsx">// import from '@es-toolkit/es-toolkit' in jsr.
import { debounce, chunk } from 'es-toolkit';

const debouncedLog = debounce(message =&gt; {
  console.log(message);
}, 300);

// This call will be debounced
debouncedLog('Hello, world!');

const array = [1, 2, 3, 4, 5, 6];
const chunkedArray = chunk(array, 2);

console.log(chunkedArray);
// Output: [[1, 2], [3, 4], [5, 6]]
</code></pre> 
<h2>Contributing</h2> 
<p>We welcome contribution from everyone in the community. Read below for detailed contribution guide.</p> 
<p><a href="https://github.com/toss/es-toolkit/raw/main/.github/CONTRIBUTING.md">CONTRIBUTING</a></p> 
<h2>License</h2> 
<p>MIT © Viva Republica, Inc. See <a href="https://raw.githubusercontent.com/toss/es-toolkit/main/LICENSE">LICENSE</a> for details.</p> 
<a href="https://toss.im" title="Toss"> 
  
  <source media="(prefers-color-scheme: dark)" /> 
  <img alt="Toss" src="https://static.toss.im/logos/png/4x/logo-toss.png" width="100" /> 
  </a>
]]></content:encoded>


</item>
<item>
<title>comfyanonymous/ComfyUI</title>
<link>https://github.com/comfyanonymous/ComfyUI</link>
<guid>https://github.com/comfyanonymous/ComfyUI</guid>
<content:encoded><![CDATA[
<div> 关键词：ComfyUI、Stable Diffusion、GUI、API、Backend

总结：
ComfyUI 是一个强大且模块化的图形界面（GUI）、API 和后端系统，专为稳定扩散模型设计，支持用户通过流程图（Nodes/Flowchart）界面构建和执行复杂的稳定扩散工作流，无需编写代码。它支持多种稳定扩散版本，包括SD1.x、SD2.x等，并提供了异步队列系统、优化的记忆管理、GPU兼容性以及CPU模式运行等功能。ComfyUI 可以加载ckpt、safetensors、diffusers模型和独立VAE、CLIP模型，并支持嵌入式文本反转等高级功能。此外，用户可以使用ComfyUI加载和保存完整的工作流程，创建自定义节点进行高级操作，如图像生成或修复等。系统还提供了快捷键支持，增强用户体验。用户可以通过命令行安装ComfyUI，并根据自己的硬件环境（如AMD、NVIDIA GPU或Intel GPU）进行相应的配置。对于Mac用户，提供了一个特定的指南来安装和运行ComfyUI。在使用过程中，用户需要注意模型路径设置、依赖安装和一些特定硬件的兼容性问题。为了提高预览质量，用户可以启用自动预览方法或手动安装额外的模型。同时，ComfyUI支持TLS/SSL加密以增强安全性。最后，前端的更新和维护被转移到了单独的仓库中，以便于开发者管理和用户报告前端相关的问题。 <div>
<p>The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface.</p><hr /><div align="center"> 
 <h1>ComfyUI</h1> 
 <p><strong>The most powerful and modular stable diffusion GUI and backend.</strong></p> 
 <p><a href="https://www.comfy.org/"><img alt="Website" src="https://img.shields.io/badge/ComfyOrg-4285F4?style=flat" /></a> <a href="https://www.comfy.org/discord"><img alt="Dynamic JSON Badge" src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2Fcomfyorg%3Fwith_counts%3Dtrue&amp;query=%24.approximate_member_count&amp;logo=discord&amp;logoColor=white&amp;label=Discord&amp;color=green&amp;suffix=%20total" /></a> <a href="https://app.element.io/#/room/%23comfyui_space%3Amatrix.org"><img alt="Matrix" src="https://img.shields.io/badge/Matrix-000000?style=flat&amp;logo=matrix&amp;logoColor=white" /></a> <br /> <a href="https://github.com/comfyanonymous/ComfyUI/releases"><img alt="" src="https://img.shields.io/github/v/release/comfyanonymous/ComfyUI?style=flat&amp;sort=semver" /></a> <a href="https://github.com/comfyanonymous/ComfyUI/releases"><img alt="" src="https://img.shields.io/github/release-date/comfyanonymous/ComfyUI?style=flat" /></a> <a href="https://github.com/comfyanonymous/ComfyUI/releases"><img alt="" src="https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/total?style=flat" /></a> <a href="https://github.com/comfyanonymous/ComfyUI/releases"><img alt="" src="https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/latest/total?style=flat&amp;label=downloads%40latest" /></a></p> 
 <!-- Workaround to display total user from https://github.com/badges/shields/issues/4500#issuecomment-2060079995 --> 
 <p><img alt="ComfyUI Screenshot" src="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/comfyui_screenshot.png" /></p> 
</div> 
<p>This ui will let you design and execute advanced stable diffusion pipelines using a graph/nodes/flowchart based interface. For some workflow examples and see what ComfyUI can do you can check out:</p> 
<h3><a href="https://comfyanonymous.github.io/ComfyUI_examples/">ComfyUI Examples</a></h3> 
<h3><a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#installing">Installing ComfyUI</a></h3> 
<h2>Features</h2> 
<ul> 
 <li>Nodes/graph/flowchart interface to experiment and create complex Stable Diffusion workflows without needing to code anything.</li> 
 <li>Fully supports SD1.x, SD2.x, <a href="https://comfyanonymous.github.io/ComfyUI_examples/sdxl/">SDXL</a>, <a href="https://comfyanonymous.github.io/ComfyUI_examples/video/">Stable Video Diffusion</a>, <a href="https://comfyanonymous.github.io/ComfyUI_examples/stable_cascade/">Stable Cascade</a>, <a href="https://comfyanonymous.github.io/ComfyUI_examples/sd3/">SD3</a> and <a href="https://comfyanonymous.github.io/ComfyUI_examples/audio/">Stable Audio</a></li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/flux/">Flux</a></li> 
 <li>Asynchronous Queue system</li> 
 <li>Many optimizations: Only re-executes the parts of the workflow that changes between executions.</li> 
 <li>Smart memory management: can automatically run models on GPUs with as low as 1GB vram.</li> 
 <li>Works even if you don't have a GPU with: <code>--cpu</code> (slow)</li> 
 <li>Can load ckpt, safetensors and diffusers models/checkpoints. Standalone VAEs and CLIP models.</li> 
 <li>Embeddings/Textual inversion</li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/lora/">Loras (regular, locon and loha)</a></li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/hypernetworks/">Hypernetworks</a></li> 
 <li>Loading full workflows (with seeds) from generated PNG, WebP and FLAC files.</li> 
 <li>Saving/Loading workflows as Json files.</li> 
 <li>Nodes interface can be used to create complex workflows like one for <a href="https://comfyanonymous.github.io/ComfyUI_examples/2_pass_txt2img/">Hires fix</a> or much more advanced ones.</li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/area_composition/">Area Composition</a></li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/inpaint/">Inpainting</a> with both regular and inpainting models.</li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/controlnet/">ControlNet and T2I-Adapter</a></li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/upscale_models/">Upscale Models (ESRGAN, ESRGAN variants, SwinIR, Swin2SR, etc...)</a></li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/unclip/">unCLIP Models</a></li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/gligen/">GLIGEN</a></li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/model_merging/">Model Merging</a></li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/lcm/">LCM models and Loras</a></li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/sdturbo/">SDXL Turbo</a></li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/aura_flow/">AuraFlow</a></li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_dit/">HunyuanDiT</a></li> 
 <li>Latent previews with <a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#how-to-show-high-quality-previews">TAESD</a></li> 
 <li>Starts up very fast.</li> 
 <li>Works fully offline: will never download anything.</li> 
 <li><a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/extra_model_paths.yaml.example">Config file</a> to set the search paths for models.</li> 
</ul> 
<p>Workflow examples can be found on the <a href="https://comfyanonymous.github.io/ComfyUI_examples/">Examples page</a></p> 
<h2>Shortcuts</h2> 
<table> 
 <thead> 
  <tr> 
   <th>Keybind</th> 
   <th>Explanation</th> 
  </tr> 
 </thead> 
 <tbody> 
  <tr> 
   <td>Ctrl + Enter</td> 
   <td>Queue up current graph for generation</td> 
  </tr> 
  <tr> 
   <td>Ctrl + Shift + Enter</td> 
   <td>Queue up current graph as first for generation</td> 
  </tr> 
  <tr> 
   <td>Ctrl + Alt + Enter</td> 
   <td>Cancel current generation</td> 
  </tr> 
  <tr> 
   <td>Ctrl + Z/Ctrl + Y</td> 
   <td>Undo/Redo</td> 
  </tr> 
  <tr> 
   <td>Ctrl + S</td> 
   <td>Save workflow</td> 
  </tr> 
  <tr> 
   <td>Ctrl + O</td> 
   <td>Load workflow</td> 
  </tr> 
  <tr> 
   <td>Ctrl + A</td> 
   <td>Select all nodes</td> 
  </tr> 
  <tr> 
   <td>Alt + C</td> 
   <td>Collapse/uncollapse selected nodes</td> 
  </tr> 
  <tr> 
   <td>Ctrl + M</td> 
   <td>Mute/unmute selected nodes</td> 
  </tr> 
  <tr> 
   <td>Ctrl + B</td> 
   <td>Bypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)</td> 
  </tr> 
  <tr> 
   <td>Delete/Backspace</td> 
   <td>Delete selected nodes</td> 
  </tr> 
  <tr> 
   <td>Ctrl + Backspace</td> 
   <td>Delete the current graph</td> 
  </tr> 
  <tr> 
   <td>Space</td> 
   <td>Move the canvas around when held and moving the cursor</td> 
  </tr> 
  <tr> 
   <td>Ctrl/Shift + Click</td> 
   <td>Add clicked node to selection</td> 
  </tr> 
  <tr> 
   <td>Ctrl + C/Ctrl + V</td> 
   <td>Copy and paste selected nodes (without maintaining connections to outputs of unselected nodes)</td> 
  </tr> 
  <tr> 
   <td>Ctrl + C/Ctrl + Shift + V</td> 
   <td>Copy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes)</td> 
  </tr> 
  <tr> 
   <td>Shift + Drag</td> 
   <td>Move multiple selected nodes at the same time</td> 
  </tr> 
  <tr> 
   <td>Ctrl + D</td> 
   <td>Load default graph</td> 
  </tr> 
  <tr> 
   <td>Alt + <code>+</code></td> 
   <td>Canvas Zoom in</td> 
  </tr> 
  <tr> 
   <td>Alt + <code>-</code></td> 
   <td>Canvas Zoom out</td> 
  </tr> 
  <tr> 
   <td>Ctrl + Shift + LMB + Vertical drag</td> 
   <td>Canvas Zoom in/out</td> 
  </tr> 
  <tr> 
   <td>Q</td> 
   <td>Toggle visibility of the queue</td> 
  </tr> 
  <tr> 
   <td>H</td> 
   <td>Toggle visibility of history</td> 
  </tr> 
  <tr> 
   <td>R</td> 
   <td>Refresh graph</td> 
  </tr> 
  <tr> 
   <td>Double-Click LMB</td> 
   <td>Open node quick search palette</td> 
  </tr> 
  <tr> 
   <td>Shift + Drag</td> 
   <td>Move multiple wires at once</td> 
  </tr> 
  <tr> 
   <td>Ctrl + Alt + LMB</td> 
   <td>Disconnect all wires from clicked slot</td> 
  </tr> 
 </tbody> 
</table> 
<p>Ctrl can also be replaced with Cmd instead for macOS users</p> 
<h1>Installing</h1> 
<h2>Windows</h2> 
<p>There is a portable standalone build for Windows that should work for running on Nvidia GPUs or for running on your CPU only on the <a href="https://github.com/comfyanonymous/ComfyUI/releases">releases page</a>.</p> 
<h3><a href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z">Direct link to download</a></h3> 
<p>Simply download, extract with <a href="https://7-zip.org">7-Zip</a> and run. Make sure you put your Stable Diffusion checkpoints/models (the huge ckpt/safetensors files) in: ComfyUI\models\checkpoints</p> 
<p>If you have trouble extracting it, right click the file -&gt; properties -&gt; unblock</p> 
<h4>How do I share models between another UI and ComfyUI?</h4> 
<p>See the <a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/extra_model_paths.yaml.example">Config file</a> to set the search paths for models. In the standalone windows build you can find this file in the ComfyUI directory. Rename this file to extra_model_paths.yaml and edit it with your favorite text editor.</p> 
<h2>Jupyter Notebook</h2> 
<p>To run it on services like paperspace, kaggle or colab you can use my <a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/notebooks/comfyui_colab.ipynb">Jupyter Notebook</a></p> 
<h2>Manual Install (Windows, Linux)</h2> 
<p>Git clone this repo.</p> 
<p>Put your SD checkpoints (the huge ckpt/safetensors files) in: models/checkpoints</p> 
<p>Put your VAE in: models/vae</p> 
<h3>AMD GPUs (Linux only)</h3> 
<p>AMD users can install rocm and pytorch with pip if you don't have it already installed, this is the command to install the stable version:</p> 
<p><code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0</code></p> 
<p>This is the command to install the nightly with ROCm 6.0 which might have some performance improvements:</p> 
<p><code>pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.1</code></p> 
<h3>NVIDIA</h3> 
<p>Nvidia users should install stable pytorch using this command:</p> 
<p><code>pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121</code></p> 
<p>This is the command to install pytorch nightly instead which might have performance improvements:</p> 
<p><code>pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu124</code></p> 
<h4>Troubleshooting</h4> 
<p>If you get the "Torch not compiled with CUDA enabled" error, uninstall torch with:</p> 
<p><code>pip uninstall torch</code></p> 
<p>And install it again with the command above.</p> 
<h3>Dependencies</h3> 
<p>Install the dependencies by opening your terminal inside the ComfyUI folder and:</p> 
<p><code>pip install -r requirements.txt</code></p> 
<p>After this you should have everything installed and can proceed to running ComfyUI.</p> 
<h3>Others:</h3> 
<h4>Intel GPUs</h4> 
<p>Intel GPU support is available for all Intel GPUs supported by Intel's Extension for Pytorch (IPEX) with the support requirements listed in the <a href="https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=gpu">Installation</a> page. Choose your platform and method of install and follow the instructions. The steps are as follows:</p> 
<ol> 
 <li>Start by installing the drivers or kernel listed or newer in the Installation page of IPEX linked above for Windows and Linux if needed.</li> 
 <li>Follow the instructions to install <a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit-download.html">Intel's oneAPI Basekit</a> for your platform.</li> 
 <li>Install the packages for IPEX using the instructions provided in the Installation page for your platform.</li> 
 <li>Follow the <a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux">ComfyUI manual installation</a> instructions for Windows and Linux and run ComfyUI normally as described above after everything is installed.</li> 
</ol> 
<p>Additional discussion and help can be found <a href="https://github.com/comfyanonymous/ComfyUI/discussions/476">here</a>.</p> 
<h4>Apple Mac silicon</h4> 
<p>You can install ComfyUI in Apple Mac silicon (M1 or M2) with any recent macOS version.</p> 
<ol> 
 <li>Install pytorch nightly. For instructions, read the <a href="https://developer.apple.com/metal/pytorch/">Accelerated PyTorch training on Mac</a> Apple Developer guide (make sure to install the latest pytorch nightly).</li> 
 <li>Follow the <a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux">ComfyUI manual installation</a> instructions for Windows and Linux.</li> 
 <li>Install the ComfyUI <a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#dependencies">dependencies</a>. If you have another Stable Diffusion UI <a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#i-already-have-another-ui-for-stable-diffusion-installed-do-i-really-have-to-install-all-of-these-dependencies">you might be able to reuse the dependencies</a>.</li> 
 <li>Launch ComfyUI by running <code>python main.py</code></li> 
</ol> 
<blockquote> 
 <p><strong>Note</strong>: Remember to add your models, VAE, LoRAs etc. to the corresponding Comfy folders, as discussed in <a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux">ComfyUI manual installation</a>.</p> 
</blockquote> 
<h4>DirectML (AMD Cards on Windows)</h4> 
<p><code>pip install torch-directml</code> Then you can launch ComfyUI with: <code>python main.py --directml</code></p> 
<h1>Running</h1> 
<p><code>python main.py</code></p> 
<h3>For AMD cards not officially supported by ROCm</h3> 
<p>Try running it with this command if you have issues:</p> 
<p>For 6700, 6600 and maybe other RDNA2 or older: <code>HSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py</code></p> 
<p>For AMD 7600 and maybe other RDNA3 cards: <code>HSA_OVERRIDE_GFX_VERSION=11.0.0 python main.py</code></p> 
<h1>Notes</h1> 
<p>Only parts of the graph that have an output with all the correct inputs will be executed.</p> 
<p>Only parts of the graph that change from each execution to the next will be executed, if you submit the same graph twice only the first will be executed. If you change the last part of the graph only the part you changed and the part that depends on it will be executed.</p> 
<p>Dragging a generated png on the webpage or loading one will give you the full workflow including seeds that were used to create it.</p> 
<p>You can use () to change emphasis of a word or phrase like: (good code:1.2) or (bad code:0.8). The default emphasis for () is 1.1. To use () characters in your actual prompt escape them like \( or \).</p> 
<p>You can use {day|night}, for wildcard/dynamic prompts. With this syntax "{wild|card|test}" will be randomly replaced by either "wild", "card" or "test" by the frontend every time you queue the prompt. To use {} characters in your actual prompt escape them like: \{ or \}.</p> 
<p>Dynamic prompts also support C-style comments, like <code>// comment</code> or <code>/* comment */</code>.</p> 
<p>To use a textual inversion concepts/embeddings in a text prompt put them in the models/embeddings directory and use them in the CLIPTextEncode node like this (you can omit the .pt extension):</p> 
<p><code>embedding:embedding_filename.pt</code></p> 
<h2>How to show high-quality previews?</h2> 
<p>Use <code>--preview-method auto</code> to enable previews.</p> 
<p>The default installation includes a fast latent preview method that's low-resolution. To enable higher-quality previews with <a href="https://github.com/madebyollin/taesd">TAESD</a>, download the <a href="https://github.com/madebyollin/taesd/raw/main/taesd_decoder.pth">taesd_decoder.pth</a> (for SD1.x and SD2.x) and <a href="https://github.com/madebyollin/taesd/raw/main/taesdxl_decoder.pth">taesdxl_decoder.pth</a> (for SDXL) models and place them in the <code>models/vae_approx</code> folder. Once they're installed, restart ComfyUI to enable high-quality previews.</p> 
<h2>How to use TLS/SSL?</h2> 
<p>Generate a self-signed certificate (not appropriate for shared/production use) and key by running the command: <code>openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -sha256 -days 3650 -nodes -subj "/C=XX/ST=StateName/L=CityName/O=CompanyName/OU=CompanySectionName/CN=CommonNameOrHostname"</code></p> 
<p>Use <code>--tls-keyfile key.pem --tls-certfile cert.pem</code> to enable TLS/SSL, the app will now be accessible with <code>https://...</code> instead of <code>http://...</code>.</p> 
<blockquote> 
 <p>Note: Windows users can use <a href="https://github.com/alexisrolland/docker-openssl">alexisrolland/docker-openssl</a> or one of the <a href="https://wiki.openssl.org/index.php/Binaries">3rd party binary distributions</a> to run the command example above. <br /><br />If you use a container, note that the volume mount <code>-v</code> can be a relative path so <code>... -v ".\:/openssl-certs" ...</code> would create the key &amp; cert files in the current directory of your command prompt or powershell terminal.</p> 
</blockquote> 
<h2>Support and dev channel</h2> 
<p><a href="https://app.element.io/#/room/%23comfyui_space%3Amatrix.org">Matrix space: #comfyui_space:matrix.org</a> (it's like discord but open source).</p> 
<p>See also: <a href="https://www.comfy.org/">https://www.comfy.org/</a></p> 
<h2>Frontend Development</h2> 
<p>As of August 15, 2024, we have transitioned to a new frontend, which is now hosted in a separate repository: <a href="https://github.com/Comfy-Org/ComfyUI_frontend">ComfyUI Frontend</a>. This repository now hosts the compiled JS (from TS/Vue) under the <code>web/</code> directory.</p> 
<h3>Reporting Issues and Requesting Features</h3> 
<p>For any bugs, issues, or feature requests related to the frontend, please use the <a href="https://github.com/Comfy-Org/ComfyUI_frontend">ComfyUI Frontend repository</a>. This will help us manage and address frontend-specific concerns more efficiently.</p> 
<h3>Using the Latest Frontend</h3> 
<p>The new frontend is now the default for ComfyUI. However, please note:</p> 
<ol> 
 <li>The frontend in the main ComfyUI repository is updated weekly.</li> 
 <li>Daily releases are available in the separate frontend repository.</li> 
</ol> 
<p>To use the most up-to-date frontend version:</p> 
<ol> 
 <li> <p>For the latest daily release, launch ComfyUI with this command line argument:</p> <pre><code>--front-end-version Comfy-Org/ComfyUI_frontend@latest
</code></pre> </li> 
 <li> <p>For a specific version, replace <code>latest</code> with the desired version number:</p> <pre><code>--front-end-version Comfy-Org/ComfyUI_frontend@1.2.2
</code></pre> </li> 
</ol> 
<p>This approach allows you to easily switch between the stable weekly release and the cutting-edge daily updates, or even specific versions for testing purposes.</p> 
<h3>Accessing the Legacy Frontend</h3> 
<p>If you need to use the legacy frontend for any reason, you can access it using the following command line argument:</p> 
<pre><code>--front-end-version Comfy-Org/ComfyUI_legacy_frontend@latest
</code></pre> 
<p>This will use a snapshot of the legacy frontend preserved in the <a href="https://github.com/Comfy-Org/ComfyUI_legacy_frontend">ComfyUI Legacy Frontend repository</a>.</p> 
<h1>QA</h1> 
<h3>Which GPU should I buy for this?</h3> 
<p><a href="https://github.com/comfyanonymous/ComfyUI/wiki/Which-GPU-should-I-buy-for-ComfyUI">See this page for some recommendations</a></p>
]]></content:encoded>


</item>
<item>
<title>ostris/ai-toolkit</title>
<link>https://github.com/ostris/ai-toolkit</link>
<guid>https://github.com/ostris/ai-toolkit</guid>
<content:encoded><![CDATA[
<div> 关键词：AI Toolkit、Ostris、Stable Diffusion、LoRA、FLUX.1

文章总结：

1. **AI Toolkit**：这是一个由Ostris创建的研究仓库，专注于实验各种AI模型，尤其是Stable Diffusion技术。它允许用户训练和自定义多种模型。

2. **Ostris与团队支持**：Ostris强调了他/她的工作离不开团队的支持，特别是Glif、和所有人。他/她鼓励支持团队的工作，具体方式是支持Glif。

3. **FLUX.1 Training**：FLUX.1是一个需要GPU资源进行训练的模型。对于特定配置的GPU（至少24GB VRAM），用户可以训练此模型。训练过程中需要一些技巧来适应有限的VRAM使用。

4. **许可证问题**：训练FLUX.1模型前，用户需接受许可证协议。这涉及到登录Hugging Face平台并接受访问权限。此外，还提供了一个名为“FLUX.1-schnell”的版本，其许可协议更为宽松，允许用户根据自己的需求自由使用和发布训练结果。

5. **运行和部署**：文章介绍了如何在本地或云环境中运行AI Toolkit模型。包括如何在Linux或Windows上安装环境，以及如何在RunPod云平台上设置和运行模型进行训练。

简而言之，AI Toolkit是一个用于研究和实验各种AI模型的平台，特别是Stable Diffusion相关技术。它提供了从本地到云的模型训练和部署解决方案，同时也强调了许可证管理的重要性。通过遵循文档中的指导，用户可以利用这个工具集进行创新性的AI研究和应用开发。 <div>
<p>Various AI scripts. Mostly Stable Diffusion stuff.</p><hr /><h1>AI Toolkit by Ostris</h1> 
<h2>IMPORTANT NOTE - READ THIS</h2> 
<p>This is my research repo. I do a lot of experiments in it and it is possible that I will break things. If something breaks, checkout an earlier commit. This repo can train a lot of things, and it is hard to keep up with all of them.</p> 
<h2>Support my work</h2> 
<a href="https://glif.app" target="_blank"> <img alt="glif.app" height="auto" src="https://raw.githubusercontent.com/ostris/ai-toolkit/main/assets/glif.svg?v=1" width="256" /> </a> 
<p>My work on this project would not be possible without the amazing support of <a href="https://glif.app/">Glif</a> and everyone on the team. If you want to support me, support Glif. <a href="https://glif.app/">Join the site</a>, <a href="https://discord.com/invite/nuR9zZ2nsh">Join us on Discord</a>, <a href="https://x.com/heyglif">follow us on Twitter</a> and come make some cool stuff with us</p> 
<h2>Installation</h2> 
<p>Requirements:</p> 
<ul> 
 <li>python &gt;3.10</li> 
 <li>Nvidia GPU with enough ram to do what you need</li> 
 <li>python venv</li> 
 <li>git</li> 
</ul> 
<p>Linux:</p> 
<pre><code class="language-bash">git clone https://github.com/ostris/ai-toolkit.git
cd ai-toolkit
git submodule update --init --recursive
python3 -m venv venv
source venv/bin/activate
# .\venv\Scripts\activate on windows
# install torch first
pip3 install torch
pip3 install -r requirements.txt
</code></pre> 
<p>Windows:</p> 
<pre><code class="language-bash">git clone https://github.com/ostris/ai-toolkit.git
cd ai-toolkit
git submodule update --init --recursive
python -m venv venv
.\venv\Scripts\activate
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
pip install -r requirements.txt
</code></pre> 
<h2>FLUX.1 Training</h2> 
<h3>WIP. I am updating docs and optimizing as fast as I can. If there are bugs open a ticket. Not knowing how to get it to work is NOT a bug. Be paitient as I continue to develop it.</h3> 
<h3>Requirements</h3> 
<p>You currently need a GPU with <strong>at least 24GB of VRAM</strong> to train FLUX.1. If you are using it as your GPU to control your monitors, you probably need to set the flag <code>low_vram: true</code> in the config file under <code>model:</code>. This will quantize the model on CPU and should allow it to train with monitors attached. Users have gotten it to work on Windows with WSL, but there are some reports of a bug when running on windows natively. I have only tested on linux for now. This is still extremely experimental and a lot of quantizing and tricks had to happen to get it to fit on 24GB at all.</p> 
<h3>FLUX.1-dev</h3> 
<p>FLUX.1-dev has a non-commercial license. Which means anything you train will inherit the non-commercial license. It is also a gated model, so you need to accept the license on HF before using it. Otherwise, this will fail. Here are the required steps to setup a license.</p> 
<ol> 
 <li>Sign into HF and accept the model access here <a href="https://huggingface.co/black-forest-labs/FLUX.1-dev">black-forest-labs/FLUX.1-dev</a></li> 
 <li>Make a file named <code>.env</code> in the root on this folder</li> 
 <li><a href="https://huggingface.co/settings/tokens/new?">Get a READ key from huggingface</a> and add it to the <code>.env</code> file like so <code>HF_TOKEN=your_key_here</code></li> 
</ol> 
<h3>FLUX.1-schnell</h3> 
<p>FLUX.1-schnell is Apache 2.0. Anything trained on it can be licensed however you want and it does not require a HF_TOKEN to train. However, it does require a special adapter to train with it, <a href="https://huggingface.co/ostris/FLUX.1-schnell-training-adapter">ostris/FLUX.1-schnell-training-adapter</a>. It is also highly experimental. For best overall quality, training on FLUX.1-dev is recommended.</p> 
<p>To use it, You just need to add the assistant to the <code>model</code> section of your config file like so:</p> 
<pre><code class="language-yaml">      model:
        name_or_path: "black-forest-labs/FLUX.1-schnell"
        assistant_lora_path: "ostris/FLUX.1-schnell-training-adapter"
        is_flux: true
        quantize: true
</code></pre> 
<p>You also need to adjust your sample steps since schnell does not require as many</p> 
<pre><code class="language-yaml">      sample:
        guidance_scale: 1  # schnell does not do guidance
        sample_steps: 4  # 1 - 4 works well
</code></pre> 
<h3>Training</h3> 
<ol> 
 <li>Copy the example config file located at <code>config/examples/train_lora_flux_24gb.yaml</code> (<code>config/examples/train_lora_flux_schnell_24gb.yaml</code> for schnell) to the <code>config</code> folder and rename it to <code>whatever_you_want.yml</code></li> 
 <li>Edit the file following the comments in the file</li> 
 <li>Run the file like so <code>python run.py config/whatever_you_want.yml</code></li> 
</ol> 
<p>A folder with the name and the training folder from the config file will be created when you start. It will have all checkpoints and images in it. You can stop the training at any time using ctrl+c and when you resume, it will pick back up from the last checkpoint.</p> 
<p>IMPORTANT. If you press crtl+c while it is saving, it will likely corrupt that checkpoint. So wait until it is done saving</p> 
<h3>Need help?</h3> 
<p>Please do not open a bug report unless it is a bug in the code. You are welcome to <a href="https://discord.gg/SzVB3wYvxF">Join my Discord</a> and ask for help there. However, please refrain from PMing me directly with general question or support. Ask in the discord and I will answer when I can.</p> 
<h3>Training in RunPod cloud</h3> 
<p>Example RunPod template: <strong>runpod/pytorch:2.2.0-py3.10-cuda12.1.1-devel-ubuntu22.04</strong></p> 
<blockquote> 
 <p>You need a minimum of 24GB VRAM, pick a GPU by your preference.</p> 
</blockquote> 
<h4>Example config ($0.5/hr):</h4> 
<ul> 
 <li>1x A40 (48 GB VRAM)</li> 
 <li>19 vCPU 100 GB RAM</li> 
</ul> 
<h4>Custom overrides (you need some storage to clone FLUX.1, store datasets, store trained models and samples):</h4> 
<ul> 
 <li>~120 GB Disk</li> 
 <li>~120 GB Pod Volume</li> 
 <li>Start Jupyter Notebook</li> 
</ul> 
<h3>1. Setup</h3> 
<pre><code>git clone https://github.com/ostris/ai-toolkit.git
cd ai-toolkit
git submodule update --init --recursive
python -m venv venv
source venv/bin/activate
pip install torch
pip install -r requirements.txt
pip install --upgrade accelerate transformers diffusers huggingface_hub #Optional, run it if you run into issues
</code></pre> 
<h3>2. Upload your dataset</h3> 
<ul> 
 <li>Create a new folder in the root, name it <code>dataset</code> or whatever you like</li> 
 <li>Drag and drop your .jpg and .txt files inside the newly created dataset folder</li> 
</ul> 
<h3>3. Login into Hugging Face with an Access Token</h3> 
<ul> 
 <li>Get a READ token from <a href="https://huggingface.co/settings/tokens">here</a></li> 
 <li>Run <code>huggingface-cli login</code> and paste your token</li> 
</ul> 
<h3>4. Training</h3> 
<ul> 
 <li>Copy an example config file located at <code>config/examples</code> to the config folder and rename it to <code>whatever_you_want.yml</code></li> 
 <li>Edit the config following the comments in the file</li> 
 <li>Change <code>folder_path: "/path/to/images/folder"</code> to your dataset path like <code>folder_path: "/workspace/ai-toolkit/your-dataset"</code></li> 
 <li>Run the file: <code>python run.py config/whatever_you_want.yml</code></li> 
</ul> 
<h3>Screenshot from RunPod</h3> 
<img alt="RunPod Training Screenshot" src="https://github.com/user-attachments/assets/53a1b8ef-92fa-4481-81a7-bde45a14a7b5" width="1728" /> 
<!--
### Training in the cloud
Coming very soon. Getting base out then will have a notebook that makes all that work. 
--> 
<hr /> 
<h2>Dataset Preparation</h2> 
<p>Datasets generally need to be a folder containing images and associated text files. Currently, the only supported formats are jpg, jpeg, and png. Webp currently has issues. The text files should be named the same as the images but with a <code>.txt</code> extension. For example <code>image2.jpg</code> and <code>image2.txt</code>. The text file should contain only the caption. You can add the word <code>[trigger]</code> in the caption file and if you have <code>trigger_word</code> in your config, it will be automatically replaced.</p> 
<p>Images are never upscaled but they are downscaled and placed in buckets for batching. <strong>You do not need to crop/resize your images</strong>. The loader will automatically resize them and can handle varying aspect ratios.</p> 
<hr /> 
<h2>EVERYTHING BELOW THIS LINE IS OUTDATED</h2> 
<p>It may still work like that, but I have not tested it in a while.</p> 
<hr /> 
<h3>Batch Image Generation</h3> 
<p>A image generator that can take frompts from a config file or form a txt file and generate them to a folder. I mainly needed this for an SDXL test I am doing but added some polish to it so it can be used for generat batch image generation. It all runs off a config file, which you can find an example of in <code>config/examples/generate.example.yaml</code>. Mere info is in the comments in the example</p> 
<hr /> 
<h3>LoRA (lierla), LoCON (LyCORIS) extractor</h3> 
<p>It is based on the extractor in the <a href="https://github.com/KohakuBlueleaf/LyCORIS">LyCORIS</a> tool, but adding some QOL features and LoRA (lierla) support. It can do multiple types of extractions in one run. It all runs off a config file, which you can find an example of in <code>config/examples/extract.example.yml</code>. Just copy that file, into the <code>config</code> folder, and rename it to <code>whatever_you_want.yml</code>. Then you can edit the file to your liking. and call it like so:</p> 
<pre><code class="language-bash">python3 run.py config/whatever_you_want.yml
</code></pre> 
<p>You can also put a full path to a config file, if you want to keep it somewhere else.</p> 
<pre><code class="language-bash">python3 run.py "/home/user/whatever_you_want.yml"
</code></pre> 
<p>More notes on how it works are available in the example config file itself. LoRA and LoCON both support extractions of 'fixed', 'threshold', 'ratio', 'quantile'. I'll update what these do and mean later. Most people used fixed, which is traditional fixed dimension extraction.</p> 
<p><code>process</code> is an array of different processes to run. You can add a few and mix and match. One LoRA, one LyCON, etc.</p> 
<hr /> 
<h3>LoRA Rescale</h3> 
<p>Change <code>&lt;lora:my_lora:4.6&gt;</code> to <code>&lt;lora:my_lora:1.0&gt;</code> or whatever you want with the same effect. A tool for rescaling a LoRA's weights. Should would with LoCON as well, but I have not tested it. It all runs off a config file, which you can find an example of in <code>config/examples/mod_lora_scale.yml</code>. Just copy that file, into the <code>config</code> folder, and rename it to <code>whatever_you_want.yml</code>. Then you can edit the file to your liking. and call it like so:</p> 
<pre><code class="language-bash">python3 run.py config/whatever_you_want.yml
</code></pre> 
<p>You can also put a full path to a config file, if you want to keep it somewhere else.</p> 
<pre><code class="language-bash">python3 run.py "/home/user/whatever_you_want.yml"
</code></pre> 
<p>More notes on how it works are available in the example config file itself. This is useful when making all LoRAs, as the ideal weight is rarely 1.0, but now you can fix that. For sliders, they can have weird scales form -2 to 2 or even -15 to 15. This will allow you to dile it in so they all have your desired scale</p> 
<hr /> 
<h3>LoRA Slider Trainer</h3> 
<a href="https://colab.research.google.com/github/ostris/ai-toolkit/blob/main/notebooks/SliderTraining.ipynb" target="_blank"> <img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" /> </a> 
<p>This is how I train most of the recent sliders I have on Civitai, you can check them out in my <a href="https://civitai.com/user/Ostris/models">Civitai profile</a>. It is based off the work by <a href="https://github.com/p1atdev/LECO">p1atdev/LECO</a> and <a href="https://github.com/rohitgandikota/erasing">rohitgandikota/erasing</a> But has been heavily modified to create sliders rather than erasing concepts. I have a lot more plans on this, but it is very functional as is. It is also very easy to use. Just copy the example config file in <code>config/examples/train_slider.example.yml</code> to the <code>config</code> folder and rename it to <code>whatever_you_want.yml</code>. Then you can edit the file to your liking. and call it like so:</p> 
<pre><code class="language-bash">python3 run.py config/whatever_you_want.yml
</code></pre> 
<p>There is a lot more information in that example file. You can even run the example as is without any modifications to see how it works. It will create a slider that turns all animals into dogs(neg) or cats(pos). Just run it like so:</p> 
<pre><code class="language-bash">python3 run.py config/examples/train_slider.example.yml
</code></pre> 
<p>And you will be able to see how it works without configuring anything. No datasets are required for this method. I will post an better tutorial soon.</p> 
<hr /> 
<h2>Extensions!!</h2> 
<p>You can now make and share custom extensions. That run within this framework and have all the inbuilt tools available to them. I will probably use this as the primary development method going forward so I dont keep adding and adding more and more features to this base repo. I will likely migrate a lot of the existing functionality as well to make everything modular. There is an example extension in the <code>extensions</code> folder that shows how to make a model merger extension. All of the code is heavily documented which is hopefully enough to get you started. To make an extension, just copy that example and replace all the things you need to.</p> 
<h3>Model Merger - Example Extension</h3> 
<p>It is located in the <code>extensions</code> folder. It is a fully finctional model merger that can merge as many models together as you want. It is a good example of how to make an extension, but is also a pretty useful feature as well since most mergers can only do one model at a time and this one will take as many as you want to feed it. There is an example config file in there, just copy that to your <code>config</code> folder and rename it to <code>whatever_you_want.yml</code>. and use it like any other config file.</p> 
<h2>WIP Tools</h2> 
<h3>VAE (Variational Auto Encoder) Trainer</h3> 
<p>This works, but is not ready for others to use and therefore does not have an example config. I am still working on it. I will update this when it is ready. I am adding a lot of features for criteria that I have used in my image enlargement work. A Critic (discriminator), content loss, style loss, and a few more. If you don't know, the VAE for stable diffusion (yes even the MSE one, and SDXL), are horrible at smaller faces and it holds SD back. I will fix this. I'll post more about this later with better examples later, but here is a quick test of a run through with various VAEs. Just went in and out. It is much worse on smaller faces than shown here.</p> 
<img height="auto" src="https://raw.githubusercontent.com/ostris/ai-toolkit/main/assets/VAE_test1.jpg" width="768" /> 
<hr /> 
<h2>TODO</h2> 
<ul> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> Add proper regs on sliders</li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> Add SDXL support (base model only for now)</li> 
 <li><input disabled="disabled" type="checkbox" /> Add plain erasing</li> 
 <li><input disabled="disabled" type="checkbox" /> Make Textual inversion network trainer (network that spits out TI embeddings)</li> 
</ul> 
<hr /> 
<h2>Change Log</h2> 
<h4>2023-08-05</h4> 
<ul> 
 <li>Huge memory rework and slider rework. Slider training is better thant ever with no more ram spikes. I also made it so all 4 parts of the slider algorythm run in one batch so they share gradient accumulation. This makes it much faster and more stable.</li> 
 <li>Updated the example config to be something more practical and more updated to current methods. It is now a detail slide and shows how to train one without a subject. 512x512 slider training for 1.5 should work on 6GB gpu now. Will test soon to verify.</li> 
</ul> 
<h4>2021-10-20</h4> 
<ul> 
 <li>Windows support bug fixes</li> 
 <li>Extensions! Added functionality to make and share custom extensions for training, merging, whatever. check out the example in the <code>extensions</code> folder. Read more about that above.</li> 
 <li>Model Merging, provided via the example extension.</li> 
</ul> 
<h4>2023-08-03</h4> 
<p>Another big refactor to make SD more modular.</p> 
<p>Made batch image generation script</p> 
<h4>2023-08-01</h4> 
<p>Major changes and update. New LoRA rescale tool, look above for details. Added better metadata so Automatic1111 knows what the base model is. Added some experiments and a ton of updates. This thing is still unstable at the moment, so hopefully there are not breaking changes.</p> 
<p>Unfortunately, I am too lazy to write a proper changelog with all the changes.</p> 
<p>I added SDXL training to sliders... but.. it does not work properly. The slider training relies on a model's ability to understand that an unconditional (negative prompt) means you do not want that concept in the output. SDXL does not understand this for whatever reason, which makes separating out concepts within the model hard. I am sure the community will find a way to fix this over time, but for now, it is not going to work properly. And if any of you are thinking "Could we maybe fix it by adding 1 or 2 more text encoders to the model as well as a few more entirely separate diffusion networks?" No. God no. It just needs a little training without every experimental new paper added to it. The KISS principal.</p> 
<h4>2023-07-30</h4> 
<p>Added "anchors" to the slider trainer. This allows you to set a prompt that will be used as a regularizer. You can set the network multiplier to force spread consistency at high weights</p>
]]></content:encoded>


</item>
<item>
<title>versotile-org/verso</title>
<link>https://github.com/versotile-org/verso</link>
<guid>https://github.com/versotile-org/verso</guid>
<content:encoded><![CDATA[
<div> 关键词：Verso、Servo、Web浏览器、多窗口支持、多进程模式

总结:
Verso是一个基于Servo引擎开发的Web浏览器项目，旨在探索Servo的多视图和多窗口功能，最终目标是构建一个成熟的功能完备的浏览器。当前版本仍处于开发阶段，主要关注于用户界面的自定义构建以及问题修复的PR接受。为了获取最佳体验，推荐使用Git、Python、LLVM、CMake等工具进行本地编译和运行。

对于不同的操作系统，Verso提供了不同的安装方式：
- 在Windows系统上，首先需要通过Scoop或Chocolatey安装Git、Python等依赖项，然后通过`cargo run`命令启动浏览器。
- MacOS用户则需要安装Homebrew并执行类似的命令来完成安装和启动过程。
- Linux用户可以借助Flatpak进行统一的环境设置与包管理，通过特定命令生成manifest文件并构建Verso应用。

Verso还支持在Nix环境中使用NixShell进行本地构建，尽管当前并未直接在Nix仓库中打包该应用。对于需要构建但不希望使用任何沙箱环境的用户，可以参考文档中的指导进行操作，但需注意在遇到构建问题时可能无法得到支持。

未来计划包括实现多窗口支持、启用多进程模式、增强安全性的沙箱功能，以及集成Gstreamer以提供更丰富的多媒体播放能力。目前的Nightly版本可以通过特定链接访问，但请注意，这些版本未经过签名，对于MacOS用户在安装后可能需要手动解除应用的安全限制（通过命令`xattr -d com.apple.quarantine /Applications/verso.app`）。

通过持续的开发和社区贡献，Verso有望在未来成为一款功能强大、用户体验优秀的Web浏览器。 <div>
<p>A web browser that plays old world blues to build new world hope</p><hr /><h1>Verso</h1> 
<p><a href="https://versotile.zulipchat.com/"><img alt="project chat" src="https://img.shields.io/badge/zulip-57a7ff?style=for-the-badge&amp;labelColor=555555&amp;logo=zulip" /></a></p> 
<p>A web browser that plays old world blues to build new world hope.</p> 
<p><img alt="verso" src="https://github.com/user-attachments/assets/48a834af-858e-4f93-969f-07fb8f5f2496" /></p> 
<p>Verso is a web browser built on top of the <a href="https://servo.org/">Servo</a> web engine. We aim to explore embedding solutions for Servo while growing it into a mature browser one day. This means we want to experiment with multi-view and multi-window first and then build UI elements entirely from Servo itself. At the moment, <a href="https://servo.org/download/">Servoshell</a> should provide a better user experience.</p> 
<p>Verso is still under development. We don't accept feature requests at the moment, and the whole navigation workflow hasn't been polished yet, either. But if you are interested, feel free to open bug-fix PRs.</p> 
<h1>Usage</h1> 
<h2>Getting Started</h2> 
<h3>Windows</h3> 
<ul> 
 <li>Install <a href="https://scoop.sh/">scoop</a> and then install other tools:</li> 
</ul> 
<pre><code class="language-sh">scoop install git python llvm cmake curl
pip install mako
</code></pre> 
<blockquote> 
 <p>You can also use chocolatey to install if you prefer it.</p> 
</blockquote> 
<ul> 
 <li>Build &amp; run:</li> 
</ul> 
<pre><code class="language-sh">cargo run
</code></pre> 
<h3>MacOS</h3> 
<ul> 
 <li>Install <a href="https://developer.apple.com/xcode/">Xcode</a></li> 
 <li>Install <a href="https://brew.sh/">Homebrew</a> and then install other tools:</li> 
</ul> 
<pre><code class="language-sh">brew install cmake pkg-config harfbuzz
pip install mako
</code></pre> 
<ul> 
 <li>Build &amp; run:</li> 
</ul> 
<pre><code class="language-sh">cargo run
</code></pre> 
<h3>Linux</h3> 
<h4>Flatpak</h4> 
<p>For unified environment setup and package experience, we choose Flatpak to build the project from the start. Please follow the <a href="https://flatpak.org/setup/">Flatpak Setup</a> page to install Flatpak based on your distribution.</p> 
<ul> 
 <li>Generate manifests and build: // TODO Exporting to a repository instead</li> 
</ul> 
<pre><code class="language-sh">python3 ./flatpak-cargo-generator.py ./Cargo.lock -o cargo-sources.json
flatpak-builder --user --install --install-deps-from=flathub --force-clean target org.versotile.verso.yml
flatpak run org.versotile.verso
</code></pre> 
<h4>Nix</h4> 
<p>We also support building Verso in nix shell. But we don't bundle it in nix at the moment.</p> 
<ul> 
 <li>For NixOS:</li> 
</ul> 
<pre><code class="language-sh">nix-shell shell.nix --run 'cargo r'
</code></pre> 
<ul> 
 <li>For non-NixOS distributions:</li> 
</ul> 
<pre><code class="language-sh">nix-shell shell.nix --run 'nixGL cargo r'
</code></pre> 
<p>If you prefer to build the project without any sandbox, please follow the instructions in <a href="https://book.servo.org/hacking/setting-up-your-environment.html#tools-for-linux">Servo book</a> to bootstrap. But please understand we don't triage any build issue without flatpak or nix setup.</p> 
<h2>Nightly Release</h2> 
<p>Nightly releases built with CrabNebula Cloud can be found at <a href="https://web.crabnebula.cloud/verso/verso-nightly/releases">releases</a>.</p> 
<blockquote> 
 <p>Packages are unsigned currently. If you have problem opening the app on macOS, try <code>xattr -d com.apple.quarantine /Applications/verso.app</code> after installation.</p> 
</blockquote> 
<h2>Future Work</h2> 
<ul> 
 <li>Multi-window support.</li> 
 <li>Enable multiprocess mode.</li> 
 <li>Enable sandbox in all platforms.</li> 
 <li>Enable <code>Gstreamer</code> feature.</li> 
</ul>
]]></content:encoded>


</item>
<item>
<title>chen08209/FlClash</title>
<link>https://github.com/chen08209/FlClash</link>
<guid>https://github.com/chen08209/FlClash</guid>
<content:encoded><![CDATA[
<div> 关键词：FlClash、ClashMeta、开源、无广告、多平台

总结：

FlClash是一款基于ClashMeta开发的跨平台代理客户端，其设计简洁易用，且完全开源，不含有任何广告。这款软件支持Android、Windows、macOS和Linux等多个操作系统，能够适应不同屏幕尺寸，提供多种主题色供用户选择，界面设计符合Material You风格，既美观又实用。它还具有通过WebDAV进行数据同步的功能，同时支持订阅链接和深色模式。

为了构建和部署FlClash，开发者需要遵循一定的步骤。对于桌面用户，他们需要安装相应的环境（如Android SDK、NDK或GCC），然后运行特定的构建脚本。对于Windows和Linux用户，需要额外安装一些辅助工具（如Inno Setup），而MacOS用户则直接运行构建脚本即可。

最后，鼓励用户通过点击页面顶部的星标按钮来支持开发者的工作，这是最简单有效的方式之一。 <div>
<p>A multi-platform proxy client based on ClashMeta,simple and easy to use, open-source and ad-free.</p><hr /><div> 
 <p><a href="https://raw.githubusercontent.com/chen08209/FlClash/main/README_zh_CN.md"><strong>简体中文</strong></a></p> 
</div> 
<h2>FlClash</h2> 
<p style="text-align: left;"> <img alt="stars" src="https://img.shields.io/github/stars/chen08209/FlClash?style=flat-square&amp;logo=github" /> <img alt="downloads" src="https://img.shields.io/github/downloads/chen08209/FlClash/total" /> <a href="https://raw.githubusercontent.com/chen08209/FlClash/main/LICENSE"> <img alt="license" src="https://img.shields.io/github/license/chen08209/FlClash" /> </a> </p> 
<p>A multi-platform proxy client based on ClashMeta, simple and easy to use, open-source and ad-free.</p> 
<p>on Desktop:</p> 
<p style="text-align: center;"> <img alt="desktop" src="https://raw.githubusercontent.com/chen08209/FlClash/main/snapshots/desktop.gif" /> </p> 
<p>on Mobile:</p> 
<p style="text-align: center;"> <img alt="mobile" src="https://raw.githubusercontent.com/chen08209/FlClash/main/snapshots/mobile.gif" /> </p> 
<h2>Features</h2> 
<p>✈️ Multi-platform: Android, Windows, macOS and Linux</p> 
<p>💻 Adaptive multiple screen sizes, Multiple color themes available</p> 
<p>💡 Based on Material You Design, <a href="https://github.com/getsurfboard/surfboard">Surfboard</a>-like UI</p> 
<p>☁️ Supports data sync via WebDAV</p> 
<p>✨ Support subscription link, Dark mode</p> 
<h2>Download</h2> 
<p><a href="https://chen08209.github.io/FlClash-fdroid-repo/repo?fingerprint=789D6D32668712EF7672F9E58DEEB15FBD6DCEEC5AE7A4371EA72F2AAE8A12FD"><img alt="Get it on F-Droid" src="https://raw.githubusercontent.com/chen08209/FlClash/main/snapshots/get-it-on-fdroid.svg?sanitize=true" width="200px" /></a> <a href="https://github.com/chen08209/FlClash/releases"><img alt="Get it on GitHub" src="https://raw.githubusercontent.com/chen08209/FlClash/main/snapshots/get-it-on-github.svg?sanitize=true" width="200px" /></a></p> 
<h2>Contact</h2> 
<p><a href="https://t.me/+G-veVtwBOl4wODc1">Telegram</a></p> 
<h2>Build</h2> 
<ol> 
 <li> <p>Update submodules</p> <pre><code class="language-bash">git submodule update --init --recursive
</code></pre> </li> 
 <li> <p>Install <code>Flutter</code> and <code>Golang</code> environment</p> </li> 
 <li> <p>Build Application</p> 
  <ul> 
   <li> <p>android</p> 
    <ol> 
     <li> <p>Install <code>Android SDK</code> , <code>Android NDK</code></p> </li> 
     <li> <p>Set <code>ANDROID_NDK</code> environment variables</p> </li> 
     <li> <p>Run Build script</p> <pre><code class="language-bash">dart .\setup.dart android
</code></pre> </li> 
    </ol> </li> 
   <li> <p>windows</p> 
    <ol> 
     <li> <p>You need a windows client</p> </li> 
     <li> <p>Install <code>Gcc</code>，<code>Inno Setup</code></p> </li> 
     <li> <p>Run build script</p> <pre><code class="language-bash">dart .\setup.dart	
</code></pre> </li> 
    </ol> </li> 
   <li> <p>linux</p> 
    <ol> 
     <li> <p>You need a linux client</p> </li> 
     <li> <p>Run build script</p> <pre><code class="language-bash">dart .\setup.dart	
</code></pre> </li> 
    </ol> </li> 
   <li> <p>macOS</p> 
    <ol> 
     <li> <p>You need a macOS client</p> </li> 
     <li> <p>Run build script</p> <pre><code class="language-bash">dart .\setup.dart	
</code></pre> </li> 
    </ol> </li> 
  </ul> </li> 
</ol> 
<h2>Star</h2> 
<p>The easiest way to support developers is to click on the star (⭐) at the top of the page.</p> 
<p style="text-align: center;"> <a href="https://api.star-history.com/svg?repos=chen08209/FlClash&amp;Date"> <img alt="start" src="https://api.star-history.com/svg?repos=chen08209/FlClash&amp;Date" width="50%" /> </a> </p>
]]></content:encoded>


</item>
<item>
<title>n8n-io/n8n</title>
<link>https://github.com/n8n-io/n8n</link>
<guid>https://github.com/n8n-io/n8n</guid>
<content:encoded><![CDATA[
<div> 关键词：n8n、自动化工具、源代码、可扩展性、集成

总结：

n8n是一款开源的自动化工作流工具，具有高度可扩展性和自定义功能。它采用节点式架构，使得用户能够轻松连接各种服务和应用，实现跨平台的任务自动化。n8n提供了超过200种不同的节点，覆盖了广泛的自动化需求。官方文档提供了详细的使用指南、示例工作流以及版本更新信息。用户可以通过npm命令行工具或在线云服务进行快速启动和操作。此外，n8n还支持与LangChain集成，引入AI功能到工作流中，增强自动化能力。对于遇到问题的用户，n8n社区提供论坛支持，帮助解决技术难题。最后，n8n遵循Apache 2.0许可协议发布，同时为商业客户提供企业级许可选项，确保了其灵活性和适应性。 <div>
<p>Free and source-available fair-code licensed workflow automation tool. Easily automate tasks across different services.</p><hr /><p><img alt="n8n.io - Workflow Automation" src="https://user-images.githubusercontent.com/65276001/173571060-9f2f6d7b-bac0-43b6-bdb2-001da9694058.png" /></p> 
<h1>n8n - Workflow automation tool</h1> 
<p>n8n is an extendable workflow automation tool. With a <a href="https://faircode.io">fair-code</a> distribution model, n8n will always have visible source code, be available to self-host, and allow you to add your own custom functions, logic and apps. n8n's node-based approach makes it highly versatile, enabling you to connect anything to everything.</p> 
<p><img alt="n8n.io - Screenshot" src="https://raw.githubusercontent.com/n8n-io/n8n/master/assets/n8n-screenshot.png" /></p> 
<h2>Demo</h2> 
<p><a href="https://www.youtube.com/watch?v=1MwSoB0gnM4"><span>📺</span> A short video (&lt; 5 min)</a> that goes over key concepts of creating workflows in n8n.</p> 
<h2>Available integrations</h2> 
<p>n8n has 200+ different nodes to automate workflows. The list can be found on: <a href="https://n8n.io/integrations">https://n8n.io/integrations</a></p> 
<h2>Documentation</h2> 
<p>The official n8n documentation can be found on our <a href="https://docs.n8n.io">documentation website</a></p> 
<p>Additional information and example workflows on the <a href="https://n8n.io">n8n.io website</a></p> 
<p>The release notes can be found <a href="https://docs.n8n.io/release-notes/">here</a> and the list of breaking changes <a href="https://github.com/n8n-io/n8n/raw/master/packages/cli/BREAKING-CHANGES.md">here</a>.</p> 
<h2>Usage</h2> 
<ul> 
 <li><span>📚</span> Learn <a href="https://docs.n8n.io/reference/cli-commands/">how to <strong>use</strong> it from the command line</a></li> 
 <li><span>🐳</span> Learn <a href="https://docs.n8n.io/hosting/installation/docker/">how to run n8n in <strong>Docker</strong></a></li> 
</ul> 
<h2>Start</h2> 
<p>You can try n8n without installing it using npx. You must have <a href="https://nodejs.org/en/">Node.js</a> installed. From the terminal, run:</p> 
<p><code>npx n8n</code></p> 
<p>This command will download everything that is needed to start n8n. You can then access n8n and start building workflows by opening <a href="http://localhost:5678">http://localhost:5678</a>.</p> 
<h2>n8n cloud</h2> 
<p>Sign-up for an <a href="https://www.n8n.io/cloud/">n8n cloud</a> account.</p> 
<p>While n8n cloud and n8n are the same in terms of features, n8n cloud provides certain conveniences such as:</p> 
<ul> 
 <li>Not having to set up and maintain your n8n instance</li> 
 <li>Managed OAuth for authentication</li> 
 <li>Easily upgrading to the newer n8n versions</li> 
</ul> 
<h2>Build with LangChain and AI in n8n (beta)</h2> 
<p>With n8n's LangChain nodes you can build AI-powered functionality within your workflows. The LangChain nodes are configurable, meaning you can choose your preferred agent, LLM, memory, and so on. Alongside the LangChain nodes, you can connect any n8n node as normal: this means you can integrate your LangChain logic with other data sources and services.</p> 
<p>Learn more in the <a href="https://docs.n8n.io/langchain/">documentation</a>.</p> 
<ul> 
 <li><a href="https://www.npmjs.com/package/@n8n/n8n-nodes-langchain">LangChain nodes package</a></li> 
 <li><a href="https://www.npmjs.com/package/@n8n/chat">Chatbot package</a></li> 
</ul> 
<h2>Support</h2> 
<p>If you have problems or questions go to our forum, we will then try to help you asap:</p> 
<p><a href="https://community.n8n.io">https://community.n8n.io</a></p> 
<h2>Jobs</h2> 
<p>If you are interested in working for n8n and so shape the future of the project check out our <a href="https://apply.workable.com/n8n/">job posts</a></p> 
<h2>What does n8n mean and how do you pronounce it?</h2> 
<p><strong>Short answer:</strong> It means "nodemation" and it is pronounced as n-eight-n.</p> 
<p><strong>Long answer:</strong> "I get that question quite often (more often than I expected) so I decided it is probably best to answer it here. While looking for a good name for the project with a free domain I realized very quickly that all the good ones I could think of were already taken. So, in the end, I chose nodemation. 'node-' in the sense that it uses a Node-View and that it uses Node.js and '-mation' for 'automation' which is what the project is supposed to help with. However, I did not like how long the name was and I could not imagine writing something that long every time in the CLI. That is when I then ended up on 'n8n'." - <strong>Jan Oberhauser, Founder and CEO, n8n.io</strong></p> 
<h2>Development setup</h2> 
<p>Have you found a bug <span>🐛</span> ? Or maybe you have a nice feature <span>✨</span> to contribute ? The <a href="https://github.com/n8n-io/n8n/raw/master/CONTRIBUTING.md">CONTRIBUTING guide</a> will help you get your development environment ready in minutes.</p> 
<h2>License</h2> 
<p>n8n is <a href="https://faircode.io">fair-code</a> distributed under the <a href="https://github.com/n8n-io/n8n/raw/master/LICENSE.md"><strong>Sustainable Use License</strong></a> and the <a href="https://github.com/n8n-io/n8n/raw/master/LICENSE_EE.md"><strong>n8n Enterprise License</strong></a>.</p> 
<p>Proprietary licenses are available for enterprise customers. <a href="mailto:license@n8n.io">Get in touch</a></p> 
<p>Additional information about the license model can be found in the <a href="https://docs.n8n.io/reference/license/">docs</a>.</p>
]]></content:encoded>


</item>
<item>
<title>electric-sql/pglite</title>
<link>https://github.com/electric-sql/pglite</link>
<guid>https://github.com/electric-sql/pglite</guid>
<content:encoded><![CDATA[
<div> 关键词：PGlite、Postgres、WASM、TypeScript、浏览器/Node.js/Bun/Deno

总结:

PGlite 是一个轻量级的 PostgreSQL 实现，它将数据库引擎打包为 WebAssembly (WASM) 格式，并提供了 TypeScript 客户端库。这种设计允许开发者在浏览器、Node.js、Bun 或 Deno 环境中直接运行 PostgreSQL，无需安装额外依赖。PGlite 的体积仅为 3MB（压缩后），并且支持多种 PostgreSQL 扩展，包括全文搜索等。

在浏览器环境中，开发者可以通过常规的包管理器或 CDN 来引入和使用 PGlite。对于内存数据库场景，开发者可以直接创建新的 PGlite 实例；若需持久化存储，可以将数据库与文件系统（Node/Bun）或 IndexedDB（浏览器）进行集成。

在 Node.js 或 Bun 环境下，PGlite 可以通过 npm 进行安装，并根据具体需求配置内存数据库或持久化数据到本地文件或 IndexedDB 中。

PGlite 的工作原理基于 PostgreSQL 的单用户模式，该模式主要用于命令行操作和启动恢复流程。通过构建一个输入/输出通道，PGlite 能够在 JavaScript 环境中与被编译为 WASM 的 PostgreSQL 进行交互。

当前版本的限制在于它仅支持单用户/单连接模式。对于贡献者而言，需要确保已安装 Emscripten 和最新版本的 Node.js，并且下载最新的 WASM 构建文件以构建 PGlite 及其依赖的工作空间项目。此外，PGlite 遵循双许可协议，开发者可以根据需要选择 PostgreSQL 许可证或自由软件许可证。

最后，PGlite 建立在前人的工作之上，特别感谢为该项目做出贡献的人们。 <div>
<p>Lightweight Postgres packaged as WASM into a TypeScript library for the browser, Node.js, Bun and Deno from https://electric-sql.com</p><hr /><p align="center"> <a href="https://electric-sql.com" target="_blank"> 
   
   <source media="(prefers-color-scheme: dark)" /> 
   <source media="(prefers-color-scheme: light)" /> 
   <img alt="ElectricSQL logo" src="https://raw.githubusercontent.com/electric-sql/meta/main/identity/ElectricSQL-logo-black.svg?sanitize=true" /> 
   </a> </p> 
<p align="center"> PGlite - the WASM build of Postgres from <a href="https://electric-sql.com" target="_blank">ElectricSQL</a>.<br /> Build reactive, realtime, local-first apps directly on Postgres. </p>
<p> </p>
<p align="center"> <a href="https://github.com/electric-sql/pglite/stargazers/"><img src="https://img.shields.io/github/stars/electric-sql/pglite?style=social&amp;label=Star" /></a> 
 <!-- <a href="https://github.com/electric-sql/pglite/actions"><img src="https://github.com/electric-sql/pglite/workflows/CI/badge.svg" alt="CI"></a> --> <a href="https://github.com/electric-sql/pglite/raw/main/LICENSE"><img alt="License - Apache 2.0" src="https://img.shields.io/badge/license-Apache_2.0-green" /></a> <a href="https://raw.githubusercontent.com/electric-sql/pglite/main/#roadmap"><img alt="Status - Alpha" src="https://img.shields.io/badge/status-alpha-orange" /></a> <a href="https://discord.electric-sql.com"><img alt="Chat - Discord" src="https://img.shields.io/discord/933657521581858818?color=5969EA&amp;label=discord" /></a> <a href="https://twitter.com/ElectricSQL" target="_blank"><img src="https://img.shields.io/twitter/follow/nestframework.svg?style=social&amp;label=Follow%20@ElectricSQL" /></a> <a href="https://fosstodon.org/@electric" target="_blank"><img src="https://img.shields.io/mastodon/follow/109599644322136925.svg?domain=https%3A%2F%2Ffosstodon.org" /></a> </p> 
<h1>PGlite - Postgres in WASM</h1> 
<p><img alt="PGlite" src="https://raw.githubusercontent.com/electric-sql/pglite/main/screenshot.png" /></p> 
<p>PGlite is a WASM Postgres build packaged into a TypeScript client library that enables you to run Postgres in the browser, Node.js and Bun, with no need to install any other dependencies. It is only 3mb gzipped and has support for many Postgres extensions, including <a href="https://github.com/pgvector/pgvector">pgvector</a>.</p> 
<pre><code class="language-javascript">import { PGlite } from "@electric-sql/pglite";

const db = new PGlite();
await db.query("select 'Hello world' as message;");
// -&gt; { rows: [ { message: "Hello world" } ] }
</code></pre> 
<p>It can be used as an ephemeral in-memory database, or with persistence either to the file system (Node/Bun) or indexedDB (Browser).</p> 
<p>Unlike previous "Postgres in the browser" projects, PGlite does not use a Linux virtual machine - it is simply Postgres in WASM.</p> 
<p>For full documentation and user guides see <a href="https://pglite.dev">pglite.dev</a>.</p> 
<h2>Browser</h2> 
<p>It can be installed and imported using your usual package manager:</p> 
<pre><code class="language-js">import { PGlite } from "@electric-sql/pglite";
</code></pre> 
<p>or using a CDN such as JSDeliver:</p> 
<pre><code class="language-js">import { PGlite } from "https://cdn.jsdelivr.net/npm/@electric-sql/pglite/dist/index.js";
</code></pre> 
<p>Then for an in-memory Postgres:</p> 
<pre><code class="language-js">const db = new PGlite()
await db.query("select 'Hello world' as message;")
// -&gt; { rows: [ { message: "Hello world" } ] }
</code></pre> 
<p>or to persist the database to indexedDB:</p> 
<pre><code class="language-js">const db = new PGlite("idb://my-pgdata");
</code></pre> 
<h2>Node/Bun</h2> 
<p>Install into your project:</p> 
<pre><code class="language-bash">npm install @electric-sql/pglite
</code></pre> 
<p>To use the in-memory Postgres:</p> 
<pre><code class="language-javascript">import { PGlite } from "@electric-sql/pglite";

const db = new PGlite();
await db.query("select 'Hello world' as message;");
// -&gt; { rows: [ { message: "Hello world" } ] }
</code></pre> 
<p>or to persist to the filesystem:</p> 
<pre><code class="language-javascript">const db = new PGlite("./path/to/pgdata");
</code></pre> 
<h2>How it works</h2> 
<p>PostgreSQL typically operates using a process forking model; whenever a client initiates a connection, a new process is forked to manage that connection. However, programs compiled with Emscripten - a C to WebAssembly (WASM) compiler - cannot fork new processes, and operates strictly in a single-process mode. As a result, PostgreSQL cannot be directly compiled to WASM for conventional operation.</p> 
<p>Fortunately, PostgreSQL includes a "single user mode" primarily intended for command-line usage during bootstrapping and recovery procedures. Building upon this capability, PGlite introduces a input/output pathway that facilitates interaction with PostgreSQL when it is compiled to WASM within a JavaScript environment.</p> 
<h2>Limitations</h2> 
<ul> 
 <li>PGlite is single user/connection.</li> 
</ul> 
<h2>How to contribute</h2> 
<p>You will need <a href="https://pnpm.io/">pnpm</a> installed, and a recent version of Node.js (v20 and above).</p> 
<p>You will also need the Postgres WASM build files, which you download from a comment under the most recently merged PR, labeled as <em>interim build files</em>, and place them under <code>packages/pglite/release</code>. These are necessary to build PGlite and the dependent workspace projects. We plan to enable a local build in the future to streamline this step.</p> 
<p>Once the requirements are met, you can install dependencies and build the workspace projects:</p> 
<pre><code class="language-bash">pnpm install
pnpm build
</code></pre> 
<p>This will build all packages in the correct order based on their dependency relationships. You can now develop any individual package using the <code>build</code> and <code>test</code> scripts, as well as the <code>stylecheck</code> and <code>typecheck</code> scripts to ensure style and type validity.</p> 
<p>When ready to open a PR, run the following command at the root of the repository:</p> 
<pre><code class="language-bash">pnpm changeset
</code></pre> 
<p>And follow the instructions to create an appropriate changeset. Please ensure any contributions that touch code are accompanied by a changeset.</p> 
<h2>Acknowledgments</h2> 
<p>PGlite builds on the work of <a href="https://github.com/kelvich">Stas Kelvich</a> of <a href="https://neon.tech">Neon</a> in this <a href="https://github.com/electric-sql/postgres-wasm">Postgres fork</a>.</p> 
<h2>License</h2> 
<p>PGlite is dual-licensed under the terms of the&nbsp;<a href="https://github.com/electric-sql/pglite/raw/main/LICENSE">Apache License 2.0</a>&nbsp;and the&nbsp;<a href="https://github.com/electric-sql/pglite/raw/main/POSTGRES-LICENSE">PostgreSQL License</a>, you can choose which you prefer.</p> 
<p>Changes to the&nbsp;<a href="https://github.com/electric-sql/postgres-wasm">Postgres source</a>&nbsp;are licensed under the PostgreSQL License.</p>
]]></content:encoded>


</item>
<item>
<title>poloclub/transformer-explainer</title>
<link>https://github.com/poloclub/transformer-explainer</link>
<guid>https://github.com/poloclub/transformer-explainer</guid>
<content:encoded><![CDATA[
<div> 关键词：Transformer Explainer、GPT、Transformer模型、交互可视化、在线实验

总结:
Transformer Explainer是一个交互式可视化工具，旨在帮助用户理解Transformer模型（如GPT）的工作原理。通过在线运行GPT-2模型，用户可以实时输入文本并观察内部组件和操作如何协同工作以预测下一个词汇。该工具提供了直观的学习体验，使任何人都能探索和学习文本生成模型的工作机制。要本地运行此工具，需要安装Node.js 20或更高版本以及NPM。运行命令后，您可以通过浏览器访问特定网址来使用它。

Transformer Explainer由来自乔治亚理工学院的研究人员Aeree Cho、Grace C. Kim、Alexander Karpekov、Alec Helbling、Zijie J. Wang、Seongmin Lee、Benjamin Hoover和Duen Horng Chau共同开发。该工具遵循开源许可证，用户可以通过提供的问题反馈或直接联系开发者进行交流。 <div>
<p>Transformer Explained: Learn How LLM Transformer Models Work with Interactive Visualization</p><hr /><h1>Transformer Explainer: Interactive Learning of Text-Generative Models</h1> 
<p>Transformer Explainer is an interactive visualization tool designed to help anyone learn how Transformer-based models like GPT work. It runs a live GPT-2 model right in your browser, allowing you to experiment with your own text and observe in real time how internal components and operations of the Transformer work together to predict the next tokens. Try Transformer Explainer at <a href="http://poloclub.github.io/transformer-explainer">http://poloclub.github.io/transformer-explainer</a> and watch a demo video on YouTube <a href="https://youtu.be/ECR4oAwocjs">https://youtu.be/ECR4oAwocjs</a> .<br /><br /> <a href="http://opensource.org/licenses/MIT"><img alt="MIT license" src="http://img.shields.io/badge/license-MIT-brightgreen.svg?sanitize=true" /></a> <a href="https://arxiv.org/abs/2408.04619"><img alt="arxiv badge" src="https://img.shields.io/badge/arXiv-2408.04619-red" /></a></p> 
<table> 
 <tbody>
  <tr> 
   <td colspan="2">
    <video src="https://github.com/poloclub/transformer-explainer/assets/5067740/5c2d6a9d-2cbf-4b01-9ce1-bdf8e190dc42" width="100%"></video></td> 
  </tr> 
  <tr> 
   <td>🚀 <a href="http://poloclub.github.io/transformer-explainer">Live Demo</a></td> 
   <td>📺 <a href="https://youtu.be/ECR4oAwocjs">Demo Video</a></td> 
  </tr> 
 </tbody>
</table> 
<h3>Research Paper</h3> 
<p><a href="https://arxiv.org/abs/2408.04619"><strong>Transformer Explainer: Interactive Learning of Text-Generative Models</strong></a>. Aeree Cho, Grace C. Kim, Alexander Karpekov, Alec Helbling, Zijie J. Wang, Seongmin Lee, Benjamin Hoover, Duen Horng Chau. <em>Poster, IEEE VIS 2024.</em></p> 
<h2>How to run locally</h2> 
<h4>Prerequisites</h4> 
<ul> 
 <li>Node.js 20 or higher</li> 
 <li>NPM</li> 
</ul> 
<h4>Steps</h4> 
<pre><code class="language-bash">git clone https://github.com/poloclub/transformer-explainer.git
cd transformer-explainer
npm install
npm run dev
</code></pre> 
<p>Then, on your web browser, access <a href="http://localhost:5173">http://localhost:5173</a>.</p> 
<h2>Credits</h2> 
<p>Transformer Explainer was created by <a href="https://aereeeee.github.io/" target="_blank">Aeree Cho</a>, <a href="https://www.linkedin.com/in/chaeyeonggracekim/" target="_blank">Grace C. Kim</a>, <a href="https://alexkarpekov.com/" target="_blank">Alexander Karpekov</a>, <a href="https://alechelbling.com/" target="_blank">Alec Helbling</a>, <a href="https://zijie.wang/" target="_blank">Jay Wang</a>, <a href="https://seongmin.xyz/" target="_blank">Seongmin Lee</a>, <a href="https://bhoov.com/" target="_blank">Benjamin Hoover</a>, and <a href="https://poloclub.github.io/polochau/" target="_blank">Polo Chau</a> at the Georgia Institute of Technology.</p> 
<h2>Citation</h2> 
<pre><code class="language-bibTeX">@article{cho2024transformer,
  title = {Transformer Explainer: Interactive Learning of Text-Generative Models},
  shorttitle = {Transformer Explainer},
  author = {Cho, Aeree and Kim, Grace C. and Karpekov, Alexander and Helbling, Alec and Wang, Zijie J. and Lee, Seongmin and Hoover, Benjamin and Chau, Duen Horng},
  journal={IEEE VIS},
  year={2024}
}
</code></pre> 
<h2>License</h2> 
<p>The software is available under the <a href="https://github.com/poloclub/transformer-explainer/raw/main/LICENSE">MIT License</a>.</p> 
<h2>Contact</h2> 
<p>If you have any questions, feel free to <a href="https://github.com/poloclub/transformer-explainer/issues/new/choose">open an issue</a> or contact <a href="https://aereeeee.github.io/">Aeree Cho</a> or any of the contributors listed above.</p>
]]></content:encoded>


</item>
<item>
<title>erincatto/box2d</title>
<link>https://github.com/erincatto/box2d</link>
<guid>https://github.com/erincatto/box2d</guid>
<content:encoded><![CDATA[
<div> 关键词：Box2D、2D 物理引擎、游戏、构建状态、兼容性

总结:

Box2D 是一款专门用于游戏开发的二维物理引擎。它提供了丰富的功能和特性，包括但不限于碰撞检测、物理模拟、系统设计等。以下是对其主要特点的总结：

1. **碰撞检测**：Box2D 支持多种形状的碰撞检测，如凸多边形、胶囊体、圆形、圆角多边形、线段和链等。同时，它还支持传感器、接触事件以及多形状的单个物体。

2. **物理模拟**：引擎采用稳健的软步解算器进行快速的平移和旋转模拟，实现连续物理。此外，它还包括岛基睡眠、关节（如轮轴、滑动、距离、鼠标等）以及各种类型的关节，如铰链、电机、弹簧和摩擦力。

3. **系统设计**：Box2D 的设计注重数据导向，使用标准可移植的 C17 编写，支持多线程和 SIMD 指令集，提高了性能和效率。

4. **构建与兼容性**：Box2D 可以在 Windows、Linux 和 macOS 上构建和运行，要求使用最新版本的 Clang 或 GCC 进行编译。对于特定平台如 Windows，需要安装 Visual Studio。而对于 Mac 和 Linux，则可以通过 CMake 进行构建。

5. **社区与贡献**：Box2D 有一个活跃的社区，鼓励用户通过问题报告或 Discord 服务器寻求帮助和支持。开发者可通过赞助来支持 Box2D 的发展，同时也提供了一些官方和第三方的版本、封装和绑定资源。 <div>
<p>Box2D is a 2D physics engine for games</p><hr /><p><img alt="Box2D Logo" src="https://box2d.org/images/logo.svg?sanitize=true" /></p> 
<h1>Build Status</h1> 
<p><a href="https://github.com/erincatto/box2d/actions"><img alt="Build Status" src="https://github.com/erincatto/box2d/actions/workflows/build.yml/badge.svg?sanitize=true" /></a></p> 
<h1>Box2D</h1> 
<p>Box2D is a 2D physics engine for games.</p> 
<p><a href="https://www.youtube.com/watch?v=dAoM-xjOWtA"><img alt="Box2D Version 3.0 Release Demo" src="https://img.youtube.com/vi/dAoM-xjOWtA/0.jpg" /></a></p> 
<h2>Features</h2> 
<h3>Collision</h3> 
<ul> 
 <li>Continuous collision detection</li> 
 <li>Contact events and sensors</li> 
 <li>Convex polygons, capsules, circles, rounded polygons, segments, and chains</li> 
 <li>Multiple shapes per body</li> 
 <li>Collision filtering</li> 
 <li>Ray casts, shape casts, and overlap queries</li> 
</ul> 
<h3>Physics</h3> 
<ul> 
 <li>Robust <em>Soft Step</em> rigid body solver</li> 
 <li>Continuous physics for fast translations and rotations</li> 
 <li>Island based sleep</li> 
 <li>Revolute, prismatic, distance, mouse joint, weld, and wheel joints</li> 
 <li>Joint limits, motors, springs, and friction</li> 
 <li>Joint and contact forces</li> 
 <li>Body movement events and sleep notification</li> 
</ul> 
<h3>System</h3> 
<ul> 
 <li>Data-oriented design</li> 
 <li>Written in portable C17</li> 
 <li>Extensive multithreading and SIMD</li> 
</ul> 
<h3>Samples</h3> 
<ul> 
 <li>OpenGL with GLFW and enkiTS</li> 
 <li>Graphical user interface with imgui</li> 
 <li>Many samples to demonstrate features and performance</li> 
</ul> 
<h2>Building</h2> 
<ul> 
 <li>Install <a href="https://cmake.org/">CMake</a></li> 
 <li>Ensure CMake is in the user <code>PATH</code></li> 
 <li>Visual Studio: run <code>build.bat</code> from the command prompt</li> 
 <li>Otherwise: run <code>build.sh</code> from a bash shell</li> 
 <li>Results are in the build sub-folder</li> 
 <li>On Windows you can open box2d.sln</li> 
</ul> 
<h2>Building for Xcode</h2> 
<ul> 
 <li>Install <a href="https://cmake.org">CMake</a></li> 
 <li>Add Cmake to the path in .zprofile (the default Terminal shell is zsh) 
  <ul> 
   <li>export PATH="/Applications/CMake.app/Contents/bin:$PATH"</li> 
  </ul> </li> 
 <li>mkdir build</li> 
 <li>cd build</li> 
 <li>cmake -G Xcode ..</li> 
 <li>open box2d.xcodeproj</li> 
 <li>Select the samples scheme</li> 
 <li>Edit the scheme to set a custom working directory to the box2d directory</li> 
 <li>You can now build and run the samples</li> 
</ul> 
<h2>Compatibility</h2> 
<p>The Box2D library and samples build and run on Windows, Linux, and Mac.</p> 
<p>Box2D should be built on recent versions of clang and gcc. You will need the latest Visual Studio version for C11 atomics to compile (17.8.3+).</p> 
<p>AVX2 CPU support is assumed on x64. You can turn this off in the CMake options and use SSE2 instead. There are some compatibility issues with very old CPUs.</p> 
<h2>Documentation</h2> 
<ul> 
 <li><a href="https://box2d.org/documentation/">Manual</a></li> 
 <li><a href="https://github.com/erincatto/box2d/raw/main/docs/migration.md">Migration Guide</a></li> 
</ul> 
<h2>Community</h2> 
<ul> 
 <li><a href="https://discord.gg/NKYgCBP">Discord</a></li> 
</ul> 
<h2>Contributing</h2> 
<p>Please do not submit pull requests. Instead, please file an issue for bugs or feature requests. For support, please visit the Discord server.</p> 
<h1>Giving Feedback</h1> 
<p>Please file an issue or start a chat on discord.</p> 
<h2>License</h2> 
<p>Box2D is developed by Erin Catto and uses the <a href="https://en.wikipedia.org/wiki/MIT_License">MIT license</a>.</p> 
<h2>Sponsorship</h2> 
<p>Support development of Box2D through <a href="https://github.com/sponsors/erincatto">Github Sponsors</a></p> 
<h2>Ports, wrappers, and bindings</h2> 
<ul> 
 <li><a href="https://github.com/EnokViking/Box2DBeef">https://github.com/EnokViking/Box2DBeef</a></li> 
 <li><a href="https://github.com/HolyBlackCat/box2cpp">https://github.com/HolyBlackCat/box2cpp</a></li> 
</ul>
]]></content:encoded>


</item>
<item>
<title>LLaVA-VL/LLaVA-NeXT</title>
<link>https://github.com/LLaVA-VL/LLaVA-NeXT</link>
<guid>https://github.com/LLaVA-VL/LLaVA-NeXT</guid>
<content:encoded><![CDATA[
<div> 关键词：LLaVA-NeXT、大型多模态模型、多图像、视频、3D任务

总结:

LLaVA-NeXT系列模型是为解决多图像、视频和三维任务而设计的一系列大型多模态模型。自发布以来，LLaVA-NeXT不断更新与优化，实现了在多个基准测试中的新状态最优性能。以下为关键更新点：

1. **多模态能力增强**：LLaVA-NeXT模型通过集成图像、文本和视频理解能力，显著提高了处理多模态数据的任务性能。

2. **性能提升**：新发布的模型如LLaVA-NeXT-Interleave和LLaVA-NeXT-Video，分别针对多图像任务和视频理解任务进行了优化，实现了在相关领域的新高点。

3. **训练策略改进**：通过使用高质量的数据集和可训练模块，LLaVA-NeXT系列模型在训练策略上进行了创新，有效提升了模型的泛化能力和任务解决效率。

4. **支持多样化硬件**：项目提供针对不同硬件平台（如Intel GPU和CPU）的支持，使得模型部署更加灵活。

5. **开源资源与社区贡献**：除了模型本身，还提供了包括评估框架、教程文档等在内的丰富资源，鼓励社区成员参与模型训练和优化，共同推动多模态AI技术的发展。

这些更新不仅展示了LLaVA-NeXT在多模态处理能力上的强大，也为未来AI研究者和开发者提供了宝贵的工具和资源。 <div>
<p></p><hr /><p align="center" width="100%"> <img height="80%" src="https://i.postimg.cc/pL17YtG4/WX20240508-220230-2x.png" width="80%" /> </p> 
<h1>LLaVA-NeXT: Open Large Multimodal Models</h1> 
<p><a href="https://arxiv.org/abs/2408.03326"><img alt="Static Badge" src="https://img.shields.io/badge/llava_onevision-paper-green" /></a> <a href="https://llava-vl.github.io/blog/"><img alt="llava_next-blog" src="https://img.shields.io/badge/llava_next-blog-green" /></a></p> 
<p><a href="https://llava-onevision.lmms-lab.com/"><img alt="llava_onevision-demo" src="https://img.shields.io/badge/llava_onevision-demo-red" /></a> <a href="https://huggingface.co/spaces/lmms-lab/LLaVA-NeXT-Interleave-Demo"><img alt="llava_next-interleave_demo" src="https://img.shields.io/badge/llava_next-interleave_demo-red" /></a> <a href="https://huggingface.co/spaces/WildVision/vision-arena"><img alt="llava_next-video_demo" src="https://img.shields.io/badge/llava_next-video_demo-red" /></a></p> 
<p><a href="https://huggingface.co/collections/lmms-lab/llava-onevision-66a259c3526e15166d6bba37"><img alt="llava_onevision-checkpoints" src="https://img.shields.io/badge/llava_onevision-checkpoints-blue" /></a> <a href="https://huggingface.co/collections/lmms-lab/llava-next-interleave-66763c55c411b340b35873d1"><img alt="llava_next-interleave_checkpoints" src="https://img.shields.io/badge/llava_next-interleave_checkpoints-blue" /></a> <a href="https://huggingface.co/collections/lmms-lab/llava-next-video-661e86f5e8dabc3ff793c944"><img alt="llava_next-video_checkpoints" src="https://img.shields.io/badge/llava_next-video_checkpoints-blue" /></a> <a href="https://huggingface.co/lmms-lab"><img alt="llava_next-image_checkpoints" src="https://img.shields.io/badge/llava_next-image_checkpoints-blue" /></a></p> 
<h2>Release Notes</h2> 
<ul> 
 <li> <p>[2024/08/06] 🔥 <strong>🚀 <a href="https://llava-vl.github.io/blog/2024-08-05-llava-onevision/">LLaVA-OneVision (OV)</a>!</strong> The new LLaVA-OV models (0.5B/7B/72B) achieve new state-of-the-art performance across single-image, multi-image, and video benchmarks, sometimes rivaling top commercial models on 47 diverse benchmarks. 📄 Explore More:</p> 
  <ul> 
   <li><a href="https://arxiv.org/abs/2408.03326">[Paper]</a>: In-depth insights, new emegerging scenarios, ie, strong video understadning through task transfer from images.</li> 
   <li><a href="https://github.com/LLaVA-VL/LLaVA-NeXT/raw/main/docs/LLaVA_OneVision.md">[LLaVA-OV Doc]</a>: Model inference and evaluation guidance.</li> 
   <li><a href="https://github.com/LLaVA-VL/LLaVA-NeXT/raw/main/scripts/train">[Scripts]</a>: Start training models on your single-image/multi-image/video data.</li> 
  </ul> </li> 
 <li> <p>[2024/07/16] 🔥 <strong>LLaVA-NeXT-Video</strong> has been upgraded. The new 32B model achieves the best open-source performance on several video benchmarks, including <a href="https://video-mme.github.io/home_page.html#leaderboard">Video-MME</a>. Please refer to <a href="https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LLaVA-NeXT-Video_0716.md">this page</a> for details, refer to <a href="https://huggingface.co/spaces/WildVision/vision-arena">llava_next-video_demo</a> for demo.</p> </li> 
 <li> <p>[2024/06/23] 🔥 <strong>LLaVA-NeXT-Interleave</strong> is released. We utilize image-text interleaved format to unify multi-image, video, and 3D tasks in one LLM and achieve <strong>SoTA</strong> performance on a wide range of benchmarks. Check out <a href="https://arxiv.org/pdf/2407.07895">paper</a>, <a href="https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/">blog</a>, and <a href="https://huggingface.co/collections/lmms-lab/llava-next-interleave-66763c55c411b340b35873d1">checkpoints</a> to see new capabilities and improved performance! We have released 0.5b, 7b, and 7b-dpo models.</p> 
  <ul> 
   <li>An all-round LLM for multi-image, video, and 3D with strong performance [<a href="https://huggingface.co/spaces/lmms-lab/LLaVA-NeXT-Interleave-Demo">demo</a>]</li> 
   <li>Construct interleave training data <a href="https://huggingface.co/datasets/lmms-lab/M4-Instruct-Data"><strong>M4-Instruct</strong></a></li> 
   <li>Construct multi-image benchmark <a href="https://huggingface.co/datasets/lmms-lab/LLaVA-NeXT-Interleave-Bench"><strong>LLaVA-Interleave Bench</strong></a></li> 
  </ul> </li> 
 <li> <p>[2024/05/25] 🔥 Wondering "<a href="https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/">What Else Influences Visual Instruction Tuning Beyond Data?</a>" Our new <a href="https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/">blog</a> summarizes empirical explorations to ablate the various design choices in improving LMMs except instruct data itself. Meanwhile, open-source the recapioned high-quality data using LLaVA-NeXT-34B on <a href="https://huggingface.co/datasets/lmms-lab/LLaVA-ReCap-118K">[COCO]</a> <a href="https://huggingface.co/datasets/lmms-lab/LLaVA-ReCap-558K">[LCS]</a> <a href="https://huggingface.co/datasets/lmms-lab/LLaVA-ReCap-CC3M">[CC3M]</a>.</p> 
  <ul> 
   <li>Architectures (LMM &amp; Vision Encoder)</li> 
   <li>Visual Representations (Resolution &amp; # Tokens)</li> 
   <li>Training Strategies (High-quality data &amp; Trainable modules)</li> 
  </ul> </li> 
 <li> <p>[2024/05/10] 🔥 <strong>LLaVA-NeXT</strong> (Stronger) models are released, with support of stronger LMM inlcuding LLama-3 (8B) and Qwen-1.5 (72B/110B) Check out [<a href="https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/">blog</a>] and [<a href="https://huggingface.co/lmms-lab">checkpoints</a>] to see improved performance!</p> </li> 
 <li> <p>[2024/05/10] 🔥 <strong>LLaVA-NeXT</strong> (Video) is released. The image-only-trained LLaVA-NeXT model is surprisingly strong on video tasks with zero-shot modality transfer. DPO training with AI feedback on videos can yield significant improvement. [<a href="https://llava-vl.github.io/blog/2024-04-30-llava-next-video/">Blog</a>], [<a href="https://huggingface.co/collections/lmms-lab/llava-next-video-661e86f5e8dabc3ff793c944">checkpoints</a>] and [<a href="https://github.com/sgl-project/sglang">sglang</a>]</p> </li> 
 <li> <p>[2024/01/30] 🔥 <strong>LLaVA-NeXT</strong> is out! With additional scaling to LLaVA-1.5, LLaVA-NeXT-34B outperforms Gemini Pro on some benchmarks. It can now process 4x more pixels and perform more tasks/applications than before. Check out the <a href="https://llava-vl.github.io/blog/2024-01-30-llava-next/">blog post</a>, and explore the <a href="https://llava.hliu.cc/">demo</a>! Models are available in <a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md">Model Zoo</a>. Training/eval data and scripts coming soon.</p> </li> 
</ul> 
<details> 
 More 
 <ul> 
  <li> <p>[2024/03/10] 🔥 Releasing <strong>LMMs-Eval</strong>, a highly efficient evaluation pipeline we used when developing LLaVA-NeXT. It supports the evaluation of LMMs on dozens of public datasets and allows new dataset onboarding, making the dev of new LMMs much faster. [<a href="https://lmms-lab.github.io/lmms-eval-blog/lmms-eval-0.1/">Blog</a>] [<a href="https://github.com/EvolvingLMMs-Lab/lmms-eval">Codebase</a>]</p> </li> 
  <li> <p>[2023/11/10] <a href="https://llava-vl.github.io/llava-plus/">LLaVA-Plus</a> is released: Learning to Use Tools for Creating Multimodal Agents, with LLaVA-Plus (LLaVA that Plug and Learn to Use Skills). [<a href="https://llava-vl.github.io/llava-plus/">Project Page</a>] [<a href="https://llavaplus.ngrok.io/">Demo</a>] [<a href="https://github.com/LLaVA-VL/LLaVA-Plus-Codebase">Code</a>] [<a href="https://arxiv.org/abs/2311.05437">Paper</a>]</p> </li> 
  <li> <p>[2023/11/02] <a href="https://llava-vl.github.io/llava-interactive/">LLaVA-Interactive</a> is released: Experience the future of human-AI multimodal interaction with an all-in-one demo for Image Chat, Segmentation, Generation and Editing. [<a href="https://llava-vl.github.io/llava-interactive/">Project Page</a>] [<a href="https://llavainteractive.ngrok.io/">Demo</a>] [<a href="https://github.com/LLaVA-VL/LLaVA-Interactive-Demo">Code</a>] [<a href="https://arxiv.org/abs/2311.00571">Paper</a>]</p> </li> 
  <li> <p>[2023/10/26] 🔥 LLaVA-1.5 with LoRA achieves comparable performance as full-model finetuning, with a reduced GPU RAM requirement (<a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md#llava-v15">ckpts</a>, <a href="https://github.com/haotian-liu/LLaVA#train">script</a>). We also provide a <a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/Finetune_Custom_Data.md">doc</a> on how to finetune LLaVA-1.5 on your own dataset with LoRA.</p> </li> 
  <li> <p>[2023/10/12] Check out the Korean LLaVA (Ko-LLaVA), created by ETRI, who has generously supported our research! [<a href="https://huggingface.co/spaces/etri-vilab/Ko-LLaVA">🤗 Demo</a>]</p> </li> 
  <li> <p>[2023/10/05] 🔥 LLaVA-1.5 is out! Achieving SoTA on 11 benchmarks, with just simple modifications to the original LLaVA, utilizes all public data, completes training in ~1 day on a single 8-A100 node, and surpasses methods like Qwen-VL-Chat that use billion-scale data. Check out the <a href="https://arxiv.org/abs/2310.03744">technical report</a>, and explore the <a href="https://llava.hliu.cc/">demo</a>! Models are available in <a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md">Model Zoo</a>. The training data and scripts of LLaVA-1.5 are released <a href="https://github.com/haotian-liu/LLaVA#train">here</a>, and evaluation scripts are released <a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/Evaluation.md">here</a>!</p> </li> 
  <li> <p>[2023/09/26] LLaVA is improved with reinforcement learning from human feedback (RLHF) to improve fact grounding and reduce hallucination. Check out the new SFT and RLHF checkpoints at project <a href="https://llava-rlhf.github.io/">[LLavA-RLHF]</a></p> </li> 
  <li> <p>[2023/09/22] <a href="https://arxiv.org/abs/2304.08485">LLaVA</a> is accepted by NeurIPS 2023 as <strong>oral presentation</strong>, and <a href="https://arxiv.org/abs/2306.00890">LLaVA-Med</a> is accepted by NeurIPS 2023 Datasets and Benchmarks Track as <strong>spotlight presentation</strong>.</p> </li> 
  <li> <p>[2023/11/06] Support <strong>Intel</strong> dGPU and CPU platforms. <a href="https://github.com/haotian-liu/LLaVA/tree/intel/docs/intel">More details here.</a></p> </li> 
  <li> <p>[2023/10/12] LLaVA is now supported in <a href="https://github.com/ggerganov/llama.cpp/pull/3436">llama.cpp</a> with 4-bit / 5-bit quantization support!</p> </li> 
  <li> <p>[2023/10/11] The training data and scripts of LLaVA-1.5 are released <a href="https://github.com/haotian-liu/LLaVA#train">here</a>, and evaluation scripts are released <a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/Evaluation.md">here</a>!</p> </li> 
  <li> <p>[2023/10/10] <a href="https://blog.roboflow.com/first-impressions-with-llava-1-5/">Roboflow Deep Dive</a>: First Impressions with LLaVA-1.5.</p> </li> 
  <li> <p>[2023/09/20] We summarize our empirical study of training 33B and 65B LLaVA models in a <a href="https://arxiv.org/abs/2309.09958">note</a>. Further, if you are interested in the comprehensive review, evolution and trend of multimodal foundation models, please check out our recent survey paper <a href="https://arxiv.org/abs/2309.10020">``Multimodal Foundation Models: From Specialists to General-Purpose Assistants''.</a></p> </li> 
 </ul> 
 <p align="center"> <img src="https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings/raw/main/images/mfm_evolution.jpeg?raw=true" width="50%/" /> </p> 
 <ul> 
  <li>[2023/07/19] 🔥 We release a major upgrade, including support for LLaMA-2, LoRA training, 4-/8-bit inference, higher resolution (336x336), and a lot more. We release <a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/LLaVA_Bench.md">LLaVA Bench</a> for benchmarking open-ended visual chat with results from Bard and Bing-Chat. We also support and verify training with RTX 3090 and RTX A6000. Check out <a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/LLaVA_from_LLaMA2.md">LLaVA-from-LLaMA-2</a>, and our <a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md">model zoo</a>!</li> 
  <li>[2023/06/26] <a href="https://vlp-tutorial.github.io/">CVPR 2023 Tutorial</a> on <strong>Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4</strong>! Please check out [<a href="https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Chunyuan_cvpr2023_tutorial_lmm.pdf">Slides</a>] [<a href="https://arxiv.org/abs/2306.14895">Notes</a>] [<a href="https://youtu.be/mkI7EPD1vp8">YouTube</a>] [<a href="https://www.bilibili.com/video/BV1Ng4y1T7v3/">Bilibli</a>].</li> 
  <li>[2023/06/11] We released the preview for the most requested feature: DeepSpeed and LoRA support! Please see documentations <a href="https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LoRA.md">here</a>.</li> 
  <li>[2023/06/01] We released <strong>LLaVA-Med: Large Language and Vision Assistant for Biomedicine</strong>, a step towards building biomedical domain large language and vision models with GPT-4 level capabilities. Checkout the <a href="https://arxiv.org/abs/2306.00890">paper</a> and <a href="https://github.com/microsoft/LLaVA-Med">page</a>.</li> 
  <li>[2023/05/06] We are releasing <a href="https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview">LLaVA-Lighting-MPT-7B-preview</a>, based on MPT-7B-Chat! See <a href="https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/#LLaVA-MPT-7b">here</a> for more details.</li> 
  <li>[2023/05/02] 🔥 We are releasing LLaVA-Lighting! Train a lite, multimodal GPT-4 with just $40 in 3 hours! See <a href="https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/#train-llava-lightning">here</a> for more details.</li> 
  <li>[2023/04/27] Thanks to the community effort, LLaVA-13B with 4-bit quantization allows you to run on a GPU with as few as 12GB VRAM! Try it out <a href="https://github.com/oobabooga/text-generation-webui/tree/main/extensions/llava">here</a>.</li> 
  <li>[2023/04/17] 🔥 We released <strong>LLaVA: Large Language and Vision Assistant</strong>. We propose visual instruction tuning, towards building large language and vision models with GPT-4 level capabilities. Checkout the <a href="https://arxiv.org/abs/2304.08485">paper</a> and <a href="https://llava.hliu.cc/">demo</a>.</li> 
 </ul> 
</details> 
<!-- <a href="https://llava.hliu.cc/"><img src="assets/demo.gif" width="70%"></a> --> 
<p><strong>Usage and License Notices</strong>: This project utilizes certain datasets and checkpoints that are subject to their respective original licenses. Users must comply with all terms and conditions of these original licenses, including but not limited to the <a href="https://openai.com/policies/terms-of-use">OpenAI Terms of Use</a> for the dataset and the specific licenses for base language models for checkpoints trained using the dataset (e.g. <a href="https://ai.meta.com/llama/license/">Llama-1/2 community license</a> for LLaMA-2 and Vicuna-v1.5, <a href="https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat/blob/main/LICENSE">Tongyi Qianwen RESEARCH LICENSE AGREEMENT</a> and <a href="https://llama.meta.com/llama3/license/">Llama-3 Research License</a>). This project does not impose any additional constraints beyond those stipulated in the original licenses. Furthermore, users are reminded to ensure that their use of the dataset and checkpoints is in compliance with all applicable laws and regulations.</p> 
<h2>Models &amp; Scripts</h2> 
<h3>Installation</h3> 
<h4>1. <strong>Clone this repository and navigate to the LLaVA folder:</strong></h4> 
<pre><code class="language-bash">git clone https://github.com/LLaVA-VL/LLaVA-NeXT
cd LLaVA-NeXT
</code></pre> 
<h4>2. <strong>Install the inference package:</strong></h4> 
<pre><code class="language-bash">conda create -n llava python=3.10 -y
conda activate llava
pip install --upgrade pip  # Enable PEP 660 support.
pip install -e ".[train]"
</code></pre> 
<h3>Project Navigation</h3> 
<p>Please checkout the following page for more inference &amp; evaluation details.</p> 
<h4>- <strong>LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild</strong></h4> 
<ul> 
 <li><a href="https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LLaVA-NeXT.md">LLaVA-NeXT-Image</a>: for image demo inference and evaluation of stronger LMMs using <a href="https://github.com/EvolvingLMMs-Lab/lmms-eval">lmms-eval</a>.</li> 
</ul> 
<h4>- LLaVA-NeXT: A Strong Zero-shot Video Understanding Model</h4> 
<ul> 
 <li><a href="https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LLaVA-NeXT-Video.md">LLaVA-NeXT-Video</a>: for video inference and evaluation scripts. We recommend to use <a href="https://lmms-lab.github.io/posts/lmms-eval-0.2/">LMMs-video</a> for evaluation.</li> 
</ul> 
<h4>- LLaVA-NeXT: Tackling Multi-image, Video, and 3D in Large Multimodal Models</h4> 
<ul> 
 <li><a href="https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LLaVA-NeXT-Interleave.md">LLaVA-NeXT-Interleave</a>: for multi-image demo and evaluation scripts.</li> 
</ul> 
<h2>SGLang for SpeedUp Inference and Deployment</h2> 
<p>We use <a href="https://github.com/sgl-project/sglang">SGLang</a> to speed up inference and deployment of LLaVA-NeXT. You could make LLaVA-NeXT as a backend API service with SGLang.</p> 
<p><strong>Prepare Environment</strong>: Following the instruction in the <a href="https://github.com/sgl-project/sglang?tab=readme-ov-file#install">sglang</a></p> 
<h3>LLaVA-NeXT (Image)</h3> 
<p>Checkout the HTTP Post/Get and SRT usage at <a href="https://github.com/sgl-project/sglang/raw/main/examples/usage/llava">sglang/examples/usage/llava</a></p> 
<h3>LLaVA-NeXT (Video)</h3> 
<p><strong>Launch and Run on (K) Nodes</strong>:</p> 
<ul> 
 <li>Go to sglang project <pre><code>cd PATH_TO/sglang
</code></pre> </li> 
 <li>First node: <pre><code class="language-sh">bash examples/usage/llava_video/srt_example_llava_v.sh K 0 YOUR_VIDEO_PATH YOUR_MODEL_PATH FRAMES_PER_VIDEO
(e.g. bash examples/usage/llava_video/srt_example_llava_v.sh K 0 examples/usage/llava_video/videos/Q98Z4OTh8RwmDonc.mp4 lmms-lab/LLaVA-NeXT-Video-7B-DPO 16)
</code></pre> </li> 
 <li>Second node: <pre><code class="language-sh">bash examples/usage/llava_video/srt_example_llava_v.sh K 1 YOUR_VIDEO_PATH YOUR_MODEL_PATH FRAMES_PER_VIDEO
</code></pre> </li> 
 <li>The K node: <pre><code class="language-sh">bash examples/usage/llava_video/srt_example_llava_v.sh K K-1 YOUR_VIDEO_PATH YOUR_MODEL_PATH FRAMES_PER_VIDEO
</code></pre> </li> 
</ul> 
<h2>Citation</h2> 
<p>If you find it useful for your research and applications, please cite related papers/blogs using this BibTeX:</p> 
<pre><code class="language-bibtex">@article{li2024llava,
  title={LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models},
  author={Li, Feng and Zhang, Renrui and Zhang, Hao and Zhang, Yuanhan and Li, Bo and Li, Wei and Ma, Zejun and Li, Chunyuan},
  journal={arXiv preprint arXiv:2407.07895},
  year={2024}
}

@misc{li2024llavanext-ablations,
	title={LLaVA-NeXT: What Else Influences Visual Instruction Tuning Beyond Data?},
	url={https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/},
	author={Li, Bo and Zhang, Hao and Zhang, Kaichen and Guo, Dong and Zhang, Yuanhan and Zhang, Renrui and Li, Feng and Liu, Ziwei and Li, Chunyuan},
	month={May},
	year={2024}
}

@misc{li2024llavanext-strong,
    title={LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild},
    url={https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/},
    author={Li, Bo and Zhang, Kaichen and Zhang, Hao and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Yuanhan and Liu, Ziwei and Li, Chunyuan},
    month={May},
    year={2024}
}

@misc{zhang2024llavanext-video,
  title={LLaVA-NeXT: A Strong Zero-shot Video Understanding Model},
  url={https://llava-vl.github.io/blog/2024-04-30-llava-next-video/},
  author={Zhang, Yuanhan and Li, Bo and Liu, haotian and Lee, Yong jae and Gui, Liangke and Fu, Di and Feng, Jiashi and Liu, Ziwei and Li, Chunyuan},
  month={April},
  year={2024}
}

@misc{liu2024llavanext,
    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    month={January},
    year={2024}
}

@misc{liu2023improvedllava,
      title={Improved Baselines with Visual Instruction Tuning}, 
      author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
      publisher={arXiv:2310.03744},
      year={2023},
}

@misc{liu2023llava,
      title={Visual Instruction Tuning}, 
      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
      publisher={NeurIPS},
      year={2023},
}
</code></pre> 
<h2>Acknowledgement</h2> 
<ul> 
 <li><a href="https://github.com/lm-sys/FastChat">Vicuna</a>: the codebase we built upon, and our base model Vicuna-13B that has the amazing language capabilities!</li> 
 <li>The LLaVA-NeXT project is currently maintained by the team along with our contributors (listed alphabetically by the first names): <a href="https://brianboli.com/">Bo Li</a>, <a href="https://www.linkedin.com/in/dongguoset/">Dong Guo</a>, <a href="https://scholar.google.com/citations?hl=zh-CN&amp;user=ybRe9GcAAAAJ&amp;view_op=list_works&amp;sortby=pubdate">Feng Li</a>, <a href="https://scholar.google.com/citations?user=B8hPxMQAAAAJ&amp;hl=en">Hao Zhang</a>, <a href="https://www.linkedin.com/in/kaichen-zhang-014b17219/?originalSubdomain=sg">Kaichen Zhang</a>, <a href="https://zrrskywalker.github.io/">Renrui Zhang</a>, <a href="https://zhangyuanhan-ai.github.io/">Yuanhan Zhang</a>, led by <a href="https://chunyuan.li/">Chunyuan Li</a> and with the guidance and help from <a href="https://hliu.cc/">Haotian Liu</a>.</li> 
 <li>The <code>﻿lmms-eval</code> framework and its core contributors, including Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, and Kairui Hu, for their support on the evaluation side.</li> 
</ul> 
<h2>Related Projects</h2> 
<ul> 
 <li><a href="https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM">Instruction Tuning with GPT-4</a></li> 
 <li><a href="https://github.com/microsoft/LLaVA-Med">LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day</a></li> 
 <li><a href="https://github.com/Luodian/Otter">Otter: In-Context Multi-Modal Instruction Tuning</a></li> 
</ul> 
<p>For future project ideas, please check out:</p> 
<ul> 
 <li><a href="https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once">SEEM: Segment Everything Everywhere All at Once</a></li> 
 <li><a href="https://github.com/IDEA-Research/Grounded-Segment-Anything">Grounded-Segment-Anything</a> to detect, segment, and generate anything by marrying <a href="https://github.com/IDEA-Research/GroundingDINO">Grounding DINO</a> and <a href="https://github.com/facebookresearch/segment-anything">Segment-Anything</a>.</li> 
</ul>
]]></content:encoded>


</item>
<item>
<title>hacksider/Deep-Live-Cam</title>
<link>https://github.com/hacksider/Deep-Live-Cam</link>
<guid>https://github.com/hacksider/Deep-Live-Cam</guid>
<content:encoded><![CDATA[
<div> 关键词：实时人脸互换、一键视频深度伪造、单张图像、AI生成媒体、法律与伦理

总结:

本文介绍了一款用于实时人脸互换和一键视频深度伪造的软件，旨在为艺术家提供工具，如动画定制角色或作为服装模特等任务。开发者意识到该软件可能被不当使用，并已内置检查机制以防止处理不适当内容，如裸体、暴力或敏感素材。如果要求，项目可能会关闭或在输出中添加水印。

安装步骤包括安装Python、pip和Git，克隆代码库，下载模型文件，使用虚拟环境安装依赖项，并根据GPU可用性选择特定的执行提供程序进行加速。软件通过命令行运行，允许用户指定源图像、目标图像或视频、输出位置、帧处理器、保持帧速率、音频和临时文件等功能。

使用方法涉及选择面部图像、目标图像或视频，启动程序，观察实时预览并处理完成后的输出文件。命令行选项提供额外控制，如保持原始帧率、音频和临时文件、处理每个面部、过滤不适当内容等。

最后，用户被提醒负责任地使用软件，特别是当使用真实人物面部时，需获得同意并在在线发布内容时明确表示为深度伪造。开发者不承担用户行为的责任。 <div>
<p>real time face swap and one-click video deepfake with only a single image</p><hr /><p><img alt="demo-gif" src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/demo.gif" /></p> 
<h2>Disclaimer</h2> 
<p>This software is meant to be a productive contribution to the rapidly growing AI-generated media industry. It will help artists with tasks such as animating a custom character or using the character as a model for clothing etc.</p> 
<p>The developers of this software are aware of its possible unethical applications and are committed to take preventative measures against them. It has a built-in check which prevents the program from working on inappropriate media including but not limited to nudity, graphic content, sensitive material such as war footage etc. We will continue to develop this project in the positive direction while adhering to law and ethics. This project may be shut down or include watermarks on the output if requested by law.</p> 
<p>Users of this software are expected to use this software responsibly while abiding by local laws. If the face of a real person is being used, users are required to get consent from the concerned person and clearly mention that it is a deepfake when posting content online. Developers of this software will not be responsible for actions of end-users.</p> 
<h2>How do I install it?</h2> 
<h3>Basic: It is more likely to work on your computer but it will also be very slow. You can follow instructions for the basic install (This usually runs via <strong>CPU</strong>)</h3> 
<h4>1.Setup your platform</h4> 
<ul> 
 <li>python (3.10 recommended)</li> 
 <li>pip</li> 
 <li>git</li> 
 <li><a href="https://www.youtube.com/watch?v=OlNWCpFdVMA">ffmpeg</a></li> 
 <li><a href="https://visualstudio.microsoft.com/visual-cpp-build-tools/">visual studio 2022 runtimes (windows)</a></li> 
</ul> 
<h4>2. Clone Repository</h4> 
<pre><code>https://github.com/hacksider/Deep-Live-Cam.git
</code></pre> 
<h4>3. Download Models</h4> 
<ol> 
 <li><a href="https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth">GFPGANv1.4</a></li> 
 <li><a href="https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx">inswapper_128_fp16.onnx</a></li> 
</ol> 
<p>Then put those 2 files on the "<strong>models</strong>" folder</p> 
<h4>4. Install dependency</h4> 
<p>We highly recommend to work with a <code>venv</code> to avoid issues.</p> 
<pre><code>pip install -r requirements.txt
</code></pre> 
<p>For MAC OS, You have to install or upgrade python-tk package:</p> 
<pre><code>brew install python-tk@3.10
</code></pre> 
<h5>DONE!!! If you dont have any GPU, You should be able to run roop using <code>python run.py</code> command. Keep in mind that while running the program for first time, it will download some models which can take time depending on your network connection.</h5> 
<h3>*Proceed if you want to use GPU Acceleration</h3> 
<h3>CUDA Execution Provider (Nvidia)*</h3> 
<ol> 
 <li> <p>Install <a href="https://developer.nvidia.com/cuda-11-8-0-download-archive">CUDA Toolkit 11.8</a></p> </li> 
 <li> <p>Install dependencies:</p> </li> 
</ol> 
<pre><code>pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu==1.16.3

</code></pre> 
<ol start="3"> 
 <li>Usage in case the provider is available:</li> 
</ol> 
<pre><code>python run.py --execution-provider cuda

</code></pre> 
<h3><a href="https://github.com/s0md3v/roop/wiki/2.-Acceleration#coreml-execution-provider-apple-silicon"></a>CoreML Execution Provider (Apple Silicon)</h3> 
<ol> 
 <li>Install dependencies:</li> 
</ol> 
<pre><code>pip uninstall onnxruntime onnxruntime-silicon
pip install onnxruntime-silicon==1.13.1

</code></pre> 
<ol start="2"> 
 <li>Usage in case the provider is available:</li> 
</ol> 
<pre><code>python run.py --execution-provider coreml

</code></pre> 
<h3><a href="https://github.com/s0md3v/roop/wiki/2.-Acceleration#coreml-execution-provider-apple-legacy"></a>CoreML Execution Provider (Apple Legacy)</h3> 
<ol> 
 <li>Install dependencies:</li> 
</ol> 
<pre><code>pip uninstall onnxruntime onnxruntime-coreml
pip install onnxruntime-coreml==1.13.1

</code></pre> 
<ol start="2"> 
 <li>Usage in case the provider is available:</li> 
</ol> 
<pre><code>python run.py --execution-provider coreml

</code></pre> 
<h3><a href="https://github.com/s0md3v/roop/wiki/2.-Acceleration#directml-execution-provider-windows"></a>DirectML Execution Provider (Windows)</h3> 
<ol> 
 <li>Install dependencies:</li> 
</ol> 
<pre><code>pip uninstall onnxruntime onnxruntime-directml
pip install onnxruntime-directml==1.15.1

</code></pre> 
<ol start="2"> 
 <li>Usage in case the provider is available:</li> 
</ol> 
<pre><code>python run.py --execution-provider directml

</code></pre> 
<h3><a href="https://github.com/s0md3v/roop/wiki/2.-Acceleration#openvino-execution-provider-intel"></a>OpenVINO™ Execution Provider (Intel)</h3> 
<ol> 
 <li>Install dependencies:</li> 
</ol> 
<pre><code>pip uninstall onnxruntime onnxruntime-openvino
pip install onnxruntime-openvino==1.15.0

</code></pre> 
<ol start="2"> 
 <li>Usage in case the provider is available:</li> 
</ol> 
<pre><code>python run.py --execution-provider openvino
</code></pre> 
<h2>How do I use it?</h2> 
<blockquote> 
 <p>Note: When you run this program for the first time, it will download some models ~300MB in size.</p> 
</blockquote> 
<p>Executing <code>python run.py</code> command will launch this window: <img alt="gui-demo" src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/instruction.png" /></p> 
<p>Choose a face (image with desired face) and the target image/video (image/video in which you want to replace the face) and click on <code>Start</code>. Open file explorer and navigate to the directory you select your output to be in. You will find a directory named <code>&lt;video_title&gt;</code> where you can see the frames being swapped in realtime. Once the processing is done, it will create the output file. That's it.</p> 
<h2>For the webcam mode</h2> 
<p>Just follow the clicks on the screenshot</p> 
<ol> 
 <li>Select a face</li> 
 <li>Click live</li> 
 <li>Wait for a few seconds (it takes a longer time, usually 10 to 30 seconds before the preview shows up)</li> 
</ol> 
<p><img alt="demo-gif" src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/demo.gif" /></p> 
<p>Just use your favorite screencapture to stream like OBS</p> 
<blockquote> 
 <p>Note: In case you want to change your face, just select another picture, the preview mode will then restart (so just wait a bit).</p> 
</blockquote> 
<p>Additional command line arguments are given below. To learn out what they do, check <a href="https://github.com/s0md3v/roop/wiki/Advanced-Options">this guide</a>.</p> 
<pre><code>options:
  -h, --help                                               show this help message and exit
  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image
  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory
  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)
  --keep-fps                                               keep original fps
  --keep-audio                                             keep original audio
  --keep-frames                                            keep temporary frames
  --many-faces                                             process every face
  --nsfw-filter                                            filter the NSFW image or video
  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder
  --video-quality [0-51]                                   adjust output video quality
  --live-mirror                                            the live camera display as you see it in the front-facing camera frame
  --live-resizable                                         the live camera frame is resizable
  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB
  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)
  --execution-threads EXECUTION_THREADS                    number of execution threads
  -v, --version                                            show program's version number and exit
</code></pre> 
<p>Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.</p> 
<h2>Want the Next Update Now?</h2> 
<p>If you want the latest and greatest build, or want to see some new great features, go to our <a href="https://github.com/hacksider/Deep-Live-Cam/tree/experimental">experimental branch</a> and experience what the contributors have given.</p> 
<h2>Credits</h2> 
<ul> 
 <li><a href="https://ffmpeg.org/">ffmpeg</a>: for making video related operations easy</li> 
 <li><a href="https://github.com/deepinsight">deepinsight</a>: for their <a href="https://github.com/deepinsight/insightface">insightface</a> project which provided a well-made library and models.</li> 
 <li><a href="https://github.com/havok2-htwo">havok2-htwo</a> : for sharing the code for webcam</li> 
 <li><a href="https://github.com/GosuDRM/nsfw-roop">GosuDRM</a> : for uncensoring roop</li> 
 <li>and <a href="https://github.com/hacksider/Deep-Live-Cam/graphs/contributors">all developers</a> behind libraries used in this project.</li> 
 <li>Foot Note: <a href="https://github.com/hacksider/roop-cam">This is originally roop-cam, see the full history of the code here.</a> Please be informed that the base author of the code is <a href="https://github.com/s0md3v/roop">s0md3v</a></li> 
</ul>
]]></content:encoded>


</item>
<item>
<title>goauthentik/authentik</title>
<link>https://github.com/goauthentik/authentik</link>
<guid>https://github.com/goauthentik/authentik</guid>
<content:encoded><![CDATA[
<div> 关键词：authentik、身份验证、开放源代码、Docker Compose、Helm Chart

总结:
authentik是一个强调灵活性和多样性的开源身份提供者，支持广泛的协议。它被推荐用于小型测试设置中的Docker Compose部署，以及大型设置中的Helm Chart部署。对于需要替换Okta、Auth0、Entra ID、Ping Identity等传统IDP的大规模企业或B2B2C业务来说，authentik是一个自托管的理想选择。用户可以根据自己的需求选择适合的部署方式。此外，authentik提供了一个开发者社区，鼓励贡献和合作，希望有更多组织使用并参与到项目中来。如果您正在使用authentik，可以向项目团队提交您的Logo以示支持，并参与贡献代码或提出改进建议。 <div>
<p>The authentication glue you need.</p><hr /><p align="center"> <img alt="authentik logo" height="150" src="https://goauthentik.io/img/icon_top_brand_colour.svg?sanitize=true" /> </p> 
<hr /> 
<p><a href="https://goauthentik.io/discord"><img alt="Join Discord" src="https://img.shields.io/discord/809154715984199690?label=Discord&amp;style=for-the-badge" /></a> <a href="https://github.com/goauthentik/authentik/actions/workflows/ci-main.yml"><img alt="GitHub Workflow Status" src="https://img.shields.io/github/actions/workflow/status/goauthentik/authentik/ci-main.yml?branch=main&amp;label=core%20build&amp;style=for-the-badge" /></a> <a href="https://github.com/goauthentik/authentik/actions/workflows/ci-outpost.yml"><img alt="GitHub Workflow Status" src="https://img.shields.io/github/actions/workflow/status/goauthentik/authentik/ci-outpost.yml?branch=main&amp;label=outpost%20build&amp;style=for-the-badge" /></a> <a href="https://github.com/goauthentik/authentik/actions/workflows/ci-web.yml"><img alt="GitHub Workflow Status" src="https://img.shields.io/github/actions/workflow/status/goauthentik/authentik/ci-web.yml?branch=main&amp;label=web%20build&amp;style=for-the-badge" /></a> <a href="https://codecov.io/gh/goauthentik/authentik"><img alt="Code Coverage" src="https://img.shields.io/codecov/c/gh/goauthentik/authentik?style=for-the-badge" /></a> <img alt="Docker pulls" src="https://img.shields.io/docker/pulls/beryju/authentik.svg?style=for-the-badge" /> <img alt="Latest version" src="https://img.shields.io/docker/v/beryju/authentik?sort=semver&amp;style=for-the-badge" /> <a href="https://www.transifex.com/authentik/authentik/"><img alt="" src="https://img.shields.io/badge/Help%20translate-transifex-blue?style=for-the-badge" /></a></p> 
<h2>What is authentik?</h2> 
<p>authentik is an open-source Identity Provider that emphasizes flexibility and versatility, with support for a wide set of protocols.</p> 
<p>Our <a href="https://goauthentik.io/pricing">enterprise offer</a> can also be used as a self-hosted replacement for large-scale deployments of Okta/Auth0, Entra ID, Ping Identity, or other legacy IdPs for employees and B2B2C use.</p> 
<h2>Installation</h2> 
<p>For small/test setups it is recommended to use Docker Compose; refer to the <a href="https://goauthentik.io/docs/installation/docker-compose/?utm_source=github">documentation</a>.</p> 
<p>For bigger setups, there is a Helm Chart <a href="https://github.com/goauthentik/helm">here</a>. This is documented <a href="https://goauthentik.io/docs/installation/kubernetes/?utm_source=github">here</a>.</p> 
<h2>Screenshots</h2> 
<table> 
 <thead> 
  <tr> 
   <th>Light</th> 
   <th>Dark</th> 
  </tr> 
 </thead> 
 <tbody> 
  <tr> 
   <td><img alt="" src="https://docs.goauthentik.io/img/screen_apps_light.jpg" /></td> 
   <td><img alt="" src="https://docs.goauthentik.io/img/screen_apps_dark.jpg" /></td> 
  </tr> 
  <tr> 
   <td><img alt="" src="https://docs.goauthentik.io/img/screen_admin_light.jpg" /></td> 
   <td><img alt="" src="https://docs.goauthentik.io/img/screen_admin_dark.jpg" /></td> 
  </tr> 
 </tbody> 
</table> 
<h2>Development</h2> 
<p>See <a href="https://goauthentik.io/developer-docs/?utm_source=github">Developer Documentation</a></p> 
<h2>Security</h2> 
<p>See <a href="https://raw.githubusercontent.com/goauthentik/authentik/main/SECURITY.md">SECURITY.md</a></p> 
<h2>Adoption and Contributions</h2> 
<p>Your organization uses authentik? We'd love to add your logo to the readme and our website! Email us @ <a href="mailto:hello@goauthentik.io">hello@goauthentik.io</a> or open a GitHub Issue/PR! For more information on how to contribute to authentik, please refer to our <a href="https://raw.githubusercontent.com/goauthentik/authentik/main/CONTRIBUTING.md">CONTRIBUTING.md file</a>.</p>
]]></content:encoded>


</item>
<item>
<title>lllyasviel/stable-diffusion-webui-forge</title>
<link>https://github.com/lllyasviel/stable-diffusion-webui-forge</link>
<guid>https://github.com/lllyasviel/stable-diffusion-webui-forge</guid>
<content:encoded><![CDATA[
<div> 关键词：Stable Diffusion WebUI Forge、安装、高级安装、状态、UnetPatcher

文章总结：

Stable Diffusion WebUI Forge是一个基于Stable Diffusion WebUI的平台，旨在简化开发过程，优化资源管理，加速推理速度并研究实验功能。其名称“Forge”灵感来源于Minecraft中的同名组件，目标是成为Stable Diffusion WebUI的扩展。

**1. 安装**
Forge提供了一个一键安装包，其中包含了Git和Python，用户只需解压缩文件、运行update.bat更新并使用run.bat启动即可。

**2. 高级安装**
对于熟悉Git的用户，可以将Forge作为SD-WebUI的分支进行安装，这样可以复用原有的检查点和扩展，但需要用户了解操作风险。

**3. 状态**
文章列出了多个组件的状态，包括基本扩散、GPU内存管理系统、LoRAs、预处理器、控制网等，均显示为正常工作。同时，指出了部分UI（如Gradio 4 UIs）和特定功能（如Microsoft Surface触压支持）的状态。

**4. UnetPatcher**
文章展示了UnetPatcher的实现代码，用于集成FreeU V2功能，包括FFT滤波器实现、设备适应性和块补丁逻辑，以及与主模型的整合方式。

**5. 建设中**
文章指出Forge正处于建设阶段，文档、用户界面和功能可能会随着更新而改变。

总结：
Stable Diffusion WebUI Forge是一个为Stable Diffusion WebUI量身定制的增强平台，通过提供一键安装包、支持高级安装选项、展示组件状态、展示UnetPatcher实现细节以及提示用户注意其仍在建设中，为用户提供了一种更高效、更灵活的使用体验。其目标是通过优化资源管理和加速推理速度来提升扩散模型的实用性，同时也为实验性功能的研究提供了便利。 <div>
<p></p><hr /><h1>Stable Diffusion WebUI Forge</h1> 
<p>Stable Diffusion WebUI Forge is a platform on top of <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">Stable Diffusion WebUI</a> (based on <a href="https://www.gradio.app/">Gradio</a> <a href="https://github.com/gradio-app/gradio"><img src="https://img.shields.io/github/stars/gradio-app/gradio" /></a>) to make development easier, optimize resource management, speed up inference, and study experimental features.</p> 
<p>The name "Forge" is inspired from "Minecraft Forge". This project is aimed at becoming SD WebUI's Forge.</p> 
<p>Forge is currently based on SD-WebUI 1.10.1 at <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/82a973c04367123ae98bd9abdf80d9eda9b910e2">this commit</a>. (Because original SD-WebUI is almost static now, Forge will sync with original WebUI every 90 days, or when important fixes.)</p> 
<h1>Quick List</h1> 
<p><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/discussions/853">Gradio 4 UI Must Read (TLDR: You need to use RIGHT MOUSE BUTTON to move canvas!)</a></p> 
<p><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/discussions/981">Flux Tutorial (BitsandBytes Models, NF4, "GPU Weight", "Offload Location", "Offload Method", etc)</a></p> 
<p><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/discussions/1050">Flux Tutorial 2 (Seperated Full Models, GGUF, Technically Correct Comparison between GGUF and NF4, etc)</a></p> 
<p><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/discussions/1181">Report Flux Performance Problems (TLDR: DO NOT set "GPU Weight" too high! Lower "GPU Weight" solves 99% problems!)</a></p> 
<p><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/discussions/1224#discussioncomment-10384104">(Save Flux BitsandBytes UNet/Checkpoint)</a></p> 
<p><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/discussions/854">LayerDiffuse Transparent Image Editing</a></p> 
<p><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/discussions/1286">(Policy) Soft Advertisement Removal Policy</a></p> 
<p>(Flux BNB NF4 / GGUF Q8_0/Q5_0/Q5_1/Q4_0/Q4_1 are all natively supported with GPU weight slider and Quene/Async Swap toggle and swap location toggle. All Flux BNB NF4 / GGUF Q8_0/Q5_0/Q4_0 have LoRA support.)</p> 
<h1>Installing Forge</h1> 
<p><strong>Just use this one-click installation package (with git and python included).</strong></p> 
<p><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/releases/download/latest/webui_forge_cu121_torch231.7z">&gt;&gt;&gt; Click Here to Download One-Click Package (CUDA 12.1 + Pytorch 2.3.1) &lt;&lt;&lt;</a></p> 
<p>Some other CUDA/Torch Versions:</p> 
<p><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/releases/download/latest/webui_forge_cu121_torch231.7z">Forge with CUDA 12.1 + Pytorch 2.3.1</a> &lt;- <strong>Recommended</strong></p> 
<p><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/releases/download/latest/webui_forge_cu124_torch24.7z">Forge with CUDA 12.4 + Pytorch 2.4</a> &lt;- <strong>Fastest</strong>, but MSVC may be broken, xformers may not work</p> 
<p><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/releases/download/latest/webui_forge_cu121_torch21.7z">Forge with CUDA 12.1 + Pytorch 2.1</a> &lt;- the previously used old environments</p> 
<p>After you download, you uncompress, use <code>update.bat</code> to update, and use <code>run.bat</code> to run.</p> 
<p>Note that running <code>update.bat</code> is important, otherwise you may be using a previous version with potential bugs unfixed.</p> 
<p><img alt="image" src="https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/c49bd60d-82bd-4086-9859-88d472582b94" /></p> 
<h3>Advanced Install</h3> 
<p>If you are proficient in Git and you want to install Forge as another branch of SD-WebUI, please see <a href="https://github.com/continue-revolution/sd-webui-animatediff/raw/forge/master/docs/how-to-use.md#you-have-a1111-and-you-know-git">here</a>. In this way, you can reuse all SD checkpoints and all extensions you installed previously in your OG SD-WebUI, but you should know what you are doing.</p> 
<p>If you know what you are doing, you can also install Forge using same method as SD-WebUI. (Install Git, Python, Git Clone the forge repo <code>https://github.com/lllyasviel/stable-diffusion-webui-forge.git</code> and then run webui-user.bat).</p> 
<h3>Previous Versions</h3> 
<p>You can download previous versions <a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/discussions/849">here</a>.</p> 
<h1>Forge Status</h1> 
<p>Based on manual test one-by-one:</p> 
<table> 
 <thead> 
  <tr> 
   <th>Component</th> 
   <th>Status</th> 
   <th>Last Test</th> 
  </tr> 
 </thead> 
 <tbody> 
  <tr> 
   <td>Basic Diffusion</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>GPU Memory Management System</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>LoRAs</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>All Preprocessors</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>All ControlNets</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>All IP-Adapters</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>All Instant-IDs</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>All Reference-only Methods</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>All Integrated Extensions</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>Popular Extensions (Adetailer, etc)</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>Gradio 4 UIs</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>Gradio 4 Forge Canvas</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>LoRA/Checkpoint Selection UI for Gradio 4</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>Photopea/OpenposeEditor/etc for ControlNet</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>Wacom 128 level touch pressure support for Canvas</td> 
   <td>Normal</td> 
   <td>2024 July 15</td> 
  </tr> 
  <tr> 
   <td>Microsoft Surface touch pressure support for Canvas</td> 
   <td>Broken, pending fix</td> 
   <td>2024 July 29</td> 
  </tr> 
  <tr> 
   <td>txt2img and img2img API Endpoints</td> 
   <td>Broken, pending fix</td> 
   <td>2024 July 29</td> 
  </tr> 
 </tbody> 
</table> 
<p>Feel free to open issue if anything is broken and I will take a look every several days. If I do not update this "Forge Status" then it means I cannot reproduce any problem. In that case, fresh re-install should help most.</p> 
<h1>UnetPatcher</h1> 
<p>Below are self-supported <strong>single file</strong> of all codes to implement FreeU V2.</p> 
<p>See also <code>extension-builtin/sd_forge_freeu/scripts/forge_freeu.py</code>:</p> 
<pre><code class="language-python">import torch
import gradio as gr

from modules import scripts


def Fourier_filter(x, threshold, scale):
    # FFT
    x_freq = torch.fft.fftn(x.float(), dim=(-2, -1))
    x_freq = torch.fft.fftshift(x_freq, dim=(-2, -1))

    B, C, H, W = x_freq.shape
    mask = torch.ones((B, C, H, W), device=x.device)

    crow, ccol = H // 2, W // 2
    mask[..., crow - threshold:crow + threshold, ccol - threshold:ccol + threshold] = scale
    x_freq = x_freq * mask

    # IFFT
    x_freq = torch.fft.ifftshift(x_freq, dim=(-2, -1))
    x_filtered = torch.fft.ifftn(x_freq, dim=(-2, -1)).real

    return x_filtered.to(x.dtype)


def patch_freeu_v2(unet_patcher, b1, b2, s1, s2):
    model_channels = unet_patcher.model.diffusion_model.config["model_channels"]
    scale_dict = {model_channels * 4: (b1, s1), model_channels * 2: (b2, s2)}
    on_cpu_devices = {}

    def output_block_patch(h, hsp, transformer_options):
        scale = scale_dict.get(h.shape[1], None)
        if scale is not None:
            hidden_mean = h.mean(1).unsqueeze(1)
            B = hidden_mean.shape[0]
            hidden_max, _ = torch.max(hidden_mean.view(B, -1), dim=-1, keepdim=True)
            hidden_min, _ = torch.min(hidden_mean.view(B, -1), dim=-1, keepdim=True)
            hidden_mean = (hidden_mean - hidden_min.unsqueeze(2).unsqueeze(3)) / (hidden_max - hidden_min).unsqueeze(2).unsqueeze(3)

            h[:, :h.shape[1] // 2] = h[:, :h.shape[1] // 2] * ((scale[0] - 1) * hidden_mean + 1)

            if hsp.device not in on_cpu_devices:
                try:
                    hsp = Fourier_filter(hsp, threshold=1, scale=scale[1])
                except:
                    print("Device", hsp.device, "does not support the torch.fft.")
                    on_cpu_devices[hsp.device] = True
                    hsp = Fourier_filter(hsp.cpu(), threshold=1, scale=scale[1]).to(hsp.device)
            else:
                hsp = Fourier_filter(hsp.cpu(), threshold=1, scale=scale[1]).to(hsp.device)

        return h, hsp

    m = unet_patcher.clone()
    m.set_model_output_block_patch(output_block_patch)
    return m


class FreeUForForge(scripts.Script):
    sorting_priority = 12  # It will be the 12th item on UI.

    def title(self):
        return "FreeU Integrated"

    def show(self, is_img2img):
        # make this extension visible in both txt2img and img2img tab.
        return scripts.AlwaysVisible

    def ui(self, *args, **kwargs):
        with gr.Accordion(open=False, label=self.title()):
            freeu_enabled = gr.Checkbox(label='Enabled', value=False)
            freeu_b1 = gr.Slider(label='B1', minimum=0, maximum=2, step=0.01, value=1.01)
            freeu_b2 = gr.Slider(label='B2', minimum=0, maximum=2, step=0.01, value=1.02)
            freeu_s1 = gr.Slider(label='S1', minimum=0, maximum=4, step=0.01, value=0.99)
            freeu_s2 = gr.Slider(label='S2', minimum=0, maximum=4, step=0.01, value=0.95)

        return freeu_enabled, freeu_b1, freeu_b2, freeu_s1, freeu_s2

    def process_before_every_sampling(self, p, *script_args, **kwargs):
        # This will be called before every sampling.
        # If you use highres fix, this will be called twice.

        freeu_enabled, freeu_b1, freeu_b2, freeu_s1, freeu_s2 = script_args

        if not freeu_enabled:
            return

        unet = p.sd_model.forge_objects.unet

        unet = patch_freeu_v2(unet, freeu_b1, freeu_b2, freeu_s1, freeu_s2)

        p.sd_model.forge_objects.unet = unet

        # Below codes will add some logs to the texts below the image outputs on UI.
        # The extra_generation_params does not influence results.
        p.extra_generation_params.update(dict(
            freeu_enabled=freeu_enabled,
            freeu_b1=freeu_b1,
            freeu_b2=freeu_b2,
            freeu_s1=freeu_s1,
            freeu_s2=freeu_s2,
        ))

        return
</code></pre> 
<p>See also <a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/raw/main/backend/nn/unet.py">Forge's Unet Implementation</a>.</p> 
<h1>Under Construction</h1> 
<p>WebUI Forge is now under some constructions, and docs / UI / functionality may change with updates.</p>
]]></content:encoded>


</item>
<item>
<title>geekan/MetaGPT</title>
<link>https://github.com/geekan/MetaGPT</link>
<guid>https://github.com/geekan/MetaGPT</guid>
<content:encoded><![CDATA[
<div> 关键词：MetaGPT、软件公司、多代理系统、自然语言编程、LLM

总结：

MetaGPT是一款基于人工智能的软件公司模拟平台，旨在通过多代理系统实现复杂任务的协作完成。其核心理念是将“代码等于流程（SOP）”，即通过机器学习模型（LLM）来模拟软件开发公司中的各个角色和流程。MetaGPT支持用户通过一句需求描述生成项目规划、用户故事、竞争分析、数据结构、API设计等文档，甚至可以作为库直接调用，实现从概念到代码的自动化过程。

MetaGPT提供了一个开放源代码的框架，允许用户配置不同的LLM模型以扮演不同的角色，如产品经理、架构师、项目经理和工程师。这使得用户可以根据实际需求灵活地组合和调整AI代理的角色和能力。此外，MetaGPT还支持多种语言、多模态输入和增量开发功能，增强了其实用性和灵活性。

在发展过程中，MetaGPT不断更新和优化，增加了对RAG（阅读理解+代码生成）模块的支持，引入了更多强大的LLM模型，并通过集成不同的API和工具扩展了其功能。这些进展使得MetaGPT在软件开发领域展现了强大的潜力，成为了一个值得关注的开源项目。

MetaGPT不仅是一个技术创新的体现，也是自然语言处理和人工智能在软件工程领域的应用探索。它为开发者提供了全新的工作方式，有望加速软件开发流程并提升开发质量。 <div>
<p>🌟 The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming</p><hr /><h1>MetaGPT: The Multi-Agent Framework</h1> 
<p align="center"> <a href=""><img alt="MetaGPT logo: Enable GPT to work in software company, collaborating to tackle more complex tasks." src="https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/resources/MetaGPT-new-log.png" width="150px" /></a> </p> 
<p align="center"> <b>Assign different roles to GPTs to form a collaborative entity for complex tasks.</b> </p> 
<p align="center"> <a href="https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/README_CN.md"><img alt="CN doc" src="https://img.shields.io/badge/%E6%96%87%E6%A1%A3-%E4%B8%AD%E6%96%87%E7%89%88-blue.svg?sanitize=true" /></a> <a href="https://raw.githubusercontent.com/geekan/MetaGPT/main/README.md"><img alt="EN doc" src="https://img.shields.io/badge/document-English-blue.svg?sanitize=true" /></a> <a href="https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/README_JA.md"><img alt="JA doc" src="https://img.shields.io/badge/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88-%E6%97%A5%E6%9C%AC%E8%AA%9E-blue.svg?sanitize=true" /></a> <a href="https://opensource.org/licenses/MIT"><img alt="License: MIT" src="https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true" /></a> <a href="https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/ROADMAP.md"><img alt="roadmap" src="https://img.shields.io/badge/ROADMAP-%E8%B7%AF%E7%BA%BF%E5%9B%BE-blue" /></a> <a href="https://discord.gg/DYn29wFk9z"><img alt="Discord Follow" src="https://dcbadge.vercel.app/api/server/DYn29wFk9z?style=flat" /></a> <a href="https://twitter.com/MetaGPT_"><img alt="Twitter Follow" src="https://img.shields.io/twitter/follow/MetaGPT?style=social" /></a> </p> 
<p align="center"> <a href="https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/geekan/MetaGPT"><img alt="Open in Dev Containers" src="https://img.shields.io/static/v1?label=Dev%20Containers&amp;message=Open&amp;color=blue&amp;logo=visualstudiocode" /></a> <a href="https://codespaces.new/geekan/MetaGPT"><img alt="Open in GitHub Codespaces" src="https://img.shields.io/badge/Github_Codespace-Open-blue?logo=github" /></a> <a href="https://huggingface.co/spaces/deepwisdom/MetaGPT" target="_blank"><img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20-Hugging%20Face-blue?color=blue&amp;logoColor=white" /></a> </p> 
<h2>News</h2> 
<p>🚀 Mar. 29, 2024: <a href="https://github.com/geekan/MetaGPT/releases/tag/v0.8.0">v0.8.0</a> released. Now you can use Data Interpreter (<a href="https://arxiv.org/abs/2402.18679">arxiv</a>, <a href="https://docs.deepwisdom.ai/main/en/DataInterpreter/">example</a>, <a href="https://github.com/geekan/MetaGPT/tree/main/examples/di">code</a>) via pypi package import. Meanwhile, we integrated RAG module and supported multiple new LLMs.</p> 
<p>🚀 Feb. 08, 2024: <a href="https://github.com/geekan/MetaGPT/releases/tag/v0.7.0">v0.7.0</a> released, supporting assigning different LLMs to different Roles. We also introduced <a href="https://github.com/geekan/MetaGPT/raw/main/examples/di/README.md">Data Interpreter</a>, a powerful agent capable of solving a wide range of real-world problems.</p> 
<p>🚀 Jan. 16, 2024: Our paper <a href="https://openreview.net/forum?id=VtmBAGCN7o">MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework </a> accepted for <strong>oral presentation (top 1.2%)</strong> at ICLR 2024, <strong>ranking #1</strong> in the LLM-based Agent category.</p> 
<p>🚀 Jan. 03, 2024: <a href="https://github.com/geekan/MetaGPT/releases/tag/v0.6.0">v0.6.0</a> released, new features include serialization, upgraded OpenAI package and supported multiple LLM, provided <a href="https://github.com/geekan/MetaGPT/raw/main/examples/debate_simple.py">minimal example for debate</a> etc.</p> 
<p>🚀 Dec. 15, 2023: <a href="https://github.com/geekan/MetaGPT/releases/tag/v0.5.0">v0.5.0</a> released, introducing some experimental features such as incremental development, multilingual, multiple programming languages, etc.</p> 
<p>🔥 Nov. 08, 2023: MetaGPT is selected into <a href="https://www.benchcouncil.org/evaluation/opencs/annual.html">Open100: Top 100 Open Source achievements</a>.</p> 
<p>🔥 Sep. 01, 2023: MetaGPT tops GitHub Trending Monthly for the <strong>17th time</strong> in August 2023.</p> 
<p>🌟 Jun. 30, 2023: MetaGPT is now open source.</p> 
<p>🌟 Apr. 24, 2023: First line of MetaGPT code committed.</p> 
<h2>Software Company as Multi-Agent System</h2> 
<ol> 
 <li>MetaGPT takes a <strong>one line requirement</strong> as input and outputs <strong>user stories / competitive analysis / requirements / data structures / APIs / documents, etc.</strong></li> 
 <li>Internally, MetaGPT includes <strong>product managers / architects / project managers / engineers.</strong> It provides the entire process of a <strong>software company along with carefully orchestrated SOPs.</strong> 
  <ol> 
   <li><code>Code = SOP(Team)</code> is the core philosophy. We materialize SOP and apply it to teams composed of LLMs.</li> 
  </ol> </li> 
</ol> 
<p><img alt="A software company consists of LLM-based roles" src="https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/resources/software_company_cd.jpeg" /></p> 
<p align="center">Software Company Multi-Agent Schematic (Gradually Implementing)</p> 
<h2>Get Started</h2> 
<h3>Installation</h3> 
<blockquote> 
 <p>Ensure that Python 3.9+ is installed on your system. You can check this by using: <code>python --version</code>.<br /> You can use conda like this: <code>conda create -n metagpt python=3.9 &amp;&amp; conda activate metagpt</code></p> 
</blockquote> 
<pre><code class="language-bash">pip install --upgrade metagpt
# or `pip install --upgrade git+https://github.com/geekan/MetaGPT.git`
# or `git clone https://github.com/geekan/MetaGPT &amp;&amp; cd MetaGPT &amp;&amp; pip install --upgrade -e .`
</code></pre> 
<p>For detailed installation guidance, please refer to <a href="https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-stable-version">cli_install</a> or <a href="https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-with-docker">docker_install</a></p> 
<h3>Configuration</h3> 
<p>You can init the config of MetaGPT by running the following command, or manually create <code>~/.metagpt/config2.yaml</code> file:</p> 
<pre><code class="language-bash"># Check https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html for more details
metagpt --init-config  # it will create ~/.metagpt/config2.yaml, just modify it to your needs
</code></pre> 
<p>You can configure <code>~/.metagpt/config2.yaml</code> according to the <a href="https://github.com/geekan/MetaGPT/raw/main/config/config2.example.yaml">example</a> and <a href="https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html">doc</a>:</p> 
<pre><code class="language-yaml">llm:
  api_type: "openai"  # or azure / ollama / groq etc. Check LLMType for more options
  model: "gpt-4-turbo"  # or gpt-3.5-turbo
  base_url: "https://api.openai.com/v1"  # or forward url / other llm url
  api_key: "YOUR_API_KEY"
</code></pre> 
<h3>Usage</h3> 
<p>After installation, you can use MetaGPT at CLI</p> 
<pre><code class="language-bash">metagpt "Create a 2048 game"  # this will create a repo in ./workspace
</code></pre> 
<p>or use it as library</p> 
<pre><code class="language-python">from metagpt.software_company import generate_repo, ProjectRepo
repo: ProjectRepo = generate_repo("Create a 2048 game")  # or ProjectRepo("&lt;path&gt;")
print(repo)  # it will print the repo structure with files
</code></pre> 
<p>You can also use <a href="https://github.com/geekan/MetaGPT/tree/main/examples/di">Data Interpreter</a> to write code:</p> 
<pre><code class="language-python">import asyncio
from metagpt.roles.di.data_interpreter import DataInterpreter

async def main():
    di = DataInterpreter()
    await di.run("Run data analysis on sklearn Iris dataset, include a plot")

asyncio.run(main())  # or await main() in a jupyter notebook setting
</code></pre> 
<h3>QuickStart &amp; Demo Video</h3> 
<ul> 
 <li>Try it on <a href="https://huggingface.co/spaces/deepwisdom/MetaGPT">MetaGPT Huggingface Space</a></li> 
 <li><a href="https://youtu.be/uT75J_KG_aY">Matthew Berman: How To Install MetaGPT - Build A Startup With One Prompt!!</a></li> 
 <li><a href="https://github.com/geekan/MetaGPT/assets/2707039/5e8c1062-8c35-440f-bb20-2b0320f8d27d">Official Demo Video</a></li> 
</ul> 
<p><a href="https://github.com/geekan/MetaGPT/assets/34952977/34345016-5d13-489d-b9f9-b82ace413419">https://github.com/geekan/MetaGPT/assets/34952977/34345016-5d13-489d-b9f9-b82ace413419</a></p> 
<h2>Tutorial</h2> 
<ul> 
 <li>🗒 <a href="https://docs.deepwisdom.ai/main/en/">Online Document</a></li> 
 <li>💻 <a href="https://docs.deepwisdom.ai/main/en/guide/get_started/quickstart.html">Usage</a></li> 
 <li>🔎 <a href="https://docs.deepwisdom.ai/main/en/guide/get_started/introduction.html">What can MetaGPT do?</a></li> 
 <li>🛠 How to build your own agents? 
  <ul> 
   <li><a href="https://docs.deepwisdom.ai/main/en/guide/tutorials/agent_101.html">MetaGPT Usage &amp; Development Guide | Agent 101</a></li> 
   <li><a href="https://docs.deepwisdom.ai/main/en/guide/tutorials/multi_agent_101.html">MetaGPT Usage &amp; Development Guide | MultiAgent 101</a></li> 
  </ul> </li> 
 <li>🧑‍💻 Contribution 
  <ul> 
   <li><a href="https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/ROADMAP.md">Develop Roadmap</a></li> 
  </ul> </li> 
 <li>🔖 Use Cases 
  <ul> 
   <li><a href="https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/interpreter/intro.html">Data Interpreter</a></li> 
   <li><a href="https://docs.deepwisdom.ai/main/en/guide/use_cases/multi_agent/debate.html">Debate</a></li> 
   <li><a href="https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/researcher.html">Researcher</a></li> 
   <li><a href="https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/receipt_assistant.html">Recepit Assistant</a></li> 
  </ul> </li> 
 <li>❓ <a href="https://docs.deepwisdom.ai/main/en/guide/faq.html">FAQs</a></li> 
</ul> 
<h2>Support</h2> 
<h3>Discord Join US</h3> 
<p>📢 Join Our <a href="https://discord.gg/ZRHeExS6xv">Discord Channel</a>! Looking forward to seeing you there! 🎉</p> 
<h3>Contributor form</h3> 
<p>📝 <a href="https://airtable.com/appInfdG0eJ9J4NNL/pagK3Fh1sGclBvVkV/form">Fill out the form</a> to become a contributor. We are looking forward to your participation!</p> 
<h3>Contact Information</h3> 
<p>If you have any questions or feedback about this project, please feel free to contact us. We highly appreciate your suggestions!</p> 
<ul> 
 <li><strong>Email:</strong> <a href="mailto:alexanderwu@deepwisdom.ai">alexanderwu@deepwisdom.ai</a></li> 
 <li><strong>GitHub Issues:</strong> For more technical inquiries, you can also create a new issue in our <a href="https://github.com/geekan/metagpt/issues">GitHub repository</a>.</li> 
</ul> 
<p>We will respond to all questions within 2-3 business days.</p> 
<h2>Citation</h2> 
<p>To stay updated with the latest research and development, follow <a href="https://twitter.com/MetaGPT_">@MetaGPT_</a> on Twitter.</p> 
<p>To cite <a href="https://openreview.net/forum?id=VtmBAGCN7o">MetaGPT</a> or <a href="https://arxiv.org/abs/2402.18679">Data Interpreter</a> in publications, please use the following BibTeX entries.</p> 
<pre><code class="language-bibtex">@inproceedings{hong2024metagpt,
      title={Meta{GPT}: Meta Programming for A Multi-Agent Collaborative Framework},
      author={Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Jinlin Wang and Ceyao Zhang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and J{\"u}rgen Schmidhuber},
      booktitle={The Twelfth International Conference on Learning Representations},
      year={2024},
      url={https://openreview.net/forum?id=VtmBAGCN7o}
}
@misc{hong2024data,
      title={Data Interpreter: An LLM Agent For Data Science}, 
      author={Sirui Hong and Yizhang Lin and Bang Liu and Bangbang Liu and Binhao Wu and Danyang Li and Jiaqi Chen and Jiayi Zhang and Jinlin Wang and Li Zhang and Lingyao Zhang and Min Yang and Mingchen Zhuge and Taicheng Guo and Tuo Zhou and Wei Tao and Wenyi Wang and Xiangru Tang and Xiangtao Lu and Xiawu Zheng and Xinbing Liang and Yaying Fei and Yuheng Cheng and Zongze Xu and Chenglin Wu},
      year={2024},
      eprint={2402.18679},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
</code></pre>
]]></content:encoded>


</item>

</channel>
</rss>