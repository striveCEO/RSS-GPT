<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>GitHub All Languages Weekly Trending</title>
<link>http://mshibanami.github.io/GitHubTrendingRSS</link>

<item>
<title>thuml/Time-Series-Library</title>
<link>https://github.com/thuml/Time-Series-Library</link>
<guid>https://github.com/thuml/Time-Series-Library</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šTSLibã€æ—¶é—´åºåˆ—åˆ†æã€æ·±åº¦å­¦ä¹ ã€æ¨¡å‹è¯„ä¼°ã€å¼€æºåº“

æ€»ç»“ï¼š

TSLibæ˜¯ä¸€ä¸ªé¢å‘æ·±åº¦å­¦ä¹ ç ”ç©¶äººå‘˜çš„æ—¶é—´åºåˆ—åˆ†æå¼€æºåº“ã€‚å®ƒæä¾›äº†ä¸€ä¸ªå¹²å‡€çš„ä»£ç åŸºç¡€ï¼Œç”¨äºè¯„ä¼°å…ˆè¿›çš„æ·±åº¦æ—¶é—´åºåˆ—æ¨¡å‹æˆ–å¼€å‘è‡ªå·±çš„æ¨¡å‹ï¼Œæ¶µç›–äº†é•¿æœŸå’ŒçŸ­æœŸé¢„æµ‹ã€ç¼ºå¤±å€¼å¡«å……ã€å¼‚å¸¸æ£€æµ‹å’Œåˆ†ç±»ç­‰äº”å¤§ä¸»æµä»»åŠ¡ã€‚è¯¥åº“çš„æœ€æ–°æ–°é—»åŒ…æ‹¬æ–°å¢äº†æµè¡Œåºåˆ—æ¨¡å‹mambaå’Œæ”¹è¿›çš„é•¿çŸ­æœŸé¢„æµ‹æ–¹æ³•iTransformerç­‰ã€‚æ­¤å¤–ï¼ŒTSLibè¿˜æä¾›äº†è¯¦ç»†çš„ä½¿ç”¨æŒ‡å—ã€å®éªŒè„šæœ¬ä»¥åŠä¸æ—¶é—´åºåˆ—åˆ†æç›¸å…³çš„åŸºå‡†æµ‹è¯•ã€‚

TSLibçš„ç”¨æˆ·å¯ä»¥åˆ©ç”¨è¿™ä¸ªç»¼åˆæ€§çš„åŸºå‡†å’Œä»£ç åº“æ¥è®­ç»ƒå’Œè¯„ä¼°æ—¶é—´åºåˆ—æ¨¡å‹ï¼Œé€šè¿‡è¿è¡Œå®éªŒè„šæœ¬æ¥å¤ç°ç ”ç©¶ç»“æœã€‚å¯¹äºé‚£äº›å¸Œæœ›è´¡çŒ®æ–°æ¨¡å‹çš„ç ”ç©¶äººå‘˜ï¼ŒTSLibä¹Ÿæ¬¢è¿ä»–ä»¬æäº¤è®ºæ–‡é“¾æ¥æˆ–å‘èµ·Pullè¯·æ±‚ï¼Œä»¥æ›´æ–°æ¨¡å‹åˆ—è¡¨å’Œæ’è¡Œæ¦œã€‚ä¸ºäº†ä¿ƒè¿›ç ”ç©¶ï¼ŒTSLibè¿˜å‘å¸ƒäº†ä¸€ç¯‡å…¨é¢çš„ç»¼è¿°è®ºæ–‡ï¼Œå¯¹å½“å‰æ”¯æŒçš„æ¨¡å‹è¿›è¡Œäº†æ·±å…¥çš„å®éªŒåˆ†æå’Œè®¾è®¡åŸåˆ™æ€»ç»“ã€‚æœ€ç»ˆï¼ŒTSLibå¾—åˆ°äº†å›½å®¶å…³é”®ç ”å‘é¡¹ç›®çš„èµ„é‡‘æ”¯æŒï¼Œå¹¶åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ã€å¼‚å¸¸æ£€æµ‹ã€åˆ†ç±»å’Œç›¸å…³é¢†åŸŸæ‹¥æœ‰ä¸°å¯Œçš„å®éªŒæ•°æ®é›†æ¥æºã€‚

æ­¤åº“ä¸ºæ·±åº¦æ—¶é—´åºåˆ—åˆ†ææä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·é›†åˆï¼Œæ—¨åœ¨ä¿ƒè¿›å­¦æœ¯ç ”ç©¶å’Œåº”ç”¨å¼€å‘ã€‚ <div>
<p>A Library for Advanced Deep Time Series Models.</p><hr /><h1>Time Series Library (TSLib)</h1> 
<p>TSLib is an open-source library for deep learning researchers, especially for deep time series analysis.</p> 
<p>We provide a neat code base to evaluate advanced deep time series models or develop your model, which covers five mainstream tasks: <strong>long- and short-term forecasting, imputation, anomaly detection, and classification.</strong></p> 
<p><span>ğŸš©</span><strong>News</strong> (2024.07) We wrote a comprehensive survey of <a href="https://arxiv.org/abs/2407.13278">[Deep Time Series Models]</a> with a rigorous benchmark based on TSLib. In this paper, we summarized the design principles of current time series models supported by insightful experiments, hoping to be helpful to future research.</p> 
<p><span>ğŸš©</span><strong>News</strong> (2024.04) Many thanks for the great work from <a href="https://github.com/thuml/Time-Series-Library/pull/378">frecklebars</a>. The famous sequential model <a href="https://arxiv.org/abs/2312.00752">Mamba</a> has been included in our library. See <a href="https://github.com/thuml/Time-Series-Library/raw/main/models/Mamba.py">this file</a>, where you need to install <code>mamba_ssm</code> with pip at first.</p> 
<p><span>ğŸš©</span><strong>News</strong> (2024.03) Given the inconsistent look-back length of various papers, we split the long-term forecasting in the leaderboard into two categories: Look-Back-96 and Look-Back-Searching. We recommend researchers read <a href="https://openreview.net/pdf?id=7oLshfEIC2">TimeMixer</a>, which includes both look-back length settings in experiments for scientific rigor.</p> 
<p><span>ğŸš©</span><strong>News</strong> (2023.10) We add an implementation to <a href="https://arxiv.org/abs/2310.06625">iTransformer</a>, which is the state-of-the-art model for long-term forecasting. The official code and complete scripts of iTransformer can be found <a href="https://github.com/thuml/iTransformer">here</a>.</p> 
<p><span>ğŸš©</span><strong>News</strong> (2023.09) We added a detailed <a href="https://github.com/thuml/Time-Series-Library/raw/main/tutorial/TimesNet_tutorial.ipynb">tutorial</a> for <a href="https://openreview.net/pdf?id=ju_Uqw384Oq">TimesNet</a> and this library, which is quite friendly to beginners of deep time series analysis.</p> 
<p><span>ğŸš©</span><strong>News</strong> (2023.02) We release the TSlib as a comprehensive benchmark and code base for time series models, which is extended from our previous GitHub repository <a href="https://github.com/thuml/Autoformer">Autoformer</a>.</p> 
<h2>Leaderboard for Time Series Analysis</h2> 
<p>Till March 2024, the top three models for five different tasks are:</p> 
<table> 
 <thead> 
  <tr> 
   <th>Model<br />Ranking</th> 
   <th>Long-term<br />Forecasting<br />Look-Back-96</th> 
   <th>Long-term<br />Forecasting<br />Look-Back-Searching</th> 
   <th>Short-term<br />Forecasting</th> 
   <th>Imputation</th> 
   <th>Classification</th> 
   <th>Anomaly<br />Detection</th> 
  </tr> 
 </thead> 
 <tbody> 
  <tr> 
   <td>ğŸ¥‡ 1st</td> 
   <td><a href="https://arxiv.org/abs/2310.06625">iTransformer</a></td> 
   <td><a href="https://openreview.net/pdf?id=7oLshfEIC2">TimeMixer</a></td> 
   <td><a href="https://arxiv.org/abs/2210.02186">TimesNet</a></td> 
   <td><a href="https://arxiv.org/abs/2210.02186">TimesNet</a></td> 
   <td><a href="https://arxiv.org/abs/2210.02186">TimesNet</a></td> 
   <td><a href="https://arxiv.org/abs/2210.02186">TimesNet</a></td> 
  </tr> 
  <tr> 
   <td>ğŸ¥ˆ 2nd</td> 
   <td><a href="https://openreview.net/pdf?id=7oLshfEIC2">TimeMixer</a></td> 
   <td><a href="https://github.com/yuqinie98/PatchTST">PatchTST</a></td> 
   <td><a href="https://github.com/thuml/Nonstationary_Transformers">Non-stationary<br />Transformer</a></td> 
   <td><a href="https://github.com/thuml/Nonstationary_Transformers">Non-stationary<br />Transformer</a></td> 
   <td><a href="https://github.com/thuml/Nonstationary_Transformers">Non-stationary<br />Transformer</a></td> 
   <td><a href="https://github.com/MAZiqing/FEDformer">FEDformer</a></td> 
  </tr> 
  <tr> 
   <td>ğŸ¥‰ 3rd</td> 
   <td><a href="https://arxiv.org/abs/2210.02186">TimesNet</a></td> 
   <td><a href="https://arxiv.org/pdf/2205.13504.pdf">DLinear</a></td> 
   <td><a href="https://github.com/MAZiqing/FEDformer">FEDformer</a></td> 
   <td><a href="https://github.com/thuml/Autoformer">Autoformer</a></td> 
   <td><a href="https://github.com/zhouhaoyi/Informer2020">Informer</a></td> 
   <td><a href="https://github.com/thuml/Autoformer">Autoformer</a></td> 
  </tr> 
 </tbody> 
</table> 
<p><strong>Note: We will keep updating this leaderboard.</strong> If you have proposed advanced and awesome models, you can send us your paper/code link or raise a pull request. We will add them to this repo and update the leaderboard as soon as possible.</p> 
<p><strong>Compared models of this leaderboard.</strong> â˜‘ means that their codes have already been included in this repo.</p> 
<ul> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>TimeMixer</strong> - TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting <a href="https://openreview.net/pdf?id=7oLshfEIC2">[ICLR 2024]</a> <a href="https://github.com/thuml/Time-Series-Library/raw/main/models/TimeMixer.py">[Code]</a>.</li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>TSMixer</strong> - TSMixer: An All-MLP Architecture for Time Series Forecasting <a href="https://arxiv.org/pdf/2303.06053.pdf">[arXiv 2023]</a> <a href="https://github.com/thuml/Time-Series-Library/raw/main/models/TSMixer.py">[Code]</a></li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>iTransformer</strong> - iTransformer: Inverted Transformers Are Effective for Time Series Forecasting <a href="https://arxiv.org/abs/2310.06625">[ICLR 2024]</a> <a href="https://github.com/thuml/Time-Series-Library/raw/main/models/iTransformer.py">[Code]</a>.</li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>PatchTST</strong> - A Time Series is Worth 64 Words: Long-term Forecasting with Transformers <a href="https://openreview.net/pdf?id=Jbdc0vTOcol">[ICLR 2023]</a> <a href="https://github.com/thuml/Time-Series-Library/raw/main/models/PatchTST.py">[Code]</a>.</li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>TimesNet</strong> - TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis <a href="https://openreview.net/pdf?id=ju_Uqw384Oq">[ICLR 2023]</a> <a href="https://github.com/thuml/Time-Series-Library/raw/main/models/TimesNet.py">[Code]</a>.</li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>DLinear</strong> - Are Transformers Effective for Time Series Forecasting? <a href="https://arxiv.org/pdf/2205.13504.pdf">[AAAI 2023]</a> <a href="https://github.com/thuml/Time-Series-Library/raw/main/models/DLinear.py">[Code]</a>.</li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>LightTS</strong> - Less Is More: Fast Multivariate Time Series Forecasting with Light Sampling-oriented MLP Structures <a href="https://arxiv.org/abs/2207.01186">[arXiv 2022]</a> <a href="https://github.com/thuml/Time-Series-Library/raw/main/models/LightTS.py">[Code]</a>.</li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>ETSformer</strong> - ETSformer: Exponential Smoothing Transformers for Time-series Forecasting <a href="https://arxiv.org/abs/2202.01381">[arXiv 2022]</a> <a href="https://github.com/thuml/Time-Series-Library/raw/main/models/ETSformer.py">[Code]</a>.</li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>Non-stationary Transformer</strong> - Non-stationary Transformers: Exploring the Stationarity in Time Series Forecasting <a href="https://openreview.net/pdf?id=ucNDIDRNjjv">[NeurIPS 2022]</a> <a href="https://github.com/thuml/Time-Series-Library/raw/main/models/Nonstationary_Transformer.py">[Code]</a>.</li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>FEDformer</strong> - FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting <a href="https://proceedings.mlr.press/v162/zhou22g.html">[ICML 2022]</a> <a href="https://github.com/thuml/Time-Series-Library/raw/main/models/FEDformer.py">[Code]</a>.</li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>Pyraformer</strong> - Pyraformer: Low-complexity Pyramidal Attention for Long-range Time Series Modeling and Forecasting <a href="https://openreview.net/pdf?id=0EXmFzUn5I">[ICLR 2022]</a> <a href="https://github.com/thuml/Time-Series-Library/raw/main/models/Pyraformer.py">[Code]</a>.</li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>Autoformer</strong> - Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting <a href="https://openreview.net/pdf?id=I55UqU-M11y">[NeurIPS 2021]</a> <a href="https://github.com/thuml/Time-Series-Library/raw/main/models/Autoformer.py">[Code]</a>.</li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>Informer</strong> - Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting <a href="https://ojs.aaai.org/index.php/AAAI/article/view/17325/17132">[AAAI 2021]</a> <a href="https://github.com/thuml/Time-Series-Library/raw/main/models/Informer.py">[Code]</a>.</li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>Reformer</strong> - Reformer: The Efficient Transformer <a href="https://openreview.net/forum?id=rkgNKkHtvB">[ICLR 2020]</a> <a href="https://github.com/thuml/Time-Series-Library/raw/main/models/Reformer.py">[Code]</a>.</li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>Transformer</strong> - Attention is All You Need <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">[NeurIPS 2017]</a> <a href="https://github.com/thuml/Time-Series-Library/raw/main/models/Transformer.py">[Code]</a>.</li> 
</ul> 
<p>See our latest paper <a href="https://arxiv.org/abs/2210.02186">[TimesNet]</a> for the comprehensive benchmark. We will release a real-time updated online version soon.</p> 
<p><strong>Newly added baselines.</strong> We will add them to the leaderboard after a comprehensive evaluation.</p> 
<ul> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>Mamba</strong> - Mamba: Linear-Time Sequence Modeling with Selective State Spaces <a href="https://arxiv.org/abs/2312.00752">[arXiv 2023]</a> <a href="https://github.com/thuml/Time-Series-Library/raw/main/models/Mamba.py">[Code]</a></li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>SegRNN</strong> - SegRNN: Segment Recurrent Neural Network for Long-Term Time Series Forecasting <a href="https://arxiv.org/abs/2308.11200.pdf">[arXiv 2023]</a> <a href="https://github.com/thuml/Time-Series-Library/raw/main/models/SegRNN.py">[Code]</a>.</li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>Koopa</strong> - Koopa: Learning Non-stationary Time Series Dynamics with Koopman Predictors <a href="https://arxiv.org/pdf/2305.18803.pdf">[NeurIPS 2023]</a> <a href="https://github.com/thuml/Time-Series-Library/raw/main/models/Koopa.py">[Code]</a>.</li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>FreTS</strong> - Frequency-domain MLPs are More Effective Learners in Time Series Forecasting <a href="https://arxiv.org/pdf/2311.06184.pdf">[NeurIPS 2023]</a> <a href="https://github.com/thuml/Time-Series-Library/raw/main/models/FreTS.py">[Code]</a>.</li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>TiDE</strong> - Long-term Forecasting with TiDE: Time-series Dense Encoder <a href="https://arxiv.org/pdf/2304.08424.pdf">[arXiv 2023]</a> <a href="https://github.com/thuml/Time-Series-Library/raw/main/models/TiDE.py">[Code]</a>.</li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>FiLM</strong> - FiLM: Frequency improved Legendre Memory Model for Long-term Time Series Forecasting <a href="https://openreview.net/forum?id=zTQdHSQUQWc">[NeurIPS 2022]</a><a href="https://github.com/thuml/Time-Series-Library/raw/main/models/FiLM.py">[Code]</a>.</li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>MICN</strong> - MICN: Multi-scale Local and Global Context Modeling for Long-term Series Forecasting <a href="https://openreview.net/pdf?id=zt53IDUR1U">[ICLR 2023]</a><a href="https://github.com/thuml/Time-Series-Library/raw/main/models/MICN.py">[Code]</a>.</li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>Crossformer</strong> - Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting <a href="https://openreview.net/pdf?id=vSVLM2j9eie">[ICLR 2023]</a><a href="https://github.com/thuml/Time-Series-Library/raw/main/models/Crossformer.py">[Code]</a>.</li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> <strong>TFT</strong> - Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting <a href="https://arxiv.org/abs/1912.09363">[arXiv 2019]</a><a href="https://github.com/thuml/Time-Series-Library/raw/main/models/TemporalFusionTransformer.py">[Code]</a>.</li> 
</ul> 
<h2>Usage</h2> 
<ol> 
 <li>Install Python 3.8. For convenience, execute the following command.</li> 
</ol> 
<pre><code>pip install -r requirements.txt
</code></pre> 
<ol start="2"> 
 <li>Prepare Data. You can obtain the well pre-processed datasets from <a href="https://drive.google.com/drive/folders/13Cg1KYOlzM5C7K8gK8NfC-F3EYxkM3D2?usp=sharing">[Google Drive]</a> or&nbsp;<a href="https://pan.baidu.com/s/1r3KhGd0Q9PJIUZdfEYoymg?pwd=i9iy">[Baidu Drive]</a>, Then place the downloaded data in the folder<code>./dataset</code>. Here is a summary of supported datasets.</li> 
</ol> 
<p align="center"> <img align="center" alt="" height="200" src="https://raw.githubusercontent.com/thuml/Time-Series-Library/main/.%5Cpic%5Cdataset.png" /> </p> 
<ol start="3"> 
 <li>Train and evaluate model. We provide the experiment scripts for all benchmarks under the folder <code>./scripts/</code>. You can reproduce the experiment results as the following examples:</li> 
</ol> 
<pre><code># long-term forecast
bash ./scripts/long_term_forecast/ETT_script/TimesNet_ETTh1.sh
# short-term forecast
bash ./scripts/short_term_forecast/TimesNet_M4.sh
# imputation
bash ./scripts/imputation/ETT_script/TimesNet_ETTh1.sh
# anomaly detection
bash ./scripts/anomaly_detection/PSM/TimesNet.sh
# classification
bash ./scripts/classification/TimesNet.sh
</code></pre> 
<ol start="4"> 
 <li>Develop your own model.</li> 
</ol> 
<ul> 
 <li>Add the model file to the folder <code>./models</code>. You can follow the <code>./models/Transformer.py</code>.</li> 
 <li>Include the newly added model in the <code>Exp_Basic.model_dict</code> of <code>./exp/exp_basic.py</code>.</li> 
 <li>Create the corresponding scripts under the folder <code>./scripts</code>.</li> 
</ul> 
<p>Note: The original code for the classification task can be found <a href="https://github.com/thuml/Flowformer/tree/main/Flowformer_TimeSeries">here</a>. It is hard to fuse all five tasks in one library. We are still working on this task.</p> 
<h2>Citation</h2> 
<p>If you find this repo useful, please cite our paper.</p> 
<pre><code>@inproceedings{wu2023timesnet,
  title={TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis},
  author={Haixu Wu and Tengge Hu and Yong Liu and Hang Zhou and Jianmin Wang and Mingsheng Long},
  booktitle={International Conference on Learning Representations},
  year={2023},
}

@article{wang2024tssurvey,
  title={Deep Time Series Models: A Comprehensive Survey and Benchmark},
  author={Yuxuan Wang and Haixu Wu and Jiaxiang Dong and Yong Liu and Mingsheng Long and Jianmin Wang},
  booktitle={arXiv preprint arXiv:2407.13278},
  year={2024},
}
</code></pre> 
<h2>Contact</h2> 
<p>If you have any questions or suggestions, feel free to contact our maintenance team:</p> 
<p>Current:</p> 
<ul> 
 <li>Haixu Wu (Ph.D. student, <a href="mailto:wuhx23@mails.tsinghua.edu.cn">wuhx23@mails.tsinghua.edu.cn</a>)</li> 
 <li>Yong Liu (Ph.D. student, <a href="mailto:liuyong21@mails.tsinghua.edu.cn">liuyong21@mails.tsinghua.edu.cn</a>)</li> 
 <li>Yuxuan Wang (Ph.D. student, <a href="mailto:wangyuxu22@mails.tsinghua.edu.cn">wangyuxu22@mails.tsinghua.edu.cn</a>)</li> 
 <li>Huikun Weng (Undergraduate, <a href="mailto:wenghk22@mails.tsinghua.edu.cn">wenghk22@mails.tsinghua.edu.cn</a>)</li> 
</ul> 
<p>Previous:</p> 
<ul> 
 <li>Tengge Hu (Master student, <a href="mailto:htg21@mails.tsinghua.edu.cn">htg21@mails.tsinghua.edu.cn</a>)</li> 
 <li>Haoran Zhang (Master student, <a href="mailto:z-hr20@mails.tsinghua.edu.cn">z-hr20@mails.tsinghua.edu.cn</a>)</li> 
 <li>Jiawei Guo (Undergraduate, <a href="mailto:guo-jw21@mails.tsinghua.edu.cn">guo-jw21@mails.tsinghua.edu.cn</a>)</li> 
</ul> 
<p>Or describe it in Issues.</p> 
<h2>Acknowledgement</h2> 
<p>This project is supported by the National Key R&amp;D Program of China (2021YFB1715200).</p> 
<p>This library is constructed based on the following repos:</p> 
<ul> 
 <li> <p>Forecasting: <a href="https://github.com/thuml/Autoformer">https://github.com/thuml/Autoformer</a>.</p> </li> 
 <li> <p>Anomaly Detection: <a href="https://github.com/thuml/Anomaly-Transformer">https://github.com/thuml/Anomaly-Transformer</a>.</p> </li> 
 <li> <p>Classification: <a href="https://github.com/thuml/Flowformer">https://github.com/thuml/Flowformer</a>.</p> </li> 
</ul> 
<p>All the experiment datasets are public, and we obtain them from the following links:</p> 
<ul> 
 <li> <p>Long-term Forecasting and Imputation: <a href="https://github.com/thuml/Autoformer">https://github.com/thuml/Autoformer</a>.</p> </li> 
 <li> <p>Short-term Forecasting: <a href="https://github.com/ServiceNow/N-BEATS">https://github.com/ServiceNow/N-BEATS</a>.</p> </li> 
 <li> <p>Anomaly Detection: <a href="https://github.com/thuml/Anomaly-Transformer">https://github.com/thuml/Anomaly-Transformer</a>.</p> </li> 
 <li> <p>Classification: <a href="https://www.timeseriesclassification.com/">https://www.timeseriesclassification.com/</a>.</p> </li> 
</ul> 
<h2>All Thanks To Our Contributors</h2> 
<a href="https://github.com/thuml/Time-Series-Library/graphs/contributors"> <img src="https://contrib.rocks/image?repo=thuml/Time-Series-Library" /> </a>
]]></content:encoded>


</item>
<item>
<title>danielmiessler/fabric</title>
<link>https://github.com/danielmiessler/fabric</link>
<guid>https://github.com/danielmiessler/fabric</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šfabricã€AIã€äººç±»è¾…åŠ©ã€æ¨¡å—åŒ–æ¡†æ¶ã€Goè¯­è¨€

æ€»ç»“ï¼š
fabric æ˜¯ä¸€ä¸ªå¼€æºçš„äººç±»è¾…åŠ©æ¡†æ¶ï¼Œåˆ©ç”¨ AI æä¾›äº†ä¸€ä¸ªæ¨¡å—åŒ–çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿé’ˆå¯¹ç‰¹å®šé—®é¢˜ä½¿ç”¨ä¼—åŒ…çš„ AI æç¤ºæ¥è§£å†³å…·ä½“ä»»åŠ¡ã€‚å…¶æ ¸å¿ƒç†å¿µæ˜¯å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªå¯å•ç‹¬å¤„ç†çš„éƒ¨åˆ†ï¼Œå¹¶é€šè¿‡é›†æˆä¸åŒçš„ AI æç¤ºæ¥å®ç°è‡ªåŠ¨åŒ–å¤„ç†ã€‚fabric æ”¯æŒç”¨æˆ·æ”¶é›†å’Œæ•´åˆå„ç§æœ‰ç”¨çš„ AI æç¤ºï¼ˆç§°ä¸ºâ€œæ¨¡å¼â€ï¼‰ï¼Œä»¥é€‚åº”æ—¥å¸¸ç”Ÿæ´»çš„ä¸åŒåœºæ™¯ï¼Œå¦‚æå–è§†é¢‘å’Œæ’­å®¢ç²¾åã€æ’°å†™è‡ªå®šä¹‰é£æ ¼çš„æ–‡ç« ã€æ€»ç»“å­¦æœ¯è®ºæ–‡ç­‰ã€‚

åœ¨å®‰è£…æ–¹é¢ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œç›´æ¥ä»ä»“åº“å®‰è£…å¹¶è¿è¡Œ setup å‘½ä»¤è¿›è¡Œåˆå§‹åŒ–é…ç½®ã€‚ä¸ºäº†ç¡®ä¿ä¸æ–°ç‰ˆæœ¬çš„å…¼å®¹æ€§ï¼Œéœ€è¦åœ¨è¿ç§»è‡³ Go è¯­è¨€ç‰ˆæœ¬æ—¶å¸è½½æ—§ç‰ˆ Python ç‰ˆæœ¬å¹¶é‡æ–°å®‰è£… Go ç‰ˆæœ¬çš„ fabricã€‚

åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­ï¼Œç”¨æˆ·å¯ä»¥è°ƒç”¨ç‰¹å®šçš„æ¨¡å¼æ¥æ‰§è¡Œæ‰€éœ€ä»»åŠ¡ï¼Œå¹¶é€šè¿‡è®¾ç½®å‚æ•°å¦‚æ¸©åº¦ã€é¡¶éƒ¨æ¦‚ç‡ç­‰æ¥è°ƒæ•´ AI çš„å“åº”ã€‚fabric è¿˜æä¾›äº†ä¸€äº›è¾…åŠ©å·¥å…·ï¼Œå¦‚ ytï¼Œç”¨äºä» YouTube è·å–è§†é¢‘è½¬å½•å†…å®¹ï¼Œä»¥ä¾¿äºåç»­çš„ AI å¤„ç†ã€‚

æ­¤å¤–ï¼Œfabric å¼ºè°ƒäº†ç”¨æˆ·è‡ªå®šä¹‰æ¨¡å¼çš„èƒ½åŠ›ï¼Œå…è®¸ç”¨æˆ·åˆ›å»ºç§æœ‰æˆ–å…¬å¼€å…±äº«çš„æ¨¡å¼ï¼Œä»¥é€‚åº”ä¸ªæ€§åŒ–éœ€æ±‚æˆ–ç¤¾åŒºåˆ†äº«ã€‚é€šè¿‡ä¸å¤šç§ AI æ¨¡å‹ï¼ˆå¦‚ openai å’Œ ollamaï¼‰å…¼å®¹ï¼Œfabric æ‰©å±•äº† AI åº”ç”¨çš„çµæ´»æ€§å’ŒèŒƒå›´ã€‚ <div>
<p>fabric is an open-source framework for augmenting humans using AI. It provides a modular framework for solving specific problems using a crowdsourced set of AI prompts that can be used anywhere.</p><hr /><div align="center"> 
 <img alt="fabriclogo" height="400" src="https://raw.githubusercontent.com/danielmiessler/fabric/main/images/fabric-logo-gif.gif" width="400" /> 
 <h1><code>fabric</code></h1> 
 <p><img alt="Static Badge" src="https://img.shields.io/badge/mission-human_flourishing_via_AI_augmentation-purple" /> <br /> <img alt="GitHub top language" src="https://img.shields.io/github/languages/top/danielmiessler/fabric" /> <img alt="GitHub last commit" src="https://img.shields.io/github/last-commit/danielmiessler/fabric" /> <a href="https://opensource.org/licenses/MIT"><img alt="License: MIT" src="https://img.shields.io/badge/License-MIT-green.svg?sanitize=true" /></a></p> 
 <p class="align center"> </p>
 <h4><code>fabric</code> is an open-source framework for augmenting humans using AI.</h4> 
 <p></p> 
 <p><a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#whatandwhy">What and Why</a> â€¢ <a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#philosophy">Philosophy</a> â€¢ <a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#Installation">Installation</a> â€¢ <a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#Usage">Usage</a> â€¢ <a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#examples">Examples</a> â€¢ <a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#just-use-the-patterns">Just Use the Patterns</a> â€¢ <a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#custom-patterns">Custom Patterns</a> â€¢ <a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#helper-apps">Helper Apps</a> â€¢ <a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#meta">Meta</a></p> 
</div> 
<h2>Navigation</h2> 
<ul> 
 <li><a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#what-and-why">What and Why</a></li> 
 <li><a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#philosophy">Philosophy</a> 
  <ul> 
   <li><a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#breaking-problems-into-components">Breaking problems into components</a></li> 
   <li><a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#too-many-prompts">Too many prompts</a></li> 
   <li><a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#our-approach-to-prompting">The Fabric approach to prompting</a></li> 
  </ul> </li> 
 <li><a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#Installation">Installation</a> 
  <ul> 
   <li><a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#Migrating">Migrating</a></li> 
   <li><a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#Upgrading">Upgrading</a></li> 
  </ul> </li> 
 <li><a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#Usage">Usage</a></li> 
 <li><a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#examples">Examples</a> 
  <ul> 
   <li><a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#just-use-the-patterns">Just use the Patterns</a></li> 
  </ul> </li> 
 <li><a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#custom-patterns">Custom Patterns</a></li> 
 <li><a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#helper-apps">Helper Apps</a></li> 
 <li><a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#meta">Meta</a> 
  <ul> 
   <li><a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#primary-contributors">Primary contributors</a></li> 
  </ul> </li> 
</ul> 
<br /> 
<blockquote> 
 <p>[!NOTE] August 20, 2024 â€” We have migrated to Go, and the transition has been pretty smooth! The biggest thing to know is that <strong>the previous installation instructions in the various Fabric videos out there will no longer work</strong> because they were for the legacy (Python) version. Check the new <a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#Installation">install instructions</a> below.</p> 
</blockquote> 
<h2>Intro videos</h2> 
<p>Keep in mind that many of these were recorded when Fabric was Python-based, so remember to use the current <a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#Installation">install instructions</a> below.</p> 
<ul> 
 <li><a href="https://www.youtube.com/watch?v=UbDyjIIGaxQ">Network Chuck</a></li> 
 <li><a href="https://www.youtube.com/watch?v=vF-MQmVxnCs">David Bombal</a></li> 
 <li><a href="https://www.youtube.com/watch?v=wPEyyigh10g">My Own Intro to the Tool</a></li> 
 <li><a href="https://www.youtube.com/results?search_query=fabric+ai">More Fabric YouTube Videos</a></li> 
</ul> 
<h2>What and why</h2> 
<p>Since the start of 2023 and GenAI we've seen a massive number of AI applications for accomplishing tasks. It's powerful, but <em>it's not easy to integrate this functionality into our lives.</em></p> 
<div align="center"> 
 <h4>In other words, AI doesn't have a capabilities problemâ€”it has an <em>integration</em> problem.</h4> 
</div> 
<p>Fabric was created to address this by enabling everyone to granularly apply AI to everyday challenges.</p> 
<h2>Philosophy</h2> 
<blockquote> 
 <p>AI isn't a thing; it's a <em>magnifier</em> of a thing. And that thing is <strong>human creativity</strong>.</p> 
</blockquote> 
<p>We believe the purpose of technology is to help humans flourish, so when we talk about AI we start with the <strong>human</strong> problems we want to solve.</p> 
<h3>Breaking problems into components</h3> 
<p>Our approach is to break problems into individual pieces (see below) and then apply AI to them one at a time. See below for some examples.</p> 
<img alt="augmented_challenges" src="https://github.com/danielmiessler/fabric/assets/50654/31997394-85a9-40c2-879b-b347e4701f06" width="2078" /> 
<h3>Too many prompts</h3> 
<p>Prompts are good for this, but the biggest challenge I faced in 2023â€”â€”which still exists todayâ€”is <strong>the sheer number of AI prompts out there</strong>. We all have prompts that are useful, but it's hard to discover new ones, know if they are good or not, <em>and manage different versions of the ones we like</em>.</p> 
<p>One of <code>fabric</code>'s primary features is helping people collect and integrate prompts, which we call <em>Patterns</em>, into various parts of their lives.</p> 
<p>Fabric has Patterns for all sorts of life and work activities, including:</p> 
<ul> 
 <li>Extracting the most interesting parts of YouTube videos and podcasts</li> 
 <li>Writing an essay in your own voice with just an idea as an input</li> 
 <li>Summarizing opaque academic papers</li> 
 <li>Creating perfectly matched AI art prompts for a piece of writing</li> 
 <li>Rating the quality of content to see if you want to read/watch the whole thing</li> 
 <li>Getting summaries of long, boring content</li> 
 <li>Explaining code to you</li> 
 <li>Turning bad documentation into usable documentation</li> 
 <li>Creating social media posts from any content input</li> 
 <li>And a million moreâ€¦</li> 
</ul> 
<h2>Installation</h2> 
<p>To install Fabric, <a href="https://go.dev/doc/install">make sure Go is installed</a>, and then run the following command.</p> 
<pre><code class="language-bash"># Install Fabric directly from the repo
go install github.com/danielmiessler/fabric@latest

# Run the setup to set up your directories and keys
fabric --setup
</code></pre> 
<h3>Environment Variables</h3> 
<p>If everything works you are good to go, but you may need to set some environment variables in your <code>~/.bashrc</code> or <code>~/.zshrc</code> file. Here is an example of what you can add:</p> 
<pre><code class="language-bash"># Golang environment variables
export GOROOT=/usr/local/go
export GOPATH=$HOME/go
export PATH=$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH:
</code></pre> 
<h3>Migration</h3> 
<p>If you have the Legacy (Python) version installed and want to migrate to the Go version, here's how you do it. It's basically two steps: 1) uninstall the Python version, and 2) install the Go version.</p> 
<pre><code class="language-bash"># Uninstall Legacy Fabric
pipx uninstall fabric

# Clear any old Fabric aliases
(check your .bashrc, .zshrc, etc.)
# Install the Go version
go install github.com/danielmiessler/fabric@latest
# Run setup for the new version. Important because things have changed
fabric --setup
</code></pre> 
<p>Then <a href="https://raw.githubusercontent.com/danielmiessler/fabric/main/#environmental-variables">set your environmental variables</a> as shown above.</p> 
<h3>Upgrading</h3> 
<p>The great thing about Go is that it's super easy to upgrade. Just run the same command you used to install it in the first place and you'll always get the latest version.</p> 
<pre><code class="language-bash">go install github.com/danielmiessler/fabric@latest
</code></pre> 
<h2>Usage</h2> 
<p>Once you have it all set up, here's how to use it.</p> 
<pre><code class="language-bash">fabric -h
</code></pre> 
<pre><code class="language-bash">usage: fabric -h
Usage:
  fabric [OPTIONS]

Application Options:
  -p, --pattern=          Choose a pattern
  -C, --context=          Choose a context
      --session=          Choose a session
  -S, --setup             Run setup
  -t, --temperature=      Set temperature (default: 0.7)
  -T, --topp=             Set top P (default: 0.9)
  -s, --stream            Stream
  -P, --presencepenalty=  Set presence penalty (default: 0.0)
  -F, --frequencypenalty= Set frequency penalty (default: 0.0)
  -l, --listpatterns      List all patterns
  -L, --listmodels        List all available models
  -x, --listcontexts      List all contexts
  -X, --listsessions      List all sessions
  -U, --updatepatterns    Update patterns
  -c, --copy              Copy to clipboard
  -m, --model=            Choose model
  -u, --url=              Choose ollama url (default: http://127.0.0.1:11434)
  -o, --output=           Output to file
  -n, --latest=           Number of latest patterns to list (default: 0)

Help Options:
  -h, --help              Show this help message

</code></pre> 
<h2>Our approach to prompting</h2> 
<p>Fabric <em>Patterns</em> are different than most prompts you'll see.</p> 
<ul> 
 <li><strong>First, we use <code>Markdown</code> to help ensure maximum readability and editability</strong>. This not only helps the creator make a good one, but also anyone who wants to deeply understand what it does. <em>Importantly, this also includes the AI you're sending it to!</em></li> 
</ul> 
<p>Here's an example of a Fabric Pattern.</p> 
<pre><code class="language-bash">https://github.com/danielmiessler/fabric/blob/main/patterns/extract_wisdom/system.md
</code></pre> 
<img alt="pattern-example" src="https://github.com/danielmiessler/fabric/assets/50654/b910c551-9263-405f-9735-71ca69bbab6d" width="1461" /> 
<ul> 
 <li> <p><strong>Next, we are extremely clear in our instructions</strong>, and we use the Markdown structure to emphasize what we want the AI to do, and in what order.</p> </li> 
 <li> <p><strong>And finally, we tend to use the System section of the prompt almost exclusively</strong>. In over a year of being heads-down with this stuff, we've just seen more efficacy from doing that. If that changes, or we're shown data that says otherwise, we will adjust.</p> </li> 
</ul> 
<h2>Examples</h2> 
<p>Now let's look at some things you can do with Fabric.</p> 
<ol> 
 <li>Run the <code>summarize</code> Pattern based on input from <code>stdin</code>. In this case, the body of an article.</li> 
</ol> 
<pre><code class="language-bash">pbpaste | fabric --pattern summarize
</code></pre> 
<ol start="2"> 
 <li>Run the <code>analyze_claims</code> Pattern with the <code>--stream</code> option to get immediate and streaming results.</li> 
</ol> 
<pre><code class="language-bash">pbpaste | fabric --stream --pattern analyze_claims
</code></pre> 
<ol start="3"> 
 <li>Run the <code>extract_wisdom</code> Pattern with the <code>--stream</code> option to get immediate and streaming results from any Youtube video (much like in the original introduction video).</li> 
</ol> 
<pre><code class="language-bash">yt --transcript https://youtube.com/watch?v=uXs-zPc63kM | fabric --stream --pattern extract_wisdom
</code></pre> 
<ol start="4"> 
 <li>Create patterns- you must create a .md file with the pattern and save it to ~/.config/fabric/patterns/[yourpatternname].</li> 
</ol> 
<h2>Just use the Patterns</h2> 
<img alt="fabric-patterns-screenshot" src="https://github.com/danielmiessler/fabric/assets/50654/9186a044-652b-4673-89f7-71cf066f32d8" width="1173" /> 
<br /> 
<br /> 
<p>If you're not looking to do anything fancy, and you just want a lot of great prompts, you can navigate to the <a href="https://github.com/danielmiessler/fabric/tree/main/patterns"><code>/patterns</code></a> directory and start exploring!</p> 
<p>We hope that if you used nothing else from Fabric, the Patterns by themselves will make the project useful.</p> 
<p>You can use any of the Patterns you see there in any AI application that you have, whether that's ChatGPT or some other app or website. Our plan and prediction is that people will soon be sharing many more than those we've published, and they will be way better than ours.</p> 
<p>The wisdom of crowds for the win.</p> 
<h2>Custom Patterns</h2> 
<p>You may want to use Fabric to create your own custom Patternsâ€”but not share them with others. No problem!</p> 
<p>Just make a directory in <code>~/.config/custompatterns/</code> (or wherever) and put your <code>.md</code> files in there.</p> 
<p>When you're ready to use them, copy them into:</p> 
<pre><code>~/.config/fabric/patterns/
</code></pre> 
<p>You can then use them like any other Patterns, but they won't be public unless you explicitly submit them as Pull Requests to the Fabric project. So don't worryâ€”they're private to you.</p> 
<p>This feature works with all openai and ollama models but does NOT work with claude. You can specify your model with the -m flag</p> 
<h2>Helper Apps</h2> 
<p>Fabric also makes use of some core helper apps (tools) to make it easier to integrate with your various workflows. Here are some examples:</p> 
<p><code>yt</code> is a helper command that extracts the transcript from a YouTube video. You can use it like this:</p> 
<pre><code class="language-bash">yt https://www.youtube.com/watch?v=lQVcbY52_gY
</code></pre> 
<p>This will return the transcript from the video, which you can then pipe into Fabric like this:</p> 
<pre><code class="language-bash">yt https://www.youtube.com/watch?v=lQVcbY52_gY | fabric --pattern extract_wisdom
</code></pre> 
<h3><code>yt</code> Installation</h3> 
<p>To install <code>yt</code>, install it the same way as you install Fabric, just with a different repo name.</p> 
<pre><code class="language-bash">go install github.com/danielmiessler/yt@latest
</code></pre> 
<p>Be sure to add your <code>YOUTUBE_API_KEY</code> to <code>~/.config/fabric/.env</code>.</p> 
<h2>Meta</h2> 
<blockquote> 
 <p>[!NOTE] Special thanks to the following people for their inspiration and contributions!</p> 
</blockquote> 
<ul> 
 <li><em>Jonathan Dunn</em> for being the absolute MVP dev on the project, including spearheading the new Go version, as well as the GUI! All this while also being a full-time medical doctor!</li> 
 <li><em>Caleb Sima</em> for pushing me over the edge of whether to make this a public project or not.</li> 
 <li><em>Eugen Eisler</em> and <em>Frederick Ros</em> for their invaluable contributions to the Go version</li> 
 <li><em>Joel Parish</em> for super useful input on the project's Github directory structure..</li> 
 <li><em>Joseph Thacker</em> for the idea of a <code>-c</code> context flag that adds pre-created context in the <code>./config/fabric/</code> directory to all Pattern queries.</li> 
 <li><em>Jason Haddix</em> for the idea of a stitch (chained Pattern) to filter content using a local model before sending on to a cloud model, i.e., cleaning customer data using <code>llama2</code> before sending on to <code>gpt-4</code> for analysis.</li> 
 <li><em>Andre Guerra</em> for assisting with numerous components to make things simpler and more maintainable.</li> 
</ul> 
<h3>Primary contributors</h3> 
<p><a href="https://github.com/danielmiessler"><img height="50" src="https://avatars.githubusercontent.com/u/50654?v=4" title="Daniel Miessler" width="50" /></a> <a href="https://github.com/xssdoctor"><img height="50" src="https://avatars.githubusercontent.com/u/9218431?v=4" title="Jonathan Dunn" width="50" /></a> <a href="https://github.com/sbehrens"><img height="50" src="https://avatars.githubusercontent.com/u/688589?v=4" title="Scott Behrens" width="50" /></a> <a href="https://github.com/agu3rra"><img height="50" src="https://avatars.githubusercontent.com/u/10410523?v=4" title="Andre Guerra" width="50" /></a></p> 
<p><code>fabric</code> was created by <a href="https://danielmiessler.com/subscribe" target="_blank">Daniel Miessler</a> in January of 2024. <br /><br /> <a href="https://twitter.com/intent/user?screen_name=danielmiessler"><img alt="X (formerly Twitter) Follow" src="https://img.shields.io/twitter/follow/danielmiessler" /></a></p>
]]></content:encoded>


</item>
<item>
<title>princeton-nlp/SWE-agent</title>
<link>https://github.com/princeton-nlp/SWE-agent</link>
<guid>https://github.com/princeton-nlp/SWE-agent</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šSWE-agentã€GitHubã€è‡ªåŠ¨ä¿®å¤ã€GPT-4ã€è½¯ä»¶å·¥ç¨‹ä»£ç†

æ€»ç»“:

æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSWE-agentçš„å·¥å…·ï¼Œå®ƒæ—¨åœ¨åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰æ¥è§£å†³å®é™…GitHubä»“åº“ä¸­çš„è½¯ä»¶å·¥ç¨‹é—®é¢˜ã€‚SWE-agenté€šè¿‡è®¾è®¡ç®€å•çš„å‘½ä»¤å’Œåé¦ˆæ ¼å¼ï¼Œå³æ‰€è°“çš„Agent-Computer Interface (ACI)ï¼Œä½¿è¯­è¨€æ¨¡å‹æ›´å®¹æ˜“æµè§ˆä»£ç ä»“åº“ã€æŸ¥çœ‹ã€ç¼–è¾‘å’Œæ‰§è¡Œä»£ç æ–‡ä»¶ã€‚è¯¥å·¥å…·åœ¨SWE-benchè¯„ä¼°é›†ä¸Šå®ç°äº†12.47%çš„é—®é¢˜è§£å†³ç‡ï¼Œè¿™æ˜¯å½“å‰çš„é¢†å…ˆæ€§èƒ½ã€‚SWE-agentç”±æ™®æ—æ–¯é¡¿å¤§å­¦çš„ç ”ç©¶äººå‘˜æ„å»ºå’Œç»´æŠ¤ã€‚

ç”¨æˆ·å¯ä»¥é€šè¿‡Webç•Œé¢æˆ–å‘½ä»¤è¡Œä½¿ç”¨SWE-agentã€‚è¦å¼€å§‹ä½¿ç”¨ï¼Œè¯·è®¿é—®æŒ‡å®šé“¾æ¥ã€‚è¯¦ç»†ä¿¡æ¯å’Œæœªæ¥åŠŸèƒ½æ›´æ–°å¯é€šè¿‡åŠ å…¥ç‰¹å®šç¤¾åŒºè·å–ã€‚å¦‚æœå¸Œæœ›å‚ä¸ä»£ç è´¡çŒ®ï¼Œæ¬¢è¿æå‡ºé—®é¢˜ã€å­¦ä¹ æ–°åŠŸèƒ½å¹¶å‚ä¸å¼€å‘è¿‡ç¨‹ã€‚é¡¹ç›®éµå¾ªMITè®¸å¯è¯ï¼Œæ„Ÿå…´è¶£çš„å¼€å‘è€…å¯ä»¥è®¿é—®LICENSEæ–‡ä»¶è·å–æ›´å¤šä¿¡æ¯ã€‚å¦‚æœæ­¤å·¥ä½œå¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘å¼•ç”¨ä»¥ä¸‹æ–‡çŒ®ï¼š

@misc{yang2024sweagent,
      title={SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering},
      author={John Yang and Carlos E. Jimenez and Alexander Wettig and Kilian Lieret and Shunyu Yao and Karthik Narasimhan and Ofir Press},
      year={2024},
      eprint={2405.15793},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
} <div>
<p>SWE-agent takes a GitHub issue and tries to automatically fix it, using GPT-4, or your LM of choice. It solves 12.47% of bugs in the SWE-bench evaluation set and takes just 1 minute to run.</p><hr /><p align="center"> <a href="https://www.swe-agent.com/"> <img alt="swe-agent.com" src="https://raw.githubusercontent.com/princeton-nlp/SWE-agent/main/assets/swe-agent-banner.png" /> </a> </p> 
<p align="center"> <a href="https://swe-agent.com"><strong>Website &amp; Demo</strong></a>&nbsp; | &nbsp; <a href="https://princeton-nlp.github.io/SWE-agent/"><strong>Documentation</strong></a>&nbsp; | &nbsp; <a href="https://discord.gg/AVEFbBn2rH"><strong>Discord</strong></a>&nbsp; | &nbsp; <a href="https://arxiv.org/abs/2405.15793"><strong>Preprint</strong></a> </p> 
<p><strong>SWE-agent turns LMs (e.g. GPT-4) into software engineering agents that can resolve issues in real GitHub repositories.</strong></p> 
<p>On <a href="https://github.com/princeton-nlp/SWE-bench">SWE-bench</a>, SWE-agent resolves 12.47% of issues, achieving the state-of-the-art performance on the full test set.</p> 
<p>We accomplish our results by designing simple LM-centric commands and feedback formats to make it easier for the LM to browse the repository, view, edit and execute code files. We call this an <strong>Agent-Computer Interface (ACI)</strong>. Read more about it in our <a href="https://arxiv.org/abs/2405.15793">paper</a>!</p> 
<p>SWE-agent is built and maintained by researchers from Princeton University.</p> 
<p><img alt="My Movie 3" src="https://github.com/princeton-nlp/SWE-agent/assets/13602468/fa201621-ec31-4644-b658-c1d0feb92253" /></p> 
<p>You can use SWE-agent either through a web interface (shown above) or through the command line.</p> 
<h2>ğŸš€ Get started!</h2> 
<p>ğŸ‘‰ Try SWE-agent in your browser: <a href="https://codespaces.new/princeton-nlp/SWE-agent"><img alt="Open in GitHub Codespaces" src="https://img.shields.io/badge/Open_in_GitHub_Codespaces-gray?logo=github" /></a> (<a href="https://princeton-nlp.github.io/SWE-agent/installation/codespaces/">more information</a>)</p> 
<p>Read our <a href="https://princeton-nlp.github.io/SWE-agent/">documentation</a> to learn more:</p> 
<ul> 
 <li><a href="https://princeton-nlp.github.io/SWE-agent/installation/">Installation</a></li> 
 <li><a href="https://princeton-nlp.github.io/SWE-agent/usage/cl_tutorial/">Command line usage</a></li> 
 <li><a href="https://princeton-nlp.github.io/SWE-agent/usage/web_ui/">Using the web UI</a></li> 
 <li><a href="https://princeton-nlp.github.io/SWE-agent/usage/benchmarking/">Benchmarking on SWE-bench</a></li> 
 <li><a href="https://princeton-nlp.github.io/SWE-agent/faq/">Frequently Asked Questions</a></li> 
</ul> 
<div align="center"> 
 <a href="https://princeton-nlp.github.io/SWE-agent/"><img src="https://raw.githubusercontent.com/princeton-nlp/SWE-agent/main/assets/doc-scrot.png" style="width: 600px;" /></a> 
</div> 
<h2>ğŸ’« Contributions <a name="contributions"></a></h2> 
<ul> 
 <li>If you'd like to ask questions, learn about upcoming features, and participate in future development, join our <a href="https://discord.gg/AVEFbBn2rH">Discord community</a>!</li> 
 <li>If you'd like to contribute to the codebase, we welcome <a href="https://github.com/princeton-nlp/SWE-agent/issues">issues</a> and <a href="https://github.com/princeton-nlp/SWE-agent/pulls">pull requests</a>!</li> 
</ul> 
<p>Contact person: <a href="https://john-b-yang.github.io/">John Yang</a> and <a href="http://www.carlosejimenez.com/">Carlos E. Jimenez</a> (Email: <a href="mailto:johnby@stanford.edu">johnby@stanford.edu</a>, <a href="mailto:carlosej@princeton.edu">carlosej@princeton.edu</a>).</p> 
<h2>ğŸ“ Citation <a name="citation"></a></h2> 
<p>If you found this work helpful, please consider citing it using the following:</p> 
<pre><code>@misc{yang2024sweagent,
      title={SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering},
      author={John Yang and Carlos E. Jimenez and Alexander Wettig and Kilian Lieret and Shunyu Yao and Karthik Narasimhan and Ofir Press},
      year={2024},
      eprint={2405.15793},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
</code></pre> 
<h2>ğŸªª License <a name="license"></a></h2> 
<p>MIT. Check <code>LICENSE</code>.</p> 
<div align="center"> 
 <p><a href="https://github.com/princeton-nlp/SWE-agent/actions/workflows/pytest.yaml"><img alt="Pytest" src="https://github.com/princeton-nlp/SWE-agent/actions/workflows/pytest.yaml/badge.svg?sanitize=true" /></a> <a href="https://github.com/princeton-nlp/SWE-agent/actions/workflows/test_build_containers.yaml"><img alt="Test build containers" src="https://github.com/princeton-nlp/SWE-agent/actions/workflows/test_build_containers.yaml/badge.svg?sanitize=true" /></a> <a href="https://github.com/princeton-nlp/SWE-agent/actions/workflows/release-dockerhub-nightly.yaml"><img alt="Release to dockerhub (nightly)" src="https://github.com/princeton-nlp/SWE-agent/actions/workflows/release-dockerhub-nightly.yaml/badge.svg?sanitize=true" /></a> <a href="https://github.com/princeton-nlp/SWE-agent/actions/workflows/release-dockerhub-release.yaml"><img alt="Release to dockerhub (release)" src="https://github.com/princeton-nlp/SWE-agent/actions/workflows/release-dockerhub-release.yaml/badge.svg?sanitize=true" /></a> <a href="https://github.com/princeton-nlp/SWE-agent/actions/workflows/build-docs.yaml"><img alt="build-docs" src="https://github.com/princeton-nlp/SWE-agent/actions/workflows/build-docs.yaml/badge.svg?sanitize=true" /></a> <a href="https://codecov.io/gh/princeton-nlp/SWE-agent"><img alt="codecov" src="https://codecov.io/gh/princeton-nlp/SWE-agent/graph/badge.svg?token=18XAVDK365" /></a> <a href="https://results.pre-commit.ci/latest/github/princeton-nlp/SWE-agent/main"><img alt="pre-commit.ci status" src="https://results.pre-commit.ci/badge/github/princeton-nlp/SWE-agent/main.svg?sanitize=true" /></a> <a href="https://github.com/princeton-nlp/SWE-agent/actions/workflows/check-links.yaml"><img alt="Markdown links" src="https://github.com/princeton-nlp/SWE-agent/actions/workflows/check-links.yaml/badge.svg?sanitize=true" /></a></p> 
</div>
]]></content:encoded>


</item>
<item>
<title>nikitabobko/AeroSpace</title>
<link>https://github.com/nikitabobko/AeroSpace</link>
<guid>https://github.com/nikitabobko/AeroSpace</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šAeroSpaceã€macOSã€i3ã€tiling window managerã€System Integrity Protectionï¼ˆSIPï¼‰

æ€»ç»“:

AeroSpaceæ˜¯ä¸€æ¬¾åŸºäºi3è®¾è®¡ç†å¿µçš„macOSçª—å£ç®¡ç†å™¨ï¼Œå®ƒä¸ºç”¨æˆ·æä¾›äº†ä¸€ä¸ªå¿«é€Ÿã€æ— åŠ¨ç”»çš„å·¥ä½œç©ºé—´åˆ‡æ¢ä½“éªŒï¼Œæ— éœ€ç¦ç”¨SIPã€‚è¿™æ¬¾è½¯ä»¶æ”¯æŒå…¨æ–‡æœ¬é…ç½®ï¼Œä¾¿äºä¸dotfilesé›†æˆï¼Œå¹¶æä¾›äº†å‘½ä»¤è¡Œä¼˜å…ˆæ¥å£ï¼ŒåŒ…æ‹¬mané¡µé¢å’Œshellå®ŒæˆåŠŸèƒ½ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡Homebrewè¿›è¡Œå®‰è£…ï¼Œä»¥è·å–è‡ªåŠ¨æ›´æ–°ã€‚AeroSpaceæ—¨åœ¨ä¸ºé«˜çº§ç”¨æˆ·å’Œå¼€å‘è€…æä¾›æœåŠ¡ï¼Œå¼ºè°ƒé”®ç›˜ä¸ºä¸­å¿ƒçš„æ“ä½œæ–¹å¼ï¼Œå¹¶å°½é‡é¿å…å¼•å…¥ç ´åæ€§æ›´æ”¹ï¼Œé™¤éåœ¨é‡å¤§ç‰ˆæœ¬ä¸­æ˜ç¡®éœ€è¦è¿›è¡Œã€‚

AeroSpaceä¸ä¾èµ–macOSçš„SpacesåŠŸèƒ½ï¼Œè€Œæ˜¯é‡‡ç”¨äº†è‡ªå·±çš„å®ç°æ–¹å¼ã€‚å®ƒå…è®¸ç”¨æˆ·åœ¨å¤šæ˜¾ç¤ºå™¨è®¾ç½®ä¸‹è¿›è¡Œé…ç½®ï¼Œå¹¶æä¾›äº†bashã€fishå’Œzshçš„å®Œæˆæ”¯æŒã€‚ç”¨æˆ·éœ€è¦æ³¨æ„ç¡®ä¿æ˜¾ç¤ºå™¨æ­£ç¡®é…ç½®ï¼Œä»¥è·å¾—æœ€ä½³ä½“éªŒã€‚æ­¤å¤–ï¼ŒAeroSpaceæä¾›äº†è‡ªåŠ¨åˆ é™¤com.apple.quarantineå±æ€§çš„åŠŸèƒ½ï¼Œç¡®ä¿åº”ç”¨å¯ä»¥æ— ç¼è¿è¡Œã€‚

ä¸ºäº†å‚ä¸é¡¹ç›®çš„å‘å±•ï¼Œç”¨æˆ·å¯ä»¥æŸ¥çœ‹è´¡çŒ®æŒ‡å—ã€æå‡ºé—®é¢˜æˆ–æäº¤æ‹‰å–è¯·æ±‚ã€‚é¡¹ç›®çš„æ ¸å¿ƒä»·å€¼è§‚åŒ…æ‹¬é¢å‘é«˜çº§ç”¨æˆ·å’Œå¼€å‘è€…ã€é”®ç›˜é©±åŠ¨çš„æ“ä½œæ–¹å¼ä»¥åŠæœ€å°åŒ–ç ´åæ€§æ›´æ”¹çš„ç­–ç•¥ã€‚AeroSpaceé¿å…ä½¿ç”¨GUIç•Œé¢è¿›è¡Œé…ç½®ï¼Œå¹¶æä¾›ç³»ç»Ÿèœå•å›¾æ ‡ä½œä¸ºåé¦ˆæœºåˆ¶ã€‚å®ƒä¸“æ³¨äºå®ç”¨åŠŸèƒ½ï¼Œé¿å…ä¸å¿…è¦çš„å¤æ‚æ€§å’Œå¤–è§‚è°ƒæ•´ã€‚

åœ¨ä¸è¿åAppleå®‰å…¨æ”¿ç­–çš„å‰æä¸‹ï¼ŒAeroSpaceæä¾›äº†ä¸€ç§æ›¿ä»£æ–¹æ¡ˆï¼Œè®©macOSç”¨æˆ·èƒ½å¤Ÿäº«å—ç±»ä¼¼äºi3çš„çª—å£ç®¡ç†ä½“éªŒã€‚é€šè¿‡éµå¾ªä¸Šè¿°å…³é”®ç‚¹ï¼Œç”¨æˆ·å¯ä»¥æ›´å¥½åœ°ç†è§£AeroSpaceçš„ç‰¹æ€§å’Œä½¿ç”¨æ–¹æ³•ã€‚ <div>
<p>AeroSpace is an i3-like tiling window manager for macOS</p><hr /><img align="right" height="40%" src="https://raw.githubusercontent.com/nikitabobko/AeroSpace/main/resources/Assets.xcassets/AppIcon.appiconset/icon.png" width="40%" /> 
<h1>AeroSpace Beta <a href="https://github.com/nikitabobko/AeroSpace/actions/workflows/build.yml"><img alt="Build" src="https://github.com/nikitabobko/AeroSpace/actions/workflows/build.yml/badge.svg?branch=main" /></a></h1> 
<p>AeroSpace is an i3-like tiling window manager for macOS</p> 
<p>Videos:</p> 
<ul> 
 <li><a href="https://www.youtube.com/watch?v=UOl7ErqWbrk">YouTube 91 sec Demo</a></li> 
 <li><a href="https://www.youtube.com/watch?v=-FoWClVHG5g">YouTube Guide by Josean Martinez</a></li> 
</ul> 
<p>Docs:</p> 
<ul> 
 <li><a href="https://nikitabobko.github.io/AeroSpace/guide">AeroSpace Guide</a></li> 
 <li><a href="https://nikitabobko.github.io/AeroSpace/commands">AeroSpace Commands</a></li> 
 <li><a href="https://nikitabobko.github.io/AeroSpace/goodness">AeroSpace Goodness</a></li> 
</ul> 
<h2>Project status</h2> 
<p>Public Beta. AeroSpace can be used as a daily driver, but expect breaking changes until 1.0 is reached.</p> 
<h2>Key features</h2> 
<ul> 
 <li>Tiling window manager based on a <a href="https://nikitabobko.github.io/AeroSpace/guide#tree">tree paradigm</a></li> 
 <li><a href="https://i3wm.org/">i3</a> inspired</li> 
 <li>Fast workspaces switching without animations and without the necessity to disable SIP</li> 
 <li>AeroSpace employs its <a href="https://nikitabobko.github.io/AeroSpace/guide#emulation-of-virtual-workspaces">own emulation of virtual workspaces</a> instead of relying on native macOS Spaces due to <a href="https://nikitabobko.github.io/AeroSpace/guide#emulation-of-virtual-workspaces">their considerable limitations</a></li> 
 <li>Plain text configuration (dotfiles friendly). See: <a href="https://nikitabobko.github.io/AeroSpace/guide#default-config">default-config.toml</a></li> 
 <li>CLI first (manpages and shell completion included)</li> 
 <li>Doesn't require disabling SIP (System Integrity Protection)</li> 
 <li><a href="https://nikitabobko.github.io/AeroSpace/guide#multiple-monitors">Proper multi-monitor support</a> (i3-like paradigm)</li> 
</ul> 
<h2>Installation</h2> 
<p>Install via <a href="https://brew.sh/">Homebrew</a> to get autoupdates (Preferred)</p> 
<pre><code>brew install --cask nikitabobko/tap/aerospace
</code></pre> 
<p><strong>(Optional)</strong> You might need to configure your shell to enable completion provided by homebrew packages: <a href="https://docs.brew.sh/Shell-Completion">https://docs.brew.sh/Shell-Completion</a> AeroSpace provides bash, fish and zsh completions.</p> 
<p>In multi-monitor setup please make sure that monitors <a href="https://nikitabobko.github.io/AeroSpace/guide#proper-monitor-arrangement">are properly arranged</a>.</p> 
<p>You can also install specific previous versions:</p> 
<pre><code>brew install --cask nikitabobko/tap/aerospace@0.12.0
</code></pre> 
<p>For the list of all the versions available for installation via brew see: <a href="https://github.com/nikitabobko/homebrew-tap/tree/main/Casks">https://github.com/nikitabobko/homebrew-tap/tree/main/Casks</a></p> 
<p><a href="https://nikitabobko.github.io/AeroSpace/guide#manual-installation">Manual installation</a></p> 
<blockquote> 
 <p>[!NOTE] By using AeroSpace, you acknowledge that it's not <a href="https://developer.apple.com/documentation/security/notarizing_macos_software_before_distribution">notarized</a>.</p> 
 <p>Notarization is a "security" feature by Apple. You send binaries to Apple, and they either approve the binaries or not. In reality, notarization is about building binaries the way Apple likes it.</p> 
 <p>Let's be honest. Tiling window manager is not something Apple will be totally ok with. Even if they approve one version, it doesn't mean that they won't revoke it (yes, they can do it), or approve further versions.</p> 
 <p>I don't have anything against notarization as a concept. I specifically don't like the way Apple does notarization. I don't have time to fight Apple.</p> 
 <p><a href="https://github.com/nikitabobko/homebrew-tap/raw/main/Casks/aerospace.rb">Homebrew installation script</a> is configured to automatically delete <code>com.apple.quarantine</code> attribute, that's why the app should work out of the box, without any warnings that "Apple cannot check AeroSpace for malicious software"</p> 
</blockquote> 
<h2>Contributing, creating issues, submitting pull requests</h2> 
<p>See: <a href="https://raw.githubusercontent.com/nikitabobko/AeroSpace/main/CONTRIBUTING.md">CONTRIBUTING.md</a></p> 
<h2>Development</h2> 
<p>A notes on how to setup the project, build it, how to run the tests, etc. can be found here: <a href="https://raw.githubusercontent.com/nikitabobko/AeroSpace/main/dev-docs/development.md">dev-docs/development.md</a></p> 
<h2>Values of the project</h2> 
<p><strong>Values</strong></p> 
<ul> 
 <li>AeroSpace is targeted at advanced users and developers</li> 
 <li>Keyboard centric</li> 
 <li>Breaking changes (configuration files, CLI, behavior) are avoided as much as possible, but it must not let the software stagnate. Thus breaking changes can happen, but with careful considerations and helpful message. <a href="https://semver.org/">Semver</a> major version is bumped in case of a breaking change (It's all guaranteed once AeroSpace reaches 1.0 version, until then breaking changes just happen)</li> 
 <li>AeroSpace doesn't use GUI, unless necessarily 
  <ul> 
   <li>AeroSpace will never provide a GUI for configuration. For advanced users, it's easier to edit a configuration file in text editor rather than navigating through checkboxes in GUI.</li> 
   <li>Status menu icon is ok, because visual feedback is needed</li> 
  </ul> </li> 
 <li>Provide <em>practical</em> features. Fancy appearance features are not <em>practical</em> (e.g. window borders, transparency, etc)</li> 
 <li>If "dark magic" (aka "private APIs", "code injections", etc) can be avoided, it must be avoided 
  <ul> 
   <li>Right now, AeroSpace uses only a single private API to get window ID of accessibility object <code>_AXUIElementGetWindow</code>. Everything else is <a href="https://developer.apple.com/documentation/applicationservices/axuielement_h">macOS public accessibility API</a>.</li> 
   <li>AeroSpace will never require you to disable SIP (System Integrity Protection). For example, yabai <a href="https://github.com/koekeishiya/yabai/issues/1863">requires you to disable SIP</a> to use some of its features. AeroSpace will either find another way (such as <a href="https://nikitabobko.github.io/AeroSpace/guide#emulation-of-virtual-workspaces">emulation of workspaces</a>) or will not implement this feature at all (window transparency and window shadowing are not <em>practical</em> features)</li> 
  </ul> </li> 
</ul> 
<p><strong>Non Values</strong></p> 
<ul> 
 <li>Play nicely with existing macOS features. If limitations are imposed then AeroSpace won't play nicely with existing macOS features 
  <ul> 
   <li>E.g. AeroSpace doesn't acknowledge the existence of macOS Spaces, and it uses <a href="https://nikitabobko.github.io/AeroSpace/guide#emulation-of-virtual-workspaces">emulation of its own workspaces</a></li> 
  </ul> </li> 
</ul> 
<h2>Tip of the day</h2> 
<pre><code class="language-bash">defaults write -g NSWindowShouldDragOnGesture -bool true
</code></pre> 
<p>Now, you can move windows by holding <code>ctrl</code>+<code>cmd</code> and dragging any part of the window (not necessarily the window title)</p> 
<p>Source: <a href="https://www.reddit.com/r/MacOS/comments/k6hiwk/keyboard_modifier_to_simplify_click_drag_of/">reddit</a></p> 
<h2>Related projects</h2> 
<ul> 
 <li><a href="https://github.com/ianyh/Amethyst">Amethyst</a></li> 
 <li><a href="https://github.com/koekeishiya/yabai">yabai</a></li> 
</ul>
]]></content:encoded>


</item>
<item>
<title>hcengineering/platform</title>
<link>https://github.com/hcengineering/platform</link>
<guid>https://github.com/hcengineering/platform</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šHuly Platformã€è‡ªæ‰˜ç®¡ã€é¢„å®‰è£…è„šæœ¬ã€Dockerã€Rushå·¥å…·

æ€»ç»“:

Huly Platform æ˜¯ä¸€ä¸ªå…¨é¢çš„ä¼ä¸šåº”ç”¨å¼€å‘æ¡†æ¶ï¼Œæ—¨åœ¨åŠ é€Ÿæ„å»ºCRMç³»ç»Ÿç­‰å•†ä¸šåº”ç”¨ã€‚è¯¥å¹³å°åŒ…å«å¤šä¸ªåº”ç”¨ç¨‹åºï¼Œå¦‚èŠå¤©ã€é¡¹ç›®ç®¡ç†ã€CRMã€HRMå’ŒATSï¼Œå¹¶æ”¯æŒå¤šä¸ªå›¢é˜Ÿåœ¨å…¶åŸºç¡€ä¸Šå¼€å‘äº§å“ã€‚

å¯¹äºå¸Œæœ›è‡ªæ‰˜ç®¡ Huly è€Œä¸æ‰“ç®—ä¿®æ”¹æˆ–è´¡çŒ®åˆ°å…¶å‘å±•çš„ç”¨æˆ·ï¼Œæ¨èä½¿ç”¨ "è‡ªæ‰˜ç®¡" ç‰ˆæœ¬ï¼Œè¯¥ç‰ˆæœ¬æä¾›äº†ä¸€ä¸ªæ–¹ä¾¿çš„ Docker å®‰è£…æ–¹æ³•ï¼Œæ—¨åœ¨å¿«é€Ÿéƒ¨ç½²å¹¶æ˜“äºä½¿ç”¨ã€‚

ä¸ºäº†ç¡®ä¿å®‰è£…æ­£ç¡®ï¼Œç”¨æˆ·éœ€è¦å®Œæˆä¸€ç³»åˆ—éªŒè¯æ­¥éª¤ï¼ŒåŒ…æ‹¬æ£€æŸ¥ Docker çš„å¯ç”¨æ€§ã€è¿è¡Œç‰¹å®šè„šæœ¬æ¥å¯åŠ¨ç¯å¢ƒï¼Œå¹¶æ ¹æ®æ–‡æ¡£ä¸­çš„è¯´æ˜æ‰§è¡Œå®‰è£…å’Œé…ç½®å‘½ä»¤ã€‚

ä¸ºäº†ç®€åŒ–å¼€å‘ç¯å¢ƒçš„è®¾ç½®ï¼ŒRush å·¥å…·è¢«å¼•å…¥ä½œä¸ºå…¨çƒå®‰è£…é€‰é¡¹ï¼Œç”¨äºç®¡ç†é¡¹ç›®ä¾èµ–å…³ç³»å’Œæ„å»ºè¿‡ç¨‹ã€‚é€šè¿‡ä½¿ç”¨ Rushï¼Œç”¨æˆ·å¯ä»¥è½»æ¾åœ°å®‰è£…é¡¹ç›®æ‰€éœ€çš„æ‰€æœ‰ä¾èµ–é¡¹ã€æ„å»ºé¡¹ç›®å¹¶éªŒè¯æºä»£ç ã€‚

æ­¤å¤–ï¼Œä¸ºäº†æ”¯æŒè‡ªæ‰˜ç®¡ç¯å¢ƒï¼ŒHuly æä¾›äº†é¢„å®‰è£…è„šæœ¬ï¼Œå…è®¸ç”¨æˆ·å¿«é€Ÿæ‰§è¡Œå…³é”®æ“ä½œï¼Œå¦‚åˆ›å»ºå·¥ä½œç©ºé—´ã€åˆ›å»ºè´¦æˆ·ã€é…ç½®ç¯å¢ƒå’Œåˆ†é…æƒé™ã€‚è¿™äº›è„šæœ¬ç®€åŒ–äº†æ•´ä¸ªæµç¨‹ï¼Œä½¿å¾—éƒ¨ç½²å’Œç®¡ç†å˜å¾—é«˜æ•ˆä¸”ç›´è§‚ã€‚

æœ€åï¼ŒHuly å¹³å°æ”¯æŒå¤šç§é›†æˆï¼Œå°½ç®¡æœ¬åœ°å®‰è£…å¯èƒ½æ— æ³•å®ç°æ‰€æœ‰åŠŸèƒ½ï¼Œä½†å…³é”®é›†æˆå¦‚ç”µå­é‚®ä»¶å‘é€ã€ç¬¬ä¸‰æ–¹å†…å®¹æºé›†æˆä»¥åŠä¸ Telegramã€Gmail ç­‰æœåŠ¡çš„é›†æˆä»ç„¶å¯ä»¥ä½¿ç”¨ã€‚åŒæ—¶ï¼Œå¹³å°è¿˜æä¾›äº†å¼€å‘æ¨¡å¼ä»¥ä¼˜åŒ–å¼€å‘ä½“éªŒï¼Œå¹¶æä¾›äº†æµ‹è¯•ç­–ç•¥æ¥ç¡®ä¿ä»£ç è´¨é‡å’ŒåŠŸèƒ½å®Œæ•´æ€§ã€‚

æ€»ä¹‹ï¼ŒHuly Platform é€šè¿‡æä¾›è‡ªæ‰˜ç®¡é€‰é¡¹ã€é¢„å®‰è£…è„šæœ¬ã€å¼ºå¤§çš„å¼€å‘å·¥å…·ï¼ˆå¦‚ Rushï¼‰å’Œå¹¿æ³›çš„é›†æˆæ”¯æŒï¼Œä¸ºå¼€å‘è€…æä¾›äº†ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨ç®€åŒ–ä¼ä¸šåº”ç”¨çš„å¼€å‘ã€éƒ¨ç½²å’Œç»´æŠ¤è¿‡ç¨‹ã€‚ <div>
<p>Huly â€” All-in-One Project Management Platform (alternative to Linear, Jira, Slack, Notion, Motion)</p><hr /><h1>Huly Platform</h1> 
<p><a href="https://x.com/huly_io"><img alt="X (formerly Twitter) Follow" src="https://img.shields.io/twitter/follow/huly_io?style=for-the-badge" /></a> <img alt="GitHub License" src="https://img.shields.io/github/license/hcengineering/platform?style=for-the-badge" /></p> 
<p>â­ï¸ Your star shines on us. Star us on GitHub!</p> 
<h2>About</h2> 
<p>The Huly Platform is a robust framework designed to accelerate the development of business applications, such as CRM systems. This repository includes several applications, such as Chat, Project Management, CRM, HRM, and ATS. Various teams are building products on top of the Platform, including <a href="https://huly.io">Huly</a> and <a href="https://tracex.co">TraceX</a>.</p> 
<p><img alt="Huly" src="https://repository-images.githubusercontent.com/392073243/6d27d5cc-38cd-4d88-affe-bb88b393180c" /></p> 
<h2>Self-Hosting</h2> 
<p>If you're primarily interested in self-hosting Huly without the intention to modify or contribute to its development, please use <a href="https://github.com/hcengineering/huly-selfhost">huly-selfhost</a>. This project offers a convenient method to host Huly using <code>docker</code>, designed for ease of use and quick setup. Explore this option to effortlessly enjoy Huly on your own server.</p> 
<h2>Activity</h2> 
<p><img alt="Alt" src="https://repobeats.axiom.co/api/embed/c42c99e21691fa60ea61b5cdf11c2e0647621534.svg?sanitize=true" title="Repobeats analytics image" /></p> 
<h2>Table of Content</h2> 
<ul> 
 <li><a href="https://raw.githubusercontent.com/hcengineering/platform/develop/#huly-platform">Huly Platform</a> 
  <ul> 
   <li><a href="https://raw.githubusercontent.com/hcengineering/platform/develop/#about">About</a></li> 
   <li><a href="https://raw.githubusercontent.com/hcengineering/platform/develop/#self-hosting">Self-Hosting</a></li> 
   <li><a href="https://raw.githubusercontent.com/hcengineering/platform/develop/#activity">Activity</a></li> 
   <li><a href="https://raw.githubusercontent.com/hcengineering/platform/develop/#table-of-content">Table of Content</a></li> 
   <li><a href="https://raw.githubusercontent.com/hcengineering/platform/develop/#pre-requisites">Pre-requisites</a></li> 
   <li><a href="https://raw.githubusercontent.com/hcengineering/platform/develop/#verification">Verification</a></li> 
   <li><a href="https://raw.githubusercontent.com/hcengineering/platform/develop/#installation">Installation</a></li> 
   <li><a href="https://raw.githubusercontent.com/hcengineering/platform/develop/#build-and-run">Build and run</a></li> 
   <li><a href="https://raw.githubusercontent.com/hcengineering/platform/develop/#run-in-development-mode">Run in development mode</a></li> 
   <li><a href="https://raw.githubusercontent.com/hcengineering/platform/develop/#update-project-structure-and-database">Update project structure and database</a></li> 
   <li><a href="https://raw.githubusercontent.com/hcengineering/platform/develop/#troubleshooting">Troubleshooting</a></li> 
   <li><a href="https://raw.githubusercontent.com/hcengineering/platform/develop/#build--watch">Build &amp; Watch</a></li> 
   <li><a href="https://raw.githubusercontent.com/hcengineering/platform/develop/#tests">Tests</a> 
    <ul> 
     <li><a href="https://raw.githubusercontent.com/hcengineering/platform/develop/#unit-tests">Unit tests</a></li> 
     <li><a href="https://raw.githubusercontent.com/hcengineering/platform/develop/#ui-tests">UI tests</a></li> 
    </ul> </li> 
   <li><a href="https://raw.githubusercontent.com/hcengineering/platform/develop/#package-publishing">Package publishing</a></li> 
   <li><a href="https://raw.githubusercontent.com/hcengineering/platform/develop/#additional-testing">Additional testing</a></li> 
  </ul> </li> 
</ul> 
<h2>Pre-requisites</h2> 
<ul> 
 <li>Before proceeding, ensure that your system meets the following requirements: 
  <ul> 
   <li><a href="https://nodejs.org/en/download/">Node.js</a> (v20.11.0 is required)</li> 
   <li><a href="https://docs.docker.com/get-docker/">Docker</a></li> 
   <li><a href="https://docs.docker.com/compose/install/">Docker Compose</a></li> 
  </ul> </li> 
</ul> 
<h2>Verification</h2> 
<p>To verify the installation, perform the following checks in your terminal:</p> 
<ul> 
 <li>Ensure that the <code>docker</code> commands are available: <pre><code class="language-bash">docker --version
docker compose version
</code></pre> </li> 
</ul> 
<h2>Fast start</h2> 
<pre><code class="language-bash">sh ./scripts/fast-start.sh
</code></pre> 
<h2>Installation</h2> 
<p>You need Microsoft's <a href="https://rushjs.io">rush</a> to install application.</p> 
<ol> 
 <li>Install Rush globally using the command: <pre><code class="language-bash">npm install -g @microsoft/rush
</code></pre> </li> 
 <li>Navigate to the repository root and run the following commands: <pre><code class="language-bash">rush install
rush build
</code></pre> </li> 
</ol> 
<p>Alternatively, you can just execute:</p> 
<pre><code class="language-bash">sh ./scripts/presetup-rush.sh
</code></pre> 
<h2>Build and run</h2> 
<p>Development environment setup requires Docker to be installed on system.</p> 
<p>Support is available for both amd64 and arm64 containers on Linux and macOS.</p> 
<pre><code class="language-bash">cd ./dev/
rush build    # Will build all the required packages. 
# rush rebuild  # could be used to omit build cache.
rush bundle   # Will prepare bundles.
rush package  # Will build all webpack packages.
rush validate # Will validate all sources with typescript and generate d.ts files required for ts-node execution.
rush svelte-check # Optional. svelte files validation using svelte-check.
rush docker:build   # Will build Docker containers for all applications in the local Docker environment.
rush docker:up # Will set up all the containers
</code></pre> 
<p>Be aware <code>rush docker:build</code> will automatically execute all required phases like build, bundle, package.</p> 
<p>Alternatively, you can just execute:</p> 
<pre><code class="language-bash">sh ./scripts/build.sh
</code></pre> 
<p>By default, Docker volumes named dev_db, dev_elastic, and dev_files will be created for the MongoDB, Elasticsearch, and MinIO instances.</p> 
<p>Before you can begin, you need to create a workspace and an account and associate it with the workspace.</p> 
<pre><code class="language-bash">cd ./tool # dev/tool in the repository root
rushx run-local create-workspace ws1 -w DevWorkspace # Create workspace
rushx run-local create-account user1 -p 1234 -f John -l Appleseed # Create account
rushx run-local configure ws1 --list --enable '*' # Enable all modules, even if they are not yet intended to be used by a wide audience.
rushx run-local assign-workspace user1 ws1 # Assign workspace to user.
rushx run-local confirm-email user1 # To allow the creation of additional test workspaces.

</code></pre> 
<p>Alternatively, you can just execute:</p> 
<pre><code class="language-bash">sh ./scripts/create-workspace.sh
</code></pre> 
<p>Accessing the URL <a href="http://localhost:8087">http://localhost:8087</a> will lead you to the app in production mode.</p> 
<p>Limitations:</p> 
<ul> 
 <li>Local installation does not support sending emails, thus disabling functionalities such as password recovery and email notifications.</li> 
 <li>Integrations with Telegram, Gmail, and other content sources are exclusively available as Docker containers, sourced from private repositories. However, these integrations are fully functional and can be utilized with the platform.</li> 
</ul> 
<h2>Run in development mode</h2> 
<p>Development mode allows for live reloading and a smoother development process.</p> 
<pre><code class="language-bash">cd dev/prod
rushx dev-server
</code></pre> 
<p>Then go to <a href="http://localhost:8080">http://localhost:8080</a></p> 
<p>Use the following login credentials:</p> 
<pre><code class="language-plain">Email: user1
Password: 1234
Workspace: ws1
</code></pre> 
<h2>Update project structure and database</h2> 
<p>If the project's structure is updated, it may be necessary to relink and rebuild the projects.</p> 
<pre><code class="language-bash">rush update
rush build
</code></pre> 
<p>It may also be necessary to upgrade the running database.</p> 
<pre><code class="language-bash">cd ./dev/tool
rushx upgrade -f
</code></pre> 
<h2>Troubleshooting</h2> 
<p>If a build fails, but the code is correct, try to delete the <a href="https://rushjs.io/pages/maintainer/build_cache/">build cache</a> and retry.</p> 
<pre><code class="language-bash"># from the project root
rm -rf common/temp/build-cache
</code></pre> 
<h2>Build &amp; Watch</h2> 
<p>For development purpose <code>rush build:watch</code> action could be used.</p> 
<p>It includes build and validate phases in watch mode.</p> 
<h2>Tests</h2> 
<h3>Unit tests</h3> 
<pre><code class="language-bash">rush test # To execute all tests

rushx test # For individual test execution inside a package directory
</code></pre> 
<h3>UI tests</h3> 
<pre><code class="language-bash">cd ./tests
rush build
rush bundle
rush docker:build
## creates test Docker containers and sets up test database
./prepare.sh
## runs UI tests
rushx uitest
</code></pre> 
<p>To execute tests in the development environment, please follow these steps:</p> 
<pre><code class="language-bash">cd ./tests
./create-local.sh ## use ./restore-local.sh if you only want to restore the workspace to a predefined initial state for sanity.
cd ./sanity
rushx dev-uitest # To execute all tests against the development environment.
rushx dev-debug -g 'pattern' # To execute tests in debug mode with only the matching test pattern.
</code></pre> 
<h2>Package publishing</h2> 
<pre><code class="language-bash">node ./common/scripts/bump.js -p projectName
</code></pre> 
<h2>Additional testing</h2> 
<p>This project is tested with BrowserStack.</p> 
<p><sub><sup>Â© 2024 <a href="https://hardcoreeng.com">Hardcore Engineering Inc</a>.</sup></sub></p>
]]></content:encoded>


</item>

<item>
<title>rustdesk/rustdesk</title>
<link>https://github.com/rustdesk/rustdesk</link>
<guid>https://github.com/rustdesk/rustdesk</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šRustDeskã€è¿œç¨‹æ¡Œé¢ã€è‡ªæ‰˜ç®¡ã€å¼€æºã€å®‰å…¨

æ€»ç»“:

RustDeskæ˜¯ä¸€æ¬¾åŸºäºRustè¯­è¨€ç¼–å†™çš„è¿œç¨‹æ¡Œé¢åº”ç”¨ï¼Œæ—¨åœ¨ä¸ºç”¨æˆ·æä¾›æ— éœ€é…ç½®å³å¯ä½¿ç”¨çš„ä¾¿æ·ä½“éªŒã€‚å®ƒå¼ºè°ƒæ•°æ®å®‰å…¨ä¸è‡ªä¸»æ§åˆ¶æƒï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä½¿ç”¨RustDeskæä¾›çš„ä¸­ç»§/è½¬æ¥æœåŠ¡å™¨ï¼Œæˆ–è‡ªè¡Œéƒ¨ç½²ã€‚RustDeskæ¬¢è¿æ‰€æœ‰äººçš„è´¡çŒ®ï¼Œæä¾›äº†è¯¦ç»†çš„å¼€å§‹æŒ‡å—ã€‚

ä¸ºäº†æ„å»ºRustDeskï¼Œå¼€å‘è€…éœ€è¦å‡†å¤‡Rustå’ŒC++çš„å¼€å‘ç¯å¢ƒï¼Œå¹¶å®‰è£…ä¾èµ–åº“å¦‚libvpxã€libyuvã€opusã€aomç­‰ã€‚æ„å»ºè¿‡ç¨‹æ¶‰åŠè®¾ç½®ç¯å¢ƒå˜é‡ã€å®‰è£…ä¾èµ–å’Œè¿è¡Œç‰¹å®šå‘½ä»¤æ¥ç¼–è¯‘ç¨‹åºã€‚å¯¹äºLinuxç”¨æˆ·ï¼Œæä¾›äº†ä¸åŒå‘è¡Œç‰ˆçš„å®‰è£…æŒ‡å¯¼ã€‚æ„å»ºRustDeskè¿˜å¯ä»¥é€šè¿‡Dockerå®¹å™¨ç®€åŒ–æµç¨‹ï¼Œè¿™å…è®¸åœ¨ä»»ä½•åœ°æ–¹å¿«é€Ÿæ„å»ºåº”ç”¨ã€‚

RustDeskçš„ä»£ç ç»“æ„å¤æ‚ï¼ŒåŒ…å«å¤šä¸ªæ¨¡å—ï¼Œå¦‚è§†é¢‘ç¼–ç ã€æ–‡ä»¶ä¼ è¾“ã€éŸ³é¢‘æœåŠ¡ç­‰ï¼Œä»¥æ”¯æŒå…¨é¢çš„è¿œç¨‹æ¡Œé¢åŠŸèƒ½ã€‚æ­¤å¤–ï¼ŒRustDeskè¿˜æ”¯æŒå¤šå¹³å°ï¼ŒåŒ…æ‹¬Windowsã€Linuxã€macOSç­‰æ“ä½œç³»ç»Ÿï¼Œå¹¶æä¾›Flutterç‰ˆæœ¬çš„å®¢æˆ·ç«¯ã€‚å…¶æ„å»ºè¿‡ç¨‹åŒ…å«äº†è¯¦ç»†çš„æ­¥éª¤è¯´æ˜ï¼Œä»¥ç¡®ä¿å¼€å‘è€…èƒ½å¤Ÿé¡ºåˆ©è¿›è¡Œæœ¬åœ°æ„å»ºã€‚

RustDeskç”±æ¬§ç›Ÿå…è´¹æœåŠ¡å™¨æ”¯æŒï¼Œè¯¥æœåŠ¡å™¨ç”±Codext GmbHæ…·æ…¨æä¾›ï¼Œç¡®ä¿äº†ç¨³å®šæ€§å’Œå¯é æ€§ï¼Œä¸ºç”¨æˆ·æä¾›äº†ä¸€ä¸ªå¯é çš„é€‰æ‹©ã€‚ <div>
<p>An open-source remote desktop application designed for self-hosting, as an alternative to TeamViewer.</p><hr /><p align="center"> <img alt="RustDesk - Your remote desktop" src="https://raw.githubusercontent.com/rustdesk/rustdesk/master/res/logo-header.svg?sanitize=true" /><br /> <a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#public-servers">Servers</a> â€¢ <a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#raw-steps-to-build">Build</a> â€¢ <a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#how-to-build-with-docker">Docker</a> â€¢ <a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#file-structure">Structure</a> â€¢ <a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#snapshot">Snapshot</a><br /> [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-UA.md">Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°</a>] | [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-CS.md">Äesky</a>] | [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ZH.md">ä¸­æ–‡</a>] | [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-HU.md">Magyar</a>] | [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ES.md">EspaÃ±ol</a>] | [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-FA.md">ÙØ§Ø±Ø³ÛŒ</a>] | [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-FR.md">FranÃ§ais</a>] | [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-DE.md">Deutsch</a>] | [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-PL.md">Polski</a>] | [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ID.md">Indonesian</a>] | [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-FI.md">Suomi</a>] | [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ML.md">à´®à´²à´¯à´¾à´³à´‚</a>] | [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-JP.md">æ—¥æœ¬èª</a>] | [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-NL.md">Nederlands</a>] | [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-IT.md">Italiano</a>] | [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-RU.md">Ğ ÑƒÑÑĞºĞ¸Ğ¹</a>] | [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-PTBR.md">PortuguÃªs (Brasil)</a>] | [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-EO.md">Esperanto</a>] | [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-KR.md">í•œêµ­ì–´</a>] | [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-AR.md">Ø§Ù„Ø¹Ø±Ø¨ÙŠ</a>] | [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-VN.md">Tiáº¿ng Viá»‡t</a>] | [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-DA.md">Dansk</a>] | [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-GR.md">Î•Î»Î»Î·Î½Î¹ÎºÎ¬</a>] | [<a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-TR.md">TÃ¼rkÃ§e</a>]<br /> <b>We need your help to translate this README, <a href="https://github.com/rustdesk/rustdesk/tree/master/src/lang">RustDesk UI</a> and <a href="https://github.com/rustdesk/doc.rustdesk.com">RustDesk Doc</a> to your native language</b> </p> 
<p>Chat with us: <a href="https://discord.gg/nDceKgxnkV">Discord</a> | <a href="https://twitter.com/rustdesk">Twitter</a> | <a href="https://www.reddit.com/r/rustdesk">Reddit</a></p> 
<p><a href="https://ko-fi.com/I2I04VU09"><img alt="ko-fi" src="https://ko-fi.com/img/githubbutton_sm.svg?sanitize=true" /></a></p> 
<p>Yet another remote desktop software, written in Rust. Works out of the box, no configuration required. You have full control of your data, with no concerns about security. You can use our rendezvous/relay server, <a href="https://rustdesk.com/server">set up your own</a>, or <a href="https://github.com/rustdesk/rustdesk-server-demo">write your own rendezvous/relay server</a>.</p> 
<p><img alt="image" src="https://user-images.githubusercontent.com/71636191/171661982-430285f0-2e12-4b1d-9957-4a58e375304d.png" /></p> 
<p>RustDesk welcomes contribution from everyone. See <a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/CONTRIBUTING.md">CONTRIBUTING.md</a> for help getting started.</p> 
<p><a href="https://github.com/rustdesk/rustdesk/wiki/FAQ"><strong>FAQ</strong></a></p> 
<p><a href="https://github.com/rustdesk/rustdesk/releases"><strong>BINARY DOWNLOAD</strong></a></p> 
<p><a href="https://github.com/rustdesk/rustdesk/releases/tag/nightly"><strong>NIGHTLY BUILD</strong></a></p> 
<p><a href="https://f-droid.org/en/packages/com.carriez.flutter_hbb"><img alt="Get it on F-Droid" height="80" src="https://fdroid.gitlab.io/artwork/badge/get-it-on.png" /></a></p> 
<h2>Dependencies</h2> 
<p>Desktop versions use Flutter or Sciter (deprecated) for GUI, this tutorial is for Sciter only, since it is easier and more friendly to start. Check out our <a href="https://github.com/rustdesk/rustdesk/raw/master/.github/workflows/flutter-build.yml">CI</a> for building Flutter version.</p> 
<p>Please download Sciter dynamic library yourself.</p> 
<p><a href="https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.win/x64/sciter.dll">Windows</a> | <a href="https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.lnx/x64/libsciter-gtk.so">Linux</a> | <a href="https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.osx/libsciter.dylib">macOS</a></p> 
<h2>Raw steps to build</h2> 
<ul> 
 <li> <p>Prepare your Rust development env and C++ build env</p> </li> 
 <li> <p>Install <a href="https://github.com/microsoft/vcpkg">vcpkg</a>, and set <code>VCPKG_ROOT</code> env variable correctly</p> 
  <ul> 
   <li>Windows: vcpkg install libvpx:x64-windows-static libyuv:x64-windows-static opus:x64-windows-static aom:x64-windows-static</li> 
   <li>Linux/macOS: vcpkg install libvpx libyuv opus aom</li> 
  </ul> </li> 
 <li> <p>run <code>cargo run</code></p> </li> 
</ul> 
<h2><a href="https://rustdesk.com/docs/en/dev/build/">Build</a></h2> 
<h2>How to build on Linux</h2> 
<h3>Ubuntu 18 (Debian 10)</h3> 
<pre><code class="language-sh">sudo apt install -y zip g++ gcc git curl wget nasm yasm libgtk-3-dev clang libxcb-randr0-dev libxdo-dev \
        libxfixes-dev libxcb-shape0-dev libxcb-xfixes0-dev libasound2-dev libpulse-dev cmake make \
        libclang-dev ninja-build libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev libpam0g-dev
</code></pre> 
<h3>openSUSE Tumbleweed</h3> 
<pre><code class="language-sh">sudo zypper install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libXfixes-devel cmake alsa-lib-devel gstreamer-devel gstreamer-plugins-base-devel xdotool-devel pam-devel
</code></pre> 
<h3>Fedora 28 (CentOS 8)</h3> 
<pre><code class="language-sh">sudo yum -y install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libxdo-devel libXfixes-devel pulseaudio-libs-devel cmake alsa-lib-devel gstreamer1-devel gstreamer1-plugins-base-devel pam-devel
</code></pre> 
<h3>Arch (Manjaro)</h3> 
<pre><code class="language-sh">sudo pacman -Syu --needed unzip git cmake gcc curl wget yasm nasm zip make pkg-config clang gtk3 xdotool libxcb libxfixes alsa-lib pipewire
</code></pre> 
<h3>Install vcpkg</h3> 
<pre><code class="language-sh">git clone https://github.com/microsoft/vcpkg
cd vcpkg
git checkout 2023.04.15
cd ..
vcpkg/bootstrap-vcpkg.sh
export VCPKG_ROOT=$HOME/vcpkg
vcpkg/vcpkg install libvpx libyuv opus aom
</code></pre> 
<h3>Fix libvpx (For Fedora)</h3> 
<pre><code class="language-sh">cd vcpkg/buildtrees/libvpx/src
cd *
./configure
sed -i 's/CFLAGS+=-I/CFLAGS+=-fPIC -I/g' Makefile
sed -i 's/CXXFLAGS+=-I/CXXFLAGS+=-fPIC -I/g' Makefile
make
cp libvpx.a $HOME/vcpkg/installed/x64-linux/lib/
cd
</code></pre> 
<h3>Build</h3> 
<pre><code class="language-sh">curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source $HOME/.cargo/env
git clone https://github.com/rustdesk/rustdesk
cd rustdesk
mkdir -p target/debug
wget https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.lnx/x64/libsciter-gtk.so
mv libsciter-gtk.so target/debug
VCPKG_ROOT=$HOME/vcpkg cargo run
</code></pre> 
<h2>How to build with Docker</h2> 
<p>Begin by cloning the repository and building the Docker container:</p> 
<pre><code class="language-sh">git clone https://github.com/rustdesk/rustdesk
cd rustdesk
docker build -t "rustdesk-builder" .
</code></pre> 
<p>Then, each time you need to build the application, run the following command:</p> 
<pre><code class="language-sh">docker run --rm -it -v $PWD:/home/user/rustdesk -v rustdesk-git-cache:/home/user/.cargo/git -v rustdesk-registry-cache:/home/user/.cargo/registry -e PUID="$(id -u)" -e PGID="$(id -g)" rustdesk-builder
</code></pre> 
<p>Note that the first build may take longer before dependencies are cached, subsequent builds will be faster. Additionally, if you need to specify different arguments to the build command, you may do so at the end of the command in the <code>&lt;OPTIONAL-ARGS&gt;</code> position. For instance, if you wanted to build an optimized release version, you would run the command above followed by <code>--release</code>. The resulting executable will be available in the target folder on your system, and can be run with:</p> 
<pre><code class="language-sh">target/debug/rustdesk
</code></pre> 
<p>Or, if you're running a release executable:</p> 
<pre><code class="language-sh">target/release/rustdesk
</code></pre> 
<p>Please ensure that you are running these commands from the root of the RustDesk repository, otherwise the application might not be able to find the required resources. Also note that other cargo subcommands such as <code>install</code> or <code>run</code> are not currently supported via this method as they would install or run the program inside the container instead of the host.</p> 
<h2>File Structure</h2> 
<ul> 
 <li><strong><a href="https://github.com/rustdesk/rustdesk/tree/master/libs/hbb_common">libs/hbb_common</a></strong>: video codec, config, tcp/udp wrapper, protobuf, fs functions for file transfer, and some other utility functions</li> 
 <li><strong><a href="https://github.com/rustdesk/rustdesk/tree/master/libs/scrap">libs/scrap</a></strong>: screen capture</li> 
 <li><strong><a href="https://github.com/rustdesk/rustdesk/tree/master/libs/enigo">libs/enigo</a></strong>: platform specific keyboard/mouse control</li> 
 <li><strong><a href="https://github.com/rustdesk/rustdesk/tree/master/libs/clipboard">libs/clipboard</a></strong>: file copy and paste implementation for Windows, Linux, macOS.</li> 
 <li><strong><a href="https://github.com/rustdesk/rustdesk/tree/master/src/ui">src/ui</a></strong>: obsolete Sciter UI (deprecated)</li> 
 <li><strong><a href="https://github.com/rustdesk/rustdesk/tree/master/src/server">src/server</a></strong>: audio/clipboard/input/video services, and network connections</li> 
 <li><strong><a href="https://github.com/rustdesk/rustdesk/tree/master/src/client.rs">src/client.rs</a></strong>: start a peer connection</li> 
 <li><strong><a href="https://github.com/rustdesk/rustdesk/tree/master/src/rendezvous_mediator.rs">src/rendezvous_mediator.rs</a></strong>: Communicate with <a href="https://github.com/rustdesk/rustdesk-server">rustdesk-server</a>, wait for remote direct (TCP hole punching) or relayed connection</li> 
 <li><strong><a href="https://github.com/rustdesk/rustdesk/tree/master/src/platform">src/platform</a></strong>: platform specific code</li> 
 <li><strong><a href="https://github.com/rustdesk/rustdesk/tree/master/flutter">flutter</a></strong>: Flutter code for desktop and mobile</li> 
 <li><strong><a href="https://github.com/rustdesk/rustdesk/tree/master/flutter/web/js">flutter/web/js</a></strong>: JavaScript for Flutter web client</li> 
</ul> 
<h2>Screenshots</h2> 
<p><img alt="Connection Manager" src="https://github.com/rustdesk/rustdesk/assets/28412477/db82d4e7-c4bc-4823-8e6f-6af7eadf7651" /></p> 
<p><img alt="Connected to a Windows PC" src="https://github.com/rustdesk/rustdesk/assets/28412477/9baa91e9-3362-4d06-aa1a-7518edcbd7ea" /></p> 
<p><img alt="File Transfer" src="https://github.com/rustdesk/rustdesk/assets/28412477/39511ad3-aa9a-4f8c-8947-1cce286a46ad" /></p> 
<p><img alt="TCP Tunneling" src="https://github.com/rustdesk/rustdesk/assets/28412477/78e8708f-e87e-4570-8373-1360033ea6c5" /></p> 
<h2><a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#public-servers">Public Servers</a></h2> 
<p>RustDesk is supported by a free EU server, graciously provided by Codext GmbH</p> 
<p align="center"> <a href="https://codext.link/rustdesk?utm_source=github"> <img alt="Codext GmbH" src="https://camo.githubusercontent.com/aaf1c033d7b35f067414edd769a63320d028e9edf256789641aab5f02786c370/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f636f6f6c6c616273696f2f6f7267616e697a6174696f6e2f342f6176617461722e737667" /> </a> </p>
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>huggingface/lerobot</title>
<link>https://github.com/huggingface/lerobot</link>
<guid>https://github.com/huggingface/lerobot</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šLeRobotã€AIã€æœºå™¨äººã€é¢„è®­ç»ƒæ¨¡å‹ã€çœŸå®ä¸–ç•Œåº”ç”¨

æ€»ç»“ï¼š

LeRobotæ˜¯ä¸€ä¸ªæ—¨åœ¨é™ä½æœºå™¨äººé¢†åŸŸå…¥é—¨é—¨æ§›çš„å¼€æºé¡¹ç›®ï¼Œç›®æ ‡æ˜¯æä¾›åŸºäºPyTorchçš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€æ•°æ®é›†å’Œå·¥å…·ï¼Œä»¥ä¿ƒè¿›çœŸå®ä¸–ç•Œçš„æœºå™¨äººåº”ç”¨ã€‚å®ƒåŒ…å«äº†æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„æœ€æ–°æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•å·²è¢«è¯æ˜å¯ä»¥åœ¨çœŸå®ç¯å¢ƒä¸­è¿›è¡Œè¿ç§»ã€‚LeRobotä¸ä»…æä¾›äº†é¢„è®­ç»ƒçš„æ¨¡å‹å’Œäººç±»æ¼”ç¤ºçš„æ•°æ®é›†ï¼Œè¿˜ä¸ºç”¨æˆ·æä¾›äº†ä¸€ä¸ªæ˜“äºä¸Šæ‰‹çš„ç¯å¢ƒæ¥å¼€å§‹æ„å»ºè‡ªå·±çš„æœºå™¨äººç³»ç»Ÿã€‚

LeRobotæ”¯æŒå¤šç§é¢„è®­ç»ƒæ¨¡å‹åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„ä½¿ç”¨ï¼ŒåŒ…æ‹¬ACTç­–ç•¥ã€TDMPCç­–ç•¥å’ŒDiffusionç­–ç•¥ã€‚ç”¨æˆ·å¯ä»¥ä¸‹è½½è¿™äº›æ¨¡å‹å¹¶è¯„ä¼°å®ƒä»¬åœ¨ç‰¹å®šç¯å¢ƒä¸­çš„è¡¨ç°ï¼Œä¾‹å¦‚ALOHAã€SimXArmæˆ–PushTç¯å¢ƒã€‚

è¯¥é¡¹ç›®ç‰¹åˆ«å¼ºè°ƒäº†ä¸ç¤¾åŒºçš„äº’åŠ¨å’Œè´¡çŒ®ï¼Œé¼“åŠ±ç”¨æˆ·ä¸Šä¼ è‡ªå·±çš„æ•°æ®é›†å’Œæ¨¡å‹åˆ°Hugging Faceç¤¾åŒºé¡µé¢ã€‚è¿™ä¸ä»…ä¿ƒè¿›äº†çŸ¥è¯†å…±äº«ï¼Œä¹ŸåŠ é€Ÿäº†çœŸå®ä¸–ç•Œæœºå™¨äººæŠ€æœ¯çš„å‘å±•ã€‚LeRobotè¿˜æä¾›äº†ä¸€å¥—å®Œæ•´çš„å¼€å‘æµç¨‹å’ŒæŒ‡å—ï¼Œä»å®‰è£…ç¯å¢ƒã€å¯è§†åŒ–æ•°æ®é›†åˆ°è®­ç»ƒå’Œè¯„ä¼°è‡ªå®šä¹‰ç­–ç•¥ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿå¿«é€Ÿä¸Šæ‰‹å¹¶æ¢ç´¢ä¸åŒçš„æœºå™¨äººä»»åŠ¡ã€‚

LeRobotçš„ç›®æ ‡æ˜¯è®©æ¯ä¸ªäººéƒ½èƒ½å‚ä¸åˆ°æœºå™¨äººæŠ€æœ¯çš„åˆ›æ–°ä¸­æ¥ï¼Œæ— è®ºæ˜¯é€šè¿‡ä½¿ç”¨ç°æœ‰çš„èµ„æºè¿˜æ˜¯é€šè¿‡è´¡çŒ®è‡ªå·±çš„å·¥ä½œæˆæœï¼Œä»è€Œæ¨åŠ¨è¿™ä¸€é¢†åŸŸçš„è¿›æ­¥ã€‚ <div>
<p>ğŸ¤— LeRobot: Making AI for Robotics more accessible with end-to-end learning</p><hr /><p align="center"> 
  
  <source media="(prefers-color-scheme: dark)" /> 
  <source media="(prefers-color-scheme: light)" /> 
  <img alt="LeRobot, Hugging Face Robotics Library" src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/lerobot-logo-thumbnail.png" /> 
  <br /> <br /> </p> 
<div align="center"> 
 <p><a href="https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml?query=branch%3Amain"><img alt="Tests" src="https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml/badge.svg?branch=main" /></a> <a href="https://codecov.io/gh/huggingface/lerobot"><img alt="Coverage" src="https://codecov.io/gh/huggingface/lerobot/branch/main/graph/badge.svg?token=TODO" /></a> <a href="https://www.python.org/downloads/"><img alt="Python versions" src="https://img.shields.io/pypi/pyversions/lerobot" /></a> <a href="https://github.com/huggingface/lerobot/raw/main/LICENSE"><img alt="License" src="https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true" /></a> <a href="https://pypi.org/project/lerobot/"><img alt="Status" src="https://img.shields.io/pypi/status/lerobot" /></a> <a href="https://pypi.org/project/lerobot/"><img alt="Version" src="https://img.shields.io/pypi/v/lerobot" /></a> <a href="https://github.com/huggingface/lerobot/tree/main/examples"><img alt="Examples" src="https://img.shields.io/badge/Examples-green.svg?sanitize=true" /></a> <a href="https://github.com/huggingface/lerobot/raw/main/CODE_OF_CONDUCT.md"><img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-v2.1%20adopted-ff69b4.svg?sanitize=true" /></a> <a href="https://discord.gg/s3KuuzsPFb"><img alt="Discord" src="https://dcbadge.vercel.app/api/server/C5P34WJ68S?style=flat" /></a></p> 
</div> 
<h2 align="center"> <p><a href="https://github.com/huggingface/lerobot/raw/main/examples/7_get_started_with_real_robot.md">Hot new tutorial: Getting started with real-world robots</a></p> </h2> 
<div align="center"> 
 <img alt="Koch v1.1 leader and follower arms" src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/tutorial/koch_v1_1_leader_follower.webp?raw=true" title="Koch v1.1 leader and follower arms" width="50%" /> 
 <p>We just dropped an in-depth tutorial on how to build your own robot!</p> 
 <p>Teach it new skills by showing it a few moves with just a laptop.</p> 
 <p>Then watch your homemade robot act autonomously ğŸ¤¯</p> 
 <p>For more info, see <a href="https://x.com/RemiCadene/status/1825455895561859185">our thread on X</a> or <a href="https://github.com/huggingface/lerobot/raw/main/examples/7_get_started_with_real_robot.md">our tutorial page</a>.</p> 
</div> 
<br /> 
<h3 align="center"> <p>LeRobot: State-of-the-art AI for real-world robotics</p> </h3> 
<hr /> 
<p>ğŸ¤— LeRobot aims to provide models, datasets, and tools for real-world robotics in PyTorch. The goal is to lower the barrier to entry to robotics so that everyone can contribute and benefit from sharing datasets and pretrained models.</p> 
<p>ğŸ¤— LeRobot contains state-of-the-art approaches that have been shown to transfer to the real-world with a focus on imitation learning and reinforcement learning.</p> 
<p>ğŸ¤— LeRobot already provides a set of pretrained models, datasets with human collected demonstrations, and simulation environments to get started without assembling a robot. In the coming weeks, the plan is to add more and more support for real-world robotics on the most affordable and capable robots out there.</p> 
<p>ğŸ¤— LeRobot hosts pretrained models and datasets on this Hugging Face community page: <a href="https://huggingface.co/lerobot">huggingface.co/lerobot</a></p> 
<h4>Examples of pretrained models on simulation environments</h4> 
<table> 
 <tbody>
  <tr> 
   <td><img alt="ACT policy on ALOHA env" src="http://remicadene.com/assets/gif/aloha_act.gif" width="100%" /></td> 
   <td><img alt="TDMPC policy on SimXArm env" src="http://remicadene.com/assets/gif/simxarm_tdmpc.gif" width="100%" /></td> 
   <td><img alt="Diffusion policy on PushT env" src="http://remicadene.com/assets/gif/pusht_diffusion.gif" width="100%" /></td> 
  </tr> 
  <tr> 
   <td align="center">ACT policy on ALOHA env</td> 
   <td align="center">TDMPC policy on SimXArm env</td> 
   <td align="center">Diffusion policy on PushT env</td> 
  </tr> 
 </tbody>
</table> 
<h3>Acknowledgment</h3> 
<ul> 
 <li>Thanks to Tony Zaho, Zipeng Fu and colleagues for open sourcing ACT policy, ALOHA environments and datasets. Ours are adapted from <a href="https://tonyzhaozh.github.io/aloha">ALOHA</a> and <a href="https://mobile-aloha.github.io">Mobile ALOHA</a>.</li> 
 <li>Thanks to Cheng Chi, Zhenjia Xu and colleagues for open sourcing Diffusion policy, Pusht environment and datasets, as well as UMI datasets. Ours are adapted from <a href="https://diffusion-policy.cs.columbia.edu">Diffusion Policy</a> and <a href="https://umi-gripper.github.io">UMI Gripper</a>.</li> 
 <li>Thanks to Nicklas Hansen, Yunhai Feng and colleagues for open sourcing TDMPC policy, Simxarm environments and datasets. Ours are adapted from <a href="https://github.com/nicklashansen/tdmpc">TDMPC</a> and <a href="https://www.yunhaifeng.com/FOWM">FOWM</a>.</li> 
 <li>Thanks to Antonio Loquercio and Ashish Kumar for their early support.</li> 
 <li>Thanks to <a href="https://sjlee.cc/">Seungjae (Jay) Lee</a>, <a href="https://mahis.life/">Mahi Shafiullah</a> and colleagues for open sourcing <a href="https://sjlee.cc/vq-bet/">VQ-BeT</a> policy and helping us adapt the codebase to our repository. The policy is adapted from <a href="https://github.com/jayLEE0301/vq_bet_official">VQ-BeT repo</a>.</li> 
</ul> 
<h2>Installation</h2> 
<p>Download our source code:</p> 
<pre><code class="language-bash">git clone https://github.com/huggingface/lerobot.git
cd lerobot
</code></pre> 
<p>Create a virtual environment with Python 3.10 and activate it, e.g. with <a href="https://docs.anaconda.com/free/miniconda/index.html"><code>miniconda</code></a>:</p> 
<pre><code class="language-bash">conda create -y -n lerobot python=3.10
conda activate lerobot
</code></pre> 
<p>Install ğŸ¤— LeRobot:</p> 
<pre><code class="language-bash">pip install -e .
</code></pre> 
<blockquote> 
 <p><strong>NOTE:</strong> Depending on your platform, If you encounter any build errors during this step you may need to install <code>cmake</code> and <code>build-essential</code> for building some of our dependencies. On linux: <code>sudo apt-get install cmake build-essential</code></p> 
</blockquote> 
<p>For simulations, ğŸ¤— LeRobot comes with gymnasium environments that can be installed as extras:</p> 
<ul> 
 <li><a href="https://github.com/huggingface/gym-aloha">aloha</a></li> 
 <li><a href="https://github.com/huggingface/gym-xarm">xarm</a></li> 
 <li><a href="https://github.com/huggingface/gym-pusht">pusht</a></li> 
</ul> 
<p>For instance, to install ğŸ¤— LeRobot with aloha and pusht, use:</p> 
<pre><code class="language-bash">pip install -e ".[aloha, pusht]"
</code></pre> 
<p>To use <a href="https://docs.wandb.ai/quickstart">Weights and Biases</a> for experiment tracking, log in with</p> 
<pre><code class="language-bash">wandb login
</code></pre> 
<p>(note: you will also need to enable WandB in the configuration. See below.)</p> 
<h2>Walkthrough</h2> 
<pre><code>.
â”œâ”€â”€ examples             # contains demonstration examples, start here to learn about LeRobot
|   â””â”€â”€ advanced         # contains even more examples for those who have mastered the basics
â”œâ”€â”€ lerobot
|   â”œâ”€â”€ configs          # contains hydra yaml files with all options that you can override in the command line
|   |   â”œâ”€â”€ default.yaml   # selected by default, it loads pusht environment and diffusion policy
|   |   â”œâ”€â”€ env            # various sim environments and their datasets: aloha.yaml, pusht.yaml, xarm.yaml
|   |   â””â”€â”€ policy         # various policies: act.yaml, diffusion.yaml, tdmpc.yaml
|   â”œâ”€â”€ common           # contains classes and utilities
|   |   â”œâ”€â”€ datasets       # various datasets of human demonstrations: aloha, pusht, xarm
|   |   â”œâ”€â”€ envs           # various sim environments: aloha, pusht, xarm
|   |   â”œâ”€â”€ policies       # various policies: act, diffusion, tdmpc
|   |   â”œâ”€â”€ robot_devices  # various real devices: dynamixel motors, opencv cameras, koch robots
|   |   â””â”€â”€ utils          # various utilities
|   â””â”€â”€ scripts          # contains functions to execute via command line
|       â”œâ”€â”€ eval.py                 # load policy and evaluate it on an environment
|       â”œâ”€â”€ train.py                # train a policy via imitation learning and/or reinforcement learning
|       â”œâ”€â”€ control_robot.py        # teleoperate a real robot, record data, run a policy
|       â”œâ”€â”€ push_dataset_to_hub.py  # convert your dataset into LeRobot dataset format and upload it to the Hugging Face hub
|       â””â”€â”€ visualize_dataset.py    # load a dataset and render its demonstrations
â”œâ”€â”€ outputs               # contains results of scripts execution: logs, videos, model checkpoints
â””â”€â”€ tests                 # contains pytest utilities for continuous integration
</code></pre> 
<h3>Visualize datasets</h3> 
<p>Check out <a href="https://raw.githubusercontent.com/huggingface/lerobot/main/examples/1_load_lerobot_dataset.py">example 1</a> that illustrates how to use our dataset class which automatically download data from the Hugging Face hub.</p> 
<p>You can also locally visualize episodes from a dataset on the hub by executing our script from the command line:</p> 
<pre><code class="language-bash">python lerobot/scripts/visualize_dataset.py \
    --repo-id lerobot/pusht \
    --episode-index 0
</code></pre> 
<p>or from a dataset in a local folder with the root <code>DATA_DIR</code> environment variable (in the following case the dataset will be searched for in <code>./my_local_data_dir/lerobot/pusht</code>)</p> 
<pre><code class="language-bash">DATA_DIR='./my_local_data_dir' python lerobot/scripts/visualize_dataset.py \
    --repo-id lerobot/pusht \
    --episode-index 0
</code></pre> 
<p>It will open <code>rerun.io</code> and display the camera streams, robot states and actions, like this:</p> 
<p><a href="https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20240505T172924Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;X-Amz-SignedHeaders=host&amp;actor_id=24889239&amp;key_id=0&amp;repo_id=748713144">https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20240505T172924Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;X-Amz-SignedHeaders=host&amp;actor_id=24889239&amp;key_id=0&amp;repo_id=748713144</a></p> 
<p>Our script can also visualize datasets stored on a distant server. See <code>python lerobot/scripts/visualize_dataset.py --help</code> for more instructions.</p> 
<h3>The <code>LeRobotDataset</code> format</h3> 
<p>A dataset in <code>LeRobotDataset</code> format is very simple to use. It can be loaded from a repository on the Hugging Face hub or a local folder simply with e.g. <code>dataset = LeRobotDataset("lerobot/aloha_static_coffee")</code> and can be indexed into like any Hugging Face and PyTorch dataset. For instance <code>dataset[0]</code> will retrieve a single temporal frame from the dataset containing observation(s) and an action as PyTorch tensors ready to be fed to a model.</p> 
<p>A specificity of <code>LeRobotDataset</code> is that, rather than retrieving a single frame by its index, we can retrieve several frames based on their temporal relationship with the indexed frame, by setting <code>delta_timestamps</code> to a list of relative times with respect to the indexed frame. For example, with <code>delta_timestamps = {"observation.image": [-1, -0.5, -0.2, 0]}</code> one can retrieve, for a given index, 4 frames: 3 "previous" frames 1 second, 0.5 seconds, and 0.2 seconds before the indexed frame, and the indexed frame itself (corresponding to the 0 entry). See example <a href="https://raw.githubusercontent.com/huggingface/lerobot/main/examples/1_load_lerobot_dataset.py">1_load_lerobot_dataset.py</a> for more details on <code>delta_timestamps</code>.</p> 
<p>Under the hood, the <code>LeRobotDataset</code> format makes use of several ways to serialize data which can be useful to understand if you plan to work more closely with this format. We tried to make a flexible yet simple dataset format that would cover most type of features and specificities present in reinforcement learning and robotics, in simulation and in real-world, with a focus on cameras and robot states but easily extended to other types of sensory inputs as long as they can be represented by a tensor.</p> 
<p>Here are the important details and internal structure organization of a typical <code>LeRobotDataset</code> instantiated with <code>dataset = LeRobotDataset("lerobot/aloha_static_coffee")</code>. The exact features will change from dataset to dataset but not the main aspects:</p> 
<pre><code>dataset attributes:
  â”œ hf_dataset: a Hugging Face dataset (backed by Arrow/parquet). Typical features example:
  â”‚  â”œ observation.images.cam_high (VideoFrame):
  â”‚  â”‚   VideoFrame = {'path': path to a mp4 video, 'timestamp' (float32): timestamp in the video}
  â”‚  â”œ observation.state (list of float32): position of an arm joints (for instance)
  â”‚  ... (more observations)
  â”‚  â”œ action (list of float32): goal position of an arm joints (for instance)
  â”‚  â”œ episode_index (int64): index of the episode for this sample
  â”‚  â”œ frame_index (int64): index of the frame for this sample in the episode ; starts at 0 for each episode
  â”‚  â”œ timestamp (float32): timestamp in the episode
  â”‚  â”œ next.done (bool): indicates the end of en episode ; True for the last frame in each episode
  â”‚  â”” index (int64): general index in the whole dataset
  â”œ episode_data_index: contains 2 tensors with the start and end indices of each episode
  â”‚  â”œ from (1D int64 tensor): first frame index for each episode â€” shape (num episodes,) starts with 0
  â”‚  â”” to: (1D int64 tensor): last frame index for each episode â€” shape (num episodes,)
  â”œ stats: a dictionary of statistics (max, mean, min, std) for each feature in the dataset, for instance
  â”‚  â”œ observation.images.cam_high: {'max': tensor with same number of dimensions (e.g. `(c, 1, 1)` for images, `(c,)` for states), etc.}
  â”‚  ...
  â”œ info: a dictionary of metadata on the dataset
  â”‚  â”œ codebase_version (str): this is to keep track of the codebase version the dataset was created with
  â”‚  â”œ fps (float): frame per second the dataset is recorded/synchronized to
  â”‚  â”œ video (bool): indicates if frames are encoded in mp4 video files to save space or stored as png files
  â”‚  â”” encoding (dict): if video, this documents the main options that were used with ffmpeg to encode the videos
  â”œ videos_dir (Path): where the mp4 videos or png images are stored/accessed
  â”” camera_keys (list of string): the keys to access camera features in the item returned by the dataset (e.g. `["observation.images.cam_high", ...]`)
</code></pre> 
<p>A <code>LeRobotDataset</code> is serialised using several widespread file formats for each of its parts, namely:</p> 
<ul> 
 <li>hf_dataset stored using Hugging Face datasets library serialization to parquet</li> 
 <li>videos are stored in mp4 format to save space or png files</li> 
 <li>episode_data_index saved using <code>safetensor</code> tensor serialization format</li> 
 <li>stats saved using <code>safetensor</code> tensor serialization format</li> 
 <li>info are saved using JSON</li> 
</ul> 
<p>Dataset can be uploaded/downloaded from the HuggingFace hub seamlessly. To work on a local dataset, you can set the <code>DATA_DIR</code> environment variable to your root dataset folder as illustrated in the above section on dataset visualization.</p> 
<h3>Evaluate a pretrained policy</h3> 
<p>Check out <a href="https://raw.githubusercontent.com/huggingface/lerobot/main/examples/2_evaluate_pretrained_policy.py">example 2</a> that illustrates how to download a pretrained policy from Hugging Face hub, and run an evaluation on its corresponding environment.</p> 
<p>We also provide a more capable script to parallelize the evaluation over multiple environments during the same rollout. Here is an example with a pretrained model hosted on <a href="https://huggingface.co/lerobot/diffusion_pusht">lerobot/diffusion_pusht</a>:</p> 
<pre><code class="language-bash">python lerobot/scripts/eval.py \
    -p lerobot/diffusion_pusht \
    eval.n_episodes=10 \
    eval.batch_size=10
</code></pre> 
<p>Note: After training your own policy, you can re-evaluate the checkpoints with:</p> 
<pre><code class="language-bash">python lerobot/scripts/eval.py -p {OUTPUT_DIR}/checkpoints/last/pretrained_model
</code></pre> 
<p>See <code>python lerobot/scripts/eval.py --help</code> for more instructions.</p> 
<h3>Train your own policy</h3> 
<p>Check out <a href="https://raw.githubusercontent.com/huggingface/lerobot/main/examples/3_train_policy.py">example 3</a> that illustrates how to train a model using our core library in python, and <a href="https://raw.githubusercontent.com/huggingface/lerobot/main/examples/4_train_policy_with_script.md">example 4</a> that shows how to use our training script from command line.</p> 
<p>In general, you can use our training script to easily train any policy. Here is an example of training the ACT policy on trajectories collected by humans on the Aloha simulation environment for the insertion task:</p> 
<pre><code class="language-bash">python lerobot/scripts/train.py \
    policy=act \
    env=aloha \
    env.task=AlohaInsertion-v0 \
    dataset_repo_id=lerobot/aloha_sim_insertion_human \
</code></pre> 
<p>The experiment directory is automatically generated and will show up in yellow in your terminal. It looks like <code>outputs/train/2024-05-05/20-21-12_aloha_act_default</code>. You can manually specify an experiment directory by adding this argument to the <code>train.py</code> python command:</p> 
<pre><code class="language-bash">    hydra.run.dir=your/new/experiment/dir
</code></pre> 
<p>In the experiment directory there will be a folder called <code>checkpoints</code> which will have the following structure:</p> 
<pre><code class="language-bash">checkpoints
â”œâ”€â”€ 000250  # checkpoint_dir for training step 250
â”‚   â”œâ”€â”€ pretrained_model  # Hugging Face pretrained model dir
â”‚   â”‚   â”œâ”€â”€ config.json  # Hugging Face pretrained model config
â”‚   â”‚   â”œâ”€â”€ config.yaml  # consolidated Hydra config
â”‚   â”‚   â”œâ”€â”€ model.safetensors  # model weights
â”‚   â”‚   â””â”€â”€ README.md  # Hugging Face model card
â”‚   â””â”€â”€ training_state.pth  # optimizer/scheduler/rng state and training step
</code></pre> 
<p>To use wandb for logging training and evaluation curves, make sure you've run <code>wandb login</code> as a one-time setup step. Then, when running the training command above, enable WandB in the configuration by adding:</p> 
<pre><code class="language-bash">    wandb.enable=true
</code></pre> 
<p>A link to the wandb logs for the run will also show up in yellow in your terminal. Here is an example of what they look like in your browser:</p> 
<p><img alt="" src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/wandb.png" /></p> 
<p>Note: For efficiency, during training every checkpoint is evaluated on a low number of episodes. You may use <code>eval.n_episodes=500</code> to evaluate on more episodes than the default. Or, after training, you may want to re-evaluate your best checkpoints on more episodes or change the evaluation settings. See <code>python lerobot/scripts/eval.py --help</code> for more instructions.</p> 
<h4>Reproduce state-of-the-art (SOTA)</h4> 
<p>We have organized our configuration files (found under <a href="https://raw.githubusercontent.com/huggingface/lerobot/main/lerobot/configs"><code>lerobot/configs</code></a>) such that they reproduce SOTA results from a given model variant in their respective original works. Simply running:</p> 
<pre><code class="language-bash">python lerobot/scripts/train.py policy=diffusion env=pusht
</code></pre> 
<p>reproduces SOTA results for Diffusion Policy on the PushT task.</p> 
<p>Pretrained policies, along with reproduction details, can be found under the "Models" section of <a href="https://huggingface.co/lerobot">https://huggingface.co/lerobot</a>.</p> 
<h2>Contribute</h2> 
<p>If you would like to contribute to ğŸ¤— LeRobot, please check out our <a href="https://github.com/huggingface/lerobot/raw/main/CONTRIBUTING.md">contribution guide</a>.</p> 
<h3>Add a new dataset</h3> 
<p>To add a dataset to the hub, you need to login using a write-access token, which can be generated from the <a href="https://huggingface.co/settings/tokens">Hugging Face settings</a>:</p> 
<pre><code class="language-bash">huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential
</code></pre> 
<p>Then point to your raw dataset folder (e.g. <code>data/aloha_static_pingpong_test_raw</code>), and push your dataset to the hub with:</p> 
<pre><code class="language-bash">python lerobot/scripts/push_dataset_to_hub.py \
--raw-dir data/aloha_static_pingpong_test_raw \
--out-dir data \
--repo-id lerobot/aloha_static_pingpong_test \
--raw-format aloha_hdf5
</code></pre> 
<p>See <code>python lerobot/scripts/push_dataset_to_hub.py --help</code> for more instructions.</p> 
<p>If your dataset format is not supported, implement your own in <code>lerobot/common/datasets/push_dataset_to_hub/${raw_format}_format.py</code> by copying examples like <a href="https://github.com/huggingface/lerobot/raw/main/lerobot/common/datasets/push_dataset_to_hub/pusht_zarr_format.py">pusht_zarr</a>, <a href="https://github.com/huggingface/lerobot/raw/main/lerobot/common/datasets/push_dataset_to_hub/umi_zarr_format.py">umi_zarr</a>, <a href="https://github.com/huggingface/lerobot/raw/main/lerobot/common/datasets/push_dataset_to_hub/aloha_hdf5_format.py">aloha_hdf5</a>, or <a href="https://github.com/huggingface/lerobot/raw/main/lerobot/common/datasets/push_dataset_to_hub/xarm_pkl_format.py">xarm_pkl</a>.</p> 
<h3>Add a pretrained policy</h3> 
<p>Once you have trained a policy you may upload it to the Hugging Face hub using a hub id that looks like <code>${hf_user}/${repo_name}</code> (e.g. <a href="https://huggingface.co/lerobot/diffusion_pusht">lerobot/diffusion_pusht</a>).</p> 
<p>You first need to find the checkpoint folder located inside your experiment directory (e.g. <code>outputs/train/2024-05-05/20-21-12_aloha_act_default/checkpoints/002500</code>). Within that there is a <code>pretrained_model</code> directory which should contain:</p> 
<ul> 
 <li><code>config.json</code>: A serialized version of the policy configuration (following the policy's dataclass config).</li> 
 <li><code>model.safetensors</code>: A set of <code>torch.nn.Module</code> parameters, saved in <a href="https://huggingface.co/docs/safetensors/index">Hugging Face Safetensors</a> format.</li> 
 <li><code>config.yaml</code>: A consolidated Hydra training configuration containing the policy, environment, and dataset configs. The policy configuration should match <code>config.json</code> exactly. The environment config is useful for anyone who wants to evaluate your policy. The dataset config just serves as a paper trail for reproducibility.</li> 
</ul> 
<p>To upload these to the hub, run the following:</p> 
<pre><code class="language-bash">huggingface-cli upload ${hf_user}/${repo_name} path/to/pretrained_model
</code></pre> 
<p>See <a href="https://github.com/huggingface/lerobot/raw/main/lerobot/scripts/eval.py">eval.py</a> for an example of how other people may use your policy.</p> 
<h3>Improve your code with profiling</h3> 
<p>An example of a code snippet to profile the evaluation of a policy:</p> 
<pre><code class="language-python">from torch.profiler import profile, record_function, ProfilerActivity

def trace_handler(prof):
    prof.export_chrome_trace(f"tmp/trace_schedule_{prof.step_num}.json")

with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    schedule=torch.profiler.schedule(
        wait=2,
        warmup=2,
        active=3,
    ),
    on_trace_ready=trace_handler
) as prof:
    with record_function("eval_policy"):
        for i in range(num_episodes):
            prof.step()
            # insert code to profile, potentially whole body of eval_policy function
</code></pre> 
<h2>Citation</h2> 
<p>If you want, you can cite this work with:</p> 
<pre><code class="language-bibtex">@misc{cadene2024lerobot,
    author = {Cadene, Remi and Alibert, Simon and Soare, Alexander and Gallouedec, Quentin and Zouitine, Adil and Wolf, Thomas},
    title = {LeRobot: State-of-the-art Machine Learning for Real-World Robotics in Pytorch},
    howpublished = "\url{https://github.com/huggingface/lerobot}",
    year = {2024}
}
</code></pre> 
<p>Additionally, if you are using any of the particular policy architecture, pretrained models, or datasets, it is recommended to cite the original authors of the work as they appear below:</p> 
<ul> 
 <li><a href="https://diffusion-policy.cs.columbia.edu">Diffusion Policy</a></li> 
</ul> 
<pre><code class="language-bibtex">@article{chi2024diffusionpolicy,
	author = {Cheng Chi and Zhenjia Xu and Siyuan Feng and Eric Cousineau and Yilun Du and Benjamin Burchfiel and Russ Tedrake and Shuran Song},
	title ={Diffusion Policy: Visuomotor Policy Learning via Action Diffusion},
	journal = {The International Journal of Robotics Research},
	year = {2024},
}
</code></pre> 
<ul> 
 <li><a href="https://tonyzhaozh.github.io/aloha">ACT or ALOHA</a></li> 
</ul> 
<pre><code class="language-bibtex">@article{zhao2023learning,
  title={Learning fine-grained bimanual manipulation with low-cost hardware},
  author={Zhao, Tony Z and Kumar, Vikash and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:2304.13705},
  year={2023}
}
</code></pre> 
<ul> 
 <li><a href="https://www.nicklashansen.com/td-mpc/">TDMPC</a></li> 
</ul> 
<pre><code class="language-bibtex">@inproceedings{Hansen2022tdmpc,
	title={Temporal Difference Learning for Model Predictive Control},
	author={Nicklas Hansen and Xiaolong Wang and Hao Su},
	booktitle={ICML},
	year={2022}
}
</code></pre> 
<ul> 
 <li><a href="https://sjlee.cc/vq-bet/">VQ-BeT</a></li> 
</ul> 
<pre><code class="language-bibtex">@article{lee2024behavior,
  title={Behavior generation with latent actions},
  author={Lee, Seungjae and Wang, Yibin and Etukuru, Haritheja and Kim, H Jin and Shafiullah, Nur Muhammad Mahi and Pinto, Lerrel},
  journal={arXiv preprint arXiv:2403.03181},
  year={2024}
}
</code></pre>
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>microsoft/generative-ai-for-beginners</title>
<link>https://github.com/microsoft/generative-ai-for-beginners</link>
<guid>https://github.com/microsoft/generative-ai-for-beginners</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šMicrosoftã€Generative AIã€è¯¾ç¨‹ã€å¼€å‘ç¯å¢ƒã€å­¦ä¹ èµ„æº

æ€»ç»“:
Microsoftä¸ºåˆå­¦è€…æä¾›äº†â€œGenerative AI for Beginnersâ€è¯¾ç¨‹ï¼Œæ—¨åœ¨é€šè¿‡18ä¸ªæ¨¡å—æ•™æˆæ„å»ºç”Ÿæˆå¼äººå·¥æ™ºèƒ½åº”ç”¨æ‰€éœ€çš„åŸºç¡€çŸ¥è¯†ã€‚æœ¬è¯¾ç¨‹è¦†ç›–ä»å…¥é—¨åˆ°è¿›é˜¶çš„å†…å®¹ï¼ŒåŒ…æ‹¬è®¾ç½®å¼€å‘ç¯å¢ƒã€ç†è§£ç”Ÿæˆå¼AIåŠå…¶å·¥ä½œåŸç†ã€é€‰æ‹©åˆé€‚çš„æ¨¡å‹ã€è´Ÿè´£ä»»åœ°æ„å»ºåº”ç”¨ã€æœ€ä½³æç¤ºå·¥ç¨‹å®è·µã€æ¨¡å‹é›†æˆä¸åº”ç”¨ç­‰ã€‚è¯¾ç¨‹è¿˜æä¾›äº†Pythonå’ŒTypeScriptä»£ç ç¤ºä¾‹ï¼Œå¹¶é¼“åŠ±å­¦å‘˜åŠ å…¥ç¤¾åŒºè·å–æ”¯æŒã€‚

è¯¾ç¨‹å†…å®¹åŒ…æ‹¬è§†é¢‘ä»‹ç»ã€è¯¦ç»†é˜…è¯»ææ–™ã€ä»£ç ç¤ºä¾‹ä»¥åŠé¢å¤–çš„å­¦ä¹ èµ„æºé“¾æ¥ã€‚å­¦å‘˜éœ€è¦å…·å¤‡åŸºæœ¬çš„Pythonæˆ–TypeScriptçŸ¥è¯†ï¼Œå¹¶ä¸”å¯ä»¥è®¿é—®Azureæˆ–OpenAIçš„APIã€‚ä¸ºäº†å¸®åŠ©å­¦å‘˜å¿«é€Ÿä¸Šæ‰‹ï¼Œè¯¾ç¨‹æä¾›äº†ä¸€èŠ‚å…³äºè®¾ç½®å¼€å‘ç¯å¢ƒçš„æŒ‡å¯¼è¯¾ã€‚æ­¤å¤–ï¼Œå¯¹äºå¯»æ±‚æ›´é«˜çº§ä»£ç ç¤ºä¾‹çš„å­¦å‘˜ï¼Œè¯¾ç¨‹è¿˜æä¾›äº†Pythonå’ŒTypeScriptä»£ç åº“ã€‚

ä¸ºäº†ä¿ƒè¿›ç¤¾åŒºäº¤æµå’Œé¡¹ç›®åˆä½œï¼Œå‚ä¸è€…å¯ä»¥åŠ å…¥è¯¾ç¨‹çš„è®¨è®ºåŒºã€‚å¯¹æ„å»ºåˆåˆ›ä¼ä¸šæ„Ÿå…´è¶£çš„å­¦å‘˜ï¼Œå¯ä»¥æ³¨å†Œè·å–å…è´¹çš„OpenAIä¿¡ç”¨å’ŒAzureä¿¡ç”¨ï¼Œä»¥ä¾¿é€šè¿‡Azure OpenAIæœåŠ¡è®¿é—®OpenAIæ¨¡å‹ã€‚å¯¹äºå¸Œæœ›è´¡çŒ®è¯¾ç¨‹å†…å®¹çš„å­¦å‘˜ï¼Œä¹Ÿæä¾›äº†åé¦ˆæ¸ é“ã€‚

æ€»ä¹‹ï¼Œâ€œGenerative AI for Beginnersâ€è¯¾ç¨‹æ˜¯ä¸€ä¸ªå…¨é¢çš„èµ„æºï¼Œæ—¨åœ¨å¸®åŠ©åˆå­¦è€…ä»é›¶å¼€å§‹æ„å»ºç”Ÿæˆå¼AIåº”ç”¨ï¼ŒåŒæ—¶æä¾›äº†ä¸°å¯Œçš„å­¦ä¹ ææ–™ã€ç¤¾åŒºæ”¯æŒå’Œå®è·µæœºä¼šï¼Œä»¥åŠ é€Ÿå­¦ä¹ è¿‡ç¨‹å¹¶ä¿ƒè¿›æŠ€èƒ½æå‡ã€‚ <div>
<p>18 Lessons, Get Started Building with Generative AI ğŸ”— https://microsoft.github.io/generative-ai-for-beginners/</p><hr /><p><img alt="Generative AI For Beginners" src="https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/images/repo-thumbnailv3.png?WT.mc_id=academic-105485-koreyst" /></p> 
<h3>18 Lessons teaching everything you need to know to start building Generative AI applications</h3> 
<p><a href="https://github.com/microsoft/Generative-AI-For-Beginners/raw/master/LICENSE?WT.mc_id=academic-105485-koreyst"><img alt="GitHub license" src="https://img.shields.io/github/license/microsoft/Generative-AI-For-Beginners.svg?sanitize=true" /></a> <a href="https://GitHub.com/microsoft/Generative-AI-For-Beginners/graphs/contributors/?WT.mc_id=academic-105485-koreyst"><img alt="GitHub contributors" src="https://img.shields.io/github/contributors/microsoft/Generative-AI-For-Beginners.svg?sanitize=true" /></a> <a href="https://GitHub.com/microsoft/Generative-AI-For-Beginners/issues/?WT.mc_id=academic-105485-koreyst"><img alt="GitHub issues" src="https://img.shields.io/github/issues/microsoft/Generative-AI-For-Beginners.svg?sanitize=true" /></a> <a href="https://GitHub.com/microsoft/Generative-AI-For-Beginners/pulls/?WT.mc_id=academic-105485-koreyst"><img alt="GitHub pull-requests" src="https://img.shields.io/github/issues-pr/microsoft/Generative-AI-For-Beginners.svg?sanitize=true" /></a> <a href="http://makeapullrequest.com?WT.mc_id=academic-105485-koreyst"><img alt="PRs Welcome" src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square" /></a></p> 
<p><a href="https://GitHub.com/microsoft/Generative-AI-For-Beginners/watchers/?WT.mc_id=academic-105485-koreyst"><img alt="GitHub watchers" src="https://img.shields.io/github/watchers/microsoft/Generative-AI-For-Beginners.svg?style=social&amp;label=Watch" /></a> <a href="https://GitHub.com/microsoft/Generative-AI-For-Beginners/network/?WT.mc_id=academic-105485-koreyst"><img alt="GitHub forks" src="https://img.shields.io/github/forks/microsoft/Generative-AI-For-Beginners.svg?style=social&amp;label=Fork" /></a> <a href="https://GitHub.com/microsoft/Generative-AI-For-Beginners/stargazers/?WT.mc_id=academic-105485-koreyst"><img alt="GitHub stars" src="https://img.shields.io/github/stars/microsoft/Generative-AI-For-Beginners.svg?style=social&amp;label=Star" /></a></p> 
<p><a href="https://aka.ms/genai-discord?WT.mc_id=academic-105485-koreyst"><img alt="" src="https://dcbadge.vercel.app/api/server/ByRwuEEgH4" /></a></p> 
<h1>Generative AI for Beginners (Version 2) - A Course</h1> 
<p>Learn the fundamentals of building Generative AI applications with our 18-lesson comprehensive course by Microsoft Cloud Advocates.</p> 
<h2>ğŸŒ± Getting Started</h2> 
<p>This course has 18 lessons. Each lesson covers its own topic so start wherever you like!</p> 
<p>Lessons are labeled either "Learn" lessons explaining a Generative AI concept or "Build" lessons that explain a concept and code examples in both <strong>Python</strong> and <strong>TypeScript</strong> when possible.</p> 
<p>Each lesson also includes a "Keep Learning" section with additional learning tools.</p> 
<p><strong>What You Need</strong></p> 
<ul> 
 <li>Access to the <a href="https://azure.microsoft.com/products/ai-services/openai-service?WT.mc_id=academic-105485-koreyst">Azure OpenAI Service</a> <strong>OR</strong> <a href="https://platform.openai.com/docs/quickstart?context=python?WT.mc_id=academic-105485-koreyst">OpenAI API</a> <strong>OR</strong> <a href="https://github.com/marketplace/models?WT.mc_id=academic-105485-koreyst">GitHub Marketplace Model Catalog</a> - <em>Only required to complete coding lessons</em></li> 
 <li>Basic knowledge of Python or TypeScript is helpful - *For absolute beginners check out these <a href="https://learn.microsoft.com/training/paths/python-language/?WT.mc_id=academic-105485-koreyst">Python</a> and <a href="https://learn.microsoft.com/training/paths/build-javascript-applications-typescript/?WT.mc_id=academic-105485-koreyst">TypeScript</a> courses.</li> 
 <li>A GitHub account to <a href="https://github.com/microsoft/generative-ai-for-beginners/fork?WT.mc_id=academic-105485-koreyst">fork this entire repo</a> to your own GitHub account</li> 
</ul> 
<p>We have created a <strong><a href="https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/00-course-setup/README.md?WT.mc_id=academic-105485-koreyst">Course Setup</a></strong> lesson to help you with setting up your development environment.</p> 
<p>Don't forget to <a href="https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars?WT.mc_id=academic-105485-koreyst">star (ğŸŒŸ) this repo</a> to find it easier later.</p> 
<h2>ğŸ§  Ready to Deploy?</h2> 
<p>If you are looking for more advanced code samples, check out our <a href="https://aka.ms/genai-beg-code?WT.mc_id=academic-105485-koreyst">collection of Generative AI Code Samples</a> in both <strong>Python</strong> and <strong>TypeScript</strong>.</p> 
<h2>ğŸ—£ï¸ Meet Other Learners, Get Support</h2> 
<p>Join our <a href="https://aka.ms/genai-discord?WT.mc_id=academic-105485-koreyst">official AI Discord server</a> to meet and network with other learners taking this course and get support.</p> 
<h2>ğŸš€ Building a Startup?</h2> 
<p>Sign up for <a href="https://aka.ms/genai-foundershub?WT.mc_id=academic-105485-koreyst">Microsoft for Startups Founders Hub</a> to receive <strong>free OpenAI credits</strong> and up to <strong>$150k towards Azure credits to access OpenAI models through Azure OpenAI Services</strong>.</p> 
<h2>ğŸ™ Want to help?</h2> 
<p>Do you have suggestions or found spelling or code errors? <a href="https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst">Raise an issue</a> or <a href="https://github.com/microsoft/generative-ai-for-beginners/pulls?WT.mc_id=academic-105485-koreyst">Create a pull request</a></p> 
<h2>ğŸ“‚ Each lesson includes:</h2> 
<ul> 
 <li>A short video introduction to the topic</li> 
 <li>A written lesson located in the README</li> 
 <li>Python and TypeScript code samples supporting Azure OpenAI and OpenAI API</li> 
 <li>Links to extra resources to continue your learning</li> 
</ul> 
<h2>ğŸ—ƒï¸ Lessons</h2> 
<table> 
 <thead> 
  <tr> 
   <th>#</th> 
   <th><strong>Lesson Link</strong></th> 
   <th><strong>Description</strong></th> 
   <th><strong>Video</strong></th> 
   <th><strong>Extra Learning</strong></th> 
  </tr> 
 </thead> 
 <tbody> 
  <tr> 
   <td>00</td> 
   <td><a href="https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/00-course-setup/README.md?WT.mc_id=academic-105485-koreyst">Course Setup</a></td> 
   <td><strong>Learn:</strong> How to Setup Your Development Environment</td> 
   <td>Coming Soon</td> 
   <td><a href="https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst">Learn More</a></td> 
  </tr> 
  <tr> 
   <td>01</td> 
   <td><a href="https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/01-introduction-to-genai/README.md?WT.mc_id=academic-105485-koreyst">Introduction to Generative AI and LLMs</a></td> 
   <td><strong>Learn:</strong> Understanding what Generative AI is and how Large Language Models (LLMs) work.</td> 
   <td><a href="https://aka.ms/gen-ai-lesson-1-gh?WT.mc_id=academic-105485-koreyst">Video</a></td> 
   <td><a href="https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst">Learn More</a></td> 
  </tr> 
  <tr> 
   <td>02</td> 
   <td><a href="https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/02-exploring-and-comparing-different-llms/README.md?WT.mc_id=academic-105485-koreyst">Exploring and comparing different LLMs</a></td> 
   <td><strong>Learn:</strong> How to select the right model for your use case</td> 
   <td><a href="https://aka.ms/gen-ai-lesson2-gh?WT.mc_id=academic-105485-koreyst">Video</a></td> 
   <td><a href="https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst">Learn More</a></td> 
  </tr> 
  <tr> 
   <td>03</td> 
   <td><a href="https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/03-using-generative-ai-responsibly/README.md?WT.mc_id=academic-105485-koreyst">Using Generative AI Responsibly</a></td> 
   <td><strong>Learn:</strong> How to build Generative AI Applications responsibly</td> 
   <td><a href="https://aka.ms/gen-ai-lesson3-gh?WT.mc_id=academic-105485-koreyst">Video</a></td> 
   <td><a href="https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst">Learn More</a></td> 
  </tr> 
  <tr> 
   <td>04</td> 
   <td><a href="https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst">Understanding Prompt Engineering Fundamentals</a></td> 
   <td><strong>Learn:</strong> Hands-on Prompt Engineering Best Practices</td> 
   <td><a href="https://aka.ms/gen-ai-lesson4-gh?WT.mc_id=academic-105485-koreyst">Video</a></td> 
   <td><a href="https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst">Learn More</a></td> 
  </tr> 
  <tr> 
   <td>05</td> 
   <td><a href="https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/05-advanced-prompts/README.md?WT.mc_id=academic-105485-koreyst">Creating Advanced Prompts</a></td> 
   <td><strong>Learn:</strong> How to apply prompt engineering techniques that improve the outcome of your prompts.</td> 
   <td><a href="https://aka.ms/gen-ai-lesson5-gh?WT.mc_id=academic-105485-koreyst">Video</a></td> 
   <td><a href="https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst">Learn More</a></td> 
  </tr> 
  <tr> 
   <td>06</td> 
   <td><a href="https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/06-text-generation-apps/README.md?WT.mc_id=academic-105485-koreyst">Building Text Generation Applications</a></td> 
   <td><strong>Build:</strong> A text generation app using Azure OpenAI / OpenAI API</td> 
   <td><a href="https://aka.ms/gen-ai-lesson6-gh?WT.mc_id=academic-105485-koreyst">Video</a></td> 
   <td><a href="https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst">Learn More</a></td> 
  </tr> 
  <tr> 
   <td>07</td> 
   <td><a href="https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/07-building-chat-applications/README.md?WT.mc_id=academic-105485-koreyst">Building Chat Applications</a></td> 
   <td><strong>Build:</strong> Techniques for efficiently building and integrating chat applications.</td> 
   <td><a href="https://aka.ms/gen-ai-lessons7-gh?WT.mc_id=academic-105485-koreyst">Video</a></td> 
   <td><a href="https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst">Learn More</a></td> 
  </tr> 
  <tr> 
   <td>08</td> 
   <td><a href="https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst">Building Search Apps Vector Databases</a></td> 
   <td><strong>Build:</strong> A search application that uses Embeddings to search for data.</td> 
   <td><a href="https://aka.ms/gen-ai-lesson8-gh?WT.mc_id=academic-105485-koreyst">Video</a></td> 
   <td><a href="https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst">Learn More</a></td> 
  </tr> 
  <tr> 
   <td>09</td> 
   <td><a href="https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/09-building-image-applications/README.md?WT.mc_id=academic-105485-koreyst">Building Image Generation Applications</a></td> 
   <td><strong>Build:</strong> A image generation application</td> 
   <td><a href="https://aka.ms/gen-ai-lesson9-gh?WT.mc_id=academic-105485-koreyst">Video</a></td> 
   <td><a href="https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst">Learn More</a></td> 
  </tr> 
  <tr> 
   <td>10</td> 
   <td><a href="https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/10-building-low-code-ai-applications/README.md?WT.mc_id=academic-105485-koreyst">Building Low Code AI Applications</a></td> 
   <td><strong>Build:</strong> A Generative AI application using Low Code tools</td> 
   <td><a href="https://aka.ms/gen-ai-lesson10-gh?WT.mc_id=academic-105485-koreyst">Video</a></td> 
   <td><a href="https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst">Learn More</a></td> 
  </tr> 
  <tr> 
   <td>11</td> 
   <td><a href="https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/11-integrating-with-function-calling/README.md?WT.mc_id=academic-105485-koreyst">Integrating External Applications with Function Calling</a></td> 
   <td><strong>Build:</strong> What is function calling and its use cases for applications</td> 
   <td><a href="https://aka.ms/gen-ai-lesson11-gh?WT.mc_id=academic-105485-koreyst">Video</a></td> 
   <td><a href="https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst">Learn More</a></td> 
  </tr> 
  <tr> 
   <td>12</td> 
   <td><a href="https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst">Designing UX for AI Applications</a></td> 
   <td><strong>Learn:</strong> How to apply UX design principles when developing Generative AI Applications</td> 
   <td><a href="https://aka.ms/gen-ai-lesson12-gh?WT.mc_id=academic-105485-koreyst">Video</a></td> 
   <td><a href="https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst">Learn More</a></td> 
  </tr> 
  <tr> 
   <td>13</td> 
   <td><a href="https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/13-securing-ai-applications/README.md?WT.mc_id=academic-105485-koreyst">Securing Your Generative AI Applications</a></td> 
   <td><strong>Learn:</strong> The threats and risks to AI systems and methods to secure these systems.</td> 
   <td><a href="https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst">Video</a></td> 
   <td><a href="https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst">Learn More</a></td> 
  </tr> 
  <tr> 
   <td>14</td> 
   <td><a href="https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst">The Generative AI Application Lifecycle</a></td> 
   <td><strong>Learn:</strong> The tools and metrics to manage the LLM Lifecycle and LLMOps</td> 
   <td><a href="https://aka.ms/gen-ai-lesson14-gh?WT.mc_id=academic-105485-koreyst">Video</a></td> 
   <td><a href="https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst">Learn More</a></td> 
  </tr> 
  <tr> 
   <td>15</td> 
   <td><a href="https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/15-rag-and-vector-databases/README.md?WT.mc_id=academic-105485-koreyst">Retrieval Augmented Generation (RAG) and Vector Databases</a></td> 
   <td><strong>Build:</strong> An application using a RAG Framework to retrieve embeddings from a Vector Databases</td> 
   <td><a href="https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst">Video</a></td> 
   <td><a href="https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst">Learn More</a></td> 
  </tr> 
  <tr> 
   <td>16</td> 
   <td><a href="https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/16-open-source-models/README.md?WT.mc_id=academic-105485-koreyst">Open Source Models and Hugging Face</a></td> 
   <td><strong>Build:</strong> An application using open source models available on Hugging Face</td> 
   <td><a href="https://aka.ms/gen-ai-lesson16-gh?WT.mc_id=academic-105485-koreyst">Video</a></td> 
   <td><a href="https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst">Learn More</a></td> 
  </tr> 
  <tr> 
   <td>17</td> 
   <td><a href="https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/17-ai-agents/README.md?WT.mc_id=academic-105485-koreyst">AI Agents</a></td> 
   <td><strong>Build:</strong> An application using an AI Agent Framework</td> 
   <td><a href="https://aka.ms/gen-ai-lesson17-gh?WT.mc_id=academic-105485-koreyst">Video</a></td> 
   <td><a href="https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst">Learn More</a></td> 
  </tr> 
  <tr> 
   <td>18</td> 
   <td><a href="https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/18-fine-tuning/README.md?WT.mc_id=academic-105485-koreyst">Fine-Tuning LLMs</a></td> 
   <td><strong>Learn:</strong> The what, why and how of fine-tuning LLMs</td> 
   <td><a href="https://aka.ms/gen-ai-lesson18-gh?WT.mc_id=academic-105485-koreyst">Video</a></td> 
   <td><a href="https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst">Learn More</a></td> 
  </tr> 
 </tbody> 
</table> 
<h3>ğŸŒŸ Special thanks</h3> 
<p>Special thanks to <a href="https://www.linkedin.com/in/john0isaac/"><strong>John Aziz</strong></a> for creating all of the GitHub Actions and workflows</p> 
<h2>ğŸ’ Other Courses</h2> 
<p>Our team produces other courses! Check out:</p> 
<ul> 
 <li><a href="https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst">ML for Beginners</a></li> 
 <li><a href="https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst">Data Science for Beginners</a></li> 
 <li><a href="https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst">AI for Beginners</a></li> 
 <li><a href="https://github.com/microsoft/Security-101??WT.mc_id=academic-96948-sayoung"><strong>NEW</strong> Cybersecurity for Beginners</a></li> 
 <li><a href="https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst">Web Dev for Beginners</a></li> 
 <li><a href="https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst">IoT for Beginners</a></li> 
 <li><a href="https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst">XR Development for Beginners</a></li> 
 <li><a href="https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst">Mastering GitHub Copilot for AI Paired Programming</a></li> 
</ul>
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>huggingface/parler-tts</title>
<link>https://github.com/huggingface/parler-tts</link>
<guid>https://github.com/huggingface/parler-tts</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šParler-TTSã€æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ã€å¼€æºã€é«˜è´¨é‡ã€è‡ªç„¶å‘éŸ³

æ€»ç»“ï¼š
Parler-TTS æ˜¯ä¸€ä¸ªç”± Stability AI å’Œçˆ±ä¸å ¡å¤§å­¦çš„ç ”ç©¶äººå‘˜ Dan Lyth å’Œ Simon King å¼€å‘çš„è½»é‡çº§æ–‡æœ¬åˆ°è¯­éŸ³ç”Ÿæˆæ¨¡å‹ã€‚è¯¥æ¨¡å‹æ—¨åœ¨äº§ç”Ÿå…·æœ‰é«˜ä¿çœŸåº¦å’Œè‡ªç„¶å‘éŸ³çš„è¯­éŸ³ï¼ŒåŒæ—¶å…è®¸ç”¨æˆ·é€šè¿‡æ–‡æœ¬æç¤ºè‡ªå®šä¹‰è¯´è¯è€…çš„é£æ ¼ï¼ŒåŒ…æ‹¬æ€§åˆ«ã€éŸ³è°ƒã€è¯­é€Ÿç­‰ã€‚Parler-TTS çš„ç‰¹åˆ«ä¹‹å¤„åœ¨äºå…¶å®Œå…¨å¼€æºï¼Œæä¾›äº†æ•°æ®é›†ã€é¢„å¤„ç†ã€è®­ç»ƒä»£ç å’Œæƒé‡çš„å…¬å¼€è®¿é—®æƒé™ï¼Œé¼“åŠ±ç¤¾åŒºæˆå‘˜åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œåˆ›æ–°å’Œå‘å±•ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿé€šè¿‡ç®€å•çš„å®‰è£…æ­¥éª¤ï¼ˆ`pip install git+https://github.com/huggingface/parler-tts.git`ï¼‰è½»æ¾é›†æˆåˆ°é¡¹ç›®ä¸­ã€‚

Parler-TTS æ”¯æŒéšæœºè¯­éŸ³ç”Ÿæˆï¼Œä»¥åŠç‰¹å®šè¯´è¯è€…çš„å£°éŸ³æ§åˆ¶ï¼Œåè€…åŸºäºå¯¹ä¸åŒè¯´è¯äººç‰¹å¾çš„è®­ç»ƒã€‚æ¨¡å‹çš„ä½¿ç”¨æ–¹æ³•ç®€å•ç›´è§‚ï¼Œé€šè¿‡æä¾›æè¿°æ€§æ–‡æœ¬å’Œç”ŸæˆæŒ‡ä»¤æ¥å®šåˆ¶è¾“å‡ºè¯­éŸ³ã€‚æ­¤å¤–ï¼ŒParler-TTS è¿˜æ”¯æŒä¼˜åŒ–æ¨ç†é€Ÿåº¦ï¼Œå¦‚ä½¿ç”¨ SDPA å’Œ Flash Attention æŠ€æœ¯ï¼Œä»¥åŠç¼–è¯‘æ¨¡å‹çš„èƒ½åŠ›ã€‚

ä¸ºäº†æé«˜è´¨é‡å’Œé€Ÿåº¦ï¼ŒParler-TTS å›¢é˜Ÿæ­£åœ¨æ¢ç´¢å¤šä¸ªæ”¹è¿›æ–¹å‘ï¼ŒåŒ…æ‹¬æ•°æ®é›†æ‰©å±•ã€æ›´å¤šè¯­è¨€æ”¯æŒã€è®­ç»ƒæµç¨‹ä¼˜åŒ–ã€æ¨¡å‹æ¶æ„æ¢ç´¢ã€ç¼–è¯‘æ”¯æŒã€é™æ€ç¼“å­˜å’Œå¤šè¯­è¨€è®­ç»ƒç­‰ã€‚è¿™äº›æ”¹è¿›æ—¨åœ¨è¿›ä¸€æ­¥æå‡æ¨¡å‹æ€§èƒ½å’Œåº”ç”¨èŒƒå›´ã€‚ <div>
<p>Inference and training library for high-quality TTS models.</p><hr /><h1>Parler-TTS</h1> 
<p>Parler-TTS is a lightweight text-to-speech (TTS) model that can generate high-quality, natural sounding speech in the style of a given speaker (gender, pitch, speaking style, etc). It is a reproduction of work from the paper <a href="https://www.text-description-to-speech.com">Natural language guidance of high-fidelity text-to-speech with synthetic annotations</a> by Dan Lyth and Simon King, from Stability AI and Edinburgh University respectively.</p> 
<p>Contrarily to other TTS models, Parler-TTS is a <strong>fully open-source</strong> release. All of the datasets, pre-processing, training code and weights are released publicly under permissive license, enabling the community to build on our work and develop their own powerful TTS models.</p> 
<p>This repository contains the inference and training code for Parler-TTS. It is designed to accompany the <a href="https://github.com/huggingface/dataspeech">Data-Speech</a> repository for dataset annotation.</p> 
<blockquote> 
 <p>[!IMPORTANT] <strong>08/08/2024:</strong> We are proud to release two new Parler-TTS checkpoints:</p> 
 <ol> 
  <li><a href="https://huggingface.co/parler-tts/parler-tts-mini-v1">Parler-TTS Mini</a>, an 880M parameter model.</li> 
  <li><a href="https://huggingface.co/parler-tts/parler-tts-large-v1">Parler-TTS Large</a>, a 2.3B parameter model.</li> 
 </ol> 
 <p>These checkpoints have been trained on 45k hours of audiobook data.</p> 
 <p>In addition, the code is optimized for much faster generation: we've added SDPA and Flash Attention 2 compatibility, as well as the ability to compile the model.</p> 
</blockquote> 
<h2>ğŸ“– Quick Index</h2> 
<ul> 
 <li><a href="https://raw.githubusercontent.com/huggingface/parler-tts/main/#installation">Installation</a></li> 
 <li><a href="https://raw.githubusercontent.com/huggingface/parler-tts/main/#usage">Usage</a> 
  <ul> 
   <li><a href="https://raw.githubusercontent.com/huggingface/parler-tts/main/#-random-voice">ğŸ² Using a random voice</a></li> 
   <li><a href="https://raw.githubusercontent.com/huggingface/parler-tts/main/#-using-a-specific-speaker">ğŸ¯ Using a specific speaker</a></li> 
  </ul> </li> 
 <li><a href="https://raw.githubusercontent.com/huggingface/parler-tts/main/#training">Training</a></li> 
 <li><a href="https://huggingface.co/spaces/parler-tts/parler_tts">Demo</a></li> 
 <li><a href="https://huggingface.co/parler-tts">Model weights and datasets</a></li> 
 <li><a href="https://raw.githubusercontent.com/huggingface/parler-tts/main/#-optimizing-inference-speed">Optimizing inference</a></li> 
</ul> 
<h2>Installation</h2> 
<p>Parler-TTS has light-weight dependencies and can be installed in one line:</p> 
<pre><code class="language-sh">pip install git+https://github.com/huggingface/parler-tts.git
</code></pre> 
<p>Apple Silicon users will need to run a follow-up command to make use the nightly PyTorch (2.4) build for bfloat16 support:</p> 
<pre><code class="language-sh">pip3 install --pre torch torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu
</code></pre> 
<h2>Usage</h2> 
<blockquote> 
 <p>[!TIP] You can directly try it out in an interactive demo <a href="https://huggingface.co/spaces/parler-tts/parler_tts">here</a>!</p> 
</blockquote> 
<p>Using Parler-TTS is as simple as "bonjour". Simply install the library once:</p> 
<pre><code class="language-sh">pip install git+https://github.com/huggingface/parler-tts.git
</code></pre> 
<h3>ğŸ² Random voice</h3> 
<p><strong>Parler-TTS</strong> has been trained to generate speech with features that can be controlled with a simple text prompt, for example:</p> 
<pre><code class="language-py">import torch
from parler_tts import ParlerTTSForConditionalGeneration
from transformers import AutoTokenizer
import soundfile as sf

device = "cuda:0" if torch.cuda.is_available() else "cpu"

model = ParlerTTSForConditionalGeneration.from_pretrained("parler-tts/parler-tts-mini-v1").to(device)
tokenizer = AutoTokenizer.from_pretrained("parler-tts/parler-tts-mini-v1")

prompt = "Hey, how are you doing today?"
description = "A female speaker delivers a slightly expressive and animated speech with a moderate speed and pitch. The recording is of very high quality, with the speaker's voice sounding clear and very close up."

input_ids = tokenizer(description, return_tensors="pt").input_ids.to(device)
prompt_input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)

generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)
audio_arr = generation.cpu().numpy().squeeze()
sf.write("parler_tts_out.wav", audio_arr, model.config.sampling_rate)
</code></pre> 
<h3>ğŸ¯ Using a specific speaker</h3> 
<p>To ensure speaker consistency across generations, this checkpoint was also trained on 34 speakers, characterized by name (e.g. Jon, Lea, Gary, Jenna, Mike, Laura).</p> 
<p>To take advantage of this, simply adapt your text description to specify which speaker to use: <code>Jon's voice is monotone yet slightly fast in delivery, with a very close recording that almost has no background noise.</code></p> 
<pre><code class="language-py">import torch
from parler_tts import ParlerTTSForConditionalGeneration
from transformers import AutoTokenizer
import soundfile as sf

device = "cuda:0" if torch.cuda.is_available() else "cpu"

model = ParlerTTSForConditionalGeneration.from_pretrained("parler-tts/parler-tts-mini-v1").to(device)
tokenizer = AutoTokenizer.from_pretrained("parler-tts/parler-tts-mini-v1")

prompt = "Hey, how are you doing today?"
description = "Jon's voice is monotone yet slightly fast in delivery, with a very close recording that almost has no background noise."

input_ids = tokenizer(description, return_tensors="pt").input_ids.to(device)
prompt_input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)

generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)
audio_arr = generation.cpu().numpy().squeeze()
sf.write("parler_tts_out.wav", audio_arr, model.config.sampling_rate)
</code></pre> 
<p><strong>Tips</strong>:</p> 
<ul> 
 <li>Include the term "very clear audio" to generate the highest quality audio, and "very noisy audio" for high levels of background noise</li> 
 <li>Punctuation can be used to control the prosody of the generations, e.g. use commas to add small breaks in speech</li> 
 <li>The remaining speech features (gender, speaking rate, pitch and reverberation) can be controlled directly through the prompt</li> 
</ul> 
<h3>âœ¨ Optimizing Inference Speed</h3> 
<p>We've set up an <a href="https://raw.githubusercontent.com/huggingface/parler-tts/main/INFERENCE.md">inference guide</a> to make generation faster. Think SDPA, torch.compile and streaming!</p> 
<p><a href="https://github.com/huggingface/parler-tts/assets/52246514/251e2488-fe6e-42c1-81cd-814c5b7795b0">https://github.com/huggingface/parler-tts/assets/52246514/251e2488-fe6e-42c1-81cd-814c5b7795b0</a></p> 
<h2>Training</h2> 
<a href="https://github.com/ylacombe/scripts_and_notebooks/raw/main/Finetuning_Parler_TTS_v1_on_a_single_speaker_dataset.ipynb" target="_blank"> <img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" /> </a> 
<p>The <a href="https://raw.githubusercontent.com/huggingface/parler-tts/main/training/">training folder</a> contains all the information to train or fine-tune your own Parler-TTS model. It consists of:</p> 
<ul> 
 <li><a href="https://raw.githubusercontent.com/huggingface/parler-tts/main/training/README.md#1-architecture">1. An introduction to the Parler-TTS architecture</a></li> 
 <li><a href="https://raw.githubusercontent.com/huggingface/parler-tts/main/training/README.md#2-getting-started">2. The first steps to get started</a></li> 
 <li><a href="https://raw.githubusercontent.com/huggingface/parler-tts/main/training/README.md#3-training">3. A training guide</a></li> 
</ul> 
<blockquote> 
 <p>[!IMPORTANT] <strong>TL;DR:</strong> After having followed the <a href="https://raw.githubusercontent.com/huggingface/parler-tts/main/training/README.md#requirements">installation steps</a>, you can reproduce the Parler-TTS Mini v1 training recipe with the following command line:</p> 
</blockquote> 
<pre><code class="language-sh">accelerate launch ./training/run_parler_tts_training.py ./helpers/training_configs/starting_point_v1.json
</code></pre> 
<blockquote> 
 <p>[!IMPORTANT] You can also follow <a href="https://github.com/ylacombe/scripts_and_notebooks/raw/main/Finetuning_Parler_TTS_v1_on_a_single_speaker_dataset.ipynb">this fine-tuning guide</a> on a mono-speaker dataset example.</p> 
</blockquote> 
<h2>Acknowledgements</h2> 
<p>This library builds on top of a number of open-source giants, to whom we'd like to extend our warmest thanks for providing these tools!</p> 
<p>Special thanks to:</p> 
<ul> 
 <li>Dan Lyth and Simon King, from Stability AI and Edinburgh University respectively, for publishing such a promising and clear research paper: <a href="https://arxiv.org/abs/2402.01912">Natural language guidance of high-fidelity text-to-speech with synthetic annotations</a>.</li> 
 <li>the many libraries used, namely <a href="https://huggingface.co/docs/datasets/v2.17.0/en/index">ğŸ¤— datasets</a>, <a href="https://huggingface.co/docs/accelerate/en/index">ğŸ¤— accelerate</a>, <a href="https://github.com/jitsi/jiwer">jiwer</a>, <a href="https://wandb.ai/">wandb</a>, and <a href="https://huggingface.co/docs/transformers/index">ğŸ¤— transformers</a>.</li> 
 <li>Descript for the <a href="https://github.com/descriptinc/descript-audio-codec">DAC codec model</a></li> 
 <li>Hugging Face ğŸ¤— for providing compute resources and time to explore!</li> 
</ul> 
<h2>Citation</h2> 
<p>If you found this repository useful, please consider citing this work and also the original Stability AI paper:</p> 
<pre><code>@misc{lacombe-etal-2024-parler-tts,
  author = {Yoach Lacombe and Vaibhav Srivastav and Sanchit Gandhi},
  title = {Parler-TTS},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/parler-tts}}
}
</code></pre> 
<pre><code>@misc{lyth2024natural,
      title={Natural language guidance of high-fidelity text-to-speech with synthetic annotations},
      author={Dan Lyth and Simon King},
      year={2024},
      eprint={2402.01912},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}
</code></pre> 
<h2>Contribution</h2> 
<p>Contributions are welcome, as the project offers many possibilities for improvement and exploration.</p> 
<p>Namely, we're looking at ways to improve both quality and speed:</p> 
<ul> 
 <li>Datasets: 
  <ul> 
   <li>Train on more data</li> 
   <li>Add more features such as accents</li> 
  </ul> </li> 
 <li>Training: 
  <ul> 
   <li>Add PEFT compatibility to do Lora fine-tuning.</li> 
   <li>Add possibility to train without description column.</li> 
   <li>Add notebook training.</li> 
   <li>Explore multilingual training.</li> 
   <li>Explore mono-speaker finetuning.</li> 
   <li>Explore more architectures.</li> 
  </ul> </li> 
 <li>Optimization: 
  <ul> 
   <li>Compilation and static cache</li> 
   <li>Support to FA2 and SDPA</li> 
  </ul> </li> 
 <li>Evaluation: 
  <ul> 
   <li>Add more evaluation metrics</li> 
  </ul> </li> 
</ul>
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>aria2/aria2</title>
<link>https://github.com/aria2/aria2</link>
<guid>https://github.com/aria2/aria2</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šaria2ã€ä¸‹è½½å·¥å…·ã€å‘½ä»¤è¡Œç•Œé¢ã€å¤šåè®®æ”¯æŒã€å¤šæºä¸‹è½½

æ€»ç»“ï¼š

æ–‡ç« ä¸»è¦ä»‹ç»äº†aria2ï¼Œè¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ã€å‘½ä»¤è¡Œæ“ä½œçš„ä¸‹è½½å·¥å…·ã€‚å®ƒæ”¯æŒå¤šç§ä¸‹è½½åè®®ï¼ŒåŒ…æ‹¬HTTPã€HTTPSã€FTPã€SFTPå’ŒBitTorrentï¼Œèƒ½å¤Ÿä»å¤šä¸ªæ¥æºæˆ–åè®®åŒæ—¶ä¸‹è½½æ–‡ä»¶ï¼Œå……åˆ†åˆ©ç”¨å¸¦å®½èµ„æºã€‚å…¶ç‰¹è‰²åŒ…æ‹¬é‡‘å±é“¾æ¥ï¼ˆMetalinkï¼‰ç‰ˆæœ¬4å’Œ3.0çš„æ”¯æŒï¼Œå…è®¸ä»å¤šä¸ªæºä¸‹è½½å¹¶éªŒè¯æ•°æ®å—çš„å®Œæ•´æ€§ï¼›æ”¯æŒHTTPä»£ç†ã€è‡ªå®šä¹‰HTTPå¤´éƒ¨ã€æ–­ç‚¹ç»­ä¼ ã€é€Ÿåº¦é™åˆ¶ç­‰åŠŸèƒ½ã€‚æ­¤å¤–ï¼Œaria2è¿˜æ”¯æŒä¸BitTorrentåè®®çš„æ·±åº¦æ•´åˆï¼Œå¦‚å¿«è¿›æ‰©å±•ã€DHTã€PEXç­‰ï¼Œæä¾›æœ¬åœ°å¯¹ç­‰å‘ç°å’Œé‡æ–°å‘½åç›®å½•ç»“æ„çš„åŠŸèƒ½ã€‚å…¶è®¾è®¡éµå¾ªC++11æ ‡å‡†ï¼Œæ”¯æŒè·¨å¹³å°æ„å»ºï¼Œå¹¶æä¾›äº†ä¸°å¯Œçš„APIæ¥å£ã€‚

æ–‡ç« è¯¦ç»†é˜è¿°äº†aria2çš„é…ç½®é€‰é¡¹ã€ä¾èµ–åº“éœ€æ±‚ã€æ„å»ºè¿‡ç¨‹ä»¥åŠç‰ˆæœ¬æ§åˆ¶ç­–ç•¥ã€‚é€šè¿‡ä½¿ç”¨ä¸åŒçš„æ„å»ºè„šæœ¬ï¼ˆå¦‚é’ˆå¯¹Mac OS Xå’ŒAndroidçš„ç‰¹å®šè„šæœ¬ï¼‰ï¼Œç”¨æˆ·å¯ä»¥è½»æ¾åœ°æ„å»ºé€‚ç”¨äºä¸åŒæ“ä½œç³»ç»Ÿç¯å¢ƒçš„aria2ç‰ˆæœ¬ã€‚æ–‡ç« è¿˜å¼ºè°ƒäº†SSLè¯ä¹¦éªŒè¯çš„é‡è¦æ€§ï¼Œå¹¶æä¾›äº†å¤šç§é…ç½®SSLå’ŒåŠ å¯†åº“çš„æ–¹æ³•ã€‚

æ€»çš„æ¥è¯´ï¼Œaria2æ˜¯ä¸€ä¸ªåŠŸèƒ½å…¨é¢ã€é«˜åº¦å¯å®šåˆ¶çš„ä¸‹è½½å·¥å…·ï¼Œé€‚åˆéœ€è¦è·¨åè®®ã€å¤šæºé«˜æ•ˆä¸‹è½½æ–‡ä»¶çš„ç”¨æˆ·ï¼Œå°¤å…¶åœ¨éœ€è¦è‡ªåŠ¨åŒ–ã€è„šæœ¬é©±åŠ¨æˆ–è€…éœ€è¦æ·±åº¦é›†æˆåˆ°å…¶ä»–ç³»ç»Ÿä¸­çš„åœºæ™¯ä¸­è¡¨ç°çªå‡ºã€‚ <div>
<p>aria2 is a lightweight multi-protocol & multi-source, cross platform download utility operated in command-line. It supports HTTP/HTTPS, FTP, SFTP, BitTorrent and Metalink.</p><hr /><h1>aria2 - The ultra fast download utility</h1> 
<h2>Disclaimer</h2> 
<p>This program comes with no warranty. You must use this program at your own risk.</p> 
<h2>Introduction</h2> 
<p>aria2 is a utility for downloading files. The supported protocols are HTTP(S), FTP, SFTP, BitTorrent, and Metalink. aria2 can download a file from multiple sources/protocols and tries to utilize your maximum download bandwidth. It supports downloading a file from HTTP(S)/FTP/SFTP and BitTorrent at the same time, while the data downloaded from HTTP(S)/FTP/SFTP is uploaded to the BitTorrent swarm. Using Metalink's chunk checksums, aria2 automatically validates chunks of data while downloading a file like BitTorrent.</p> 
<p>The project page is located at <a href="https://aria2.github.io/">https://aria2.github.io/</a>.</p> 
<p>See the <code>aria2 Online Manual &lt;https://aria2.github.io/manual/en/html/&gt;</code>_ (<code>Russian translation &lt;https://aria2.github.io/manual/ru/html/&gt;</code><em>, <code>Portuguese translation &lt;https://aria2.github.io/manual/pt/html/&gt;</code></em>) to learn how to use aria2.</p> 
<h2>Features</h2> 
<p>Here is a list of features:</p> 
<ul> 
 <li>Command-line interface</li> 
 <li>Download files through HTTP(S)/FTP/SFTP/BitTorrent</li> 
 <li>Segmented downloading</li> 
 <li>Metalink version 4 (RFC 5854) support(HTTP/FTP/SFTP/BitTorrent)</li> 
 <li>Metalink version 3.0 support(HTTP/FTP/SFTP/BitTorrent)</li> 
 <li>Metalink/HTTP (RFC 6249) support</li> 
 <li>HTTP/1.1 implementation</li> 
 <li>HTTP Proxy support</li> 
 <li>HTTP BASIC authentication support</li> 
 <li>HTTP Proxy authentication support</li> 
 <li>Well-known environment variables for proxy: <code>http_proxy</code>, <code>https_proxy</code>, <code>ftp_proxy</code>, <code>all_proxy</code> and <code>no_proxy</code></li> 
 <li>HTTP gzip, deflate content encoding support</li> 
 <li>Verify peer using given trusted CA certificate in HTTPS</li> 
 <li>Client certificate authentication in HTTPS</li> 
 <li>Chunked transfer encoding support</li> 
 <li>Load Cookies from the file using the Firefox3 format, Chromium/Google Chrome and the Mozilla/Firefox (1.x/2.x)/Netscape format.</li> 
 <li>Save Cookies in the Mozilla/Firefox (1.x/2.x)/Netscape format.</li> 
 <li>Custom HTTP Header support</li> 
 <li>Persistent Connections support</li> 
 <li>FTP/SFTP through HTTP Proxy</li> 
 <li>Download/Upload speed throttling</li> 
 <li>BitTorrent extensions: Fast extension, DHT, PEX, MSE/PSE, Multi-Tracker, UDP tracker</li> 
 <li>BitTorrent <code>WEB-Seeding &lt;http://getright.com/seedtorrent.html&gt;</code>_. aria2 requests chunk more than piece size to reduce the request overhead. It also supports pipelined requests with piece size.</li> 
 <li>BitTorrent Local Peer Discovery</li> 
 <li>Rename/change the directory structure of BitTorrent downloads completely</li> 
 <li>JSON-RPC (over HTTP and WebSocket)/XML-RPC interface</li> 
 <li>Run as a daemon process</li> 
 <li>Selective download in multi-file torrent/Metalink</li> 
 <li>Chunk checksum validation in Metalink</li> 
 <li>Can disable segmented downloading in Metalink</li> 
 <li>Netrc support</li> 
 <li>Configuration file support</li> 
 <li>Download URIs found in a text file or stdin and the destination directory and output file name can be specified optionally</li> 
 <li>Parameterized URI support</li> 
 <li>IPv6 support with Happy Eyeballs</li> 
 <li>Disk cache to reduce disk activity</li> 
</ul> 
<h2>Versioning and release schedule</h2> 
<p>We use 3 numbers for the aria2 version: MAJOR.MINOR.PATCH. We will ship MINOR updates on the 15th of every month. We may skip a release if we have had no changes since the last release. The feature and documentation freeze happens 10 days before the release day (the 5th day of the month) for translation teams. We will raise an issue about the upcoming release around that day.</p> 
<p>We may release PATCH releases between regular releases if we have security issues.</p> 
<p>The MAJOR version will stay at 1 for the time being.</p> 
<h2>How to get source code</h2> 
<p>We maintain the source code at Github: <a href="https://github.com/aria2/aria2">https://github.com/aria2/aria2</a></p> 
<p>To get the latest source code, run the following command::</p> 
<pre><code>$ git clone https://github.com/aria2/aria2.git
</code></pre> 
<p>This will create an aria2 directory in your current directory and source files are stored there.</p> 
<h2>Dependency</h2> 
<p>======================== ======================================== features dependency ======================== ======================================== HTTPS OSX or GnuTLS or OpenSSL or Windows SFTP libssh2 BitTorrent None. Optional: libnettle+libgmp or libgcrypt or OpenSSL (see note) Metalink libxml2 or Expat. Checksum None. Optional: OSX or libnettle or libgcrypt or OpenSSL or Windows (see note) gzip, deflate in HTTP zlib Async DNS C-Ares Firefox3/Chromium cookie libsqlite3 XML-RPC libxml2 or Expat. JSON-RPC over WebSocket libnettle or libgcrypt or OpenSSL ======================== ========================================</p> 
<p>.. note::</p> 
<p>libxml2 has precedence over Expat if both libraries are installed. If you prefer Expat, run configure with <code>--without-libxml2</code>.</p> 
<p>.. note::</p> 
<p>On Apple OSX, OS-level SSL/TLS support will be preferred. Hence neither GnuTLS nor OpenSSL is required on that platform. If you'd like to disable this behavior, run configure with <code>--without-appletls</code>.</p> 
<p>GnuTLS has precedence over OpenSSL if both libraries are installed. If you prefer OpenSSL, run configure with <code>--without-gnutls</code> <code>--with-openssl</code>.</p> 
<p>On Windows, there is SSL implementation available that is based on the native Windows SSL capabilities (Schannel) and it will be preferred. Hence neither GnuTLS nor OpenSSL is required on that platform. If you'd like to disable this behavior, run configure with <code>--without-wintls</code>.</p> 
<p>.. note::</p> 
<p>On Apple OSX, the OS-level checksum support will be preferred, unless aria2 is configured with <code>--without-appletls</code>.</p> 
<p>libnettle has precedence over libgcrypt if both libraries are installed. If you prefer libgcrypt, run configure with <code>--without-libnettle --with-libgcrypt</code>. If OpenSSL is selected over GnuTLS, neither libnettle nor libgcrypt will be used.</p> 
<p>If none of the optional dependencies are installed, an internal implementation that only supports md5 and sha1 will be used.</p> 
<p>On Windows, there is SSL implementation available that is based on the native Windows capabilities and it will be preferred, unless aria2 is configured with <code>--without-wintls</code>.</p> 
<p>A user can have one of the following configurations for SSL and crypto libraries:</p> 
<ul> 
 <li>OpenSSL</li> 
 <li>GnuTLS + libgcrypt</li> 
 <li>GnuTLS + libnettle</li> 
 <li>Apple TLS (OSX only)</li> 
 <li>Windows TLS (Windows only)</li> 
</ul> 
<p>You can disable BitTorrent and Metalink support by providing <code>--disable-bittorrent</code> and <code>--disable-metalink</code> to the configure script respectively.</p> 
<p>To enable async DNS support, you need c-ares.</p> 
<ul> 
 <li>c-ares: <a href="http://c-ares.haxx.se/">http://c-ares.haxx.se/</a></li> 
</ul> 
<h2>How to build</h2> 
<p>aria2 is primarily written in C++. Initially, it was written based on C++98/C++03 standard features. We are now migrating aria2 to the C++11 standard. The current source code requires a C++11 aware compiler. For well-known compilers, such as g++ and clang, the <code>-std=c++11</code> or <code>-std=c++0x</code> flag must be supported.</p> 
<p>To build aria2 from the source package, you need the following development packages (package name may vary depending on the distribution you use):</p> 
<ul> 
 <li>libgnutls-dev (Required for HTTPS, BitTorrent, Checksum support)</li> 
 <li>nettle-dev (Required for BitTorrent, Checksum support)</li> 
 <li>libgmp-dev (Required for BitTorrent)</li> 
 <li>libssh2-1-dev (Required for SFTP support)</li> 
 <li>libc-ares-dev (Required for async DNS support)</li> 
 <li>libxml2-dev (Required for Metalink support)</li> 
 <li>zlib1g-dev (Required for gzip, deflate decoding support in HTTP)</li> 
 <li>libsqlite3-dev (Required for Firefox3/Chromium cookie support)</li> 
 <li>pkg-config (Required to detect installed libraries)</li> 
</ul> 
<p>You can use libgcrypt-dev instead of nettle-dev and libgmp-dev:</p> 
<ul> 
 <li>libgpg-error-dev (Required for BitTorrent, Checksum support)</li> 
 <li>libgcrypt-dev (Required for BitTorrent, Checksum support)</li> 
</ul> 
<p>You can use libssl-dev instead of libgnutls-dev, nettle-dev, libgmp-dev, libgpg-error-dev and libgcrypt-dev:</p> 
<ul> 
 <li>libssl-dev (Required for HTTPS, BitTorrent, Checksum support)</li> 
</ul> 
<p>You can use libexpat1-dev instead of libxml2-dev:</p> 
<ul> 
 <li>libexpat1-dev (Required for Metalink support)</li> 
</ul> 
<p>On Fedora you need the following packages: gcc, gcc-c++, kernel-devel, libgcrypt-devel, libxml2-devel, openssl-devel, gettext-devel, cppunit</p> 
<p>If you downloaded source code from a git repository, you have to install the following packages to get autoconf macros:</p> 
<ul> 
 <li>libxml2-dev</li> 
 <li>libcppunit-dev</li> 
 <li>autoconf</li> 
 <li>automake</li> 
 <li>autotools-dev</li> 
 <li>autopoint</li> 
 <li>libtool</li> 
</ul> 
<p>And run the following command to generate configure script and other files necessary to build the program::</p> 
<pre><code>$ autoreconf -i
</code></pre> 
<p>Also, you need <code>Sphinx &lt;http://sphinx-doc.org/&gt;</code>_ to build the man page.</p> 
<p>If you are building aria2 for Mac OS X, take a look at the makerelease-osx.mk GNU Make makefile.</p> 
<p>The quickest way to build aria2 is first to run configure script::</p> 
<pre><code>$ ./configure
</code></pre> 
<p>To build statically linked aria2, use <code>ARIA2_STATIC=yes</code> command-line option::</p> 
<pre><code>$ ./configure ARIA2_STATIC=yes
</code></pre> 
<p>After configuration is done, run <code>make</code> to compile the program::</p> 
<pre><code>$ make
</code></pre> 
<p>See <code>Cross-compiling Windows binary</code>_ to create a Windows binary. See <code>Cross-compiling Android binary</code>_ to create an Android binary.</p> 
<p>The configure script checks available libraries and enables as many features as possible except for experimental features not enabled by default.</p> 
<p>Since 1.1.0, aria2 checks the certificate of HTTPS servers by default. If you build with OpenSSL or the recent version of GnuTLS which has <code>gnutls_certificate_set_x509_system_trust()</code> function and the library is properly configured to locate the system-wide CA certificates store, aria2 will automatically load those certificates at the startup. If it is not the case, I recommend supplying the path to the CA bundle file. For example, in Debian the path to CA bundle file is '/etc/ssl/certs/ca-certificates.crt' (in ca-certificates package). This may vary depending on your distribution. You can give it to configure script using <code>--with-ca-bundle option</code>::</p> 
<pre><code>$ ./configure --with-ca-bundle='/etc/ssl/certs/ca-certificates.crt'
$ make
</code></pre> 
<p>Without <code>--with-ca-bundle</code> option, you will encounter the error when accessing HTTPS servers because the certificate cannot be verified without the CA bundle. In such a case, you can specify the CA bundle file using aria2's <code>--ca-certificate</code> option. If you don't have the CA bundle file installed, then the last resort is to disable the certificate validation using <code>--check-certificate=false</code>.</p> 
<p>Using the native OSX (AppleTLS) and/or Windows (WinTLS) implementation will automatically use the system certificate store, so <code>--with-ca-bundle</code> is not necessary and will be ignored when using these implementations.</p> 
<p>By default, the bash_completion file named <code>aria2c</code> is installed to the directory <code>$prefix/share/doc/aria2/bash_completion</code>. To change the install directory of the file, use <code>--with-bashcompletiondir</code> option.</p> 
<p>After a <code>make</code>, the executable is located at <code>src/aria2c</code>.</p> 
<p>aria2 uses CppUnit for automated unit testing. To run the unit test::</p> 
<pre><code>$ make check
</code></pre> 
<h2>Cross-compiling Windows binary</h2> 
<p>In this section, we describe how to build a Windows binary using a mingw-w64 (<a href="http://mingw-w64.org/doku.php">http://mingw-w64.org/doku.php</a>) cross-compiler on Debian Linux. The MinGW (<a href="http://www.mingw.org/">http://www.mingw.org/</a>) may not be able to build aria2.</p> 
<p>The easiest way to build Windows binary is using Dockerfile.mingw. See Dockerfile.mingw how to build a binary. If you cannot use Dockerfile, then continue to read the following paragraphs.</p> 
<p>Basically, after compiling and installing depended libraries, you can do cross-compile just passing appropriate <code>--host</code> option and specifying <code>CPPFLAGS</code>, <code>LDFLAGS</code>, and <code>PKG_CONFIG_LIBDIR</code> variables to configure. For convenience and to lower our own development cost, we provide an easier way to configure the build settings.</p> 
<p><code>mingw-config</code> script is a configure script wrapper for mingw-w64. We use it to create official Windows build. This script assumes the following libraries have been built for cross-compile:</p> 
<ul> 
 <li>c-ares</li> 
 <li>expat</li> 
 <li>sqlite3</li> 
 <li>zlib</li> 
 <li>libssh2</li> 
 <li>cppunit</li> 
</ul> 
<p>Some environment variables can be adjusted to change build settings:</p> 
<p><code>HOST</code> cross-compile to build programs to run on <code>HOST</code>. It defaults to <code>i686-w64-mingw32</code>. To build a 64bit binary, specify <code>x86_64-w64-mingw32</code>.</p> 
<p><code>PREFIX</code> Prefix to the directory where dependent libraries are installed. It defaults to <code>/usr/local/$HOST</code>. <code>-I$PREFIX/include</code> will be added to <code>CPPFLAGS</code>. <code>-L$PREFIX/lib</code> will be added to <code>LDFLAGS</code>. <code>$PREFIX/lib/pkgconfig</code> will be set to <code>PKG_CONFIG_LIBDIR</code>.</p> 
<p>For example, to build a 64bit binary do this::</p> 
<pre><code>$ HOST=x86_64-w64-mingw32 ./mingw-config
</code></pre> 
<p>If you want libaria2 dll with <code>--enable-libaria2</code>, then don't use <code>ARIA2_STATIC=yes</code> and prepare the DLL version of external libraries.</p> 
<h2>Cross-compiling Android binary</h2> 
<p>In this section, we describe how to build Android binary using Android NDK cross-compiler on Debian Linux.</p> 
<p>At the time of this writing, Android NDK r21e should compile aria2 without errors.</p> 
<p><code>android-config</code> script is a configure script wrapper for Android build. We use it to create an official Android build. This script assumes the following libraries have been built for cross-compile:</p> 
<ul> 
 <li>c-ares</li> 
 <li>openssl</li> 
 <li>expat</li> 
 <li>zlib</li> 
 <li>libssh2</li> 
</ul> 
<p>When building the above libraries, make sure that disable shared library and enable only static library. We are going to link those libraries statically.</p> 
<p><code>android-config</code> assumes that <code>$ANDROID_HOME</code> and <code>$NDK</code> environment variables are defined.</p> 
<p>We currently use Android NDK r21e. <code>$NDK</code> should point to the directory to Android NDK. The build tools will be found under <code>$NDK/toolchains/llvm/prebuilt/linux-x86_64/bin/</code>.</p> 
<p>All the dependent libraries must be installed under <code>$ANDROID_HOME/usr/local</code>.</p> 
<p>After <code>android-config</code>, run <code>make</code> to compile sources.</p> 
<h2>Building documentation</h2> 
<p><code>Sphinx &lt;http://sphinx-doc.org/&gt;</code>_ is used to building the documentation. aria2 man pages will be build when you run <code>make</code> if they are not up-to-date. You can also build an HTML version of the aria2 man page by <code>make html</code>. The HTML version manual is also available <code>online &lt;https://aria2.github.io/manual/en/html/&gt;</code>_ (<code>Russian translation &lt;https://aria2.github.io/manual/ru/html/&gt;</code><em>, <code>Portuguese translation &lt;https://aria2.github.io/manual/pt/html/&gt;</code></em>).</p> 
<h2>BitTorrent</h2> 
<p>About file names</p> 
<pre><code>The file name of the downloaded file is determined as follows:

single-file mode
    If "name" key is present in .torrent file, the file name is the value
    of "name" key. Otherwise, the file name is the base name of .torrent
    file appended by ".file". For example, .torrent file is
    "test.torrent", then file name is "test.torrent.file".  The
    directory to store the downloaded file can be specified by -d
    option.

multi-file mode
    The complete directory/file structure mentioned in .torrent file
    is created.  The directory to store the top directory of
    downloaded files can be specified by -d option.

Before download starts, a complete directory structure is created if
needed. By default, aria2 opens at most 100 files mentioned in
.torrent file, and directly writes to and reads from these files.
The number of files to open simultaneously can be controlled by
``--bt-max-open-files`` option.

DHT
~~~

aria2 supports mainline compatible DHT. By default, the routing table
for IPv4 DHT is saved to ``$XDG_CACHE_HOME/aria2/dht.dat`` and the
routing table for IPv6 DHT is saved to
``$XDG_CACHE_HOME/aria2/dht6.dat`` unless files exist at
``$HOME/.aria2/dht.dat`` or ``$HOME/.aria2/dht6.dat``. aria2 uses the
same port number to listen on for both IPv4 and IPv6 DHT.

UDP tracker
~~~~~~~~~~~

UDP tracker support is enabled when IPv4 DHT is enabled.  The port
number of the UDP tracker is shared with DHT. Use ``--dht-listen-port``
option to change the port number.

Other things should be noted
</code></pre> 
<ul> 
 <li><code>-o</code> option is used to change the file name of .torrent file itself, not a file name of a file in .torrent file. For this purpose, use <code>--index-out</code> option instead.</li> 
 <li>The port numbers that aria2 uses by default are 6881-6999 for TCP and UDP.</li> 
 <li>aria2 doesn't configure port-forwarding automatically. Please configure your router or firewall manually.</li> 
 <li>The maximum number of peers is 55. This limit may be exceeded when the download rate is low. This download rate can be adjusted using <code>--bt-request-peer-speed-limit</code> option.</li> 
 <li>As of release 0.10.0, aria2 stops sending request messages after selective download completes.</li> 
</ul> 
<h2>Metalink</h2> 
<p>The current implementation supports HTTP(S)/FTP/SFTP/BitTorrent. The other P2P protocols are ignored. Both Metalink4 (RFC 5854) and Metalink version 3.0 documents are supported.</p> 
<p>For checksum verification, md5, sha-1, sha-224, sha-256, sha-384, and sha-512 are supported. If multiple hash algorithms are provided, aria2 uses a stronger one. If whole file checksum verification fails, aria2 doesn't retry the download and just exits with a non-zero return code.</p> 
<p>The supported user preferences are version, language, location, protocol, and os.</p> 
<p>If chunk checksums are provided in the Metalink file, aria2 automatically validates chunks of data during download. This behavior can be turned off by a command-line option.</p> 
<p>If a signature is included in a Metalink file, aria2 saves it as a file after the completion of the download. The file name is download file name + ".sig". If the same file already exists, the signature file is not saved.</p> 
<p>In Metalink4, a multi-file torrent could appear in metalink:metaurl element. Since aria2 cannot download 2 same torrents at the same time, aria2 groups files in metalink:file element which has the same BitTorrent metaurl, and downloads them from a single BitTorrent swarm. This is a basically multi-file torrent download with file selection, so the adjacent files which are not in Metalink document but share the same piece with the selected file are also created.</p> 
<p>If relative URI is specified in metalink:url or metalink:metaurl element, aria2 uses the URI of Metalink file as base URI to resolve the relative URI. If relative URI is found in the Metalink file which is read from the local disk, aria2 uses the value of <code>--metalink-base-uri</code> option as base URI. If this option is not specified, the relative URI will be ignored.</p> 
<h2>Metalink/HTTP</h2> 
<p>The current implementation only uses rel=duplicate links. aria2 understands Digest header fields and check whether it matches the digest value from other sources. If it differs, drop the connection. aria2 also uses this digest value to perform checksum verification after the download is finished. aria2 recognizes geo value. To tell aria2 which location you prefer, you can use <code>--metalink-location</code> option.</p> 
<h2>netrc</h2> 
<p>netrc support is enabled by default for HTTP(S)/FTP/SFTP. To disable netrc support, specify -n command-line option. Your .netrc file should have correct permissions(600).</p> 
<h2>WebSocket</h2> 
<p>The WebSocket server embedded in aria2 implements the specification defined in RFC 6455. The supported protocol version is 13.</p> 
<h2>libaria2</h2> 
<p>The libaria2 is a C++ library that offers aria2 functionality to the client code. Currently, libaria2 is not built by default. To enable libaria2, use <code>--enable-libaria2</code> configure option. By default, only the shared library is built. To build a static library, use <code>--enable-static</code> configure option as well. See libaria2 documentation to know how to use API.</p> 
<h2>References</h2> 
<ul> 
 <li> <p><code>aria2 Online Manual &lt;https://aria2.github.io/manual/en/html/&gt;</code>_</p> </li> 
 <li> <p><a href="https://aria2.github.io/">https://aria2.github.io/</a></p> </li> 
 <li> <p><code>RFC 959 FILE TRANSFER PROTOCOL (FTP) &lt;http://tools.ietf.org/html/rfc959&gt;</code>_</p> </li> 
 <li> <p><code>RFC 1738 Uniform Resource Locators (URL) &lt;http://tools.ietf.org/html/rfc1738&gt;</code>_</p> </li> 
 <li> <p><code>RFC 2428 FTP Extensions for IPv6 and NATs &lt;http://tools.ietf.org/html/rfc2428&gt;</code>_</p> </li> 
 <li> <p><code>RFC 2616 Hypertext Transfer Protocol -- HTTP/1.1 &lt;http://tools.ietf.org/html/rfc2616&gt;</code>_</p> </li> 
 <li> <p><code>RFC 3659 Extensions to FTP &lt;http://tools.ietf.org/html/rfc3659&gt;</code>_</p> </li> 
 <li> <p><code>RFC 3986 Uniform Resource Identifier (URI): Generic Syntax &lt;http://tools.ietf.org/html/rfc3986&gt;</code>_</p> </li> 
 <li> <p><code>RFC 4038 Application Aspects of IPv6 Transition &lt;http://tools.ietf.org/html/rfc4038&gt;</code>_</p> </li> 
 <li> <p><code>RFC 5854 The Metalink Download Description Format &lt;http://tools.ietf.org/html/rfc5854&gt;</code>_</p> </li> 
 <li> <p><code>RFC 6249 Metalink/HTTP: Mirrors and Hashes &lt;http://tools.ietf.org/html/rfc6249&gt;</code>_</p> </li> 
 <li> <p><code>RFC 6265 HTTP State Management Mechanism &lt;http://tools.ietf.org/html/rfc6265&gt;</code>_</p> </li> 
 <li> <p><code>RFC 6266 Use of the Content-Disposition Header Field in the Hypertext Transfer Protocol (HTTP) &lt;http://tools.ietf.org/html/rfc6266&gt;</code>_</p> </li> 
 <li> <p><code>RFC 6455 The WebSocket Protocol &lt;http://tools.ietf.org/html/rfc6455&gt;</code>_</p> </li> 
 <li> <p><code>RFC 6555 Happy Eyeballs: Success with Dual-Stack Hosts &lt;http://tools.ietf.org/html/rfc6555&gt;</code>_</p> </li> 
 <li> <p><code>The BitTorrent Protocol Specification &lt;http://www.bittorrent.org/beps/bep_0003.html&gt;</code>_</p> </li> 
 <li> <p><code>BitTorrent: DHT Protocol &lt;http://www.bittorrent.org/beps/bep_0005.html&gt;</code>_</p> </li> 
 <li> <p><code>BitTorrent: Fast Extension &lt;http://www.bittorrent.org/beps/bep_0006.html&gt;</code>_</p> </li> 
 <li> <p><code>BitTorrent: IPv6 Tracker Extension &lt;http://www.bittorrent.org/beps/bep_0007.html&gt;</code>_</p> </li> 
 <li> <p><code>BitTorrent: Extension for Peers to Send Metadata Files &lt;http://www.bittorrent.org/beps/bep_0009.html&gt;</code>_</p> </li> 
 <li> <p><code>BitTorrent: Extension Protocol &lt;http://www.bittorrent.org/beps/bep_0010.html&gt;</code>_</p> </li> 
 <li> <p><code>BitTorrent: Multitracker Metadata Extension &lt;http://www.bittorrent.org/beps/bep_0012.html&gt;</code>_</p> </li> 
 <li> <p><code>BitTorrent: UDP Tracker Protocol for BitTorrent &lt;http://www.bittorrent.org/beps/bep_0015.html&gt;</code>_ and <code>BitTorrent udp-tracker protocol specification &lt;http://www.rasterbar.com/products/libtorrent/udp_tracker_protocol.html&gt;</code>_.</p> </li> 
 <li> <p><code>BitTorrent: WebSeed - HTTP/FTP Seeding (GetRight style) &lt;http://www.bittorrent.org/beps/bep_0019.html&gt;</code>_</p> </li> 
 <li> <p><code>BitTorrent: Private Torrents &lt;http://www.bittorrent.org/beps/bep_0027.html&gt;</code>_</p> </li> 
 <li> <p><code>BitTorrent: BitTorrent DHT Extensions for IPv6 &lt;http://www.bittorrent.org/beps/bep_0032.html&gt;</code>_</p> </li> 
 <li> <p><code>BitTorrent: Message Stream Encryption &lt;http://wiki.vuze.com/w/Message_Stream_Encryption&gt;</code>_</p> </li> 
 <li> <p><code>Kademlia: A Peer-to-peer Information System Based on the XOR Metric &lt;https://pdos.csail.mit.edu/~petar/papers/maymounkov-kademlia-lncs.pdf&gt;</code>_</p> </li> 
</ul>
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>toss/es-toolkit</title>
<link>https://github.com/toss/es-toolkit</link>
<guid>https://github.com/toss/es-toolkit</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šes-toolkit, é«˜æ€§èƒ½, å°å‹bundle, TypeScriptæ”¯æŒ, æ€§èƒ½ä¼˜åŒ–

æ€»ç»“:

es-toolkit æ˜¯ä¸€æ¬¾é«˜æ€§èƒ½ã€å°å‹ä½“ç§¯çš„ç°ä»£JavaScriptå·¥å…·åº“ã€‚å®ƒæä¾›äº†å„ç§å®ç”¨çš„å‡½æ•°ï¼Œå¦‚debounceã€chunkç­‰ï¼Œæ—¨åœ¨æå‡å¼€å‘æ•ˆç‡ã€‚es-toolkitåœ¨ç°ä»£JavaScriptç¯å¢ƒä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œæ¯”å…¶ä»–åº“å¿«äº†2-3å€ï¼ŒåŒæ—¶ä½“ç§¯å‡å°‘äº†97%ï¼Œè¿™ä½¿å¾—å®ƒæˆä¸ºè½»é‡çº§é¡¹ç›®å’Œè¿½æ±‚é«˜æ•ˆæ‰§è¡Œçš„ç†æƒ³é€‰æ‹©ã€‚

es-toolkitè¿˜å…·æœ‰å†…ç½®çš„ç±»å‹æ³¨è§£æ”¯æŒï¼Œä¸TypeScriptå…¼å®¹ï¼Œæä¾›æ¸…æ™°ä¸”å¼ºå¤§çš„ç±»å‹å®šä¹‰ã€‚å®ƒåŒ…æ‹¬æœ‰ç”¨çš„ç±»å‹å®ˆå«ï¼Œå¸®åŠ©å¼€å‘è€…æ›´å¥½åœ°ç†è§£æ•°æ®ç»“æ„ã€‚æ­¤å¤–ï¼Œes-toolkitçš„ä»£ç ç»è¿‡å…¨é¢æµ‹è¯•ï¼Œæ‹¥æœ‰100%çš„è¦†ç›–ç‡ï¼Œç¡®ä¿äº†å…¶ç¨³å®šæ€§å’Œå¯é æ€§ã€‚

å¯¹äºé‚£äº›å¯»æ±‚é«˜æ€§èƒ½ã€è½»é‡çº§ä¸”ç±»å‹å®‰å…¨çš„JavaScriptåº“çš„å¼€å‘è€…æ¥è¯´ï¼Œes-toolkitæ˜¯ä¸€ä¸ªå€¼å¾—è€ƒè™‘çš„é€‰æ‹©ã€‚æ— è®ºæ˜¯éœ€è¦å¤„ç†å¤§é‡æ•°æ®è¿˜æ˜¯å¸Œæœ›ä¼˜åŒ–ä»£ç æ‰§è¡Œé€Ÿåº¦ï¼Œes-toolkitéƒ½èƒ½æä¾›æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚åŒæ—¶ï¼Œes-toolkitæ¬¢è¿ç¤¾åŒºæˆå‘˜çš„è´¡çŒ®ï¼Œå…±åŒæ¨åŠ¨å…¶å‘å±•å’Œæ”¹è¿›ã€‚ <div>
<p>A modern JavaScript utility library that's 2-3 times faster and up to 97% smallerâ€”a major upgrade to lodash.</p><hr /><p><img alt="" src="https://raw.githubusercontent.com/toss/es-toolkit/main/docs/public/og.png" /></p> 
<h1>es-toolkit Â· <a href="https://github.com/toss/slash/raw/main/LICENSE"><img alt="MIT License" src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" /></a> <a href="https://codecov.io/gh/toss/es-toolkit"><img alt="codecov" src="https://codecov.io/gh/toss/es-toolkit/graph/badge.svg?token=8N5S3AR3C7" /></a> <a href="https://www.npmjs.com/package/es-toolkit"><img alt="NPM badge" src="https://img.shields.io/npm/v/es-toolkit?logo=npm" /></a> <a href="https://jsr.io/@es-toolkit/es-toolkit"><img alt="JSR badge" src="https://jsr.io/badges/@es-toolkit/es-toolkit" /></a></h1> 
<p>English | <a href="https://github.com/toss/es-toolkit/raw/main/README-ko_kr.md">í•œêµ­ì–´</a> | <a href="https://github.com/toss/es-toolkit/raw/main/README-zh_hans.md">ç®€ä½“ä¸­æ–‡</a> | <a href="https://github.com/toss/es-toolkit/raw/main/README-ja_jp.md">æ—¥æœ¬èª</a></p> 
<p>es-toolkit is a state-of-the-art, high-performance JavaScript utility library with a small bundle size and strong type annotations.</p> 
<ul> 
 <li>es-toolkit offers a variety of everyday utility functions with modern implementations, such as <a href="https://es-toolkit.slash.page/reference/function/debounce.html">debounce</a>, <a href="https://es-toolkit.slash.page/reference/promise/delay.html">delay</a>, <a href="https://es-toolkit.slash.page/reference/array/chunk.html">chunk</a>, <a href="https://es-toolkit.slash.page/reference/math/sum.html">sum</a>, and <a href="https://es-toolkit.slash.page/reference/object/pick.html">pick</a>.</li> 
 <li>Designed with performance in mind, es-toolkit achieves <a href="https://es-toolkit.slash.page/performance.html">2-3Ã— better performance</a> in modern JavaScript environments.</li> 
 <li>es-toolkit supports tree shaking out of the box, and <a href="https://es-toolkit.slash.page/bundle-size.html">reduces JavaScript code by up to 97%</a> compared to other libraries.</li> 
 <li>es-toolkit includes built-in TypeScript support, with straightforward yet robust types. It also provides useful type guards such as <a href="https://es-toolkit.slash.page/reference/predicate/isNotNil.html">isNotNil</a>.</li> 
 <li>es-toolkit is battle-tested with 100% test coverage, ensuring reliability and robustness.</li> 
</ul> 
<h2>Examples</h2> 
<pre><code class="language-tsx">// import from '@es-toolkit/es-toolkit' in jsr.
import { debounce, chunk } from 'es-toolkit';

const debouncedLog = debounce(message =&gt; {
  console.log(message);
}, 300);

// This call will be debounced
debouncedLog('Hello, world!');

const array = [1, 2, 3, 4, 5, 6];
const chunkedArray = chunk(array, 2);

console.log(chunkedArray);
// Output: [[1, 2], [3, 4], [5, 6]]
</code></pre> 
<h2>Contributing</h2> 
<p>We welcome contribution from everyone in the community. Read below for detailed contribution guide.</p> 
<p><a href="https://github.com/toss/es-toolkit/raw/main/.github/CONTRIBUTING.md">CONTRIBUTING</a></p> 
<h2>License</h2> 
<p>MIT Â© Viva Republica, Inc. See <a href="https://raw.githubusercontent.com/toss/es-toolkit/main/LICENSE">LICENSE</a> for details.</p> 
<a href="https://toss.im" title="Toss"> 
  
  <source media="(prefers-color-scheme: dark)" /> 
  <img alt="Toss" src="https://static.toss.im/logos/png/4x/logo-toss.png" width="100" /> 
  </a>
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>comfyanonymous/ComfyUI</title>
<link>https://github.com/comfyanonymous/ComfyUI</link>
<guid>https://github.com/comfyanonymous/ComfyUI</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šComfyUIã€Stable Diffusionã€GUIã€APIã€Backend

æ€»ç»“ï¼š
ComfyUI æ˜¯ä¸€ä¸ªå¼ºå¤§ä¸”æ¨¡å—åŒ–çš„å›¾å½¢ç•Œé¢ï¼ˆGUIï¼‰ã€API å’Œåç«¯ç³»ç»Ÿï¼Œä¸“ä¸ºç¨³å®šæ‰©æ•£æ¨¡å‹è®¾è®¡ï¼Œæ”¯æŒç”¨æˆ·é€šè¿‡æµç¨‹å›¾ï¼ˆNodes/Flowchartï¼‰ç•Œé¢æ„å»ºå’Œæ‰§è¡Œå¤æ‚çš„ç¨³å®šæ‰©æ•£å·¥ä½œæµï¼Œæ— éœ€ç¼–å†™ä»£ç ã€‚å®ƒæ”¯æŒå¤šç§ç¨³å®šæ‰©æ•£ç‰ˆæœ¬ï¼ŒåŒ…æ‹¬SD1.xã€SD2.xç­‰ï¼Œå¹¶æä¾›äº†å¼‚æ­¥é˜Ÿåˆ—ç³»ç»Ÿã€ä¼˜åŒ–çš„è®°å¿†ç®¡ç†ã€GPUå…¼å®¹æ€§ä»¥åŠCPUæ¨¡å¼è¿è¡Œç­‰åŠŸèƒ½ã€‚ComfyUI å¯ä»¥åŠ è½½ckptã€safetensorsã€diffusersæ¨¡å‹å’Œç‹¬ç«‹VAEã€CLIPæ¨¡å‹ï¼Œå¹¶æ”¯æŒåµŒå…¥å¼æ–‡æœ¬åè½¬ç­‰é«˜çº§åŠŸèƒ½ã€‚æ­¤å¤–ï¼Œç”¨æˆ·å¯ä»¥ä½¿ç”¨ComfyUIåŠ è½½å’Œä¿å­˜å®Œæ•´çš„å·¥ä½œæµç¨‹ï¼Œåˆ›å»ºè‡ªå®šä¹‰èŠ‚ç‚¹è¿›è¡Œé«˜çº§æ“ä½œï¼Œå¦‚å›¾åƒç”Ÿæˆæˆ–ä¿®å¤ç­‰ã€‚ç³»ç»Ÿè¿˜æä¾›äº†å¿«æ·é”®æ”¯æŒï¼Œå¢å¼ºç”¨æˆ·ä½“éªŒã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå®‰è£…ComfyUIï¼Œå¹¶æ ¹æ®è‡ªå·±çš„ç¡¬ä»¶ç¯å¢ƒï¼ˆå¦‚AMDã€NVIDIA GPUæˆ–Intel GPUï¼‰è¿›è¡Œç›¸åº”çš„é…ç½®ã€‚å¯¹äºMacç”¨æˆ·ï¼Œæä¾›äº†ä¸€ä¸ªç‰¹å®šçš„æŒ‡å—æ¥å®‰è£…å’Œè¿è¡ŒComfyUIã€‚åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­ï¼Œç”¨æˆ·éœ€è¦æ³¨æ„æ¨¡å‹è·¯å¾„è®¾ç½®ã€ä¾èµ–å®‰è£…å’Œä¸€äº›ç‰¹å®šç¡¬ä»¶çš„å…¼å®¹æ€§é—®é¢˜ã€‚ä¸ºäº†æé«˜é¢„è§ˆè´¨é‡ï¼Œç”¨æˆ·å¯ä»¥å¯ç”¨è‡ªåŠ¨é¢„è§ˆæ–¹æ³•æˆ–æ‰‹åŠ¨å®‰è£…é¢å¤–çš„æ¨¡å‹ã€‚åŒæ—¶ï¼ŒComfyUIæ”¯æŒTLS/SSLåŠ å¯†ä»¥å¢å¼ºå®‰å…¨æ€§ã€‚æœ€åï¼Œå‰ç«¯çš„æ›´æ–°å’Œç»´æŠ¤è¢«è½¬ç§»åˆ°äº†å•ç‹¬çš„ä»“åº“ä¸­ï¼Œä»¥ä¾¿äºå¼€å‘è€…ç®¡ç†å’Œç”¨æˆ·æŠ¥å‘Šå‰ç«¯ç›¸å…³çš„é—®é¢˜ã€‚ <div>
<p>The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface.</p><hr /><div align="center"> 
 <h1>ComfyUI</h1> 
 <p><strong>The most powerful and modular stable diffusion GUI and backend.</strong></p> 
 <p><a href="https://www.comfy.org/"><img alt="Website" src="https://img.shields.io/badge/ComfyOrg-4285F4?style=flat" /></a> <a href="https://www.comfy.org/discord"><img alt="Dynamic JSON Badge" src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2Fcomfyorg%3Fwith_counts%3Dtrue&amp;query=%24.approximate_member_count&amp;logo=discord&amp;logoColor=white&amp;label=Discord&amp;color=green&amp;suffix=%20total" /></a> <a href="https://app.element.io/#/room/%23comfyui_space%3Amatrix.org"><img alt="Matrix" src="https://img.shields.io/badge/Matrix-000000?style=flat&amp;logo=matrix&amp;logoColor=white" /></a> <br /> <a href="https://github.com/comfyanonymous/ComfyUI/releases"><img alt="" src="https://img.shields.io/github/v/release/comfyanonymous/ComfyUI?style=flat&amp;sort=semver" /></a> <a href="https://github.com/comfyanonymous/ComfyUI/releases"><img alt="" src="https://img.shields.io/github/release-date/comfyanonymous/ComfyUI?style=flat" /></a> <a href="https://github.com/comfyanonymous/ComfyUI/releases"><img alt="" src="https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/total?style=flat" /></a> <a href="https://github.com/comfyanonymous/ComfyUI/releases"><img alt="" src="https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/latest/total?style=flat&amp;label=downloads%40latest" /></a></p> 
 <!-- Workaround to display total user from https://github.com/badges/shields/issues/4500#issuecomment-2060079995 --> 
 <p><img alt="ComfyUI Screenshot" src="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/comfyui_screenshot.png" /></p> 
</div> 
<p>This ui will let you design and execute advanced stable diffusion pipelines using a graph/nodes/flowchart based interface. For some workflow examples and see what ComfyUI can do you can check out:</p> 
<h3><a href="https://comfyanonymous.github.io/ComfyUI_examples/">ComfyUI Examples</a></h3> 
<h3><a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#installing">Installing ComfyUI</a></h3> 
<h2>Features</h2> 
<ul> 
 <li>Nodes/graph/flowchart interface to experiment and create complex Stable Diffusion workflows without needing to code anything.</li> 
 <li>Fully supports SD1.x, SD2.x, <a href="https://comfyanonymous.github.io/ComfyUI_examples/sdxl/">SDXL</a>, <a href="https://comfyanonymous.github.io/ComfyUI_examples/video/">Stable Video Diffusion</a>, <a href="https://comfyanonymous.github.io/ComfyUI_examples/stable_cascade/">Stable Cascade</a>, <a href="https://comfyanonymous.github.io/ComfyUI_examples/sd3/">SD3</a> and <a href="https://comfyanonymous.github.io/ComfyUI_examples/audio/">Stable Audio</a></li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/flux/">Flux</a></li> 
 <li>Asynchronous Queue system</li> 
 <li>Many optimizations: Only re-executes the parts of the workflow that changes between executions.</li> 
 <li>Smart memory management: can automatically run models on GPUs with as low as 1GB vram.</li> 
 <li>Works even if you don't have a GPU with: <code>--cpu</code> (slow)</li> 
 <li>Can load ckpt, safetensors and diffusers models/checkpoints. Standalone VAEs and CLIP models.</li> 
 <li>Embeddings/Textual inversion</li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/lora/">Loras (regular, locon and loha)</a></li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/hypernetworks/">Hypernetworks</a></li> 
 <li>Loading full workflows (with seeds) from generated PNG, WebP and FLAC files.</li> 
 <li>Saving/Loading workflows as Json files.</li> 
 <li>Nodes interface can be used to create complex workflows like one for <a href="https://comfyanonymous.github.io/ComfyUI_examples/2_pass_txt2img/">Hires fix</a> or much more advanced ones.</li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/area_composition/">Area Composition</a></li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/inpaint/">Inpainting</a> with both regular and inpainting models.</li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/controlnet/">ControlNet and T2I-Adapter</a></li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/upscale_models/">Upscale Models (ESRGAN, ESRGAN variants, SwinIR, Swin2SR, etc...)</a></li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/unclip/">unCLIP Models</a></li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/gligen/">GLIGEN</a></li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/model_merging/">Model Merging</a></li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/lcm/">LCM models and Loras</a></li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/sdturbo/">SDXL Turbo</a></li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/aura_flow/">AuraFlow</a></li> 
 <li><a href="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_dit/">HunyuanDiT</a></li> 
 <li>Latent previews with <a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#how-to-show-high-quality-previews">TAESD</a></li> 
 <li>Starts up very fast.</li> 
 <li>Works fully offline: will never download anything.</li> 
 <li><a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/extra_model_paths.yaml.example">Config file</a> to set the search paths for models.</li> 
</ul> 
<p>Workflow examples can be found on the <a href="https://comfyanonymous.github.io/ComfyUI_examples/">Examples page</a></p> 
<h2>Shortcuts</h2> 
<table> 
 <thead> 
  <tr> 
   <th>Keybind</th> 
   <th>Explanation</th> 
  </tr> 
 </thead> 
 <tbody> 
  <tr> 
   <td>Ctrl + Enter</td> 
   <td>Queue up current graph for generation</td> 
  </tr> 
  <tr> 
   <td>Ctrl + Shift + Enter</td> 
   <td>Queue up current graph as first for generation</td> 
  </tr> 
  <tr> 
   <td>Ctrl + Alt + Enter</td> 
   <td>Cancel current generation</td> 
  </tr> 
  <tr> 
   <td>Ctrl + Z/Ctrl + Y</td> 
   <td>Undo/Redo</td> 
  </tr> 
  <tr> 
   <td>Ctrl + S</td> 
   <td>Save workflow</td> 
  </tr> 
  <tr> 
   <td>Ctrl + O</td> 
   <td>Load workflow</td> 
  </tr> 
  <tr> 
   <td>Ctrl + A</td> 
   <td>Select all nodes</td> 
  </tr> 
  <tr> 
   <td>Alt + C</td> 
   <td>Collapse/uncollapse selected nodes</td> 
  </tr> 
  <tr> 
   <td>Ctrl + M</td> 
   <td>Mute/unmute selected nodes</td> 
  </tr> 
  <tr> 
   <td>Ctrl + B</td> 
   <td>Bypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)</td> 
  </tr> 
  <tr> 
   <td>Delete/Backspace</td> 
   <td>Delete selected nodes</td> 
  </tr> 
  <tr> 
   <td>Ctrl + Backspace</td> 
   <td>Delete the current graph</td> 
  </tr> 
  <tr> 
   <td>Space</td> 
   <td>Move the canvas around when held and moving the cursor</td> 
  </tr> 
  <tr> 
   <td>Ctrl/Shift + Click</td> 
   <td>Add clicked node to selection</td> 
  </tr> 
  <tr> 
   <td>Ctrl + C/Ctrl + V</td> 
   <td>Copy and paste selected nodes (without maintaining connections to outputs of unselected nodes)</td> 
  </tr> 
  <tr> 
   <td>Ctrl + C/Ctrl + Shift + V</td> 
   <td>Copy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes)</td> 
  </tr> 
  <tr> 
   <td>Shift + Drag</td> 
   <td>Move multiple selected nodes at the same time</td> 
  </tr> 
  <tr> 
   <td>Ctrl + D</td> 
   <td>Load default graph</td> 
  </tr> 
  <tr> 
   <td>Alt + <code>+</code></td> 
   <td>Canvas Zoom in</td> 
  </tr> 
  <tr> 
   <td>Alt + <code>-</code></td> 
   <td>Canvas Zoom out</td> 
  </tr> 
  <tr> 
   <td>Ctrl + Shift + LMB + Vertical drag</td> 
   <td>Canvas Zoom in/out</td> 
  </tr> 
  <tr> 
   <td>Q</td> 
   <td>Toggle visibility of the queue</td> 
  </tr> 
  <tr> 
   <td>H</td> 
   <td>Toggle visibility of history</td> 
  </tr> 
  <tr> 
   <td>R</td> 
   <td>Refresh graph</td> 
  </tr> 
  <tr> 
   <td>Double-Click LMB</td> 
   <td>Open node quick search palette</td> 
  </tr> 
  <tr> 
   <td>Shift + Drag</td> 
   <td>Move multiple wires at once</td> 
  </tr> 
  <tr> 
   <td>Ctrl + Alt + LMB</td> 
   <td>Disconnect all wires from clicked slot</td> 
  </tr> 
 </tbody> 
</table> 
<p>Ctrl can also be replaced with Cmd instead for macOS users</p> 
<h1>Installing</h1> 
<h2>Windows</h2> 
<p>There is a portable standalone build for Windows that should work for running on Nvidia GPUs or for running on your CPU only on the <a href="https://github.com/comfyanonymous/ComfyUI/releases">releases page</a>.</p> 
<h3><a href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z">Direct link to download</a></h3> 
<p>Simply download, extract with <a href="https://7-zip.org">7-Zip</a> and run. Make sure you put your Stable Diffusion checkpoints/models (the huge ckpt/safetensors files) in: ComfyUI\models\checkpoints</p> 
<p>If you have trouble extracting it, right click the file -&gt; properties -&gt; unblock</p> 
<h4>How do I share models between another UI and ComfyUI?</h4> 
<p>See the <a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/extra_model_paths.yaml.example">Config file</a> to set the search paths for models. In the standalone windows build you can find this file in the ComfyUI directory. Rename this file to extra_model_paths.yaml and edit it with your favorite text editor.</p> 
<h2>Jupyter Notebook</h2> 
<p>To run it on services like paperspace, kaggle or colab you can use my <a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/notebooks/comfyui_colab.ipynb">Jupyter Notebook</a></p> 
<h2>Manual Install (Windows, Linux)</h2> 
<p>Git clone this repo.</p> 
<p>Put your SD checkpoints (the huge ckpt/safetensors files) in: models/checkpoints</p> 
<p>Put your VAE in: models/vae</p> 
<h3>AMD GPUs (Linux only)</h3> 
<p>AMD users can install rocm and pytorch with pip if you don't have it already installed, this is the command to install the stable version:</p> 
<p><code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0</code></p> 
<p>This is the command to install the nightly with ROCm 6.0 which might have some performance improvements:</p> 
<p><code>pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.1</code></p> 
<h3>NVIDIA</h3> 
<p>Nvidia users should install stable pytorch using this command:</p> 
<p><code>pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121</code></p> 
<p>This is the command to install pytorch nightly instead which might have performance improvements:</p> 
<p><code>pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu124</code></p> 
<h4>Troubleshooting</h4> 
<p>If you get the "Torch not compiled with CUDA enabled" error, uninstall torch with:</p> 
<p><code>pip uninstall torch</code></p> 
<p>And install it again with the command above.</p> 
<h3>Dependencies</h3> 
<p>Install the dependencies by opening your terminal inside the ComfyUI folder and:</p> 
<p><code>pip install -r requirements.txt</code></p> 
<p>After this you should have everything installed and can proceed to running ComfyUI.</p> 
<h3>Others:</h3> 
<h4>Intel GPUs</h4> 
<p>Intel GPU support is available for all Intel GPUs supported by Intel's Extension for Pytorch (IPEX) with the support requirements listed in the <a href="https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=gpu">Installation</a> page. Choose your platform and method of install and follow the instructions. The steps are as follows:</p> 
<ol> 
 <li>Start by installing the drivers or kernel listed or newer in the Installation page of IPEX linked above for Windows and Linux if needed.</li> 
 <li>Follow the instructions to install <a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit-download.html">Intel's oneAPI Basekit</a> for your platform.</li> 
 <li>Install the packages for IPEX using the instructions provided in the Installation page for your platform.</li> 
 <li>Follow the <a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux">ComfyUI manual installation</a> instructions for Windows and Linux and run ComfyUI normally as described above after everything is installed.</li> 
</ol> 
<p>Additional discussion and help can be found <a href="https://github.com/comfyanonymous/ComfyUI/discussions/476">here</a>.</p> 
<h4>Apple Mac silicon</h4> 
<p>You can install ComfyUI in Apple Mac silicon (M1 or M2) with any recent macOS version.</p> 
<ol> 
 <li>Install pytorch nightly. For instructions, read the <a href="https://developer.apple.com/metal/pytorch/">Accelerated PyTorch training on Mac</a> Apple Developer guide (make sure to install the latest pytorch nightly).</li> 
 <li>Follow the <a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux">ComfyUI manual installation</a> instructions for Windows and Linux.</li> 
 <li>Install the ComfyUI <a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#dependencies">dependencies</a>. If you have another Stable Diffusion UI <a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#i-already-have-another-ui-for-stable-diffusion-installed-do-i-really-have-to-install-all-of-these-dependencies">you might be able to reuse the dependencies</a>.</li> 
 <li>Launch ComfyUI by running <code>python main.py</code></li> 
</ol> 
<blockquote> 
 <p><strong>Note</strong>: Remember to add your models, VAE, LoRAs etc. to the corresponding Comfy folders, as discussed in <a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux">ComfyUI manual installation</a>.</p> 
</blockquote> 
<h4>DirectML (AMD Cards on Windows)</h4> 
<p><code>pip install torch-directml</code> Then you can launch ComfyUI with: <code>python main.py --directml</code></p> 
<h1>Running</h1> 
<p><code>python main.py</code></p> 
<h3>For AMD cards not officially supported by ROCm</h3> 
<p>Try running it with this command if you have issues:</p> 
<p>For 6700, 6600 and maybe other RDNA2 or older: <code>HSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py</code></p> 
<p>For AMD 7600 and maybe other RDNA3 cards: <code>HSA_OVERRIDE_GFX_VERSION=11.0.0 python main.py</code></p> 
<h1>Notes</h1> 
<p>Only parts of the graph that have an output with all the correct inputs will be executed.</p> 
<p>Only parts of the graph that change from each execution to the next will be executed, if you submit the same graph twice only the first will be executed. If you change the last part of the graph only the part you changed and the part that depends on it will be executed.</p> 
<p>Dragging a generated png on the webpage or loading one will give you the full workflow including seeds that were used to create it.</p> 
<p>You can use () to change emphasis of a word or phrase like: (good code:1.2) or (bad code:0.8). The default emphasis for () is 1.1. To use () characters in your actual prompt escape them like \( or \).</p> 
<p>You can use {day|night}, for wildcard/dynamic prompts. With this syntax "{wild|card|test}" will be randomly replaced by either "wild", "card" or "test" by the frontend every time you queue the prompt. To use {} characters in your actual prompt escape them like: \{ or \}.</p> 
<p>Dynamic prompts also support C-style comments, like <code>// comment</code> or <code>/* comment */</code>.</p> 
<p>To use a textual inversion concepts/embeddings in a text prompt put them in the models/embeddings directory and use them in the CLIPTextEncode node like this (you can omit the .pt extension):</p> 
<p><code>embedding:embedding_filename.pt</code></p> 
<h2>How to show high-quality previews?</h2> 
<p>Use <code>--preview-method auto</code> to enable previews.</p> 
<p>The default installation includes a fast latent preview method that's low-resolution. To enable higher-quality previews with <a href="https://github.com/madebyollin/taesd">TAESD</a>, download the <a href="https://github.com/madebyollin/taesd/raw/main/taesd_decoder.pth">taesd_decoder.pth</a> (for SD1.x and SD2.x) and <a href="https://github.com/madebyollin/taesd/raw/main/taesdxl_decoder.pth">taesdxl_decoder.pth</a> (for SDXL) models and place them in the <code>models/vae_approx</code> folder. Once they're installed, restart ComfyUI to enable high-quality previews.</p> 
<h2>How to use TLS/SSL?</h2> 
<p>Generate a self-signed certificate (not appropriate for shared/production use) and key by running the command: <code>openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -sha256 -days 3650 -nodes -subj "/C=XX/ST=StateName/L=CityName/O=CompanyName/OU=CompanySectionName/CN=CommonNameOrHostname"</code></p> 
<p>Use <code>--tls-keyfile key.pem --tls-certfile cert.pem</code> to enable TLS/SSL, the app will now be accessible with <code>https://...</code> instead of <code>http://...</code>.</p> 
<blockquote> 
 <p>Note: Windows users can use <a href="https://github.com/alexisrolland/docker-openssl">alexisrolland/docker-openssl</a> or one of the <a href="https://wiki.openssl.org/index.php/Binaries">3rd party binary distributions</a> to run the command example above. <br /><br />If you use a container, note that the volume mount <code>-v</code> can be a relative path so <code>... -v ".\:/openssl-certs" ...</code> would create the key &amp; cert files in the current directory of your command prompt or powershell terminal.</p> 
</blockquote> 
<h2>Support and dev channel</h2> 
<p><a href="https://app.element.io/#/room/%23comfyui_space%3Amatrix.org">Matrix space: #comfyui_space:matrix.org</a> (it's like discord but open source).</p> 
<p>See also: <a href="https://www.comfy.org/">https://www.comfy.org/</a></p> 
<h2>Frontend Development</h2> 
<p>As of August 15, 2024, we have transitioned to a new frontend, which is now hosted in a separate repository: <a href="https://github.com/Comfy-Org/ComfyUI_frontend">ComfyUI Frontend</a>. This repository now hosts the compiled JS (from TS/Vue) under the <code>web/</code> directory.</p> 
<h3>Reporting Issues and Requesting Features</h3> 
<p>For any bugs, issues, or feature requests related to the frontend, please use the <a href="https://github.com/Comfy-Org/ComfyUI_frontend">ComfyUI Frontend repository</a>. This will help us manage and address frontend-specific concerns more efficiently.</p> 
<h3>Using the Latest Frontend</h3> 
<p>The new frontend is now the default for ComfyUI. However, please note:</p> 
<ol> 
 <li>The frontend in the main ComfyUI repository is updated weekly.</li> 
 <li>Daily releases are available in the separate frontend repository.</li> 
</ol> 
<p>To use the most up-to-date frontend version:</p> 
<ol> 
 <li> <p>For the latest daily release, launch ComfyUI with this command line argument:</p> <pre><code>--front-end-version Comfy-Org/ComfyUI_frontend@latest
</code></pre> </li> 
 <li> <p>For a specific version, replace <code>latest</code> with the desired version number:</p> <pre><code>--front-end-version Comfy-Org/ComfyUI_frontend@1.2.2
</code></pre> </li> 
</ol> 
<p>This approach allows you to easily switch between the stable weekly release and the cutting-edge daily updates, or even specific versions for testing purposes.</p> 
<h3>Accessing the Legacy Frontend</h3> 
<p>If you need to use the legacy frontend for any reason, you can access it using the following command line argument:</p> 
<pre><code>--front-end-version Comfy-Org/ComfyUI_legacy_frontend@latest
</code></pre> 
<p>This will use a snapshot of the legacy frontend preserved in the <a href="https://github.com/Comfy-Org/ComfyUI_legacy_frontend">ComfyUI Legacy Frontend repository</a>.</p> 
<h1>QA</h1> 
<h3>Which GPU should I buy for this?</h3> 
<p><a href="https://github.com/comfyanonymous/ComfyUI/wiki/Which-GPU-should-I-buy-for-ComfyUI">See this page for some recommendations</a></p>
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>ostris/ai-toolkit</title>
<link>https://github.com/ostris/ai-toolkit</link>
<guid>https://github.com/ostris/ai-toolkit</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šAI Toolkitã€Ostrisã€Stable Diffusionã€LoRAã€FLUX.1

æ–‡ç« æ€»ç»“ï¼š

1. **AI Toolkit**ï¼šè¿™æ˜¯ä¸€ä¸ªç”±Ostrisåˆ›å»ºçš„ç ”ç©¶ä»“åº“ï¼Œä¸“æ³¨äºå®éªŒå„ç§AIæ¨¡å‹ï¼Œå°¤å…¶æ˜¯Stable DiffusionæŠ€æœ¯ã€‚å®ƒå…è®¸ç”¨æˆ·è®­ç»ƒå’Œè‡ªå®šä¹‰å¤šç§æ¨¡å‹ã€‚

2. **Ostrisä¸å›¢é˜Ÿæ”¯æŒ**ï¼šOstriså¼ºè°ƒäº†ä»–/å¥¹çš„å·¥ä½œç¦»ä¸å¼€å›¢é˜Ÿçš„æ”¯æŒï¼Œç‰¹åˆ«æ˜¯Glifã€å’Œæ‰€æœ‰äººã€‚ä»–/å¥¹é¼“åŠ±æ”¯æŒå›¢é˜Ÿçš„å·¥ä½œï¼Œå…·ä½“æ–¹å¼æ˜¯æ”¯æŒGlifã€‚

3. **FLUX.1 Training**ï¼šFLUX.1æ˜¯ä¸€ä¸ªéœ€è¦GPUèµ„æºè¿›è¡Œè®­ç»ƒçš„æ¨¡å‹ã€‚å¯¹äºç‰¹å®šé…ç½®çš„GPUï¼ˆè‡³å°‘24GB VRAMï¼‰ï¼Œç”¨æˆ·å¯ä»¥è®­ç»ƒæ­¤æ¨¡å‹ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦ä¸€äº›æŠ€å·§æ¥é€‚åº”æœ‰é™çš„VRAMä½¿ç”¨ã€‚

4. **è®¸å¯è¯é—®é¢˜**ï¼šè®­ç»ƒFLUX.1æ¨¡å‹å‰ï¼Œç”¨æˆ·éœ€æ¥å—è®¸å¯è¯åè®®ã€‚è¿™æ¶‰åŠåˆ°ç™»å½•Hugging Faceå¹³å°å¹¶æ¥å—è®¿é—®æƒé™ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†ä¸€ä¸ªåä¸ºâ€œFLUX.1-schnellâ€çš„ç‰ˆæœ¬ï¼Œå…¶è®¸å¯åè®®æ›´ä¸ºå®½æ¾ï¼Œå…è®¸ç”¨æˆ·æ ¹æ®è‡ªå·±çš„éœ€æ±‚è‡ªç”±ä½¿ç”¨å’Œå‘å¸ƒè®­ç»ƒç»“æœã€‚

5. **è¿è¡Œå’Œéƒ¨ç½²**ï¼šæ–‡ç« ä»‹ç»äº†å¦‚ä½•åœ¨æœ¬åœ°æˆ–äº‘ç¯å¢ƒä¸­è¿è¡ŒAI Toolkitæ¨¡å‹ã€‚åŒ…æ‹¬å¦‚ä½•åœ¨Linuxæˆ–Windowsä¸Šå®‰è£…ç¯å¢ƒï¼Œä»¥åŠå¦‚ä½•åœ¨RunPodäº‘å¹³å°ä¸Šè®¾ç½®å’Œè¿è¡Œæ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚

ç®€è€Œè¨€ä¹‹ï¼ŒAI Toolkitæ˜¯ä¸€ä¸ªç”¨äºç ”ç©¶å’Œå®éªŒå„ç§AIæ¨¡å‹çš„å¹³å°ï¼Œç‰¹åˆ«æ˜¯Stable Diffusionç›¸å…³æŠ€æœ¯ã€‚å®ƒæä¾›äº†ä»æœ¬åœ°åˆ°äº‘çš„æ¨¡å‹è®­ç»ƒå’Œéƒ¨ç½²è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶ä¹Ÿå¼ºè°ƒäº†è®¸å¯è¯ç®¡ç†çš„é‡è¦æ€§ã€‚é€šè¿‡éµå¾ªæ–‡æ¡£ä¸­çš„æŒ‡å¯¼ï¼Œç”¨æˆ·å¯ä»¥åˆ©ç”¨è¿™ä¸ªå·¥å…·é›†è¿›è¡Œåˆ›æ–°æ€§çš„AIç ”ç©¶å’Œåº”ç”¨å¼€å‘ã€‚ <div>
<p>Various AI scripts. Mostly Stable Diffusion stuff.</p><hr /><h1>AI Toolkit by Ostris</h1> 
<h2>IMPORTANT NOTE - READ THIS</h2> 
<p>This is my research repo. I do a lot of experiments in it and it is possible that I will break things. If something breaks, checkout an earlier commit. This repo can train a lot of things, and it is hard to keep up with all of them.</p> 
<h2>Support my work</h2> 
<a href="https://glif.app" target="_blank"> <img alt="glif.app" height="auto" src="https://raw.githubusercontent.com/ostris/ai-toolkit/main/assets/glif.svg?v=1" width="256" /> </a> 
<p>My work on this project would not be possible without the amazing support of <a href="https://glif.app/">Glif</a> and everyone on the team. If you want to support me, support Glif. <a href="https://glif.app/">Join the site</a>, <a href="https://discord.com/invite/nuR9zZ2nsh">Join us on Discord</a>, <a href="https://x.com/heyglif">follow us on Twitter</a> and come make some cool stuff with us</p> 
<h2>Installation</h2> 
<p>Requirements:</p> 
<ul> 
 <li>python &gt;3.10</li> 
 <li>Nvidia GPU with enough ram to do what you need</li> 
 <li>python venv</li> 
 <li>git</li> 
</ul> 
<p>Linux:</p> 
<pre><code class="language-bash">git clone https://github.com/ostris/ai-toolkit.git
cd ai-toolkit
git submodule update --init --recursive
python3 -m venv venv
source venv/bin/activate
# .\venv\Scripts\activate on windows
# install torch first
pip3 install torch
pip3 install -r requirements.txt
</code></pre> 
<p>Windows:</p> 
<pre><code class="language-bash">git clone https://github.com/ostris/ai-toolkit.git
cd ai-toolkit
git submodule update --init --recursive
python -m venv venv
.\venv\Scripts\activate
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
pip install -r requirements.txt
</code></pre> 
<h2>FLUX.1 Training</h2> 
<h3>WIP. I am updating docs and optimizing as fast as I can. If there are bugs open a ticket. Not knowing how to get it to work is NOT a bug. Be paitient as I continue to develop it.</h3> 
<h3>Requirements</h3> 
<p>You currently need a GPU with <strong>at least 24GB of VRAM</strong> to train FLUX.1. If you are using it as your GPU to control your monitors, you probably need to set the flag <code>low_vram: true</code> in the config file under <code>model:</code>. This will quantize the model on CPU and should allow it to train with monitors attached. Users have gotten it to work on Windows with WSL, but there are some reports of a bug when running on windows natively. I have only tested on linux for now. This is still extremely experimental and a lot of quantizing and tricks had to happen to get it to fit on 24GB at all.</p> 
<h3>FLUX.1-dev</h3> 
<p>FLUX.1-dev has a non-commercial license. Which means anything you train will inherit the non-commercial license. It is also a gated model, so you need to accept the license on HF before using it. Otherwise, this will fail. Here are the required steps to setup a license.</p> 
<ol> 
 <li>Sign into HF and accept the model access here <a href="https://huggingface.co/black-forest-labs/FLUX.1-dev">black-forest-labs/FLUX.1-dev</a></li> 
 <li>Make a file named <code>.env</code> in the root on this folder</li> 
 <li><a href="https://huggingface.co/settings/tokens/new?">Get a READ key from huggingface</a> and add it to the <code>.env</code> file like so <code>HF_TOKEN=your_key_here</code></li> 
</ol> 
<h3>FLUX.1-schnell</h3> 
<p>FLUX.1-schnell is Apache 2.0. Anything trained on it can be licensed however you want and it does not require a HF_TOKEN to train. However, it does require a special adapter to train with it, <a href="https://huggingface.co/ostris/FLUX.1-schnell-training-adapter">ostris/FLUX.1-schnell-training-adapter</a>. It is also highly experimental. For best overall quality, training on FLUX.1-dev is recommended.</p> 
<p>To use it, You just need to add the assistant to the <code>model</code> section of your config file like so:</p> 
<pre><code class="language-yaml">      model:
        name_or_path: "black-forest-labs/FLUX.1-schnell"
        assistant_lora_path: "ostris/FLUX.1-schnell-training-adapter"
        is_flux: true
        quantize: true
</code></pre> 
<p>You also need to adjust your sample steps since schnell does not require as many</p> 
<pre><code class="language-yaml">      sample:
        guidance_scale: 1  # schnell does not do guidance
        sample_steps: 4  # 1 - 4 works well
</code></pre> 
<h3>Training</h3> 
<ol> 
 <li>Copy the example config file located at <code>config/examples/train_lora_flux_24gb.yaml</code> (<code>config/examples/train_lora_flux_schnell_24gb.yaml</code> for schnell) to the <code>config</code> folder and rename it to <code>whatever_you_want.yml</code></li> 
 <li>Edit the file following the comments in the file</li> 
 <li>Run the file like so <code>python run.py config/whatever_you_want.yml</code></li> 
</ol> 
<p>A folder with the name and the training folder from the config file will be created when you start. It will have all checkpoints and images in it. You can stop the training at any time using ctrl+c and when you resume, it will pick back up from the last checkpoint.</p> 
<p>IMPORTANT. If you press crtl+c while it is saving, it will likely corrupt that checkpoint. So wait until it is done saving</p> 
<h3>Need help?</h3> 
<p>Please do not open a bug report unless it is a bug in the code. You are welcome to <a href="https://discord.gg/SzVB3wYvxF">Join my Discord</a> and ask for help there. However, please refrain from PMing me directly with general question or support. Ask in the discord and I will answer when I can.</p> 
<h3>Training in RunPod cloud</h3> 
<p>Example RunPod template: <strong>runpod/pytorch:2.2.0-py3.10-cuda12.1.1-devel-ubuntu22.04</strong></p> 
<blockquote> 
 <p>You need a minimum of 24GB VRAM, pick a GPU by your preference.</p> 
</blockquote> 
<h4>Example config ($0.5/hr):</h4> 
<ul> 
 <li>1x A40 (48 GB VRAM)</li> 
 <li>19 vCPU 100 GB RAM</li> 
</ul> 
<h4>Custom overrides (you need some storage to clone FLUX.1, store datasets, store trained models and samples):</h4> 
<ul> 
 <li>~120 GB Disk</li> 
 <li>~120 GB Pod Volume</li> 
 <li>Start Jupyter Notebook</li> 
</ul> 
<h3>1. Setup</h3> 
<pre><code>git clone https://github.com/ostris/ai-toolkit.git
cd ai-toolkit
git submodule update --init --recursive
python -m venv venv
source venv/bin/activate
pip install torch
pip install -r requirements.txt
pip install --upgrade accelerate transformers diffusers huggingface_hub #Optional, run it if you run into issues
</code></pre> 
<h3>2. Upload your dataset</h3> 
<ul> 
 <li>Create a new folder in the root, name it <code>dataset</code> or whatever you like</li> 
 <li>Drag and drop your .jpg and .txt files inside the newly created dataset folder</li> 
</ul> 
<h3>3. Login into Hugging Face with an Access Token</h3> 
<ul> 
 <li>Get a READ token from <a href="https://huggingface.co/settings/tokens">here</a></li> 
 <li>Run <code>huggingface-cli login</code> and paste your token</li> 
</ul> 
<h3>4. Training</h3> 
<ul> 
 <li>Copy an example config file located at <code>config/examples</code> to the config folder and rename it to <code>whatever_you_want.yml</code></li> 
 <li>Edit the config following the comments in the file</li> 
 <li>Change <code>folder_path: "/path/to/images/folder"</code> to your dataset path like <code>folder_path: "/workspace/ai-toolkit/your-dataset"</code></li> 
 <li>Run the file: <code>python run.py config/whatever_you_want.yml</code></li> 
</ul> 
<h3>Screenshot from RunPod</h3> 
<img alt="RunPod Training Screenshot" src="https://github.com/user-attachments/assets/53a1b8ef-92fa-4481-81a7-bde45a14a7b5" width="1728" /> 
<!--
### Training in the cloud
Coming very soon. Getting base out then will have a notebook that makes all that work. 
--> 
<hr /> 
<h2>Dataset Preparation</h2> 
<p>Datasets generally need to be a folder containing images and associated text files. Currently, the only supported formats are jpg, jpeg, and png. Webp currently has issues. The text files should be named the same as the images but with a <code>.txt</code> extension. For example <code>image2.jpg</code> and <code>image2.txt</code>. The text file should contain only the caption. You can add the word <code>[trigger]</code> in the caption file and if you have <code>trigger_word</code> in your config, it will be automatically replaced.</p> 
<p>Images are never upscaled but they are downscaled and placed in buckets for batching. <strong>You do not need to crop/resize your images</strong>. The loader will automatically resize them and can handle varying aspect ratios.</p> 
<hr /> 
<h2>EVERYTHING BELOW THIS LINE IS OUTDATED</h2> 
<p>It may still work like that, but I have not tested it in a while.</p> 
<hr /> 
<h3>Batch Image Generation</h3> 
<p>A image generator that can take frompts from a config file or form a txt file and generate them to a folder. I mainly needed this for an SDXL test I am doing but added some polish to it so it can be used for generat batch image generation. It all runs off a config file, which you can find an example of in <code>config/examples/generate.example.yaml</code>. Mere info is in the comments in the example</p> 
<hr /> 
<h3>LoRA (lierla), LoCON (LyCORIS) extractor</h3> 
<p>It is based on the extractor in the <a href="https://github.com/KohakuBlueleaf/LyCORIS">LyCORIS</a> tool, but adding some QOL features and LoRA (lierla) support. It can do multiple types of extractions in one run. It all runs off a config file, which you can find an example of in <code>config/examples/extract.example.yml</code>. Just copy that file, into the <code>config</code> folder, and rename it to <code>whatever_you_want.yml</code>. Then you can edit the file to your liking. and call it like so:</p> 
<pre><code class="language-bash">python3 run.py config/whatever_you_want.yml
</code></pre> 
<p>You can also put a full path to a config file, if you want to keep it somewhere else.</p> 
<pre><code class="language-bash">python3 run.py "/home/user/whatever_you_want.yml"
</code></pre> 
<p>More notes on how it works are available in the example config file itself. LoRA and LoCON both support extractions of 'fixed', 'threshold', 'ratio', 'quantile'. I'll update what these do and mean later. Most people used fixed, which is traditional fixed dimension extraction.</p> 
<p><code>process</code> is an array of different processes to run. You can add a few and mix and match. One LoRA, one LyCON, etc.</p> 
<hr /> 
<h3>LoRA Rescale</h3> 
<p>Change <code>&lt;lora:my_lora:4.6&gt;</code> to <code>&lt;lora:my_lora:1.0&gt;</code> or whatever you want with the same effect. A tool for rescaling a LoRA's weights. Should would with LoCON as well, but I have not tested it. It all runs off a config file, which you can find an example of in <code>config/examples/mod_lora_scale.yml</code>. Just copy that file, into the <code>config</code> folder, and rename it to <code>whatever_you_want.yml</code>. Then you can edit the file to your liking. and call it like so:</p> 
<pre><code class="language-bash">python3 run.py config/whatever_you_want.yml
</code></pre> 
<p>You can also put a full path to a config file, if you want to keep it somewhere else.</p> 
<pre><code class="language-bash">python3 run.py "/home/user/whatever_you_want.yml"
</code></pre> 
<p>More notes on how it works are available in the example config file itself. This is useful when making all LoRAs, as the ideal weight is rarely 1.0, but now you can fix that. For sliders, they can have weird scales form -2 to 2 or even -15 to 15. This will allow you to dile it in so they all have your desired scale</p> 
<hr /> 
<h3>LoRA Slider Trainer</h3> 
<a href="https://colab.research.google.com/github/ostris/ai-toolkit/blob/main/notebooks/SliderTraining.ipynb" target="_blank"> <img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" /> </a> 
<p>This is how I train most of the recent sliders I have on Civitai, you can check them out in my <a href="https://civitai.com/user/Ostris/models">Civitai profile</a>. It is based off the work by <a href="https://github.com/p1atdev/LECO">p1atdev/LECO</a> and <a href="https://github.com/rohitgandikota/erasing">rohitgandikota/erasing</a> But has been heavily modified to create sliders rather than erasing concepts. I have a lot more plans on this, but it is very functional as is. It is also very easy to use. Just copy the example config file in <code>config/examples/train_slider.example.yml</code> to the <code>config</code> folder and rename it to <code>whatever_you_want.yml</code>. Then you can edit the file to your liking. and call it like so:</p> 
<pre><code class="language-bash">python3 run.py config/whatever_you_want.yml
</code></pre> 
<p>There is a lot more information in that example file. You can even run the example as is without any modifications to see how it works. It will create a slider that turns all animals into dogs(neg) or cats(pos). Just run it like so:</p> 
<pre><code class="language-bash">python3 run.py config/examples/train_slider.example.yml
</code></pre> 
<p>And you will be able to see how it works without configuring anything. No datasets are required for this method. I will post an better tutorial soon.</p> 
<hr /> 
<h2>Extensions!!</h2> 
<p>You can now make and share custom extensions. That run within this framework and have all the inbuilt tools available to them. I will probably use this as the primary development method going forward so I dont keep adding and adding more and more features to this base repo. I will likely migrate a lot of the existing functionality as well to make everything modular. There is an example extension in the <code>extensions</code> folder that shows how to make a model merger extension. All of the code is heavily documented which is hopefully enough to get you started. To make an extension, just copy that example and replace all the things you need to.</p> 
<h3>Model Merger - Example Extension</h3> 
<p>It is located in the <code>extensions</code> folder. It is a fully finctional model merger that can merge as many models together as you want. It is a good example of how to make an extension, but is also a pretty useful feature as well since most mergers can only do one model at a time and this one will take as many as you want to feed it. There is an example config file in there, just copy that to your <code>config</code> folder and rename it to <code>whatever_you_want.yml</code>. and use it like any other config file.</p> 
<h2>WIP Tools</h2> 
<h3>VAE (Variational Auto Encoder) Trainer</h3> 
<p>This works, but is not ready for others to use and therefore does not have an example config. I am still working on it. I will update this when it is ready. I am adding a lot of features for criteria that I have used in my image enlargement work. A Critic (discriminator), content loss, style loss, and a few more. If you don't know, the VAE for stable diffusion (yes even the MSE one, and SDXL), are horrible at smaller faces and it holds SD back. I will fix this. I'll post more about this later with better examples later, but here is a quick test of a run through with various VAEs. Just went in and out. It is much worse on smaller faces than shown here.</p> 
<img height="auto" src="https://raw.githubusercontent.com/ostris/ai-toolkit/main/assets/VAE_test1.jpg" width="768" /> 
<hr /> 
<h2>TODO</h2> 
<ul> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> Add proper regs on sliders</li> 
 <li><input checked="checked" disabled="disabled" type="checkbox" /> Add SDXL support (base model only for now)</li> 
 <li><input disabled="disabled" type="checkbox" /> Add plain erasing</li> 
 <li><input disabled="disabled" type="checkbox" /> Make Textual inversion network trainer (network that spits out TI embeddings)</li> 
</ul> 
<hr /> 
<h2>Change Log</h2> 
<h4>2023-08-05</h4> 
<ul> 
 <li>Huge memory rework and slider rework. Slider training is better thant ever with no more ram spikes. I also made it so all 4 parts of the slider algorythm run in one batch so they share gradient accumulation. This makes it much faster and more stable.</li> 
 <li>Updated the example config to be something more practical and more updated to current methods. It is now a detail slide and shows how to train one without a subject. 512x512 slider training for 1.5 should work on 6GB gpu now. Will test soon to verify.</li> 
</ul> 
<h4>2021-10-20</h4> 
<ul> 
 <li>Windows support bug fixes</li> 
 <li>Extensions! Added functionality to make and share custom extensions for training, merging, whatever. check out the example in the <code>extensions</code> folder. Read more about that above.</li> 
 <li>Model Merging, provided via the example extension.</li> 
</ul> 
<h4>2023-08-03</h4> 
<p>Another big refactor to make SD more modular.</p> 
<p>Made batch image generation script</p> 
<h4>2023-08-01</h4> 
<p>Major changes and update. New LoRA rescale tool, look above for details. Added better metadata so Automatic1111 knows what the base model is. Added some experiments and a ton of updates. This thing is still unstable at the moment, so hopefully there are not breaking changes.</p> 
<p>Unfortunately, I am too lazy to write a proper changelog with all the changes.</p> 
<p>I added SDXL training to sliders... but.. it does not work properly. The slider training relies on a model's ability to understand that an unconditional (negative prompt) means you do not want that concept in the output. SDXL does not understand this for whatever reason, which makes separating out concepts within the model hard. I am sure the community will find a way to fix this over time, but for now, it is not going to work properly. And if any of you are thinking "Could we maybe fix it by adding 1 or 2 more text encoders to the model as well as a few more entirely separate diffusion networks?" No. God no. It just needs a little training without every experimental new paper added to it. The KISS principal.</p> 
<h4>2023-07-30</h4> 
<p>Added "anchors" to the slider trainer. This allows you to set a prompt that will be used as a regularizer. You can set the network multiplier to force spread consistency at high weights</p>
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>versotile-org/verso</title>
<link>https://github.com/versotile-org/verso</link>
<guid>https://github.com/versotile-org/verso</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šVersoã€Servoã€Webæµè§ˆå™¨ã€å¤šçª—å£æ”¯æŒã€å¤šè¿›ç¨‹æ¨¡å¼

æ€»ç»“:
Versoæ˜¯ä¸€ä¸ªåŸºäºServoå¼•æ“å¼€å‘çš„Webæµè§ˆå™¨é¡¹ç›®ï¼Œæ—¨åœ¨æ¢ç´¢Servoçš„å¤šè§†å›¾å’Œå¤šçª—å£åŠŸèƒ½ï¼Œæœ€ç»ˆç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªæˆç†Ÿçš„åŠŸèƒ½å®Œå¤‡çš„æµè§ˆå™¨ã€‚å½“å‰ç‰ˆæœ¬ä»å¤„äºå¼€å‘é˜¶æ®µï¼Œä¸»è¦å…³æ³¨äºç”¨æˆ·ç•Œé¢çš„è‡ªå®šä¹‰æ„å»ºä»¥åŠé—®é¢˜ä¿®å¤çš„PRæ¥å—ã€‚ä¸ºäº†è·å–æœ€ä½³ä½“éªŒï¼Œæ¨èä½¿ç”¨Gitã€Pythonã€LLVMã€CMakeç­‰å·¥å…·è¿›è¡Œæœ¬åœ°ç¼–è¯‘å’Œè¿è¡Œã€‚

å¯¹äºä¸åŒçš„æ“ä½œç³»ç»Ÿï¼ŒVersoæä¾›äº†ä¸åŒçš„å®‰è£…æ–¹å¼ï¼š
- åœ¨Windowsç³»ç»Ÿä¸Šï¼Œé¦–å…ˆéœ€è¦é€šè¿‡Scoopæˆ–Chocolateyå®‰è£…Gitã€Pythonç­‰ä¾èµ–é¡¹ï¼Œç„¶åé€šè¿‡`cargo run`å‘½ä»¤å¯åŠ¨æµè§ˆå™¨ã€‚
- MacOSç”¨æˆ·åˆ™éœ€è¦å®‰è£…Homebrewå¹¶æ‰§è¡Œç±»ä¼¼çš„å‘½ä»¤æ¥å®Œæˆå®‰è£…å’Œå¯åŠ¨è¿‡ç¨‹ã€‚
- Linuxç”¨æˆ·å¯ä»¥å€ŸåŠ©Flatpakè¿›è¡Œç»Ÿä¸€çš„ç¯å¢ƒè®¾ç½®ä¸åŒ…ç®¡ç†ï¼Œé€šè¿‡ç‰¹å®šå‘½ä»¤ç”Ÿæˆmanifestæ–‡ä»¶å¹¶æ„å»ºVersoåº”ç”¨ã€‚

Versoè¿˜æ”¯æŒåœ¨Nixç¯å¢ƒä¸­ä½¿ç”¨NixShellè¿›è¡Œæœ¬åœ°æ„å»ºï¼Œå°½ç®¡å½“å‰å¹¶æœªç›´æ¥åœ¨Nixä»“åº“ä¸­æ‰“åŒ…è¯¥åº”ç”¨ã€‚å¯¹äºéœ€è¦æ„å»ºä½†ä¸å¸Œæœ›ä½¿ç”¨ä»»ä½•æ²™ç®±ç¯å¢ƒçš„ç”¨æˆ·ï¼Œå¯ä»¥å‚è€ƒæ–‡æ¡£ä¸­çš„æŒ‡å¯¼è¿›è¡Œæ“ä½œï¼Œä½†éœ€æ³¨æ„åœ¨é‡åˆ°æ„å»ºé—®é¢˜æ—¶å¯èƒ½æ— æ³•å¾—åˆ°æ”¯æŒã€‚

æœªæ¥è®¡åˆ’åŒ…æ‹¬å®ç°å¤šçª—å£æ”¯æŒã€å¯ç”¨å¤šè¿›ç¨‹æ¨¡å¼ã€å¢å¼ºå®‰å…¨æ€§çš„æ²™ç®±åŠŸèƒ½ï¼Œä»¥åŠé›†æˆGstreamerä»¥æä¾›æ›´ä¸°å¯Œçš„å¤šåª’ä½“æ’­æ”¾èƒ½åŠ›ã€‚ç›®å‰çš„Nightlyç‰ˆæœ¬å¯ä»¥é€šè¿‡ç‰¹å®šé“¾æ¥è®¿é—®ï¼Œä½†è¯·æ³¨æ„ï¼Œè¿™äº›ç‰ˆæœ¬æœªç»è¿‡ç­¾åï¼Œå¯¹äºMacOSç”¨æˆ·åœ¨å®‰è£…åå¯èƒ½éœ€è¦æ‰‹åŠ¨è§£é™¤åº”ç”¨çš„å®‰å…¨é™åˆ¶ï¼ˆé€šè¿‡å‘½ä»¤`xattr -d com.apple.quarantine /Applications/verso.app`ï¼‰ã€‚

é€šè¿‡æŒç»­çš„å¼€å‘å’Œç¤¾åŒºè´¡çŒ®ï¼ŒVersoæœ‰æœ›åœ¨æœªæ¥æˆä¸ºä¸€æ¬¾åŠŸèƒ½å¼ºå¤§ã€ç”¨æˆ·ä½“éªŒä¼˜ç§€çš„Webæµè§ˆå™¨ã€‚ <div>
<p>A web browser that plays old world blues to build new world hope</p><hr /><h1>Verso</h1> 
<p><a href="https://versotile.zulipchat.com/"><img alt="project chat" src="https://img.shields.io/badge/zulip-57a7ff?style=for-the-badge&amp;labelColor=555555&amp;logo=zulip" /></a></p> 
<p>A web browser that plays old world blues to build new world hope.</p> 
<p><img alt="verso" src="https://github.com/user-attachments/assets/48a834af-858e-4f93-969f-07fb8f5f2496" /></p> 
<p>Verso is a web browser built on top of the <a href="https://servo.org/">Servo</a> web engine. We aim to explore embedding solutions for Servo while growing it into a mature browser one day. This means we want to experiment with multi-view and multi-window first and then build UI elements entirely from Servo itself. At the moment, <a href="https://servo.org/download/">Servoshell</a> should provide a better user experience.</p> 
<p>Verso is still under development. We don't accept feature requests at the moment, and the whole navigation workflow hasn't been polished yet, either. But if you are interested, feel free to open bug-fix PRs.</p> 
<h1>Usage</h1> 
<h2>Getting Started</h2> 
<h3>Windows</h3> 
<ul> 
 <li>Install <a href="https://scoop.sh/">scoop</a> and then install other tools:</li> 
</ul> 
<pre><code class="language-sh">scoop install git python llvm cmake curl
pip install mako
</code></pre> 
<blockquote> 
 <p>You can also use chocolatey to install if you prefer it.</p> 
</blockquote> 
<ul> 
 <li>Build &amp; run:</li> 
</ul> 
<pre><code class="language-sh">cargo run
</code></pre> 
<h3>MacOS</h3> 
<ul> 
 <li>Install <a href="https://developer.apple.com/xcode/">Xcode</a></li> 
 <li>Install <a href="https://brew.sh/">Homebrew</a> and then install other tools:</li> 
</ul> 
<pre><code class="language-sh">brew install cmake pkg-config harfbuzz
pip install mako
</code></pre> 
<ul> 
 <li>Build &amp; run:</li> 
</ul> 
<pre><code class="language-sh">cargo run
</code></pre> 
<h3>Linux</h3> 
<h4>Flatpak</h4> 
<p>For unified environment setup and package experience, we choose Flatpak to build the project from the start. Please follow the <a href="https://flatpak.org/setup/">Flatpak Setup</a> page to install Flatpak based on your distribution.</p> 
<ul> 
 <li>Generate manifests and build: // TODO Exporting to a repository instead</li> 
</ul> 
<pre><code class="language-sh">python3 ./flatpak-cargo-generator.py ./Cargo.lock -o cargo-sources.json
flatpak-builder --user --install --install-deps-from=flathub --force-clean target org.versotile.verso.yml
flatpak run org.versotile.verso
</code></pre> 
<h4>Nix</h4> 
<p>We also support building Verso in nix shell. But we don't bundle it in nix at the moment.</p> 
<ul> 
 <li>For NixOS:</li> 
</ul> 
<pre><code class="language-sh">nix-shell shell.nix --run 'cargo r'
</code></pre> 
<ul> 
 <li>For non-NixOS distributions:</li> 
</ul> 
<pre><code class="language-sh">nix-shell shell.nix --run 'nixGL cargo r'
</code></pre> 
<p>If you prefer to build the project without any sandbox, please follow the instructions in <a href="https://book.servo.org/hacking/setting-up-your-environment.html#tools-for-linux">Servo book</a> to bootstrap. But please understand we don't triage any build issue without flatpak or nix setup.</p> 
<h2>Nightly Release</h2> 
<p>Nightly releases built with CrabNebula Cloud can be found at <a href="https://web.crabnebula.cloud/verso/verso-nightly/releases">releases</a>.</p> 
<blockquote> 
 <p>Packages are unsigned currently. If you have problem opening the app on macOS, try <code>xattr -d com.apple.quarantine /Applications/verso.app</code> after installation.</p> 
</blockquote> 
<h2>Future Work</h2> 
<ul> 
 <li>Multi-window support.</li> 
 <li>Enable multiprocess mode.</li> 
 <li>Enable sandbox in all platforms.</li> 
 <li>Enable <code>Gstreamer</code> feature.</li> 
</ul>
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>chen08209/FlClash</title>
<link>https://github.com/chen08209/FlClash</link>
<guid>https://github.com/chen08209/FlClash</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šFlClashã€ClashMetaã€å¼€æºã€æ— å¹¿å‘Šã€å¤šå¹³å°

æ€»ç»“ï¼š

FlClashæ˜¯ä¸€æ¬¾åŸºäºClashMetaå¼€å‘çš„è·¨å¹³å°ä»£ç†å®¢æˆ·ç«¯ï¼Œå…¶è®¾è®¡ç®€æ´æ˜“ç”¨ï¼Œä¸”å®Œå…¨å¼€æºï¼Œä¸å«æœ‰ä»»ä½•å¹¿å‘Šã€‚è¿™æ¬¾è½¯ä»¶æ”¯æŒAndroidã€Windowsã€macOSå’ŒLinuxç­‰å¤šä¸ªæ“ä½œç³»ç»Ÿï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒå±å¹•å°ºå¯¸ï¼Œæä¾›å¤šç§ä¸»é¢˜è‰²ä¾›ç”¨æˆ·é€‰æ‹©ï¼Œç•Œé¢è®¾è®¡ç¬¦åˆMaterial Youé£æ ¼ï¼Œæ—¢ç¾è§‚åˆå®ç”¨ã€‚å®ƒè¿˜å…·æœ‰é€šè¿‡WebDAVè¿›è¡Œæ•°æ®åŒæ­¥çš„åŠŸèƒ½ï¼ŒåŒæ—¶æ”¯æŒè®¢é˜…é“¾æ¥å’Œæ·±è‰²æ¨¡å¼ã€‚

ä¸ºäº†æ„å»ºå’Œéƒ¨ç½²FlClashï¼Œå¼€å‘è€…éœ€è¦éµå¾ªä¸€å®šçš„æ­¥éª¤ã€‚å¯¹äºæ¡Œé¢ç”¨æˆ·ï¼Œä»–ä»¬éœ€è¦å®‰è£…ç›¸åº”çš„ç¯å¢ƒï¼ˆå¦‚Android SDKã€NDKæˆ–GCCï¼‰ï¼Œç„¶åè¿è¡Œç‰¹å®šçš„æ„å»ºè„šæœ¬ã€‚å¯¹äºWindowså’ŒLinuxç”¨æˆ·ï¼Œéœ€è¦é¢å¤–å®‰è£…ä¸€äº›è¾…åŠ©å·¥å…·ï¼ˆå¦‚Inno Setupï¼‰ï¼Œè€ŒMacOSç”¨æˆ·åˆ™ç›´æ¥è¿è¡Œæ„å»ºè„šæœ¬å³å¯ã€‚

æœ€åï¼Œé¼“åŠ±ç”¨æˆ·é€šè¿‡ç‚¹å‡»é¡µé¢é¡¶éƒ¨çš„æ˜Ÿæ ‡æŒ‰é’®æ¥æ”¯æŒå¼€å‘è€…çš„å·¥ä½œï¼Œè¿™æ˜¯æœ€ç®€å•æœ‰æ•ˆçš„æ–¹å¼ä¹‹ä¸€ã€‚ <div>
<p>A multi-platform proxy client based on ClashMeta,simple and easy to use, open-source and ad-free.</p><hr /><div> 
 <p><a href="https://raw.githubusercontent.com/chen08209/FlClash/main/README_zh_CN.md"><strong>ç®€ä½“ä¸­æ–‡</strong></a></p> 
</div> 
<h2>FlClash</h2> 
<p style="text-align: left;"> <img alt="stars" src="https://img.shields.io/github/stars/chen08209/FlClash?style=flat-square&amp;logo=github" /> <img alt="downloads" src="https://img.shields.io/github/downloads/chen08209/FlClash/total" /> <a href="https://raw.githubusercontent.com/chen08209/FlClash/main/LICENSE"> <img alt="license" src="https://img.shields.io/github/license/chen08209/FlClash" /> </a> </p> 
<p>A multi-platform proxy client based on ClashMeta, simple and easy to use, open-source and ad-free.</p> 
<p>on Desktop:</p> 
<p style="text-align: center;"> <img alt="desktop" src="https://raw.githubusercontent.com/chen08209/FlClash/main/snapshots/desktop.gif" /> </p> 
<p>on Mobile:</p> 
<p style="text-align: center;"> <img alt="mobile" src="https://raw.githubusercontent.com/chen08209/FlClash/main/snapshots/mobile.gif" /> </p> 
<h2>Features</h2> 
<p>âœˆï¸ Multi-platform: Android, Windows, macOS and Linux</p> 
<p>ğŸ’» Adaptive multiple screen sizes, Multiple color themes available</p> 
<p>ğŸ’¡ Based on Material You Design, <a href="https://github.com/getsurfboard/surfboard">Surfboard</a>-like UI</p> 
<p>â˜ï¸ Supports data sync via WebDAV</p> 
<p>âœ¨ Support subscription link, Dark mode</p> 
<h2>Download</h2> 
<p><a href="https://chen08209.github.io/FlClash-fdroid-repo/repo?fingerprint=789D6D32668712EF7672F9E58DEEB15FBD6DCEEC5AE7A4371EA72F2AAE8A12FD"><img alt="Get it on F-Droid" src="https://raw.githubusercontent.com/chen08209/FlClash/main/snapshots/get-it-on-fdroid.svg?sanitize=true" width="200px" /></a> <a href="https://github.com/chen08209/FlClash/releases"><img alt="Get it on GitHub" src="https://raw.githubusercontent.com/chen08209/FlClash/main/snapshots/get-it-on-github.svg?sanitize=true" width="200px" /></a></p> 
<h2>Contact</h2> 
<p><a href="https://t.me/+G-veVtwBOl4wODc1">Telegram</a></p> 
<h2>Build</h2> 
<ol> 
 <li> <p>Update submodules</p> <pre><code class="language-bash">git submodule update --init --recursive
</code></pre> </li> 
 <li> <p>Install <code>Flutter</code> and <code>Golang</code> environment</p> </li> 
 <li> <p>Build Application</p> 
  <ul> 
   <li> <p>android</p> 
    <ol> 
     <li> <p>Install <code>Android SDK</code> , <code>Android NDK</code></p> </li> 
     <li> <p>Set <code>ANDROID_NDK</code> environment variables</p> </li> 
     <li> <p>Run Build script</p> <pre><code class="language-bash">dart .\setup.dart android
</code></pre> </li> 
    </ol> </li> 
   <li> <p>windows</p> 
    <ol> 
     <li> <p>You need a windows client</p> </li> 
     <li> <p>Install <code>Gcc</code>ï¼Œ<code>Inno Setup</code></p> </li> 
     <li> <p>Run build script</p> <pre><code class="language-bash">dart .\setup.dart	
</code></pre> </li> 
    </ol> </li> 
   <li> <p>linux</p> 
    <ol> 
     <li> <p>You need a linux client</p> </li> 
     <li> <p>Run build script</p> <pre><code class="language-bash">dart .\setup.dart	
</code></pre> </li> 
    </ol> </li> 
   <li> <p>macOS</p> 
    <ol> 
     <li> <p>You need a macOS client</p> </li> 
     <li> <p>Run build script</p> <pre><code class="language-bash">dart .\setup.dart	
</code></pre> </li> 
    </ol> </li> 
  </ul> </li> 
</ol> 
<h2>Star</h2> 
<p>The easiest way to support developers is to click on the star (â­) at the top of the page.</p> 
<p style="text-align: center;"> <a href="https://api.star-history.com/svg?repos=chen08209/FlClash&amp;Date"> <img alt="start" src="https://api.star-history.com/svg?repos=chen08209/FlClash&amp;Date" width="50%" /> </a> </p>
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>n8n-io/n8n</title>
<link>https://github.com/n8n-io/n8n</link>
<guid>https://github.com/n8n-io/n8n</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šn8nã€è‡ªåŠ¨åŒ–å·¥å…·ã€æºä»£ç ã€å¯æ‰©å±•æ€§ã€é›†æˆ

æ€»ç»“ï¼š

n8næ˜¯ä¸€æ¬¾å¼€æºçš„è‡ªåŠ¨åŒ–å·¥ä½œæµå·¥å…·ï¼Œå…·æœ‰é«˜åº¦å¯æ‰©å±•æ€§å’Œè‡ªå®šä¹‰åŠŸèƒ½ã€‚å®ƒé‡‡ç”¨èŠ‚ç‚¹å¼æ¶æ„ï¼Œä½¿å¾—ç”¨æˆ·èƒ½å¤Ÿè½»æ¾è¿æ¥å„ç§æœåŠ¡å’Œåº”ç”¨ï¼Œå®ç°è·¨å¹³å°çš„ä»»åŠ¡è‡ªåŠ¨åŒ–ã€‚n8næä¾›äº†è¶…è¿‡200ç§ä¸åŒçš„èŠ‚ç‚¹ï¼Œè¦†ç›–äº†å¹¿æ³›çš„è‡ªåŠ¨åŒ–éœ€æ±‚ã€‚å®˜æ–¹æ–‡æ¡£æä¾›äº†è¯¦ç»†çš„ä½¿ç”¨æŒ‡å—ã€ç¤ºä¾‹å·¥ä½œæµä»¥åŠç‰ˆæœ¬æ›´æ–°ä¿¡æ¯ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡npmå‘½ä»¤è¡Œå·¥å…·æˆ–åœ¨çº¿äº‘æœåŠ¡è¿›è¡Œå¿«é€Ÿå¯åŠ¨å’Œæ“ä½œã€‚æ­¤å¤–ï¼Œn8nè¿˜æ”¯æŒä¸LangChainé›†æˆï¼Œå¼•å…¥AIåŠŸèƒ½åˆ°å·¥ä½œæµä¸­ï¼Œå¢å¼ºè‡ªåŠ¨åŒ–èƒ½åŠ›ã€‚å¯¹äºé‡åˆ°é—®é¢˜çš„ç”¨æˆ·ï¼Œn8nç¤¾åŒºæä¾›è®ºå›æ”¯æŒï¼Œå¸®åŠ©è§£å†³æŠ€æœ¯éš¾é¢˜ã€‚æœ€åï¼Œn8néµå¾ªApache 2.0è®¸å¯åè®®å‘å¸ƒï¼ŒåŒæ—¶ä¸ºå•†ä¸šå®¢æˆ·æä¾›ä¼ä¸šçº§è®¸å¯é€‰é¡¹ï¼Œç¡®ä¿äº†å…¶çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚ <div>
<p>Free and source-available fair-code licensed workflow automation tool. Easily automate tasks across different services.</p><hr /><p><img alt="n8n.io - Workflow Automation" src="https://user-images.githubusercontent.com/65276001/173571060-9f2f6d7b-bac0-43b6-bdb2-001da9694058.png" /></p> 
<h1>n8n - Workflow automation tool</h1> 
<p>n8n is an extendable workflow automation tool. With a <a href="https://faircode.io">fair-code</a> distribution model, n8n will always have visible source code, be available to self-host, and allow you to add your own custom functions, logic and apps. n8n's node-based approach makes it highly versatile, enabling you to connect anything to everything.</p> 
<p><img alt="n8n.io - Screenshot" src="https://raw.githubusercontent.com/n8n-io/n8n/master/assets/n8n-screenshot.png" /></p> 
<h2>Demo</h2> 
<p><a href="https://www.youtube.com/watch?v=1MwSoB0gnM4"><span>ğŸ“º</span> A short video (&lt; 5 min)</a> that goes over key concepts of creating workflows in n8n.</p> 
<h2>Available integrations</h2> 
<p>n8n has 200+ different nodes to automate workflows. The list can be found on: <a href="https://n8n.io/integrations">https://n8n.io/integrations</a></p> 
<h2>Documentation</h2> 
<p>The official n8n documentation can be found on our <a href="https://docs.n8n.io">documentation website</a></p> 
<p>Additional information and example workflows on the <a href="https://n8n.io">n8n.io website</a></p> 
<p>The release notes can be found <a href="https://docs.n8n.io/release-notes/">here</a> and the list of breaking changes <a href="https://github.com/n8n-io/n8n/raw/master/packages/cli/BREAKING-CHANGES.md">here</a>.</p> 
<h2>Usage</h2> 
<ul> 
 <li><span>ğŸ“š</span> Learn <a href="https://docs.n8n.io/reference/cli-commands/">how to <strong>use</strong> it from the command line</a></li> 
 <li><span>ğŸ³</span> Learn <a href="https://docs.n8n.io/hosting/installation/docker/">how to run n8n in <strong>Docker</strong></a></li> 
</ul> 
<h2>Start</h2> 
<p>You can try n8n without installing it using npx. You must have <a href="https://nodejs.org/en/">Node.js</a> installed. From the terminal, run:</p> 
<p><code>npx n8n</code></p> 
<p>This command will download everything that is needed to start n8n. You can then access n8n and start building workflows by opening <a href="http://localhost:5678">http://localhost:5678</a>.</p> 
<h2>n8n cloud</h2> 
<p>Sign-up for an <a href="https://www.n8n.io/cloud/">n8n cloud</a> account.</p> 
<p>While n8n cloud and n8n are the same in terms of features, n8n cloud provides certain conveniences such as:</p> 
<ul> 
 <li>Not having to set up and maintain your n8n instance</li> 
 <li>Managed OAuth for authentication</li> 
 <li>Easily upgrading to the newer n8n versions</li> 
</ul> 
<h2>Build with LangChain and AI in n8n (beta)</h2> 
<p>With n8n's LangChain nodes you can build AI-powered functionality within your workflows. The LangChain nodes are configurable, meaning you can choose your preferred agent, LLM, memory, and so on. Alongside the LangChain nodes, you can connect any n8n node as normal: this means you can integrate your LangChain logic with other data sources and services.</p> 
<p>Learn more in the <a href="https://docs.n8n.io/langchain/">documentation</a>.</p> 
<ul> 
 <li><a href="https://www.npmjs.com/package/@n8n/n8n-nodes-langchain">LangChain nodes package</a></li> 
 <li><a href="https://www.npmjs.com/package/@n8n/chat">Chatbot package</a></li> 
</ul> 
<h2>Support</h2> 
<p>If you have problems or questions go to our forum, we will then try to help you asap:</p> 
<p><a href="https://community.n8n.io">https://community.n8n.io</a></p> 
<h2>Jobs</h2> 
<p>If you are interested in working for n8n and so shape the future of the project check out our <a href="https://apply.workable.com/n8n/">job posts</a></p> 
<h2>What does n8n mean and how do you pronounce it?</h2> 
<p><strong>Short answer:</strong> It means "nodemation" and it is pronounced as n-eight-n.</p> 
<p><strong>Long answer:</strong> "I get that question quite often (more often than I expected) so I decided it is probably best to answer it here. While looking for a good name for the project with a free domain I realized very quickly that all the good ones I could think of were already taken. So, in the end, I chose nodemation. 'node-' in the sense that it uses a Node-View and that it uses Node.js and '-mation' for 'automation' which is what the project is supposed to help with. However, I did not like how long the name was and I could not imagine writing something that long every time in the CLI. That is when I then ended up on 'n8n'." - <strong>Jan Oberhauser, Founder and CEO, n8n.io</strong></p> 
<h2>Development setup</h2> 
<p>Have you found a bug <span>ğŸ›</span> ? Or maybe you have a nice feature <span>âœ¨</span> to contribute ? The <a href="https://github.com/n8n-io/n8n/raw/master/CONTRIBUTING.md">CONTRIBUTING guide</a> will help you get your development environment ready in minutes.</p> 
<h2>License</h2> 
<p>n8n is <a href="https://faircode.io">fair-code</a> distributed under the <a href="https://github.com/n8n-io/n8n/raw/master/LICENSE.md"><strong>Sustainable Use License</strong></a> and the <a href="https://github.com/n8n-io/n8n/raw/master/LICENSE_EE.md"><strong>n8n Enterprise License</strong></a>.</p> 
<p>Proprietary licenses are available for enterprise customers. <a href="mailto:license@n8n.io">Get in touch</a></p> 
<p>Additional information about the license model can be found in the <a href="https://docs.n8n.io/reference/license/">docs</a>.</p>
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>electric-sql/pglite</title>
<link>https://github.com/electric-sql/pglite</link>
<guid>https://github.com/electric-sql/pglite</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šPGliteã€Postgresã€WASMã€TypeScriptã€æµè§ˆå™¨/Node.js/Bun/Deno

æ€»ç»“:

PGlite æ˜¯ä¸€ä¸ªè½»é‡çº§çš„ PostgreSQL å®ç°ï¼Œå®ƒå°†æ•°æ®åº“å¼•æ“æ‰“åŒ…ä¸º WebAssembly (WASM) æ ¼å¼ï¼Œå¹¶æä¾›äº† TypeScript å®¢æˆ·ç«¯åº“ã€‚è¿™ç§è®¾è®¡å…è®¸å¼€å‘è€…åœ¨æµè§ˆå™¨ã€Node.jsã€Bun æˆ– Deno ç¯å¢ƒä¸­ç›´æ¥è¿è¡Œ PostgreSQLï¼Œæ— éœ€å®‰è£…é¢å¤–ä¾èµ–ã€‚PGlite çš„ä½“ç§¯ä»…ä¸º 3MBï¼ˆå‹ç¼©åï¼‰ï¼Œå¹¶ä¸”æ”¯æŒå¤šç§ PostgreSQL æ‰©å±•ï¼ŒåŒ…æ‹¬å…¨æ–‡æœç´¢ç­‰ã€‚

åœ¨æµè§ˆå™¨ç¯å¢ƒä¸­ï¼Œå¼€å‘è€…å¯ä»¥é€šè¿‡å¸¸è§„çš„åŒ…ç®¡ç†å™¨æˆ– CDN æ¥å¼•å…¥å’Œä½¿ç”¨ PGliteã€‚å¯¹äºå†…å­˜æ•°æ®åº“åœºæ™¯ï¼Œå¼€å‘è€…å¯ä»¥ç›´æ¥åˆ›å»ºæ–°çš„ PGlite å®ä¾‹ï¼›è‹¥éœ€æŒä¹…åŒ–å­˜å‚¨ï¼Œå¯ä»¥å°†æ•°æ®åº“ä¸æ–‡ä»¶ç³»ç»Ÿï¼ˆNode/Bunï¼‰æˆ– IndexedDBï¼ˆæµè§ˆå™¨ï¼‰è¿›è¡Œé›†æˆã€‚

åœ¨ Node.js æˆ– Bun ç¯å¢ƒä¸‹ï¼ŒPGlite å¯ä»¥é€šè¿‡ npm è¿›è¡Œå®‰è£…ï¼Œå¹¶æ ¹æ®å…·ä½“éœ€æ±‚é…ç½®å†…å­˜æ•°æ®åº“æˆ–æŒä¹…åŒ–æ•°æ®åˆ°æœ¬åœ°æ–‡ä»¶æˆ– IndexedDB ä¸­ã€‚

PGlite çš„å·¥ä½œåŸç†åŸºäº PostgreSQL çš„å•ç”¨æˆ·æ¨¡å¼ï¼Œè¯¥æ¨¡å¼ä¸»è¦ç”¨äºå‘½ä»¤è¡Œæ“ä½œå’Œå¯åŠ¨æ¢å¤æµç¨‹ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªè¾“å…¥/è¾“å‡ºé€šé“ï¼ŒPGlite èƒ½å¤Ÿåœ¨ JavaScript ç¯å¢ƒä¸­ä¸è¢«ç¼–è¯‘ä¸º WASM çš„ PostgreSQL è¿›è¡Œäº¤äº’ã€‚

å½“å‰ç‰ˆæœ¬çš„é™åˆ¶åœ¨äºå®ƒä»…æ”¯æŒå•ç”¨æˆ·/å•è¿æ¥æ¨¡å¼ã€‚å¯¹äºè´¡çŒ®è€…è€Œè¨€ï¼Œéœ€è¦ç¡®ä¿å·²å®‰è£… Emscripten å’Œæœ€æ–°ç‰ˆæœ¬çš„ Node.jsï¼Œå¹¶ä¸”ä¸‹è½½æœ€æ–°çš„ WASM æ„å»ºæ–‡ä»¶ä»¥æ„å»º PGlite åŠå…¶ä¾èµ–çš„å·¥ä½œç©ºé—´é¡¹ç›®ã€‚æ­¤å¤–ï¼ŒPGlite éµå¾ªåŒè®¸å¯åè®®ï¼Œå¼€å‘è€…å¯ä»¥æ ¹æ®éœ€è¦é€‰æ‹© PostgreSQL è®¸å¯è¯æˆ–è‡ªç”±è½¯ä»¶è®¸å¯è¯ã€‚

æœ€åï¼ŒPGlite å»ºç«‹åœ¨å‰äººçš„å·¥ä½œä¹‹ä¸Šï¼Œç‰¹åˆ«æ„Ÿè°¢ä¸ºè¯¥é¡¹ç›®åšå‡ºè´¡çŒ®çš„äººä»¬ã€‚ <div>
<p>Lightweight Postgres packaged as WASM into a TypeScript library for the browser, Node.js, Bun and Deno from https://electric-sql.com</p><hr /><p align="center"> <a href="https://electric-sql.com" target="_blank"> 
   
   <source media="(prefers-color-scheme: dark)" /> 
   <source media="(prefers-color-scheme: light)" /> 
   <img alt="ElectricSQL logo" src="https://raw.githubusercontent.com/electric-sql/meta/main/identity/ElectricSQL-logo-black.svg?sanitize=true" /> 
   </a> </p> 
<p align="center"> PGlite - the WASM build of Postgres from <a href="https://electric-sql.com" target="_blank">ElectricSQL</a>.<br /> Build reactive, realtime, local-first apps directly on Postgres. </p>
<p> </p>
<p align="center"> <a href="https://github.com/electric-sql/pglite/stargazers/"><img src="https://img.shields.io/github/stars/electric-sql/pglite?style=social&amp;label=Star" /></a> 
 <!-- <a href="https://github.com/electric-sql/pglite/actions"><img src="https://github.com/electric-sql/pglite/workflows/CI/badge.svg" alt="CI"></a> --> <a href="https://github.com/electric-sql/pglite/raw/main/LICENSE"><img alt="License - Apache 2.0" src="https://img.shields.io/badge/license-Apache_2.0-green" /></a> <a href="https://raw.githubusercontent.com/electric-sql/pglite/main/#roadmap"><img alt="Status - Alpha" src="https://img.shields.io/badge/status-alpha-orange" /></a> <a href="https://discord.electric-sql.com"><img alt="Chat - Discord" src="https://img.shields.io/discord/933657521581858818?color=5969EA&amp;label=discord" /></a> <a href="https://twitter.com/ElectricSQL" target="_blank"><img src="https://img.shields.io/twitter/follow/nestframework.svg?style=social&amp;label=Follow%20@ElectricSQL" /></a> <a href="https://fosstodon.org/@electric" target="_blank"><img src="https://img.shields.io/mastodon/follow/109599644322136925.svg?domain=https%3A%2F%2Ffosstodon.org" /></a> </p> 
<h1>PGlite - Postgres in WASM</h1> 
<p><img alt="PGlite" src="https://raw.githubusercontent.com/electric-sql/pglite/main/screenshot.png" /></p> 
<p>PGlite is a WASM Postgres build packaged into a TypeScript client library that enables you to run Postgres in the browser, Node.js and Bun, with no need to install any other dependencies. It is only 3mb gzipped and has support for many Postgres extensions, including <a href="https://github.com/pgvector/pgvector">pgvector</a>.</p> 
<pre><code class="language-javascript">import { PGlite } from "@electric-sql/pglite";

const db = new PGlite();
await db.query("select 'Hello world' as message;");
// -&gt; { rows: [ { message: "Hello world" } ] }
</code></pre> 
<p>It can be used as an ephemeral in-memory database, or with persistence either to the file system (Node/Bun) or indexedDB (Browser).</p> 
<p>Unlike previous "Postgres in the browser" projects, PGlite does not use a Linux virtual machine - it is simply Postgres in WASM.</p> 
<p>For full documentation and user guides see <a href="https://pglite.dev">pglite.dev</a>.</p> 
<h2>Browser</h2> 
<p>It can be installed and imported using your usual package manager:</p> 
<pre><code class="language-js">import { PGlite } from "@electric-sql/pglite";
</code></pre> 
<p>or using a CDN such as JSDeliver:</p> 
<pre><code class="language-js">import { PGlite } from "https://cdn.jsdelivr.net/npm/@electric-sql/pglite/dist/index.js";
</code></pre> 
<p>Then for an in-memory Postgres:</p> 
<pre><code class="language-js">const db = new PGlite()
await db.query("select 'Hello world' as message;")
// -&gt; { rows: [ { message: "Hello world" } ] }
</code></pre> 
<p>or to persist the database to indexedDB:</p> 
<pre><code class="language-js">const db = new PGlite("idb://my-pgdata");
</code></pre> 
<h2>Node/Bun</h2> 
<p>Install into your project:</p> 
<pre><code class="language-bash">npm install @electric-sql/pglite
</code></pre> 
<p>To use the in-memory Postgres:</p> 
<pre><code class="language-javascript">import { PGlite } from "@electric-sql/pglite";

const db = new PGlite();
await db.query("select 'Hello world' as message;");
// -&gt; { rows: [ { message: "Hello world" } ] }
</code></pre> 
<p>or to persist to the filesystem:</p> 
<pre><code class="language-javascript">const db = new PGlite("./path/to/pgdata");
</code></pre> 
<h2>How it works</h2> 
<p>PostgreSQL typically operates using a process forking model; whenever a client initiates a connection, a new process is forked to manage that connection. However, programs compiled with Emscripten - a C to WebAssembly (WASM) compiler - cannot fork new processes, and operates strictly in a single-process mode. As a result, PostgreSQL cannot be directly compiled to WASM for conventional operation.</p> 
<p>Fortunately, PostgreSQL includes a "single user mode" primarily intended for command-line usage during bootstrapping and recovery procedures. Building upon this capability, PGlite introduces a input/output pathway that facilitates interaction with PostgreSQL when it is compiled to WASM within a JavaScript environment.</p> 
<h2>Limitations</h2> 
<ul> 
 <li>PGlite is single user/connection.</li> 
</ul> 
<h2>How to contribute</h2> 
<p>You will need <a href="https://pnpm.io/">pnpm</a> installed, and a recent version of Node.js (v20 and above).</p> 
<p>You will also need the Postgres WASM build files, which you download from a comment under the most recently merged PR, labeled as <em>interim build files</em>, and place them under <code>packages/pglite/release</code>. These are necessary to build PGlite and the dependent workspace projects. We plan to enable a local build in the future to streamline this step.</p> 
<p>Once the requirements are met, you can install dependencies and build the workspace projects:</p> 
<pre><code class="language-bash">pnpm install
pnpm build
</code></pre> 
<p>This will build all packages in the correct order based on their dependency relationships. You can now develop any individual package using the <code>build</code> and <code>test</code> scripts, as well as the <code>stylecheck</code> and <code>typecheck</code> scripts to ensure style and type validity.</p> 
<p>When ready to open a PR, run the following command at the root of the repository:</p> 
<pre><code class="language-bash">pnpm changeset
</code></pre> 
<p>And follow the instructions to create an appropriate changeset. Please ensure any contributions that touch code are accompanied by a changeset.</p> 
<h2>Acknowledgments</h2> 
<p>PGlite builds on the work of <a href="https://github.com/kelvich">Stas Kelvich</a> of <a href="https://neon.tech">Neon</a> in this <a href="https://github.com/electric-sql/postgres-wasm">Postgres fork</a>.</p> 
<h2>License</h2> 
<p>PGlite is dual-licensed under the terms of the&nbsp;<a href="https://github.com/electric-sql/pglite/raw/main/LICENSE">Apache License 2.0</a>&nbsp;and the&nbsp;<a href="https://github.com/electric-sql/pglite/raw/main/POSTGRES-LICENSE">PostgreSQL License</a>, you can choose which you prefer.</p> 
<p>Changes to the&nbsp;<a href="https://github.com/electric-sql/postgres-wasm">Postgres source</a>&nbsp;are licensed under the PostgreSQL License.</p>
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>poloclub/transformer-explainer</title>
<link>https://github.com/poloclub/transformer-explainer</link>
<guid>https://github.com/poloclub/transformer-explainer</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šTransformer Explainerã€GPTã€Transformeræ¨¡å‹ã€äº¤äº’å¯è§†åŒ–ã€åœ¨çº¿å®éªŒ

æ€»ç»“:
Transformer Explaineræ˜¯ä¸€ä¸ªäº¤äº’å¼å¯è§†åŒ–å·¥å…·ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·ç†è§£Transformeræ¨¡å‹ï¼ˆå¦‚GPTï¼‰çš„å·¥ä½œåŸç†ã€‚é€šè¿‡åœ¨çº¿è¿è¡ŒGPT-2æ¨¡å‹ï¼Œç”¨æˆ·å¯ä»¥å®æ—¶è¾“å…¥æ–‡æœ¬å¹¶è§‚å¯Ÿå†…éƒ¨ç»„ä»¶å’Œæ“ä½œå¦‚ä½•ååŒå·¥ä½œä»¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯æ±‡ã€‚è¯¥å·¥å…·æä¾›äº†ç›´è§‚çš„å­¦ä¹ ä½“éªŒï¼Œä½¿ä»»ä½•äººéƒ½èƒ½æ¢ç´¢å’Œå­¦ä¹ æ–‡æœ¬ç”Ÿæˆæ¨¡å‹çš„å·¥ä½œæœºåˆ¶ã€‚è¦æœ¬åœ°è¿è¡Œæ­¤å·¥å…·ï¼Œéœ€è¦å®‰è£…Node.js 20æˆ–æ›´é«˜ç‰ˆæœ¬ä»¥åŠNPMã€‚è¿è¡Œå‘½ä»¤åï¼Œæ‚¨å¯ä»¥é€šè¿‡æµè§ˆå™¨è®¿é—®ç‰¹å®šç½‘å€æ¥ä½¿ç”¨å®ƒã€‚

Transformer Explainerç”±æ¥è‡ªä¹”æ²»äºšç†å·¥å­¦é™¢çš„ç ”ç©¶äººå‘˜Aeree Choã€Grace C. Kimã€Alexander Karpekovã€Alec Helblingã€Zijie J. Wangã€Seongmin Leeã€Benjamin Hooverå’ŒDuen Horng Chauå…±åŒå¼€å‘ã€‚è¯¥å·¥å…·éµå¾ªå¼€æºè®¸å¯è¯ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡æä¾›çš„é—®é¢˜åé¦ˆæˆ–ç›´æ¥è”ç³»å¼€å‘è€…è¿›è¡Œäº¤æµã€‚ <div>
<p>Transformer Explained: Learn How LLM Transformer Models Work with Interactive Visualization</p><hr /><h1>Transformer Explainer: Interactive Learning of Text-Generative Models</h1> 
<p>Transformer Explainer is an interactive visualization tool designed to help anyone learn how Transformer-based models like GPT work. It runs a live GPT-2 model right in your browser, allowing you to experiment with your own text and observe in real time how internal components and operations of the Transformer work together to predict the next tokens. Try Transformer Explainer at <a href="http://poloclub.github.io/transformer-explainer">http://poloclub.github.io/transformer-explainer</a> and watch a demo video on YouTube <a href="https://youtu.be/ECR4oAwocjs">https://youtu.be/ECR4oAwocjs</a> .<br /><br /> <a href="http://opensource.org/licenses/MIT"><img alt="MIT license" src="http://img.shields.io/badge/license-MIT-brightgreen.svg?sanitize=true" /></a> <a href="https://arxiv.org/abs/2408.04619"><img alt="arxiv badge" src="https://img.shields.io/badge/arXiv-2408.04619-red" /></a></p> 
<table> 
 <tbody>
  <tr> 
   <td colspan="2">
    <video src="https://github.com/poloclub/transformer-explainer/assets/5067740/5c2d6a9d-2cbf-4b01-9ce1-bdf8e190dc42" width="100%"></video></td> 
  </tr> 
  <tr> 
   <td>ğŸš€ <a href="http://poloclub.github.io/transformer-explainer">Live Demo</a></td> 
   <td>ğŸ“º <a href="https://youtu.be/ECR4oAwocjs">Demo Video</a></td> 
  </tr> 
 </tbody>
</table> 
<h3>Research Paper</h3> 
<p><a href="https://arxiv.org/abs/2408.04619"><strong>Transformer Explainer: Interactive Learning of Text-Generative Models</strong></a>. Aeree Cho, Grace C. Kim, Alexander Karpekov, Alec Helbling, Zijie J. Wang, Seongmin Lee, Benjamin Hoover, Duen Horng Chau. <em>Poster, IEEE VIS 2024.</em></p> 
<h2>How to run locally</h2> 
<h4>Prerequisites</h4> 
<ul> 
 <li>Node.js 20 or higher</li> 
 <li>NPM</li> 
</ul> 
<h4>Steps</h4> 
<pre><code class="language-bash">git clone https://github.com/poloclub/transformer-explainer.git
cd transformer-explainer
npm install
npm run dev
</code></pre> 
<p>Then, on your web browser, access <a href="http://localhost:5173">http://localhost:5173</a>.</p> 
<h2>Credits</h2> 
<p>Transformer Explainer was created by <a href="https://aereeeee.github.io/" target="_blank">Aeree Cho</a>, <a href="https://www.linkedin.com/in/chaeyeonggracekim/" target="_blank">Grace C. Kim</a>, <a href="https://alexkarpekov.com/" target="_blank">Alexander Karpekov</a>, <a href="https://alechelbling.com/" target="_blank">Alec Helbling</a>, <a href="https://zijie.wang/" target="_blank">Jay Wang</a>, <a href="https://seongmin.xyz/" target="_blank">Seongmin Lee</a>, <a href="https://bhoov.com/" target="_blank">Benjamin Hoover</a>, and <a href="https://poloclub.github.io/polochau/" target="_blank">Polo Chau</a> at the Georgia Institute of Technology.</p> 
<h2>Citation</h2> 
<pre><code class="language-bibTeX">@article{cho2024transformer,
  title = {Transformer Explainer: Interactive Learning of Text-Generative Models},
  shorttitle = {Transformer Explainer},
  author = {Cho, Aeree and Kim, Grace C. and Karpekov, Alexander and Helbling, Alec and Wang, Zijie J. and Lee, Seongmin and Hoover, Benjamin and Chau, Duen Horng},
  journal={IEEE VIS},
  year={2024}
}
</code></pre> 
<h2>License</h2> 
<p>The software is available under the <a href="https://github.com/poloclub/transformer-explainer/raw/main/LICENSE">MIT License</a>.</p> 
<h2>Contact</h2> 
<p>If you have any questions, feel free to <a href="https://github.com/poloclub/transformer-explainer/issues/new/choose">open an issue</a> or contact <a href="https://aereeeee.github.io/">Aeree Cho</a> or any of the contributors listed above.</p>
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>erincatto/box2d</title>
<link>https://github.com/erincatto/box2d</link>
<guid>https://github.com/erincatto/box2d</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šBox2Dã€2D ç‰©ç†å¼•æ“ã€æ¸¸æˆã€æ„å»ºçŠ¶æ€ã€å…¼å®¹æ€§

æ€»ç»“:

Box2D æ˜¯ä¸€æ¬¾ä¸“é—¨ç”¨äºæ¸¸æˆå¼€å‘çš„äºŒç»´ç‰©ç†å¼•æ“ã€‚å®ƒæä¾›äº†ä¸°å¯Œçš„åŠŸèƒ½å’Œç‰¹æ€§ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºç¢°æ’æ£€æµ‹ã€ç‰©ç†æ¨¡æ‹Ÿã€ç³»ç»Ÿè®¾è®¡ç­‰ã€‚ä»¥ä¸‹æ˜¯å¯¹å…¶ä¸»è¦ç‰¹ç‚¹çš„æ€»ç»“ï¼š

1. **ç¢°æ’æ£€æµ‹**ï¼šBox2D æ”¯æŒå¤šç§å½¢çŠ¶çš„ç¢°æ’æ£€æµ‹ï¼Œå¦‚å‡¸å¤šè¾¹å½¢ã€èƒ¶å›Šä½“ã€åœ†å½¢ã€åœ†è§’å¤šè¾¹å½¢ã€çº¿æ®µå’Œé“¾ç­‰ã€‚åŒæ—¶ï¼Œå®ƒè¿˜æ”¯æŒä¼ æ„Ÿå™¨ã€æ¥è§¦äº‹ä»¶ä»¥åŠå¤šå½¢çŠ¶çš„å•ä¸ªç‰©ä½“ã€‚

2. **ç‰©ç†æ¨¡æ‹Ÿ**ï¼šå¼•æ“é‡‡ç”¨ç¨³å¥çš„è½¯æ­¥è§£ç®—å™¨è¿›è¡Œå¿«é€Ÿçš„å¹³ç§»å’Œæ—‹è½¬æ¨¡æ‹Ÿï¼Œå®ç°è¿ç»­ç‰©ç†ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åŒ…æ‹¬å²›åŸºç¡çœ ã€å…³èŠ‚ï¼ˆå¦‚è½®è½´ã€æ»‘åŠ¨ã€è·ç¦»ã€é¼ æ ‡ç­‰ï¼‰ä»¥åŠå„ç§ç±»å‹çš„å…³èŠ‚ï¼Œå¦‚é“°é“¾ã€ç”µæœºã€å¼¹ç°§å’Œæ‘©æ“¦åŠ›ã€‚

3. **ç³»ç»Ÿè®¾è®¡**ï¼šBox2D çš„è®¾è®¡æ³¨é‡æ•°æ®å¯¼å‘ï¼Œä½¿ç”¨æ ‡å‡†å¯ç§»æ¤çš„ C17 ç¼–å†™ï¼Œæ”¯æŒå¤šçº¿ç¨‹å’Œ SIMD æŒ‡ä»¤é›†ï¼Œæé«˜äº†æ€§èƒ½å’Œæ•ˆç‡ã€‚

4. **æ„å»ºä¸å…¼å®¹æ€§**ï¼šBox2D å¯ä»¥åœ¨ Windowsã€Linux å’Œ macOS ä¸Šæ„å»ºå’Œè¿è¡Œï¼Œè¦æ±‚ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„ Clang æˆ– GCC è¿›è¡Œç¼–è¯‘ã€‚å¯¹äºç‰¹å®šå¹³å°å¦‚ Windowsï¼Œéœ€è¦å®‰è£… Visual Studioã€‚è€Œå¯¹äº Mac å’Œ Linuxï¼Œåˆ™å¯ä»¥é€šè¿‡ CMake è¿›è¡Œæ„å»ºã€‚

5. **ç¤¾åŒºä¸è´¡çŒ®**ï¼šBox2D æœ‰ä¸€ä¸ªæ´»è·ƒçš„ç¤¾åŒºï¼Œé¼“åŠ±ç”¨æˆ·é€šè¿‡é—®é¢˜æŠ¥å‘Šæˆ– Discord æœåŠ¡å™¨å¯»æ±‚å¸®åŠ©å’Œæ”¯æŒã€‚å¼€å‘è€…å¯é€šè¿‡èµåŠ©æ¥æ”¯æŒ Box2D çš„å‘å±•ï¼ŒåŒæ—¶ä¹Ÿæä¾›äº†ä¸€äº›å®˜æ–¹å’Œç¬¬ä¸‰æ–¹çš„ç‰ˆæœ¬ã€å°è£…å’Œç»‘å®šèµ„æºã€‚ <div>
<p>Box2D is a 2D physics engine for games</p><hr /><p><img alt="Box2D Logo" src="https://box2d.org/images/logo.svg?sanitize=true" /></p> 
<h1>Build Status</h1> 
<p><a href="https://github.com/erincatto/box2d/actions"><img alt="Build Status" src="https://github.com/erincatto/box2d/actions/workflows/build.yml/badge.svg?sanitize=true" /></a></p> 
<h1>Box2D</h1> 
<p>Box2D is a 2D physics engine for games.</p> 
<p><a href="https://www.youtube.com/watch?v=dAoM-xjOWtA"><img alt="Box2D Version 3.0 Release Demo" src="https://img.youtube.com/vi/dAoM-xjOWtA/0.jpg" /></a></p> 
<h2>Features</h2> 
<h3>Collision</h3> 
<ul> 
 <li>Continuous collision detection</li> 
 <li>Contact events and sensors</li> 
 <li>Convex polygons, capsules, circles, rounded polygons, segments, and chains</li> 
 <li>Multiple shapes per body</li> 
 <li>Collision filtering</li> 
 <li>Ray casts, shape casts, and overlap queries</li> 
</ul> 
<h3>Physics</h3> 
<ul> 
 <li>Robust <em>Soft Step</em> rigid body solver</li> 
 <li>Continuous physics for fast translations and rotations</li> 
 <li>Island based sleep</li> 
 <li>Revolute, prismatic, distance, mouse joint, weld, and wheel joints</li> 
 <li>Joint limits, motors, springs, and friction</li> 
 <li>Joint and contact forces</li> 
 <li>Body movement events and sleep notification</li> 
</ul> 
<h3>System</h3> 
<ul> 
 <li>Data-oriented design</li> 
 <li>Written in portable C17</li> 
 <li>Extensive multithreading and SIMD</li> 
</ul> 
<h3>Samples</h3> 
<ul> 
 <li>OpenGL with GLFW and enkiTS</li> 
 <li>Graphical user interface with imgui</li> 
 <li>Many samples to demonstrate features and performance</li> 
</ul> 
<h2>Building</h2> 
<ul> 
 <li>Install <a href="https://cmake.org/">CMake</a></li> 
 <li>Ensure CMake is in the user <code>PATH</code></li> 
 <li>Visual Studio: run <code>build.bat</code> from the command prompt</li> 
 <li>Otherwise: run <code>build.sh</code> from a bash shell</li> 
 <li>Results are in the build sub-folder</li> 
 <li>On Windows you can open box2d.sln</li> 
</ul> 
<h2>Building for Xcode</h2> 
<ul> 
 <li>Install <a href="https://cmake.org">CMake</a></li> 
 <li>Add Cmake to the path in .zprofile (the default Terminal shell is zsh) 
  <ul> 
   <li>export PATH="/Applications/CMake.app/Contents/bin:$PATH"</li> 
  </ul> </li> 
 <li>mkdir build</li> 
 <li>cd build</li> 
 <li>cmake -G Xcode ..</li> 
 <li>open box2d.xcodeproj</li> 
 <li>Select the samples scheme</li> 
 <li>Edit the scheme to set a custom working directory to the box2d directory</li> 
 <li>You can now build and run the samples</li> 
</ul> 
<h2>Compatibility</h2> 
<p>The Box2D library and samples build and run on Windows, Linux, and Mac.</p> 
<p>Box2D should be built on recent versions of clang and gcc. You will need the latest Visual Studio version for C11 atomics to compile (17.8.3+).</p> 
<p>AVX2 CPU support is assumed on x64. You can turn this off in the CMake options and use SSE2 instead. There are some compatibility issues with very old CPUs.</p> 
<h2>Documentation</h2> 
<ul> 
 <li><a href="https://box2d.org/documentation/">Manual</a></li> 
 <li><a href="https://github.com/erincatto/box2d/raw/main/docs/migration.md">Migration Guide</a></li> 
</ul> 
<h2>Community</h2> 
<ul> 
 <li><a href="https://discord.gg/NKYgCBP">Discord</a></li> 
</ul> 
<h2>Contributing</h2> 
<p>Please do not submit pull requests. Instead, please file an issue for bugs or feature requests. For support, please visit the Discord server.</p> 
<h1>Giving Feedback</h1> 
<p>Please file an issue or start a chat on discord.</p> 
<h2>License</h2> 
<p>Box2D is developed by Erin Catto and uses the <a href="https://en.wikipedia.org/wiki/MIT_License">MIT license</a>.</p> 
<h2>Sponsorship</h2> 
<p>Support development of Box2D through <a href="https://github.com/sponsors/erincatto">Github Sponsors</a></p> 
<h2>Ports, wrappers, and bindings</h2> 
<ul> 
 <li><a href="https://github.com/EnokViking/Box2DBeef">https://github.com/EnokViking/Box2DBeef</a></li> 
 <li><a href="https://github.com/HolyBlackCat/box2cpp">https://github.com/HolyBlackCat/box2cpp</a></li> 
</ul>
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>LLaVA-VL/LLaVA-NeXT</title>
<link>https://github.com/LLaVA-VL/LLaVA-NeXT</link>
<guid>https://github.com/LLaVA-VL/LLaVA-NeXT</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šLLaVA-NeXTã€å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ã€å¤šå›¾åƒã€è§†é¢‘ã€3Dä»»åŠ¡

æ€»ç»“:

LLaVA-NeXTç³»åˆ—æ¨¡å‹æ˜¯ä¸ºè§£å†³å¤šå›¾åƒã€è§†é¢‘å’Œä¸‰ç»´ä»»åŠ¡è€Œè®¾è®¡çš„ä¸€ç³»åˆ—å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ã€‚è‡ªå‘å¸ƒä»¥æ¥ï¼ŒLLaVA-NeXTä¸æ–­æ›´æ–°ä¸ä¼˜åŒ–ï¼Œå®ç°äº†åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„æ–°çŠ¶æ€æœ€ä¼˜æ€§èƒ½ã€‚ä»¥ä¸‹ä¸ºå…³é”®æ›´æ–°ç‚¹ï¼š

1. **å¤šæ¨¡æ€èƒ½åŠ›å¢å¼º**ï¼šLLaVA-NeXTæ¨¡å‹é€šè¿‡é›†æˆå›¾åƒã€æ–‡æœ¬å’Œè§†é¢‘ç†è§£èƒ½åŠ›ï¼Œæ˜¾è‘—æé«˜äº†å¤„ç†å¤šæ¨¡æ€æ•°æ®çš„ä»»åŠ¡æ€§èƒ½ã€‚

2. **æ€§èƒ½æå‡**ï¼šæ–°å‘å¸ƒçš„æ¨¡å‹å¦‚LLaVA-NeXT-Interleaveå’ŒLLaVA-NeXT-Videoï¼Œåˆ†åˆ«é’ˆå¯¹å¤šå›¾åƒä»»åŠ¡å’Œè§†é¢‘ç†è§£ä»»åŠ¡è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå®ç°äº†åœ¨ç›¸å…³é¢†åŸŸçš„æ–°é«˜ç‚¹ã€‚

3. **è®­ç»ƒç­–ç•¥æ”¹è¿›**ï¼šé€šè¿‡ä½¿ç”¨é«˜è´¨é‡çš„æ•°æ®é›†å’Œå¯è®­ç»ƒæ¨¡å—ï¼ŒLLaVA-NeXTç³»åˆ—æ¨¡å‹åœ¨è®­ç»ƒç­–ç•¥ä¸Šè¿›è¡Œäº†åˆ›æ–°ï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œä»»åŠ¡è§£å†³æ•ˆç‡ã€‚

4. **æ”¯æŒå¤šæ ·åŒ–ç¡¬ä»¶**ï¼šé¡¹ç›®æä¾›é’ˆå¯¹ä¸åŒç¡¬ä»¶å¹³å°ï¼ˆå¦‚Intel GPUå’ŒCPUï¼‰çš„æ”¯æŒï¼Œä½¿å¾—æ¨¡å‹éƒ¨ç½²æ›´åŠ çµæ´»ã€‚

5. **å¼€æºèµ„æºä¸ç¤¾åŒºè´¡çŒ®**ï¼šé™¤äº†æ¨¡å‹æœ¬èº«ï¼Œè¿˜æä¾›äº†åŒ…æ‹¬è¯„ä¼°æ¡†æ¶ã€æ•™ç¨‹æ–‡æ¡£ç­‰åœ¨å†…çš„ä¸°å¯Œèµ„æºï¼Œé¼“åŠ±ç¤¾åŒºæˆå‘˜å‚ä¸æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–ï¼Œå…±åŒæ¨åŠ¨å¤šæ¨¡æ€AIæŠ€æœ¯çš„å‘å±•ã€‚

è¿™äº›æ›´æ–°ä¸ä»…å±•ç¤ºäº†LLaVA-NeXTåœ¨å¤šæ¨¡æ€å¤„ç†èƒ½åŠ›ä¸Šçš„å¼ºå¤§ï¼Œä¹Ÿä¸ºæœªæ¥AIç ”ç©¶è€…å’Œå¼€å‘è€…æä¾›äº†å®è´µçš„å·¥å…·å’Œèµ„æºã€‚ <div>
<p></p><hr /><p align="center" width="100%"> <img height="80%" src="https://i.postimg.cc/pL17YtG4/WX20240508-220230-2x.png" width="80%" /> </p> 
<h1>LLaVA-NeXT: Open Large Multimodal Models</h1> 
<p><a href="https://arxiv.org/abs/2408.03326"><img alt="Static Badge" src="https://img.shields.io/badge/llava_onevision-paper-green" /></a> <a href="https://llava-vl.github.io/blog/"><img alt="llava_next-blog" src="https://img.shields.io/badge/llava_next-blog-green" /></a></p> 
<p><a href="https://llava-onevision.lmms-lab.com/"><img alt="llava_onevision-demo" src="https://img.shields.io/badge/llava_onevision-demo-red" /></a> <a href="https://huggingface.co/spaces/lmms-lab/LLaVA-NeXT-Interleave-Demo"><img alt="llava_next-interleave_demo" src="https://img.shields.io/badge/llava_next-interleave_demo-red" /></a> <a href="https://huggingface.co/spaces/WildVision/vision-arena"><img alt="llava_next-video_demo" src="https://img.shields.io/badge/llava_next-video_demo-red" /></a></p> 
<p><a href="https://huggingface.co/collections/lmms-lab/llava-onevision-66a259c3526e15166d6bba37"><img alt="llava_onevision-checkpoints" src="https://img.shields.io/badge/llava_onevision-checkpoints-blue" /></a> <a href="https://huggingface.co/collections/lmms-lab/llava-next-interleave-66763c55c411b340b35873d1"><img alt="llava_next-interleave_checkpoints" src="https://img.shields.io/badge/llava_next-interleave_checkpoints-blue" /></a> <a href="https://huggingface.co/collections/lmms-lab/llava-next-video-661e86f5e8dabc3ff793c944"><img alt="llava_next-video_checkpoints" src="https://img.shields.io/badge/llava_next-video_checkpoints-blue" /></a> <a href="https://huggingface.co/lmms-lab"><img alt="llava_next-image_checkpoints" src="https://img.shields.io/badge/llava_next-image_checkpoints-blue" /></a></p> 
<h2>Release Notes</h2> 
<ul> 
 <li> <p>[2024/08/06] ğŸ”¥ <strong>ğŸš€ <a href="https://llava-vl.github.io/blog/2024-08-05-llava-onevision/">LLaVA-OneVision (OV)</a>!</strong> The new LLaVA-OV models (0.5B/7B/72B) achieve new state-of-the-art performance across single-image, multi-image, and video benchmarks, sometimes rivaling top commercial models on 47 diverse benchmarks. ğŸ“„ Explore More:</p> 
  <ul> 
   <li><a href="https://arxiv.org/abs/2408.03326">[Paper]</a>: In-depth insights, new emegerging scenarios, ie, strong video understadning through task transfer from images.</li> 
   <li><a href="https://github.com/LLaVA-VL/LLaVA-NeXT/raw/main/docs/LLaVA_OneVision.md">[LLaVA-OV Doc]</a>: Model inference and evaluation guidance.</li> 
   <li><a href="https://github.com/LLaVA-VL/LLaVA-NeXT/raw/main/scripts/train">[Scripts]</a>: Start training models on your single-image/multi-image/video data.</li> 
  </ul> </li> 
 <li> <p>[2024/07/16] ğŸ”¥ <strong>LLaVA-NeXT-Video</strong> has been upgraded. The new 32B model achieves the best open-source performance on several video benchmarks, including <a href="https://video-mme.github.io/home_page.html#leaderboard">Video-MME</a>. Please refer to <a href="https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LLaVA-NeXT-Video_0716.md">this page</a> for details, refer to <a href="https://huggingface.co/spaces/WildVision/vision-arena">llava_next-video_demo</a> for demo.</p> </li> 
 <li> <p>[2024/06/23] ğŸ”¥ <strong>LLaVA-NeXT-Interleave</strong> is released. We utilize image-text interleaved format to unify multi-image, video, and 3D tasks in one LLM and achieve <strong>SoTA</strong> performance on a wide range of benchmarks. Check out <a href="https://arxiv.org/pdf/2407.07895">paper</a>, <a href="https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/">blog</a>, and <a href="https://huggingface.co/collections/lmms-lab/llava-next-interleave-66763c55c411b340b35873d1">checkpoints</a> to see new capabilities and improved performance! We have released 0.5b, 7b, and 7b-dpo models.</p> 
  <ul> 
   <li>An all-round LLM for multi-image, video, and 3D with strong performance [<a href="https://huggingface.co/spaces/lmms-lab/LLaVA-NeXT-Interleave-Demo">demo</a>]</li> 
   <li>Construct interleave training data <a href="https://huggingface.co/datasets/lmms-lab/M4-Instruct-Data"><strong>M4-Instruct</strong></a></li> 
   <li>Construct multi-image benchmark <a href="https://huggingface.co/datasets/lmms-lab/LLaVA-NeXT-Interleave-Bench"><strong>LLaVA-Interleave Bench</strong></a></li> 
  </ul> </li> 
 <li> <p>[2024/05/25] ğŸ”¥ Wondering "<a href="https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/">What Else Influences Visual Instruction Tuning Beyond Data?</a>" Our new <a href="https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/">blog</a> summarizes empirical explorations to ablate the various design choices in improving LMMs except instruct data itself. Meanwhile, open-source the recapioned high-quality data using LLaVA-NeXT-34B on <a href="https://huggingface.co/datasets/lmms-lab/LLaVA-ReCap-118K">[COCO]</a> <a href="https://huggingface.co/datasets/lmms-lab/LLaVA-ReCap-558K">[LCS]</a> <a href="https://huggingface.co/datasets/lmms-lab/LLaVA-ReCap-CC3M">[CC3M]</a>.</p> 
  <ul> 
   <li>Architectures (LMM &amp; Vision Encoder)</li> 
   <li>Visual Representations (Resolution &amp; # Tokens)</li> 
   <li>Training Strategies (High-quality data &amp; Trainable modules)</li> 
  </ul> </li> 
 <li> <p>[2024/05/10] ğŸ”¥ <strong>LLaVA-NeXT</strong> (Stronger) models are released, with support of stronger LMM inlcuding LLama-3 (8B) and Qwen-1.5 (72B/110B) Check out [<a href="https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/">blog</a>] and [<a href="https://huggingface.co/lmms-lab">checkpoints</a>] to see improved performance!</p> </li> 
 <li> <p>[2024/05/10] ğŸ”¥ <strong>LLaVA-NeXT</strong> (Video) is released. The image-only-trained LLaVA-NeXT model is surprisingly strong on video tasks with zero-shot modality transfer. DPO training with AI feedback on videos can yield significant improvement. [<a href="https://llava-vl.github.io/blog/2024-04-30-llava-next-video/">Blog</a>], [<a href="https://huggingface.co/collections/lmms-lab/llava-next-video-661e86f5e8dabc3ff793c944">checkpoints</a>] and [<a href="https://github.com/sgl-project/sglang">sglang</a>]</p> </li> 
 <li> <p>[2024/01/30] ğŸ”¥ <strong>LLaVA-NeXT</strong> is out! With additional scaling to LLaVA-1.5, LLaVA-NeXT-34B outperforms Gemini Pro on some benchmarks. It can now process 4x more pixels and perform more tasks/applications than before. Check out the <a href="https://llava-vl.github.io/blog/2024-01-30-llava-next/">blog post</a>, and explore the <a href="https://llava.hliu.cc/">demo</a>! Models are available in <a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md">Model Zoo</a>. Training/eval data and scripts coming soon.</p> </li> 
</ul> 
<details> 
 More 
 <ul> 
  <li> <p>[2024/03/10] ğŸ”¥ Releasing <strong>LMMs-Eval</strong>, a highly efficient evaluation pipeline we used when developing LLaVA-NeXT. It supports the evaluation of LMMs on dozens of public datasets and allows new dataset onboarding, making the dev of new LMMs much faster. [<a href="https://lmms-lab.github.io/lmms-eval-blog/lmms-eval-0.1/">Blog</a>] [<a href="https://github.com/EvolvingLMMs-Lab/lmms-eval">Codebase</a>]</p> </li> 
  <li> <p>[2023/11/10] <a href="https://llava-vl.github.io/llava-plus/">LLaVA-Plus</a> is released: Learning to Use Tools for Creating Multimodal Agents, with LLaVA-Plus (LLaVA that Plug and Learn to Use Skills). [<a href="https://llava-vl.github.io/llava-plus/">Project Page</a>] [<a href="https://llavaplus.ngrok.io/">Demo</a>] [<a href="https://github.com/LLaVA-VL/LLaVA-Plus-Codebase">Code</a>] [<a href="https://arxiv.org/abs/2311.05437">Paper</a>]</p> </li> 
  <li> <p>[2023/11/02] <a href="https://llava-vl.github.io/llava-interactive/">LLaVA-Interactive</a> is released: Experience the future of human-AI multimodal interaction with an all-in-one demo for Image Chat, Segmentation, Generation and Editing. [<a href="https://llava-vl.github.io/llava-interactive/">Project Page</a>] [<a href="https://llavainteractive.ngrok.io/">Demo</a>] [<a href="https://github.com/LLaVA-VL/LLaVA-Interactive-Demo">Code</a>] [<a href="https://arxiv.org/abs/2311.00571">Paper</a>]</p> </li> 
  <li> <p>[2023/10/26] ğŸ”¥ LLaVA-1.5 with LoRA achieves comparable performance as full-model finetuning, with a reduced GPU RAM requirement (<a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md#llava-v15">ckpts</a>, <a href="https://github.com/haotian-liu/LLaVA#train">script</a>). We also provide a <a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/Finetune_Custom_Data.md">doc</a> on how to finetune LLaVA-1.5 on your own dataset with LoRA.</p> </li> 
  <li> <p>[2023/10/12] Check out the Korean LLaVA (Ko-LLaVA), created by ETRI, who has generously supported our research! [<a href="https://huggingface.co/spaces/etri-vilab/Ko-LLaVA">ğŸ¤— Demo</a>]</p> </li> 
  <li> <p>[2023/10/05] ğŸ”¥ LLaVA-1.5 is out! Achieving SoTA on 11 benchmarks, with just simple modifications to the original LLaVA, utilizes all public data, completes training in ~1 day on a single 8-A100 node, and surpasses methods like Qwen-VL-Chat that use billion-scale data. Check out the <a href="https://arxiv.org/abs/2310.03744">technical report</a>, and explore the <a href="https://llava.hliu.cc/">demo</a>! Models are available in <a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md">Model Zoo</a>. The training data and scripts of LLaVA-1.5 are released <a href="https://github.com/haotian-liu/LLaVA#train">here</a>, and evaluation scripts are released <a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/Evaluation.md">here</a>!</p> </li> 
  <li> <p>[2023/09/26] LLaVA is improved with reinforcement learning from human feedback (RLHF) to improve fact grounding and reduce hallucination. Check out the new SFT and RLHF checkpoints at project <a href="https://llava-rlhf.github.io/">[LLavA-RLHF]</a></p> </li> 
  <li> <p>[2023/09/22] <a href="https://arxiv.org/abs/2304.08485">LLaVA</a> is accepted by NeurIPS 2023 as <strong>oral presentation</strong>, and <a href="https://arxiv.org/abs/2306.00890">LLaVA-Med</a> is accepted by NeurIPS 2023 Datasets and Benchmarks Track as <strong>spotlight presentation</strong>.</p> </li> 
  <li> <p>[2023/11/06] Support <strong>Intel</strong> dGPU and CPU platforms. <a href="https://github.com/haotian-liu/LLaVA/tree/intel/docs/intel">More details here.</a></p> </li> 
  <li> <p>[2023/10/12] LLaVA is now supported in <a href="https://github.com/ggerganov/llama.cpp/pull/3436">llama.cpp</a> with 4-bit / 5-bit quantization support!</p> </li> 
  <li> <p>[2023/10/11] The training data and scripts of LLaVA-1.5 are released <a href="https://github.com/haotian-liu/LLaVA#train">here</a>, and evaluation scripts are released <a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/Evaluation.md">here</a>!</p> </li> 
  <li> <p>[2023/10/10] <a href="https://blog.roboflow.com/first-impressions-with-llava-1-5/">Roboflow Deep Dive</a>: First Impressions with LLaVA-1.5.</p> </li> 
  <li> <p>[2023/09/20] We summarize our empirical study of training 33B and 65B LLaVA models in a <a href="https://arxiv.org/abs/2309.09958">note</a>. Further, if you are interested in the comprehensive review, evolution and trend of multimodal foundation models, please check out our recent survey paper <a href="https://arxiv.org/abs/2309.10020">``Multimodal Foundation Models: From Specialists to General-Purpose Assistants''.</a></p> </li> 
 </ul> 
 <p align="center"> <img src="https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings/raw/main/images/mfm_evolution.jpeg?raw=true" width="50%/" /> </p> 
 <ul> 
  <li>[2023/07/19] ğŸ”¥ We release a major upgrade, including support for LLaMA-2, LoRA training, 4-/8-bit inference, higher resolution (336x336), and a lot more. We release <a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/LLaVA_Bench.md">LLaVA Bench</a> for benchmarking open-ended visual chat with results from Bard and Bing-Chat. We also support and verify training with RTX 3090 and RTX A6000. Check out <a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/LLaVA_from_LLaMA2.md">LLaVA-from-LLaMA-2</a>, and our <a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md">model zoo</a>!</li> 
  <li>[2023/06/26] <a href="https://vlp-tutorial.github.io/">CVPR 2023 Tutorial</a> on <strong>Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4</strong>! Please check out [<a href="https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Chunyuan_cvpr2023_tutorial_lmm.pdf">Slides</a>] [<a href="https://arxiv.org/abs/2306.14895">Notes</a>] [<a href="https://youtu.be/mkI7EPD1vp8">YouTube</a>] [<a href="https://www.bilibili.com/video/BV1Ng4y1T7v3/">Bilibli</a>].</li> 
  <li>[2023/06/11] We released the preview for the most requested feature: DeepSpeed and LoRA support! Please see documentations <a href="https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LoRA.md">here</a>.</li> 
  <li>[2023/06/01] We released <strong>LLaVA-Med: Large Language and Vision Assistant for Biomedicine</strong>, a step towards building biomedical domain large language and vision models with GPT-4 level capabilities. Checkout the <a href="https://arxiv.org/abs/2306.00890">paper</a> and <a href="https://github.com/microsoft/LLaVA-Med">page</a>.</li> 
  <li>[2023/05/06] We are releasing <a href="https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview">LLaVA-Lighting-MPT-7B-preview</a>, based on MPT-7B-Chat! See <a href="https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/#LLaVA-MPT-7b">here</a> for more details.</li> 
  <li>[2023/05/02] ğŸ”¥ We are releasing LLaVA-Lighting! Train a lite, multimodal GPT-4 with just $40 in 3 hours! See <a href="https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/#train-llava-lightning">here</a> for more details.</li> 
  <li>[2023/04/27] Thanks to the community effort, LLaVA-13B with 4-bit quantization allows you to run on a GPU with as few as 12GB VRAM! Try it out <a href="https://github.com/oobabooga/text-generation-webui/tree/main/extensions/llava">here</a>.</li> 
  <li>[2023/04/17] ğŸ”¥ We released <strong>LLaVA: Large Language and Vision Assistant</strong>. We propose visual instruction tuning, towards building large language and vision models with GPT-4 level capabilities. Checkout the <a href="https://arxiv.org/abs/2304.08485">paper</a> and <a href="https://llava.hliu.cc/">demo</a>.</li> 
 </ul> 
</details> 
<!-- <a href="https://llava.hliu.cc/"><img src="assets/demo.gif" width="70%"></a> --> 
<p><strong>Usage and License Notices</strong>: This project utilizes certain datasets and checkpoints that are subject to their respective original licenses. Users must comply with all terms and conditions of these original licenses, including but not limited to the <a href="https://openai.com/policies/terms-of-use">OpenAI Terms of Use</a> for the dataset and the specific licenses for base language models for checkpoints trained using the dataset (e.g. <a href="https://ai.meta.com/llama/license/">Llama-1/2 community license</a> for LLaMA-2 and Vicuna-v1.5, <a href="https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat/blob/main/LICENSE">Tongyi Qianwen RESEARCH LICENSE AGREEMENT</a> and <a href="https://llama.meta.com/llama3/license/">Llama-3 Research License</a>). This project does not impose any additional constraints beyond those stipulated in the original licenses. Furthermore, users are reminded to ensure that their use of the dataset and checkpoints is in compliance with all applicable laws and regulations.</p> 
<h2>Models &amp; Scripts</h2> 
<h3>Installation</h3> 
<h4>1. <strong>Clone this repository and navigate to the LLaVA folder:</strong></h4> 
<pre><code class="language-bash">git clone https://github.com/LLaVA-VL/LLaVA-NeXT
cd LLaVA-NeXT
</code></pre> 
<h4>2. <strong>Install the inference package:</strong></h4> 
<pre><code class="language-bash">conda create -n llava python=3.10 -y
conda activate llava
pip install --upgrade pip  # Enable PEP 660 support.
pip install -e ".[train]"
</code></pre> 
<h3>Project Navigation</h3> 
<p>Please checkout the following page for more inference &amp; evaluation details.</p> 
<h4>- <strong>LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild</strong></h4> 
<ul> 
 <li><a href="https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LLaVA-NeXT.md">LLaVA-NeXT-Image</a>: for image demo inference and evaluation of stronger LMMs using <a href="https://github.com/EvolvingLMMs-Lab/lmms-eval">lmms-eval</a>.</li> 
</ul> 
<h4>- LLaVA-NeXT: A Strong Zero-shot Video Understanding Model</h4> 
<ul> 
 <li><a href="https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LLaVA-NeXT-Video.md">LLaVA-NeXT-Video</a>: for video inference and evaluation scripts. We recommend to use <a href="https://lmms-lab.github.io/posts/lmms-eval-0.2/">LMMs-video</a> for evaluation.</li> 
</ul> 
<h4>- LLaVA-NeXT: Tackling Multi-image, Video, and 3D in Large Multimodal Models</h4> 
<ul> 
 <li><a href="https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LLaVA-NeXT-Interleave.md">LLaVA-NeXT-Interleave</a>: for multi-image demo and evaluation scripts.</li> 
</ul> 
<h2>SGLang for SpeedUp Inference and Deployment</h2> 
<p>We use <a href="https://github.com/sgl-project/sglang">SGLang</a> to speed up inference and deployment of LLaVA-NeXT. You could make LLaVA-NeXT as a backend API service with SGLang.</p> 
<p><strong>Prepare Environment</strong>: Following the instruction in the <a href="https://github.com/sgl-project/sglang?tab=readme-ov-file#install">sglang</a></p> 
<h3>LLaVA-NeXT (Image)</h3> 
<p>Checkout the HTTP Post/Get and SRT usage at <a href="https://github.com/sgl-project/sglang/raw/main/examples/usage/llava">sglang/examples/usage/llava</a></p> 
<h3>LLaVA-NeXT (Video)</h3> 
<p><strong>Launch and Run on (K) Nodes</strong>:</p> 
<ul> 
 <li>Go to sglang project <pre><code>cd PATH_TO/sglang
</code></pre> </li> 
 <li>First node: <pre><code class="language-sh">bash examples/usage/llava_video/srt_example_llava_v.sh K 0 YOUR_VIDEO_PATH YOUR_MODEL_PATH FRAMES_PER_VIDEO
(e.g. bash examples/usage/llava_video/srt_example_llava_v.sh K 0 examples/usage/llava_video/videos/Q98Z4OTh8RwmDonc.mp4 lmms-lab/LLaVA-NeXT-Video-7B-DPO 16)
</code></pre> </li> 
 <li>Second node: <pre><code class="language-sh">bash examples/usage/llava_video/srt_example_llava_v.sh K 1 YOUR_VIDEO_PATH YOUR_MODEL_PATH FRAMES_PER_VIDEO
</code></pre> </li> 
 <li>The K node: <pre><code class="language-sh">bash examples/usage/llava_video/srt_example_llava_v.sh K K-1 YOUR_VIDEO_PATH YOUR_MODEL_PATH FRAMES_PER_VIDEO
</code></pre> </li> 
</ul> 
<h2>Citation</h2> 
<p>If you find it useful for your research and applications, please cite related papers/blogs using this BibTeX:</p> 
<pre><code class="language-bibtex">@article{li2024llava,
  title={LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models},
  author={Li, Feng and Zhang, Renrui and Zhang, Hao and Zhang, Yuanhan and Li, Bo and Li, Wei and Ma, Zejun and Li, Chunyuan},
  journal={arXiv preprint arXiv:2407.07895},
  year={2024}
}

@misc{li2024llavanext-ablations,
	title={LLaVA-NeXT: What Else Influences Visual Instruction Tuning Beyond Data?},
	url={https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/},
	author={Li, Bo and Zhang, Hao and Zhang, Kaichen and Guo, Dong and Zhang, Yuanhan and Zhang, Renrui and Li, Feng and Liu, Ziwei and Li, Chunyuan},
	month={May},
	year={2024}
}

@misc{li2024llavanext-strong,
    title={LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild},
    url={https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/},
    author={Li, Bo and Zhang, Kaichen and Zhang, Hao and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Yuanhan and Liu, Ziwei and Li, Chunyuan},
    month={May},
    year={2024}
}

@misc{zhang2024llavanext-video,
  title={LLaVA-NeXT: A Strong Zero-shot Video Understanding Model},
  url={https://llava-vl.github.io/blog/2024-04-30-llava-next-video/},
  author={Zhang, Yuanhan and Li, Bo and Liu, haotian and Lee, Yong jae and Gui, Liangke and Fu, Di and Feng, Jiashi and Liu, Ziwei and Li, Chunyuan},
  month={April},
  year={2024}
}

@misc{liu2024llavanext,
    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    month={January},
    year={2024}
}

@misc{liu2023improvedllava,
      title={Improved Baselines with Visual Instruction Tuning}, 
      author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
      publisher={arXiv:2310.03744},
      year={2023},
}

@misc{liu2023llava,
      title={Visual Instruction Tuning}, 
      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
      publisher={NeurIPS},
      year={2023},
}
</code></pre> 
<h2>Acknowledgement</h2> 
<ul> 
 <li><a href="https://github.com/lm-sys/FastChat">Vicuna</a>: the codebase we built upon, and our base model Vicuna-13B that has the amazing language capabilities!</li> 
 <li>The LLaVA-NeXT project is currently maintained by the team along with our contributors (listed alphabetically by the first names): <a href="https://brianboli.com/">Bo Li</a>, <a href="https://www.linkedin.com/in/dongguoset/">Dong Guo</a>, <a href="https://scholar.google.com/citations?hl=zh-CN&amp;user=ybRe9GcAAAAJ&amp;view_op=list_works&amp;sortby=pubdate">Feng Li</a>, <a href="https://scholar.google.com/citations?user=B8hPxMQAAAAJ&amp;hl=en">Hao Zhang</a>, <a href="https://www.linkedin.com/in/kaichen-zhang-014b17219/?originalSubdomain=sg">Kaichen Zhang</a>, <a href="https://zrrskywalker.github.io/">Renrui Zhang</a>, <a href="https://zhangyuanhan-ai.github.io/">Yuanhan Zhang</a>, led by <a href="https://chunyuan.li/">Chunyuan Li</a> and with the guidance and help from <a href="https://hliu.cc/">Haotian Liu</a>.</li> 
 <li>The <code>ï»¿lmms-eval</code> framework and its core contributors, including Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, and Kairui Hu, for their support on the evaluation side.</li> 
</ul> 
<h2>Related Projects</h2> 
<ul> 
 <li><a href="https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM">Instruction Tuning with GPT-4</a></li> 
 <li><a href="https://github.com/microsoft/LLaVA-Med">LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day</a></li> 
 <li><a href="https://github.com/Luodian/Otter">Otter: In-Context Multi-Modal Instruction Tuning</a></li> 
</ul> 
<p>For future project ideas, please check out:</p> 
<ul> 
 <li><a href="https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once">SEEM: Segment Everything Everywhere All at Once</a></li> 
 <li><a href="https://github.com/IDEA-Research/Grounded-Segment-Anything">Grounded-Segment-Anything</a> to detect, segment, and generate anything by marrying <a href="https://github.com/IDEA-Research/GroundingDINO">Grounding DINO</a> and <a href="https://github.com/facebookresearch/segment-anything">Segment-Anything</a>.</li> 
</ul>
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>hacksider/Deep-Live-Cam</title>
<link>https://github.com/hacksider/Deep-Live-Cam</link>
<guid>https://github.com/hacksider/Deep-Live-Cam</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šå®æ—¶äººè„¸äº’æ¢ã€ä¸€é”®è§†é¢‘æ·±åº¦ä¼ªé€ ã€å•å¼ å›¾åƒã€AIç”Ÿæˆåª’ä½“ã€æ³•å¾‹ä¸ä¼¦ç†

æ€»ç»“:

æœ¬æ–‡ä»‹ç»äº†ä¸€æ¬¾ç”¨äºå®æ—¶äººè„¸äº’æ¢å’Œä¸€é”®è§†é¢‘æ·±åº¦ä¼ªé€ çš„è½¯ä»¶ï¼Œæ—¨åœ¨ä¸ºè‰ºæœ¯å®¶æä¾›å·¥å…·ï¼Œå¦‚åŠ¨ç”»å®šåˆ¶è§’è‰²æˆ–ä½œä¸ºæœè£…æ¨¡ç‰¹ç­‰ä»»åŠ¡ã€‚å¼€å‘è€…æ„è¯†åˆ°è¯¥è½¯ä»¶å¯èƒ½è¢«ä¸å½“ä½¿ç”¨ï¼Œå¹¶å·²å†…ç½®æ£€æŸ¥æœºåˆ¶ä»¥é˜²æ­¢å¤„ç†ä¸é€‚å½“å†…å®¹ï¼Œå¦‚è£¸ä½“ã€æš´åŠ›æˆ–æ•æ„Ÿç´ æã€‚å¦‚æœè¦æ±‚ï¼Œé¡¹ç›®å¯èƒ½ä¼šå…³é—­æˆ–åœ¨è¾“å‡ºä¸­æ·»åŠ æ°´å°ã€‚

å®‰è£…æ­¥éª¤åŒ…æ‹¬å®‰è£…Pythonã€pipå’ŒGitï¼Œå…‹éš†ä»£ç åº“ï¼Œä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼Œä½¿ç”¨è™šæ‹Ÿç¯å¢ƒå®‰è£…ä¾èµ–é¡¹ï¼Œå¹¶æ ¹æ®GPUå¯ç”¨æ€§é€‰æ‹©ç‰¹å®šçš„æ‰§è¡Œæä¾›ç¨‹åºè¿›è¡ŒåŠ é€Ÿã€‚è½¯ä»¶é€šè¿‡å‘½ä»¤è¡Œè¿è¡Œï¼Œå…è®¸ç”¨æˆ·æŒ‡å®šæºå›¾åƒã€ç›®æ ‡å›¾åƒæˆ–è§†é¢‘ã€è¾“å‡ºä½ç½®ã€å¸§å¤„ç†å™¨ã€ä¿æŒå¸§é€Ÿç‡ã€éŸ³é¢‘å’Œä¸´æ—¶æ–‡ä»¶ç­‰åŠŸèƒ½ã€‚

ä½¿ç”¨æ–¹æ³•æ¶‰åŠé€‰æ‹©é¢éƒ¨å›¾åƒã€ç›®æ ‡å›¾åƒæˆ–è§†é¢‘ï¼Œå¯åŠ¨ç¨‹åºï¼Œè§‚å¯Ÿå®æ—¶é¢„è§ˆå¹¶å¤„ç†å®Œæˆåçš„è¾“å‡ºæ–‡ä»¶ã€‚å‘½ä»¤è¡Œé€‰é¡¹æä¾›é¢å¤–æ§åˆ¶ï¼Œå¦‚ä¿æŒåŸå§‹å¸§ç‡ã€éŸ³é¢‘å’Œä¸´æ—¶æ–‡ä»¶ã€å¤„ç†æ¯ä¸ªé¢éƒ¨ã€è¿‡æ»¤ä¸é€‚å½“å†…å®¹ç­‰ã€‚

æœ€åï¼Œç”¨æˆ·è¢«æé†’è´Ÿè´£ä»»åœ°ä½¿ç”¨è½¯ä»¶ï¼Œç‰¹åˆ«æ˜¯å½“ä½¿ç”¨çœŸå®äººç‰©é¢éƒ¨æ—¶ï¼Œéœ€è·å¾—åŒæ„å¹¶åœ¨åœ¨çº¿å‘å¸ƒå†…å®¹æ—¶æ˜ç¡®è¡¨ç¤ºä¸ºæ·±åº¦ä¼ªé€ ã€‚å¼€å‘è€…ä¸æ‰¿æ‹…ç”¨æˆ·è¡Œä¸ºçš„è´£ä»»ã€‚ <div>
<p>real time face swap and one-click video deepfake with only a single image</p><hr /><p><img alt="demo-gif" src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/demo.gif" /></p> 
<h2>Disclaimer</h2> 
<p>This software is meant to be a productive contribution to the rapidly growing AI-generated media industry. It will help artists with tasks such as animating a custom character or using the character as a model for clothing etc.</p> 
<p>The developers of this software are aware of its possible unethical applications and are committed to take preventative measures against them. It has a built-in check which prevents the program from working on inappropriate media including but not limited to nudity, graphic content, sensitive material such as war footage etc. We will continue to develop this project in the positive direction while adhering to law and ethics. This project may be shut down or include watermarks on the output if requested by law.</p> 
<p>Users of this software are expected to use this software responsibly while abiding by local laws. If the face of a real person is being used, users are required to get consent from the concerned person and clearly mention that it is a deepfake when posting content online. Developers of this software will not be responsible for actions of end-users.</p> 
<h2>How do I install it?</h2> 
<h3>Basic: It is more likely to work on your computer but it will also be very slow. You can follow instructions for the basic install (This usually runs via <strong>CPU</strong>)</h3> 
<h4>1.Setup your platform</h4> 
<ul> 
 <li>python (3.10 recommended)</li> 
 <li>pip</li> 
 <li>git</li> 
 <li><a href="https://www.youtube.com/watch?v=OlNWCpFdVMA">ffmpeg</a></li> 
 <li><a href="https://visualstudio.microsoft.com/visual-cpp-build-tools/">visual studio 2022 runtimes (windows)</a></li> 
</ul> 
<h4>2. Clone Repository</h4> 
<pre><code>https://github.com/hacksider/Deep-Live-Cam.git
</code></pre> 
<h4>3. Download Models</h4> 
<ol> 
 <li><a href="https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth">GFPGANv1.4</a></li> 
 <li><a href="https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx">inswapper_128_fp16.onnx</a></li> 
</ol> 
<p>Then put those 2 files on the "<strong>models</strong>" folder</p> 
<h4>4. Install dependency</h4> 
<p>We highly recommend to work with a <code>venv</code> to avoid issues.</p> 
<pre><code>pip install -r requirements.txt
</code></pre> 
<p>For MAC OS, You have to install or upgrade python-tk package:</p> 
<pre><code>brew install python-tk@3.10
</code></pre> 
<h5>DONE!!! If you dont have any GPU, You should be able to run roop using <code>python run.py</code> command. Keep in mind that while running the program for first time, it will download some models which can take time depending on your network connection.</h5> 
<h3>*Proceed if you want to use GPU Acceleration</h3> 
<h3>CUDA Execution Provider (Nvidia)*</h3> 
<ol> 
 <li> <p>Install <a href="https://developer.nvidia.com/cuda-11-8-0-download-archive">CUDA Toolkit 11.8</a></p> </li> 
 <li> <p>Install dependencies:</p> </li> 
</ol> 
<pre><code>pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu==1.16.3

</code></pre> 
<ol start="3"> 
 <li>Usage in case the provider is available:</li> 
</ol> 
<pre><code>python run.py --execution-provider cuda

</code></pre> 
<h3><a href="https://github.com/s0md3v/roop/wiki/2.-Acceleration#coreml-execution-provider-apple-silicon"></a>CoreML Execution Provider (Apple Silicon)</h3> 
<ol> 
 <li>Install dependencies:</li> 
</ol> 
<pre><code>pip uninstall onnxruntime onnxruntime-silicon
pip install onnxruntime-silicon==1.13.1

</code></pre> 
<ol start="2"> 
 <li>Usage in case the provider is available:</li> 
</ol> 
<pre><code>python run.py --execution-provider coreml

</code></pre> 
<h3><a href="https://github.com/s0md3v/roop/wiki/2.-Acceleration#coreml-execution-provider-apple-legacy"></a>CoreML Execution Provider (Apple Legacy)</h3> 
<ol> 
 <li>Install dependencies:</li> 
</ol> 
<pre><code>pip uninstall onnxruntime onnxruntime-coreml
pip install onnxruntime-coreml==1.13.1

</code></pre> 
<ol start="2"> 
 <li>Usage in case the provider is available:</li> 
</ol> 
<pre><code>python run.py --execution-provider coreml

</code></pre> 
<h3><a href="https://github.com/s0md3v/roop/wiki/2.-Acceleration#directml-execution-provider-windows"></a>DirectML Execution Provider (Windows)</h3> 
<ol> 
 <li>Install dependencies:</li> 
</ol> 
<pre><code>pip uninstall onnxruntime onnxruntime-directml
pip install onnxruntime-directml==1.15.1

</code></pre> 
<ol start="2"> 
 <li>Usage in case the provider is available:</li> 
</ol> 
<pre><code>python run.py --execution-provider directml

</code></pre> 
<h3><a href="https://github.com/s0md3v/roop/wiki/2.-Acceleration#openvino-execution-provider-intel"></a>OpenVINOâ„¢ Execution Provider (Intel)</h3> 
<ol> 
 <li>Install dependencies:</li> 
</ol> 
<pre><code>pip uninstall onnxruntime onnxruntime-openvino
pip install onnxruntime-openvino==1.15.0

</code></pre> 
<ol start="2"> 
 <li>Usage in case the provider is available:</li> 
</ol> 
<pre><code>python run.py --execution-provider openvino
</code></pre> 
<h2>How do I use it?</h2> 
<blockquote> 
 <p>Note: When you run this program for the first time, it will download some models ~300MB in size.</p> 
</blockquote> 
<p>Executing <code>python run.py</code> command will launch this window: <img alt="gui-demo" src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/instruction.png" /></p> 
<p>Choose a face (image with desired face) and the target image/video (image/video in which you want to replace the face) and click on <code>Start</code>. Open file explorer and navigate to the directory you select your output to be in. You will find a directory named <code>&lt;video_title&gt;</code> where you can see the frames being swapped in realtime. Once the processing is done, it will create the output file. That's it.</p> 
<h2>For the webcam mode</h2> 
<p>Just follow the clicks on the screenshot</p> 
<ol> 
 <li>Select a face</li> 
 <li>Click live</li> 
 <li>Wait for a few seconds (it takes a longer time, usually 10 to 30 seconds before the preview shows up)</li> 
</ol> 
<p><img alt="demo-gif" src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/demo.gif" /></p> 
<p>Just use your favorite screencapture to stream like OBS</p> 
<blockquote> 
 <p>Note: In case you want to change your face, just select another picture, the preview mode will then restart (so just wait a bit).</p> 
</blockquote> 
<p>Additional command line arguments are given below. To learn out what they do, check <a href="https://github.com/s0md3v/roop/wiki/Advanced-Options">this guide</a>.</p> 
<pre><code>options:
  -h, --help                                               show this help message and exit
  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image
  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory
  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)
  --keep-fps                                               keep original fps
  --keep-audio                                             keep original audio
  --keep-frames                                            keep temporary frames
  --many-faces                                             process every face
  --nsfw-filter                                            filter the NSFW image or video
  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder
  --video-quality [0-51]                                   adjust output video quality
  --live-mirror                                            the live camera display as you see it in the front-facing camera frame
  --live-resizable                                         the live camera frame is resizable
  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB
  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)
  --execution-threads EXECUTION_THREADS                    number of execution threads
  -v, --version                                            show program's version number and exit
</code></pre> 
<p>Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.</p> 
<h2>Want the Next Update Now?</h2> 
<p>If you want the latest and greatest build, or want to see some new great features, go to our <a href="https://github.com/hacksider/Deep-Live-Cam/tree/experimental">experimental branch</a> and experience what the contributors have given.</p> 
<h2>Credits</h2> 
<ul> 
 <li><a href="https://ffmpeg.org/">ffmpeg</a>: for making video related operations easy</li> 
 <li><a href="https://github.com/deepinsight">deepinsight</a>: for their <a href="https://github.com/deepinsight/insightface">insightface</a> project which provided a well-made library and models.</li> 
 <li><a href="https://github.com/havok2-htwo">havok2-htwo</a> : for sharing the code for webcam</li> 
 <li><a href="https://github.com/GosuDRM/nsfw-roop">GosuDRM</a> : for uncensoring roop</li> 
 <li>and <a href="https://github.com/hacksider/Deep-Live-Cam/graphs/contributors">all developers</a> behind libraries used in this project.</li> 
 <li>Foot Note: <a href="https://github.com/hacksider/roop-cam">This is originally roop-cam, see the full history of the code here.</a> Please be informed that the base author of the code is <a href="https://github.com/s0md3v/roop">s0md3v</a></li> 
</ul>
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>goauthentik/authentik</title>
<link>https://github.com/goauthentik/authentik</link>
<guid>https://github.com/goauthentik/authentik</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šauthentikã€èº«ä»½éªŒè¯ã€å¼€æ”¾æºä»£ç ã€Docker Composeã€Helm Chart

æ€»ç»“:
authentikæ˜¯ä¸€ä¸ªå¼ºè°ƒçµæ´»æ€§å’Œå¤šæ ·æ€§çš„å¼€æºèº«ä»½æä¾›è€…ï¼Œæ”¯æŒå¹¿æ³›çš„åè®®ã€‚å®ƒè¢«æ¨èç”¨äºå°å‹æµ‹è¯•è®¾ç½®ä¸­çš„Docker Composeéƒ¨ç½²ï¼Œä»¥åŠå¤§å‹è®¾ç½®ä¸­çš„Helm Chartéƒ¨ç½²ã€‚å¯¹äºéœ€è¦æ›¿æ¢Oktaã€Auth0ã€Entra IDã€Ping Identityç­‰ä¼ ç»ŸIDPçš„å¤§è§„æ¨¡ä¼ä¸šæˆ–B2B2Cä¸šåŠ¡æ¥è¯´ï¼Œauthentikæ˜¯ä¸€ä¸ªè‡ªæ‰˜ç®¡çš„ç†æƒ³é€‰æ‹©ã€‚ç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚é€‰æ‹©é€‚åˆçš„éƒ¨ç½²æ–¹å¼ã€‚æ­¤å¤–ï¼Œauthentikæä¾›äº†ä¸€ä¸ªå¼€å‘è€…ç¤¾åŒºï¼Œé¼“åŠ±è´¡çŒ®å’Œåˆä½œï¼Œå¸Œæœ›æœ‰æ›´å¤šç»„ç»‡ä½¿ç”¨å¹¶å‚ä¸åˆ°é¡¹ç›®ä¸­æ¥ã€‚å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨authentikï¼Œå¯ä»¥å‘é¡¹ç›®å›¢é˜Ÿæäº¤æ‚¨çš„Logoä»¥ç¤ºæ”¯æŒï¼Œå¹¶å‚ä¸è´¡çŒ®ä»£ç æˆ–æå‡ºæ”¹è¿›å»ºè®®ã€‚ <div>
<p>The authentication glue you need.</p><hr /><p align="center"> <img alt="authentik logo" height="150" src="https://goauthentik.io/img/icon_top_brand_colour.svg?sanitize=true" /> </p> 
<hr /> 
<p><a href="https://goauthentik.io/discord"><img alt="Join Discord" src="https://img.shields.io/discord/809154715984199690?label=Discord&amp;style=for-the-badge" /></a> <a href="https://github.com/goauthentik/authentik/actions/workflows/ci-main.yml"><img alt="GitHub Workflow Status" src="https://img.shields.io/github/actions/workflow/status/goauthentik/authentik/ci-main.yml?branch=main&amp;label=core%20build&amp;style=for-the-badge" /></a> <a href="https://github.com/goauthentik/authentik/actions/workflows/ci-outpost.yml"><img alt="GitHub Workflow Status" src="https://img.shields.io/github/actions/workflow/status/goauthentik/authentik/ci-outpost.yml?branch=main&amp;label=outpost%20build&amp;style=for-the-badge" /></a> <a href="https://github.com/goauthentik/authentik/actions/workflows/ci-web.yml"><img alt="GitHub Workflow Status" src="https://img.shields.io/github/actions/workflow/status/goauthentik/authentik/ci-web.yml?branch=main&amp;label=web%20build&amp;style=for-the-badge" /></a> <a href="https://codecov.io/gh/goauthentik/authentik"><img alt="Code Coverage" src="https://img.shields.io/codecov/c/gh/goauthentik/authentik?style=for-the-badge" /></a> <img alt="Docker pulls" src="https://img.shields.io/docker/pulls/beryju/authentik.svg?style=for-the-badge" /> <img alt="Latest version" src="https://img.shields.io/docker/v/beryju/authentik?sort=semver&amp;style=for-the-badge" /> <a href="https://www.transifex.com/authentik/authentik/"><img alt="" src="https://img.shields.io/badge/Help%20translate-transifex-blue?style=for-the-badge" /></a></p> 
<h2>What is authentik?</h2> 
<p>authentik is an open-source Identity Provider that emphasizes flexibility and versatility, with support for a wide set of protocols.</p> 
<p>Our <a href="https://goauthentik.io/pricing">enterprise offer</a> can also be used as a self-hosted replacement for large-scale deployments of Okta/Auth0, Entra ID, Ping Identity, or other legacy IdPs for employees and B2B2C use.</p> 
<h2>Installation</h2> 
<p>For small/test setups it is recommended to use Docker Compose; refer to the <a href="https://goauthentik.io/docs/installation/docker-compose/?utm_source=github">documentation</a>.</p> 
<p>For bigger setups, there is a Helm Chart <a href="https://github.com/goauthentik/helm">here</a>. This is documented <a href="https://goauthentik.io/docs/installation/kubernetes/?utm_source=github">here</a>.</p> 
<h2>Screenshots</h2> 
<table> 
 <thead> 
  <tr> 
   <th>Light</th> 
   <th>Dark</th> 
  </tr> 
 </thead> 
 <tbody> 
  <tr> 
   <td><img alt="" src="https://docs.goauthentik.io/img/screen_apps_light.jpg" /></td> 
   <td><img alt="" src="https://docs.goauthentik.io/img/screen_apps_dark.jpg" /></td> 
  </tr> 
  <tr> 
   <td><img alt="" src="https://docs.goauthentik.io/img/screen_admin_light.jpg" /></td> 
   <td><img alt="" src="https://docs.goauthentik.io/img/screen_admin_dark.jpg" /></td> 
  </tr> 
 </tbody> 
</table> 
<h2>Development</h2> 
<p>See <a href="https://goauthentik.io/developer-docs/?utm_source=github">Developer Documentation</a></p> 
<h2>Security</h2> 
<p>See <a href="https://raw.githubusercontent.com/goauthentik/authentik/main/SECURITY.md">SECURITY.md</a></p> 
<h2>Adoption and Contributions</h2> 
<p>Your organization uses authentik? We'd love to add your logo to the readme and our website! Email us @ <a href="mailto:hello@goauthentik.io">hello@goauthentik.io</a> or open a GitHub Issue/PR! For more information on how to contribute to authentik, please refer to our <a href="https://raw.githubusercontent.com/goauthentik/authentik/main/CONTRIBUTING.md">CONTRIBUTING.md file</a>.</p>
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>lllyasviel/stable-diffusion-webui-forge</title>
<link>https://github.com/lllyasviel/stable-diffusion-webui-forge</link>
<guid>https://github.com/lllyasviel/stable-diffusion-webui-forge</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šStable Diffusion WebUI Forgeã€å®‰è£…ã€é«˜çº§å®‰è£…ã€çŠ¶æ€ã€UnetPatcher

æ–‡ç« æ€»ç»“ï¼š

Stable Diffusion WebUI Forgeæ˜¯ä¸€ä¸ªåŸºäºStable Diffusion WebUIçš„å¹³å°ï¼Œæ—¨åœ¨ç®€åŒ–å¼€å‘è¿‡ç¨‹ï¼Œä¼˜åŒ–èµ„æºç®¡ç†ï¼ŒåŠ é€Ÿæ¨ç†é€Ÿåº¦å¹¶ç ”ç©¶å®éªŒåŠŸèƒ½ã€‚å…¶åç§°â€œForgeâ€çµæ„Ÿæ¥æºäºMinecraftä¸­çš„åŒåç»„ä»¶ï¼Œç›®æ ‡æ˜¯æˆä¸ºStable Diffusion WebUIçš„æ‰©å±•ã€‚

**1. å®‰è£…**
Forgeæä¾›äº†ä¸€ä¸ªä¸€é”®å®‰è£…åŒ…ï¼Œå…¶ä¸­åŒ…å«äº†Gitå’ŒPythonï¼Œç”¨æˆ·åªéœ€è§£å‹ç¼©æ–‡ä»¶ã€è¿è¡Œupdate.batæ›´æ–°å¹¶ä½¿ç”¨run.batå¯åŠ¨å³å¯ã€‚

**2. é«˜çº§å®‰è£…**
å¯¹äºç†Ÿæ‚‰Gitçš„ç”¨æˆ·ï¼Œå¯ä»¥å°†Forgeä½œä¸ºSD-WebUIçš„åˆ†æ”¯è¿›è¡Œå®‰è£…ï¼Œè¿™æ ·å¯ä»¥å¤ç”¨åŸæœ‰çš„æ£€æŸ¥ç‚¹å’Œæ‰©å±•ï¼Œä½†éœ€è¦ç”¨æˆ·äº†è§£æ“ä½œé£é™©ã€‚

**3. çŠ¶æ€**
æ–‡ç« åˆ—å‡ºäº†å¤šä¸ªç»„ä»¶çš„çŠ¶æ€ï¼ŒåŒ…æ‹¬åŸºæœ¬æ‰©æ•£ã€GPUå†…å­˜ç®¡ç†ç³»ç»Ÿã€LoRAsã€é¢„å¤„ç†å™¨ã€æ§åˆ¶ç½‘ç­‰ï¼Œå‡æ˜¾ç¤ºä¸ºæ­£å¸¸å·¥ä½œã€‚åŒæ—¶ï¼ŒæŒ‡å‡ºäº†éƒ¨åˆ†UIï¼ˆå¦‚Gradio 4 UIsï¼‰å’Œç‰¹å®šåŠŸèƒ½ï¼ˆå¦‚Microsoft Surfaceè§¦å‹æ”¯æŒï¼‰çš„çŠ¶æ€ã€‚

**4. UnetPatcher**
æ–‡ç« å±•ç¤ºäº†UnetPatcherçš„å®ç°ä»£ç ï¼Œç”¨äºé›†æˆFreeU V2åŠŸèƒ½ï¼ŒåŒ…æ‹¬FFTæ»¤æ³¢å™¨å®ç°ã€è®¾å¤‡é€‚åº”æ€§å’Œå—è¡¥ä¸é€»è¾‘ï¼Œä»¥åŠä¸ä¸»æ¨¡å‹çš„æ•´åˆæ–¹å¼ã€‚

**5. å»ºè®¾ä¸­**
æ–‡ç« æŒ‡å‡ºForgeæ­£å¤„äºå»ºè®¾é˜¶æ®µï¼Œæ–‡æ¡£ã€ç”¨æˆ·ç•Œé¢å’ŒåŠŸèƒ½å¯èƒ½ä¼šéšç€æ›´æ–°è€Œæ”¹å˜ã€‚

æ€»ç»“ï¼š
Stable Diffusion WebUI Forgeæ˜¯ä¸€ä¸ªä¸ºStable Diffusion WebUIé‡èº«å®šåˆ¶çš„å¢å¼ºå¹³å°ï¼Œé€šè¿‡æä¾›ä¸€é”®å®‰è£…åŒ…ã€æ”¯æŒé«˜çº§å®‰è£…é€‰é¡¹ã€å±•ç¤ºç»„ä»¶çŠ¶æ€ã€å±•ç¤ºUnetPatcherå®ç°ç»†èŠ‚ä»¥åŠæç¤ºç”¨æˆ·æ³¨æ„å…¶ä»åœ¨å»ºè®¾ä¸­ï¼Œä¸ºç”¨æˆ·æä¾›äº†ä¸€ç§æ›´é«˜æ•ˆã€æ›´çµæ´»çš„ä½¿ç”¨ä½“éªŒã€‚å…¶ç›®æ ‡æ˜¯é€šè¿‡ä¼˜åŒ–èµ„æºç®¡ç†å’ŒåŠ é€Ÿæ¨ç†é€Ÿåº¦æ¥æå‡æ‰©æ•£æ¨¡å‹çš„å®ç”¨æ€§ï¼ŒåŒæ—¶ä¹Ÿä¸ºå®éªŒæ€§åŠŸèƒ½çš„ç ”ç©¶æä¾›äº†ä¾¿åˆ©ã€‚ <div>
<p></p><hr /><h1>Stable Diffusion WebUI Forge</h1> 
<p>Stable Diffusion WebUI Forge is a platform on top of <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">Stable Diffusion WebUI</a> (based on <a href="https://www.gradio.app/">Gradio</a> <a href="https://github.com/gradio-app/gradio"><img src="https://img.shields.io/github/stars/gradio-app/gradio" /></a>) to make development easier, optimize resource management, speed up inference, and study experimental features.</p> 
<p>The name "Forge" is inspired from "Minecraft Forge". This project is aimed at becoming SD WebUI's Forge.</p> 
<p>Forge is currently based on SD-WebUI 1.10.1 at <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/82a973c04367123ae98bd9abdf80d9eda9b910e2">this commit</a>. (Because original SD-WebUI is almost static now, Forge will sync with original WebUI every 90 days, or when important fixes.)</p> 
<h1>Quick List</h1> 
<p><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/discussions/853">Gradio 4 UI Must Read (TLDR: You need to use RIGHT MOUSE BUTTON to move canvas!)</a></p> 
<p><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/discussions/981">Flux Tutorial (BitsandBytes Models, NF4, "GPU Weight", "Offload Location", "Offload Method", etc)</a></p> 
<p><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/discussions/1050">Flux Tutorial 2 (Seperated Full Models, GGUF, Technically Correct Comparison between GGUF and NF4, etc)</a></p> 
<p><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/discussions/1181">Report Flux Performance Problems (TLDR: DO NOT set "GPU Weight" too high! Lower "GPU Weight" solves 99% problems!)</a></p> 
<p><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/discussions/1224#discussioncomment-10384104">(Save Flux BitsandBytes UNet/Checkpoint)</a></p> 
<p><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/discussions/854">LayerDiffuse Transparent Image Editing</a></p> 
<p><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/discussions/1286">(Policy) Soft Advertisement Removal Policy</a></p> 
<p>(Flux BNB NF4 / GGUF Q8_0/Q5_0/Q5_1/Q4_0/Q4_1 are all natively supported with GPU weight slider and Quene/Async Swap toggle and swap location toggle. All Flux BNB NF4 / GGUF Q8_0/Q5_0/Q4_0 have LoRA support.)</p> 
<h1>Installing Forge</h1> 
<p><strong>Just use this one-click installation package (with git and python included).</strong></p> 
<p><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/releases/download/latest/webui_forge_cu121_torch231.7z">&gt;&gt;&gt; Click Here to Download One-Click Package (CUDA 12.1 + Pytorch 2.3.1) &lt;&lt;&lt;</a></p> 
<p>Some other CUDA/Torch Versions:</p> 
<p><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/releases/download/latest/webui_forge_cu121_torch231.7z">Forge with CUDA 12.1 + Pytorch 2.3.1</a> &lt;- <strong>Recommended</strong></p> 
<p><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/releases/download/latest/webui_forge_cu124_torch24.7z">Forge with CUDA 12.4 + Pytorch 2.4</a> &lt;- <strong>Fastest</strong>, but MSVC may be broken, xformers may not work</p> 
<p><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/releases/download/latest/webui_forge_cu121_torch21.7z">Forge with CUDA 12.1 + Pytorch 2.1</a> &lt;- the previously used old environments</p> 
<p>After you download, you uncompress, use <code>update.bat</code> to update, and use <code>run.bat</code> to run.</p> 
<p>Note that running <code>update.bat</code> is important, otherwise you may be using a previous version with potential bugs unfixed.</p> 
<p><img alt="image" src="https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/c49bd60d-82bd-4086-9859-88d472582b94" /></p> 
<h3>Advanced Install</h3> 
<p>If you are proficient in Git and you want to install Forge as another branch of SD-WebUI, please see <a href="https://github.com/continue-revolution/sd-webui-animatediff/raw/forge/master/docs/how-to-use.md#you-have-a1111-and-you-know-git">here</a>. In this way, you can reuse all SD checkpoints and all extensions you installed previously in your OG SD-WebUI, but you should know what you are doing.</p> 
<p>If you know what you are doing, you can also install Forge using same method as SD-WebUI. (Install Git, Python, Git Clone the forge repo <code>https://github.com/lllyasviel/stable-diffusion-webui-forge.git</code> and then run webui-user.bat).</p> 
<h3>Previous Versions</h3> 
<p>You can download previous versions <a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/discussions/849">here</a>.</p> 
<h1>Forge Status</h1> 
<p>Based on manual test one-by-one:</p> 
<table> 
 <thead> 
  <tr> 
   <th>Component</th> 
   <th>Status</th> 
   <th>Last Test</th> 
  </tr> 
 </thead> 
 <tbody> 
  <tr> 
   <td>Basic Diffusion</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>GPU Memory Management System</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>LoRAs</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>All Preprocessors</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>All ControlNets</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>All IP-Adapters</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>All Instant-IDs</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>All Reference-only Methods</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>All Integrated Extensions</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>Popular Extensions (Adetailer, etc)</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>Gradio 4 UIs</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>Gradio 4 Forge Canvas</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>LoRA/Checkpoint Selection UI for Gradio 4</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>Photopea/OpenposeEditor/etc for ControlNet</td> 
   <td>Normal</td> 
   <td>2024 July 27</td> 
  </tr> 
  <tr> 
   <td>Wacom 128 level touch pressure support for Canvas</td> 
   <td>Normal</td> 
   <td>2024 July 15</td> 
  </tr> 
  <tr> 
   <td>Microsoft Surface touch pressure support for Canvas</td> 
   <td>Broken, pending fix</td> 
   <td>2024 July 29</td> 
  </tr> 
  <tr> 
   <td>txt2img and img2img API Endpoints</td> 
   <td>Broken, pending fix</td> 
   <td>2024 July 29</td> 
  </tr> 
 </tbody> 
</table> 
<p>Feel free to open issue if anything is broken and I will take a look every several days. If I do not update this "Forge Status" then it means I cannot reproduce any problem. In that case, fresh re-install should help most.</p> 
<h1>UnetPatcher</h1> 
<p>Below are self-supported <strong>single file</strong> of all codes to implement FreeU V2.</p> 
<p>See also <code>extension-builtin/sd_forge_freeu/scripts/forge_freeu.py</code>:</p> 
<pre><code class="language-python">import torch
import gradio as gr

from modules import scripts


def Fourier_filter(x, threshold, scale):
    # FFT
    x_freq = torch.fft.fftn(x.float(), dim=(-2, -1))
    x_freq = torch.fft.fftshift(x_freq, dim=(-2, -1))

    B, C, H, W = x_freq.shape
    mask = torch.ones((B, C, H, W), device=x.device)

    crow, ccol = H // 2, W // 2
    mask[..., crow - threshold:crow + threshold, ccol - threshold:ccol + threshold] = scale
    x_freq = x_freq * mask

    # IFFT
    x_freq = torch.fft.ifftshift(x_freq, dim=(-2, -1))
    x_filtered = torch.fft.ifftn(x_freq, dim=(-2, -1)).real

    return x_filtered.to(x.dtype)


def patch_freeu_v2(unet_patcher, b1, b2, s1, s2):
    model_channels = unet_patcher.model.diffusion_model.config["model_channels"]
    scale_dict = {model_channels * 4: (b1, s1), model_channels * 2: (b2, s2)}
    on_cpu_devices = {}

    def output_block_patch(h, hsp, transformer_options):
        scale = scale_dict.get(h.shape[1], None)
        if scale is not None:
            hidden_mean = h.mean(1).unsqueeze(1)
            B = hidden_mean.shape[0]
            hidden_max, _ = torch.max(hidden_mean.view(B, -1), dim=-1, keepdim=True)
            hidden_min, _ = torch.min(hidden_mean.view(B, -1), dim=-1, keepdim=True)
            hidden_mean = (hidden_mean - hidden_min.unsqueeze(2).unsqueeze(3)) / (hidden_max - hidden_min).unsqueeze(2).unsqueeze(3)

            h[:, :h.shape[1] // 2] = h[:, :h.shape[1] // 2] * ((scale[0] - 1) * hidden_mean + 1)

            if hsp.device not in on_cpu_devices:
                try:
                    hsp = Fourier_filter(hsp, threshold=1, scale=scale[1])
                except:
                    print("Device", hsp.device, "does not support the torch.fft.")
                    on_cpu_devices[hsp.device] = True
                    hsp = Fourier_filter(hsp.cpu(), threshold=1, scale=scale[1]).to(hsp.device)
            else:
                hsp = Fourier_filter(hsp.cpu(), threshold=1, scale=scale[1]).to(hsp.device)

        return h, hsp

    m = unet_patcher.clone()
    m.set_model_output_block_patch(output_block_patch)
    return m


class FreeUForForge(scripts.Script):
    sorting_priority = 12  # It will be the 12th item on UI.

    def title(self):
        return "FreeU Integrated"

    def show(self, is_img2img):
        # make this extension visible in both txt2img and img2img tab.
        return scripts.AlwaysVisible

    def ui(self, *args, **kwargs):
        with gr.Accordion(open=False, label=self.title()):
            freeu_enabled = gr.Checkbox(label='Enabled', value=False)
            freeu_b1 = gr.Slider(label='B1', minimum=0, maximum=2, step=0.01, value=1.01)
            freeu_b2 = gr.Slider(label='B2', minimum=0, maximum=2, step=0.01, value=1.02)
            freeu_s1 = gr.Slider(label='S1', minimum=0, maximum=4, step=0.01, value=0.99)
            freeu_s2 = gr.Slider(label='S2', minimum=0, maximum=4, step=0.01, value=0.95)

        return freeu_enabled, freeu_b1, freeu_b2, freeu_s1, freeu_s2

    def process_before_every_sampling(self, p, *script_args, **kwargs):
        # This will be called before every sampling.
        # If you use highres fix, this will be called twice.

        freeu_enabled, freeu_b1, freeu_b2, freeu_s1, freeu_s2 = script_args

        if not freeu_enabled:
            return

        unet = p.sd_model.forge_objects.unet

        unet = patch_freeu_v2(unet, freeu_b1, freeu_b2, freeu_s1, freeu_s2)

        p.sd_model.forge_objects.unet = unet

        # Below codes will add some logs to the texts below the image outputs on UI.
        # The extra_generation_params does not influence results.
        p.extra_generation_params.update(dict(
            freeu_enabled=freeu_enabled,
            freeu_b1=freeu_b1,
            freeu_b2=freeu_b2,
            freeu_s1=freeu_s1,
            freeu_s2=freeu_s2,
        ))

        return
</code></pre> 
<p>See also <a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/raw/main/backend/nn/unet.py">Forge's Unet Implementation</a>.</p> 
<h1>Under Construction</h1> 
<p>WebUI Forge is now under some constructions, and docs / UI / functionality may change with updates.</p>
]]></content:encoded>
<pubDate></pubDate>
</item>
<item>
<title>geekan/MetaGPT</title>
<link>https://github.com/geekan/MetaGPT</link>
<guid>https://github.com/geekan/MetaGPT</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯ï¼šMetaGPTã€è½¯ä»¶å…¬å¸ã€å¤šä»£ç†ç³»ç»Ÿã€è‡ªç„¶è¯­è¨€ç¼–ç¨‹ã€LLM

æ€»ç»“ï¼š

MetaGPTæ˜¯ä¸€æ¬¾åŸºäºäººå·¥æ™ºèƒ½çš„è½¯ä»¶å…¬å¸æ¨¡æ‹Ÿå¹³å°ï¼Œæ—¨åœ¨é€šè¿‡å¤šä»£ç†ç³»ç»Ÿå®ç°å¤æ‚ä»»åŠ¡çš„åä½œå®Œæˆã€‚å…¶æ ¸å¿ƒç†å¿µæ˜¯å°†â€œä»£ç ç­‰äºæµç¨‹ï¼ˆSOPï¼‰â€ï¼Œå³é€šè¿‡æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆLLMï¼‰æ¥æ¨¡æ‹Ÿè½¯ä»¶å¼€å‘å…¬å¸ä¸­çš„å„ä¸ªè§’è‰²å’Œæµç¨‹ã€‚MetaGPTæ”¯æŒç”¨æˆ·é€šè¿‡ä¸€å¥éœ€æ±‚æè¿°ç”Ÿæˆé¡¹ç›®è§„åˆ’ã€ç”¨æˆ·æ•…äº‹ã€ç«äº‰åˆ†æã€æ•°æ®ç»“æ„ã€APIè®¾è®¡ç­‰æ–‡æ¡£ï¼Œç”šè‡³å¯ä»¥ä½œä¸ºåº“ç›´æ¥è°ƒç”¨ï¼Œå®ç°ä»æ¦‚å¿µåˆ°ä»£ç çš„è‡ªåŠ¨åŒ–è¿‡ç¨‹ã€‚

MetaGPTæä¾›äº†ä¸€ä¸ªå¼€æ”¾æºä»£ç çš„æ¡†æ¶ï¼Œå…è®¸ç”¨æˆ·é…ç½®ä¸åŒçš„LLMæ¨¡å‹ä»¥æ‰®æ¼”ä¸åŒçš„è§’è‰²ï¼Œå¦‚äº§å“ç»ç†ã€æ¶æ„å¸ˆã€é¡¹ç›®ç»ç†å’Œå·¥ç¨‹å¸ˆã€‚è¿™ä½¿å¾—ç”¨æˆ·å¯ä»¥æ ¹æ®å®é™…éœ€æ±‚çµæ´»åœ°ç»„åˆå’Œè°ƒæ•´AIä»£ç†çš„è§’è‰²å’Œèƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒMetaGPTè¿˜æ”¯æŒå¤šç§è¯­è¨€ã€å¤šæ¨¡æ€è¾“å…¥å’Œå¢é‡å¼€å‘åŠŸèƒ½ï¼Œå¢å¼ºäº†å…¶å®ç”¨æ€§å’Œçµæ´»æ€§ã€‚

åœ¨å‘å±•è¿‡ç¨‹ä¸­ï¼ŒMetaGPTä¸æ–­æ›´æ–°å’Œä¼˜åŒ–ï¼Œå¢åŠ äº†å¯¹RAGï¼ˆé˜…è¯»ç†è§£+ä»£ç ç”Ÿæˆï¼‰æ¨¡å—çš„æ”¯æŒï¼Œå¼•å…¥äº†æ›´å¤šå¼ºå¤§çš„LLMæ¨¡å‹ï¼Œå¹¶é€šè¿‡é›†æˆä¸åŒçš„APIå’Œå·¥å…·æ‰©å±•äº†å…¶åŠŸèƒ½ã€‚è¿™äº›è¿›å±•ä½¿å¾—MetaGPTåœ¨è½¯ä»¶å¼€å‘é¢†åŸŸå±•ç°äº†å¼ºå¤§çš„æ½œåŠ›ï¼Œæˆä¸ºäº†ä¸€ä¸ªå€¼å¾—å…³æ³¨çš„å¼€æºé¡¹ç›®ã€‚

MetaGPTä¸ä»…æ˜¯ä¸€ä¸ªæŠ€æœ¯åˆ›æ–°çš„ä½“ç°ï¼Œä¹Ÿæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†å’Œäººå·¥æ™ºèƒ½åœ¨è½¯ä»¶å·¥ç¨‹é¢†åŸŸçš„åº”ç”¨æ¢ç´¢ã€‚å®ƒä¸ºå¼€å‘è€…æä¾›äº†å…¨æ–°çš„å·¥ä½œæ–¹å¼ï¼Œæœ‰æœ›åŠ é€Ÿè½¯ä»¶å¼€å‘æµç¨‹å¹¶æå‡å¼€å‘è´¨é‡ã€‚ <div>
<p>ğŸŒŸ The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming</p><hr /><h1>MetaGPT: The Multi-Agent Framework</h1> 
<p align="center"> <a href=""><img alt="MetaGPT logo: Enable GPT to work in software company, collaborating to tackle more complex tasks." src="https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/resources/MetaGPT-new-log.png" width="150px" /></a> </p> 
<p align="center"> <b>Assign different roles to GPTs to form a collaborative entity for complex tasks.</b> </p> 
<p align="center"> <a href="https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/README_CN.md"><img alt="CN doc" src="https://img.shields.io/badge/%E6%96%87%E6%A1%A3-%E4%B8%AD%E6%96%87%E7%89%88-blue.svg?sanitize=true" /></a> <a href="https://raw.githubusercontent.com/geekan/MetaGPT/main/README.md"><img alt="EN doc" src="https://img.shields.io/badge/document-English-blue.svg?sanitize=true" /></a> <a href="https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/README_JA.md"><img alt="JA doc" src="https://img.shields.io/badge/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88-%E6%97%A5%E6%9C%AC%E8%AA%9E-blue.svg?sanitize=true" /></a> <a href="https://opensource.org/licenses/MIT"><img alt="License: MIT" src="https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true" /></a> <a href="https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/ROADMAP.md"><img alt="roadmap" src="https://img.shields.io/badge/ROADMAP-%E8%B7%AF%E7%BA%BF%E5%9B%BE-blue" /></a> <a href="https://discord.gg/DYn29wFk9z"><img alt="Discord Follow" src="https://dcbadge.vercel.app/api/server/DYn29wFk9z?style=flat" /></a> <a href="https://twitter.com/MetaGPT_"><img alt="Twitter Follow" src="https://img.shields.io/twitter/follow/MetaGPT?style=social" /></a> </p> 
<p align="center"> <a href="https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/geekan/MetaGPT"><img alt="Open in Dev Containers" src="https://img.shields.io/static/v1?label=Dev%20Containers&amp;message=Open&amp;color=blue&amp;logo=visualstudiocode" /></a> <a href="https://codespaces.new/geekan/MetaGPT"><img alt="Open in GitHub Codespaces" src="https://img.shields.io/badge/Github_Codespace-Open-blue?logo=github" /></a> <a href="https://huggingface.co/spaces/deepwisdom/MetaGPT" target="_blank"><img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20-Hugging%20Face-blue?color=blue&amp;logoColor=white" /></a> </p> 
<h2>News</h2> 
<p>ğŸš€ Mar. 29, 2024: <a href="https://github.com/geekan/MetaGPT/releases/tag/v0.8.0">v0.8.0</a> released. Now you can use Data Interpreter (<a href="https://arxiv.org/abs/2402.18679">arxiv</a>, <a href="https://docs.deepwisdom.ai/main/en/DataInterpreter/">example</a>, <a href="https://github.com/geekan/MetaGPT/tree/main/examples/di">code</a>) via pypi package import. Meanwhile, we integrated RAG module and supported multiple new LLMs.</p> 
<p>ğŸš€ Feb. 08, 2024: <a href="https://github.com/geekan/MetaGPT/releases/tag/v0.7.0">v0.7.0</a> released, supporting assigning different LLMs to different Roles. We also introduced <a href="https://github.com/geekan/MetaGPT/raw/main/examples/di/README.md">Data Interpreter</a>, a powerful agent capable of solving a wide range of real-world problems.</p> 
<p>ğŸš€ Jan. 16, 2024: Our paper <a href="https://openreview.net/forum?id=VtmBAGCN7o">MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework </a> accepted for <strong>oral presentation (top 1.2%)</strong> at ICLR 2024, <strong>ranking #1</strong> in the LLM-based Agent category.</p> 
<p>ğŸš€ Jan. 03, 2024: <a href="https://github.com/geekan/MetaGPT/releases/tag/v0.6.0">v0.6.0</a> released, new features include serialization, upgraded OpenAI package and supported multiple LLM, provided <a href="https://github.com/geekan/MetaGPT/raw/main/examples/debate_simple.py">minimal example for debate</a> etc.</p> 
<p>ğŸš€ Dec. 15, 2023: <a href="https://github.com/geekan/MetaGPT/releases/tag/v0.5.0">v0.5.0</a> released, introducing some experimental features such as incremental development, multilingual, multiple programming languages, etc.</p> 
<p>ğŸ”¥ Nov. 08, 2023: MetaGPT is selected into <a href="https://www.benchcouncil.org/evaluation/opencs/annual.html">Open100: Top 100 Open Source achievements</a>.</p> 
<p>ğŸ”¥ Sep. 01, 2023: MetaGPT tops GitHub Trending Monthly for the <strong>17th time</strong> in August 2023.</p> 
<p>ğŸŒŸ Jun. 30, 2023: MetaGPT is now open source.</p> 
<p>ğŸŒŸ Apr. 24, 2023: First line of MetaGPT code committed.</p> 
<h2>Software Company as Multi-Agent System</h2> 
<ol> 
 <li>MetaGPT takes a <strong>one line requirement</strong> as input and outputs <strong>user stories / competitive analysis / requirements / data structures / APIs / documents, etc.</strong></li> 
 <li>Internally, MetaGPT includes <strong>product managers / architects / project managers / engineers.</strong> It provides the entire process of a <strong>software company along with carefully orchestrated SOPs.</strong> 
  <ol> 
   <li><code>Code = SOP(Team)</code> is the core philosophy. We materialize SOP and apply it to teams composed of LLMs.</li> 
  </ol> </li> 
</ol> 
<p><img alt="A software company consists of LLM-based roles" src="https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/resources/software_company_cd.jpeg" /></p> 
<p align="center">Software Company Multi-Agent Schematic (Gradually Implementing)</p> 
<h2>Get Started</h2> 
<h3>Installation</h3> 
<blockquote> 
 <p>Ensure that Python 3.9+ is installed on your system. You can check this by using: <code>python --version</code>.<br /> You can use conda like this: <code>conda create -n metagpt python=3.9 &amp;&amp; conda activate metagpt</code></p> 
</blockquote> 
<pre><code class="language-bash">pip install --upgrade metagpt
# or `pip install --upgrade git+https://github.com/geekan/MetaGPT.git`
# or `git clone https://github.com/geekan/MetaGPT &amp;&amp; cd MetaGPT &amp;&amp; pip install --upgrade -e .`
</code></pre> 
<p>For detailed installation guidance, please refer to <a href="https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-stable-version">cli_install</a> or <a href="https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-with-docker">docker_install</a></p> 
<h3>Configuration</h3> 
<p>You can init the config of MetaGPT by running the following command, or manually create <code>~/.metagpt/config2.yaml</code> file:</p> 
<pre><code class="language-bash"># Check https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html for more details
metagpt --init-config  # it will create ~/.metagpt/config2.yaml, just modify it to your needs
</code></pre> 
<p>You can configure <code>~/.metagpt/config2.yaml</code> according to the <a href="https://github.com/geekan/MetaGPT/raw/main/config/config2.example.yaml">example</a> and <a href="https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html">doc</a>:</p> 
<pre><code class="language-yaml">llm:
  api_type: "openai"  # or azure / ollama / groq etc. Check LLMType for more options
  model: "gpt-4-turbo"  # or gpt-3.5-turbo
  base_url: "https://api.openai.com/v1"  # or forward url / other llm url
  api_key: "YOUR_API_KEY"
</code></pre> 
<h3>Usage</h3> 
<p>After installation, you can use MetaGPT at CLI</p> 
<pre><code class="language-bash">metagpt "Create a 2048 game"  # this will create a repo in ./workspace
</code></pre> 
<p>or use it as library</p> 
<pre><code class="language-python">from metagpt.software_company import generate_repo, ProjectRepo
repo: ProjectRepo = generate_repo("Create a 2048 game")  # or ProjectRepo("&lt;path&gt;")
print(repo)  # it will print the repo structure with files
</code></pre> 
<p>You can also use <a href="https://github.com/geekan/MetaGPT/tree/main/examples/di">Data Interpreter</a> to write code:</p> 
<pre><code class="language-python">import asyncio
from metagpt.roles.di.data_interpreter import DataInterpreter

async def main():
    di = DataInterpreter()
    await di.run("Run data analysis on sklearn Iris dataset, include a plot")

asyncio.run(main())  # or await main() in a jupyter notebook setting
</code></pre> 
<h3>QuickStart &amp; Demo Video</h3> 
<ul> 
 <li>Try it on <a href="https://huggingface.co/spaces/deepwisdom/MetaGPT">MetaGPT Huggingface Space</a></li> 
 <li><a href="https://youtu.be/uT75J_KG_aY">Matthew Berman: How To Install MetaGPT - Build A Startup With One Prompt!!</a></li> 
 <li><a href="https://github.com/geekan/MetaGPT/assets/2707039/5e8c1062-8c35-440f-bb20-2b0320f8d27d">Official Demo Video</a></li> 
</ul> 
<p><a href="https://github.com/geekan/MetaGPT/assets/34952977/34345016-5d13-489d-b9f9-b82ace413419">https://github.com/geekan/MetaGPT/assets/34952977/34345016-5d13-489d-b9f9-b82ace413419</a></p> 
<h2>Tutorial</h2> 
<ul> 
 <li>ğŸ—’ <a href="https://docs.deepwisdom.ai/main/en/">Online Document</a></li> 
 <li>ğŸ’» <a href="https://docs.deepwisdom.ai/main/en/guide/get_started/quickstart.html">Usage</a></li> 
 <li>ğŸ” <a href="https://docs.deepwisdom.ai/main/en/guide/get_started/introduction.html">What can MetaGPT do?</a></li> 
 <li>ğŸ›  How to build your own agents? 
  <ul> 
   <li><a href="https://docs.deepwisdom.ai/main/en/guide/tutorials/agent_101.html">MetaGPT Usage &amp; Development Guide | Agent 101</a></li> 
   <li><a href="https://docs.deepwisdom.ai/main/en/guide/tutorials/multi_agent_101.html">MetaGPT Usage &amp; Development Guide | MultiAgent 101</a></li> 
  </ul> </li> 
 <li>ğŸ§‘â€ğŸ’» Contribution 
  <ul> 
   <li><a href="https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/ROADMAP.md">Develop Roadmap</a></li> 
  </ul> </li> 
 <li>ğŸ”– Use Cases 
  <ul> 
   <li><a href="https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/interpreter/intro.html">Data Interpreter</a></li> 
   <li><a href="https://docs.deepwisdom.ai/main/en/guide/use_cases/multi_agent/debate.html">Debate</a></li> 
   <li><a href="https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/researcher.html">Researcher</a></li> 
   <li><a href="https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/receipt_assistant.html">Recepit Assistant</a></li> 
  </ul> </li> 
 <li>â“ <a href="https://docs.deepwisdom.ai/main/en/guide/faq.html">FAQs</a></li> 
</ul> 
<h2>Support</h2> 
<h3>Discord Join US</h3> 
<p>ğŸ“¢ Join Our <a href="https://discord.gg/ZRHeExS6xv">Discord Channel</a>! Looking forward to seeing you there! ğŸ‰</p> 
<h3>Contributor form</h3> 
<p>ğŸ“ <a href="https://airtable.com/appInfdG0eJ9J4NNL/pagK3Fh1sGclBvVkV/form">Fill out the form</a> to become a contributor. We are looking forward to your participation!</p> 
<h3>Contact Information</h3> 
<p>If you have any questions or feedback about this project, please feel free to contact us. We highly appreciate your suggestions!</p> 
<ul> 
 <li><strong>Email:</strong> <a href="mailto:alexanderwu@deepwisdom.ai">alexanderwu@deepwisdom.ai</a></li> 
 <li><strong>GitHub Issues:</strong> For more technical inquiries, you can also create a new issue in our <a href="https://github.com/geekan/metagpt/issues">GitHub repository</a>.</li> 
</ul> 
<p>We will respond to all questions within 2-3 business days.</p> 
<h2>Citation</h2> 
<p>To stay updated with the latest research and development, follow <a href="https://twitter.com/MetaGPT_">@MetaGPT_</a> on Twitter.</p> 
<p>To cite <a href="https://openreview.net/forum?id=VtmBAGCN7o">MetaGPT</a> or <a href="https://arxiv.org/abs/2402.18679">Data Interpreter</a> in publications, please use the following BibTeX entries.</p> 
<pre><code class="language-bibtex">@inproceedings{hong2024metagpt,
      title={Meta{GPT}: Meta Programming for A Multi-Agent Collaborative Framework},
      author={Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Jinlin Wang and Ceyao Zhang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and J{\"u}rgen Schmidhuber},
      booktitle={The Twelfth International Conference on Learning Representations},
      year={2024},
      url={https://openreview.net/forum?id=VtmBAGCN7o}
}
@misc{hong2024data,
      title={Data Interpreter: An LLM Agent For Data Science}, 
      author={Sirui Hong and Yizhang Lin and Bang Liu and Bangbang Liu and Binhao Wu and Danyang Li and Jiaqi Chen and Jiayi Zhang and Jinlin Wang and Li Zhang and Lingyao Zhang and Min Yang and Mingchen Zhuge and Taicheng Guo and Tuo Zhou and Wei Tao and Wenyi Wang and Xiangru Tang and Xiangtao Lu and Xiawu Zheng and Xinbing Liang and Yaying Fei and Yuheng Cheng and Zongze Xu and Chenglin Wu},
      year={2024},
      eprint={2402.18679},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
</code></pre>
]]></content:encoded>
<pubDate></pubDate>
</item>
</channel>
</rss>