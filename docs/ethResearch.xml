<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Ethereum Research - Latest topics</title>
<link>https://ethresear.ch/latest</link>

<item>
<title>AI4Science Oracle for DeSci: Revolutionizing Scientific Discovery with Blockchain</title>
<link>https://ethresear.ch/t/ai4science-oracle-for-desci-revolutionizing-scientific-discovery-with-blockchain/21114</link>
<guid>https://ethresear.ch/t/ai4science-oracle-for-desci-revolutionizing-scientific-discovery-with-blockchain/21114</guid>
<content:encoded><![CDATA[
<div> 关键词：AI4Science Oracle、区块链、正式验证、新颖性评估、代币奖励

总结:
该项目提出构建一个AI4Science Oracle，这是一个去中心化且可验证的平台，旨在激励人工智能在科学领域的开发。该平台鼓励个人贡献科学公式、发现或猜想，并通过正式验证和基于AI的新颖性评估来评价其正确性和新颖性。对满足条件的贡献者给予代币奖励。通过将新颖的科学发现系统地整合进AI模型中，项目旨在加速AI在科学研究中的能力，并促进一个推动人类与AI在科学发现中合作的社区生态系统的发展。具体流程包括提交提案、验证阶段（正确性验证与新颖性评估）、奖励机制、挑战期、集成到AI模型以及采用分布式执行和可验证的方式进行操作。最终，这个项目将为跨科学领域的人类与AI贡献搭建桥梁，推动科学发现的进步。 <div>
<p>By <a href="https://x.com/0xKDConway" rel="noopener nofollow ugc">KD.Conway</a> and <a href="https://t.me/magic_Cxsea" rel="noopener nofollow ugc">Chelsea</a></p>
<h2><a class="anchor" href="https://ethresear.ch#p-51453-abstract-1" name="p-51453-abstract-1"></a>Abstract</h2>
<p>This proposal outlines a project to build an <strong>AI4Science Oracle</strong>, a decentralized and verifiable platform that incentivizes the development of AI for science. The platform enables individuals to contribute scientific formulas, findings, or conjectures, which are evaluated for correctness and novelty using formal verification and AI-based novelty assessment. Contributors who meet these criteria are rewarded with tokens. The project aims to accelerate AI’s capabilities in science by systematically integrating novel scientific findings into AI models, fostering a community-driven ecosystem for advancing human and AI collaboration in scientific discovery.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51453-background-2" name="p-51453-background-2"></a>Background</h2>
<ul>
<li>
<p><strong>AI4Science (AI for Science):</strong> uses artificial intelligence to accelerate scientific discoveries, model complex phenomena, and solve problems across disciplines like biology, physics, and medicine.</p>
</li>
<li>
<p><strong>DeSci (Decentralized Science)</strong> leverages decentralized technologies, such as blockchain and DAOs, to make scientific research more transparent, accessible, and community-driven.</p>
</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-51453-workflow-3" name="p-51453-workflow-3"></a>Workflow</h2>
<p>The <strong>AI4Science Oracle</strong> operates through a systematic and decentralized workflow, ensuring that contributions are validated, incentivized, and integrated into the broader AI4Science ecosystem. Let’s take AI4Math as an example.The steps are as follows:</p>
<h4><a class="anchor" href="https://ethresear.ch#p-51453-h-1-submission-of-proposals-4" name="p-51453-h-1-submission-of-proposals-4"></a>1. <strong>Submission of Proposals</strong></h4>
<ul>
<li>
<p>Individuals submit their mathematical formulas, findings, or conjectures to the AI4Science Oracle via a decentralized platform.</p>
</li>
<li>
<p>Each submission is recorded on the blockchain to ensure transparency and immutability.</p>
</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-51453-h-2-validation-phase-5" name="p-51453-h-2-validation-phase-5"></a>2. <strong>Validation Phase</strong></h4>
<p>The oracle conducts two core tests to evaluate the submissions:</p>
<ul>
<li>
<p><strong>Correctness Verification</strong>:</p>
<ul>
<li>The oracle uses formal verification, random testing, and other validation techniques to ensure the correctness of the submitted results.</li>
</ul>
</li>
<li>
<p><strong>Novelty Assessment</strong>:</p>
<ul>
<li>
<p>Leveraging the AI4Science model, the oracle evaluates the “novelty distance” between the proposed result and existing knowledge.</p>
</li>
<li>
<p>Submissions that can be easily extrapolated or derived from known formulas fail the novelty test, ensuring only significant contributions pass.</p>
</li>
</ul>
</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-51453-h-3-rewards-6" name="p-51453-h-3-rewards-6"></a>3. <strong>Rewards</strong></h4>
<ul>
<li>
<p>Submissions that pass both tests earn the proposer AI4Science tokens as a reward for their contribution.</p>
</li>
<li>
<p>Tokens are minted exclusively for those who contribute validated results, ensuring value is tied to scientific advancement.</p>
</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-51453-h-4-challenge-period-7" name="p-51453-h-4-challenge-period-7"></a>4. <strong>Challenge Period</strong></h4>
<ul>
<li>
<p>Once a proposal passes the initial validation, it enters a public challenge period.</p>
</li>
<li>
<p>During this time, anyone can challenge the proposal by:</p>
<ul>
<li>
<p><strong>Providing a Counterexample</strong>: Demonstrating the incorrectness of the submission.</p>
</li>
<li>
<p><strong>Submitting a Formal Proof</strong>: Proving the correctness or invalidity of the proposal.</p>
</li>
</ul>
</li>
<li>
<p>Successful challengers receive AI4Science tokens as a reward for their contributions to the validation process.</p>
</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-51453-h-5-integration-into-ai4science-models-8" name="p-51453-h-5-integration-into-ai4science-models-8"></a>5. <strong>Integration into AI4Science Models</strong></h4>
<ul>
<li>
<p>Proposals deemed correct and novel after the challenge period are integrated into the AI4Science model.</p>
</li>
<li>
<p>These validated findings are used to improve the AI model’s reasoning, capabilities, and knowledge base, accelerating its ability to solve more complex problems.</p>
</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-51453-h-6-decentralized-execution-and-verifiability-9" name="p-51453-h-6-decentralized-execution-and-verifiability-9"></a>6. <strong>Decentralized Execution and Verifiability</strong></h4>
<ul>
<li>
<p>All operations, including validation, challenges, and token distribution, are conducted in a decentralized and verifiable manner.</p>
</li>
<li>
<p>Technologies such as opML (Optimistic Machine Learning) are employed to ensure trustless and verifiable execution of AI models and validation algorithms.</p>
</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-51453-h-7-ecosystem-growth-10" name="p-51453-h-7-ecosystem-growth-10"></a>7. <strong>Ecosystem Growth</strong></h4>
<ul>
<li>
<p>By continuously validating and integrating novel findings, the AI4Science Oracle incentivizes contributions and fosters a thriving community of researchers and validators.</p>
</li>
<li>
<p>The resulting improvements in AI capabilities further accelerate scientific discovery, creating a positive feedback loop of innovation.</p>
</li>
</ul>
<p>This proposal ensures a robust, transparent, and decentralized system for advancing both human and AI contributions to science.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51453-conclusion-11" name="p-51453-conclusion-11"></a>Conclusion</h2>
<p>The <strong>AI4Science Oracle</strong> bridges the gap between human ingenuity and AI capabilities, offering a decentralized, transparent, and incentivized approach to advancing scientific discovery. By rewarding contributors and challengers and continuously integrating novel findings into AI models, this project not only accelerates the development of AI4Math but also lays the foundation for broader applications across scientific fields.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/ai4science-oracle-for-desci-revolutionizing-scientific-discovery-with-blockchain/21114">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sat, 30 Nov 2024 09:41:42 +0000</pubDate>
<pubDate>Sat, 30 Nov 2024 09:41:42 +0000</pubDate>
</item>

<item>
<title>Dynamic Finalization Considering 51% Attacks</title>
<link>https://ethresear.ch/t/dynamic-finalization-considering-51-attacks/21112</link>
<guid>https://ethresear.ch/t/dynamic-finalization-considering-51-attacks/21112</guid>
<content:encoded><![CDATA[
<div> 关键词：PoS Ethereum、51%攻击、防御机制、Close Vote Detection、Emergent Dynamic Finalization

总结:
本文针对PoS以太坊网络中的攻击方法进行了分类，并特别关注了危险的51%攻击及其风险。文章提出了两种新的防御机制：Close Vote Detection（用于检测潜在的51%攻击可能性）和Emergent Dynamic Finalization（当风险升高时延迟最终确认）。文章分析了不同类型的攻击，包括最终性延迟33%攻击、双重最终性34%攻击以及控制未来和过去的短重组织与审查51%及66%攻击。其中，51%及以上的攻击因其高隐蔽性和可持续性而更具威胁。文章强调了51%攻击可能导致的巨大影响，因为成功实施后攻击者可利用利润进一步扩大其优势至66%攻击。

为了应对51%攻击，文章提出了Close Vote Detection机制，通过统计接近对半投票的槽位数量来预判攻击的可能性，并在紧急情况下动态增加所需最终确认的epoch数。Emergent Dynamic Finalization机制则是在超过一定阈值的“接近投票”发生时大幅提高最终化所需的epochs数，为社区采取社会层面的反制措施赢得时间。

总之，该提案旨在强化PoS以太坊的安全性，尤其是增强对致命性的51%攻击的防御能力，同时也指出了新防御机制可能带来的问题及未来的挑战。 <div>
<p>from <a href="https://x.com/titaniaresearch" rel="noopener nofollow ugc">Titania Research</a></p>
<p><em>Thank you <a href="https://x.com/ChenXuan_C" rel="noopener nofollow ugc">Ambition</a>, <a href="https://x.com/terencechain" rel="noopener nofollow ugc">terence</a>, <a href="https://x.com/artofkot" rel="noopener nofollow ugc">Artem</a>, Titania Research Protocol Team for discussion and feedback</em></p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51450-tldr-1" name="p-51450-tldr-1"></a>TL;DR</h2>
<p>This document classifies attack methods against PoS Ethereum and proposes countermeasures, particularly against the notably dangerous 51% attack. The main points are as follows:</p>
<ol>
<li><strong>Classification of Attack Methods:</strong> Two indicators, attack stealthability and attack sustainability, are introduced to analyze known attack methods.</li>
<li><strong>Risks of a 51% Attack:</strong> It highlights the particular danger posed by attacks where the attacker controls more than 51% of the staking ratio and explains why this is the case.</li>
<li><strong>Proposals for New Defenses:</strong> Two new mechanisms are suggested to counter the high likelihood of a 51% attack: Close Vote Detection, which detects the potential for such an attack, and Emergent Dynamic Finalization, which delays finalization when the risk is elevated.</li>
<li><strong>Concerns and Future Challenges:</strong> It addresses potential issues with the proposed mechanisms and discusses future research directions.</li>
</ol>
<p>The aim of this proposal is to enhance the security of PoS Ethereum, specifically by strengthening defenses against the perilous 51% attack.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51450-h-1-classification-of-existing-attack-methods-using-attack-stealthability-and-attack-sustainability-2" name="p-51450-h-1-classification-of-existing-attack-methods-using-attack-stealthability-and-attack-sustainability-2"></a>1. Classification of Existing Attack Methods Using Attack Stealthability and Attack Sustainability</h2>
<p>Several attack methods against PoS Ethereum are known, with potential outcomes that attackers might realistically target, including reorg, double finality, and finality delay. A crucial factor in this analysis is the staking ratio required for an attack, indicating the minimum stake necessary, which serves as a barrier to entry. However, nearly as critical is attack sustainability, which measures how continuously an attacker can maintain the attack. If an attack is sustainable, it could cause significant damage. Additionally, attack stealthability is also important, as it indicates how covertly an attacker can execute an attack. If a protocol cannot detect an attack, it becomes difficult to determine whether defensive measures are necessary. Higher values for both metrics indicate a more negative outlook from the protocol’s perspective. The representative attack methods analyzed include:</p>
<ol>
<li>Finality delay 33% attack</li>
<li>Double finality 34% attack</li>
<li>Short-reorg &amp; censoring 51% attack (control over future)</li>
<li>Short-reorg &amp; censoring 66% attack (control over past and future)</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-51450-a-finality-delay-33-attack-3" name="p-51450-a-finality-delay-33-attack-3"></a>A: Finality delay 33% attack</h3>
<p>The finality delay is an attack that can be executed with a staking ratio of 33%. The attacker prevents finalization by failing to provide 33% of attestations. A defensive measure during this attack is the inactivity leak mechanism. This mechanism identifies validators who either fail to attest or attest against the majority, reducing the staked ETH of such inactive validators. During a 33% attack, the inactivity leak activates, causing the attacker’s ETH to decrease and fall below the amount needed to sustain the finality delay. Consequently, the attack’s sustainability is relatively low and temporary, making it easier to detect due to the inactivity leak.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51450-b-double-finality-34-attack-4" name="p-51450-b-double-finality-34-attack-4"></a>B: Double finality 34% attack</h3>
<p>Double finality refers to an attack wherein the attacker submits attestations to finalize two branches simultaneously. To achieve double finality, the attacker requires a staking ratio of 34%. The attacker engages in double voting for the 34% of attestations, working to finalize both forks. Defensive measures during this attack include the slashing mechanism. Since double voting is prohibited, the attacker would lose their staked ETH, making the attack easily detectable (low undetectability). Furthermore, the substantial slashing penalty means that attacking will likely only happen once; if the attacker had the budget to attack multiple times, they would likely choose a 66% attack instead. Thus, attack sustainability for this method is also very low.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51450-c-short-reorg-censoring-51-attack-control-over-future-5" name="p-51450-c-short-reorg-censoring-51-attack-control-over-future-5"></a>C: Short-reorg &amp; censoring 51% attack (control over future)</h3>
<p>When an attacker possesses a staking ratio of 51%, they can manipulate the fork choice algorithm. Attacks A and B were directed at the Casper FFG (finality gadget), whereas this attack targets the LMD GHOST (fork choice algorithm). In this scenario, the attacker can freely create the heaviest branch in LMD GHOST, causing honest validators to follow the attacker’s branch, resulting in finalization. This enables the attacker to censor specific transactions and perform short-term reorganization (reorg) to maximize their miner extractable value (MEV) without incurring slashing penalties.</p>
<p>In attacks A and B, mechanisms existed to reduce the attacker’s potential upon occurrence. In attack A, the inactivity leak decreases the attacker’s staking ratio below the 33% threshold, rendering the attack impossible. In attack B, one-third of their staking ratio is slashed during that epoch, making repeated attacks effectively unfeasible.</p>
<p>However, there are currently no algorithmic defensive measures against attack C. Even if there is a slot with a 51% voting ratio, there is no way to distinguish whether that attestation is malicious or a legitimate disagreement among honest validators. This means that attack undetectability is significantly high. Once an attack succeeds, the attacker can persistently continue the attack until a hard fork decision is made through the social layer, resulting in very high attack sustainability.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51450-d-short-reorg-censoring-66-attack-control-over-past-and-future-6" name="p-51450-d-short-reorg-censoring-66-attack-control-over-past-and-future-6"></a>D: Short-reorg &amp; censoring 66% attack (control over past and future)</h3>
<p>In the short-reorg &amp; censoring 66% attack, the attacker can freely manipulate finalization, rewriting past chains and finalizing new branches. The characteristics of attack D are similar to attack C, with both exhibiting high undetectability and high sustainability.</p>
<p>A critical point to highlight is that after executing a 51% attack, the attacker can utilize the profits to aim for a 66% attack. The potential gains from a 51% attack are significantly higher compared to the 33% and 34% attacks, and because they incur no penalties such as inactivity leak or slashing, a successful attempt could exponentially increase their dominance.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51450-summary-of-attack-methods-7" name="p-51450-summary-of-attack-methods-7"></a>Summary of attack methods</h3>
<p>The following table summarizes the characteristics of the representative attack methods analyzed:</p>
<div class="md-table">
<table>
<thead>
<tr>
<th>Attack Method</th>
<th>Staking Ratio</th>
<th>Attack Stealthability</th>
<th>Attack Sustainability</th>
</tr>
</thead>
<tbody>
<tr>
<td>A. Finality delay attack</td>
<td>33%</td>
<td>Low</td>
<td>Low</td>
</tr>
<tr>
<td>B. Double finality attack</td>
<td>34%</td>
<td>Low</td>
<td>Low</td>
</tr>
<tr>
<td>C. Short-reorg &amp; censoring attack (control over future)</td>
<td>51%</td>
<td>High</td>
<td>High</td>
</tr>
<tr>
<td>D. Short-reorg &amp; censoring attack (control over past and future)</td>
<td>66%</td>
<td>High</td>
<td>High</td>
</tr>
</tbody>
</table>
</div><p>From this table, an interesting trend can be observed: attacks at the 33% and 34% levels (A and B) are easy to detect and exhibit low sustainability, while attacks of 51% and higher (C and D) are difficult to detect and show high sustainability, illustrating a clear dichotomy.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51450-h-2-the-potential-impact-of-a-51-attack-8" name="p-51450-h-2-the-potential-impact-of-a-51-attack-8"></a>2. The Potential Impact of a 51% Attack</h2>
<p>I would like to emphasize the importance of considering worst-case scenarios regarding the security of PoS Ethereum. Simply put, there is a real possibility that Ethereum could face a situation described as ‘game over.’ If such a scenario were to occur, all past activities and data within countless ecosystems would be rendered null and void.</p>
<p>Referring to the earlier table, attacks A and B have low levels of both attack undetectability and attack sustainability. From the perspective of an attacker, there is a high likelihood that their actions will be exposed, and these attacks tend to be short-lived.</p>
<p>In contrast, attacks C and D exhibit high levels of both attack stealthiness and sustainability. For attackers, these actions are less likely to be detected, allowing them to sustain the attack over a longer period and potentially reap immense profits. When considering which of the two attacks, C or D, to focus on, we must first pay attention to the staking ratio as a barrier to attack. While both attacks could cause significant damage, attack C, which requires a smaller absolute amount to execute, is more realistically targeted (especially considering its potential to lead to attack D). In light of these considerations, this discussion will explore defensive measures against short-reorganization and censoring 51% attacks.</p>
<p>The key issue with short-reorganization and censoring 51% attacks, as mentioned above, is their high levels of attack undetectability and sustainability, which imply that the potential damage could be extensive.</p>
<p>Let’s delve deeper into attack sustainability. The reason these attacks are sustainable is that the only defensive measure available is a hard fork through social consensus, which takes considerable time (as demonstrated by the DAO incident, which took a month from the discovery of the hack to the hard fork). During this interval, blocks and epochs finalized by the attacker will accumulate on the legitimate chain. Honest validators risk being penalized for attesting to blocks on an illegitimate chain that has become the minority despite being the canonical one. The crux of the matter lies in the fact that the number of epochs required for finalization is fixed; hence, even in emergencies, the finalization occurs over the same two epochs (approximately 13 minutes) as it does under normal circumstances.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51450-h-3-proposals-for-detecting-and-defending-against-51-attacks-9" name="p-51450-h-3-proposals-for-detecting-and-defending-against-51-attacks-9"></a>3. Proposals for Detecting and Defending Against 51% Attacks</h2>
<p>In the event of a 51% attack, we anticipate that attestations will exhibit a tight margin, such as 50.5% vs. 49.5%, and such close contests are relatively rare during normal operations. We introduce a metric to indicate the likelihood of the current epoch being attacked based on the number of slots where the head votes are ‘close.’ Furthermore, as this metric increases, the number of epochs necessary for finalization will rise exponentially. This mechanism allows for the algorithmic postponement of finalization during emergencies, enabling the community to respond to attackers through social means without requiring a hard fork. Because normal finalization periods will remain unchanged, this implementation can be seamlessly integrated without compromising user experience. We propose the close vote detection mechanism for the former and emergent dynamic finalization for the latter as defenses against 51% attacks.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51450-close-vote-detection-10" name="p-51450-close-vote-detection-10"></a>Close Vote Detection</h3>
<p>When a 51% attack occurs, attackers will deliberately choose a head that appears canonical by being the heaviest. Honest validators can still propose blocks, but attackers can easily manipulate the canonical head through short-term reorganizations whenever they find the proposed blocks undesirable. The closer the attacker’s staking ratio is to 50%, the closer the amount of attestations will be to 50%. Such attestations that are very near to 50% of the head will be referred to as ‘close votes.’ Currently, the determination of whether to finalize an epoch is made at the last slot of that epoch, where we will add the counting of close votes.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51450-emergent-dynamic-finalization-11" name="p-51450-emergent-dynamic-finalization-11"></a>Emergent Dynamic Finalization</h3>
<p>If the occurrence of close votes exceeds a certain threshold, the system will recognize a state of emergency and significantly increase the number of epochs required for finalization. As a result, the attacker will need to maintain a substantial majority of votes over a longer period to achieve finalization. During this time, the community will have the opportunity to implement countermeasures. Specifically, if the number of slots classified as close votes in the current epoch exceeds a certain threshold, the required number of epochs for finalization will be raised dramatically from the standard two. We refer to this as emergency mode. While there is plenty of room for debate on what this value should be, aiming for a significant improvement over the DAO incident’s month-long delay might suggest trying a value like <span class="math">2^{15}</span>. This would require the attacker to continue their assault for about nine days (32,768 * 12 seconds ≈ 4,551,168 seconds ≈ 9 days), providing the community ample time to implement countermeasures quickly. This defensive mechanism ensures that normal network operations are unaffected and activates only during emergencies, thereby allowing for smooth implementation without degrading user experience. Moreover, since it functions algorithmically, it can be executed immediately without waiting for human judgment, allowing for rapid responses.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51450-formalization-12" name="p-51450-formalization-12"></a>Formalization</h3>
<p>Let’s define the following symbols, where <span class="math">W, E, F</span> are parameters:</p>
<ul>
<li><span class="math">i</span>: Slot index of the current epoch, ranging from 1 to 32</li>
<li><span class="math">C_i</span>: Indicates whether the voting at slot index <span class="math">i</span> is close (1) or not (0)</li>
<li><span class="math">V_i</span>: The percentage of attestations at slot index <span class="math">i</span>, expressed in %</li>
<li><span class="math">F</span>: The number of epochs required for finalization</li>
</ul>
<p>In its simplest initial form, we propose the following:</p>
<p><span class="math">
\ C_i =  \begin{cases}
1 &amp; (|V_i-0.5| &lt; W) \\
0 &amp; (otherwise)
\end{cases}
</span></p>
<p><span class="math">
\ F = \begin{cases}
D &amp; (\sum_{1\le i \le32} C_i \ge E) \\
2 &amp; (otherwise)
\end{cases}
</span></p>
<p>Here are the parameters defined:</p>
<ul>
<li><span class="math">W</span>: The percentage point deviation from 50% that qualifies as a close vote</li>
<li><span class="math">E</span>: The threshold number of close vote slots to trigger the emergency mode</li>
<li><span class="math">D</span>: The number of epochs required for finalization when in emergency mode</li>
</ul>
<p>The formulas provided define two indicators indicating the possibility of a 51% attack. First, <span class="math">C_i</span> indicates whether a specific slot is considered a close vote, yielding 1 when <span class="math">|V_i - 0.5|</span> falls within the threshold <span class="math">W</span>. Second, <span class="math">F</span> indicates the number of epochs required for finalization. Hence, if the number of close vote slots reaches the threshold <span class="math">E</span>, the required number of epochs increases to <span class="math">D</span>, thereby planning for sustained attacks and mitigating their potential impacts.</p>
<p>Let’s consider specific values:</p>
<p><span class="math">
\ W = 1\% \\
\ E = 4 \\
\ D = 2^{15}
</span></p>
<p>Thus, we have:</p>
<p><span class="math">
\ C_i =  \begin{cases}
1 \ ,&amp; \text{if} \ |V_i-0.5| &lt; 0.01 \\
0 \ ,&amp; \text{otherwise}
\end{cases}
</span></p>
<p><span class="math">
\ F = \begin{cases}
2^{15} \ , &amp; \text{if} \ \sum_{1\le i \le32} C_i \ge 4
\\
2 \ ,&amp; \text{otherwise}
\end{cases}
</span></p>
<p>With these settings, if the attestation percentage <span class="math">V_i</span> for any slot is within ±1% of 50%, that slot will be counted as a close vote. If, for instance, 4 out of the 32 slots are close votes, the total of <span class="math">C_i</span> will be 4, requiring <span class="math">F</span> to be set to <span class="math">2^{15}</span>. Consequently, the attacker will not be able to finalize the chain for approximately nine days, allowing the community enough time to implement a quick hard fork to restore the legitimate Ethereum blockchain.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51450-reducing-the-estimated-maximum-damage-13" name="p-51450-reducing-the-estimated-maximum-damage-13"></a>Reducing the estimated maximum damage</h3>
<p>The goal of this proposal is to reduce the estimated maximum damage during a 51% attack. It aims to mitigate the likelihood of a ‘game over’ scenario. While it’s challenging to discuss specific quantitative changes, it is feasible to set the parameter <span class="math">D</span> to ensure that the duration does not extend to a month like in the DAO incident. It is essential to consider that the anticipated response time from the social layer should also be factored into this aspect.</p>
<p>Moreover, various services that interact with Ethereum, such as other chains and centralized exchanges, can operate based on this <span class="math">D</span>. By introducing algorithmic mechanisms, the surrounding ecosystems will also be able to respond algorithmically.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51450-h-4-concerns-and-future-work-14" name="p-51450-h-4-concerns-and-future-work-14"></a>4. Concerns and Future Work</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51450-concerns-about-new-finality-delay-mechanisms-15" name="p-51450-concerns-about-new-finality-delay-mechanisms-15"></a>Concerns about new finality delay mechanisms</h3>
<p>There is a concern that this proposal may inadvertently create a new finality delay mechanism. For example, it is possible to randomly control 51% dominance over <span class="math">L</span> occurrences among 32 slots, which can be easily calculated using a binomial distribution. While the economic incentive to delay finality is generally low, we cannot rule out potential incentives that may not have been considered. If such incentives arise, they could potentially be addressed by introducing a reputation system. Since attestations involve signatures, attempts to impersonate other validators would require significant time to execute.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51450-scrutinizing-the-procedure-for-implementing-a-hard-fork-via-social-layer-16" name="p-51450-scrutinizing-the-procedure-for-implementing-a-hard-fork-via-social-layer-16"></a>Scrutinizing the procedure for implementing a hard fork via social layer</h3>
<p>To determine optimal parameters, we need to carefully examine the specific procedures required to execute a hard fork through the social layer.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51450-determining-parameters-w-e-d-and-formula-f-through-empirical-evidence-17" name="p-51450-determining-parameters-w-e-d-and-formula-f-through-empirical-evidence-17"></a>Determining parameters <span class="math">W, E, D</span> and formula <span class="math">F</span> through empirical evidence</h3>
<p>It is necessary to empirically determine suitable values for parameters <span class="math">W</span> (defining the range for close votes), <span class="math">E</span> (defining the threshold for emergency mode activation), and <span class="math">D</span> (defining how much to delay finalization). Additionally, <span class="math">D</span> is a component of the formula <span class="math">F</span>, but we could also consider a more dynamic design where the increase in the number of close votes <span class="math">\sum_i C_i</span> would result in a greater value for <span class="math">F</span>.</p>
<p><span class="math">
\ F = \begin{cases}
2^{\sum_i C_i - E+\text{const}} \ ,&amp; \text{if} \ \sum_i C_i \ge E
\\
2 \ ,&amp; \text{otherwise}
\end{cases}
</span></p>
<h3><a class="anchor" href="https://ethresear.ch#p-51450-determining-the-specifications-of-attestation-18" name="p-51450-determining-the-specifications-of-attestation-18"></a>Determining the specifications of attestation</h3>
<p>We need to determine the specifications for attestations.</p>
<ul>
<li>How to handle justifications during emergency mode</li>
<li>The behavior of inactivity leaks during emergency mode</li>
<li>How to specifically update the data types submitted through attestations.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-51450-h-5-conclusion-19" name="p-51450-h-5-conclusion-19"></a>5. Conclusion</h2>
<p>In this proposal, we focused on the particularly dangerous 51% attack as one of the attack methods against PoS Ethereum, discussing its risks and implications while proposing new defense strategies. Specifically, we aimed to enhance resistance to 51% attacks by introducing mechanisms like Close Vote Detection and Emergent Dynamic Finalization.</p>
<p>Future research should further explore the effectiveness of the proposed defense strategies and their applicability to other attack methods. There is also a need to continue investigating parameter optimization and specific implementation methods.</p>
<p>Additionally, analyzing attack methods against different consensus algorithms and formulating defense strategies based on social incentives are valuable directions for further discussion. I look forward to engaging with the Ethereum community about the value of these ideas and addressing any concerns.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51450-reference-20" name="p-51450-reference-20"></a>Reference</h2>
<ul>
<li><a class="inline-onebox" href="https://ethereum.org/en/developers/docs/consensus-mechanisms/pos/attack-and-defense" rel="noopener nofollow ugc">Ethereum proof-of-stake attack and defense | ethereum.org</a></li>
<li><a class="inline-onebox" href="https://ethereum.org/en/history/#dao-fork-summary" rel="noopener nofollow ugc">History and Forks of Ethereum | ethereum.org</a></li>
<li><a class="inline-onebox" href="https://www.coindesk.com/learn/understanding-the-dao-attack" rel="noopener nofollow ugc">Understanding The DAO Attack</a></li>
<li><a class="inline-onebox" href="https://blog.ethereum.org/2016/07/20/hard-fork-completed" rel="noopener nofollow ugc">Hard Fork Completed | Ethereum Foundation Blog</a></li>
</ul>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/dynamic-finalization-considering-51-attacks/21112">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 29 Nov 2024 15:28:27 +0000</pubDate>
</item>
<item>
<title>QUIC Support Among Ethereum Consensus Layer Clients</title>
<link>https://ethresear.ch/t/quic-support-among-ethereum-consensus-layer-clients/21102</link>
<guid>https://ethresear.ch/t/quic-support-among-ethereum-consensus-layer-clients/21102</guid>
<content:encoded><![CDATA[
<div> 关键词：QUIC、Ethereum共识层客户端、Nebula Crawler、IPv4、IPv6

总结:
<br /><br />
本文分析了使用Nebula Crawler对以太坊共识层（CL）客户端中QUIC协议采纳情况的研究。约有42%的CL节点支持QUIC，其中主要是由于Lighthouse客户端默认开启QUIC。虽然QUIC支持度正在增长，但主要集中在IPv4上，IPv6上的采用率极低。文章介绍了QUIC在以太坊网络中的作用以及libp2p提供的支持，并指出Lighthouse、Prysm和Grandine等客户端在QUIC支持方面的现状和差异。目前只有少量QUIC启用节点运行在IPv6上，大部分仍在IPv4上。随着规范的成熟，预计其他客户端也将逐步引入QUIC支持。ProbeLab将持续监测并报告QUIC采纳趋势，为相关利益方提供持续的洞察。 <div>
<p><strong>TL;DR</strong>: We (<a href="https://probelab.io" rel="noopener nofollow ugc">ProbeLab</a>) analyzed the adoption of QUIC among Ethereum Consensus Layer (CL) clients using the Nebula Crawler. Approximately 42% of CL nodes support QUIC, primarily because Lighthouse has it enabled by default. While QUIC support is growing, mainly over IPv4, there’s minimal adoption over IPv6. This post details our findings and encourages monitoring future trends on <a href="http://probelab.io" rel="noopener nofollow ugc">probelab.io</a>.</p>
<hr />
<p>The Ethereum network is continually evolving to improve performance, scalability, and security. One of the recent advancements is the integration of QUIC support in Ethereum Consensus Layer clients. This post examines the current state of QUIC adoption among these clients, based on data collected and analyzed using the Nebula Crawler on <a href="http://probelab.io" rel="noopener nofollow ugc">probelab.io</a>.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51431-introduction-to-quic-and-libp2p-1" name="p-51431-introduction-to-quic-and-libp2p-1"></a>Introduction to QUIC and libp2p</h2>
<p>QUIC is a transport protocol built on top of UDP, designed to enhance the performance of connection-oriented applications. It offers several benefits:</p>
<ul>
<li><strong>Reduced Latency</strong>: QUIC integrates the handshake processes, reducing connection establishment times.</li>
<li><strong>Improved Congestion Control</strong>: It adapts effectively to network conditions, enhancing throughput.</li>
<li><strong>Stream Multiplexing</strong>: Allows multiple streams within a single connection without head-of-line blocking.</li>
</ul>
<p>In the Ethereum ecosystem, libp2p provides QUIC support, enabling Consensus Layer clients to leverage these advantages for peer-to-peer communication.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51431-incorporating-quic-into-ethereums-enr-2" name="p-51431-incorporating-quic-into-ethereums-enr-2"></a>Incorporating QUIC into Ethereum’s ENR</h2>
<p>An open pull request recommends including QUIC entries in Ethereum Node Records (ENRs): <a href="https://github.com/ethereum/consensus-specs/pull/3644" rel="noopener nofollow ugc">ethereum/consensus-specs#3644</a>. Sigma Prime has been driving this initiative by adding QUIC addresses to ENRs proactively.</p>
<p>To facilitate this integration, we implemented ENR QUIC entry parsing in <a href="https://github.com/ethereum/go-ethereum" rel="noopener nofollow ugc">go-ethereum</a>: <a href="https://github.com/ethereum/go-ethereum/pull/30283" rel="noopener nofollow ugc">ethereum/go-ethereum#30283</a>. This enhancement allows clients like Prysm and tools like the Nebula Crawler to recognize and utilize QUIC addresses effectively.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51431-data-collection-methodology-3" name="p-51431-data-collection-methodology-3"></a>Data Collection Methodology</h2>
<p>At ProbeLab, we conduct two crawls per hour of the Ethereum CL network using the Nebula Crawler. We are able to identify which clients support QUIC and quantify the number of nodes for each client. The Nebula Crawler achieves this by:</p>
<ul>
<li><strong>Parsing ENRs</strong>: Extracting QUIC addresses present in the node records.</li>
<li><strong>Using libp2p’s Identify Protocol</strong>: Discovering QUIC addresses advertised by the node after opening a libp2p connection.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-51431-findings-on-quic-support-4" name="p-51431-findings-on-quic-support-4"></a>Findings on QUIC Support</h2>
<p>Our analysis reveals insightful trends in QUIC adoption among Ethereum CL clients. Below are the key observations, with full results available at <a href="https://probelab.io/ethereum/discv5/2024-46/#discv5-quic-support-plot" rel="noopener nofollow ugc">probelab.io</a>.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51431-overall-quic-support-5" name="p-51431-overall-quic-support-5"></a>Overall QUIC Support</h3>
<ul>
<li><strong>Total QUIC-enabled Nodes</strong>: Approximately 42% (~3,700 nodes) of the CL nodes support QUIC.</li>
<li><strong>Client Distribution</strong>: Out of these QUIC-supporting nodes, the vast majority are running Lighthouse.</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/7/b74e093c7d745ae0143431f134bc7cd69477d5c7.png" title="all-nodes"><img alt="all-nodes" height="375" src="https://ethresear.ch/uploads/default/optimized/3X/b/7/b74e093c7d745ae0143431f134bc7cd69477d5c7_2_489x375.png" width="489" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#p-51431-client-specific-insights-6" name="p-51431-client-specific-insights-6"></a>Client-Specific Insights</h3>
<h3><a class="anchor" href="https://ethresear.ch#p-51431-lighthouse-7" name="p-51431-lighthouse-7"></a>Lighthouse</h3>
<ul>
<li><strong>QUIC Support</strong>: An impressive 98% (~3,600 nodes) of Lighthouse nodes have QUIC enabled. Lighthouse has QUIC support enabled by default.</li>
<li><strong>Observation</strong>: Lighthouse nodes constitute the majority of QUIC-supporting nodes in the CL network.</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/1/4199a1bec6478bd55e3f58f8fa4a14a3d287552e.png" title="lighthouse"><img alt="lighthouse" height="375" src="https://ethresear.ch/uploads/default/optimized/3X/4/1/4199a1bec6478bd55e3f58f8fa4a14a3d287552e_2_489x375.png" width="489" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#p-51431-prysm-8" name="p-51431-prysm-8"></a>Prysm</h3>
<ul>
<li><strong>QUIC Support</strong>: Around 2% (~75 nodes) of Prysm nodes support QUIC.</li>
<li><strong>Note</strong>: QUIC support isn’t enabled by default in Prysm, but the presence of these nodes is encouraging for future adoption.</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/0/7/07f6ba2a90041d626df4cd63689d76e2be95ea95.png" title="prysm"><img alt="prysm" height="375" src="https://ethresear.ch/uploads/default/optimized/3X/0/7/07f6ba2a90041d626df4cd63689d76e2be95ea95_2_489x375.png" width="489" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#p-51431-grandine-9" name="p-51431-grandine-9"></a>Grandine</h3>
<ul>
<li><strong>QUIC Support</strong>: All nodes are advertising QUIC addresses.</li>
<li><strong>Comment</strong>: Grandine seems to have QUIC enabled by default but doesn’t constitute a significant portion of the network yet. An honorable mention for its proactive support.</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/c/2ca613916198d295e29411a662fb9339008d0ffa.png" title="grandine"><img alt="grandine" height="375" src="https://ethresear.ch/uploads/default/optimized/3X/2/c/2ca613916198d295e29411a662fb9339008d0ffa_2_489x375.png" width="489" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#p-51431-other-clients-10" name="p-51431-other-clients-10"></a>Other Clients</h3>
<ul>
<li><strong>Clients</strong>: Teku, Nimbus, Lodestar, Erigon.</li>
<li><strong>QUIC Support</strong>: Currently, none of these clients support QUIC.</li>
<li><strong>Expectation</strong>: With recent spec changes, we expect that these clients will incorporate QUIC support in upcoming releases.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51431-quic-over-ipv6-vs-ipv4-11" name="p-51431-quic-over-ipv6-vs-ipv4-11"></a>QUIC over IPv6 vs. IPv4</h3>
<p>Despite the growing support for QUIC, primarily due to Lighthouse, there are only a few nodes supporting QUIC over IPv6. The overwhelming majority of QUIC-enabled nodes operate over IPv4. This indicates an area for potential growth in IPv6 adoption.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51431-conclusion-and-future-outlook-12" name="p-51431-conclusion-and-future-outlook-12"></a>Conclusion and Future Outlook</h2>
<p>The integration of QUIC into Ethereum’s networking stack represents a significant step toward enhanced network performance. While Lighthouse leads in adoption, we are encouraged by the initial support seen in Prysm and Grandine. As the specification matures, we anticipate broader adoption across other clients.</p>
<p>At ProbeLab, we will continue to monitor and report on QUIC adoption trends. Our <a href="https://probelab.io/ethereum/discv5/" rel="noopener nofollow ugc">discv5 CL Weekly Reports</a> will provide ongoing insights, so we encourage stakeholders to stay tuned for further updates.</p>
<hr />
<p><em>For detailed data and interactive charts, please visit</em> <a href="https://probelab.io/" rel="noopener nofollow ugc">probelab.io</a><em>.</em></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/quic-support-among-ethereum-consensus-layer-clients/21102">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 28 Nov 2024 12:01:30 +0000</pubDate>
</item>
<item>
<title>Deterministic Consensus using Overpass Channels [𝗏.2] in Distributed Ledger Technology</title>
<link>https://ethresear.ch/t/deterministic-consensus-using-overpass-channels-2-in-distributed-ledger-technology/21046</link>
<guid>https://ethresear.ch/t/deterministic-consensus-using-overpass-channels-2-in-distributed-ledger-technology/21046</guid>
<content:encoded><![CDATA[
<div> 关键词：Overpass协议、零知识证明、单边状态通道、性能特性、安全性保证

总结:

Overpass协议是一种区块链第二层扩容解决方案，通过自我证明的单边状态通道实现了确定性共识。该协议的关键创新在于利用零知识证明（Zero-knowledge Proofs）确保了无需共识、无需对方在线即可进行单边更新，并能即时完成最终确认。此外，Overpass还提供了可证明的安全性，攻击者伪造有效状态的成功概率仅为2^{-\lambda}。

在性能特性方面，Overpass实现了O(n * m)的交易吞吐量、瞬时最终性和与树深度成对数关系的成本开销。其设计使得参与者可以独立生成和验证数学证明，从而实现无信任操作、即时最终性和低通信成本。同时，Overpass能够处理复杂的跨链操作、提供隐私增强功能，并支持递归证明以实现高效的批量验证。

安全性和效率上的突破使Overpass相对于传统的二层解决方案（如状态通道、闪电网络和Plasma）具有明显优势，包括无需依赖共识机制、对手方或观察塔，以及实现了即时的数学确定性最终性。

总的来说，Overpass协议以其独特的设计理念和高效实施为区块链技术带来了重大变革，有望应用于高频交易平台、零售支付网络、跨境支付和金融服务等多个领域，提供更快速、安全、低成本和可扩展的交易解决方案。 <div>
<p><strong>Brandon “Cryptskii” Ramsay</strong><br />
info@overpass.network<br />
November 26th 2024</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-abstract-1" name="p-51309-abstract-1"></a>Abstract</h2>
<p>This paper presents a formal analysis of the Overpass protocol, a novel layer-2 scaling solution that achieves deterministic consensus through self-proving unilateral state channels. The protocol achieves unprecedented guarantees through zero-knowledge proofs:</p>
<ul>
<li>Instant finality through self-proving state transitions</li>
<li>Unilateral updates without counterparty presence</li>
<li>Provable security bounds of <span class="math">2^{-\lambda}</span> for all attack vectors</li>
<li>Throughput scaling of <span class="math">O(n \cdot m)</span> for <span class="math">n</span> wallets and <span class="math">m</span> channels</li>
<li>Sub-logarithmic cost scaling (<span class="math">O(\log d)</span> for tree depth <span class="math">d</span>)</li>
</ul>
<p>Through rigorous mathematical proofs, we demonstrate how the protocol’s novel combination of zero-knowledge proofs and Sparse Merkle Trees (SMTs) enables individual participants to make instant, provably valid state transitions without consensus, watchtowers, or challenge periods. Each state update generates its own mathematical proof of correctness, enabling true unilateral operation while maintaining global consistency. We establish comprehensive security properties, performance characteristics, and economic guarantees through both theoretical analysis and practical examples.</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-introduction-2" name="p-51309-introduction-2"></a>Introduction</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-problem-statement-and-motivation-3" name="p-51309-problem-statement-and-motivation-3"></a>Problem Statement and Motivation</h3>
<p>Layer-2 blockchain scaling solutions face a fundamental trilemma:</p>
<ul>
<li><strong>Finality vs. Liveness:</strong> Traditional channels require both parties online or complex challenge periods</li>
<li><strong>Security vs. Speed:</strong> Existing solutions trade instant finality for probabilistic security</li>
<li><strong>Scalability vs. Cost:</strong> Higher throughput typically requires expensive validation</li>
</ul>
<p>Existing approaches face fundamental limitations:</p>
<div class="math">
\begin{split}
State_{channels} &amp;: \text{Requires bilateral presence} \\
Lightning &amp;: \text{Uses challenge periods} \\
Plasma &amp;: \text{Needs watchtowers} \\
Rollups &amp;: \text{Requires consensus}
\end{split}
</div>
<p>Consider Alice’s high-volume marketplace requirements:</p>
<ul>
<li>Transaction volume: <span class="math">&gt;10^4</span> microtransactions daily</li>
<li>Finality requirement: Instant (no waiting periods)</li>
<li>Security threshold: <span class="math">Pr[\text{double-spend}] \leq 2^{-128}</span></li>
<li>Maximum per-transaction cost: <span class="math">\leq \$0.01</span></li>
<li>Operational constraint: Must work when counterparties offline</li>
</ul>
<p>No existing layer-2 solution satisfies these constraints because they all require some form of interactive protocol, challenge period, or external validation.</p>
<hr />
<p><strong>The fundamental challenge Overpass solves:</strong> enabling thousands of fast, cheap transactions while maintaining security. Think of traditional blockchain systems like a busy highway with a single toll booth—everyone must wait in line and pay a high fee. Overpass, instead, creates multiple parallel roads (channels) with automated toll systems (cryptographic proofs), allowing many people to travel simultaneously while still ensuring no one can cheat the system.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-contribution-4" name="p-51309-contribution-4"></a>Contribution</h3>
<p>The Overpass protocol resolves these challenges through a fundamental innovation: self-proving unilateral state channels. Each state transition generates its own zero-knowledge proof of validity, enabling:</p>
<ol>
<li>
<p><strong>Unilateral Operation:</strong></p>
<div class="math">
Valid(State_{new}) \iff VerifyProof(\pi_{validity})  
</div>
<p>No other validation required.</p>
</li>
<li>
<p><strong>Hierarchical State Management:</strong></p>
<div class="math">
\mathcal{H} = \{Root \rightarrow Wallet \rightarrow Channel\}
</div>
<p>Each level maintains independent proof verification.</p>
</li>
<li>
<p><strong>Security Guarantees:</strong></p>
<div class="math">
\begin{split}
   Security_{total} &amp;= Pr[\text{Break proof system}] \\
   &amp;\leq 2^{-\lambda}
   \end{split}
</div>
</li>
<li>
<p><strong>Performance Characteristics:</strong></p>
<div class="math">
\begin{split}
   TPS_{Overpass} &amp;= O(n \cdot m) \text{ (throughput)} \\
   Time_{finality} &amp;= O(1) \text{ (instant)} \\
   Cost_{tx} &amp;= O(\log d) \text{ (proof size)}
   \end{split} 
</div>
</li>
</ol>
<p>Where:</p>
<ul>
<li><span class="math">n</span>: Number of parallel channels (<span class="math">\approx 2^{20}</span> practical maximum)</li>
<li><span class="math">m</span>: Transactions per channel (<span class="math">\approx 2^{16}</span> practical maximum)</li>
<li><span class="math">d</span>: Tree depth (<span class="math">= \log_2(n \cdot m)</span>)</li>
<li><span class="math">\lambda</span>: Security parameter (<span class="math">\geq 128</span> bits)</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-what-this-means-in-practice-5" name="p-51309-what-this-means-in-practice-5"></a>What This Means in Practice</h3>
<p>The key innovation in Overpass is that each participant can independently update their state by generating mathematical proofs, without requiring counterparty presence, consensus, watchtowers, or challenge periods. This achieves:</p>
<ul>
<li><strong>True Unilateral Operation:</strong> Updates possible while counterparty offline</li>
<li><strong>Instant Finality:</strong> No waiting for confirmations or challenges</li>
<li><strong>Mathematical Security:</strong> Proofs guarantee correctness</li>
<li><strong>Minimal Trust:</strong> No reliance on external validators</li>
<li><strong>Practical Performance:</strong> <span class="math">&gt;10^6</span> TPS at <span class="math">&lt;\$0.01</span> per transaction</li>
</ul>
<p>The following sections provide rigorous mathematical proofs of these claims, along with practical implementation guidance.</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-system-model-and-core-definitions-6" name="p-51309-system-model-and-core-definitions-6"></a>System Model and Core Definitions</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-network-model-7" name="p-51309-network-model-7"></a>Network Model</h3>
<p><strong>Definition (Network Model)</strong><br />
The Overpass network consists of:</p>
<div class="math">
\begin{split}
System = \{&amp; Participants, Channels, L1\} \text{ where:} \\
Participants &amp;= \{p_1, ..., p_k\} \text{ (channel owners)} \\
Channels &amp;= \{c_1, ..., c_m\} \text{ (state channels)} \\
L1 &amp;= \text{Settlement layer (e.g., Ethereum)}
\end{split}
</div>
<p><strong>Key Property:</strong> No synchronous communication required between participants.</p>
<hr />
<p>Imagine the Overpass network as a bustling financial district in a major city. The <strong>Participants</strong> are like the businesses and individuals operating within this district, each managing their own accounts (channels). The <strong>Channels</strong> are the various financial roads connecting these accounts, facilitating seamless transactions. The <strong>L1 Settlement Layer</strong> acts as the central bank, ensuring that all transactions are officially recorded and secured. Just as no physical roads require all drivers to communicate with each other to navigate, Overpass channels operate independently without needing constant communication between participants.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-cryptographic-primitives-8" name="p-51309-cryptographic-primitives-8"></a>Cryptographic Primitives</h3>
<p><strong>Definition (Core Components)</strong><br />
Let <span class="math">\mathcal{S} = (\mathcal{H}, \mathcal{P}, \mathcal{Z})</span> be a tuple where:</p>
<div class="math">
\begin{split}
\mathcal{H}: &amp; \{0,1\}^* \rightarrow \{0,1\}^\lambda \text{ (Hash Function)} \\
\mathcal{P}: &amp; \text{ProverSystem} \text{ (zk-SNARK Prover)} \\
\mathcal{Z}: &amp; \text{VerifierSystem} \text{ (zk-SNARK Verifier)}
\end{split}
</div>
<p>With properties:</p>
<ul>
<li><span class="math">\mathcal{H}</span>: Collision-resistant hash function</li>
<li><span class="math">\mathcal{P}</span>: Creates validity proofs for state transitions</li>
<li><span class="math">\mathcal{Z}</span>: Verifies proofs with perfect completeness</li>
</ul>
<hr />
<p>Think of cryptographic primitives as the building blocks of a secure vault system. The <strong>Hash Function</strong> is like a unique lock that ensures each vault (transaction) can only be accessed with the correct key, preventing unauthorized access. The <strong>zk-SNARK Prover</strong> is akin to a master key generator that creates proofs (keys) verifying the validity of each transaction without revealing its contents. The <strong>Verifier System</strong> acts as the security guard, quickly checking these proofs to confirm their authenticity before allowing access. This ensures that every transaction is both secure and efficiently verifiable.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-channel-construction-9" name="p-51309-channel-construction-9"></a>Channel Construction</h3>
<p><strong>Definition (Unilateral Channel)</strong><br />
A channel <span class="math">c</span> is defined as:</p>
<div class="math">
\begin{split}
Channel_c = (&amp;State_c, \pi_c) \text{ where:} \\
State_c = \{&amp;balances, nonce, metadata\} \\
\pi_c = &amp;\text{Proof of state validity}
\end{split}
</div>
<p>With properties:</p>
<ul>
<li>Unilateral updates: No counterparty needed</li>
<li>Self-proving: Validity proven by <span class="math">\pi_c</span></li>
<li>Instant finality: No challenge period</li>
</ul>
<hr />
<p>Constructing a channel in Overpass is similar to setting up a private payment lane between two stores in a shopping mall. This lane allows for quick and direct transactions without the need for shoppers to queue at the main checkout counters. Each transaction within this lane is automatically verified by built-in sensors (proofs), ensuring that every payment is legitimate and instantly recorded, eliminating the need for external supervision or manual checks.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-hierarchical-structure-10" name="p-51309-hierarchical-structure-10"></a>Hierarchical Structure</h3>
<p><strong>Definition (Protocol Hierarchy)</strong><br />
Two-level hierarchy <span class="math">\mathcal{H}</span>:</p>
<div class="math">
\begin{split}
\mathcal{H} = \{&amp;Root, Channels\} \text{ where:} \\
Root &amp;: \text{Global SMT root} \\
Channels &amp;: \text{Individual states}
\end{split}
</div>
<p>Each level uses a Sparse Merkle Tree:</p>
<div class="math">
SMT = (V, E, root, \Pi)
</div>
<p>Where:</p>
<ul>
<li><span class="math">V</span>: Channel states</li>
<li><span class="math">E</span>: State transitions</li>
<li><span class="math">root</span>: Merkle root</li>
<li><span class="math">\Pi</span>: Validity proofs</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/a/3adc4d403c49e0e3091eda53d858eedad7a1f303.png" title=""><img alt="" height="433" src="https://ethresear.ch/uploads/default/optimized/3X/3/a/3adc4d403c49e0e3091eda53d858eedad7a1f303_2_690x433.png" width="690" /></a></div><p></p>
<hr />
<p>The hierarchical structure of Overpass can be compared to a well-organized corporate hierarchy. At the top level, the <strong>Root</strong> serves as the CEO, overseeing the entire organization. Each <strong>Channel</strong> acts like a department manager, responsible for their specific teams (transactions). This clear structure ensures accountability and efficient management, where each department independently verifies its activities while contributing to the overall integrity of the company.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-state-representation-11" name="p-51309-state-representation-11"></a>State Representation</h3>
<p><strong>Definition (Channel State)</strong><br />
For a channel <span class="math">c</span>:</p>
<div class="math">
\begin{split}
State_c = \{&amp;balances: \mathbb{N}^k, \\
&amp;nonce: \mathbb{N}, \\
&amp;metadata: \{id, config\}\}
\end{split}
</div>
<p>With invariants proven by <span class="math">\pi_c</span>:</p>
<ul>
<li><span class="math">\sum_i balances_i = Total_c</span> (conservation)</li>
<li><span class="math">nonce_{t+1} &gt; nonce_t</span> (monotonicity)</li>
</ul>
<hr />
<p>Representing the channel state in Overpass is akin to maintaining an up-to-date ledger in a financial office. Each <strong>State_c</strong> includes detailed records of account balances (like individual bank accounts), a <strong>nonce</strong> to track the sequence of transactions (similar to transaction numbers), and metadata for additional context (like transaction descriptions). Just as a ledger ensures that all financial transactions are accurately recorded and sequentially ordered, Overpass ensures that all state changes are transparent and tamper-proof.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-state-transitions-12" name="p-51309-state-transitions-12"></a>State Transitions</h3>
<p><strong>Definition (Valid State Transition)</strong><br />
A transition <span class="math">\Delta</span> is valid if:</p>
<div class="math">
\begin{split}
Valid(\Delta) \iff&amp; \exists \pi: \\
&amp; VerifyProof(\pi, State_t \xrightarrow{\Delta} State_{t+1}) = 1
\end{split}
</div>
<hr />
<p><strong>Theorem (State Transition Security)</strong><br />
For any adversary <span class="math">\mathcal{A}</span>:</p>
<div class="math">
\begin{split}
&amp; Pr[\mathcal{A} \text{ creates valid } \pi \text{ for invalid } \Delta] \\
&amp; \leq 2^{-\lambda}
\end{split}
</div>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-proof-state-transition-security-13" name="p-51309-proof-state-transition-security-13"></a>Proof: State Transition Security</h3>
<p><strong>Intuitive Explanation:</strong><br />
This theorem asserts that any adversary attempting to create a valid proof <span class="math">\pi</span> for an invalid state transition <span class="math">\Delta</span> has a negligible probability of success, bounded by <span class="math">2^{-\lambda}</span>. This security guarantee is foundational to ensuring the integrity of state transitions within the Overpass protocol.</p>
<p><strong>Formal Proof:</strong><br />
By the soundness property of ZK-SNARKs, any attempt to generate a proof for an invalid statement (in this case, an invalid state transition) will fail with high probability. Specifically:</p>
<ul>
<li>The soundness error of the ZK-SNARK is at most <span class="math">2^{-\lambda}</span>.</li>
<li>Therefore, the probability that an adversary can generate a valid proof for an invalid transition is bounded by <span class="math">2^{-\lambda}</span>.</li>
<li>No additional verification steps can reduce this probability further.</li>
</ul>
<p>Thus, the theorem holds.</p>
<hr />
<p>Valid state transitions in Overpass are like approved changes in a company’s financial records. Each time a transaction occurs, it must pass a rigorous approval process (proof verification) to ensure accuracy and legitimacy. This is similar to how a company ensures that every financial entry is reviewed and approved before being recorded, maintaining the integrity of the entire financial system.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-key-properties-14" name="p-51309-key-properties-14"></a>Key Properties</h3>
<p>This model provides:</p>
<ol>
<li><strong>Unilateral Operation:</strong> Participants update independently.</li>
<li><strong>Instant Finality:</strong> Valid proof means valid state.</li>
<li><strong>No Trust Required:</strong> Pure cryptographic security.</li>
<li><strong>Simple Hierarchy:</strong> Two-level structure sufficient.</li>
</ol>
<p>These properties emerge from the self-proving nature of the ZK proofs, requiring no external validation or consensus.</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-core-protocol-mechanisms-and-operations-15" name="p-51309-core-protocol-mechanisms-and-operations-15"></a>Core Protocol Mechanisms and Operations</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-state-updates-16" name="p-51309-state-updates-16"></a>State Updates</h3>
<p><strong>Definition (State Update):</strong><br />
A state update is self-contained:<br />
<span class="math">
\begin{split}
Update = (&amp;State_{new}, \pi_{validity}) \text{ where:} \\
State_{new} = \{&amp;balances', nonce', metadata'\} \\
\pi_{validity} = &amp;\text{Proof of correct transition}
\end{split}
</span></p>
<hr />
<p><strong>Example (Payment Transaction):</strong><br />
Consider a payment:<br />
<span class="math">
\begin{split}
State_{t} = \{&amp;balance_A = 100, \\
&amp;balance_B = 50, \\
&amp;nonce = 15\}
\end{split}
</span></p>
<p>Update to:<br />
<span class="math">
\begin{split}
State_{t+1} = \{&amp;balance_A = 97, \\
&amp;balance_B = 53, \\
&amp;nonce = 16\}
\end{split}
</span></p>
<p>With proof <span class="math">\pi_{validity}</span> showing:</p>
<ul>
<li>Conservation: <span class="math">100 + 50 = 97 + 53</span></li>
<li>Monotonicity: <span class="math">16 &gt; 15</span></li>
<li>Valid ownership: <span class="math">A</span> controls funds</li>
</ul>
<hr />
<p>The core protocol mechanisms of Overpass can be likened to the operations of an automated trading system in a stock exchange. Each <strong>State Update</strong> is like executing a trade—it’s processed autonomously, validated through complex algorithms (proof generation and verification), and instantly reflected in the system without manual intervention. This automation ensures high-speed, reliable transactions that scale effortlessly with increased trading volume.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-update-protocol-17" name="p-51309-update-protocol-17"></a>Update Protocol</h3>
<pre><code class="lang-algorithm">**Algorithm 1: State Update Protocol**
1. Function UpdateState(state_old, update)
2.   state_new ← ComputeNewState(state_old, update)
3.   π_validity ← GenerateProof(state_old, state_new)
4.   assert VerifyProof(π_validity)
5.   root_new ← UpdateMerkleRoot(state_new)
6.   return (state_new, π_validity)
</code></pre>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-security-guarantees-18" name="p-51309-security-guarantees-18"></a>Security Guarantees</h3>
<p><strong>Theorem (Update Security):</strong><br />
For any update:</p>
<div class="math">
\begin{split}
&amp; VerifyProof(\pi_{validity}) = 1 \implies \\
&amp; ValidTransition(State_{old} \rightarrow State_{new})
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
This theorem ensures that if a proof <span class="math">\pi_{validity}</span> verifies successfully, the state transition from <span class="math">State_{old}</span> to <span class="math">State_{new}</span> is indeed valid. This guarantees that only legitimate state updates are accepted by the protocol.</p>
<p><strong>Formal Proof:</strong><br />
Given the properties of ZK-SNARKs:</p>
<ul>
<li><strong>Perfect Completeness:</strong> If the state transition is valid, then a valid proof exists and will always verify.</li>
<li><strong>Soundness:</strong> If the state transition is invalid, no valid proof can be generated that verifies successfully.</li>
</ul>
<p>Therefore, if <span class="math">VerifyProof(\pi_{validity}) = 1</span>, it must be that the state transition is valid.</p>
<hr />
<p><strong>Theorem (Atomic Updates):</strong><br />
Updates are atomic:</p>
<div class="math">
\{State={Statenewif VerifyProof(π)=1Stateoldotherwise\begin{split} State = \begin{cases} State_{new} &amp; \text{if } VerifyProof(\pi) = 1 \\ State_{old} &amp; \text{otherwise} \end{cases} \end{split}}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
Atomicity ensures that a state update either fully succeeds or fails without partial changes. This prevents inconsistent states from arising due to failed updates.</p>
<p><strong>Formal Proof:</strong><br />
The protocol’s update mechanism includes an assertion that the proof verifies successfully before committing the new state. If <span class="math">VerifyProof(\pi) = 1</span>, the new state is accepted. If the proof fails to verify, the state remains unchanged. This binary outcome guarantees atomicity.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-performance-characteristics-19" name="p-51309-performance-characteristics-19"></a>Performance Characteristics</h3>
<p>Concrete performance metrics:</p>
<ol>
<li>
<p><strong>Time Complexity:</strong></p>
<div class="math">
\begin{split}
   Time_{prove} &amp;= O(\log n) \text{ (proof generation)} \\
   Time_{verify} &amp;= O(1) \text{ (verification)} \\
   Time_{update} &amp;= O(1) \text{ (state update)}
   \end{split}
</div>
</li>
<li>
<p><strong>Space Complexity:</strong></p>
<div class="math">
\begin{split}
   Size_{proof} &amp;= O(\log n) \text{ (proof size)} \\
   Size_{state} &amp;= O(1) \text{ (state size)}
   \end{split}
</div>
</li>
</ol>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-practical-benefits-20" name="p-51309-practical-benefits-20"></a>Practical Benefits</h3>
<p>The unilateral ZKP design provides:</p>
<ol>
<li>
<p><strong>Simplicity:</strong></p>
<ul>
<li>Single proof per update</li>
<li>No multi-phase protocol</li>
<li>No coordination needed</li>
<li>Self-contained verification</li>
</ul>
</li>
<li>
<p><strong>Security:</strong></p>
<ul>
<li>Mathematical proof of correctness</li>
<li>No trust assumptions</li>
<li>No external validation</li>
<li>Instant finality</li>
</ul>
</li>
<li>
<p><strong>Efficiency:</strong></p>
<ul>
<li>Minimal communication</li>
<li>Fast verification</li>
<li>Low overhead</li>
<li>Scalable design</li>
</ul>
</li>
</ol>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-fundamental-security-theorems-21" name="p-51309-fundamental-security-theorems-21"></a>Fundamental Security Theorems</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-security-model-22" name="p-51309-security-model-22"></a>Security Model</h3>
<p>The security of Overpass reduces to the security of its cryptographic primitives:</p>
<p><strong>Definition (Security Model):</strong></p>
<div class="math">
\begin{split}
Primitives = \{&amp;ZK\text{-}SNARK_{security}, \\
&amp;Hash_{collision}, \\
&amp;Merkle_{binding}\}
\end{split}
</div>
<p>Against any PPT adversary with:</p>
<ul>
<li>Standard computational bounds</li>
<li>Access to public parameters</li>
<li>Ability to generate proofs</li>
</ul>
<hr />
<p>The fundamental security theorems of Overpass are comparable to the rigorous safety standards in aviation. Just as aircraft undergo extensive testing to ensure they can withstand extreme conditions, Overpass’s mathematical proofs ensure that its system is impervious to common attack vectors. This means businesses and users can trust that their transactions are secure, much like passengers trust that their flights are safe.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-core-security-properties-23" name="p-51309-core-security-properties-23"></a>Core Security Properties</h3>
<p><strong>Theorem (Proof Security):</strong><br />
For any adversary <span class="math">\mathcal{A}</span>:</p>
<div class="math">
\begin{split}
&amp; Pr[\mathcal{A} \text{ creates valid proof for invalid state}] \\
&amp; \leq 2^{-\lambda}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
This theorem ensures that the probability of an adversary successfully forging a proof for an invalid state is negligible, bounded by <span class="math">2^{-\lambda}</span>. This is crucial for maintaining the integrity and trustworthiness of state transitions within the protocol.</p>
<p><strong>Formal Proof:</strong><br />
By the soundness property of ZK-SNARKs:</p>
<ul>
<li>The probability that an adversary can generate a valid proof for an invalid state is at most the soundness error of the ZK-SNARK, which is <span class="math">2^{-\lambda}</span>.</li>
<li>This bound holds under the assumption that the underlying cryptographic primitives are secure and the adversary is computationally bounded.</li>
</ul>
<p>Therefore, the probability <span class="math">Pr[\mathcal{A} \text{ creates valid proof for invalid state}] \leq 2^{-\lambda}</span>.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-double-spend-prevention-24" name="p-51309-double-spend-prevention-24"></a>Double-Spend Prevention</h3>
<p><strong>Theorem (Double-Spend Prevention):</strong><br />
The probability of creating conflicting valid states is:</p>
<div class="math">
\begin{split}
&amp; Pr[Valid(State_1) \land Valid(State_2) \land Conflict(State_1, State_2)] \\
&amp; \leq 2^{-\lambda}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
This theorem guarantees that the likelihood of an adversary successfully creating two conflicting valid states (i.e., double-spending) is extremely low, bounded by <span class="math">2^{-\lambda}</span>. This is essential for preventing double-spending attacks in the protocol.</p>
<p><strong>Formal Proof:</strong><br />
Leveraging the soundness of ZK-SNARKs:</p>
<ul>
<li>Each valid state transition is accompanied by a proof that must verify successfully.</li>
<li>Nonces ensure unique ordering of state transitions, preventing replay or conflicting updates.</li>
<li>For two conflicting states to both be valid, the adversary must forge proofs for at least one invalid state transition.</li>
<li>The probability of successfully forging such a proof is bounded by <span class="math">2^{-\lambda}</span>.</li>
</ul>
<p>Thus, <span class="math">Pr[Valid(State_1) \land Valid(State_2) \land Conflict(State_1, State_2)] \leq 2^{-\lambda}</span>.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-state-integrity-25" name="p-51309-state-integrity-25"></a>State Integrity</h3>
<p><strong>Theorem (State Integrity):</strong><br />
For any state transition sequence:</p>
<div class="math">
\begin{split}
&amp; State_0 \xrightarrow{\pi_1} State_1 \xrightarrow{\pi_2} State_2 \rightarrow ... \\
&amp; \prod_i VerifyProof(\pi_i) = 1 \implies AllValid(State_i)
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
This theorem ensures that if all proofs in a sequence of state transitions verify successfully, then all resultant states are valid. This maintains the integrity of the entire state transition history.</p>
<p><strong>Formal Proof:</strong><br />
By induction:</p>
<ul>
<li><strong>Base Case:</strong> <span class="math">VerifyProof(\pi_1) = 1 \implies State_1</span> is valid.</li>
<li><strong>Inductive Step:</strong> Assuming <span class="math">State_i</span> is valid, if <span class="math">VerifyProof(\pi_{i+1}) = 1</span>, then <span class="math">State_{i+1}</span> is also valid.</li>
<li><strong>Conclusion:</strong> Therefore, if all proofs verify, all states in the sequence are valid.</li>
</ul>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-security-composition-26" name="p-51309-security-composition-26"></a>Security Composition</h3>
<p><strong>Theorem (Overall Security):</strong><br />
System security reduces to primitive security:</p>
<div class="math">
\begin{split}
Security_{system} = &amp;Security_{ZK\text{-}SNARK} \\
= &amp;2^{-\lambda}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
The overall security of the Overpass protocol is directly inherited from the security of its underlying cryptographic primitives, particularly the ZK-SNARKs.</p>
<p><strong>Formal Proof:</strong><br />
Since all security guarantees (such as state transition validity and double-spend prevention) are based on the soundness of ZK-SNARKs, the system’s security level is equivalent to that of the ZK-SNARKs used. Given that the ZK-SNARKs have a soundness error of <span class="math">2^{-\lambda}</span>, the entire system inherits this security level.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-practical-implications-27" name="p-51309-practical-implications-27"></a>Practical Implications</h3>
<p>These guarantees provide:</p>
<ol>
<li>
<p><strong>Instant Finality</strong></p>
<ul>
<li>No waiting periods needed</li>
<li>No probabilistic confirmation</li>
<li>No external validation</li>
<li>Pure mathematical certainty</li>
</ul>
</li>
<li>
<p><strong>Unconditional Security</strong></p>
<ul>
<li>No network assumptions</li>
<li>No trust requirements</li>
<li>No timing dependencies</li>
<li>Pure cryptographic guarantees</li>
</ul>
</li>
<li>
<p><strong>Practical Performance</strong></p>
<ul>
<li>Fast verification</li>
<li>Compact proofs</li>
<li>Minimal computation</li>
<li>Efficient storage</li>
</ul>
</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#p-51309-advanced-security-properties-28" name="p-51309-advanced-security-properties-28"></a>Advanced Security Properties</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-multi-state-updates-29" name="p-51309-multi-state-updates-29"></a>Multi-State Updates</h3>
<p><strong>Definition (State Update Group):</strong><br />
A group of related updates <span class="math">G = \{u_1, ..., u_k\}</span> where each <span class="math">u_i</span> produces a new state:</p>
<div class="math">
\begin{split}
G_{proof} = \pi: &amp;State_{t} \xrightarrow{u_1,...,u_k} State_{t+k} \\
&amp;\text{Single proof covers all updates}
\end{split}
</div>
<hr />
<p><strong>Theorem (Group Update Security):</strong><br />
For any update group <span class="math">G</span>:</p>
<div class="math">
\begin{split}
VerifyProof(\pi_G) = 1 \iff&amp; \text{ All updates valid} \land \\
&amp;\text{ All state changes correct} \land \\
&amp;\text{ All invariants preserved}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
This theorem ensures that when a group proof <span class="math">\pi_G</span> verifies successfully, it implies that every individual update within the group is valid, all state changes are correctly applied, and all protocol invariants are maintained.</p>
<p><strong>Formal Proof:</strong><br />
The group proof <span class="math">\pi_G</span> encompasses all updates <span class="math">u_1, ..., u_k</span>. For <span class="math">\pi_G</span> to verify:</p>
<ul>
<li>Each update <span class="math">u_i</span> must individually satisfy the state transition rules.</li>
<li>The collective state changes must adhere to global invariants such as total balance conservation and nonce monotonicity.</li>
<li>No conflicting updates are present within the group.</li>
</ul>
<p>Therefore, if <span class="math">VerifyProof(\pi_G) = 1</span>, all updates are valid, state changes are correct, and invariants are preserved.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-cross-channel-operations-30" name="p-51309-cross-channel-operations-30"></a>Cross-Channel Operations</h3>
<p><strong>Theorem (Cross-Channel Atomicity):</strong><br />
For updates across channels <span class="math">c_1,...,c_n</span>:</p>
<div class="math">
\begin{split}
\pi_{cross}: &amp;State_{t} \xrightarrow{update} State_{t+1} \\
VerifyProof(\pi_{cross}) = 1 \implies&amp; \text{ All channels updated validly}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
This theorem guarantees that when a cross-channel proof <span class="math">\pi_{cross}</span> verifies successfully, all involved channels <span class="math">c_1,...,c_n</span> have been updated correctly and consistently.</p>
<p><strong>Formal Proof:</strong><br />
The ZK proof circuit for cross-channel updates enforces:</p>
<ul>
<li>Conservation of total value across all channels.</li>
<li>Valid state transitions for each individual channel.</li>
<li>Atomicity, ensuring that either all channel updates succeed or none do.</li>
</ul>
<p>Thus, if <span class="math">VerifyProof(\pi_{cross}) = 1</span>, it ensures that all channels have been updated validly and atomically.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-state-composition-31" name="p-51309-state-composition-31"></a>State Composition</h3>
<p><strong>Theorem (Compositional Security):</strong><br />
Multiple valid proofs compose securely:</p>
<div class="math">
\begin{split}
\forall \pi_1,...,\pi_k: &amp;\bigwedge_i VerifyProof(\pi_i) = 1 \implies \\
&amp;Valid(State_{final})
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
This theorem states that if a series of proofs <span class="math">\pi_1,...,\pi_k</span> all verify successfully, then the final state after all transitions is valid. This ensures that composing multiple valid updates maintains overall system integrity.</p>
<p><strong>Formal Proof:</strong><br />
Each proof <span class="math">\pi_i</span> ensures that the corresponding state transition is valid. By the integrity of individual proofs:</p>
<ul>
<li>Each transition preserves the required invariants.</li>
<li>Sequential application of valid transitions maintains state consistency.</li>
</ul>
<p>Therefore, the final state <span class="math">State_{final}</span> is valid.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-practical-implications-32" name="p-51309-practical-implications-32"></a>Practical Implications</h3>
<p>This simplified design enables:</p>
<ol>
<li>
<p><strong>Complex Operations</strong></p>
<ul>
<li>Multi-party transactions</li>
<li>Cross-channel transfers</li>
<li>Atomic swaps</li>
<li>State composition</li>
</ul>
</li>
<li>
<p><strong>Security Guarantees</strong></p>
<ul>
<li>Single-proof verification</li>
<li>Deterministic outcomes</li>
<li>No coordination needed</li>
<li>Instant finality</li>
</ul>
</li>
<li>
<p><strong>Implementation</strong></p>
</li>
</ol>
<pre><code class="lang-algorithm">**Algorithm 2: Multi-State Update**
1. Function UpdateMultiple(states, updates)
2.   new_states ← ComputeNewStates(states, updates)
3.   π ← GenerateProof(states, new_states)
4.   assert VerifyProof(π)
5.   return (new_states, π)
</code></pre>
<hr />
<p>Advanced security properties in Overpass function like the multi-layered security protocols of a high-security facility. <strong>Group Update Security</strong> ensures that batches of transactions are processed securely, similar to how a secure facility handles groups of visitors with coordinated access protocols. <strong>Cross-Channel Atomicity</strong> guarantees that interconnected transactions are executed flawlessly, akin to synchronized operations in a complex manufacturing process where each step must align perfectly to ensure product quality.</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-liveness-properties-33" name="p-51309-liveness-properties-33"></a>Liveness Properties</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-unilateral-progress-34" name="p-51309-unilateral-progress-34"></a>Unilateral Progress</h3>
<p><strong>Theorem (Unilateral Progress):</strong><br />
Any participant can always progress their state:</p>
<div class="math">
\begin{split}
&amp; \forall p \in Participants: \\
&amp; State_p \xrightarrow{update} State_{p'} \text{ possible if } \\
&amp; \exists \pi: VerifyProof(\pi) = 1
\end{split}
</div>
<p>Independent of:</p>
<ul>
<li>Other participants’ availability</li>
<li>Network conditions</li>
<li>System load</li>
<li>External factors</li>
</ul>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
This theorem ensures that any participant can independently update their state without relying on the availability or participation of others, and regardless of external conditions.</p>
<p><strong>Formal Proof:</strong><br />
The protocol allows unilateral updates by:</p>
<ul>
<li>Allowing participants to generate and verify proofs independently.</li>
<li>Not requiring any interaction or synchronization with other participants.</li>
</ul>
<p>Therefore, as long as a participant can generate a valid proof, they can progress their state irrespective of other factors.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-settlement-guarantee-35" name="p-51309-settlement-guarantee-35"></a>Settlement Guarantee</h3>
<p><strong>Theorem (Settlement Finality):</strong><br />
For any valid state update:</p>
<div class="math">
\begin{split}
Time_{settle} &amp;= Time_{prove} + Time_{verify} \\
&amp;= O(\log n) + O(1) \\
&amp;\approx \text{constant time}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
Settlement finality refers to the time it takes for a state update to be finalized and irrevocable on the settlement layer. This theorem states that the total time is effectively constant due to the logarithmic time for proof generation and constant time for verification.</p>
<p><strong>Formal Proof:</strong></p>
<ul>
<li><span class="math">Time_{prove} = O(\log n)</span>: Proof generation scales logarithmically with the number of channels.</li>
<li><span class="math">Time_{verify} = O(1)</span>: Proof verification time remains constant regardless of the number of channels.</li>
</ul>
<p>Since <span class="math">O(\log n) + O(1) = O(\log n)</span> and for practical purposes with large <span class="math">n</span>, <span class="math">O(\log n)</span> is considered effectively constant, especially when optimized with hardware acceleration.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-l1-settlement-36" name="p-51309-l1-settlement-36"></a>L1 Settlement</h3>
<p><strong>Theorem (L1 Settlement):</strong><br />
Any valid state can settle to L1:</p>
<div class="math">
\begin{split}
&amp; \forall State, \pi: VerifyProof(\pi) = 1 \implies \\
&amp; SettleToL1(State) \text{ succeeds in } O(1) \text{ L1 blocks}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
This theorem ensures that any state verified as valid can be settled on the Layer 1 blockchain within a constant number of blocks, ensuring quick and reliable settlement.</p>
<p><strong>Formal Proof:</strong></p>
<ul>
<li>Upon successful verification of the proof <span class="math">\pi</span>, the state is deemed valid.</li>
<li>The settlement transaction is prepared and submitted to L1.</li>
<li>Due to L1’s consensus mechanism, the transaction will be included in the next available block.</li>
<li>Assuming L1 has a constant block time, settlement occurs in <span class="math">O(1)</span> blocks.</li>
</ul>
<p>Thus, any valid state can be settled to L1 within a constant number of blocks.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-practical-guarantees-37" name="p-51309-practical-guarantees-37"></a>Practical Guarantees</h3>
<p>The system provides:</p>
<ol>
<li>
<p><strong>Instant Progress</strong></p>
<ul>
<li>No coordination needed</li>
<li>No waiting periods</li>
<li>No external dependencies</li>
<li>No failure modes</li>
</ul>
</li>
<li>
<p><strong>Settlement Assurance</strong></p>
<ul>
<li>Guaranteed L1 settlement</li>
<li>Fixed settlement time</li>
<li>No challenge periods</li>
<li>No reversion possible</li>
</ul>
</li>
<li>
<p><strong>Operational Properties</strong></p>
<ul>
<li>Deterministic operation</li>
<li>Load-independent</li>
<li>Network-independent</li>
<li>Always available</li>
</ul>
</li>
</ol>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-implementation-38" name="p-51309-implementation-38"></a>Implementation</h3>
<pre><code class="lang-algorithm">**Algorithm 3: Unilateral State Update**
1. Function UpdateState(state, update)
2.   new_state ← ComputeNewState(state, update)
3.   π ← GenerateProof(state, new_state)
4.   assert VerifyProof(π)
5.   root_new ← UpdateMerkleRoot(new_state)
6.   return (new_state, π)
</code></pre>
<p>Key properties:</p>
<ul>
<li>Self-contained execution</li>
<li>No external dependencies</li>
<li>Immediate completion</li>
<li>Guaranteed finality</li>
</ul>
<hr />
<p>Liveness guarantees in Overpass are like the reliability of a 24/7 customer support center. No matter the time or external conditions, participants can always initiate and complete transactions, just as customers can always reach support services when needed. This ensures that the system remains operational and responsive, providing businesses with the confidence that their transactions will always be processed without unnecessary delays.</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-economic-analysis-39" name="p-51309-economic-analysis-39"></a>Economic Analysis</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-cost-model-40" name="p-51309-cost-model-40"></a>Cost Model</h3>
<p><strong>Definition (Operational Costs):</strong><br />
For any state update <span class="math">u</span>:</p>
<div class="math">
\begin{split}
Cost_{update} = &amp;Cost_{compute}(u) + \\
&amp;Cost_{storage}(u) + \\
&amp;Cost_{settlement}(u)
\end{split}
</div>
<p>With components:</p>
<div class="math">
\begin{split}
Cost_{compute}(u) &amp;= c_p \cdot Size_{circuit}(u) \\
Cost_{storage}(u) &amp;= c_s \cdot Size_{state}(u) \\
Cost_{settlement}(u) &amp;= \begin{cases}
c_l \cdot Gas_{L1} &amp; \text{if L1 settlement} \\
0 &amp; \text{otherwise}
\end{cases}
\end{split}
</div>
<p>Where:</p>
<ul>
<li><span class="math">c_p</span>: Cost per circuit constraint</li>
<li><span class="math">c_s</span>: Cost per byte of storage</li>
<li><span class="math">c_l</span>: L1 gas cost</li>
</ul>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-circuit-size-analysis-41" name="p-51309-circuit-size-analysis-41"></a>Circuit Size Analysis</h3>
<p><strong>Theorem (Circuit Complexity):</strong><br />
For standard operations:</p>
<div class="math">
\begin{split}
Size_{circuit}(u) = &amp;Size_{base} + \\
&amp;Size_{transition}(u) + \\
&amp;Size_{verification}(u)
\end{split}
</div>
<p>Where:</p>
<div class="math">
\begin{split}
Size_{base} &amp;= O(1) \text{ (constant overhead)} \\
Size_{transition} &amp;= O(\log n) \text{ (state update)} \\
Size_{verification} &amp;= O(1) \text{ (proof verification)}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
The size of the circuit required for generating and verifying proofs scales logarithmically with the number of channels due to the hierarchical structure, while other components remain constant.</p>
<p><strong>Formal Proof:</strong></p>
<ul>
<li><span class="math">Size_{base}</span> accounts for fixed operations unrelated to the number of channels.</li>
<li><span class="math">Size_{transition}</span> involves operations proportional to <span class="math">\log n</span>, stemming from the use of Sparse Merkle Trees which have logarithmic depth.</li>
<li><span class="math">Size_{verification}</span> is constant as proof verification does not depend on the number of channels.</li>
</ul>
<p>Thus, the total circuit size is <span class="math">O(1) + O(\log n) + O(1) = O(\log n)</span>.</p>
<hr />
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/f/5fc085d4d56fc1210388e34c3d5f506f53dd42e4.png" title=""><img alt="" height="352" src="https://ethresear.ch/uploads/default/optimized/3X/5/f/5fc085d4d56fc1210388e34c3d5f506f53dd42e4_2_690x352.png" width="690" /></a></div><p></p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-storage-requirements-42" name="p-51309-storage-requirements-42"></a>Storage Requirements</h3>
<p><strong>Theorem (Storage Costs):</strong><br />
State storage requirements:</p>
<div class="math">
\begin{split}
Size_{state}(u) = &amp;Size_{balance} + \\
&amp;Size_{proof} + \\
&amp;Size_{metadata}
\end{split}
</div>
<p>With bounds:</p>
<div class="math">
\begin{split}
Size_{balance} &amp;= O(1) \text{ bytes per account} \\
Size_{proof} &amp;= O(\log n) \text{ bytes} \\
Size_{metadata} &amp;= O(1) \text{ bytes}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
The storage required for each state update comprises fixed-size components (balances and metadata) and a variable-size component (proof), which scales logarithmically with the number of channels.</p>
<p><strong>Formal Proof:</strong></p>
<ul>
<li><span class="math">Size_{balance}</span> is constant per account as it only needs to store the numerical balance.</li>
<li><span class="math">Size_{proof}</span> scales with <span class="math">\log n</span> due to the Sparse Merkle Tree structure.</li>
<li><span class="math">Size_{metadata}</span> remains constant as it includes fixed attributes like channel ID and configuration.</li>
</ul>
<p>Therefore, <span class="math">Size_{state}(u) = O(1) + O(\log n) + O(1) = O(\log n)</span>.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-l1-settlement-costs-43" name="p-51309-l1-settlement-costs-43"></a>L1 Settlement Costs</h3>
<p><strong>Theorem (Settlement Economics):</strong><br />
L1 settlement costs for state <span class="math">s</span>:</p>
<div class="math">
\begin{split}
Gas_{L1}(s) = &amp;Gas_{base} + \\
&amp;Gas_{proof} \cdot Size_{proof} + \\
&amp;Gas_{data} \cdot Size_{state}(s)
\end{split}
</div>
<p>Where:</p>
<ul>
<li><span class="math">Gas_{base}</span>: Base transaction cost</li>
<li><span class="math">Gas_{proof}</span>: Cost per proof byte</li>
<li><span class="math">Gas_{data}</span>: Cost per state byte</li>
</ul>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
The total gas cost for settling a state on Layer 1 includes a fixed base cost, a variable cost based on the size of the proof, and another variable cost based on the size of the state data.</p>
<p><strong>Formal Proof:</strong></p>
<ul>
<li><span class="math">Gas_{base}</span> accounts for the fixed overhead of initiating a transaction on L1.</li>
<li><span class="math">Gas_{proof} \cdot Size_{proof}</span> represents the variable cost associated with transmitting the proof data.</li>
<li><span class="math">Gas_{data} \cdot Size_{state}(s)</span> accounts for the storage of state data on L1.</li>
</ul>
<p>Thus, the total settlement cost is the sum of these components.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-total-cost-analysis-44" name="p-51309-total-cost-analysis-44"></a>Total Cost Analysis</h3>
<p><strong>Theorem (Total Cost Bounds):</strong><br />
For any update <span class="math">u</span>:</p>
<div class="math">
\begin{split}
Cost_{total}(u) \leq &amp;\alpha \cdot \log(n) + \\
&amp;\beta \cdot Size_{state}(u) + \\
&amp;\gamma \cdot \mathbb{1}_{settlement}
\end{split}
</div>
<p>Where:</p>
<ul>
<li><span class="math">\alpha</span>: Circuit computation coefficient</li>
<li><span class="math">\beta</span>: Storage coefficient</li>
<li><span class="math">\gamma</span>: L1 settlement coefficient</li>
<li><span class="math">\mathbb{1}_{settlement}</span>: Settlement indicator</li>
</ul>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
The total cost of a state update is bounded by the sum of costs associated with proof computation, storage, and optional settlement. Each component scales differently with system parameters.</p>
<p><strong>Formal Proof:</strong></p>
<ul>
<li><span class="math">\alpha \cdot \log(n)</span> represents the cost associated with the logarithmic scaling of proof computation.</li>
<li><span class="math">\beta \cdot Size_{state}(u)</span> accounts for the storage cost, which also scales logarithmically.</li>
<li><span class="math">\gamma \cdot \mathbb{1}_{settlement}</span> includes the fixed or variable cost of settling to L1, depending on whether settlement occurs.</li>
</ul>
<p>Thus, <span class="math">Cost_{total}(u)</span> is bounded by the sum of these three components.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-practical-cost-examples-45" name="p-51309-practical-cost-examples-45"></a>Practical Cost Examples</h3>
<ol>
<li>
<p><strong>Simple Transfer</strong><br />
For basic value transfer:</p>
<div class="math">
\begin{split}
   Cost_{transfer} &amp;\approx 0.001\$ \text{ (proof)} \\
   &amp;+ 0.0001\$ \text{ (storage)} \\
   &amp;+ 0\$ \text{ (no settlement)}
   \end{split}
</div>
</li>
<li>
<p><strong>Complex Update</strong><br />
For multi-party transfer:</p>
<div class="math">
\begin{split}
   Cost_{complex} &amp;\approx 0.005\$ \text{ (proof)} \\
   &amp;+ 0.0005\$ \text{ (storage)} \\
   &amp;+ 0\$ \text{ (no settlement)}
   \end{split}
</div>
</li>
<li>
<p><strong>L1 Settlement</strong><br />
With L1 settlement:</p>
<div class="math">
\begin{split}
   Cost_{settle} &amp;\approx 0.001\$ \text{ (proof)} \\
   &amp;+ 0.0001\$ \text{ (storage)} \\
   &amp;+ 5\$ \text{ (L1 gas)}
   \end{split}
</div>
</li>
</ol>
<hr />
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/3/a364d056defa3bb37956579ea2b6753ad5ec5b4d.png" title=""><img alt="" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/a/3/a364d056defa3bb37956579ea2b6753ad5ec5b4d_2_508x500.png" width="508" /></a></div><p></p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-economic-benefits-46" name="p-51309-economic-benefits-46"></a>Economic Benefits</h3>
<p>This cost model provides:</p>
<ol>
<li>
<p><strong>Predictable Costs</strong></p>
<ul>
<li>Fixed computation costs</li>
<li>Linear storage scaling</li>
<li>Optional L1 settlement</li>
<li>No congestion pricing</li>
</ul>
</li>
<li>
<p><strong>Economic Efficiency</strong></p>
<ul>
<li>Minimal overhead</li>
<li>Batching benefits</li>
<li>Proof amortization</li>
<li>Storage optimization</li>
</ul>
</li>
<li>
<p><strong>User Advantages</strong></p>
<ul>
<li>Low base fees</li>
<li>Predictable costs</li>
<li>Settlement flexibility</li>
<li>Cost optimization options</li>
</ul>
</li>
</ol>
<p>This economic model enables efficient operation at any scale.</p>
<hr />
<p>The economic model of Overpass can be compared to a highly efficient utility service. Just as electricity costs are predictable and scale with usage, Overpass ensures that transaction costs remain low and scale logarithmically with the number of users and transactions. This predictability allows businesses to budget effectively, knowing that their operational costs will remain manageable even as their transaction volume grows exponentially.</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-practical-implementation-47" name="p-51309-practical-implementation-47"></a>Practical Implementation</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-core-components-48" name="p-51309-core-components-48"></a>Core Components</h3>
<p><strong>Definition (System Architecture):</strong><br />
The Overpass implementation consists of:</p>
<div class="math">
\begin{split}
System = \{&amp;Prover, \\
&amp;Verifier, \\
&amp;Storage, \\
&amp;L1Interface\}
\end{split}
</div>
<p>With key parameters:</p>
<div class="math">
\begin{split}
Parameters = \{&amp;\lambda = 128 \text{ (security)}, \\
&amp;d = 32 \text{ (tree depth)}, \\
&amp;m = 2^{16} \text{ (max channels)}, \\
&amp;n = 2^{20} \text{ (max states)}\}
\end{split}
</div>
<hr />
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/e/bed30100fde094f291458afbbe7a7b48f2673df8.png" title=""><img alt="" height="348" src="https://ethresear.ch/uploads/default/optimized/3X/b/e/bed30100fde094f291458afbbe7a7b48f2673df8_2_690x348.png" width="690" /></a></div><p></p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-proof-circuit-49" name="p-51309-proof-circuit-49"></a>Proof Circuit</h3>
<p><strong>Definition (Proof Circuit):</strong><br />
The Proof Circuit is a critical component of the Overpass protocol, responsible for generating and verifying zero-knowledge proofs for each state transition. The process begins with the Prover generating a proof <span class="math">\pi</span> that encapsulates the validity of the state transition from <span class="math">State_{old}</span> to <span class="math">State_{new}</span>. The Verifier then checks the integrity of <span class="math">\pi</span>. If the proof is valid, the new state is accepted; otherwise, the update is rejected.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-channel-operations-50" name="p-51309-channel-operations-50"></a>Channel Operations</h3>
<pre><code class="lang-algorithm">**Algorithm 4: Channel Management**
1. Function UpdateState(current_state, update)
2.   new_state ← ComputeNewState(current_state, update)
3.   proof ← GenerateProof(current_state, new_state)
4.   assert VerifyProof(proof)
5.   store_state(new_state, proof)
6.   return (new_state, proof)

7. Function SettleToL1(state, proof)
8.   tx ← PrepareSettlementTx(state, proof)
9.   send_to_l1(tx)
10.  return tx_hash
</code></pre>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-storage-layout-51" name="p-51309-storage-layout-51"></a>Storage Layout</h3>
<pre><code class="lang-algorithm">**Algorithm 5: State Storage**
1. Function StoreState(state, proof)
2.   **State Storage:**  
3.   key ← hash(state)  
4.   store_value(key, state)  

5.   **Proof Storage:**  
6.   proof_key ← hash(proof)  
7.   store_value(proof_key, proof)  

8.   **Index Update:**  
9.   update_merkle_tree(state)
</code></pre>
<p>The <code>StoreState</code> function handles the storage of both the state and its corresponding proof. By hashing the state and proof, unique keys are generated for efficient retrieval. Updating the Merkle tree ensures that the global state remains consistent and verifiable.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-optimization-techniques-52" name="p-51309-optimization-techniques-52"></a>Optimization Techniques</h3>
<p>Key optimizations:</p>
<ol>
<li>
<p><strong>Proof Generation</strong></p>
<ul>
<li>GPU acceleration: Utilize parallel processing capabilities to speed up proof generation.</li>
<li>Circuit optimization: Design efficient circuits to reduce the computational overhead.</li>
<li>Parallel computation: Generate multiple proofs simultaneously to increase throughput.</li>
<li>Proof caching: Store frequently used proofs to avoid redundant computations.</li>
</ul>
</li>
<li>
<p><strong>State Management</strong></p>
<ul>
<li>Efficient Merkle trees: Implement optimized data structures for faster state verification.</li>
<li>State compression: Reduce the size of state data to minimize storage and transmission costs.</li>
<li>Lazy evaluation: Defer computation of certain state aspects until necessary.</li>
<li>Batch updates: Process multiple state updates in a single operation to enhance efficiency.</li>
</ul>
</li>
<li>
<p><strong>L1 Integration</strong></p>
<ul>
<li>Batched settlements: Aggregate multiple settlements into a single transaction to save gas.</li>
<li>Gas optimization: Optimize smart contract code to reduce gas consumption.</li>
<li>Proof aggregation: Combine multiple proofs into a single aggregated proof for efficiency.</li>
<li>Smart contract efficiency: Design lean smart contracts to handle settlements effectively.</li>
</ul>
</li>
</ol>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-deployment-requirements-53" name="p-51309-deployment-requirements-53"></a>Deployment Requirements</h3>
<p><strong>Production specifications:</strong></p>
<ol>
<li>
<p><strong>Computation Node</strong></p>
<div class="math">
\begin{split}
   Hardware_{min} = \{&amp;CPU: 32\text{ cores}, \\
   &amp;GPU: \text{CUDA-capable}, \\
   &amp;RAM: 128\text{ GB}, \\
   &amp;SSD: 2\text{ TB NVMe}\}
   \end{split}
</div>
</li>
<li>
<p><strong>Software Stack</strong></p>
<div class="math">
\begin{split}
   Software = \{&amp;OS: \text{Ubuntu 22.04}, \\
   &amp;Language: \text{Rust 1.70+}, \\
   &amp;Storage: \text{RocksDB}, \\
   &amp;ZK: \text{PLONKY2}\}
   \end{split}
</div>
</li>
</ol>
<p><strong>Intuitive Explanation:</strong><br />
The specified hardware ensures that the system can handle high-throughput proof generation and verification efficiently. The software stack is chosen for performance and security, with Rust providing memory safety and concurrency, RocksDB offering fast storage, and PLONKY2 facilitating efficient ZK-SNARKs.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-system-monitoring-54" name="p-51309-system-monitoring-54"></a>System Monitoring</h3>
<p>Key metrics:</p>
<ol>
<li>
<p><strong>Performance</strong></p>
<div class="math">
\begin{split}
   Metrics = \{&amp;Time_{proof}, \\
   &amp;Time_{verify}, \\
   &amp;Size_{state}, \\
   &amp;Size_{proof}\}
   \end{split}
</div>
</li>
<li>
<p><strong>Resources</strong></p>
<div class="math">
\begin{split}
   Resources = \{&amp;CPU_{usage}, \\
   &amp;GPU_{usage}, \\
   &amp;Memory_{usage}, \\
   &amp;Disk_{io}\}
   \end{split}
</div>
</li>
</ol>
<p><strong>Intuitive Explanation:</strong><br />
Monitoring these metrics ensures that the system operates within optimal parameters. Tracking proof times and resource usage helps in identifying bottlenecks and optimizing performance. Resource metrics are crucial for maintaining system stability and scalability.</p>
<hr />
<p>Implementing Overpass is similar to deploying a robust IT infrastructure in a large enterprise. The <strong>Prover</strong>, <strong>Verifier</strong>, <strong>Storage</strong>, and <strong>L1 Interface</strong> components are like the servers, security systems, databases, and network interfaces that keep a business running smoothly. The recommended hardware and software stack ensures that the system is both powerful and reliable, capable of handling high transaction volumes with ease, much like a well-designed IT system supports a growing company’s needs.</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-extensions-and-future-work-55" name="p-51309-extensions-and-future-work-55"></a>Extensions and Future Work</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-cross-chain-integration-56" name="p-51309-cross-chain-integration-56"></a>Cross-Chain Integration</h3>
<p>Cross-chain transfers can leverage ZK proofs directly:</p>
<p><strong>Definition (Cross-Chain Proof):</strong><br />
A cross-chain transfer requires:</p>
<div class="math">
\begin{split}
\pi_{cross} = \{&amp;\pi_{source}: \text{Proof of valid source state}, \\
&amp;\pi_{lock}: \text{Proof of value lock}, \\
&amp;\pi_{destination}: \text{Proof of valid target state}\}
\end{split}
</div>
<p><strong>Theorem (Cross-Chain Security):</strong><br />
Security reduces to individual proof verification:</p>
<div class="math">
\begin{split}
Security_{cross} = &amp;Security_{ZK} \\
= &amp;2^{-\lambda}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
Cross-chain transfers rely on multiple proofs to ensure the validity of each step in the transfer process. This theorem states that the overall security of cross-chain operations is equivalent to the security of the underlying ZK proofs.</p>
<p><strong>Formal Proof:</strong></p>
<ul>
<li>Each component proof (<span class="math">\pi_{source}</span>, <span class="math">\pi_{lock}</span>, <span class="math">\pi_{destination}</span>) must individually verify successfully.</li>
<li>Since each proof has a security bound of <span class="math">2^{-\lambda}</span>, the combined security of verifying all proofs remains at <span class="math">2^{-\lambda}</span>, assuming independent security guarantees.</li>
</ul>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-privacy-enhancements-57" name="p-51309-privacy-enhancements-57"></a>Privacy Enhancements</h3>
<p>Privacy improvements through nested proofs:</p>
<p><strong>Definition (Private State):</strong><br />
A private state update:</p>
<div class="math">
\begin{split}
\pi_{private} &amp;: State_{hidden} \rightarrow State'_{hidden} \\
\text{where } &amp;State_{hidden} = Commit(State_{real})
\end{split}
</div>
<p><strong>Theorem (Privacy Guarantees):</strong><br />
For any adversary <span class="math">\mathcal{A}</span>:</p>
<div class="math">
\begin{split}
Pr[\mathcal{A}(State_{hidden}) &amp;\rightarrow State_{real}] \\
&amp;\leq 2^{-\lambda}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
This theorem ensures that the actual state cannot be inferred from the hidden state with any significant probability, preserving the privacy of participants.</p>
<p><strong>Formal Proof:</strong></p>
<ul>
<li><span class="math">State_{hidden}</span> is a cryptographic commitment to <span class="math">State_{real}</span>, ensuring that the real state is concealed.</li>
<li>The zero-knowledge property ensures that no information about <span class="math">State_{real}</span> is leaked through <span class="math">\pi_{private}</span>.</li>
<li>Therefore, the probability that an adversary can deduce <span class="math">State_{real}</span> from <span class="math">State_{hidden}</span> is bounded by the soundness error of the ZK-SNARK, which is <span class="math">2^{-\lambda}</span>.</li>
</ul>
<p>Thus, <span class="math">Pr[\mathcal{A}(State_{hidden}) \rightarrow State_{real}] \leq 2^{-\lambda}</span>.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-recursive-proofs-58" name="p-51309-recursive-proofs-58"></a>Recursive Proofs</h3>
<p><strong>Definition (Recursive Proof Chain):</strong><br />
Recursive composition of proofs:</p>
<div class="math">
\begin{split}
\pi_{recursive} &amp;: \text{Prove}(\pi_1 \land \pi_2 \land ... \land \pi_n) \\
Size(\pi_{recursive}) &amp;= O(1) \text{ regardless of } n
\end{split}
</div>
<hr />
<p><strong>Theorem (Recursive Scalability):</strong><br />
With recursive proofs:</p>
<div class="math">
\begin{split}
Verification_{time} &amp;= O(1) \\
Proof_{size} &amp;= O(1) \\
Security &amp;= 2^{-\lambda}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
Recursive proofs allow multiple proofs to be combined into a single proof without increasing the size or verification time, enabling scalable verification regardless of the number of underlying proofs.</p>
<p><strong>Formal Proof:</strong></p>
<ul>
<li>The recursive proof <span class="math">\pi_{recursive}</span> encapsulates multiple individual proofs <span class="math">\pi_1, ..., \pi_n</span>.</li>
<li>Due to recursion, the verification of <span class="math">\pi_{recursive}</span> remains constant in time and size, regardless of <span class="math">n</span>.</li>
<li>The security of the recursive proof is maintained as each individual proof contributes to the overall security without introducing additional vulnerabilities.</li>
</ul>
<p>Therefore, <span class="math">Verification_{time} = O(1)</span>, <span class="math">Proof_{size} = O(1)</span>, and <span class="math">Security = 2^{-\lambda}</span>.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-future-research-priorities-59" name="p-51309-future-research-priorities-59"></a>Future Research Priorities</h3>
<ol>
<li>
<p><strong>Proof System Improvements</strong></p>
<ul>
<li>Faster proof generation</li>
<li>More efficient circuits</li>
<li>Hardware acceleration</li>
<li>Proof compression</li>
</ul>
</li>
<li>
<p><strong>Privacy Enhancements</strong></p>
<ul>
<li>Hidden state transitions</li>
<li>Anonymous ownership</li>
<li>Confidential amounts</li>
<li>Metadata protection</li>
</ul>
</li>
<li>
<p><strong>Recursive Composition</strong></p>
<ul>
<li>Efficient recursion</li>
<li>Proof aggregation</li>
<li>Batch verification</li>
<li>Constant-size proofs</li>
</ul>
</li>
</ol>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-implementation-roadmap-60" name="p-51309-implementation-roadmap-60"></a>Implementation Roadmap</h3>
<ol>
<li>
<p><strong>Phase 1: Core Optimization</strong></p>
<div class="math">
\begin{split}
   Optimize = \{&amp;Circuit_{efficiency}, \\
   &amp;Proof_{generation}, \\
   &amp;Hardware_{acceleration}, \\
   &amp;Storage_{compression}\}
   \end{split}
</div>
</li>
<li>
<p><strong>Phase 2: Advanced Features</strong></p>
<div class="math">
\begin{split}
   Features = \{&amp;Privacy_{enhancements}, \\
   &amp;Recursive_{proofs}, \\
   &amp;Cross_{chain}, \\
   &amp;L1_{integration}\}
   \end{split}
</div>
</li>
<li>
<p><strong>Phase 3: Ecosystem</strong></p>
<div class="math">
\begin{split}
   Ecosystem = \{&amp;Developer_{tools}, \\
   &amp;Client_{libraries}, \\
   &amp;Integration_{APIs}, \\
   &amp;Testing_{frameworks}\}
   \end{split}
</div>
</li>
</ol>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-research-challenges-61" name="p-51309-research-challenges-61"></a>Research Challenges</h3>
<p>Key open problems:</p>
<ol>
<li>
<p><strong>Theoretical</strong></p>
<div class="math">
\begin{split}
   Challenges = \{&amp;Proof_{efficiency}, \\
   &amp;Circuit_{optimization}, \\
   &amp;Privacy_{techniques}, \\
   &amp;Recursion_{methods}\}
   \end{split}
</div>
</li>
<li>
<p><strong>Technical</strong></p>
<div class="math">
\begin{split}
   Engineering = \{&amp;Hardware_{speedup}, \\
   &amp;Storage_{scaling}, \\
   &amp;Proof_{compression}, \\
   &amp;Implementation_{tools}\}
   \end{split}
</div>
</li>
</ol>
<hr />
<p>The potential extensions of Overpass are comparable to the future expansions of a tech company’s product line. <strong>Cross-Chain Integration</strong> is like integrating new software platforms, allowing Overpass to connect seamlessly with other blockchain systems. <strong>Privacy Enhancements</strong> are akin to adding new security features to protect user data. <strong>Recursive Proofs</strong> enable Overpass to scale effortlessly, much like a company can expand its operations without compromising on quality or efficiency.</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-summary-and-comparative-analysis-62" name="p-51309-summary-and-comparative-analysis-62"></a>Summary and Comparative Analysis</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-core-protocol-achievements-63" name="p-51309-core-protocol-achievements-63"></a>Core Protocol Achievements</h3>
<p>The Overpass protocol demonstrates:</p>
<ol>
<li>
<p><strong>Security Guarantee:</strong></p>
<div class="math">
\begin{split}
   Security_{system} = &amp;Security_{ZK\text{-}SNARK} \\
   = &amp;2^{-\lambda}
   \end{split}
</div>
</li>
<li>
<p><strong>Performance:</strong></p>
<div class="math">
\begin{split}
   Performance = \{&amp;Time_{prove} = O(\log n), \\
   &amp;Time_{verify} = O(1), \\
   &amp;Time_{settlement} = O(1)\}
   \end{split}
</div>
</li>
<li>
<p><strong>Costs:</strong></p>
<div class="math">
\begin{split}
   Cost_{total} = &amp;Cost_{proof} + \\
   &amp;Cost_{storage} + \\
   &amp;(Optional)Cost_{L1}
   \end{split}
</div>
</li>
</ol>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-comparative-analysis-64" name="p-51309-comparative-analysis-64"></a>Comparative Analysis</h3>
<p><strong>Theorem (System Comparison):</strong><br />
Traditional L2s vs Overpass:</p>
<ol>
<li>
<p><strong>Security Model</strong></p>
<div class="math">
\begin{split}
   Security_{L2} &amp;= \begin{cases}
   Consensus_{security} &amp; \text{(Rollups)} \\
   Challenge_{period} &amp; \text{(Channels)} \\
   Watchtower_{reliability} &amp; \text{(Plasma)}
   \end{cases} \\
   Security_{Overpass} &amp;= 2^{-\lambda} \text{ (ZK proof)}
   \end{split}
</div>
</li>
<li>
<p><strong>Finality Time</strong></p>
<div class="math">
\begin{split}
   Time_{L2} &amp;= \begin{cases}
   O(\text{blocks}) &amp; \text{(Rollups)} \\
   O(\text{days}) &amp; \text{(Channels)} \\
   O(\text{hours}) &amp; \text{(Plasma)}
   \end{cases} \\
   Time_{Overpass} &amp;= O(1) \text{ (instant)}
   \end{split}
</div>
</li>
<li>
<p><strong>Dependencies</strong></p>
<div class="math">
\begin{split}
   Requires_{L2} &amp;= \begin{cases}
   Consensus &amp; \text{(Rollups)} \\
   Counterparty &amp; \text{(Channels)} \\
   Watchtowers &amp; \text{(Plasma)}
   \end{cases} \\
   Requires_{Overpass} &amp;= \text{None (self-proving)}
   \end{split}
</div>
</li>
</ol>
<p><strong>Intuitive Explanation:</strong><br />
This theorem compares the security models, finality times, and dependencies of traditional Layer-2 solutions (Rollups, Channels, Plasma) with Overpass. It highlights how Overpass achieves a higher security guarantee with instant finality and minimal dependencies.</p>
<p><strong>Formal Proof:</strong></p>
<ul>
<li><strong>Security Model:</strong> Traditional L2s rely on consensus mechanisms, challenge periods, or watchtowers, each introducing potential vulnerabilities. Overpass relies solely on the soundness of ZK-SNARKs, providing a direct security guarantee of <span class="math">2^{-\lambda}</span>.</li>
<li><strong>Finality Time:</strong> Rollups depend on block confirmations, Channels require waiting periods for challenges, and Plasma relies on watchtower operations. Overpass, through instant proof verification, achieves finality in constant time.</li>
<li><strong>Dependencies:</strong> Traditional L2s require consensus participation, active counterparty engagement, or reliable watchtowers. Overpass eliminates these dependencies by enabling self-proving state transitions.</li>
</ul>
<p>Therefore, Overpass offers superior security and efficiency compared to traditional Layer-2 solutions.<br />
\end{proof}</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-key-innovations-65" name="p-51309-key-innovations-65"></a>Key Innovations</h3>
<p><strong>Theorem (Core Advantages):</strong><br />
Overpass provides fundamental benefits:</p>
<ol>
<li>
<p><strong>Unilateral Operation</strong></p>
<div class="math">
\begin{split}
   Independent_{operation} = &amp;No_{consensus} \land \\
   &amp;No_{counterparty} \land \\
   &amp;No_{watchtowers}
   \end{split}
</div>
</li>
<li>
<p><strong>Pure Cryptographic Security</strong></p>
<div class="math">
\begin{split}
   Security_{basis} = &amp;ZK\text{-}SNARK_{soundness} \land \\
   &amp;Hash_{collision} \land \\
   &amp;Merkle_{binding}
   \end{split}
</div>
</li>
<li>
<p><strong>Practical Efficiency</strong></p>
<div class="math">
\begin{split}
   Cost_{update} &amp;= O(\log n) \text{ computation} \\
   Time_{update} &amp;= O(1) \text{ latency} \\
   Storage_{update} &amp;= O(1) \text{ space}
   \end{split}
</div>
</li>
</ol>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
This theorem summarizes the primary advantages of Overpass, emphasizing its ability to operate unilaterally without reliance on consensus mechanisms or counterparties, its robust cryptographic security, and its efficient computational and storage requirements.</p>
<p><strong>Formal Proof:</strong></p>
<ul>
<li><strong>Unilateral Operation:</strong> Overpass allows participants to independently update their state without needing consensus, counterparty involvement, or watchtowers, as demonstrated in previous theorems.</li>
<li><strong>Pure Cryptographic Security:</strong> The security of Overpass is based solely on the soundness of ZK-SNARKs, collision resistance of hash functions, and binding properties of Merkle trees.</li>
<li><strong>Practical Efficiency:</strong>
<ul>
<li>Computational costs scale logarithmically with the number of channels (<span class="math">O(\log n)</span>).</li>
<li>Proof verification and state updates occur in constant time (<span class="math">O(1)</span>).</li>
<li>Storage requirements per update remain constant (<span class="math">O(1)</span>).</li>
</ul>
</li>
</ul>
<p>Thus, Overpass achieves its core advantages through its innovative design and efficient implementation.</p>
<hr />
<p>In summary, Overpass stands out in the blockchain landscape much like a high-performance sports car in the automotive industry. While traditional Layer-2 solutions are like standard vehicles—reliable but limited in speed and efficiency—Overpass offers unparalleled performance with instant finality, minimal costs, and robust security. This makes it an attractive choice for businesses seeking both speed and reliability without the complexities and limitations of existing solutions.</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-final-conclusions-66" name="p-51309-final-conclusions-66"></a>Final Conclusions</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-key-contributions-67" name="p-51309-key-contributions-67"></a>Key Contributions</h3>
<ol>
<li>
<p><strong>Theoretical</strong></p>
<ul>
<li>Novel unilateral ZKP channel design</li>
<li>Pure cryptographic security model</li>
<li>Self-proving state transitions</li>
<li>Instant mathematical finality</li>
</ul>
</li>
<li>
<p><strong>Technical</strong></p>
<ul>
<li>Efficient proof circuits</li>
<li>Minimal dependencies</li>
<li>Simple state model</li>
<li>Practical implementation</li>
</ul>
</li>
<li>
<p><strong>Practical</strong></p>
<ul>
<li>Production-ready design</li>
<li>Clear scaling path</li>
<li>Low operating costs</li>
<li>Easy deployment</li>
</ul>
</li>
</ol>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-impact-68" name="p-51309-impact-68"></a>Impact</h3>
<p>The Overpass protocol represents a fundamental rethinking of blockchain scaling:</p>
<ol>
<li>
<p><strong>Novel Paradigm</strong></p>
<ul>
<li>Proofs replace consensus</li>
<li>Unilateral replaces bilateral</li>
<li>Mathematics replaces game theory</li>
<li>Simplicity replaces complexity</li>
</ul>
</li>
<li>
<p><strong>Real-World Benefits</strong></p>
<ul>
<li>Instant finality</li>
<li>Independent operation</li>
<li>Mathematical security</li>
<li>Practical efficiency</li>
</ul>
</li>
</ol>
<hr />
<p>The final conclusions highlight Overpass as a transformative innovation in blockchain technology, comparable to the advent of the internet in the late 20th century. Just as the internet revolutionized communication and commerce by enabling instant, scalable interactions, Overpass redefines blockchain transactions by offering deterministic consensus, instant finality, and unlimited scalability. These mathematical guarantees lay the foundation for building real-world financial infrastructure that is as reliable and efficient as critical systems in aviation or space exploration.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-practical-applications-and-use-cases-69" name="p-51309-practical-applications-and-use-cases-69"></a><strong>Practical Applications and Use Cases</strong></h3>
<p><strong>Practical Insert:</strong></p>
<p><strong>High-Frequency Trading Platforms:</strong><br />
Imagine a stock exchange where trades are executed and settled within milliseconds, ensuring traders can capitalize on fleeting market opportunities without delay. Overpass provides the mathematical certainty and speed required for such high-stakes environments, eliminating risks like double-spending and ensuring fair execution orders.</p>
<p><strong>Retail Payment Networks:</strong><br />
Picture a global retail chain where customers can make purchases instantly without worrying about transaction delays or high fees. Overpass enables point-of-sale transactions to finalize instantly with minimal costs, enhancing customer experience and reducing operational expenses for retailers.</p>
<p><strong>Cross-Border Payments:</strong><br />
Consider international businesses that require swift and reliable cross-border transactions without the unpredictability of traditional banking systems. Overpass offers mathematically guaranteed settlement and predictable execution times, streamlining global commerce with transparent and efficient fee structures.</p>
<p><strong>Financial Services:</strong><br />
Envision financial institutions that can automate compliance checks and instantaneously reconcile transactions, all while maintaining tamper-proof audit trails. Overpass empowers these services with the necessary mathematical proofs to ensure integrity and efficiency, revolutionizing how financial operations are conducted.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51309-references-70" name="p-51309-references-70"></a>References</h2>
<ol>
<li>
<p>Ramsay, B. “Cryptskii” (2024). Overpass Channels: Horizontally Scalable, Privacy-Enhanced, with Independent Verification, Fluid Liquidity, and Robust Censorship Proof, Payments. <em>Cryptology ePrint Archive</em>, Paper 2024/1526. <a href="https://eprint.iacr.org/2024/1526" rel="noopener nofollow ugc">Overpass v1</a></p>
</li>
<li>
<p>Hioki, L., Dompeldorius, A., &amp; Hashimoto, Y. (2024). Plasma Next: Plasma without Online Requirements. <em>Ethereum Research</em>. Retrieved from <a href="https://ethresear.ch/t/plasma-next-plasma-without-online-requirements/18786">Plasma Next</a></p>
</li>
<li>
<p>Nakamoto, S. (2008). Bitcoin: A Peer-to-Peer Electronic Cash System. <a href="https://bitcoin.org/bitcoin.pdf" rel="noopener nofollow ugc">Bitcoin</a></p>
</li>
<li>
<p>Merkle, R. C. (1987). A Digital Signature Based on a Conventional Encryption Function. In <em>Advances in Cryptology — CRYPTO ’87</em> (pp. 369–378). Springer Berlin Heidelberg.</p>
</li>
<li>
<p>Groth, J. (2016). On the Size of Pairing-Based Non-interactive Arguments. In <em>Annual International Conference on the Theory and Applications of Cryptographic Techniques</em> (pp. 305–326). Springer.</p>
</li>
<li>
<p>Ben-Sasson, E., Bentov, I., Horesh, Y., &amp; Riabzev, M. (2019). Scalable Zero Knowledge with No Trusted Setup. In <em>Advances in Cryptology – CRYPTO 2019</em> (pp. 701–732). Springer.</p>
</li>
<li>
<p>Buterin, V. (2016). Chain Interoperability. <em>R3 Research Paper</em>.</p>
</li>
<li>
<p>Poon, J., &amp; Dryja, T. (2016). The Bitcoin Lightning Network: Scalable Off-Chain Instant Payments. <em>Technical Report</em>.</p>
</li>
<li>
<p>Khalil, R., &amp; Gervais, A. (2018). NOCUST – A Non-Custodial 2nd-Layer Financial Intermediary. <em>Cryptology ePrint Archive</em>, Report 2018/642.</p>
</li>
<li>
<p>Gudgeon, L., Moreno-Sanchez, P., Roos, S., McCorry, P., &amp; Gervais, A. (2020). SoK: Layer-Two Blockchain Protocols. In <em>Financial Cryptography and Data Security</em>. Springer.</p>
</li>
<li>
<p>Zamani, M., Movahedi, M., &amp; Raykova, M. (2018). RapidChain: Scaling Blockchain via Full Sharding. In <em>Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security</em> (pp. 931–948).</p>
</li>
<li>
<p>Kokoris-Kogias, E., Jovanovic, P., Gasser, L., Gailly, N., Syta, E., &amp; Ford, B. (2018). OmniLedger: A Secure, Scale-Out, Decentralized Ledger via Sharding. In <em>2018 IEEE Symposium on Security and Privacy (SP)</em> (pp. 583–598).</p>
</li>
<li>
<p>Yu, M., Sahraei, S., Li, S., Avestimehr, S., Kannan, S., &amp; Viswanath, P. (2020). Ohie: Blockchain Scaling Made Simple. In <em>IEEE Symposium on Security and Privacy (SP)</em>.</p>
</li>
<li>
<p>Goldberg, S., Naor, M., Papadopoulos, D., &amp; Reyzin, L. (2020). SPHINX: A Password Store that Perfectly Hides Passwords from Itself. In <em>2020 IEEE Symposium on Security and Privacy (SP)</em> (pp. 1051–1069).</p>
</li>
<li>
<p>Boneh, D., Bünz, B., &amp; Fisch, B. (2019). Batching Techniques for Accumulators with Applications to IOPs and Stateless Blockchains. In <em>Annual International Cryptology Conference</em> (pp. 561–586). Springer.</p>
</li>
<li>
<p>Gabizon, A., Williamson, Z., &amp; Ciobotaru, O. (2019). PLONK: Permutations over Lagrange-bases for Oecumenical Noninteractive Arguments of Knowledge. <em>Cryptology ePrint Archive</em>, Report 2019/953.</p>
</li>
<li>
<p>Maller, M., Bowe, S., Kohlweiss, M., &amp; Meiklejohn, S. (2019). Sonic: Zero-Knowledge SNARKs from Linear-Size Universal and Updateable Structured Reference Strings. In <em>Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security</em> (pp. 2111–2128).</p>
</li>
<li>
<p>Ben-Sasson, E., Bentov, I., Horesh, Y., &amp; Riabzev, M. (2018). Scalable, Transparent, and Post-Quantum Secure Computational Integrity. <em>Cryptology ePrint Archive</em>, Report 2018/046.</p>
</li>
<li>
<p>Chiesa, A., Hu, Y., Maller, M., Mishra, P., Vesely, N., &amp; Ward, N. (2019). Marlin: Preprocessing zkSNARKs with Universal and Updatable SRS. <em>Cryptology ePrint Archive</em>, Report 2019/1047.</p>
</li>
<li>
<p>Bünz, B., Bootle, J., Boneh, D., Poelstra, A., Wuille, P., &amp; Maxwell, G. (2018). Bulletproofs: Short Proofs for Confidential Transactions and More. In <em>2018 IEEE Symposium on Security and Privacy (SP)</em> (pp. 315–334).</p>
</li>
<li>
<p>Wang, J., &amp; Wang, H. (2020). Fractal: Post-Quantum and Transparent Recursive Proofs from Holography. <em>Cryptology ePrint Archive</em>, Report 2020/1280.</p>
</li>
<li>
<p>Tomescu, A., Abraham, I., Buterin, V., Drake, J., Feist, D., &amp; Khovratovich, D. (2020). Aggregatable Subvector Commitments for Stateless Cryptocurrencies. In <em>Security and Cryptography for Networks</em> (pp. 45–64). Springer.</p>
</li>
</ol>
            <p><small>7 posts - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/deterministic-consensus-using-overpass-channels-2-in-distributed-ledger-technology/21046">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 19 Nov 2024 18:21:41 +0000</pubDate>
</item>
<item>
<title>Block Arrivals, Home Stakers &amp; Bumping the blob count</title>
<link>https://ethresear.ch/t/block-arrivals-home-stakers-bumping-the-blob-count/21096</link>
<guid>https://ethresear.ch/t/block-arrivals-home-stakers-bumping-the-blob-count/21096</guid>
<content:encoded><![CDATA[
<div> 关键词：区块到达时间、家庭质押者、blob计数、网络带宽、EIP7623

总结:<br />
本文分析了以太坊网络中区块和blob到达时间数据，发现由于家庭质押者的参与，对网络状况有了更深入的理解。研究显示，在当前最敏感的带宽场景下，区块和blob到达时间表现健康。结合EIP7623提案，分析结果支持将blob计数增加到4/8或6/9。文章指出，家庭质押者在网络中的地位重要，但他们面临的带宽资源有限，因此其成为评估blob计数增加安全性的关键群体。通过考察过去六周的数据，得出结论：仅从区块/blob到达时间这一指标来看，将blob计数从（目标3/最大6）提高到（目标4/最大8）或（目标6/最大9）似乎是可行的，但这只是决定blob计数调整时需要考虑的多个因素之一。 <div>
<h1><a class="anchor" href="https://ethresear.ch#p-51417-block-arrivals-home-stakers-bumping-the-blob-count-1" name="p-51417-block-arrivals-home-stakers-bumping-the-blob-count-1"></a>Block Arrivals, Home Stakers &amp; Bumping the blob count</h1>
<blockquote>
<p>Thanks to all the community members that are selflessly sharing their data that allowed this analysis to be possible, to <a href="https://migalabs.io/" rel="noopener nofollow ugc">MigaLabs</a> for their validator entity data and to <a href="https://probelab.io" rel="noopener nofollow ugc">ProbeLab</a> for an early version of their Mainnet bandwidth report.</p>
<p>Special thanks to <a href="https://twitter.com/parithoshj" rel="noopener nofollow ugc">Parithosh</a>, <a href="https://twitter.com/nero_eth" rel="noopener nofollow ugc">Toni</a>, <a href="https://twitter.com/cortze_" rel="noopener nofollow ugc">Mikel</a>, <a href="https://twitter.com/savid" rel="noopener nofollow ugc">Andrew</a>, and <a href="https://twitter.com/mattevansnz" rel="noopener nofollow ugc">Matty</a> for their feedback and help throughout this analysis.</p>
</blockquote>
<h2><a class="anchor" href="https://ethresear.ch#p-51417-tldr-2" name="p-51417-tldr-2"></a>TL;DR</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51417-summary-3" name="p-51417-summary-3"></a>Summary</h3>
<ul>
<li>Timing games have historically made it hard to predict a safe blob count bump</li>
<li>ethPandaOps recently started receiving block arrival timing data from the community</li>
<li>Analysis of the most bandwidth sensitive scenario shows a healthy block &amp; blobs arrival time</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51417-outcomes-4" name="p-51417-outcomes-4"></a>Outcomes</h3>
<ul>
<li>When naively extrapolating this data and combining with EIP7623, <strong>this analysis supports increasing the blob count in <a href="https://eips.ethereum.org/EIPS/eip-7691" rel="noopener nofollow ugc">EIP7691</a> to either 4/8 or 6/9.</strong>
<ul>
<li>This is only one data point, but it leads to a more optimistic outlook on a potential blob count bump than previous analysis.</li>
</ul>
</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-51417-intro-5" name="p-51417-intro-5"></a>Intro</h2>
<p>The <a href="https://ethpandaops.io/posts/contribute-to-xatu-data/" rel="noopener nofollow ugc">ethPandaOps team has recently started receiving data from members of the community</a>. Home stakers are one of the Ethereum network’s most valuable assets, and this scheme is starting to shine a light on how they see the network. A <a href="https://ethpandaops.io/data/xatu/#get-started" rel="noopener nofollow ugc">sidecar binary</a> is run alongside a beacon node, and records the events happening on the node via the beacon API event stream. These events are sent to the ethPandaOps team, who then publishes the data (with a small delay &amp; some data scrubbing).</p>
<p>For more info:</p>
<ul>
<li><a href="https://ethpandaops.io/posts/contribute-to-xatu-data/#data-collection" rel="noopener nofollow ugc">Data collection</a></li>
<li><a href="https://ethpandaops.io/data/xatu/#available-data" rel="noopener nofollow ugc">Accessing the data</a></li>
</ul>
<p>The team has been collecting community data for around 6 weeks, and we now have enough data to make some interesting observations that were previously not possible.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51417-background-6" name="p-51417-background-6"></a>Background</h2>
<p>With the arrival of <a href="https://eips.ethereum.org/EIPS/eip-4844" rel="noopener nofollow ugc">EIP4844</a>, a block is only considered valid once the block &amp; the blobs referenced in the block are received by a node. This block/blob bundle has until 4s in the slot to be received by the majority of the network otherwise it runs the risk of being re-orged.</p>
<p><a href="https://timing.pics/" rel="noopener nofollow ugc">Sophisticated operators (and now MEV-Relays) play block proposal timing games</a>. These operators submit their blocks as late as possible to maximise their profit while minimizing their risk of being re-orged. These timing games have historically obfuscated block arrival timing data.</p>
<p><a href="https://probelab.io/" rel="noopener nofollow ugc">Unreleased, upcoming research from ProbeLab</a> indicates that the 50th percentile of non-cloud nodes on the network have 20Mbps of upload bandwidth.</p>
<p><a href="https://dune.com/queries/3757544/6319515" rel="noopener nofollow ugc">Blob usage on Mainnet continues to grow.</a></p>
<h2><a class="anchor" href="https://ethresear.ch#p-51417-problem-statement-7" name="p-51417-problem-statement-7"></a>Problem statement</h2>
<p>Ethereum’s decentralization is paramount to it’s success. <a href="https://eips.ethereum.org/EIPS/eip-7691" rel="noopener nofollow ugc">EIP7691</a> aims to increase the blob count, and runs the risk of unintentionally excluding some node operators from it’s set if the parameters are too high.</p>
<p>We need to:</p>
<ul>
<li>Ensure that a blob count increase is safe for home stakers, as this group of actors is the “worst case” for a blob count increase as they have the lowest available bandwidth.</li>
<li>Ensure that the network has enough data throughput to support Layer 2 growth.</li>
</ul>
<p>Given the existing landscape, we can make some assumptions with regards to a potential blob count increase:</p>
<p><strong>Least at risk:</strong><br />
Counterintuitively, if you looked at block arrival data, operators playing timing games would appear to be the most at risk of being impacted by a blob count increase. Being reorged out for proposing late is bad for business, and we can assume they’ll adjust their block proposal timings accordingly. <strong>A blob count increase is unlikely to be problematic here.</strong></p>
<p><strong>Most at risk:</strong><br />
A solo staker building a block locally (no mev-relay) and being attested to by other home stakers. In this scenario:</p>
<ul>
<li>The proposer:
<ul>
<li>needs to publish their block and all blobs to the network. This node needs to publish the block (~100KB), and then all of the blobs (128KB each) to all of its mesh peers as fast as possible.
<ul>
<li>when building blocks locally, the proposer does not have the help of the MEV Relay gossiping the block/blobs bundle to its own peers.</li>
</ul>
</li>
</ul>
</li>
<li>The attesters:
<ul>
<li>need to download the block/blobs bundle from the p2p network before 4s in to the slot.</li>
</ul>
</li>
</ul>
<p><strong>This analysis will ask the following questions:</strong></p>
<ul>
<li><strong>Question 1</strong>: How is 3/6 performing on Mainnet?</li>
<li><strong>Question 2</strong>: Does arrival time scale with block/blob bundle size?</li>
<li><strong>Question 3</strong>: How much more can we support on Mainnet today?</li>
</ul>
<p><strong>We’ll answer these questions from the perspective of a home staker as this is our most at-risk group of operators.</strong></p>
<h2><a class="anchor" href="https://ethresear.ch#p-51417-analysis-8" name="p-51417-analysis-8"></a>Analysis</h2>
<pre><code class="lang-auto">Start: 2024-10-04T22:00:00Z
End: 2024-11-25T02:00:00Z
Blocks: 366,384
Arrival events: 75,945,392
Countries: 9 (Australia, Bulgaria, Czechia, Germany, Italy, Spain, The Netherlands, United Kingdom, United States)
</code></pre>
<p>Check out the juypter notebook <a href="https://github.com/ethpandaops/xatu-data/blob/6ff49435d3e2bd078155ad06040d584e89ea3289/analysis/mainnet/blob-bump-prediction/prediction.ipynb" rel="noopener nofollow ugc">here</a>.</p>
<p>The timing data was captured by Xatu Sentry, a sidecar binary that runs alongside a beacon node and records many events via the beacon API event stream. These events can be considered <em>worst case</em> for block arrival times as there is additional processing time and network overhead for the event to be recorded. From analysis, this overhead ranges between 50-300ms but we have kept the timing data as-is in the interest of safety.</p>
<p>Each beacon node implementation emits <code>block</code>, <code>head</code>, and <code>blob_sidecar</code> events in different orders. To address this, we define a block/blob bundle as “arrived” only after all associated events for the slot have been recorded from each beacon node. This is once again a <em>worst case</em> scenario.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51417-question-1-9" name="p-51417-question-1-9"></a>Question 1</h3>
<p><strong>How is 3/6 performing on Mainnet?</strong></p>
<blockquote>
<p>TL;DR: Pretty well!</p>
</blockquote>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/2/3221f9f6e15311cbe773da0ffec6cd9df34fe899.png" title=""><img alt="" height="475" src="https://ethresear.ch/uploads/default/optimized/3X/3/2/3221f9f6e15311cbe773da0ffec6cd9df34fe899_2_690x475.png" width="690" /></a></div><p></p>
<p>This chart shows block/blob bundle seen arrival times against the combined size of the bundle. When looking at locally built blocks proposed by solo stakers and seen by home users, a lot of block/blob bundles are seen before 4s.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/0/503985d3e43892b27f5b8b85bdb4282958eb96a4.png" title=""><img alt="" height="475" src="https://ethresear.ch/uploads/default/optimized/3X/5/0/503985d3e43892b27f5b8b85bdb4282958eb96a4_2_690x475.png" width="690" /></a></div><p></p>
<p>We should also look at blocks provided by MEV Relay as the reality is that a lot of blocks are proposed via this route. We can see an increase in the <code>min</code>, as there are additional round trips involved in this process, but things still look healthy!</p>
<p><strong>Outcome:</strong> Block/blob bundles are arriving well within the 4s deadline for our home users.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51417-question-2-10" name="p-51417-question-2-10"></a>Question 2</h3>
<p><strong>Does arrival time scale with block/blob bundle size?</strong></p>
<blockquote>
<p>TL;DR: Yes</p>
</blockquote>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/5/95b81d2dec4ec6041818b4b717f5961957a0f90e.jpeg" title=""><img alt="" height="471" src="https://ethresear.ch/uploads/default/optimized/3X/9/5/95b81d2dec4ec6041818b4b717f5961957a0f90e_2_690x471.jpeg" width="690" /></a></div><p></p>
<p>The trend lines show the 99th, 95th, and 50th percentiles of arrival times - meaning what percentage of blocks arrive were seen faster than that line.</p>
<p>These percentile trend lines answer our question: as bundle sizes increase, arrival times also increase. This suggests bandwidth is the primary bottleneck for these actors.<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/7/576cfaad3fa0db3276a657322f7180f6a4edf1b8.jpeg" title=""><img alt="" height="471" src="https://ethresear.ch/uploads/default/optimized/3X/5/7/576cfaad3fa0db3276a657322f7180f6a4edf1b8_2_690x471.jpeg" width="690" /></a></div><p></p>
<p>Again looking at blocks provided via MEV Relay, we see a similar story.</p>
<p><strong>Outcome:</strong> Yes, arrival times scale with block/blob bundles size</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51417-question-3-11" name="p-51417-question-3-11"></a>Question 3:</h3>
<p><strong>How much more can we support on Mainnet today?</strong></p>
<blockquote>
<p>TLDR: 4/8 and 6/9 are both achievable</p>
</blockquote>
<p>To answer this question we need to check how big the block is. The 99th percentile of compressed beacon block size through our time period is <code>101KB</code>. Our blobs are fixed at a size of <code>128KB</code>.</p>
<p>Using these parameters, we can overlay the block/blob count:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/c/3c3dbd09011f7924bf7c6eb43536b51fcb659c66.jpeg" title=""><img alt="" height="471" src="https://ethresear.ch/uploads/default/optimized/3X/3/c/3c3dbd09011f7924bf7c6eb43536b51fcb659c66_2_690x471.jpeg" width="690" /></a></div><p></p>
<p>We can <strong>very naively</strong> plot a trend line to see when would cross the 4s attestation deadline.<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/5/55e090af2d5dcc7100554dc6d544be282c54d989.jpeg" title=""><img alt="" height="471" src="https://ethresear.ch/uploads/default/optimized/3X/5/5/55e090af2d5dcc7100554dc6d544be282c54d989_2_690x471.jpeg" width="690" /></a></div><p></p>
<p>This trend line assumes a linear relationship between blob count and arrival time. Under this assumption, we can support up to 14 blobs per block while maintaining 95% of block/blob bundles arriving within the deadline. The 95% target provides margin for the 50-300ms processing overhead in our measurements, while also accounting for outliers.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/e/9ea71149e7ec70d2fb3c96f23538dbc1c85a79fb.jpeg" title=""><img alt="" height="471" src="https://ethresear.ch/uploads/default/optimized/3X/9/e/9ea71149e7ec70d2fb3c96f23538dbc1c85a79fb_2_690x471.jpeg" width="690" /></a></div><p></p>
<p>When looking at MEV Relay blocks specifically, the data shows an even more optimistic picture - the 95th percentile trend line indicates support for up to 20 blobs per block. This improved performance can be attributed to MEV Relays being high-bandwidth nodes that help distribute blocks and blobs across the network in parallel with the proposer.</p>
<h5><a class="anchor" href="https://ethresear.ch#p-51417-eip7623-12" name="p-51417-eip7623-12"></a>EIP7623</h5>
<p><a href="https://eips.ethereum.org/EIPS/eip-7623" rel="noopener nofollow ugc">EIP7623</a> improves the worst case compressed block size to ~720KB - about 7x larger than our average historical block. Let’s analyze if we can still support more blobs with this increased block size.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/d/fd1ab2e34d37c616c3f6a59bd19cab1e5daf7cbe.jpeg" title=""><img alt="" height="471" src="https://ethresear.ch/uploads/default/optimized/3X/f/d/fd1ab2e34d37c616c3f6a59bd19cab1e5daf7cbe_2_690x471.jpeg" width="690" /></a></div><p></p>
<p>Even with an absolute <em>worst case</em> block size with <a href="https://eips.ethereum.org/EIPS/eip-7623" rel="noopener nofollow ugc">EIP7623</a> we still support a blob increase. Note that the current maximum compressed block size on Mainnet is 1.79MB (and we’re seemingly going ok!), so take this data point with a grain of salt.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/d/fdcce8fa1e9401070d2d913db490fac1667c47a0.jpeg" title=""><img alt="" height="472" src="https://ethresear.ch/uploads/default/optimized/3X/f/d/fdcce8fa1e9401070d2d913db490fac1667c47a0_2_690x472.jpeg" width="690" /></a></div><br />
The trend for MEV Relay blocks again supports a much higher blob count compared to locally built blocks.<p></p>
<p><strong>Outcome:</strong> The data supports a blob count increase, especially if <a href="https://eips.ethereum.org/EIPS/eip-7623" rel="noopener nofollow ugc">EIP7623</a> is included at the same time. 4/8 or 6/9 are both safe to apply. There is potential for a higher blob count, but we’ll need to see how the network performs with an initial bump first.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51417-conclusion-13" name="p-51417-conclusion-13"></a>Conclusion</h3>
<p>Ethereum’s decentralization is fundamental, and home stakers play a crucial role in this picture. The network is a delicate and intricate system that demands thoughtful and deliberate consideration.</p>
<p>Our analysis indicates that block arrival performance surpasses initial expectations for nodes with limited bandwidth. The community-contributed data offers valuable real-world insights into the network’s capabilities, and we would like to once again thank those who are contributing their data.</p>
<p>While we naively assumed a linear relationship between blob count and arrival time, this is a simplified view of a highly complex distributed system. Additionally, there are ongoing work streams that could either improve or worsen bandwidth requirements over time. Our analysis is focused on the data available to us now, based on observations from the past six weeks of network performance.</p>
<p>Based on block/blob arrival metrics alone, increasing the blob count from (target 3/max 6) to (target 4/max 8) or (target 6/max 9) appears to be viable. <strong>However, this is just one of many factors to evaluate when deciding on blob count adjustments.</strong></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/block-arrivals-home-stakers-bumping-the-blob-count/21096">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 27 Nov 2024 12:36:03 +0000</pubDate>
</item>
<item>
<title>Deterministic Consensus using Overpass Channels in Distributed Ledger Technology</title>
<link>https://ethresear.ch/t/deterministic-consensus-using-overpass-channels-in-distributed-ledger-technology/21046</link>
<guid>https://ethresear.ch/t/deterministic-consensus-using-overpass-channels-in-distributed-ledger-technology/21046</guid>
<content:encoded><![CDATA[
<div> 关键词：Overpass、零知识证明、状态通道、即时最终性、扩展性

总结:<br />
本文介绍了Overpass协议，这是一种基于零知识证明（ZK-SNARKs）的新型二层区块链扩容解决方案。Overpass实现了单边状态通道，允许参与者无需共识、对方在线或看门人即可进行独立的状态更新，从而达到即时最终性。协议提供了严格的数学安全性保证，其安全边界为2^-λ，并具有线性的吞吐量扩展性和对数级的成本增长。此外，Overpass满足了高交易频率市场、零售支付网络、跨境支付和金融服务等多个实际应用场景的需求。文章通过理论分析、性能评估以及经济模型探讨了该协议的设计原理、优势及未来可能的研究方向。 <div>
<p><strong>Brandon “Cryptskii” Ramsay</strong><br />
info@overpass.network<br />
November 26th 2024</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-abstract-1" name="p-51309-abstract-1"></a>Abstract</h2>
<p>This paper presents a formal analysis of the Overpass protocol, a novel layer-2 scaling solution that achieves deterministic consensus through self-proving unilateral state channels. The protocol achieves unprecedented guarantees through zero-knowledge proofs:</p>
<ul>
<li>Instant finality through self-proving state transitions</li>
<li>Unilateral updates without counterparty presence</li>
<li>Provable security bounds of <span class="math">2^{-\lambda}</span> for all attack vectors</li>
<li>Throughput scaling of <span class="math">O(n \cdot m)</span> for <span class="math">n</span> wallets and <span class="math">m</span> channels</li>
<li>Sub-logarithmic cost scaling (<span class="math">O(\log d)</span> for tree depth <span class="math">d</span>)</li>
</ul>
<p>Through rigorous mathematical proofs, we demonstrate how the protocol’s novel combination of zero-knowledge proofs and Sparse Merkle Trees (SMTs) enables individual participants to make instant, provably valid state transitions without consensus, watchtowers, or challenge periods. Each state update generates its own mathematical proof of correctness, enabling true unilateral operation while maintaining global consistency. We establish comprehensive security properties, performance characteristics, and economic guarantees through both theoretical analysis and practical examples.</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-introduction-2" name="p-51309-introduction-2"></a>Introduction</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-problem-statement-and-motivation-3" name="p-51309-problem-statement-and-motivation-3"></a>Problem Statement and Motivation</h3>
<p>Layer-2 blockchain scaling solutions face a fundamental trilemma:</p>
<ul>
<li><strong>Finality vs. Liveness:</strong> Traditional channels require both parties online or complex challenge periods</li>
<li><strong>Security vs. Speed:</strong> Existing solutions trade instant finality for probabilistic security</li>
<li><strong>Scalability vs. Cost:</strong> Higher throughput typically requires expensive validation</li>
</ul>
<p>Existing approaches face fundamental limitations:</p>
<div class="math">
\begin{split}
State_{channels} &amp;: \text{Requires bilateral presence} \\
Lightning &amp;: \text{Uses challenge periods} \\
Plasma &amp;: \text{Needs watchtowers} \\
Rollups &amp;: \text{Requires consensus}
\end{split}
</div>
<p>Consider Alice’s high-volume marketplace requirements:</p>
<ul>
<li>Transaction volume: <span class="math">&gt;10^4</span> microtransactions daily</li>
<li>Finality requirement: Instant (no waiting periods)</li>
<li>Security threshold: <span class="math">Pr[\text{double-spend}] \leq 2^{-128}</span></li>
<li>Maximum per-transaction cost: <span class="math">\leq \$0.01</span></li>
<li>Operational constraint: Must work when counterparties offline</li>
</ul>
<p>No existing layer-2 solution satisfies these constraints because they all require some form of interactive protocol, challenge period, or external validation.</p>
<hr />
<p><strong>The fundamental challenge Overpass solves:</strong> enabling thousands of fast, cheap transactions while maintaining security. Think of traditional blockchain systems like a busy highway with a single toll booth—everyone must wait in line and pay a high fee. Overpass, instead, creates multiple parallel roads (channels) with automated toll systems (cryptographic proofs), allowing many people to travel simultaneously while still ensuring no one can cheat the system.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-contribution-4" name="p-51309-contribution-4"></a>Contribution</h3>
<p>The Overpass protocol resolves these challenges through a fundamental innovation: self-proving unilateral state channels. Each state transition generates its own zero-knowledge proof of validity, enabling:</p>
<ol>
<li>
<p><strong>Unilateral Operation:</strong></p>
<div class="math">
Valid(State_{new}) \iff VerifyProof(\pi_{validity})  
</div>
<p>No other validation required.</p>
</li>
<li>
<p><strong>Hierarchical State Management:</strong></p>
<div class="math">
\mathcal{H} = \{Root \rightarrow Wallet \rightarrow Channel\}
</div>
<p>Each level maintains independent proof verification.</p>
</li>
<li>
<p><strong>Security Guarantees:</strong></p>
<div class="math">
\begin{split}
   Security_{total} &amp;= Pr[\text{Break proof system}] \\
   &amp;\leq 2^{-\lambda}
   \end{split}
</div>
</li>
<li>
<p><strong>Performance Characteristics:</strong></p>
<div class="math">
\begin{split}
   TPS_{Overpass} &amp;= O(n \cdot m) \text{ (throughput)} \\
   Time_{finality} &amp;= O(1) \text{ (instant)} \\
   Cost_{tx} &amp;= O(\log d) \text{ (proof size)}
   \end{split} 
</div>
</li>
</ol>
<p>Where:</p>
<ul>
<li><span class="math">n</span>: Number of parallel channels (<span class="math">\approx 2^{20}</span> practical maximum)</li>
<li><span class="math">m</span>: Transactions per channel (<span class="math">\approx 2^{16}</span> practical maximum)</li>
<li><span class="math">d</span>: Tree depth (<span class="math">= \log_2(n \cdot m)</span>)</li>
<li><span class="math">\lambda</span>: Security parameter (<span class="math">\geq 128</span> bits)</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-what-this-means-in-practice-5" name="p-51309-what-this-means-in-practice-5"></a>What This Means in Practice</h3>
<p>The key innovation in Overpass is that each participant can independently update their state by generating mathematical proofs, without requiring counterparty presence, consensus, watchtowers, or challenge periods. This achieves:</p>
<ul>
<li><strong>True Unilateral Operation:</strong> Updates possible while counterparty offline</li>
<li><strong>Instant Finality:</strong> No waiting for confirmations or challenges</li>
<li><strong>Mathematical Security:</strong> Proofs guarantee correctness</li>
<li><strong>Minimal Trust:</strong> No reliance on external validators</li>
<li><strong>Practical Performance:</strong> <span class="math">&gt;10^6</span> TPS at <span class="math">&lt;\$0.01</span> per transaction</li>
</ul>
<p>The following sections provide rigorous mathematical proofs of these claims, along with practical implementation guidance.</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-system-model-and-core-definitions-6" name="p-51309-system-model-and-core-definitions-6"></a>System Model and Core Definitions</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-network-model-7" name="p-51309-network-model-7"></a>Network Model</h3>
<p><strong>Definition (Network Model)</strong><br />
The Overpass network consists of:</p>
<div class="math">
\begin{split}
System = \{&amp; Participants, Channels, L1\} \text{ where:} \\
Participants &amp;= \{p_1, ..., p_k\} \text{ (channel owners)} \\
Channels &amp;= \{c_1, ..., c_m\} \text{ (state channels)} \\
L1 &amp;= \text{Settlement layer (e.g., Ethereum)}
\end{split}
</div>
<p><strong>Key Property:</strong> No synchronous communication required between participants.</p>
<hr />
<p>Imagine the Overpass network as a bustling financial district in a major city. The <strong>Participants</strong> are like the businesses and individuals operating within this district, each managing their own accounts (channels). The <strong>Channels</strong> are the various financial roads connecting these accounts, facilitating seamless transactions. The <strong>L1 Settlement Layer</strong> acts as the central bank, ensuring that all transactions are officially recorded and secured. Just as no physical roads require all drivers to communicate with each other to navigate, Overpass channels operate independently without needing constant communication between participants.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-cryptographic-primitives-8" name="p-51309-cryptographic-primitives-8"></a>Cryptographic Primitives</h3>
<p><strong>Definition (Core Components)</strong><br />
Let <span class="math">\mathcal{S} = (\mathcal{H}, \mathcal{P}, \mathcal{Z})</span> be a tuple where:</p>
<div class="math">
\begin{split}
\mathcal{H}: &amp; \{0,1\}^* \rightarrow \{0,1\}^\lambda \text{ (Hash Function)} \\
\mathcal{P}: &amp; \text{ProverSystem} \text{ (zk-SNARK Prover)} \\
\mathcal{Z}: &amp; \text{VerifierSystem} \text{ (zk-SNARK Verifier)}
\end{split}
</div>
<p>With properties:</p>
<ul>
<li><span class="math">\mathcal{H}</span>: Collision-resistant hash function</li>
<li><span class="math">\mathcal{P}</span>: Creates validity proofs for state transitions</li>
<li><span class="math">\mathcal{Z}</span>: Verifies proofs with perfect completeness</li>
</ul>
<hr />
<p>Think of cryptographic primitives as the building blocks of a secure vault system. The <strong>Hash Function</strong> is like a unique lock that ensures each vault (transaction) can only be accessed with the correct key, preventing unauthorized access. The <strong>zk-SNARK Prover</strong> is akin to a master key generator that creates proofs (keys) verifying the validity of each transaction without revealing its contents. The <strong>Verifier System</strong> acts as the security guard, quickly checking these proofs to confirm their authenticity before allowing access. This ensures that every transaction is both secure and efficiently verifiable.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-channel-construction-9" name="p-51309-channel-construction-9"></a>Channel Construction</h3>
<p><strong>Definition (Unilateral Channel)</strong><br />
A channel <span class="math">c</span> is defined as:</p>
<div class="math">
\begin{split}
Channel_c = (&amp;State_c, \pi_c) \text{ where:} \\
State_c = \{&amp;balances, nonce, metadata\} \\
\pi_c = &amp;\text{Proof of state validity}
\end{split}
</div>
<p>With properties:</p>
<ul>
<li>Unilateral updates: No counterparty needed</li>
<li>Self-proving: Validity proven by <span class="math">\pi_c</span></li>
<li>Instant finality: No challenge period</li>
</ul>
<hr />
<p>Constructing a channel in Overpass is similar to setting up a private payment lane between two stores in a shopping mall. This lane allows for quick and direct transactions without the need for shoppers to queue at the main checkout counters. Each transaction within this lane is automatically verified by built-in sensors (proofs), ensuring that every payment is legitimate and instantly recorded, eliminating the need for external supervision or manual checks.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-hierarchical-structure-10" name="p-51309-hierarchical-structure-10"></a>Hierarchical Structure</h3>
<p><strong>Definition (Protocol Hierarchy)</strong><br />
Two-level hierarchy <span class="math">\mathcal{H}</span>:</p>
<div class="math">
\begin{split}
\mathcal{H} = \{&amp;Root, Channels\} \text{ where:} \\
Root &amp;: \text{Global SMT root} \\
Channels &amp;: \text{Individual states}
\end{split}
</div>
<p>Each level uses a Sparse Merkle Tree:</p>
<div class="math">
SMT = (V, E, root, \Pi)
</div>
<p>Where:</p>
<ul>
<li><span class="math">V</span>: Channel states</li>
<li><span class="math">E</span>: State transitions</li>
<li><span class="math">root</span>: Merkle root</li>
<li><span class="math">\Pi</span>: Validity proofs</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/a/3adc4d403c49e0e3091eda53d858eedad7a1f303.png" title="Screenshot 2024-11-26 at 6.07.29 PM"><img alt="Screenshot 2024-11-26 at 6.07.29 PM" height="324" src="https://ethresear.ch/uploads/default/optimized/3X/3/a/3adc4d403c49e0e3091eda53d858eedad7a1f303_2_517x324.png" width="517" /></a></div><p></p>
<hr />
<p>The hierarchical structure of Overpass can be compared to a well-organized corporate hierarchy. At the top level, the <strong>Root</strong> serves as the CEO, overseeing the entire organization. Each <strong>Channel</strong> acts like a department manager, responsible for their specific teams (transactions). This clear structure ensures accountability and efficient management, where each department independently verifies its activities while contributing to the overall integrity of the company.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-state-representation-11" name="p-51309-state-representation-11"></a>State Representation</h3>
<p><strong>Definition (Channel State)</strong><br />
For a channel <span class="math">c</span>:</p>
<div class="math">
\begin{split}
State_c = \{&amp;balances: \mathbb{N}^k, \\
&amp;nonce: \mathbb{N}, \\
&amp;metadata: \{id, config\}\}
\end{split}
</div>
<p>With invariants proven by <span class="math">\pi_c</span>:</p>
<ul>
<li><span class="math">\sum_i balances_i = Total_c</span> (conservation)</li>
<li><span class="math">nonce_{t+1} &gt; nonce_t</span> (monotonicity)</li>
</ul>
<hr />
<p>Representing the channel state in Overpass is akin to maintaining an up-to-date ledger in a financial office. Each <strong>State_c</strong> includes detailed records of account balances (like individual bank accounts), a <strong>nonce</strong> to track the sequence of transactions (similar to transaction numbers), and metadata for additional context (like transaction descriptions). Just as a ledger ensures that all financial transactions are accurately recorded and sequentially ordered, Overpass ensures that all state changes are transparent and tamper-proof.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-state-transitions-12" name="p-51309-state-transitions-12"></a>State Transitions</h3>
<p><strong>Definition (Valid State Transition)</strong><br />
A transition <span class="math">\Delta</span> is valid if:</p>
<div class="math">
\begin{split}
Valid(\Delta) \iff&amp; \exists \pi: \\
&amp; VerifyProof(\pi, State_t \xrightarrow{\Delta} State_{t+1}) = 1
\end{split}
</div>
<hr />
<p><strong>Theorem (State Transition Security)</strong><br />
For any adversary <span class="math">\mathcal{A}</span>:</p>
<div class="math">
\begin{split}
&amp; Pr[\mathcal{A} \text{ creates valid } \pi \text{ for invalid } \Delta] \\
&amp; \leq 2^{-\lambda}
\end{split}
</div>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-proof-state-transition-security-13" name="p-51309-proof-state-transition-security-13"></a>Proof: State Transition Security</h3>
<p><strong>Intuitive Explanation:</strong><br />
This theorem asserts that any adversary attempting to create a valid proof <span class="math">\pi</span> for an invalid state transition <span class="math">\Delta</span> has a negligible probability of success, bounded by <span class="math">2^{-\lambda}</span>. This security guarantee is foundational to ensuring the integrity of state transitions within the Overpass protocol.</p>
<p><strong>Formal Proof:</strong><br />
By the soundness property of ZK-SNARKs, any attempt to generate a proof for an invalid statement (in this case, an invalid state transition) will fail with high probability. Specifically:</p>
<ul>
<li>The soundness error of the ZK-SNARK is at most <span class="math">2^{-\lambda}</span>.</li>
<li>Therefore, the probability that an adversary can generate a valid proof for an invalid transition is bounded by <span class="math">2^{-\lambda}</span>.</li>
<li>No additional verification steps can reduce this probability further.</li>
</ul>
<p>Thus, the theorem holds.</p>
<hr />
<p>Valid state transitions in Overpass are like approved changes in a company’s financial records. Each time a transaction occurs, it must pass a rigorous approval process (proof verification) to ensure accuracy and legitimacy. This is similar to how a company ensures that every financial entry is reviewed and approved before being recorded, maintaining the integrity of the entire financial system.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-key-properties-14" name="p-51309-key-properties-14"></a>Key Properties</h3>
<p>This model provides:</p>
<ol>
<li><strong>Unilateral Operation:</strong> Participants update independently.</li>
<li><strong>Instant Finality:</strong> Valid proof means valid state.</li>
<li><strong>No Trust Required:</strong> Pure cryptographic security.</li>
<li><strong>Simple Hierarchy:</strong> Two-level structure sufficient.</li>
</ol>
<p>These properties emerge from the self-proving nature of the ZK proofs, requiring no external validation or consensus.</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-core-protocol-mechanisms-and-operations-15" name="p-51309-core-protocol-mechanisms-and-operations-15"></a>Core Protocol Mechanisms and Operations</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-state-updates-16" name="p-51309-state-updates-16"></a>State Updates</h3>
<p><strong>Definition (State Update):</strong><br />
A state update is self-contained:<br />
<span class="math">
\begin{split}
Update = (&amp;State_{new}, \pi_{validity}) \text{ where:} \\
State_{new} = \{&amp;balances', nonce', metadata'\} \\
\pi_{validity} = &amp;\text{Proof of correct transition}
\end{split}
</span></p>
<hr />
<p><strong>Example (Payment Transaction):</strong><br />
Consider a payment:<br />
<span class="math">
\begin{split}
State_{t} = \{&amp;balance_A = 100, \\
&amp;balance_B = 50, \\
&amp;nonce = 15\}
\end{split}
</span></p>
<p>Update to:<br />
<span class="math">
\begin{split}
State_{t+1} = \{&amp;balance_A = 97, \\
&amp;balance_B = 53, \\
&amp;nonce = 16\}
\end{split}
</span></p>
<p>With proof <span class="math">\pi_{validity}</span> showing:</p>
<ul>
<li>Conservation: <span class="math">100 + 50 = 97 + 53</span></li>
<li>Monotonicity: <span class="math">16 &gt; 15</span></li>
<li>Valid ownership: <span class="math">A</span> controls funds</li>
</ul>
<hr />
<p>The core protocol mechanisms of Overpass can be likened to the operations of an automated trading system in a stock exchange. Each <strong>State Update</strong> is like executing a trade—it’s processed autonomously, validated through complex algorithms (proof generation and verification), and instantly reflected in the system without manual intervention. This automation ensures high-speed, reliable transactions that scale effortlessly with increased trading volume.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-update-protocol-17" name="p-51309-update-protocol-17"></a>Update Protocol</h3>
<pre><code class="lang-algorithm">**Algorithm 1: State Update Protocol**
1. Function UpdateState(state_old, update)
2.   state_new ← ComputeNewState(state_old, update)
3.   π_validity ← GenerateProof(state_old, state_new)
4.   assert VerifyProof(π_validity)
5.   root_new ← UpdateMerkleRoot(state_new)
6.   return (state_new, π_validity)
</code></pre>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-security-guarantees-18" name="p-51309-security-guarantees-18"></a>Security Guarantees</h3>
<p><strong>Theorem (Update Security):</strong><br />
For any update:</p>
<div class="math">
\begin{split}
&amp; VerifyProof(\pi_{validity}) = 1 \implies \\
&amp; ValidTransition(State_{old} \rightarrow State_{new})
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
This theorem ensures that if a proof <span class="math">\pi_{validity}</span> verifies successfully, the state transition from <span class="math">State_{old}</span> to <span class="math">State_{new}</span> is indeed valid. This guarantees that only legitimate state updates are accepted by the protocol.</p>
<p><strong>Formal Proof:</strong><br />
Given the properties of ZK-SNARKs:</p>
<ul>
<li><strong>Perfect Completeness:</strong> If the state transition is valid, then a valid proof exists and will always verify.</li>
<li><strong>Soundness:</strong> If the state transition is invalid, no valid proof can be generated that verifies successfully.</li>
</ul>
<p>Therefore, if <span class="math">VerifyProof(\pi_{validity}) = 1</span>, it must be that the state transition is valid.</p>
<hr />
<p><strong>Theorem (Atomic Updates):</strong><br />
Updates are atomic:</p>
<div class="math">
\{State={Statenewif VerifyProof(π)=1Stateoldotherwise\begin{split} State = \begin{cases} State_{new} &amp; \text{if } VerifyProof(\pi) = 1 \\ State_{old} &amp; \text{otherwise} \end{cases} \end{split}}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
Atomicity ensures that a state update either fully succeeds or fails without partial changes. This prevents inconsistent states from arising due to failed updates.</p>
<p><strong>Formal Proof:</strong><br />
The protocol’s update mechanism includes an assertion that the proof verifies successfully before committing the new state. If <span class="math">VerifyProof(\pi) = 1</span>, the new state is accepted. If the proof fails to verify, the state remains unchanged. This binary outcome guarantees atomicity.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-performance-characteristics-19" name="p-51309-performance-characteristics-19"></a>Performance Characteristics</h3>
<p>Concrete performance metrics:</p>
<ol>
<li>
<p><strong>Time Complexity:</strong></p>
<div class="math">
\begin{split}
   Time_{prove} &amp;= O(\log n) \text{ (proof generation)} \\
   Time_{verify} &amp;= O(1) \text{ (verification)} \\
   Time_{update} &amp;= O(1) \text{ (state update)}
   \end{split}
</div>
</li>
<li>
<p><strong>Space Complexity:</strong></p>
<div class="math">
\begin{split}
   Size_{proof} &amp;= O(\log n) \text{ (proof size)} \\
   Size_{state} &amp;= O(1) \text{ (state size)}
   \end{split}
</div>
</li>
</ol>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-practical-benefits-20" name="p-51309-practical-benefits-20"></a>Practical Benefits</h3>
<p>The unilateral ZKP design provides:</p>
<ol>
<li>
<p><strong>Simplicity:</strong></p>
<ul>
<li>Single proof per update</li>
<li>No multi-phase protocol</li>
<li>No coordination needed</li>
<li>Self-contained verification</li>
</ul>
</li>
<li>
<p><strong>Security:</strong></p>
<ul>
<li>Mathematical proof of correctness</li>
<li>No trust assumptions</li>
<li>No external validation</li>
<li>Instant finality</li>
</ul>
</li>
<li>
<p><strong>Efficiency:</strong></p>
<ul>
<li>Minimal communication</li>
<li>Fast verification</li>
<li>Low overhead</li>
<li>Scalable design</li>
</ul>
</li>
</ol>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-fundamental-security-theorems-21" name="p-51309-fundamental-security-theorems-21"></a>Fundamental Security Theorems</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-security-model-22" name="p-51309-security-model-22"></a>Security Model</h3>
<p>The security of Overpass reduces to the security of its cryptographic primitives:</p>
<p><strong>Definition (Security Model):</strong></p>
<div class="math">
\begin{split}
Primitives = \{&amp;ZK\text{-}SNARK_{security}, \\
&amp;Hash_{collision}, \\
&amp;Merkle_{binding}\}
\end{split}
</div>
<p>Against any PPT adversary with:</p>
<ul>
<li>Standard computational bounds</li>
<li>Access to public parameters</li>
<li>Ability to generate proofs</li>
</ul>
<hr />
<p>The fundamental security theorems of Overpass are comparable to the rigorous safety standards in aviation. Just as aircraft undergo extensive testing to ensure they can withstand extreme conditions, Overpass’s mathematical proofs ensure that its system is impervious to common attack vectors. This means businesses and users can trust that their transactions are secure, much like passengers trust that their flights are safe.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-core-security-properties-23" name="p-51309-core-security-properties-23"></a>Core Security Properties</h3>
<p><strong>Theorem (Proof Security):</strong><br />
For any adversary <span class="math">\mathcal{A}</span>:</p>
<div class="math">
\begin{split}
&amp; Pr[\mathcal{A} \text{ creates valid proof for invalid state}] \\
&amp; \leq 2^{-\lambda}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
This theorem ensures that the probability of an adversary successfully forging a proof for an invalid state is negligible, bounded by <span class="math">2^{-\lambda}</span>. This is crucial for maintaining the integrity and trustworthiness of state transitions within the protocol.</p>
<p><strong>Formal Proof:</strong><br />
By the soundness property of ZK-SNARKs:</p>
<ul>
<li>The probability that an adversary can generate a valid proof for an invalid state is at most the soundness error of the ZK-SNARK, which is <span class="math">2^{-\lambda}</span>.</li>
<li>This bound holds under the assumption that the underlying cryptographic primitives are secure and the adversary is computationally bounded.</li>
</ul>
<p>Therefore, the probability <span class="math">Pr[\mathcal{A} \text{ creates valid proof for invalid state}] \leq 2^{-\lambda}</span>.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-double-spend-prevention-24" name="p-51309-double-spend-prevention-24"></a>Double-Spend Prevention</h3>
<p><strong>Theorem (Double-Spend Prevention):</strong><br />
The probability of creating conflicting valid states is:</p>
<div class="math">
\begin{split}
&amp; Pr[Valid(State_1) \land Valid(State_2) \land Conflict(State_1, State_2)] \\
&amp; \leq 2^{-\lambda}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
This theorem guarantees that the likelihood of an adversary successfully creating two conflicting valid states (i.e., double-spending) is extremely low, bounded by <span class="math">2^{-\lambda}</span>. This is essential for preventing double-spending attacks in the protocol.</p>
<p><strong>Formal Proof:</strong><br />
Leveraging the soundness of ZK-SNARKs:</p>
<ul>
<li>Each valid state transition is accompanied by a proof that must verify successfully.</li>
<li>Nonces ensure unique ordering of state transitions, preventing replay or conflicting updates.</li>
<li>For two conflicting states to both be valid, the adversary must forge proofs for at least one invalid state transition.</li>
<li>The probability of successfully forging such a proof is bounded by <span class="math">2^{-\lambda}</span>.</li>
</ul>
<p>Thus, <span class="math">Pr[Valid(State_1) \land Valid(State_2) \land Conflict(State_1, State_2)] \leq 2^{-\lambda}</span>.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-state-integrity-25" name="p-51309-state-integrity-25"></a>State Integrity</h3>
<p><strong>Theorem (State Integrity):</strong><br />
For any state transition sequence:</p>
<div class="math">
\begin{split}
&amp; State_0 \xrightarrow{\pi_1} State_1 \xrightarrow{\pi_2} State_2 \rightarrow ... \\
&amp; \prod_i VerifyProof(\pi_i) = 1 \implies AllValid(State_i)
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
This theorem ensures that if all proofs in a sequence of state transitions verify successfully, then all resultant states are valid. This maintains the integrity of the entire state transition history.</p>
<p><strong>Formal Proof:</strong><br />
By induction:</p>
<ul>
<li><strong>Base Case:</strong> <span class="math">VerifyProof(\pi_1) = 1 \implies State_1</span> is valid.</li>
<li><strong>Inductive Step:</strong> Assuming <span class="math">State_i</span> is valid, if <span class="math">VerifyProof(\pi_{i+1}) = 1</span>, then <span class="math">State_{i+1}</span> is also valid.</li>
<li><strong>Conclusion:</strong> Therefore, if all proofs verify, all states in the sequence are valid.</li>
</ul>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-security-composition-26" name="p-51309-security-composition-26"></a>Security Composition</h3>
<p><strong>Theorem (Overall Security):</strong><br />
System security reduces to primitive security:</p>
<div class="math">
\begin{split}
Security_{system} = &amp;Security_{ZK\text{-}SNARK} \\
= &amp;2^{-\lambda}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
The overall security of the Overpass protocol is directly inherited from the security of its underlying cryptographic primitives, particularly the ZK-SNARKs.</p>
<p><strong>Formal Proof:</strong><br />
Since all security guarantees (such as state transition validity and double-spend prevention) are based on the soundness of ZK-SNARKs, the system’s security level is equivalent to that of the ZK-SNARKs used. Given that the ZK-SNARKs have a soundness error of <span class="math">2^{-\lambda}</span>, the entire system inherits this security level.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-practical-implications-27" name="p-51309-practical-implications-27"></a>Practical Implications</h3>
<p>These guarantees provide:</p>
<ol>
<li>
<p><strong>Instant Finality</strong></p>
<ul>
<li>No waiting periods needed</li>
<li>No probabilistic confirmation</li>
<li>No external validation</li>
<li>Pure mathematical certainty</li>
</ul>
</li>
<li>
<p><strong>Unconditional Security</strong></p>
<ul>
<li>No network assumptions</li>
<li>No trust requirements</li>
<li>No timing dependencies</li>
<li>Pure cryptographic guarantees</li>
</ul>
</li>
<li>
<p><strong>Practical Performance</strong></p>
<ul>
<li>Fast verification</li>
<li>Compact proofs</li>
<li>Minimal computation</li>
<li>Efficient storage</li>
</ul>
</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#p-51309-advanced-security-properties-28" name="p-51309-advanced-security-properties-28"></a>Advanced Security Properties</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-multi-state-updates-29" name="p-51309-multi-state-updates-29"></a>Multi-State Updates</h3>
<p><strong>Definition (State Update Group):</strong><br />
A group of related updates <span class="math">G = \{u_1, ..., u_k\}</span> where each <span class="math">u_i</span> produces a new state:</p>
<div class="math">
\begin{split}
G_{proof} = \pi: &amp;State_{t} \xrightarrow{u_1,...,u_k} State_{t+k} \\
&amp;\text{Single proof covers all updates}
\end{split}
</div>
<hr />
<p><strong>Theorem (Group Update Security):</strong><br />
For any update group <span class="math">G</span>:</p>
<div class="math">
\begin{split}
VerifyProof(\pi_G) = 1 \iff&amp; \text{ All updates valid} \land \\
&amp;\text{ All state changes correct} \land \\
&amp;\text{ All invariants preserved}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
This theorem ensures that when a group proof <span class="math">\pi_G</span> verifies successfully, it implies that every individual update within the group is valid, all state changes are correctly applied, and all protocol invariants are maintained.</p>
<p><strong>Formal Proof:</strong><br />
The group proof <span class="math">\pi_G</span> encompasses all updates <span class="math">u_1, ..., u_k</span>. For <span class="math">\pi_G</span> to verify:</p>
<ul>
<li>Each update <span class="math">u_i</span> must individually satisfy the state transition rules.</li>
<li>The collective state changes must adhere to global invariants such as total balance conservation and nonce monotonicity.</li>
<li>No conflicting updates are present within the group.</li>
</ul>
<p>Therefore, if <span class="math">VerifyProof(\pi_G) = 1</span>, all updates are valid, state changes are correct, and invariants are preserved.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-cross-channel-operations-30" name="p-51309-cross-channel-operations-30"></a>Cross-Channel Operations</h3>
<p><strong>Theorem (Cross-Channel Atomicity):</strong><br />
For updates across channels <span class="math">c_1,...,c_n</span>:</p>
<div class="math">
\begin{split}
\pi_{cross}: &amp;State_{t} \xrightarrow{update} State_{t+1} \\
VerifyProof(\pi_{cross}) = 1 \implies&amp; \text{ All channels updated validly}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
This theorem guarantees that when a cross-channel proof <span class="math">\pi_{cross}</span> verifies successfully, all involved channels <span class="math">c_1,...,c_n</span> have been updated correctly and consistently.</p>
<p><strong>Formal Proof:</strong><br />
The ZK proof circuit for cross-channel updates enforces:</p>
<ul>
<li>Conservation of total value across all channels.</li>
<li>Valid state transitions for each individual channel.</li>
<li>Atomicity, ensuring that either all channel updates succeed or none do.</li>
</ul>
<p>Thus, if <span class="math">VerifyProof(\pi_{cross}) = 1</span>, it ensures that all channels have been updated validly and atomically.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-state-composition-31" name="p-51309-state-composition-31"></a>State Composition</h3>
<p><strong>Theorem (Compositional Security):</strong><br />
Multiple valid proofs compose securely:</p>
<div class="math">
\begin{split}
\forall \pi_1,...,\pi_k: &amp;\bigwedge_i VerifyProof(\pi_i) = 1 \implies \\
&amp;Valid(State_{final})
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
This theorem states that if a series of proofs <span class="math">\pi_1,...,\pi_k</span> all verify successfully, then the final state after all transitions is valid. This ensures that composing multiple valid updates maintains overall system integrity.</p>
<p><strong>Formal Proof:</strong><br />
Each proof <span class="math">\pi_i</span> ensures that the corresponding state transition is valid. By the integrity of individual proofs:</p>
<ul>
<li>Each transition preserves the required invariants.</li>
<li>Sequential application of valid transitions maintains state consistency.</li>
</ul>
<p>Therefore, the final state <span class="math">State_{final}</span> is valid.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-practical-implications-32" name="p-51309-practical-implications-32"></a>Practical Implications</h3>
<p>This simplified design enables:</p>
<ol>
<li>
<p><strong>Complex Operations</strong></p>
<ul>
<li>Multi-party transactions</li>
<li>Cross-channel transfers</li>
<li>Atomic swaps</li>
<li>State composition</li>
</ul>
</li>
<li>
<p><strong>Security Guarantees</strong></p>
<ul>
<li>Single-proof verification</li>
<li>Deterministic outcomes</li>
<li>No coordination needed</li>
<li>Instant finality</li>
</ul>
</li>
<li>
<p><strong>Implementation</strong></p>
</li>
</ol>
<pre><code class="lang-algorithm">**Algorithm 2: Multi-State Update**
1. Function UpdateMultiple(states, updates)
2.   new_states ← ComputeNewStates(states, updates)
3.   π ← GenerateProof(states, new_states)
4.   assert VerifyProof(π)
5.   return (new_states, π)
</code></pre>
<hr />
<p>Advanced security properties in Overpass function like the multi-layered security protocols of a high-security facility. <strong>Group Update Security</strong> ensures that batches of transactions are processed securely, similar to how a secure facility handles groups of visitors with coordinated access protocols. <strong>Cross-Channel Atomicity</strong> guarantees that interconnected transactions are executed flawlessly, akin to synchronized operations in a complex manufacturing process where each step must align perfectly to ensure product quality.</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-liveness-properties-33" name="p-51309-liveness-properties-33"></a>Liveness Properties</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-unilateral-progress-34" name="p-51309-unilateral-progress-34"></a>Unilateral Progress</h3>
<p><strong>Theorem (Unilateral Progress):</strong><br />
Any participant can always progress their state:</p>
<div class="math">
\begin{split}
&amp; \forall p \in Participants: \\
&amp; State_p \xrightarrow{update} State_{p'} \text{ possible if } \\
&amp; \exists \pi: VerifyProof(\pi) = 1
\end{split}
</div>
<p>Independent of:</p>
<ul>
<li>Other participants’ availability</li>
<li>Network conditions</li>
<li>System load</li>
<li>External factors</li>
</ul>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
This theorem ensures that any participant can independently update their state without relying on the availability or participation of others, and regardless of external conditions.</p>
<p><strong>Formal Proof:</strong><br />
The protocol allows unilateral updates by:</p>
<ul>
<li>Allowing participants to generate and verify proofs independently.</li>
<li>Not requiring any interaction or synchronization with other participants.</li>
</ul>
<p>Therefore, as long as a participant can generate a valid proof, they can progress their state irrespective of other factors.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-settlement-guarantee-35" name="p-51309-settlement-guarantee-35"></a>Settlement Guarantee</h3>
<p><strong>Theorem (Settlement Finality):</strong><br />
For any valid state update:</p>
<div class="math">
\begin{split}
Time_{settle} &amp;= Time_{prove} + Time_{verify} \\
&amp;= O(\log n) + O(1) \\
&amp;\approx \text{constant time}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
Settlement finality refers to the time it takes for a state update to be finalized and irrevocable on the settlement layer. This theorem states that the total time is effectively constant due to the logarithmic time for proof generation and constant time for verification.</p>
<p><strong>Formal Proof:</strong></p>
<ul>
<li><span class="math">Time_{prove} = O(\log n)</span>: Proof generation scales logarithmically with the number of channels.</li>
<li><span class="math">Time_{verify} = O(1)</span>: Proof verification time remains constant regardless of the number of channels.</li>
</ul>
<p>Since <span class="math">O(\log n) + O(1) = O(\log n)</span> and for practical purposes with large <span class="math">n</span>, <span class="math">O(\log n)</span> is considered effectively constant, especially when optimized with hardware acceleration.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-l1-settlement-36" name="p-51309-l1-settlement-36"></a>L1 Settlement</h3>
<p><strong>Theorem (L1 Settlement):</strong><br />
Any valid state can settle to L1:</p>
<div class="math">
\begin{split}
&amp; \forall State, \pi: VerifyProof(\pi) = 1 \implies \\
&amp; SettleToL1(State) \text{ succeeds in } O(1) \text{ L1 blocks}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
This theorem ensures that any state verified as valid can be settled on the Layer 1 blockchain within a constant number of blocks, ensuring quick and reliable settlement.</p>
<p><strong>Formal Proof:</strong></p>
<ul>
<li>Upon successful verification of the proof <span class="math">\pi</span>, the state is deemed valid.</li>
<li>The settlement transaction is prepared and submitted to L1.</li>
<li>Due to L1’s consensus mechanism, the transaction will be included in the next available block.</li>
<li>Assuming L1 has a constant block time, settlement occurs in <span class="math">O(1)</span> blocks.</li>
</ul>
<p>Thus, any valid state can be settled to L1 within a constant number of blocks.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-practical-guarantees-37" name="p-51309-practical-guarantees-37"></a>Practical Guarantees</h3>
<p>The system provides:</p>
<ol>
<li>
<p><strong>Instant Progress</strong></p>
<ul>
<li>No coordination needed</li>
<li>No waiting periods</li>
<li>No external dependencies</li>
<li>No failure modes</li>
</ul>
</li>
<li>
<p><strong>Settlement Assurance</strong></p>
<ul>
<li>Guaranteed L1 settlement</li>
<li>Fixed settlement time</li>
<li>No challenge periods</li>
<li>No reversion possible</li>
</ul>
</li>
<li>
<p><strong>Operational Properties</strong></p>
<ul>
<li>Deterministic operation</li>
<li>Load-independent</li>
<li>Network-independent</li>
<li>Always available</li>
</ul>
</li>
</ol>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-implementation-38" name="p-51309-implementation-38"></a>Implementation</h3>
<pre><code class="lang-algorithm">**Algorithm 3: Unilateral State Update**
1. Function UpdateState(state, update)
2.   new_state ← ComputeNewState(state, update)
3.   π ← GenerateProof(state, new_state)
4.   assert VerifyProof(π)
5.   root_new ← UpdateMerkleRoot(new_state)
6.   return (new_state, π)
</code></pre>
<p>Key properties:</p>
<ul>
<li>Self-contained execution</li>
<li>No external dependencies</li>
<li>Immediate completion</li>
<li>Guaranteed finality</li>
</ul>
<hr />
<p>Liveness guarantees in Overpass are like the reliability of a 24/7 customer support center. No matter the time or external conditions, participants can always initiate and complete transactions, just as customers can always reach support services when needed. This ensures that the system remains operational and responsive, providing businesses with the confidence that their transactions will always be processed without unnecessary delays.</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-economic-analysis-39" name="p-51309-economic-analysis-39"></a>Economic Analysis</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-cost-model-40" name="p-51309-cost-model-40"></a>Cost Model</h3>
<p><strong>Definition (Operational Costs):</strong><br />
For any state update <span class="math">u</span>:</p>
<div class="math">
\begin{split}
Cost_{update} = &amp;Cost_{compute}(u) + \\
&amp;Cost_{storage}(u) + \\
&amp;Cost_{settlement}(u)
\end{split}
</div>
<p>With components:</p>
<div class="math">
\begin{split}
Cost_{compute}(u) &amp;= c_p \cdot Size_{circuit}(u) \\
Cost_{storage}(u) &amp;= c_s \cdot Size_{state}(u) \\
Cost_{settlement}(u) &amp;= \begin{cases}
c_l \cdot Gas_{L1} &amp; \text{if L1 settlement} \\
0 &amp; \text{otherwise}
\end{cases}
\end{split}
</div>
<p>Where:</p>
<ul>
<li><span class="math">c_p</span>: Cost per circuit constraint</li>
<li><span class="math">c_s</span>: Cost per byte of storage</li>
<li><span class="math">c_l</span>: L1 gas cost</li>
</ul>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-circuit-size-analysis-41" name="p-51309-circuit-size-analysis-41"></a>Circuit Size Analysis</h3>
<p><strong>Theorem (Circuit Complexity):</strong><br />
For standard operations:</p>
<div class="math">
\begin{split}
Size_{circuit}(u) = &amp;Size_{base} + \\
&amp;Size_{transition}(u) + \\
&amp;Size_{verification}(u)
\end{split}
</div>
<p>Where:</p>
<div class="math">
\begin{split}
Size_{base} &amp;= O(1) \text{ (constant overhead)} \\
Size_{transition} &amp;= O(\log n) \text{ (state update)} \\
Size_{verification} &amp;= O(1) \text{ (proof verification)}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
The size of the circuit required for generating and verifying proofs scales logarithmically with the number of channels due to the hierarchical structure, while other components remain constant.</p>
<p><strong>Formal Proof:</strong></p>
<ul>
<li><span class="math">Size_{base}</span> accounts for fixed operations unrelated to the number of channels.</li>
<li><span class="math">Size_{transition}</span> involves operations proportional to <span class="math">\log n</span>, stemming from the use of Sparse Merkle Trees which have logarithmic depth.</li>
<li><span class="math">Size_{verification}</span> is constant as proof verification does not depend on the number of channels.</li>
</ul>
<p>Thus, the total circuit size is <span class="math">O(1) + O(\log n) + O(1) = O(\log n)</span>.</p>
<hr />
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/f/5fc085d4d56fc1210388e34c3d5f506f53dd42e4.png" title="Screenshot 2024-11-26 at 6.09.32 PM"><img alt="Screenshot 2024-11-26 at 6.09.32 PM" height="264" src="https://ethresear.ch/uploads/default/optimized/3X/5/f/5fc085d4d56fc1210388e34c3d5f506f53dd42e4_2_517x264.png" width="517" /></a></div><p></p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-storage-requirements-42" name="p-51309-storage-requirements-42"></a>Storage Requirements</h3>
<p><strong>Theorem (Storage Costs):</strong><br />
State storage requirements:</p>
<div class="math">
\begin{split}
Size_{state}(u) = &amp;Size_{balance} + \\
&amp;Size_{proof} + \\
&amp;Size_{metadata}
\end{split}
</div>
<p>With bounds:</p>
<div class="math">
\begin{split}
Size_{balance} &amp;= O(1) \text{ bytes per account} \\
Size_{proof} &amp;= O(\log n) \text{ bytes} \\
Size_{metadata} &amp;= O(1) \text{ bytes}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
The storage required for each state update comprises fixed-size components (balances and metadata) and a variable-size component (proof), which scales logarithmically with the number of channels.</p>
<p><strong>Formal Proof:</strong></p>
<ul>
<li><span class="math">Size_{balance}</span> is constant per account as it only needs to store the numerical balance.</li>
<li><span class="math">Size_{proof}</span> scales with <span class="math">\log n</span> due to the Sparse Merkle Tree structure.</li>
<li><span class="math">Size_{metadata}</span> remains constant as it includes fixed attributes like channel ID and configuration.</li>
</ul>
<p>Therefore, <span class="math">Size_{state}(u) = O(1) + O(\log n) + O(1) = O(\log n)</span>.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-l1-settlement-costs-43" name="p-51309-l1-settlement-costs-43"></a>L1 Settlement Costs</h3>
<p><strong>Theorem (Settlement Economics):</strong><br />
L1 settlement costs for state <span class="math">s</span>:</p>
<div class="math">
\begin{split}
Gas_{L1}(s) = &amp;Gas_{base} + \\
&amp;Gas_{proof} \cdot Size_{proof} + \\
&amp;Gas_{data} \cdot Size_{state}(s)
\end{split}
</div>
<p>Where:</p>
<ul>
<li><span class="math">Gas_{base}</span>: Base transaction cost</li>
<li><span class="math">Gas_{proof}</span>: Cost per proof byte</li>
<li><span class="math">Gas_{data}</span>: Cost per state byte</li>
</ul>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
The total gas cost for settling a state on Layer 1 includes a fixed base cost, a variable cost based on the size of the proof, and another variable cost based on the size of the state data.</p>
<p><strong>Formal Proof:</strong></p>
<ul>
<li><span class="math">Gas_{base}</span> accounts for the fixed overhead of initiating a transaction on L1.</li>
<li><span class="math">Gas_{proof} \cdot Size_{proof}</span> represents the variable cost associated with transmitting the proof data.</li>
<li><span class="math">Gas_{data} \cdot Size_{state}(s)</span> accounts for the storage of state data on L1.</li>
</ul>
<p>Thus, the total settlement cost is the sum of these components.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-total-cost-analysis-44" name="p-51309-total-cost-analysis-44"></a>Total Cost Analysis</h3>
<p><strong>Theorem (Total Cost Bounds):</strong><br />
For any update <span class="math">u</span>:</p>
<div class="math">
\begin{split}
Cost_{total}(u) \leq &amp;\alpha \cdot \log(n) + \\
&amp;\beta \cdot Size_{state}(u) + \\
&amp;\gamma \cdot \mathbb{1}_{settlement}
\end{split}
</div>
<p>Where:</p>
<ul>
<li><span class="math">\alpha</span>: Circuit computation coefficient</li>
<li><span class="math">\beta</span>: Storage coefficient</li>
<li><span class="math">\gamma</span>: L1 settlement coefficient</li>
<li><span class="math">\mathbb{1}_{settlement}</span>: Settlement indicator</li>
</ul>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
The total cost of a state update is bounded by the sum of costs associated with proof computation, storage, and optional settlement. Each component scales differently with system parameters.</p>
<p><strong>Formal Proof:</strong></p>
<ul>
<li><span class="math">\alpha \cdot \log(n)</span> represents the cost associated with the logarithmic scaling of proof computation.</li>
<li><span class="math">\beta \cdot Size_{state}(u)</span> accounts for the storage cost, which also scales logarithmically.</li>
<li><span class="math">\gamma \cdot \mathbb{1}_{settlement}</span> includes the fixed or variable cost of settling to L1, depending on whether settlement occurs.</li>
</ul>
<p>Thus, <span class="math">Cost_{total}(u)</span> is bounded by the sum of these three components.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-practical-cost-examples-45" name="p-51309-practical-cost-examples-45"></a>Practical Cost Examples</h3>
<ol>
<li>
<p><strong>Simple Transfer</strong><br />
For basic value transfer:</p>
<div class="math">
\begin{split}
   Cost_{transfer} &amp;\approx 0.001\$ \text{ (proof)} \\
   &amp;+ 0.0001\$ \text{ (storage)} \\
   &amp;+ 0\$ \text{ (no settlement)}
   \end{split}
</div>
</li>
<li>
<p><strong>Complex Update</strong><br />
For multi-party transfer:</p>
<div class="math">
\begin{split}
   Cost_{complex} &amp;\approx 0.005\$ \text{ (proof)} \\
   &amp;+ 0.0005\$ \text{ (storage)} \\
   &amp;+ 0\$ \text{ (no settlement)}
   \end{split}
</div>
</li>
<li>
<p><strong>L1 Settlement</strong><br />
With L1 settlement:</p>
<div class="math">
\begin{split}
   Cost_{settle} &amp;\approx 0.001\$ \text{ (proof)} \\
   &amp;+ 0.0001\$ \text{ (storage)} \\
   &amp;+ 5\$ \text{ (L1 gas)}
   \end{split}
</div>
</li>
</ol>
<hr />
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/3/a364d056defa3bb37956579ea2b6753ad5ec5b4d.png" title="Screenshot 2024-11-26 at 6.08.34 PM"><img alt="Screenshot 2024-11-26 at 6.08.34 PM" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/a/3/a364d056defa3bb37956579ea2b6753ad5ec5b4d_2_508x500.png" width="508" /></a></div><p></p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-economic-benefits-46" name="p-51309-economic-benefits-46"></a>Economic Benefits</h3>
<p>This cost model provides:</p>
<ol>
<li>
<p><strong>Predictable Costs</strong></p>
<ul>
<li>Fixed computation costs</li>
<li>Linear storage scaling</li>
<li>Optional L1 settlement</li>
<li>No congestion pricing</li>
</ul>
</li>
<li>
<p><strong>Economic Efficiency</strong></p>
<ul>
<li>Minimal overhead</li>
<li>Batching benefits</li>
<li>Proof amortization</li>
<li>Storage optimization</li>
</ul>
</li>
<li>
<p><strong>User Advantages</strong></p>
<ul>
<li>Low base fees</li>
<li>Predictable costs</li>
<li>Settlement flexibility</li>
<li>Cost optimization options</li>
</ul>
</li>
</ol>
<p>This economic model enables efficient operation at any scale.</p>
<hr />
<p>The economic model of Overpass can be compared to a highly efficient utility service. Just as electricity costs are predictable and scale with usage, Overpass ensures that transaction costs remain low and scale logarithmically with the number of users and transactions. This predictability allows businesses to budget effectively, knowing that their operational costs will remain manageable even as their transaction volume grows exponentially.</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-practical-implementation-47" name="p-51309-practical-implementation-47"></a>Practical Implementation</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-core-components-48" name="p-51309-core-components-48"></a>Core Components</h3>
<p><strong>Definition (System Architecture):</strong><br />
The Overpass implementation consists of:</p>
<div class="math">
\begin{split}
System = \{&amp;Prover, \\
&amp;Verifier, \\
&amp;Storage, \\
&amp;L1Interface\}
\end{split}
</div>
<p>With key parameters:</p>
<div class="math">
\begin{split}
Parameters = \{&amp;\lambda = 128 \text{ (security)}, \\
&amp;d = 32 \text{ (tree depth)}, \\
&amp;m = 2^{16} \text{ (max channels)}, \\
&amp;n = 2^{20} \text{ (max states)}\}
\end{split}
</div>
<hr />
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/e/bed30100fde094f291458afbbe7a7b48f2673df8.png" title="Screenshot 2024-11-26 at 6.10.35 PM"><img alt="Screenshot 2024-11-26 at 6.10.35 PM" height="261" src="https://ethresear.ch/uploads/default/optimized/3X/b/e/bed30100fde094f291458afbbe7a7b48f2673df8_2_517x261.png" width="517" /></a></div><p></p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-proof-circuit-49" name="p-51309-proof-circuit-49"></a>Proof Circuit</h3>
<p><strong>Definition (Proof Circuit):</strong><br />
The Proof Circuit is a critical component of the Overpass protocol, responsible for generating and verifying zero-knowledge proofs for each state transition. The process begins with the Prover generating a proof <span class="math">\pi</span> that encapsulates the validity of the state transition from <span class="math">State_{old}</span> to <span class="math">State_{new}</span>. The Verifier then checks the integrity of <span class="math">\pi</span>. If the proof is valid, the new state is accepted; otherwise, the update is rejected.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-channel-operations-50" name="p-51309-channel-operations-50"></a>Channel Operations</h3>
<pre><code class="lang-algorithm">**Algorithm 4: Channel Management**
1. Function UpdateState(current_state, update)
2.   new_state ← ComputeNewState(current_state, update)
3.   proof ← GenerateProof(current_state, new_state)
4.   assert VerifyProof(proof)
5.   store_state(new_state, proof)
6.   return (new_state, proof)

7. Function SettleToL1(state, proof)
8.   tx ← PrepareSettlementTx(state, proof)
9.   send_to_l1(tx)
10.  return tx_hash
</code></pre>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-storage-layout-51" name="p-51309-storage-layout-51"></a>Storage Layout</h3>
<pre><code class="lang-algorithm">**Algorithm 5: State Storage**
1. Function StoreState(state, proof)
2.   **State Storage:**  
3.   key ← hash(state)  
4.   store_value(key, state)  

5.   **Proof Storage:**  
6.   proof_key ← hash(proof)  
7.   store_value(proof_key, proof)  

8.   **Index Update:**  
9.   update_merkle_tree(state)
</code></pre>
<p>The <code>StoreState</code> function handles the storage of both the state and its corresponding proof. By hashing the state and proof, unique keys are generated for efficient retrieval. Updating the Merkle tree ensures that the global state remains consistent and verifiable.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-optimization-techniques-52" name="p-51309-optimization-techniques-52"></a>Optimization Techniques</h3>
<p>Key optimizations:</p>
<ol>
<li>
<p><strong>Proof Generation</strong></p>
<ul>
<li>GPU acceleration: Utilize parallel processing capabilities to speed up proof generation.</li>
<li>Circuit optimization: Design efficient circuits to reduce the computational overhead.</li>
<li>Parallel computation: Generate multiple proofs simultaneously to increase throughput.</li>
<li>Proof caching: Store frequently used proofs to avoid redundant computations.</li>
</ul>
</li>
<li>
<p><strong>State Management</strong></p>
<ul>
<li>Efficient Merkle trees: Implement optimized data structures for faster state verification.</li>
<li>State compression: Reduce the size of state data to minimize storage and transmission costs.</li>
<li>Lazy evaluation: Defer computation of certain state aspects until necessary.</li>
<li>Batch updates: Process multiple state updates in a single operation to enhance efficiency.</li>
</ul>
</li>
<li>
<p><strong>L1 Integration</strong></p>
<ul>
<li>Batched settlements: Aggregate multiple settlements into a single transaction to save gas.</li>
<li>Gas optimization: Optimize smart contract code to reduce gas consumption.</li>
<li>Proof aggregation: Combine multiple proofs into a single aggregated proof for efficiency.</li>
<li>Smart contract efficiency: Design lean smart contracts to handle settlements effectively.</li>
</ul>
</li>
</ol>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-deployment-requirements-53" name="p-51309-deployment-requirements-53"></a>Deployment Requirements</h3>
<p><strong>Production specifications:</strong></p>
<ol>
<li>
<p><strong>Computation Node</strong></p>
<div class="math">
\begin{split}
   Hardware_{min} = \{&amp;CPU: 32\text{ cores}, \\
   &amp;GPU: \text{CUDA-capable}, \\
   &amp;RAM: 128\text{ GB}, \\
   &amp;SSD: 2\text{ TB NVMe}\}
   \end{split}
</div>
</li>
<li>
<p><strong>Software Stack</strong></p>
<div class="math">
\begin{split}
   Software = \{&amp;OS: \text{Ubuntu 22.04}, \\
   &amp;Language: \text{Rust 1.70+}, \\
   &amp;Storage: \text{RocksDB}, \\
   &amp;ZK: \text{PLONKY2}\}
   \end{split}
</div>
</li>
</ol>
<p><strong>Intuitive Explanation:</strong><br />
The specified hardware ensures that the system can handle high-throughput proof generation and verification efficiently. The software stack is chosen for performance and security, with Rust providing memory safety and concurrency, RocksDB offering fast storage, and PLONKY2 facilitating efficient ZK-SNARKs.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-system-monitoring-54" name="p-51309-system-monitoring-54"></a>System Monitoring</h3>
<p>Key metrics:</p>
<ol>
<li>
<p><strong>Performance</strong></p>
<div class="math">
\begin{split}
   Metrics = \{&amp;Time_{proof}, \\
   &amp;Time_{verify}, \\
   &amp;Size_{state}, \\
   &amp;Size_{proof}\}
   \end{split}
</div>
</li>
<li>
<p><strong>Resources</strong></p>
<div class="math">
\begin{split}
   Resources = \{&amp;CPU_{usage}, \\
   &amp;GPU_{usage}, \\
   &amp;Memory_{usage}, \\
   &amp;Disk_{io}\}
   \end{split}
</div>
</li>
</ol>
<p><strong>Intuitive Explanation:</strong><br />
Monitoring these metrics ensures that the system operates within optimal parameters. Tracking proof times and resource usage helps in identifying bottlenecks and optimizing performance. Resource metrics are crucial for maintaining system stability and scalability.</p>
<hr />
<p>Implementing Overpass is similar to deploying a robust IT infrastructure in a large enterprise. The <strong>Prover</strong>, <strong>Verifier</strong>, <strong>Storage</strong>, and <strong>L1 Interface</strong> components are like the servers, security systems, databases, and network interfaces that keep a business running smoothly. The recommended hardware and software stack ensures that the system is both powerful and reliable, capable of handling high transaction volumes with ease, much like a well-designed IT system supports a growing company’s needs.</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-extensions-and-future-work-55" name="p-51309-extensions-and-future-work-55"></a>Extensions and Future Work</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-cross-chain-integration-56" name="p-51309-cross-chain-integration-56"></a>Cross-Chain Integration</h3>
<p>Cross-chain transfers can leverage ZK proofs directly:</p>
<p><strong>Definition (Cross-Chain Proof):</strong><br />
A cross-chain transfer requires:</p>
<div class="math">
\begin{split}
\pi_{cross} = \{&amp;\pi_{source}: \text{Proof of valid source state}, \\
&amp;\pi_{lock}: \text{Proof of value lock}, \\
&amp;\pi_{destination}: \text{Proof of valid target state}\}
\end{split}
</div>
<p><strong>Theorem (Cross-Chain Security):</strong><br />
Security reduces to individual proof verification:</p>
<div class="math">
\begin{split}
Security_{cross} = &amp;Security_{ZK} \\
= &amp;2^{-\lambda}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
Cross-chain transfers rely on multiple proofs to ensure the validity of each step in the transfer process. This theorem states that the overall security of cross-chain operations is equivalent to the security of the underlying ZK proofs.</p>
<p><strong>Formal Proof:</strong></p>
<ul>
<li>Each component proof (<span class="math">\pi_{source}</span>, <span class="math">\pi_{lock}</span>, <span class="math">\pi_{destination}</span>) must individually verify successfully.</li>
<li>Since each proof has a security bound of <span class="math">2^{-\lambda}</span>, the combined security of verifying all proofs remains at <span class="math">2^{-\lambda}</span>, assuming independent security guarantees.</li>
</ul>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-privacy-enhancements-57" name="p-51309-privacy-enhancements-57"></a>Privacy Enhancements</h3>
<p>Privacy improvements through nested proofs:</p>
<p><strong>Definition (Private State):</strong><br />
A private state update:</p>
<div class="math">
\begin{split}
\pi_{private} &amp;: State_{hidden} \rightarrow State'_{hidden} \\
\text{where } &amp;State_{hidden} = Commit(State_{real})
\end{split}
</div>
<p><strong>Theorem (Privacy Guarantees):</strong><br />
For any adversary <span class="math">\mathcal{A}</span>:</p>
<div class="math">
\begin{split}
Pr[\mathcal{A}(State_{hidden}) &amp;\rightarrow State_{real}] \\
&amp;\leq 2^{-\lambda}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
This theorem ensures that the actual state cannot be inferred from the hidden state with any significant probability, preserving the privacy of participants.</p>
<p><strong>Formal Proof:</strong></p>
<ul>
<li><span class="math">State_{hidden}</span> is a cryptographic commitment to <span class="math">State_{real}</span>, ensuring that the real state is concealed.</li>
<li>The zero-knowledge property ensures that no information about <span class="math">State_{real}</span> is leaked through <span class="math">\pi_{private}</span>.</li>
<li>Therefore, the probability that an adversary can deduce <span class="math">State_{real}</span> from <span class="math">State_{hidden}</span> is bounded by the soundness error of the ZK-SNARK, which is <span class="math">2^{-\lambda}</span>.</li>
</ul>
<p>Thus, <span class="math">Pr[\mathcal{A}(State_{hidden}) \rightarrow State_{real}] \leq 2^{-\lambda}</span>.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-recursive-proofs-58" name="p-51309-recursive-proofs-58"></a>Recursive Proofs</h3>
<p><strong>Definition (Recursive Proof Chain):</strong><br />
Recursive composition of proofs:</p>
<div class="math">
\begin{split}
\pi_{recursive} &amp;: \text{Prove}(\pi_1 \land \pi_2 \land ... \land \pi_n) \\
Size(\pi_{recursive}) &amp;= O(1) \text{ regardless of } n
\end{split}
</div>
<hr />
<p><strong>Theorem (Recursive Scalability):</strong><br />
With recursive proofs:</p>
<div class="math">
\begin{split}
Verification_{time} &amp;= O(1) \\
Proof_{size} &amp;= O(1) \\
Security &amp;= 2^{-\lambda}
\end{split}
</div>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
Recursive proofs allow multiple proofs to be combined into a single proof without increasing the size or verification time, enabling scalable verification regardless of the number of underlying proofs.</p>
<p><strong>Formal Proof:</strong></p>
<ul>
<li>The recursive proof <span class="math">\pi_{recursive}</span> encapsulates multiple individual proofs <span class="math">\pi_1, ..., \pi_n</span>.</li>
<li>Due to recursion, the verification of <span class="math">\pi_{recursive}</span> remains constant in time and size, regardless of <span class="math">n</span>.</li>
<li>The security of the recursive proof is maintained as each individual proof contributes to the overall security without introducing additional vulnerabilities.</li>
</ul>
<p>Therefore, <span class="math">Verification_{time} = O(1)</span>, <span class="math">Proof_{size} = O(1)</span>, and <span class="math">Security = 2^{-\lambda}</span>.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-future-research-priorities-59" name="p-51309-future-research-priorities-59"></a>Future Research Priorities</h3>
<ol>
<li>
<p><strong>Proof System Improvements</strong></p>
<ul>
<li>Faster proof generation</li>
<li>More efficient circuits</li>
<li>Hardware acceleration</li>
<li>Proof compression</li>
</ul>
</li>
<li>
<p><strong>Privacy Enhancements</strong></p>
<ul>
<li>Hidden state transitions</li>
<li>Anonymous ownership</li>
<li>Confidential amounts</li>
<li>Metadata protection</li>
</ul>
</li>
<li>
<p><strong>Recursive Composition</strong></p>
<ul>
<li>Efficient recursion</li>
<li>Proof aggregation</li>
<li>Batch verification</li>
<li>Constant-size proofs</li>
</ul>
</li>
</ol>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-implementation-roadmap-60" name="p-51309-implementation-roadmap-60"></a>Implementation Roadmap</h3>
<ol>
<li>
<p><strong>Phase 1: Core Optimization</strong></p>
<div class="math">
\begin{split}
   Optimize = \{&amp;Circuit_{efficiency}, \\
   &amp;Proof_{generation}, \\
   &amp;Hardware_{acceleration}, \\
   &amp;Storage_{compression}\}
   \end{split}
</div>
</li>
<li>
<p><strong>Phase 2: Advanced Features</strong></p>
<div class="math">
\begin{split}
   Features = \{&amp;Privacy_{enhancements}, \\
   &amp;Recursive_{proofs}, \\
   &amp;Cross_{chain}, \\
   &amp;L1_{integration}\}
   \end{split}
</div>
</li>
<li>
<p><strong>Phase 3: Ecosystem</strong></p>
<div class="math">
\begin{split}
   Ecosystem = \{&amp;Developer_{tools}, \\
   &amp;Client_{libraries}, \\
   &amp;Integration_{APIs}, \\
   &amp;Testing_{frameworks}\}
   \end{split}
</div>
</li>
</ol>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-research-challenges-61" name="p-51309-research-challenges-61"></a>Research Challenges</h3>
<p>Key open problems:</p>
<ol>
<li>
<p><strong>Theoretical</strong></p>
<div class="math">
\begin{split}
   Challenges = \{&amp;Proof_{efficiency}, \\
   &amp;Circuit_{optimization}, \\
   &amp;Privacy_{techniques}, \\
   &amp;Recursion_{methods}\}
   \end{split}
</div>
</li>
<li>
<p><strong>Technical</strong></p>
<div class="math">
\begin{split}
   Engineering = \{&amp;Hardware_{speedup}, \\
   &amp;Storage_{scaling}, \\
   &amp;Proof_{compression}, \\
   &amp;Implementation_{tools}\}
   \end{split}
</div>
</li>
</ol>
<hr />
<p>The potential extensions of Overpass are comparable to the future expansions of a tech company’s product line. <strong>Cross-Chain Integration</strong> is like integrating new software platforms, allowing Overpass to connect seamlessly with other blockchain systems. <strong>Privacy Enhancements</strong> are akin to adding new security features to protect user data. <strong>Recursive Proofs</strong> enable Overpass to scale effortlessly, much like a company can expand its operations without compromising on quality or efficiency.</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-summary-and-comparative-analysis-62" name="p-51309-summary-and-comparative-analysis-62"></a>Summary and Comparative Analysis</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-core-protocol-achievements-63" name="p-51309-core-protocol-achievements-63"></a>Core Protocol Achievements</h3>
<p>The Overpass protocol demonstrates:</p>
<ol>
<li>
<p><strong>Security Guarantee:</strong></p>
<div class="math">
\begin{split}
   Security_{system} = &amp;Security_{ZK\text{-}SNARK} \\
   = &amp;2^{-\lambda}
   \end{split}
</div>
</li>
<li>
<p><strong>Performance:</strong></p>
<div class="math">
\begin{split}
   Performance = \{&amp;Time_{prove} = O(\log n), \\
   &amp;Time_{verify} = O(1), \\
   &amp;Time_{settlement} = O(1)\}
   \end{split}
</div>
</li>
<li>
<p><strong>Costs:</strong></p>
<div class="math">
\begin{split}
   Cost_{total} = &amp;Cost_{proof} + \\
   &amp;Cost_{storage} + \\
   &amp;(Optional)Cost_{L1}
   \end{split}
</div>
</li>
</ol>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-comparative-analysis-64" name="p-51309-comparative-analysis-64"></a>Comparative Analysis</h3>
<p><strong>Theorem (System Comparison):</strong><br />
Traditional L2s vs Overpass:</p>
<ol>
<li>
<p><strong>Security Model</strong></p>
<div class="math">
\begin{split}
   Security_{L2} &amp;= \begin{cases}
   Consensus_{security} &amp; \text{(Rollups)} \\
   Challenge_{period} &amp; \text{(Channels)} \\
   Watchtower_{reliability} &amp; \text{(Plasma)}
   \end{cases} \\
   Security_{Overpass} &amp;= 2^{-\lambda} \text{ (ZK proof)}
   \end{split}
</div>
</li>
<li>
<p><strong>Finality Time</strong></p>
<div class="math">
\begin{split}
   Time_{L2} &amp;= \begin{cases}
   O(\text{blocks}) &amp; \text{(Rollups)} \\
   O(\text{days}) &amp; \text{(Channels)} \\
   O(\text{hours}) &amp; \text{(Plasma)}
   \end{cases} \\
   Time_{Overpass} &amp;= O(1) \text{ (instant)}
   \end{split}
</div>
</li>
<li>
<p><strong>Dependencies</strong></p>
<div class="math">
\begin{split}
   Requires_{L2} &amp;= \begin{cases}
   Consensus &amp; \text{(Rollups)} \\
   Counterparty &amp; \text{(Channels)} \\
   Watchtowers &amp; \text{(Plasma)}
   \end{cases} \\
   Requires_{Overpass} &amp;= \text{None (self-proving)}
   \end{split}
</div>
</li>
</ol>
<p><strong>Intuitive Explanation:</strong><br />
This theorem compares the security models, finality times, and dependencies of traditional Layer-2 solutions (Rollups, Channels, Plasma) with Overpass. It highlights how Overpass achieves a higher security guarantee with instant finality and minimal dependencies.</p>
<p><strong>Formal Proof:</strong></p>
<ul>
<li><strong>Security Model:</strong> Traditional L2s rely on consensus mechanisms, challenge periods, or watchtowers, each introducing potential vulnerabilities. Overpass relies solely on the soundness of ZK-SNARKs, providing a direct security guarantee of <span class="math">2^{-\lambda}</span>.</li>
<li><strong>Finality Time:</strong> Rollups depend on block confirmations, Channels require waiting periods for challenges, and Plasma relies on watchtower operations. Overpass, through instant proof verification, achieves finality in constant time.</li>
<li><strong>Dependencies:</strong> Traditional L2s require consensus participation, active counterparty engagement, or reliable watchtowers. Overpass eliminates these dependencies by enabling self-proving state transitions.</li>
</ul>
<p>Therefore, Overpass offers superior security and efficiency compared to traditional Layer-2 solutions.<br />
\end{proof}</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-key-innovations-65" name="p-51309-key-innovations-65"></a>Key Innovations</h3>
<p><strong>Theorem (Core Advantages):</strong><br />
Overpass provides fundamental benefits:</p>
<ol>
<li>
<p><strong>Unilateral Operation</strong></p>
<div class="math">
\begin{split}
   Independent_{operation} = &amp;No_{consensus} \land \\
   &amp;No_{counterparty} \land \\
   &amp;No_{watchtowers}
   \end{split}
</div>
</li>
<li>
<p><strong>Pure Cryptographic Security</strong></p>
<div class="math">
\begin{split}
   Security_{basis} = &amp;ZK\text{-}SNARK_{soundness} \land \\
   &amp;Hash_{collision} \land \\
   &amp;Merkle_{binding}
   \end{split}
</div>
</li>
<li>
<p><strong>Practical Efficiency</strong></p>
<div class="math">
\begin{split}
   Cost_{update} &amp;= O(\log n) \text{ computation} \\
   Time_{update} &amp;= O(1) \text{ latency} \\
   Storage_{update} &amp;= O(1) \text{ space}
   \end{split}
</div>
</li>
</ol>
<p><strong>Proof:</strong></p>
<p><strong>Intuitive Explanation:</strong><br />
This theorem summarizes the primary advantages of Overpass, emphasizing its ability to operate unilaterally without reliance on consensus mechanisms or counterparties, its robust cryptographic security, and its efficient computational and storage requirements.</p>
<p><strong>Formal Proof:</strong></p>
<ul>
<li><strong>Unilateral Operation:</strong> Overpass allows participants to independently update their state without needing consensus, counterparty involvement, or watchtowers, as demonstrated in previous theorems.</li>
<li><strong>Pure Cryptographic Security:</strong> The security of Overpass is based solely on the soundness of ZK-SNARKs, collision resistance of hash functions, and binding properties of Merkle trees.</li>
<li><strong>Practical Efficiency:</strong>
<ul>
<li>Computational costs scale logarithmically with the number of channels (<span class="math">O(\log n)</span>).</li>
<li>Proof verification and state updates occur in constant time (<span class="math">O(1)</span>).</li>
<li>Storage requirements per update remain constant (<span class="math">O(1)</span>).</li>
</ul>
</li>
</ul>
<p>Thus, Overpass achieves its core advantages through its innovative design and efficient implementation.</p>
<hr />
<p>In summary, Overpass stands out in the blockchain landscape much like a high-performance sports car in the automotive industry. While traditional Layer-2 solutions are like standard vehicles—reliable but limited in speed and efficiency—Overpass offers unparalleled performance with instant finality, minimal costs, and robust security. This makes it an attractive choice for businesses seeking both speed and reliability without the complexities and limitations of existing solutions.</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-final-conclusions-66" name="p-51309-final-conclusions-66"></a>Final Conclusions</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-key-contributions-67" name="p-51309-key-contributions-67"></a>Key Contributions</h3>
<ol>
<li>
<p><strong>Theoretical</strong></p>
<ul>
<li>Novel unilateral ZKP channel design</li>
<li>Pure cryptographic security model</li>
<li>Self-proving state transitions</li>
<li>Instant mathematical finality</li>
</ul>
</li>
<li>
<p><strong>Technical</strong></p>
<ul>
<li>Efficient proof circuits</li>
<li>Minimal dependencies</li>
<li>Simple state model</li>
<li>Practical implementation</li>
</ul>
</li>
<li>
<p><strong>Practical</strong></p>
<ul>
<li>Production-ready design</li>
<li>Clear scaling path</li>
<li>Low operating costs</li>
<li>Easy deployment</li>
</ul>
</li>
</ol>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-impact-68" name="p-51309-impact-68"></a>Impact</h3>
<p>The Overpass protocol represents a fundamental rethinking of blockchain scaling:</p>
<ol>
<li>
<p><strong>Novel Paradigm</strong></p>
<ul>
<li>Proofs replace consensus</li>
<li>Unilateral replaces bilateral</li>
<li>Mathematics replaces game theory</li>
<li>Simplicity replaces complexity</li>
</ul>
</li>
<li>
<p><strong>Real-World Benefits</strong></p>
<ul>
<li>Instant finality</li>
<li>Independent operation</li>
<li>Mathematical security</li>
<li>Practical efficiency</li>
</ul>
</li>
</ol>
<hr />
<p>The final conclusions highlight Overpass as a transformative innovation in blockchain technology, comparable to the advent of the internet in the late 20th century. Just as the internet revolutionized communication and commerce by enabling instant, scalable interactions, Overpass redefines blockchain transactions by offering deterministic consensus, instant finality, and unlimited scalability. These mathematical guarantees lay the foundation for building real-world financial infrastructure that is as reliable and efficient as critical systems in aviation or space exploration.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-practical-applications-and-use-cases-69" name="p-51309-practical-applications-and-use-cases-69"></a><strong>Practical Applications and Use Cases</strong></h3>
<p><strong>Practical Insert:</strong></p>
<p><strong>High-Frequency Trading Platforms:</strong><br />
Imagine a stock exchange where trades are executed and settled within milliseconds, ensuring traders can capitalize on fleeting market opportunities without delay. Overpass provides the mathematical certainty and speed required for such high-stakes environments, eliminating risks like double-spending and ensuring fair execution orders.</p>
<p><strong>Retail Payment Networks:</strong><br />
Picture a global retail chain where customers can make purchases instantly without worrying about transaction delays or high fees. Overpass enables point-of-sale transactions to finalize instantly with minimal costs, enhancing customer experience and reducing operational expenses for retailers.</p>
<p><strong>Cross-Border Payments:</strong><br />
Consider international businesses that require swift and reliable cross-border transactions without the unpredictability of traditional banking systems. Overpass offers mathematically guaranteed settlement and predictable execution times, streamlining global commerce with transparent and efficient fee structures.</p>
<p><strong>Financial Services:</strong><br />
Envision financial institutions that can automate compliance checks and instantaneously reconcile transactions, all while maintaining tamper-proof audit trails. Overpass empowers these services with the necessary mathematical proofs to ensure integrity and efficiency, revolutionizing how financial operations are conducted.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51309-references-70" name="p-51309-references-70"></a>References</h2>
<p>[1] Ramsay, B. “Cryptskii” (2024). Overpass Channels: Horizontally Scalable, Privacy-Enhanced, with Independent Verification, Fluid Liquidity, and Robust Censorship Proof, Payments. <em>Cryptology ePrint Archive</em>, Paper 2024/1526. <a href="https://eprint.iacr.org/2024/1526" rel="noopener nofollow ugc">https://eprint.iacr.org/2024/1526</a></p>
<p>[2] Nakamoto, S. (2008). Bitcoin: A Peer-to-Peer Electronic Cash System. <a href="https://bitcoin.org/bitcoin.pdf" rel="noopener nofollow ugc">https://bitcoin.org/bitcoin.pdf</a></p>
<p>[3] Merkle, R. C. (1987). A Digital Signature Based on a Conventional Encryption Function. In <em>Advances in Cryptology — CRYPTO ’87</em>, pages 369-378. Springer Berlin Heidelberg.</p>
<p>[4] Groth, J. (2016). On the Size of Pairing-Based Non-interactive Arguments. In <em>Annual International Conference on the Theory and Applications of Cryptographic Techniques</em>, pages 305-326. Springer.</p>
<p>[5] Ben-Sasson, E., Bentov, I., Horesh, Y., &amp; Riabzev, M. (2019). Scalable Zero Knowledge with No Trusted Setup. In <em>Advances in Cryptology – CRYPTO 2019</em>, pages 701-732. Springer.</p>
<p>[6] Buterin, V. (2016). Chain Interoperability. R3 Research Paper.</p>
<p>[7] Poon, J., &amp; Dryja, T. (2016). The Bitcoin Lightning Network: Scalable Off-Chain Instant Payments. Technical Report.</p>
<p>[8] Khalil, R., &amp; Gervais, A. (2018). NOCUST – A Non-Custodial 2nd-Layer Financial Intermediary. <em>Cryptology ePrint Archive</em>, Report 2018/642.</p>
<p>[9] Gudgeon, L., Moreno-Sanchez, P., Roos, S., McCorry, P., &amp; Gervais, A. (2020). SoK: Layer-Two Blockchain Protocols. In <em>Financial Cryptography and Data Security</em>. Springer.</p>
<p>[10] Zamani, M., Movahedi, M., &amp; Raykova, M. (2018). RapidChain: Scaling Blockchain via Full Sharding. In <em>Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security</em>, pages 931-948.</p>
<p>[11] Kokoris-Kogias, E., Jovanovic, P., Gasser, L., Gailly, N., Syta, E., &amp; Ford, B. (2018). OmniLedger: A Secure, Scale-Out, Decentralized Ledger via Sharding. In <em>2018 IEEE Symposium on Security and Privacy (SP)</em>, pages 583-598.</p>
<p>[12] Yu, M., Sahraei, S., Li, S., Avestimehr, S., Kannan, S., &amp; Viswanath, P. (2020). Ohie: Blockchain Scaling Made Simple. In <em>IEEE Symposium on Security and Privacy (SP)</em>.</p>
<p>[13] Goldberg, S., Naor, M., Papadopoulos, D., &amp; Reyzin, L. (2020). SPHINX: A Password Store that Perfectly Hides Passwords from Itself. In <em>2020 IEEE Symposium on Security and Privacy (SP)</em>, pages 1051-1069.</p>
<p>[14] Boneh, D., Bünz, B., &amp; Fisch, B. (2019). Batching Techniques for Accumulators with Applications to IOPs and Stateless Blockchains. In <em>Annual International Cryptology Conference</em>, pages 561-586. Springer.</p>
<p>[15] Gabizon, A., Williamson, Z., &amp; Ciobotaru, O. (2019). PLONK: Permutations over Lagrange-bases for Oecumenical Noninteractive Arguments of Knowledge. <em>Cryptology ePrint Archive</em>, Report 2019/953.</p>
<p>[16] Maller, M., Bowe, S., Kohlweiss, M., &amp; Meiklejohn, S. (2019). Sonic: Zero-Knowledge SNARKs from Linear-Size Universal and Updateable Structured Reference Strings. In <em>Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security</em>, pages 2111-2128.</p>
<p>[17] Ben-Sasson, E., Bentov, I., Horesh, Y., &amp; Riabzev, M. (2018). Scalable, Transparent, and Post-Quantum Secure Computational Integrity. <em>Cryptology ePrint Archive</em>, Report 2018/046.</p>
<p>[18] Chiesa, A., Hu, Y., Maller, M., Mishra, P., Vesely, N., &amp; Ward, N. (2019). Marlin: Preprocessing zkSNARKs with Universal and Updatable SRS. <em>Cryptology ePrint Archive</em>, Report 2019/1047.</p>
<p>[19] Bünz, B., Bootle, J., Boneh, D., Poelstra, A., Wuille, P., &amp; Maxwell, G. (2018). Bulletproofs: Short Proofs for Confidential Transactions and More. In <em>2018 IEEE Symposium on Security and Privacy (SP)</em>, pages 315-334.</p>
<p>[20] Wang, J., &amp; Wang, H. (2020). Fractal: Post-Quantum and Transparent Recursive Proofs from Holography. <em>Cryptology ePrint Archive</em>, Report 2020/1280.</p>
<p>[21] Tomescu, A., Abraham, I., Buterin, V., Drake, J., Feist, D., &amp; Khovratovich, D. (2020). Aggregatable Subvector Commitments for Stateless Cryptocurrencies. In <em>Security and Cryptography for Networks</em>, pages 45-64. Springer.</p>
            <p><small>7 posts - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/deterministic-consensus-using-overpass-channels-in-distributed-ledger-technology/21046">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 19 Nov 2024 18:21:41 +0000</pubDate>
</item>
<item>
<title>In-Protocol Transaction Ordering</title>
<link>https://ethresear.ch/t/in-protocol-transaction-ordering/21084</link>
<guid>https://ethresear.ch/t/in-protocol-transaction-ordering/21084</guid>
<content:encoded><![CDATA[
<div> 关键词: FOCIL、交易排序、奖励与惩罚、随机种子、确定性排序

总结:
本文提出了一个基于FOCIL设计提案的改进方案，旨在进一步去除区块链协议中的中心化构建者工作流。该方案关注交易选择和排序两个方面，而FOCIL已经很好地解决了交易选择问题。为防止恶意行为和最大化公平性，该方案提出使用“包含列表”和“包含种子”，以确保交易在区块内的确定性和非可预测性排序。

具体设计包括三个关键要素：交易截止时间、包含种子的选择时间和交易随机化的可验证性。提议在前一个槽位（slot N-1）由提议者生成并传播一个随机数（nonce），在交易截止时间之后，用于创建包含种子。这样可以确保无法提前“挖掘”特定排序位置的交易。同时，为了避免攻击，如最后一个提交包含列表的委员会成员操纵种子或合谋，提议者需要在一个早期的时间点公布一个加密后的随机值，并在生成区块时揭示其真实值。

此外，该方案还考虑了奖励和处罚机制的更新，强调搜索者（即提供包含列表的参与者）应获得更大的奖励份额，而提议者的职责减少，因此他们的奖励也相应降低。对于未被纳入当前区块的剩余交易，文章探讨了几种可能的处理方式，例如使用一种启发式方法决定哪些交易进入下一个区块。

总的来说，该方案通过引入确定性和可验证的交易排序机制，以及调整经济激励措施，旨在实现更去中心化、公平和安全的交易处理流程。 <div>
<h2><a class="anchor" href="https://ethresear.ch#p-51392-preface-1" name="p-51392-preface-1"></a>Preface</h2>
<p>This idea is an extension to the FOCIL design proposal. This was initially created concurrently/independently to FOCIL, and started as a way to funnel and order transactions with intention to fully remove the builder workflow from the protocol design. In speaking with several people it was brought to light that FOCIL was progressing, and after going through it, the timing put forth in that design was much better (required 1 less slot and used a larger committee) for transaction selection than this idea proposed.</p>
<p>This idea was whittled down to build on top of FOCIL instead of being a competitor as they achieved the same goal.  It’s possible that some of the idea contained here can be integrated with FOCIL but it is provided separately to facilitate targeted discussion and to not muddy the waters of that ongoing design.</p>
<p>Special thanks to Phil Ngo, Nico Flaig, Cayman Nava, Guillaume Ballet, Greg Markou, Gajinder Singh, Navie Chan and many many others for taking the time to help hone this idea.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51392-abstract-2" name="p-51392-abstract-2"></a>Abstract</h2>
<p>Where this proposal focusses is the ordering of transactions.  It also addresses rewards and penalties to coincide with the new inclusion committees and updated proposer duties.</p>
<p>There are two main duties, with regards to builders, of the proposal process that need to be addressed to facilitate exclusion of centralized forces through crypto economic means.</p>
<ul>
<li>Selection of transactions for inclusion</li>
<li>Ordering of transactions</li>
</ul>
<p>FOCIL is an excellent solution to address the heart of the first of those two topics, but it can be further refined by restricting transactions included in blocks to ONLY those on the inclusion list.  This removes economic incentives, through loss of agency, to add transactions to a block that would negatively affect ordering to capture MEV.</p>
<p>Ordering will become a deterministic process using the Aggregated Inclusion List, put forth via FOCIL, and an Inclusion Seed.  The Inclusion Seed is entropy generated on a slot-wise basis, specifically to prevent collusive and extractive behaviors during transaction ordering.</p>
<p>Because the bulk of the work during block building (inclusion and ordering of transactions in a block that is proposed on the current head) is removed from the proposer the rewards mechanisms should be updated proportionally to compensate the parties providing the value to the protocol.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51392-design-3" name="p-51392-design-3"></a>Design</h2>
<p>There are three things that need to be accounted for to provision deterministic, non-gameable ordering. In particular ordering that is probabilistically infeasible to predict such that sandwich attacks and multi-block mev are economically disincentivized.</p>
<ol>
<li>The cutoff time in which transactions can no longer be included in a block for slot N</li>
<li>The time in which the Inclusion Seed is selected for slot N</li>
<li>The verifiability of the randomization of transactions within slot N</li>
</ol>
<p>The key to the ordering heuristic is such that the first two items happen in that order. If the seed is not known until after the window for inclusion closes, the heuristic can be built such that “mining an ordered transaction”, to be executed at a certain position in the transaction list, is infeasible.</p>
<p>The third item ensures that once the seed is known, any node on the network can calculate the same order of the transactions to prove compliance with the protocol. It also allows rewards and penalties to be assessed for (non)compliance.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51392-timing-considerations-4" name="p-51392-timing-considerations-4"></a>Timing Considerations</h3>
<p><em><strong>Slot N-1</strong></em><br />
t0: Proposer of slot N selects a random nonce, prepares and gossips it for use by the Inclusion Committee<br />
t9: Cutoff for FOCIL IL committee to select IL from local mempool and gossip individual IL based on consensus head of slot N-1, IL includes the hashed nonce produced by the producer in slot N</p>
<p><em><strong>Slot N</strong></em><br />
t0: Inclusion nonce, gossiped in N-1 is un-blinded and used to create Inclusion Seed. Inclusion Seed is sent to EL for use as an argument to an idempotent ordering function. An ordered list is produced that represents a full blocks worth of gas, and that list of transactions is executed. Block is produced jointly by EL/CL and CL adds un-blinded nonce to block before releasing to the network.<br />
t4: Attesters must verify blinded/revealed nonce, inclusion seed and transaction ordering during block validity checks. Attesters vote on valid block at slot N.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51392-inclusion-seed-5" name="p-51392-inclusion-seed-5"></a>Inclusion Seed</h3>
<p>The purpose of the Inclusion Seed is to provide the entropy to the transaction order.  It should be similar to RANDAO such that it uses on-chain data that is provable and knowable by everyone that follows the chain. However, RANDAO is too infrequently refreshed as it is epoch based and not slot based. For slot-wise ordering the entropy would also need to be slot-wise. The key to minimizing extractive behavior is that the seed needs to be selected after the window for transaction inclusion has closed in slot N-1, but prior to the end of slot N-1, so that it is available to the proposer of slot N.</p>
<p>When using the assumption that this proposal builds on top of FOCIL then the ideal source for entropy selection would be the aggregated Inclusion Lists which would be hard to mine against.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-51392-timing-based-seed-attack-solution-6" name="p-51392-timing-based-seed-attack-solution-6"></a>Timing-Based Seed Attack Solution</h4>
<p>A likely attack vector to this scheme though would be waiting to be the last committee member to submit an IL which will open the opportunity to include content that affects the seed, and thus the final ordering. It would also incentivize timing games with the knock-on effect of detracting from network propagation of the un-aggregated lists.</p>
<p>A workaround would be to have the IL aggregation process include some additional entropy in the final list, such as the signature or a nonce.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-51392-aggregator-collusion-seed-attack-solution-7" name="p-51392-aggregator-collusion-seed-attack-solution-7"></a>Aggregator-Collusion Seed Attack Solution</h4>
<p>Signing the list or using a simple nonce is insufficient though, as it would open attack surface for the final committee member to collude with the next proposer. For the collusion to work the collusive committee member could either have access to the next proposers key or simply coordinate with the next proposer to mine a transaction (and IL with its inclusion) for slot N.</p>
<p>To prevent against this attack a reveal process should be used. The first duty a proposer would do is gossip a signed hash in the slot prior to proposal.  The gossiped message would need to be announced very early, within 0-2 sec into the slot, ideally before the block is published (realistically just needs to be before the un-aggregated inclusion lists are gossiped as they need to include the nonce in the IL for validation purposes).</p>
<p>The announcement of the blinded nonce before gossip of the un-aggregated lists, with revealed afterwards, makes it impossible to collude in mining a transaction unless the entire inclusion committee participates in the scheme.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-51392-nonce-generation-and-reveal-mechanism-8" name="p-51392-nonce-generation-and-reveal-mechanism-8"></a>Nonce Generation and Reveal Mechanism</h4>
<p>To ensure adequate entropy the proposer would select some random bytes, perhaps in the range of 8-32 bytes to prevent brute force guessing within the alloted time, and then mix in some stateful randomness from RANDAO.  The randomness would then be hashed to blind the true value and the hash would be signed by the proposer of slot and, but critically it would be gossiped in slot N-1 so it could be appended to the IL’s to prove that it was received prior to IL creation.</p>
<p>In slot N the proposer, the only participant that should know the true un-blinded value thus far, would pass the nonce to the EL to the EL can order the transactions to protocol specifications.  The un-blinded value would be appended to the block so it can be revealed to the attestors for block validation and voting.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-51392-inclusion-seed-verification-9" name="p-51392-inclusion-seed-verification-9"></a>Inclusion Seed Verification</h4>
<p>The Inclusion Seed used to shuffle the transactions could then be verified by attestors by checking the hashed nonce in the Inclusion Lists matches the hash of the un-blinded nonce value included in the block once the stateful randomness (from RANDAO) was mixed in.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51392-deterministic-and-verifiable-ordering-10" name="p-51392-deterministic-and-verifiable-ordering-10"></a>Deterministic and Verifiable Ordering</h3>
<p>The only two pieces that are needed for ordering to be both deterministic and verifiable are known inputs and and a well-known, idempotent ordering function such that:</p>
<p>Torderd, Tremaining = f(ILagg, IS)</p>
<p>where:<br />
ILagg - Aggregated Inclusion List<br />
IS - Inclusion Seed<br />
Tordered - Ordered list of transactions<br />
Tremaining - Left-Over transactions that roll over to next block</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51392-ordering-algorithm-11" name="p-51392-ordering-algorithm-11"></a>Ordering Algorithm</h3>
<p>Ordering could be easily achieved by multiplying the transaction hash by the Inclusion Seed, letting values that overflow wrap around, and then putting the transactions in numeric order. This is overly simplistic and may not provide resistance to mining transactions that could game the system.</p>
<p>Another option is ordering the transactions in the Aggregated Inclusion List numerically, by hash, and then running an algorithm similar to swap-or-not over the set utilizing the Inclusion Seed instead of RANDAO. This may be more cpu intensive, but for a small set (1000± transactions) it should be relatively performant.</p>
<p>The sender address can alternately be used instead of the transaction hash as the root value that gets ordered but preference currently is to the transaction hash.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51392-ordering-heuristic-12" name="p-51392-ordering-heuristic-12"></a>Ordering Heuristic</h3>
<p>There are some user considerations that need to be addressed when implementing deterministic, non-gameable ordering. There is a balance that needs to be achieved between the crypto economic incentives of priority fees and rigor for prevention of sandwich attacks and monitoring the mempool for value extraction through transaction copies.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-51392-pseudo-random-ordering-13" name="p-51392-pseudo-random-ordering-13"></a>Pseudo-Random Ordering</h4>
<p>Everyone will have the same fair shot at the order within a block.  While the most egalitarian solution this ignores the crypto economics of paying a higher priority fee.  The most important instance where ignoring this makes sense is for reducing the potential for someone to monitor the transaction pool, for transactions that are profitable to front-run.  The illegitimate actor can simply submit a transaction that with a higher priority fee.</p>
<p>Randomized ordering takes away much of the economic incentive to copy transactions but does not address undesirable spamming of several conditional-execution transactions to increase probability of being executed ahead of the legitimate actor.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-51392-priority-fee-aware-pseudo-random-ordering-14" name="p-51392-priority-fee-aware-pseudo-random-ordering-14"></a>Priority-Fee Aware Pseudo-Random Ordering</h4>
<p>To address the spamming of conditional-execution transactions to try to ensure one randomly executes first, a heuristic that crafts tranches of transactions, bundled by fees, such that higher fees go towards the beginning of the block could be used. For transactions with relatively high priority fee (likely profitable arbitrage) it would be very expensive to match the fee for a large number of transactions. Keeping pseudo-randomness within the tranches will make that gamble, or single transaction out bidding expensive and without guarantee of success. It would also be possible to craft the tranches dynamically so that if there are transactions, with fees that are outliers, the bounds of the tranches can be structured to promote fairer execution.</p>
<pre><code class="lang-txt">As an example for a given block, the transaction priority fees in gwei:
`[ 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 4, 100 ]`

The tranches could be crafted like:
`[ 1, 1, 1, 1, 1, 1, 1, 1, 1 ]`
`[ 2, 2, 2, 2, 2, 2, 2 ]`
`[ 4, 100 ]`
</code></pre>
<h4><a class="anchor" href="https://ethresear.ch#p-51392-priority-fee-based-ordering-non-pseudo-random-15" name="p-51392-priority-fee-based-ordering-non-pseudo-random-15"></a>Priority-Fee Based Ordering (Non-Pseudo-Random)</h4>
<p>Once encrypted mem-pools are fully implemented it would be possible to move to a fully priority-fee based ordering heuristic. This would be the ideal situation that solely relies on market dynamics to set pricing for execution order.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51392-rewards-and-penalties-16" name="p-51392-rewards-and-penalties-16"></a>Rewards and Penalties</h3>
<p>In this paradigm, the searchers adding transactions to the Inclusion Lists are the ones that would receive a bulk of the reward.  The block proposer would now be strictly following the protocol and have reduced agency. With the reduction of duty a majority of the proposal rewards should get shifted to searchers to avoid inflation. This will also have the added benefit of smoothing rewards from a single proposer to the whole IL committee. It could be argued that the proposer generating and gossiping the Inclusion Nonce at N-1 is now a critical point of failure though because the proposal process would hinge upon them.  Thus the block reward that is earned in slot N should be requisite of the nonce gossip at slot N-1.</p>
<p>The rewards mechanism of FOCIL would also need to be extended to accommodate the moving of partial block rewards to the searchers for including novel transactions.  This idea is referenced by the FOCIL team but is not finalized yet and thus needs to be updated here once that proposal progresses.</p>
<p>Because the IL Committee now hold the critical piece to transaction selection the task is now of high importance. Thusly, equivocation by a committee member should be slashable similar to creating conflicting attestations or blocks.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51392-left-over-transactions-17" name="p-51392-left-over-transactions-17"></a>Left-Over Transactions</h3>
<p>The implication of having multiple un-aggregated inclusion lists and all transactions for a block being sourced through combining them into a single list is the potential for more transactions being submitted than can fit into a single block. This means that the Inclusion Pool (transactions submitted by IL committee members) might need to be inserted into the subsequent block. How this happens is still an area of research but some suggestions proposed so far are:</p>
<ul>
<li>Do not guarantee IL transactions MUST be included. More like Inclusion Suggestions Lists and a heuristic will be used to include transactions in the block that is reproducible. All transactions that are not included may be added to subsequent block lists</li>
<li>Limiting IL size to prevent overflow</li>
<li>Adding a time weighting such that transactions that have waited for inclusion get higher precedence in the next block. This could be added to the priority fee for instance.  If a large collection of high fee transactions are submitted which prevent inclusion the time weighting could be increased for subsequent blocks to help push the transaction through eventually</li>
<li>Setting a threshold for sequencing.  If the transaction has a very low priority fee, and does not make it into several blocks it gets removed from the overflow</li>
</ul>
<p>Of the suggestions the first is easiest, and most pragmatic to implement. The only thing that would be needed is to develop the heuristic for block inclusion for transactions on a list.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51392-interactions-with-other-eips-and-existing-roadmap-18" name="p-51392-interactions-with-other-eips-and-existing-roadmap-18"></a>Interactions with Other EIP’s and Existing Roadmap</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51392-account-abstractions-19" name="p-51392-account-abstractions-19"></a>Account Abstractions</h3>
<p>It would be possible to have the EL’s include AA generated transactions to the IL when a list is generated. There are some nuances to fees for prioritization with this though.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51392-epbs-20" name="p-51392-epbs-20"></a>ePBS</h3>
<p>This proposal would remove the need for ePBS and the complexity that it adds to the protocol</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51392-proposer-attester-separation-21" name="p-51392-proposer-attester-separation-21"></a>Proposer-Attester Separation</h3>
<p>The chance for Execution Tickets, and other slot auction style proposals, to create further vertical integration by the builders is a real concern. If ordering was done in protocol it reduces this risk substantially. This would be a boon to all suggestions that separate the proposer duty from attester duties which would greatly increase the successful implementation of protocol developments that allow for very light attestation-only clients</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51392-discussion-22" name="p-51392-discussion-22"></a>Discussion</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51392-protocol-simplification-23" name="p-51392-protocol-simplification-23"></a>Protocol Simplification</h3>
<p>The more complex systems become, the more difficult game theory is when planning and implementing protocols within the system. I argue that building widgets on top of widgets to solve problems creates more attacks surface within the system the widgets are designed to protect. In the case of building protocols to inhibit MEV/builders/relays without doing away with builder flow creates more nooks for exploitation to live in. We must address the existing system design and attack the root cause, selection and ordering of transactions without the help of “out of protocol” opaque solutions.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51392-preserving-cypherpunk-and-egalitarian-principles-in-the-protocol-24" name="p-51392-preserving-cypherpunk-and-egalitarian-principles-in-the-protocol-24"></a>Preserving Cypherpunk and Egalitarian Principles in the Protocol</h3>
<p>Make Ethereum Cypherpunk Again. This is a call to action on our pilgrimage to MECA. To preserve the ethos that brought us all here.  Builders and the ecosystem that has built up around them must not be allowed to usurp L1. Allowing actors to extract value from honest participants is not only antithetical to the ideals of blockchain but it dissuades adoption by traditional industries. Blockchain brings Rule of Law to the digital space. The idea that single participant is above the protocol. Restoring and preserving that will entice experimentation with blockchain as the root of trust for inter-system interactions.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51392-layered-settlement-25" name="p-51392-layered-settlement-25"></a>Layered Settlement</h3>
<p>The traditional financial system is built upon layers of infrastructure. It is important to highlight this because there may be significant push-back to the trade-offs this proposal introduces. Settlement time will potentially increase for some participants. Some transactions may fail because builders provide some level of transaction coordination (at the expense of the MEV). Transaction fees might go up.  L1 is not, and should not be designed for instantaneous settlement.  Consensus takes time and the tradeoff for that speed is decentralization and the security it imbues.  Builders will be upset and if incentives are tuned correctly because value extraction will be converted into priority fees. Transaction volume will move to L2.  But these are healthy things for an L2 centric scaling roadmap.</p>
<p>Being able to facilitate liquidity pool price discovery, ie very large buys/sells between LPs, across L2s and other application chains like the Uniswap rollup will benefit from surety of price discovery and honest auctions.  The trade off is small transactions will be too expensive to run on L1. This is similar to how our existing financial system works today.  Shares are held by DTCC on behalf of brokers for the benefit of the broker’s clients.  It is costly and difficult for individuals to transact small positions because order books at that level are for very very large quantities.  Brokers facilitate this by providing localized liquidity and pay for the service through transaction sequencing (and in some instances sandwich trades).  The end consumers, however, get to create trades for no fee (other than the slippage that is introduced by the broker).</p>
<p>The bottom line is consensus takes time and the trade-off between decentralization and throughput is real. For the benefit of the protocol over the long term, preserving the decentralization at all costs is the secret sauce of what will preserve Ethereum far into the future.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/in-protocol-transaction-ordering/21084">Read full topic</a></p>
]]></content:encoded>
<pubDate>Mon, 25 Nov 2024 08:18:48 +0000</pubDate>
</item>
<item>
<title>Bitcoin: A Peer-to-Peer Electronic CASH System ~ in part by: Overpass Channels</title>
<link>https://ethresear.ch/t/bitcoin-a-peer-to-peer-electronic-cash-system-in-part-by-overpass-channels/20987</link>
<guid>https://ethresear.ch/t/bitcoin-a-peer-to-peer-electronic-cash-system-in-part-by-overpass-channels/20987</guid>
<content:encoded><![CDATA[
<div> 关键词：Overpass Channels、Bitcoin、隐私、可扩展性、信任less桥接

总结:<br />
本文提出了基于Overpass Channels架构的比特币Layer 2解决方案，旨在实现无需改变比特币协议和共识模型前提下的高交易量处理，同时确保隐私性和可扩展性。对比了Overpass Channels与BitVM2，突显了前者的隐私保护、经济中立和可扩展性的优势。Overpass通过分布式存储、优化状态管理和隐私增强型zk-SNARKs等技术手段，实现了与比特币HTLC的无缝集成，保证了与比特币核心属性的一致性。文章通过定理和证明展示了Overpass Channels如何维持比特币的安全属性，并分析了其在隐私、可扩展性、经济中立性和安全性方面的优越性。最后，展望了Overpass Channels作为多链环境中的通用现金层的潜力。 <div>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-overpass-channel-sub-paper-1" name="p-51185-overpass-channel-sub-paper-1"></a>Overpass Channel Sub-paper:</h2>
<h1><a class="anchor" href="https://ethresear.ch#p-51185-bitcoin-bo-instant-private-massively-scalable-liquid-bitcoin-with-true-trustless-bridge-pro-maxi-choice-l1-heterogenous-2" name="p-51185-bitcoin-bo-instant-private-massively-scalable-liquid-bitcoin-with-true-trustless-bridge-pro-maxi-choice-l1-heterogenous-2"></a>Bitcoin (B²O): Instant, Private, Massively Scalable, Liquid Bitcoin with true trustless bridge - Pro Maxi Choice  - [L1 heterogenous]</h1>
<p><strong>Author</strong>: Brandon “Cryptskii” Ramsay<br />
<strong>Date</strong>: 2024-11-14</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-abstract-3" name="p-51185-abstract-3"></a>Abstract</h2>
<p>In response to the growing economic challenges faced by traditional financial systems, Bitcoin’s significance as a decentralized, censorship-resistant store of value continues to rise. Building on the Overpass Channels architecture, we propose a privacy-preserving, scalable Layer 2 solution that enables high-volume transactions on Bitcoin without altering its protocol or consensus model. This paper presents a comparative analysis of Overpass Channels and BitVM2, substantiating Overpass’s superiority in privacy, economic neutrality, and scalability. We formalize the system’s operational assumptions and provide rigorous theorems and proofs that validate Overpass’s ability to maintain Bitcoin’s security properties and monetary principles, setting a new benchmark for scalability on Bitcoin’s blockchain.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51185-h-1-introduction-4" name="p-51185-h-1-introduction-4"></a>1. Introduction</h1>
<p>The escalating volatility within traditional financial systems underscores Bitcoin’s foundational role as a decentralized store of value. As Bitcoin adoption grows, the need for scalable and private transaction mechanisms is evident. Leveraging the Overpass Channels architecture<br />
<a href="https://eprint.iacr.org/2024/1526" rel="noopener nofollow ugc">Overpass.2024</a>, we introduce a solution specifically designed to scale Bitcoin transactions without altering its consensus or core protocol. By contrasting Overpass Channels with BitVM2, we elucidate the distinct advantages of our approach in maintaining privacy and network integrity while ensuring economic neutrality.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-h-11-motivation-5" name="p-51185-h-11-motivation-5"></a>1.1 Motivation</h2>
<p>Given the limitations of traditional Layer 2 solutions—often requiring protocol adjustments or trust-based assumptions—the Overpass Channels approach offers a uniquely adaptable, non-invasive solution that enables Bitcoin to scale without compromising its decentralized ethos. While recent advancements like BitVM2 have made strides in SNARK-based verification, Overpass Channels address these challenges through its established hierarchical structure [Section 9.1] and privacy-focused mechanisms [Section 3].</p>
<ul>
<li><strong>Distributed Storage</strong>: Utilizes Overpass’s distributed storage model [Section 10] for efficient transaction handling.</li>
<li><strong>Optimized State Management</strong>: Employs hierarchical sparse Merkle trees [Section 12] for lightweight Bitcoin state management.</li>
<li><strong>Privacy-Enhanced zk-SNARKs</strong>: Integrates Plonky2-based zk-SNARKs [Section 3.8] to preserve transaction privacy.</li>
<li><strong>Compatibility with Bitcoin’s HTLC</strong>: Ensures seamless Bitcoin integration through HTLC adaptation [Section 8.2].</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-h-12-core-principles-6" name="p-51185-h-12-core-principles-6"></a>1.2 Core Principles</h2>
<p>Our design prioritizes the following principles to ensure Overpass Channels aligns with Bitcoin’s core properties:</p>
<ol>
<li><strong>Protocol Integrity</strong>: Achieves scalability without protocol modifications to Bitcoin.</li>
<li><strong>Economic Consistency</strong>: Preserves Bitcoin’s economic incentives and fee structure.</li>
<li><strong>Trustless Design</strong>: Implements trustless operation based on Overpass’s proven cryptographic assumptions [Section 6].</li>
<li><strong>Privacy Assurance</strong>: Enhances transaction privacy by default, following Overpass’s established privacy guarantees [Section 18].</li>
<li><strong>Decentralization Support</strong>: Maintains economic neutrality to avoid concentration of network power.</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-comparison-framework-7" name="p-51185-comparison-framework-7"></a>Comparison Framework</h3>
<p>To formalize the comparison between Overpass Channels and BitVM2, we establish a rigorous evaluation framework based on privacy, scalability, economic neutrality, and security. Each metric is substantiated through theorem-proof structures that quantify the systems’ respective capabilities.</p>
<p><strong>Definition (Layer 2 Security Preservation)</strong>: A Layer 2 solution <span class="math">S</span> preserves Bitcoin’s security model if and only if:<br />
<span class="math">
\forall t \in T, \; P(\text{attack} \mid S) \leq P(\text{attack} \mid \text{Bitcoin})
</span><br />
where <span class="math">T</span> is the set of all transaction types, and <span class="math">P(\text{attack})</span> represents the probability of a successful attack.</p>
<p><strong>Theorem (Security Preservation in Overpass Channels)</strong>: Overpass Channels maintain Bitcoin’s security properties with respect to consensus and decentralization by ensuring that no additional vulnerabilities are introduced in state management or transaction validation:<br />
<span class="math">
P(\text{attack} \mid \text{Overpass}) = P(\text{attack} \mid \text{Bitcoin}).
</span></p>
<p><strong>Proof</strong>: Let <span class="math">A</span> be an adversary aiming to compromise transactions in Overpass Channels. For any attack strategy <span class="math">\sigma</span>:</p>
<ol>
<li>
<p>The adversary must either:</p>
<ul>
<li>Break Bitcoin’s security assumptions, or</li>
<li>Exploit a flaw in Overpass’s zk-SNARK verification or channel closure mechanism.</li>
</ul>
</li>
<li>
<p>Overpass Channels enforce the following:</p>
<ul>
<li>zk-SNARK soundness guarantees transaction validity.</li>
<li>Channel closure requires a valid Bitcoin transaction, preserving the network’s security model.</li>
<li>No additional cryptographic assumptions beyond standard zk-SNARK soundness are introduced.</li>
</ul>
</li>
<li>
<p>Consequently, the security of Overpass Channels is bounded by Bitcoin’s own security assumptions and the integrity of zk-SNARK proofs:<br />
<span class="math">P(\text{attack} \mid \text{Overpass}) = P(\text{attack} \mid \text{Bitcoin})</span></p>
</li>
</ol>
<p>This completes the proof, showing that Overpass Channels do not degrade Bitcoin’s security guarantees.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-technical-architecture-8" name="p-51185-technical-architecture-8"></a>Technical Architecture</h2>
<p>The integration of Overpass Channels with Bitcoin leverages several technical mechanisms to achieve scalability and privacy while preserving security. We provide a structured comparison with BitVM2 to highlight Overpass’s unique advantages.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-unilateral-payment-channels-9" name="p-51185-unilateral-payment-channels-9"></a>Unilateral Payment Channels</h3>
<p>Overpass Channels introduce a unilateral payment channel structure specifically optimized for Bitcoin, distinct from BitVM2’s state model.</p>
<p><strong>Definition (Bitcoin-Compatible Unilateral Channel)</strong><br />
A Bitcoin-compatible unilateral channel <span class="math">C</span> is defined as a tuple <span class="math">(pk_s, pk_r, v, t, \sigma)</span> where:</p>
<ul>
<li><span class="math">pk_s</span>: Sender’s public key</li>
<li><span class="math">pk_r</span>: Receiver’s public key</li>
<li><span class="math">v</span>: Channel value in satoshis</li>
<li><span class="math">t</span>: Timelock value</li>
<li><span class="math">\sigma</span>: Channel signature</li>
</ul>
<p>satisfying the following property:<br />
<span class="math">{ValidChannel}(C) \iff {VerifyBitcoinSig}(sigma, (pk_s, pk_r, v, t)) = {true}</span></p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-cryptographic-constructions-for-bitcoin-channels-10" name="p-51185-cryptographic-constructions-for-bitcoin-channels-10"></a>Cryptographic Constructions for Bitcoin Channels</h3>
<p>Overpass Channels ensure privacy and security through cryptographic constructions designed to operate efficiently on Bitcoin’s existing infrastructure. This approach contrasts with BitVM2’s focus on sequential verification, yielding distinct privacy and efficiency advantages.</p>
<p><strong>Theorem (Channel State Privacy)</strong><br />
Given a channel state <span class="math">S</span> and its corresponding zk-SNARK proof <span class="math">\pi</span>, no adversary <span class="math">A</span> can determine the transaction history or current balances with probability greater than negligible, while still being able to verify the validity of the state.</p>
<p><strong>Proof</strong><br />
Let <span class="math">S</span> be a channel state and <span class="math">\pi</span> its corresponding zk-SNARK proof. Privacy is ensured through a series of games:</p>
<ol>
<li>
<p><strong>Game 0</strong>: The real privacy game, where an adversary <span class="math">A</span> attempts to learn information about the channel state <span class="math">S</span>.</p>
</li>
<li>
<p><strong>Game 1</strong>: Modify Game 0 by replacing the real zk-SNARK proof with a simulated proof.</p>
<p>By the zero-knowledge property of zk-SNARKs:<br />
<span class="math">\left| \Pr[A \text{ wins Game 0}] - \Pr[A \text{ wins Game 1}] \right| \leq \text{negl}(\lambda)</span><br />
where <span class="math">\text{negl}(\lambda)</span> is a negligible function in the security parameter <span class="math">\lambda</span>.</p>
</li>
<li>
<p><strong>Game 2</strong>: Replace the real channel state <span class="math">S</span> with a random, valid state.</p>
<p>By the hiding property of the commitment scheme:<br />
$\left| \Pr[A \text{ wins Game 1}] - \Pr[A \text{ wins Game 2}] \right| \leq \text{negl}(\lambda)$$</p>
</li>
</ol>
<p>In Game 2, the adversary receives no information about the actual channel state <span class="math">S</span>, resulting in:<br />
<span class="math">\Pr[A \text{ wins Game 2}] = \frac{1}{2}</span></p>
<p>Through this sequence of games, we conclude that <span class="math">A</span>'s advantage in the real game (Game 0) is negligible, establishing privacy for the Overpass Channels.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-channel-operations-and-bitcoin-script-integration-11" name="p-51185-channel-operations-and-bitcoin-script-integration-11"></a>Channel Operations and Bitcoin Script Integration</h3>
<p>Overpass Channels implement functionality through Bitcoin-compatible scripts, enabling secure channel operations without modifying Bitcoin’s protocol. This approach differs from BitVM2, which requires sequential verification stages, by focusing on privacy preservation and operational efficiency.</p>
<p><strong>Algorithm: Channel Opening on Bitcoin</strong></p>
<p><strong>Require:</strong> Sender keys <span class="math">sk_s</span>, <span class="math">pk_s</span>, Receiver public key <span class="math">pk_r</span>, Channel value <span class="math">v</span></p>
<ol>
<li>
<p>Generate funding transaction <span class="math">T_f</span> with the following script:</p>
<pre><code class="lang-auto">OP_IF
   OP_SHA256 H(revocation_key)
   OP_EQUALVERIFY
   pk_r OP_CHECKSIG
OP_ELSE
   timeout OP_CHECKLOCKTIMEVERIFY
   OP_DROP
   pk_s OP_CHECKSIG
OP_ENDIF
</code></pre>
</li>
<li>
<p>Broadcast <span class="math">T_f</span> to the Bitcoin network.</p>
</li>
<li>
<p>Generate zk-SNARK proof <span class="math">\pi</span> of the channel state validity.</p>
</li>
</ol>
<p><strong>Ensure:</strong> <span class="math">(T_f, \pi)</span></p>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-comparison-with-bitvm2-12" name="p-51185-comparison-with-bitvm2-12"></a>Comparison with BitVM2</h2>
<p>Overpass Channels and BitVM2 both utilize zk-SNARKs to enable advanced transaction verification on Bitcoin. However, their approaches to state management, privacy, and scalability vary significantly. This section provides a detailed comparison to illustrate the advantages of Overpass Channels over BitVM2.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-architectural-differences-13" name="p-51185-architectural-differences-13"></a>Architectural Differences</h3>
<p>The core architectural design of each system impacts their performance and scalability. Overpass Channels leverage distributed state management and privacy-preserving mechanisms, while BitVM2 emphasizes sequential verification stages.</p>
<div class="md-table">
<table>
<thead>
<tr>
<th>Feature</th>
<th>Overpass Channels</th>
<th>BitVM2</th>
</tr>
</thead>
<tbody>
<tr>
<td>State Model</td>
<td>Privacy-preserving off-chain</td>
<td>Off-chain with on-chain verification</td>
</tr>
<tr>
<td>Privacy</td>
<td>Full transaction privacy</td>
<td>Basic transaction privacy</td>
</tr>
<tr>
<td>Scalability</td>
<td><span class="math">O(n)</span> horizontal scaling</td>
<td><span class="math">O(n)</span> with verification overhead</td>
</tr>
<tr>
<td>Trust Model</td>
<td>Bitcoin-equivalent</td>
<td>Bitcoin-equivalent with setup</td>
</tr>
<tr>
<td>Impact on Miners</td>
<td>Neutral</td>
<td>Neutral with verification cost</td>
</tr>
<tr>
<td>Verification Method</td>
<td>Optimized SNARK proofs</td>
<td>Sequential SNARK-based verification</td>
</tr>
</tbody>
</table>
</div><h3><a class="anchor" href="https://ethresear.ch#p-51185-economic-implications-14" name="p-51185-economic-implications-14"></a>Economic Implications</h3>
<p>The economic implications of each approach significantly affect Bitcoin’s fee market and miner incentives. While both systems maintain Bitcoin’s security model, their respective costs and operational overhead differ.</p>
<p><strong>Theorem (Incentive Compatibility)</strong><br />
Let <span class="math">M</span> represent Bitcoin miners, and let <span class="math">I(m)</span> be the expected income of a miner <span class="math">m</span>. Under both Overpass Channels and BitVM2:<br />
<span class="math">\forall m \in M: E[I(m) \mid L2] \geq E[I(m) \mid Bitcoin]</span><br />
with system-specific overhead distributions as follows:<br />
<span class="math">O_{\text{Overpass}} = O_{\text{constant}}</span><br />
<span class="math">O_{\text{BitVM2}} = O_{\text{verification}} + O_{\text{setup}}</span></p>
<p><strong>Proof</strong><br />
For Overpass Channels:</p>
<ol>
<li>Channel operations rely on standard Bitcoin transactions.</li>
<li>Verification burden remains constant due to optimized SNARK proofs.</li>
<li>Mining decentralization and fee structures remain unaffected.</li>
</ol>
<p>For BitVM2:</p>
<ol>
<li>Similar reliance on standard Bitcoin transactions.</li>
<li>Initial setup and verification costs introduced.</li>
<li>Verification overhead potentially impacts miner fees due to increased computational requirements.</li>
</ol>
<p>Therefore, both systems preserve Bitcoin’s incentive model, although Overpass offers a more consistent and lower overhead for miners.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-network-effects-and-liquidity-15" name="p-51185-network-effects-and-liquidity-15"></a>Network Effects and Liquidity</h3>
<p>The liquidity distribution and network effects of each system are crucial for Bitcoin’s economic stability. Overpass Channels achieve liquidity efficiency with minimized operational costs, offering an advantage over BitVM2’s verification overhead.</p>
<p><strong>Theorem (Liquidity Preservation)</strong><br />
In a network with total liquidity <span class="math">L</span>, both systems preserve Bitcoin’s liquidity pool:<br />
<span class="math">L_{\text{effective}} = L_{\text{total}} - O_{\text{system}}</span><br />
where:<br />
<span class="math">O_{\text{Overpass}} &lt; O_{\text{BitVM2}}</span><br />
due to Overpass’s optimized state management and lack of setup costs.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-security-considerations-and-risk-analysis-16" name="p-51185-security-considerations-and-risk-analysis-16"></a>Security Considerations and Risk Analysis</h2>
<p>Layer 2 solutions must be carefully analyzed for security implications to ensure they do not compromise Bitcoin’s core properties. This section provides a comprehensive examination of the security models for Overpass Channels and BitVM2, focusing on privacy, attack surface, and resistance to double-spend attacks.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-attack-surface-analysis-17" name="p-51185-attack-surface-analysis-17"></a>Attack Surface Analysis</h3>
<p>The attack surface of each system represents the potential vulnerability points that could be exploited by adversaries. Overpass Channels and BitVM2 both introduce minimal attack surfaces, but their structural differences affect the composition of these surfaces.</p>
<p><strong>Definition (Attack Surface Extension)</strong><br />
For a Layer 2 solution <span class="math">L</span>, the attack surface extension <span class="math">E(L)</span> is defined as:<br />
<span class="math">E(L) = \{(v, p) \mid v \in V(L) \setminus V(Bitcoin), p &gt; 0\}</span><br />
where <span class="math">V(L)</span> is the set of potential vulnerability points in <span class="math">L</span> and <span class="math">p</span> is the probability of successful exploitation.</p>
<p><strong>Theorem (Equivalent Base Extension)</strong><br />
Both systems maintain minimal attack surface extension:<br />
<span class="math">|E(\text{Overpass})| = O(1)</span><br />
<span class="math">|E(\text{BitVM2})| = O(1)</span><br />
with different vulnerability classes:<br />
<span class="math">V_{\text{Overpass}} = \{V_{\text{privacy}}, V_{\text{state}}\}</span><br />
<span class="math">V_{\text{BitVM2}} = \{V_{\text{setup}}, V_{\text{verify}}\}</span></p>
<p><strong>Proof</strong><br />
For both Overpass Channels and BitVM2:</p>
<ol>
<li>State transitions and transaction validity are secured by zk-SNARKs.</li>
<li>Channel operations rely on standard Bitcoin transaction security.</li>
<li>No additional consensus requirements are introduced.</li>
</ol>
<p>Key distinctions include:</p>
<ol>
<li>
<p><strong>Privacy Mechanism</strong>:</p>
<ul>
<li>Overpass: Full privacy achieved through state channels.</li>
<li>BitVM2: Basic privacy limited by sequential verification.</li>
</ul>
</li>
<li>
<p><strong>Setup Requirements</strong>:</p>
<ul>
<li>Overpass: Direct channel initialization without additional setup.</li>
<li>BitVM2: Requires an initial verification setup phase.</li>
</ul>
</li>
</ol>
<p>Thus, both systems achieve minimal and comparable attack surface extensions, though the structure of vulnerability classes differs.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-double-spend-prevention-18" name="p-51185-double-spend-prevention-18"></a>Double-Spend Prevention</h3>
<p>Double-spend prevention is essential for maintaining Bitcoin’s integrity as a monetary system. Both Overpass Channels and BitVM2 implement robust mechanisms to prevent double-spend attacks.</p>
<p><strong>Theorem (Double-Spend Prevention)</strong><br />
For both systems, the probability of a successful double-spend attack <span class="math">P(DS)</span> is bounded by:<br />
<span class="math">P(DS) \leq \min(P(\text{Bitcoin\_DS}), P(\text{zk\_break}))</span><br />
where <span class="math">P(\text{Bitcoin\_DS})</span> represents the probability of a double-spend on Bitcoin and <span class="math">P(\text{zk\_break})</span> represents the probability of breaking the zk-SNARK system.</p>
<p><strong>Proof</strong><br />
Let <span class="math">A</span> be an adversary attempting a double-spend attack. For success, <span class="math">A</span> must either:</p>
<ol>
<li>Compromise Bitcoin’s underlying security model with probability <span class="math">P(\text{Bitcoin\_DS})</span>.</li>
<li>Generate a false zk-SNARK proof with probability <span class="math">P(\text{zk\_break})</span>.</li>
</ol>
<p>Additionally, both systems enforce a channel closure mechanism that ensures:<br />
<span class="math">\forall s_1, s_2 \in \text{States}: \text{Close}(s_1) \land \text{Close}(s_2) \implies s_1 = s_2</span></p>
<p>Thus, the probability of a successful double-spend attack is bounded by the minimum probability of either compromising Bitcoin’s security or breaking the zk-SNARK proof system, regardless of system-specific differences.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-impact-on-bitcoins-security-model-19" name="p-51185-impact-on-bitcoins-security-model-19"></a>Impact on Bitcoin’s Security Model</h3>
<p>Each Layer 2 solution must be assessed for its impact on Bitcoin’s core security properties, such as decentralization, censorship resistance, and immutability. Overpass Channels and BitVM2 maintain these properties, though their verification and state management differ.</p>
<p><strong>Definition (Security Model Preservation)</strong><br />
A Layer 2 solution <span class="math">S</span> preserves Bitcoin’s security model if:<br />
<span class="math">\forall p \in \text{Properties(Bitcoin)}: \text{Guarantee}(p \mid S) \geq \text{Guarantee}(p \mid \text{Bitcoin})</span><br />
where <span class="math">\text{Properties(Bitcoin)}</span> includes decentralization, censorship resistance, and immutability.</p>
<p><strong>Theorem (Security Model Impact)</strong><br />
Both Overpass Channels and BitVM2 maintain Bitcoin’s security model with distinct architectural trade-offs:<br />
<span class="math">\Delta_{\text{security}}(\text{Overpass}) = \Delta_{\text{security}}(\text{BitVM2}) = 0</span><br />
though they follow different verification pathways:<br />
<span class="math">\text{Path}_{\text{Overpass}} = \{\text{Privacy}, \text{StateManagement}\}</span><br />
<span class="math">\text{Path}_{\text{BitVM2}} = \{\text{Setup}, \text{VerificationFlow}\}</span></p>
<p><strong>Proof</strong><br />
To assess security preservation, consider the following for both systems:</p>
<ol>
<li>
<p><strong>Consensus Requirements</strong>:</p>
<ul>
<li>Both systems operate without modifying Bitcoin’s consensus.</li>
</ul>
</li>
<li>
<p><strong>Cryptographic Assumptions</strong>:</p>
<ul>
<li>Each system relies on zk-SNARKs, ensuring equivalent cryptographic strength.</li>
</ul>
</li>
<li>
<p><strong>State and Transaction Management</strong>:</p>
<ul>
<li>Overpass: Employs integrated, privacy-preserving state channels, minimizing exposure.</li>
<li>BitVM2: Utilizes a sequential verification process that introduces verification layers but maintains on-chain compatibility.</li>
</ul>
</li>
<li>
<p><strong>Implementation Distinctions</strong>:</p>
<ul>
<li>Overpass prioritizes direct state transitions, reducing operational overhead.</li>
<li>BitVM2 requires setup and verification sequences, increasing complexity.</li>
</ul>
</li>
</ol>
<p>Therefore, both systems preserve Bitcoin’s security model while following distinct approaches to verification and state management.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-liveness-and-availability-analysis-20" name="p-51185-liveness-and-availability-analysis-20"></a>Liveness and Availability Analysis</h3>
<p>The liveness and availability of transactions are critical for user experience and adoption. Overpass Channels and BitVM2 achieve comparable liveness guarantees through different transaction handling mechanisms.</p>
<p><strong>Theorem (Liveness Guarantee)</strong><br />
Under both systems, transaction liveness <span class="math">L(t)</span> for a transaction <span class="math">t</span> is guaranteed with probability:<br />
<span class="math">P(L(t)) \geq 1 - (1 - p)^k</span><br />
where <span class="math">p</span> is the probability of successful Bitcoin transaction inclusion and <span class="math">k</span> is the number of confirmation attempts.</p>
<p><strong>Proof</strong><br />
For both systems:</p>
<ol>
<li>
<p><strong>Channel Operations</strong>:</p>
<ul>
<li>Rely on standard Bitcoin transactions for channel creation and closure.</li>
</ul>
</li>
<li>
<p><strong>Verification Methodology</strong>:</p>
<ul>
<li>Both systems use zk-SNARK proofs for verification, enabling off-chain transaction finality.</li>
</ul>
</li>
<li>
<p><strong>Channel Closure Attempts</strong>:</p>
<ul>
<li>With <span class="math">k</span> attempts, the probability of successful closure is given by:<br />
<span class="math">P(\text{closure\_success}) = 1 - (1 - p)^k</span></li>
</ul>
</li>
</ol>
<p>Since each system relies on Bitcoin’s underlying liveness properties for final settlement, they both achieve equivalent liveness guarantees.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-long-term-security-implications-21" name="p-51185-long-term-security-implications-21"></a>Long-term Security Implications</h3>
<p>Both Overpass Channels and BitVM2 must be evaluated for their long-term security impacts, especially in terms of protocol longevity and resistance to future attack vectors.</p>
<p><strong>Theorem (Security Model Evolution)</strong><br />
The long-term security impact <span class="math">I(t)</span> of both Layer 2 solutions at time <span class="math">t</span> satisfies:<br />
<span class="math">\lim_{t \to \infty} I(t) = 0</span><br />
with differing composition vectors:<br />
<span class="math">V_{\text{Overpass}}(t) = \{v_{\text{privacy}}(t), v_{\text{state}}(t)\}</span><br />
<span class="math">V_{\text{BitVM2}}(t) = \{v_{\text{setup}}(t), v_{\text{verify}}(t)\}</span></p>
<p><strong>Proof</strong><br />
Consider the following security properties for both systems:</p>
<ol>
<li><strong>Longevity of Cryptographic Assumptions</strong>:</li>
</ol>
<ul>
<li>Both rely on zk-SNARKs with long-term security guarantees, ensuring consistency over time.</li>
</ul>
<ol start="2">
<li><strong>System-Specific Implications</strong>:</li>
</ol>
<ul>
<li>Overpass: Long-term stability due to privacy-preserving channels and minimal setup requirements.</li>
<li>BitVM2: Security preserved through on-chain verification, though with added complexity in setup and verification stages.</li>
</ul>
<ol start="3">
<li><strong>Impact on Bitcoin’s Security</strong>:</li>
</ol>
<ul>
<li>Neither system requires alterations to Bitcoin’s protocol, preserving the core security properties indefinitely.</li>
</ul>
<p>Thus, the long-term security impact remains neutral for both systems, with each maintaining minimal additional risk over time.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-privacy-guarantees-and-economic-implications-22" name="p-51185-privacy-guarantees-and-economic-implications-22"></a>Privacy Guarantees and Economic Implications</h2>
<p>The privacy and economic characteristics of a Layer 2 solution significantly affect Bitcoin’s fungibility and monetary stability. Overpass Channels and BitVM2 both employ zk-SNARKs, yet their approaches to privacy and economic neutrality are fundamentally different.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-privacy-model-23" name="p-51185-privacy-model-23"></a>Privacy Model</h3>
<p>Privacy within a Layer 2 solution is critical for ensuring that transactions are indistinguishable, preserving Bitcoin’s fungibility. Overpass Channels provide enhanced privacy over BitVM2 due to its integrated, privacy-preserving state channels.</p>
<p><strong>Definition (Transaction Privacy)</strong><br />
A transaction <span class="math">T</span> in a Layer 2 system provides <span class="math">\delta</span>-privacy if for any adversary <span class="math">A</span>:<br />
<span class="math">\left| \Pr[A(T) = 1] - \Pr[A(T') = 1] \right| \leq \delta</span><br />
where <span class="math">T'</span> is any other valid transaction with identical public parameters.</p>
<p><strong>Theorem (Privacy Guarantees)</strong><br />
Overpass Channels achieve an enhanced level of privacy, denoted <span class="math">\varepsilon</span>-privacy:<br />
<span class="math">\varepsilon_{\text{Overpass}} \leq \frac{1}{2^\lambda}</span><br />
compared to BitVM2’s basic transaction privacy:<br />
<span class="math">\varepsilon_{\text{BitVM2}} \leq \frac{1}{2^\lambda} + \delta_{\text{state}}</span><br />
where <span class="math">\delta_{\text{state}}</span> represents additional information leakage due to BitVM2’s state verification.</p>
<p><strong>Proof</strong><br />
Let <span class="math">A</span> be an adversary attempting to distinguish between transactions:</p>
<ol>
<li><strong>Base zk-SNARK Privacy</strong>:</li>
</ol>
<ul>
<li>By the zero-knowledge property of zk-SNARKs, for any input <span class="math">x</span> and witness <span class="math">w</span>:<br />
<span class="math">\{\text{Prove}(x, w)\} \approx_c \{\text{Sim}(x)\}</span></li>
</ul>
<ol start="2">
<li><strong>System-Specific Privacy Distinctions</strong>:</li>
</ol>
<ul>
<li>
<p>Overpass: Full state privacy, leading to negligible information leakage:<br />
<span class="math">\left| \Pr[A(\pi, P, U) = 1] - \Pr[A(\text{Sim}(\pi), P, U) = 1] \right| \leq \frac{1}{2^\lambda}</span></p>
</li>
<li>
<p>BitVM2: State verification introduces potential leakage:<br />
<span class="math">\left| \Pr[A(\pi, P, U) = 1] - \Pr[A(\text{Sim}(\pi), P, U) = 1] \right| \leq \frac{1}{2^\lambda} + \delta_{\text{state}}</span></p>
</li>
</ul>
<ol start="3">
<li><strong>Conclusion</strong>:<br />
While both systems provide robust privacy through zk-SNARKs, Overpass achieves stronger privacy guarantees due to its privacy-preserving state channels, resulting in reduced leakage.</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-economic-impact-analysis-24" name="p-51185-economic-impact-analysis-24"></a>Economic Impact Analysis</h3>
<p>The economic implications of each system on Bitcoin’s fee market and miner incentives are essential to maintaining a balanced ecosystem.</p>
<p><strong>Theorem (Fee Market Preservation)</strong><br />
Under both systems, Bitcoin’s fee market equilibrium <span class="math">E</span> remains stable:<br />
<span class="math">|E_{\text{L2}} - E_{\text{Bitcoin}}| \leq \epsilon</span><br />
where <span class="math">\epsilon</span> is a negligible factor, with differing overhead distributions:<br />
<span class="math">\epsilon_{\text{Overpass}} = O_{\text{channel}} + O_{\text{privacy}}</span><br />
<span class="math">\epsilon_{\text{BitVM2}} = O_{\text{setup}} + O_{\text{verify}}</span></p>
<p><strong>Proof</strong><br />
For a transaction <span class="math">t</span>, the fee function <span class="math">F(t)</span> can be expressed as:<br />
<span class="math">F(t) = \alpha \cdot s(t) + \beta \cdot p(t)</span><br />
where <span class="math">s(t)</span> is the transaction size, and <span class="math">p(t)</span> is the priority.</p>
<ol>
<li><strong>Overpass Channels</strong>:</li>
</ol>
<ul>
<li>Operations incur minimal overhead due to privacy-preserving channels.</li>
<li>Fee structure remains consistent with Bitcoin’s standard model.</li>
</ul>
<ol start="2">
<li><strong>BitVM2</strong>:</li>
</ol>
<ul>
<li>Additional setup and verification phases introduce operational overhead.</li>
<li>The fee model remains consistent but with added verification costs.</li>
</ul>
<p>Thus, while both systems preserve the equilibrium of Bitcoin’s fee market, Overpass offers a more efficient fee structure by minimizing extraneous costs.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-liquidity-efficiency-25" name="p-51185-liquidity-efficiency-25"></a>Liquidity Efficiency</h3>
<p>Efficient liquidity utilization is essential for a Layer 2 solution to scale while maintaining user accessibility and network sustainability. Overpass Channels provide a more optimized liquidity model than BitVM2 due to minimized verification and operational overhead.</p>
<p><strong>Theorem (Liquidity Utilization)</strong><br />
Both systems achieve efficient liquidity utilization <span class="math">U</span>, with different optimization paths:</p>
<p>For Overpass Channels:<br />
<span class="math">U_{\text{Overpass}} = \frac{L_{\text{active}}}{L_{\text{total}}} \cdot \prod_{i=1}^n r_i</span></p>
<p>For BitVM2:<br />
<span class="math">U_{\text{BitVM2}} = \frac{L_{\text{active}}}{L_{\text{total}}} \cdot \prod_{i=1}^n (r_i - \sigma_i)</span></p>
<p>where <span class="math">L_{\text{active}}</span> is the active channel liquidity, <span class="math">L_{\text{total}}</span> is the total liquidity, <span class="math">r_i</span> represents rebalancing factors, and <span class="math">\sigma_i</span> indicates verification overhead in BitVM2.</p>
<p><strong>Proof</strong><br />
Consider the set <span class="math">C</span> of all channels in the system. For each channel <span class="math">c \in C</span>:</p>
<ol>
<li>
<p><strong>Liquidity Utilization</strong>:<br />
<span class="math">u(c) = \frac{v(c)}{V(c)} \cdot r(c)</span><br />
where <span class="math">v(c)</span> is the value utilized and <span class="math">V(c)</span> is the channel capacity.</p>
</li>
<li>
<p><strong>System-Specific Utilization Factors</strong>:</p>
</li>
</ol>
<ul>
<li>
<p>Overpass Channels:<br />
<span class="math">U_{\text{Overpass}} = \frac{\sum_{c \in C} u(c) \cdot V(c)}{\sum_{c \in C} V(c)}</span><br />
indicating minimal operational costs and high liquidity efficiency.</p>
</li>
<li>
<p>BitVM2:<br />
<span class="math">U_{\text{BitVM2}} = \frac{\sum_{c \in C} (u(c) - \sigma(c)) \cdot V(c)}{\sum_{c \in C} V(c)}</span><br />
where <span class="math">\sigma(c)</span> reflects verification overhead, reducing effective liquidity.</p>
</li>
</ul>
<ol start="3">
<li><strong>Conclusion</strong>:<br />
Overpass Channels exhibit greater liquidity efficiency as they avoid the additional verification overhead imposed by BitVM2.</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-economic-centralization-resistance-26" name="p-51185-economic-centralization-resistance-26"></a>Economic Centralization Resistance</h3>
<p>Preserving decentralization within the economic model is crucial to avoid power concentration in a Layer 2 solution. Overpass Channels and BitVM2 maintain Bitcoin’s decentralization, but Overpass’s structure is inherently more resistant to centralization.</p>
<p><strong>Definition (Centralization Resistance)</strong><br />
A system <span class="math">S</span> is <span class="math">\rho</span>-centralization resistant if no entity <span class="math">e</span> can control more than <span class="math">\rho</span> fraction of the system’s economic activity:<br />
<span class="math">\forall e: \frac{\text{Control}(e)}{\text{Total}} \leq \rho</span></p>
<p><strong>Theorem (Decentralization Maintenance)</strong><br />
Both systems maintain Bitcoin’s centralization resistance bound <span class="math">\rho</span>:<br />
<span class="math">\rho_{\text{L2}} \leq \rho_{\text{Bitcoin}}</span><br />
though they differ in their resistance mechanisms:<br />
<span class="math">R_{\text{Overpass}} = \{R_{\text{privacy}}, R_{\text{state}}\}</span><br />
<span class="math">R_{\text{BitVM2}} = \{R_{\text{setup}}, R_{\text{verify}}\}</span></p>
<p><strong>Proof</strong><br />
For both systems, we examine centralization resistance as follows:</p>
<ol>
<li><strong>Architectural Aspects</strong>:</li>
</ol>
<ul>
<li>
<p>Overpass Channels:</p>
<ul>
<li>Privacy-preserving channels reduce reliance on trusted parties.</li>
<li>Distributed state management minimizes central control.</li>
</ul>
</li>
<li>
<p>BitVM2:</p>
<ul>
<li>Initial setup and verification dependencies may centralize certain operations.</li>
</ul>
</li>
</ul>
<ol start="2">
<li><strong>Economic Distribution</strong>:</li>
</ol>
<ul>
<li>Both systems employ decentralized transaction processing and verification to avoid reliance on centralized entities.</li>
<li>Dynamic rebalancing mechanisms distribute control across network participants.</li>
</ul>
<p>Thus, Overpass Channels provide a higher resistance to centralization due to minimized setup dependencies and enhanced privacy, while BitVM2 maintains resistance but with increased operational complexity.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-long-term-economic-stability-27" name="p-51185-long-term-economic-stability-27"></a>Long-term Economic Stability</h3>
<p>Ensuring economic stability over time is critical for the viability of a Layer 2 solution on Bitcoin. Both Overpass Channels and BitVM2 aim to preserve Bitcoin’s economic model; however, Overpass offers more consistent long-term stability due to its minimal operational overhead and direct transaction management.</p>
<p><strong>Theorem (Economic Model Preservation)</strong><br />
Both systems preserve Bitcoin’s long-term economic stability:<br />
<span class="math">\lim_{t \to \infty} |M_{\text{L2}}(t) - M_{\text{Bitcoin}}(t)| = 0</span><br />
where <span class="math">M(t)</span> represents the economic model at time <span class="math">t</span>. Each system has different stability vectors:<br />
<span class="math">S_{\text{Overpass}}(t) = \{S_{\text{privacy}}(t), S_{\text{channel}}(t)\}</span><br />
<span class="math">S_{\text{BitVM2}}(t) = \{S_{\text{verify}}(t), S_{\text{setup}}(t)\}</span></p>
<p><strong>Proof</strong><br />
To examine economic stability, we consider the following for each system:</p>
<ol>
<li><strong>Monetary Properties</strong>:</li>
</ol>
<ul>
<li>Both Overpass Channels and BitVM2:
<ul>
<li>Preserve Bitcoin’s fixed supply.</li>
<li>Maintain its issuance schedule.</li>
<li>Do not alter mining incentives or economic dynamics.</li>
</ul>
</li>
</ul>
<ol start="2">
<li><strong>System-Specific Characteristics</strong>:</li>
</ol>
<ul>
<li><strong>Overpass Channels</strong>:
<ul>
<li>The privacy-focused, channel-based structure ensures consistent fee and operational costs.</li>
<li>Direct state management minimizes fluctuations in transaction handling fees.</li>
</ul>
</li>
<li><strong>BitVM2</strong>:
<ul>
<li>Additional setup and verification stages introduce occasional cost spikes, which may lead to minor fee market adjustments over time.</li>
<li>The sequential verification process results in varying operational expenses.</li>
</ul>
</li>
</ul>
<ol start="3">
<li>
<p><strong>Network Effects</strong>:</p>
<ul>
<li>Both systems are designed to maintain decentralization and support censorship resistance, ensuring long-term usability and user accessibility.</li>
</ul>
<p>As <span class="math">t \to \infty</span>, both systems converge towards stable economic models with minor fluctuations for BitVM2 due to its additional verification overhead. Overpass Channels, however, offer a smoother economic trajectory with fewer cost variations.</p>
</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-comparative-analysis-of-trustless-mechanisms-28" name="p-51185-comparative-analysis-of-trustless-mechanisms-28"></a>Comparative Analysis of Trustless Mechanisms</h2>
<p>A fundamental requirement for Layer 2 solutions on Bitcoin is the minimization of trust assumptions. Overpass Channels and BitVM2 each establish distinct trust models, yet Overpass achieves stronger trust minimization due to its direct channel structure and privacy integration.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-trust-model-foundations-29" name="p-51185-trust-model-foundations-29"></a>Trust Model Foundations</h3>
<p>The level of trust required by a Layer 2 system impacts its alignment with Bitcoin’s trustless design. We formalize the trust minimization for each system.</p>
<p><strong>Theorem (Trust Minimization)</strong><br />
For both Layer 2 systems <span class="math">B</span>, the trust requirement <span class="math">T(B)</span> can be defined as:<br />
<span class="math">T(B) = \sum_{i=1}^n w_i \cdot t_i</span><br />
where <span class="math">w_i</span> represents trust weights and <span class="math">t_i</span> represents individual trust assumptions. Each system has unique trust vectors:<br />
<span class="math">T_{\text{Overpass}} = \{t_{\text{privacy}}, t_{\text{state}}\}</span><br />
<span class="math">T_{\text{BitVM2}} = \{t_{\text{setup}}, t_{\text{verify}}\}</span></p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-bridge-trust-models-30" name="p-51185-bridge-trust-models-30"></a>Bridge Trust Models</h3>
<p>Layer 2 solutions require secure bridging mechanisms with Bitcoin’s Layer 1 to facilitate interoperability while preserving trust assumptions.</p>
<p><strong>Definition (Bridge Security)</strong><br />
A bridge transaction maintains Bitcoin’s trust assumptions if:<br />
<span class="math">\forall \text{tx} \in \text{Transactions}: \text{Trust}(\text{tx}) \subseteq \text{Trust}(\text{Bitcoin})</span><br />
where <span class="math">\text{Trust(Bitcoin)}</span> encompasses Bitcoin’s base security assumptions.</p>
<p><strong>Theorem (Trust Preservation)</strong><br />
Both systems preserve Bitcoin’s trust model through different bridging mechanisms:<br />
<span class="math">T(\text{L2}) = T(\text{Bitcoin}) + T(\text{SNARK})</span><br />
where <span class="math">T(\text{SNARK})</span> represents the trust assumption introduced by zk-SNARKs. Distinct implementation paths are followed:<br />
<span class="math">\text{Path}_{\text{Overpass}} = \{\text{Privacy}, \text{StateTransition}\}</span><br />
<span class="math">\text{Path}_{\text{BitVM2}} = \{\text{Setup}, \text{VerificationFlow}\}</span></p>
<p><strong>Proof</strong><br />
The preservation of trust assumptions is achieved by both systems through:</p>
<ol>
<li>
<p><strong>zk-SNARK Trust Requirement</strong>:</p>
<ul>
<li>Both systems introduce SNARK-based proofs, which assume soundness and non-interactivity.</li>
</ul>
</li>
<li>
<p><strong>System-Specific Mechanisms</strong>:</p>
<ul>
<li>
<p><strong>Overpass Channels</strong>:</p>
<ul>
<li>Direct channel state transitions ensure trust minimization.</li>
<li>Integrated privacy reduces the reliance on trusted setups.</li>
</ul>
</li>
<li>
<p><strong>BitVM2</strong>:</p>
<ul>
<li>Requires an initial setup phase, adding a layer of trust for configuration integrity.</li>
<li>Sequential verification process may introduce dependencies on verification nodes.</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>In summary, both systems maintain Bitcoin’s trust model, but Overpass achieves a higher degree of trust minimization by avoiding setup requirements and emphasizing privacy-preserving operations.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-conclusion-31" name="p-51185-conclusion-31"></a>Conclusion</h2>
<p>This paper has provided a detailed comparative analysis of Overpass Channels and BitVM2 as Layer 2 solutions for Bitcoin, focusing on scalability, privacy, security, and economic neutrality. Through rigorous theorem-proof structures, we have demonstrated Overpass Channels’ unique advantages in privacy preservation, efficient liquidity utilization, and trust minimization, establishing it as a leading solution for scaling Bitcoin without altering its core protocol.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-summary-of-key-findings-32" name="p-51185-summary-of-key-findings-32"></a>Summary of Key Findings</h3>
<p>Overpass Channels emerge as a compelling choice for high-volume, privacy-preserving transactions on Bitcoin, offering the following distinct advantages:</p>
<ul>
<li><strong>Enhanced Privacy</strong>: Through integrated privacy-preserving state channels, Overpass ensures stronger privacy guarantees, minimizing information leakage compared to BitVM2.</li>
<li><strong>Scalability and Efficiency</strong>: Achieving <span class="math">O(n)</span> horizontal scaling with minimal verification overhead, Overpass efficiently supports high transaction throughput, whereas BitVM2 incurs higher verification and setup costs.</li>
<li><strong>Economic Neutrality and Stability</strong>: Closely aligned with Bitcoin’s fee market structure, Overpass preserves Bitcoin’s economic neutrality without introducing additional cost burdens.</li>
<li><strong>Trustless Design</strong>: Overpass Channels eliminate the need for trusted setups and emphasize zk-SNARK-based verification, achieving stronger trust minimization than BitVM2’s setup-dependent model.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-overpass-channels-as-the-cash-layer-for-layer-1-blockchains-33" name="p-51185-overpass-channels-as-the-cash-layer-for-layer-1-blockchains-33"></a>Overpass Channels as the Cash Layer for Layer 1 Blockchains</h3>
<p>While Bitcoin serves as an optimal reserve asset and “gold layer” of a decentralized financial network, Overpass Channels have the potential to become the “cash layer” not only for Bitcoin but for any Layer 1 blockchain that integrates with its architecture. By extending Overpass Channels as a universal Layer 2 solution, any compatible blockchain can benefit from instant, privacy-preserving transactions with high scalability, thus providing a cash layer capable of supporting everyday transactional demands across various blockchain ecosystems.</p>
<p>This analysis specifically highlights Overpass Channels in the context of Bitcoin as an extension of the original Overpass Channels research. However, the modular design of Overpass allows seamless integration with multiple blockchains, enhancing each one with Overpass’s advanced privacy and scalability benefits. This interoperability offers a transformative vision: a decentralized, multi-chain economy where Bitcoin and Overpass work symbiotically, with Bitcoin as the global reserve and Overpass as the universal, privacy-preserving cash layer.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-future-directions-34" name="p-51185-future-directions-34"></a>Future Directions</h3>
<p>Several areas of future research and development can help realize the full potential of Overpass Channels across multiple blockchain networks:</p>
<ol>
<li><strong>zk-SNARK Optimization</strong>: Further research into zk-SNARK efficiency can reduce computational overhead, making verification faster and more accessible across diverse Layer 1 blockchains.</li>
<li><strong>Expanding Integration Capabilities</strong>: Developing tools and protocols for seamless Overpass integration with other blockchains will extend its applicability as a cash layer beyond Bitcoin.</li>
<li><strong>Real-world Deployment and Audits</strong>: Comprehensive security audits and real-world testing will validate Overpass’s privacy and scalability claims, ensuring robust performance across different blockchain networks.</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-final-remarks-35" name="p-51185-final-remarks-35"></a>Final Remarks</h3>
<p>In conclusion, Overpass Channels represent a groundbreaking Layer 2 solution that enhances the scalability and privacy of Bitcoin and has the potential to serve as a universal cash layer across various Layer 1 blockchains. By offering a scalable, privacy-focused transaction layer, Overpass can redefine the usability and accessibility of decentralized finance. This cash layer for the Internet enables a flexible, interoperable financial system that respects user privacy and decentralization principles, positioning Bitcoin and Overpass as essential building blocks in the future of a decentralized global economy.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-references-36" name="p-51185-references-36"></a>References</h3>
<ol>
<li>
<p>Ramsay, B., “Overpass Channels: Horizontally Scalable, Privacy-Enhanced, with Independent Verification, Fluid Liquidity, and Robust Censorship Proof Payments,” Cryptology ePrint Archive, Paper 2024/1526, 2024.</p>
</li>
<li>
<p>Linus, R., Aumayr, L., Zamyatin, A., Pelosi, A., Avarikioti, Z., Maffei, M., “BitVM2: Bridging Bitcoin to Second Layers,” presented by ZeroSync, TU Wien, BOB, University of Pisa, University of Camerino, and Common Prefix, 2024.</p>
</li>
<li>
<p>Nakamoto, S., “Bitcoin: A Peer-to-Peer Electronic Cash System,” <a href="http://Bitcoin.org" rel="noopener nofollow ugc">Bitcoin.org</a>, 2008.</p>
</li>
</ol>
            <p><small>2 posts - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/bitcoin-a-peer-to-peer-electronic-cash-system-in-part-by-overpass-channels/20987">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 14 Nov 2024 23:54:37 +0000</pubDate>
</item>
<item>
<title>Concerning Ethereum's Fee Issues</title>
<link>https://ethresear.ch/t/concerning-ethereums-fee-issues/21070</link>
<guid>https://ethresear.ch/t/concerning-ethereums-fee-issues/21070</guid>
<content:encoded><![CDATA[
<div> 关键词: Ethereum、gas费用、智能合约、交易费用、简化交换应用

总结:
在考虑以太坊交易中的gas成本（交易费）时，发现单纯发送交易的成本相对较低，约为1美元以下。然而，执行涉及更多操作的智能合约任务时，费用会变得更高。例如，通过自动化做市商如Uniswap进行代币交换，每次合同调用的成本大约为16美元。文章提出了一种简化交换应用的概念，该应用仅包含PEPE和USDC两种代币，定期从中心化交易所推送PEPE/USD汇率，并基于存储的汇率进行代币交换，无需订单簿、市场制造、多维交换、存款或取款功能。这样的简单智能合约预计可以降低计算成本并减少交易费用。

由此带来的优势可能包括：降低成本敏感用户的交易费用、更易于实现以及降低被攻击的风险和合同风险。问题在于这种简约智能合约的方法是否已被探索以及它能多大程度上解决交易费用问题。

a) 简约智能合约的方法是否已经被探索？
b) 这种方法能否帮助解决交易费用问题？
c) 如果可以解决问题，那么费用可以减少多少？ <div>
<p>I have been looking at the Ethereum prices for gas costs (transaction fees?).</p>
<p>Gas tracking resources  — <a href="https://etherscan.io/gastracker" rel="noopener nofollow ugc">https://etherscan.io/gastracker</a></p>
<p>I noticed that simply sending a transaction is actually decently cheap, about under $1.</p>
<p>What’s seems to be costly for making transactions is <a href="https://etherscan.io/gastracker#gasguzzler" rel="noopener nofollow ugc">smart contract calls</a>.</p>
<p>My assumption is that, the more operations a smart contract runs to execute its task the costlier it’s is in terms of fees.</p>
<p>It is said that swapping, costs about $16 per contract call.</p>
<p>I am assuming that this is expensive because the swap is probably made via an automated market maker like uniswap. (i.e lots of code to run)</p>
<p>Here is my idea.</p>
<p>What if I am to strip down the concept of swapping a coin in to a very simple smart contract.</p>
<p>By simple, I mean;</p>
<ul>
<li>No order books</li>
<li>No market making</li>
<li>Higher dimensional swaps (ability to swap any combination of assets)</li>
<li>No Deposits</li>
<li>No Withdraws</li>
</ul>
<p>How the basic swap application would work. (Let’s say a PEPE/USDC swap application)</p>
<ul>
<li>Create a contract that only holds PEPE &amp; USDC</li>
<li>Periodically push exchange rates between PEPE/USD to contract from centralised exchange</li>
<li>When users send USDC to swap contact it simply uses the stored exchange rate to send an equivalent amount of PEPE</li>
<li>When users send an an amount of PEPE to the swap contract it also uses the stored exchange rate to send back a proportional amount of USDC to the sender.</li>
<li>Has basic error handling (when not enough USDC or PEPE to swap, unsupported coins etc)</li>
</ul>
<p>I am assuming that these stripped down smart contracts would cost way less since there would be less compute.</p>
<p>Why may this be useful.</p>
<ol>
<li>
<p>Cheaper transaction costs for fee sensitive users.</p>
</li>
<li>
<p>Easier to implement</p>
</li>
<li>
<p>Lower surface of attack and hence, reduced contract risk (hacking risk)</p>
</li>
</ol>
<p>My questions for this forum.</p>
<p>a) Has this minimalist approach to smart contracts been explored?</p>
<p>b) Can it help in solving fee issues?</p>
<p>c) If it can help, by how much can fees be reduced?</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/concerning-ethereums-fee-issues/21070">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sat, 23 Nov 2024 19:13:40 +0000</pubDate>
</item>
<item>
<title>RFC: Using this.balance as a Storage-Free Deactivation Mechanism Post-Cancun</title>
<link>https://ethresear.ch/t/rfc-using-this-balance-as-a-storage-free-deactivation-mechanism-post-cancun/21056</link>
<guid>https://ethresear.ch/t/rfc-using-this-balance-as-a-storage-free-deactivation-mechanism-post-cancun/21056</guid>
<content:encoded><![CDATA[
<div> 关键词: RFC、Cancun升级、EIP-6780、合约激活/停用、this.balance

总结:
本文提出了一种利用内置变量this.balance作为存储无关的合约停用机制建议。在Cancun升级和EIP-6780改变后，动态停用和重新激活合约变得更加复杂，尤其是在不希望每次交易都查询存储的情况下。该提议针对一种路由合约，它持有用户授权但不持币，需要一种可靠的、无需查询或存储自定义状态变量的方式来切换活跃与非活跃状态。

具体实现逻辑为：当合约余额(this.balance)为0时，表示合约处于停用状态，拒绝所有交互；当合约余额为1wei时，则表示处于激活状态。合约的状态切换通过名为activate()和deactivate()的函数完成，这两个函数由一个不可变的DEACTIVATOR地址调用，分别向合约发送1wei以激活合约，或将合约内的ETH转移回DEACTIVATOR以停用合约。

此机制的特点包括：
1. 不读取存储：状态检查仅依赖于this.balance，避免了从存储中读取数据的gas成本。
2. 高效状态切换：激活或停用只需进行最小限度的ETH转账操作。
3. 安全性：合约没有receive函数，确保其无法意外或恶意地接收ETH。

这个方法提供了一种轻量级的处理路由器停用的方式，特别是在因合约出现漏洞导致用户需手动撤销授权，或者因gas成本和复杂度而不希望每个交易都查询状态的情况下。使用合约的ETH余额作为切换开关可以在无需存储变量的同时，实现快速而高效的态 <div>
<h3><a class="anchor" href="https://ethresear.ch#p-51335-rfc-using-thisbalance-as-a-storage-free-deactivation-mechanism-post-cancun-1" name="p-51335-rfc-using-thisbalance-as-a-storage-free-deactivation-mechanism-post-cancun-1"></a>RFC: Using <code>this.balance</code> as a Storage-Free Deactivation Mechanism Post-Cancun</h3>
<p>After the Cancun upgrade and the changes introduced by EIP-6780, the ability to deactivate and reactivate a contract dynamically has become more challenging, particularly in scenarios where querying storage for every transaction is not desirable. I am proposing a minimalistic mechanism for activating and deactivating a contract (specifically a router contract) using the built-in <code>this.balance</code> variable as a control flag.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51335-use-case-2" name="p-51335-use-case-2"></a>Use Case</h3>
<p>The router contract in question:</p>
<ul>
<li><strong>Holds user approvals</strong> but no tokens or significant business logic.</li>
<li>Does <strong>not hold ETH</strong> in its normal operation.</li>
<li>Needs a reliable, storage-free way to toggle between an <strong>active</strong> and <strong>inactive</strong> state.</li>
</ul>
<p>This is particularly useful in cases where:</p>
<ul>
<li>A bug is identified in the contract, and users need to be prevented from interacting with it until it is fixed.</li>
<li>There’s a need to deactivate the router swiftly without requiring manual revocation of user approvals.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51335-proposed-mechanism-3" name="p-51335-proposed-mechanism-3"></a>Proposed Mechanism</h3>
<p>The mechanism relies solely on the contract’s ETH balance (<code>this.balance</code>) to toggle its state. The logic is as follows:</p>
<ol>
<li><strong>State Definition:</strong></li>
</ol>
<ul>
<li><strong>Deactivated:</strong> <code>this.balance == 0</code>. The contract rejects all interactions.</li>
<li><strong>Activated:</strong> <code>this.balance == 1 wei</code>. The contract functions normally.</li>
</ul>
<ol start="2">
<li><strong>State Transitions:</strong></li>
</ol>
<ul>
<li><strong>Activate the Contract:</strong> The <code>activate()</code> function (restricted to an immutable DEACTIVATOR address) sends 1 wei to the contract, setting it to an active state.</li>
<li><strong>Deactivate the Contract:</strong> The <code>deactivate()</code> function (also restricted) transfers all ETH in the contract back to the DEACTIVATOR, setting the balance to zero.</li>
</ul>
<ol start="3">
<li><strong>Key Features:</strong></li>
</ol>
<ul>
<li>The router has <strong>no <code>receive</code> function</strong>, ensuring that ETH cannot be accidentally or maliciously sent to it.</li>
<li>All state transitions rely exclusively on the ETH balance, eliminating the need to query or store custom state variables.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51335-benefits-4" name="p-51335-benefits-4"></a>Benefits</h3>
<ol>
<li><strong>No Storage Reads:</strong> State checking relies on <code>this.balance</code>, avoiding the gas cost of reading from storage.</li>
<li><strong>Efficient State Transitions:</strong> Activating or deactivating involves only minimal ETH transfers.</li>
<li><strong>Safety:</strong> The absence of a <code>receive</code> function ensures the router cannot accidentally accumulate ETH.</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-51335-why-this-is-useful-5" name="p-51335-why-this-is-useful-5"></a>Why This Is Useful</h3>
<p>This mechanism provides a lightweight way to handle router deactivation, especially in scenarios where:</p>
<ul>
<li>Bugs in router contracts force users to manually revoke approvals.</li>
<li>State querying for every transaction is undesirable due to gas costs or complexity.</li>
</ul>
<p>Using the contract’s ETH balance as a toggle avoids the need for storage variables while enabling quick and efficient state changes. This approach could benefit other contract designs requiring similar activation/deactivation mechanisms in a storage-free context.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/rfc-using-this-balance-as-a-storage-free-deactivation-mechanism-post-cancun/21056">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 21 Nov 2024 09:26:39 +0000</pubDate>
</item>
<item>
<title>Transport privacy exploration of the Validator-Relayer Builder API</title>
<link>https://ethresear.ch/t/transport-privacy-exploration-of-the-validator-relayer-builder-api/21050</link>
<guid>https://ethresear.ch/t/transport-privacy-exploration-of-the-validator-relayer-builder-api/21050</guid>
<content:encoded><![CDATA[
<div> 关键词: MEV、Proposer-Builder Separation (PBS)、隐私泄露、网络攻击、验证器

总结:
本文探讨了以太坊生态系统中Proposer-Builder Separation (PBS)环境下，由于运输层元数据泄露导致的隐私风险。研究通过名为“Metaclear”的实验展示了如何利用这种漏洞进行针对性干扰，阻碍区块生产并可能实现不属于传统MEV定义的价值提取。研究重点在于验证器与中继之间的接口处的隐私泄露，并通过修改MEV-Boost中继的源代码收集相关HTTP调用的元数据。实验结果显示，一旦验证器被去匿名化，它们就容易受到旨在干扰未来区块提议的网络攻击。

具体来说，文章指出了以下几个要点：

1. 在PBS环境中，验证器和区块构造者的元数据可通过HTTP调用暴露给中继，使恶意参与者能够识别并针对这些节点发起网络攻击。
2. 当前最广泛应用的MEV-Boost中继存在潜在的安全隐患，其设计并未充分保护验证器的元数据隐私。
3. 实验通过模拟网络攻击，如TCP SYN洪水攻击，证实了针对验证器的攻击可以成功地阻止它们提出区块，从而影响交易处理并可能导致价值流失。
4. 文章进一步提出了其他可能的攻击场景，包括利用泄露的元数据对验证器、区块构建者以及中继自身进行更复杂的攻击策略，甚至可能操纵RANDAO生成以控制区块提议权。
5. 文章呼吁在设计PBS规范时应考虑集成网络级元数据隐私协议，并扩展MEV考量范围至包括运输层元数据在内的更广泛领域。

综上所述，“Metaclear”实验揭示了在PBS环境中运输层隐私泄露带来的实际风险，并提出了新的MEV提取可能性。为保护网络安全与公平性，需要加强区块链网络在运输层的数据隐私保护措施。 <div>
<h1><a class="anchor" href="https://ethresear.ch#p-51319-transport-privacy-exploration-of-the-validator-relayer-builder-api-1" name="p-51319-transport-privacy-exploration-of-the-validator-relayer-builder-api-1"></a>Transport privacy exploration of the Validator-Relayer Builder API</h1>
<p><em>Special thanks to <a class="mention" href="https://ethresear.ch/u/nero_eth">@Nero_eth</a> and <a class="mention" href="https://ethresear.ch/u/liobaheimbach">@liobaheimbach</a></em></p>
<h2><a class="anchor" href="https://ethresear.ch#p-51319-abstract-2" name="p-51319-abstract-2"></a>Abstract</h2>
<p>The availability of metadata from the networking layer of Ethereum, particularly in Proposer-Builder Separation (PBS)-enabled environments poses real and immediate privacy risks. Among other concerns, it allows and incentivizes adversaries to target and interfere with block production processes to prevent certain transactions or even entire blocks from being processed. Our experiment, “Metaclear,” investigates how transport-level privacy leaks at validator-relay interfaces allow targeted disruption via metadata analysis and low-cost network-layer attacks. We implemented a transport metadata harvesting pipeline of the full MEV architecture stack to link validators’ public keys to the IP address of their consensus client as well as that of the mev-boost software and executed attacks in a lab setting to demonstrate the practical applications of this work. We found that once de-anonymized, block proposers became vulnerable to network attacks aimed at interfering with future block proposals. We also identified scenarios where different parties besides proposers can be attacked when transport privacy is unprotected. Through this experiment, we want to i) challenge the trust assumptions in MEV infrastructure such as relayers, ii) advocate researchers and engineers to integrate network-level metadata privacy protocols into the design of enshrined PBS, and iii) expand the scope of MEV considerations to include transport-layer metadata.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51319-table-of-contents-3" name="p-51319-table-of-contents-3"></a>Table of contents</h2>
<p><a href="https://ethresear.ch#p-51319-abstract-2">Abstract</a><br />
<a href="https://ethresear.ch#h-1-introduction-4">1. Introduction</a><br />
<a href="https://ethresear.ch#p-51319-h-11-mev-relay-in-pbs-5">1.1. MEV Relay in PBS</a><br />
<a href="https://ethresear.ch#p-51319-h-111-definition-of-terms-6">1.1.1 Definition of terms</a><br />
<a href="https://ethresear.ch#p-51319-h-12-privacy-issues-in-relays-7">1.2. Privacy issues in Relays</a><br />
<a href="https://ethresear.ch#p-51319-h-13-randomness-generation-by-validators-8">1.3. Randomness generation by validators</a><br />
<a href="https://ethresear.ch#p-51319-h-14-contributions-9">1.4. Contributions</a><br />
<a href="https://ethresear.ch#p-51319-h-2-methodology-10">2. Methodology</a><br />
<a href="https://ethresear.ch#p-51319-h-21-metadata-collection-in-relay-http-calls-11">2.1. Metadata collection in Relay HTTP calls</a><br />
<a href="https://ethresear.ch#p-51319-h-22-attestation-data-collection-12">2.2. Attestation data collection</a><br />
<a href="https://ethresear.ch#p-51319-h-23-de-anonymization-13">2.3. De-anonymization</a><br />
<a href="https://ethresear.ch#p-51319-h-24-simulation-of-network-attacks-14">2.4. Simulation of network attacks</a><br />
<a href="https://ethresear.ch#p-51319-h-3-setup-and-environment-15">3. Setup and environment</a><br />
<a href="https://ethresear.ch#p-51319-h-31-architecture-16">3.1. Architecture</a><br />
<a href="https://ethresear.ch#p-51319-h-32-environment-and-test-parameters-17">3.2. Environment and test parameters</a><br />
<a href="https://ethresear.ch#p-51319-h-4-results-18">4. Results</a><br />
<a href="https://ethresear.ch#p-51319-h-41-linking-public-key-and-ip-addresses-of-validators-19">4.1. Linking public key and IP addresses of validators</a><br />
<a href="https://ethresear.ch#p-51319-h-42-identifying-victim-validators-20">4.2. Identifying victim validators</a><br />
<a href="https://ethresear.ch#p-51319-h-43-viable-network-attacks-21">4.3. Viable network attacks</a><br />
<a href="https://ethresear.ch#p-51319-h-5-discussion-further-attack-scenarios-and-network-consequences-22">5. Discussion: Further Attack scenarios and Network Consequences</a><br />
<a href="https://ethresear.ch#p-51319-h-51-network-topology-can-be-mapped-using-relay-metadata-23">5.1. Network topology can be mapped using Relay metadata</a><br />
<a href="https://ethresear.ch#p-51319-h-52-solo-stakers-are-more-vulnerable-to-attack-24">5.2. Solo stakers are more vulnerable to attack</a><br />
<a href="https://ethresear.ch#p-51319-h-53-randao-can-be-exploited-and-manipulated-25">5.3. RANDAO can be exploited and manipulated</a><br />
<a href="https://ethresear.ch#p-51319-h-54-blobs-can-be-disrupted-26">5.4. Blobs can be disrupted</a><br />
<a href="https://ethresear.ch#p-51319-h-55-recently-bootstrapped-sparsely-connected-beacon-nodes-are-more-vulnerable-to-covert-flash-attacks-27">5.5. Recently bootstrapped, sparsely connected beacon nodes are more vulnerable to “covert flash attacks”</a><br />
<a href="https://ethresear.ch#p-51319-h-56-selectively-attack-multi-block-mev-28">5.6. Selectively attack multi-block MEV</a><br />
<a href="https://ethresear.ch#p-51319-h-57-malicious-relay-can-more-easily-conduct-a-metadata-based-attack-without-leaving-traces-29">5.7. Malicious Relay can more easily conduct a metadata-based attack without leaving traces</a><br />
<a href="https://ethresear.ch#p-51319-h-58-decentralizing-relays-makes-attacks-more-likely-not-less-30">5.8. Decentralizing Relays makes attacks more likely, not less</a><br />
<a href="https://ethresear.ch#p-51319-h-59-valuable-metadata-can-make-relay-a-victim-31">5.9. Valuable metadata can make Relay a victim</a><br />
<a href="https://ethresear.ch#p-51319-h-510-builders-can-be-made-to-underperform-32">5.10. Builders can be made to underperform</a><br />
<a href="https://ethresear.ch#p-51319-h-511-a-new-class-of-mev-33">5.11. A new class of MEV?</a><br />
<a href="https://ethresear.ch#p-51319-h-6-limitations-34">6. Limitations</a><br />
<a href="https://ethresear.ch#p-51319-h-7-future-research-35">7. Future research</a><br />
<a href="https://ethresear.ch#p-51319-bibliography-36">Bibliography</a></p>
<h2><a class="anchor" href="https://ethresear.ch#p-51319-h-1-introduction-4" name="p-51319-h-1-introduction-4"></a>1. Introduction</h2>
<p>One standard definition of Maximal Extractable Value (MEV) defines MEV as “the maximum value that can be extracted from block production […] by including, excluding, and changing the order of transactions in a block <a href="https://ethereum.org/en/developers/docs/mev/" rel="noopener nofollow ugc" title="Maximal extractable value (MEV) | ethereum.org.">[1]</a>.” However, these are not the only actions which can be taken by network participants to disrupt or extract value. Indeed, the current focus on mempool data – comprising pending transactions <a href="https://assets.ey.com/content/dam/ey-sites/ey-com/en_us/topics/financial-services/ey-an-introduction-to-maximal-extractable-value-on-ethereum.pdf" rel="noopener nofollow ugc" title="G. Damalas and P. Ambrus, “An introduction to maximal extractable value on Ethereum,” Mar. 2023">[2]</a> – provides a limited perspective and overlooks other potentially valuable sources of data that could be exploited for extracting value.</p>
<p>One such underexplored source is the data collected from the networking layer that underpins blockchain systems. For a distributed network to function properly, various subprotocols coordinate states among nodes, such as the broadcasting of mempool transactions and the gossiping of attestations. We <a href="https://medium.com/hoprnet/proof-of-stake-validator-sniping-research-8670c4a88a1c" rel="noopener nofollow ugc" title="S. Bürgel and L. Pohanka, “Proof-of-Stake Validator Sniping Research,” HOPR.">[3]</a> and other researchers <a href="https://doi.org/10.48550/arXiv.2409.04366" rel="noopener nofollow ugc" title="L. Heimbach, Y. Vonlanthen, J. Villacis, L. Kiffer, and R. Wattenhofer, “Deanonymizing Ethereum Validators: The P2P Network Has a Privacy Issue”">[4]</a> have previously shown how data harvesting from these low-level communication protocols and collecting relevant metadata can be a source of valuable information, for example by linking validator public keys to their IP address. Once parties involved in block production have been de-anonymized, adversaries can target them, interfering with their block production processes and potentially excluding expected transactions from a block or entire blocks altogether.</p>
<p>This risk becomes even more pronounced in a Proposer-Builder Separation (PBS) - enabled environment, where the introduction of builders – responsible for constructing blocks – adds complexity to the communication protocol. When validators connect to a block-building service that contains a trusted party in the middle to moderate block delivery, the trusted party processes privileged information including validator metadata. When validators connect to a decentralized network of builders, their metadata is exposed to more parties. With more metadata to observe, the potential for privacy leakage increases, creating opportunities for MEV disruption and perhaps extraction. However, the potential for MEV extraction in PBS has not been thoroughly explored <a href="https://ethereum.org/en/roadmap/pbs/" rel="noopener nofollow ugc" title="Ethereum Foundation, “Ethereum Roadmap: PBS and MEV,” ethereum.org.">[5]</a>.</p>
<p>This experiment, named “Metaclear”, seeks to assess the feasibility of these attacks by examining transport-level privacy in the context of PBS in a practical manner and in a controlled lab environment, with a specific focus on privacy leaks at the interfaces between validators and relays. “Metaclear” reveals how metadata can de-anonymize stakeholders involved in block creation and demonstrates potential methods for exploiting these vulnerabilities to disrupt block production and extract additional value which falls outside the traditional definition of MEV.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-11-mev-relay-in-pbs-5" name="p-51319-h-11-mev-relay-in-pbs-5"></a>1.1. MEV Relay in PBS</h3>
<p>The PBS scheme was introduced as part of Ethereum’s transition to Proof-of-Stake (PoS), aiming to bifurcate block production into two distinct roles: block builders, responsible for collecting and ordering transactions, and proposers, tasked with proposing new validated blocks. While the design and specifications have been discussed since 2018 <a href="https://ethresear.ch/t/proposer-block-builder-separation-friendly-fee-market-designs/9725" title="V. Buterin, “Proposer/block builder separation-friendly fee market designs - Economics,” Ethereum Research.">[6]</a>, consensus on the design of enshrined PBS (ePBS) or in-protocol PBS (IP-PBS) has not yet been reached. Various proposals for ePBS, such as two-slot PBS <a href="https://ethresear.ch/t/two-slot-proposer-builder-separation/10980" title="V. Buterin, “Two-slot proposer/builder separation - Proof-of-Stake,” Ethereum Research.">[7]</a>, single-slot PBS <a href="https://ethresear.ch/t/single-slot-pbs-using-attesters-as-distributed-availability-oracle/11877" title="V. Buterin, “Single-slot PBS using attesters as distributed availability oracle - Proof-of-Stake,” Ethereum Research.">[8]</a>, Protocol-Enforced Proposer Commitment (PEPC) <a href="https://ethresear.ch/t/unbundling-pbs-towards-protocol-enforced-proposer-commitments-pepc/13879" title="B. Monnot, “Unbundling PBS: Towards protocol-enforced proposer commitments (PEPC) - Economics,” Ethereum Research.">[9]</a>, and Payload-timeliness committee (PTC) <a href="https://ethresear.ch/t/payload-timeliness-committee-ptc-an-epbs-design/16054" title="M. Neuder, “Payload-timeliness committee (PTC) – an ePBS design - Proof-of-Stake,” Ethereum Research.">[10]</a>, emphasize different assumptions about the desired degree of decentralization, economic incentives, and appropriate positioning on the spectrum of censorship resistance.</p>
<p>Currently, the most widely adopted implementation by block proposers, based on slot share <a href="https://mevboost.pics/mevboost.pics" rel="noopener nofollow ugc" title="T. Wahrstätter, “MEV-Boost Dashboard,” mevboost.pics.">[11]</a>, is an out-of-protocol PBS called MEV-Boost. MEV-Boost is a market-driven block-building platform developed by Flashbots <a href="https://boost.flashbots.net/" rel="noopener nofollow ugc" title="Flashbots Ltd, “MEV-Boost in a Nutshell,” MEV-Boost in a Nutshell.">[12]</a>. In a nutshell, MEV-Boost requires at least one trusted centralized middleman called “Relay”, which sits in between block builders and Ethereum validators and which prevents validators from extracting value from builders and also prevents builders from withholding information from validators. Validators run an instance of MEV-Boost alongside their client software so that when they are selected as a block proposer, MEV-Boost requests block-building information from the Relay. Block contents are fed by block builders at the beginning of a slot. The Relay implements the latest Builder API specification <a href="https://ethereum.github.io/builder-specs/" rel="noopener nofollow ugc" title="Ethereum Foundation, “Builder-API.”">[13]</a>.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/0/2006f08ea6e79bd49ec7ec94bb2e09928889e963.png" title="Figure 1 Architecture of MEV-Boost, an implementation of PBS by Flashbots"><img alt="Figure 1 Architecture of MEV-Boost, an implementation of PBS by Flashbots" height="188" src="https://ethresear.ch/uploads/default/optimized/3X/2/0/2006f08ea6e79bd49ec7ec94bb2e09928889e963_2_690x188.png" title="Figure 1 Architecture of MEV-Boost, an implementation of PBS by Flashbots" width="690" /></a></div><br />
<em><strong>Figure 1</strong> Architecture of MEV-Boost, an implementation of PBS by Flashbots <a href="https://github.com/flashbots/mev-boost" rel="noopener nofollow ugc" title="*Flashbot/MEV-Boost*. Flashbots.">[14]</a></em><p></p>
<h4><a class="anchor" href="https://ethresear.ch#p-51319-h-111-definition-of-terms-6" name="p-51319-h-111-definition-of-terms-6"></a>1.1.1 Definition of terms</h4>
<p>The terms “proposer” and “builder” can carry different meanings depending on the context in which they are discussed.</p>
<p>In the Ethereum Proof of Stake context, the term “proposer” refers to one of the main duties of validators. When a validator is randomly chosen for a specific slot, their responsibility is to propose a new block containing transactions for that slot. The term “builder” refers to the block builders who are responsible for creating blocks of transactions and offering them to the block proposer for each slot <a href="https://ethereum.org/en/roadmap/pbs/" rel="noopener nofollow ugc" title="Ethereum Foundation, “Ethereum Roadmap: PBS and MEV,” ethereum.org.">[5]</a>. For example, MEV-Boost (from the mev-boost instance running along the beacon client to the entire marketplace with block builders and searchers) is considered a “builder” within PBS implementations <a href="https://github.com/flashbots/mev-boost" rel="noopener nofollow ugc" title="*Flashbot/MEV-Boost*. Flashbots.">[14]</a>.</p>
<p>In the context of MEV-Boost as one PBS implementation with a block-building marketplace, “MEV Boost” has three major components (see Figure 1 for its architecture): Builder, Relay, and mev-boost. Here, “Builder” refers to block builders who provide the ordered payload of a full block considering MEV extraction and reward distribution. “mev-boost” in blue in Figure 1 refers to the narrow sense of mev-boost, a sidecar middleware run by validators, which provides access to the “MEV-Boost” marketplace.</p>
<p>When considering the “Relay” component of MEV-Boost, “builder” has the same meaning as in the MEV-Boost context, referring to block builders external to MEV-Boost. Here, builders call the Relay HTTP API <a href="https://flashbots.github.io/relay-specs/" rel="noopener nofollow ugc" title="Flashbots Ltd, Relay-API.">[15]</a>. “Proposers” are validators with mev-boost middleware. Relays implement the Ethereum Builder API specs <a href="https://ethereum.github.io/builder-specs/" rel="noopener nofollow ugc" title="Ethereum Foundation, Builder-API.">[13]</a> to handle API calls from mev-boost middleware.</p>
<p>Since our research is focused on the architecture of MEV-Boost, for clarity we will use the same terminology as defined for the Relay component.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-12-privacy-issues-in-relays-7" name="p-51319-h-12-privacy-issues-in-relays-7"></a>1.2. Privacy issues in Relays</h3>
<p>Relays play a special role in current PBS, because they protect searchers and builders from proposers who may try to steal the value of bundles and blocks via, e.g., unbundling. Relays also provide validity checks on transactions with an EVM execution client <a href="https://ethresear.ch/t/relays-in-a-post-epbs-world/16278" title="M. Neuder, “Relays in a post-ePBS world - Proof-of-Stake,” Ethereum Research.">[16]</a>. However, the centralized and intermediary nature of Relays requires both validators and builders to trust Relays in two different ways: first, that the data delivered by the Relays is accurate, and second that the Relays themselves will not actively attack the validators or builders. Indeed, facets of Relay design such as payload sharing and value spoofing <a href="https://collective.flashbots.net/t/post-mortem-for-a-relay-vulnerability-leading-to-proposers-falling-back-to-local-block-production-nov-10-2022/727" rel="noopener nofollow ugc" title="C. Hager, “Post-mortem for a relay vulnerability leading to proposers falling back to local block production (Nov. 10, 2022) - Relays,” Nov. 2022.">[17]</a> have been exploited in the past <a href="https://github.com/flashbots/mev-boost/blob/4035cb3c8c8f9b0118a0170049203f0167c604a0/docs/audit-20220620.md" rel="noopener nofollow ugc" title="lotusbumi, “MEV-Boost Security Assessment (audit),” Jun. 2022">[18]</a>.</p>
<p>These trust assumptions are well known, and countermeasures are already implemented such as signatures and mechanisms against known unbundling attacks which would prevent relays from allowing arbitrary insertion or deletion of transactions from a bundle [<a href="https://collective.flashbots.net/t/post-mortem-april-3rd-2023-mev-boost-relay-incident-and-related-timing-issue/1540" rel="noopener nofollow ugc" title="R. Miller, “Post mortem: April 3rd, 2023 mev-boost relay incident and related timing issue - The Flashbots Ship,” Apr. 2023">19</a>, <a href="https://blog.sigmaprime.io/mev-unbundling-rpc.html" rel="noopener nofollow ugc" title="M. Sproul, “Unbundling attacks on MEV relays using RPC”">20</a>]. However, the true extent of the trust assumptions is much broader, and less explored. Relays possess the advantage of collecting metadata of validators and builders. Metadata leaks via HTTP calls can reveal information about nodes’ IP address and location within the networking layer [<a href="https://doi.org/10.1007/11767831_1" rel="noopener nofollow ugc" title="G. D. Bissias, M. Liberatore, D. Jensen, and B. N. Levine, “Privacy Vulnerabilities in Encrypted HTTP Streams,” in *Privacy Enhancing Technologies*, G. Danezis and D. Martin, Eds., Berlin, Heidelberg: Springer, 2006, pp. 1–11. doi: 10.1007/11767831_1.">21</a>, <a href="https://doi.org/10.1145/384268.378789" rel="noopener nofollow ugc" title="F. D. Smith, F. H. Campos, K. Jeffay, and D. Ott, “What TCP/IP Protocol Headers Can Tell Us About the Web,” *ACM SIGMETRICS Perform. Eval. Rev.*, vol. 29, no. 1, pp. 245–256, Jun. 2001, doi: 384268.378789.">22</a>]. By correlating data collected on the consensus layer and more, nodes can be identified with relative ease and thus become a target of attacks on the networking layer. Proposals such as Block Negotiation Layer (BNL) <a href="https://ethresear.ch/t/realigning-block-building-incentives-and-responsibilities-block-negotiation-layer/16666" title="Ł. Miłkowski, “Realigning block building incentives and responsibilities - Block Negotiation Layer - Proof-of-Stake / Block proposer,” Ethereum Research.">[23]</a> intend to improve network security by modifying the Relay mechanism with the proposer’s intent. However, despite the strong demand for protecting validator privacy and even an explicit desire to separate on-chain addresses from IP addresses <a href="https://ethresear.ch/t/mev-boost-merge-ready-flashbots-architecture/11177" title="S. Gosselin, “MEV-Boost: Merge ready Flashbots Architecture - The Merge,” Ethereum Research.">[24]</a>, the issue of metadata privacy leaks at the transport layer remains unaddressed, leaving a significant gap in proposer security. In fact, the original MEV Boost architecture explicitly mentioned that “communication between MEV-Boost and relays […] must protect validator privacy by not associating validator key and IP address” <a href="https://ethresear.ch/t/mev-boost-merge-ready-flashbots-architecture/11177" title="S. Gosselin, “MEV-Boost: Merge ready Flashbots Architecture - The Merge,” Ethereum Research.">[24]</a>. Yet, no technical solutions were implemented to protect validator privacy.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-13-randomness-generation-by-validators-8" name="p-51319-h-13-randomness-generation-by-validators-8"></a>1.3. Randomness generation by validators</h3>
<p>Randomness plays an important role in selecting block proposers in Ethereum <a href="https://eth2book.info/capella/part2/building_blocks/randomness/" rel="noopener nofollow ugc" title="B. Edgington, *Upgrading Ethereum | 2.9.2 Randomness*. 2023">[25]</a>. The RANDAO value is designed to be unpredictable to the other validators as other validators cannot predict a proposer’s entropy contribution without knowing their private keys. However, a proposer’s contribution to the randomness calculation can be influenced by a malicious proposer choosing between broadcasting or withholding a block <a href="http://arxiv.org/abs/2409.19883" rel="noopener nofollow ugc" title="K. Alpturer and S. M. Weinberg, “Optimal RANDAO Manipulation in Ethereum,” Sep. 29, 2024, *arXiv*: arXiv:2409.19883.">[26]</a>, or by accidentally missed proposals due to network conditions <a href="https://eips.ethereum.org/EIPS/eip-4399" rel="noopener nofollow ugc" title="M. Kalinin and D. Ryan, “EIP-4399: Supplant DIFFICULTY opcode with PREVRANDAO,” Ethereum Improvement Proposals.">[27]</a>.</p>
<p>But there is a third scenario we would like to highlight: block proposers can be purposefully disrupted by third parties to ensure they miss their proposal slot. This scenario is feasible thanks to the transport-layer privacy leaks outlined above.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-14-contributions-9" name="p-51319-h-14-contributions-9"></a>1.4. Contributions</h3>
<p>In our work we i) showcase the feasibility of metadata collection by privileged Relays in a sandbox environment, ii) identify the true trust assumptions surrounding Relays and demonstrate how Relays can de-anonymize validators and conduct attacks without leaving traces, iii) construct a sandbox environment to conduct attacks on the network layer and demonstrate the negative consequences of such attacks on block production, iv) outline attack scenarios on other components, v) discuss possible ways to target these attacks to extract value, as a potential new class of MEV and vi) suggest changes in protocol to prevent and mitigate the effects of these metadata leaks, particularly on solo stakers.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51319-h-2-methodology-10" name="p-51319-h-2-methodology-10"></a>2. Methodology</h2>
<p>In order to gather metadata from the HTTP calls utilized in MEV-Boost-Relay, we first identified all relevant HTTP calls used by the Relay. After that, we made changes to the Relay source code to allow for metadata collection. We then set up a sandbox Ethereum testnet using the Kurtosis “ethereum-package” from EthPandaOps <a href="https://ethpandaops.io/posts/kurtosis-deep-dive/" rel="noopener nofollow ugc" title="B. Busa and P. Jayanthi, “Kurtosis: A Deep Dive to Local Devnets.”">[28]</a> to test the new setup. The sandbox environment<sup class="footnote-ref"><a href="https://ethresear.ch#footnote-51319-1" id="footnote-ref-51319-1">[1]</a></sup> includes services such as consensus clients, execution clients, validators, MEV flood simulator, transaction and blob spammer, beacon metrics gazer, beacon explorer, networking tools and containers, and Grafana dashboards. Using the metadata collected from the HTTP calls, as well as additional data about validators, we compiled a dashboard to reveal the identities of validators and to highlight important information that was subsequently used to conduct network attacks within the sandbox environment.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-21-metadata-collection-in-relay-http-calls-11" name="p-51319-h-21-metadata-collection-in-relay-http-calls-11"></a>2.1. Metadata collection in Relay HTTP calls</h3>
<p>Focusing on the most popular MEV-Boost-Relay implementation<sup class="footnote-ref"><a href="https://ethresear.ch#footnote-51319-2" id="footnote-ref-51319-2">[2]</a></sup>, validators and block builders communicate with Relays via RESTful HTTP APIs <a href="https://flashbots.notion.site/Relay-API-Documentation-5fb0819366954962bc02e81cb33840f5#854339c909a042d0bbca6e8f8069674e" rel="noopener nofollow ugc" title="Flashbots Ltd, “Relay API Documentation.”">[29]</a>. Specifically, MEV-Boost communicates with the Builder API (Proposer API) [<a href="https://ethereum.github.io/builder-specs/" rel="noopener nofollow ugc" title="Ethereum Foundation, “Builder-API.”">13</a>, <a href="https://github.com/flashbots/mev-boost" rel="noopener nofollow ugc" title="*Flashbot/MEV-Boost*. Flashbots.">14</a>], while block builders interact with the Relay API <a href="https://flashbots.github.io/relay-specs/" rel="noopener nofollow ugc" title="Flashbots Ltd, Relay-API.">[15]</a>. The Relay, in turn, communicates with its beacon client through the Beacon API <a href="https://github.com/ethereum/beacon-APIs" rel="noopener nofollow ugc" title="Ethereum Foundation, *ethereum/beacon-APIs*. (Oct. 09, 2024).">[30]</a>. We extended the sequence diagram of MEV-Boost <a href="https://github.com/flashbots/mev-boost" rel="noopener nofollow ugc" title="*Flashbot/MEV-Boost*. Flashbots.">[14]</a> to provide an overview of HTTP calls within the Relay, as illustrated in Figure 2.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/0/6/06c27e25dff33c837d6e8c19662954850d0a737d.png" title="Figure 2 Sequence diagram of MEV Relay, extracted from source code flashbots/mev-boost-relay."><img alt="Figure 2 Sequence diagram of MEV Relay, extracted from source code flashbots/mev-boost-relay." height="500" src="https://ethresear.ch/uploads/default/optimized/3X/0/6/06c27e25dff33c837d6e8c19662954850d0a737d_2_640x500.png" width="640" /></a></div><p></p>
<p><em><strong>Figure 2</strong> Sequence diagram of MEV Relay, extracted from source code flashbots/mev-boost-relay <a href="https://github.com/flashbots/mev-boost-relay" rel="noopener nofollow ugc" title="*flashbots/mev-boost-relay*. (Apr. 03, 2024). Go. Flashbots.">[31]</a>. See a zoomable version <a href="https://kroki.io/mermaid/svg/eNqtVk1v2zAMvfdX6NLD0Ba5B0MPRbHuVBTLsGshy0wsRJY8UU6afz9KllX5K-5huTSlKOqRfI8Mwt8WtIBnyQ-W1zeMPg23TgrZcO2YMBpBY4uTkxpO74Ux6CYnFhS_4Lz5vQBOMSeHhTLi-F60UpVg8Sac_5ZOwZb98vfYDuwJbLDf3kYbOh9BH1jbhINX44AZcosQtt3fzg97r-7s4fHxLke0ZQdwu4sWO8ddTDc_f6ALD33YgSu5YePLtBb9BTSgXA8d_b4c94exx52ooGwVrAbPnWdf-DaMj22BwsoCmDPUsYsyvGTcOTK1DpBxXbIKyAYn0K5LThnTsFc4M1Smo8da3amQ8IcrWXJnLPqn2oa-Aztqc9bslI6G0ZZ6Mwo3SHMFyps1jUGwz62TlN7eWEoxovnS46MAg7dBlz2DM3iBnlMOJ-ltP_MfUTm5EIakR8_6g0QHNr0RnJNDyhuXXAMIKw-VY2afxBRREBNkDQSkbu7Z98JuHgmt3JPM5EGTICxEs6cGUkhgUrPnp5xkvmYZ4BE-Os2S7yv21A2HpXINJ8h29P-ocIGhbwTblFJwpS6ptcNreami7QVc1jvPj7YRpvZjqImtR3aSvJtRY8r5xCdI5wKv8obRhEBQIByQIEmIHfQEYplNZy5dR2xFN7i_n4S6nD4Nglo6UvVTeGaQeP4yV1coVIIwJfEDuZbuwmgOieN1EqGsW8WdNDpjVRwOjppYyPKaFEiQP2k6wRUJDF0CbgX7AWxKk5ZVRnx23ojWmf0-mCIyCsQKOveYMNrCfLRACelYpYpjdbPCiFGt58b0SEEpiaHzZ_8z31jp3pHV3FEjkMFHE9i0fLPjW5dk3AVT6d5lVLsGqy9y5u6bz1wFYaOAvf8s4cEgymYqAn-D-DuhwKg2bxnY2cxkSdvLV6VfcWhaK-Aqa_KgSxulaQslsQp9vMIvr4OkXBqXJXz0suiH7lgYUQMD6vgp53rqVVQ32vHArbpslPe1_sceJoeIrQuxJNqUSWEpV8Gp875BmrZ7uNe1aG9NHex-oCiW_cgbrcq0KYdcnm_X_-Ly0maZeekfwzrOyw" rel="noopener nofollow ugc">here</a>.</em></p>
<p>Below is a summary of relevant HTTP calls:</p>
<p>Proposer APIs called by mev-boost implemented in the Relay</p>
<div class="md-table">
<table>
<thead>
<tr>
<th>Path</th>
<th>Endpoint name</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>GET</strong> /eth/v1/builder/status</td>
<td><code>getStatus</code></td>
</tr>
<tr>
<td><strong>POST</strong> /eth/v1/builder/validators</td>
<td><code>registerValidator</code></td>
</tr>
<tr>
<td><strong>GET</strong> /eth/v1/builder/header/{slot}/{parent_hash}</td>
<td><code>getHeader</code></td>
</tr>
<tr>
<td><strong>POST</strong> /eth/v1/builder/blinded_blocks</td>
<td><code>getPayload</code></td>
</tr>
<tr>
<td>(equivalent to <strong>POST</strong> /eth/v1/builder/blinded_blocks</td>
<td><code>submitBlindedBlock</code>)</td>
</tr>
</tbody>
</table>
</div><p>Relay API called by block builders implemented in the Relay</p>
<div class="md-table">
<table>
<thead>
<tr>
<th>Path</th>
<th>Endpoint name</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>GET</strong> /relay/v1/builder/validators</td>
<td><code>builderGetValidators</code></td>
</tr>
<tr>
<td><strong>POST</strong> /relay/v1/builder/blocks</td>
<td><code>submitNewBlock</code></td>
</tr>
</tbody>
</table>
</div><p>Selected BeaconAPIs used by the Relay:</p>
<div class="md-table">
<table>
<thead>
<tr>
<th>Path</th>
<th>Endpoint name</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>GET</strong> /eth/v1/events</td>
<td><code>subscribeToEvents</code></td>
</tr>
<tr>
<td><strong>GET</strong> /eth/v1/beacon/states/{state_id}/validators</td>
<td><code>getStateValidators</code></td>
</tr>
<tr>
<td><strong>GET</strong> /eth/v1/node/syncing</td>
<td><code>getSyncStatus</code></td>
</tr>
<tr>
<td><strong>GET</strong> /eth/v1/validator/duties/proposer/{epoch}</td>
<td><code>getProposerDuties</code></td>
</tr>
<tr>
<td><strong>POST</strong> /eth/v1/beacon/blocks</td>
<td><code>publishBlock</code></td>
</tr>
<tr>
<td><strong>POST</strong> /eth/v2/beacon/blocks</td>
<td><code>publishBlock</code></td>
</tr>
<tr>
<td><strong>GET</strong> /eth/v1/beacon/genesis</td>
<td><code>getGenesis</code></td>
</tr>
<tr>
<td><strong>GET</strong> /eth/v1/config/fork_schedule</td>
<td><code>getForkSchedule</code></td>
</tr>
<tr>
<td><strong>GET</strong> /eth/v1/beacon/headers</td>
<td><code>getBeaconHeader</code></td>
</tr>
<tr>
<td><strong>GET</strong> /eth/v1/beacon/headers/{block_id}</td>
<td><code>getBeaconHeaderAt</code></td>
</tr>
<tr>
<td><strong>GET</strong> /eth/v1/config/spec</td>
<td><code>getSpec</code></td>
</tr>
<tr>
<td><strong>GET</strong> /eth/v1/beacon/states/{state_id}/randao</td>
<td><code>getRandao</code></td>
</tr>
<tr>
<td><strong>GET</strong> eth/v1/beacon/states/{state_id}/withdrawals</td>
<td><code>getWithdrawals</code></td>
</tr>
</tbody>
</table>
</div><p>Our experiment focuses on extracting metadata, particularly the IP addresses and client types, from the `X-Forwarded-For` and `X-Real-IP` headers within the source code<sup class="footnote-ref"><a href="https://ethresear.ch#footnote-51319-3" id="footnote-ref-51319-3">[3]</a></sup>.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-22-attestation-data-collection-12" name="p-51319-h-22-attestation-data-collection-12"></a>2.2. Attestation data collection</h3>
<p>Our team at HOPR previously conducted research on “validator sniping” <a href="https://medium.com/hoprnet/proof-of-stake-validator-sniping-research-8670c4a88a1c" rel="noopener nofollow ugc" title="S. Bürgel and L. Pohanka, “Proof-of-Stake Validator Sniping Research,” HOPR.">[3]</a>, which involved linking validators’ IP addresses with their public keys collected from validator attestations. In this experiment, we expanded our modifications to beacon clients to include aggregated attestations as an additional data source for inferring the most likely IP address of a validator.</p>
<p>We calculated the percentage of occurrences for each validator’s public key and peer ID pair. For each validator public key, we identified the pair with the highest occurrence percentage as the most probable match. This allowed us to link the validator’s IP address with their public key.</p>
<p>Each consensus client may serve multiple validators. To locate a validator with their consensus client’s public-facing IP address, we first established the link between validators’ public keys and the peer ID used in the networking layer of consensus clients from collected attestations. We then listed the percentage of occurrences of a certain peer ID paired with a specific pub key from received attestations, and. We calculated the percentage of occurrences for each validator’s public key and peer ID pair. For each validator public key, we identified the pair with the highest occurrence percentage as the most probable match. We further associated the obtained public key and peer ID pairs with nodes’ multi-addresses, as the link between peer ID and multi-addresses is created when establishing connections in the libp2p protocol.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-23-de-anonymization-13" name="p-51319-h-23-de-anonymization-13"></a>2.3. De-anonymization</h3>
<p>We started by matching the IP addresses we gathered from HTTP calls with the public keys that validators provided during the registerValidator call. This helped us connect validator public keys with their corresponding MEV-Boost IP address.</p>
<p>Next, we used validator attestations to link the most likely validator public key with their Beacon Client IP address.</p>
<p>Finally, by combining these connections, we were able to acquire both the MEV-Boost and Beacon Client (or consensus client) IP addresses for each validator. With this information, we could estimate the location of a validator, as these processes necessitate low latency and are therefore likely to be situated in close geographic proximity.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-24-simulation-of-network-attacks-14" name="p-51319-h-24-simulation-of-network-attacks-14"></a>2.4. Simulation of network attacks</h3>
<p>To conduct production-level network attacks on production servers in the real world, a significant amount of resources is required. We have simplified the test to demonstrate the technical viability of network attacks. As more than 90% of denial of service (DoS) attacks use TCP and TCP SYN flooding is the most commonly used attack <a href="https://doi.org/10.1109/INFCOM.2002.1019404" rel="noopener nofollow ugc" title="H. Wang, D. Zhang, and K. G. Shin, “Detecting SYN flooding attacks,” in *Proceedings.Twenty-First Annual Joint Conference of the IEEE Computer and Communications Societies*, Jun. 2002, pp. 1530–1539. doi: 10.1109/INFCOM.2002.1019404.">[32]</a>, we started the network attacks with a raw ICMP and TCP flood. We used the “syn_flood” service to simulate a network adversary, sending out an ICMP ping flood and TCP SYN flood using ‘iputils-ping’ and ‘hping3’ respectively to beacon client, validator clients, and mev-boost instances. To analyze the impact on memory and network performance, the changes were inspected using the Kubernetes dashboard, as shown in Figure 4 and onwards.</p>
<p>For a more efficient simulation of network attacks, the Attacknet <a href="https://github.com/crytic/attacknet" rel="noopener nofollow ugc" title="crytic/attacknet: Tool and testing methodology for subjecting blockchain devnets to simulated network and side channel attacks.">[33]</a> developed by Trail of Bits was utilized to execute memory attacks, bandwidth attacks, and clock skew attacks.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51319-h-3-setup-and-environment-15" name="p-51319-h-3-setup-and-environment-15"></a>3. Setup and environment</h2>
<p>The simulation environment is an Ethereum testnet in a sandbox based on the Kurtosis Ethereum package developed by EthPandaOps. It has been modified for specific data collection and analysis purposes. Key modifications are given in the architecture breakdown. The source code can be found at hoprnet/metaclear-ethereum-package<sup class="footnote-ref"><a href="https://ethresear.ch#footnote-51319-4" id="footnote-ref-51319-4">[4]</a></sup>.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-31-architecture-16" name="p-51319-h-31-architecture-16"></a>3.1. Architecture</h3>
<p>We modified the implementation of various components used in MEV-Boost to collect relevant data and metadata, as highlighted in yellow in Figure 3.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/4/442ec3279ca2ec69e1fd48edefe0c2a5a57b8464.png" title="Figure 3 Testing architecture, adapted from MEV-Boost Relay architecture diagram 34"><img alt="Figure 3 Testing architecture, adapted from MEV-Boost Relay architecture diagram 34" height="388" src="https://ethresear.ch/uploads/default/optimized/3X/4/4/442ec3279ca2ec69e1fd48edefe0c2a5a57b8464_2_690x388.png" width="690" /></a></div><br />
<em><strong>Figure 3</strong> Testing architecture, adapted from MEV-Boost Relay architecture diagram <a href="https://flashbots.notion.site/Running-MEV-Boost-Relay-at-scale-4040ccd5186c425d9a860cbb29bbfe09" rel="noopener nofollow ugc" title="Flashbots Ltd, “Running MEV-Boost-Relay at scale.”">[34]</a>. The system (circled in dark blue) is divided into three main sections: Proposer, Relay, and Block Builders. Components highlighted in yellow indicate the “metaclear-” forked versions of the original implementation, where modifications have been introduced to collect data and metadata. Components filled with blue are services for testing system behaviors under network attacks.</em><p></p>
<p><strong>Proposer Section:</strong></p>
<ul>
<li>Beacon Node: Lighthouse client that manages consensus and coordinates the actions of validators. Here it has been enhanced with “metaclear” modifications to extract public keys from attestation and aggregated attestations, as well as the mapping of public keys and multi-address from the networking layer. The docker image that it runs is `hoprnet/metaclear-lighthouse`<sup class="footnote-ref"><a href="https://ethresear.ch#footnote-51319-5" id="footnote-ref-51319-5">[5]</a></sup>.</li>
<li>MEV-Boost: A middleware used by validators to interact with the MEV-Boost marketplace.</li>
<li>Execution Node: Geth client running `ethereum/client-go:latest`, which is the execution environment where transactions are processed.</li>
<li>Validators: 64 validators per proposer instance. It manages the duties of an Eth2.0 validator, including proposing and attesting to blocks.</li>
</ul>
<p><strong>Relay Section:</strong></p>
<ul>
<li>Services: The main service of the metaclear MEV-Boost Relay is where HTTP calls are handled. The single instance of housekeeper coordinates sync between beacon, db, and APIs. The website displays data from the datastore. The major modifications are as follows.
<ul>
<li>API: Handles all the external facing calls of proposer APIs and block builder APIs. Metadata is collected from the HTTP headers.</li>
<li>Housekeeper: As it updates known validators and proposer duties, it’s modified to store those data persistently.</li>
</ul>
</li>
<li>Clients: A local beacon and an execution client to gain knowledge on validator activity and simulate blocks submitted from block builders. It uses the same modified image as nodes in the Prosper sections.</li>
<li>Data store: Data management component of the metaclear MEV-Boost Relay. It consists of an in-memory cache, Redis, and Postgres database. Here below are the major modifications.
<ul>
<li>Postgres: Two new tables are created to collect HTTP header metadata, proposer duties, and RANDO values.</li>
</ul>
</li>
</ul>
<p><strong>Block Builders Section:</strong></p>
<ul>
<li>Block builder (MEV-Flood): Deploys mock Uniswap contracts and builds bundles on those mock Uniswap transactions. These entities are responsible for constructing blocks that are optimized for MEV extraction. In this architecture, one block builder instance is shown interacting with the Relay. The “flashbots/mev-flood” package is used to create mock MEV bundles.</li>
</ul>
<p><strong>Network tools:</strong></p>
<ul>
<li>Syn_flood: A dedicated service used to SYN flood targets, where it runs a containerized `hping3` TCP/IP packet assembler.</li>
<li>Tools such as `iputils-ping` and `tcpdump` are installed in the services, along with beacon client instances and mev-boost instances.</li>
<li>AttackNet and Chaos-mesh: Tools used for injecting faults and simulating network attacks on the system. We created three types of test suites<sup class="footnote-ref"><a href="https://ethresear.ch#footnote-51319-6" id="footnote-ref-51319-6">[6]</a></sup>: “memory-stress” for memory exhaustion, “network-bandwidth” for saturated bandwidth, and “clock-skew” for synchronization disruption.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-32-environment-and-test-parameters-17" name="p-51319-h-32-environment-and-test-parameters-17"></a>3.2. Environment and test parameters</h3>
<p>All the images are run in minikube with docker as VM driver</p>
<ul>
<li>Minikube v1.33.1 on Darwin 14.6.1 (arm64) with memory of 4GB</li>
<li>Kubernetes v1.30.0 on Docker 26.1.1</li>
<li>Kurtosis engine 1.1.0</li>
</ul>
<p>MEV flood generates bundles every 15 seconds. The interval between slots is reduced to 6 seconds from the standard 12s for faster testing. The number of consensus layer (CL) clients varies between 2 to 3 in different testing scenarios. Each CL instance contains 64 validator public keys.</p>
<p>In the TCP SYN flood, 15000 packets are sent from random IP sources to the observed IP address and port. In the memory stress chaos test, the test plan has three scenarios, targeting one validator client (“vc-1-geth-lighthouse”), one mev-boost instance (“mev-boost-2-lighthouse-geth”), and one beacon client (“cl-3-lighthouse-geth”). Each belongs to a different beacon instance. For each scenario, 50 workers stress 10 MB/worker for a total of 10 minutes.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51319-h-4-results-18" name="p-51319-h-4-results-18"></a>4. Results</h2>
<p>We compiled a Grafana dashboard to collect and visualize metrics from the metaclear experiment and visualize how a malicious Relay could identify validators with attestations from beacon clients and metadata from HTTP requests.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-41-linking-public-key-and-ip-addresses-of-validators-19" name="p-51319-h-41-linking-public-key-and-ip-addresses-of-validators-19"></a>4.1. Linking public key and IP addresses of validators</h3>
<p>Public keys of validators can be linked with their IP addresses from two datastreams. One datastream is validator attestation. The link between validator public key to their consensus layer (CL) client peer IDs can be probabilistically established by observing aggregated and non-aggregated attestations, as shown in Figure 4(b). As peer IDs in CL can be translated into multiaddress and thus IP addresses, as shown in Figure 4(a), links between validator public keys with IP addresses can be obtained via CL peer IDs, as in Figure 4(c).</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/c/bced7b1320231fc0916d3d6224dd817b7fc51674.jpeg" title="Figure 4a) displays information from an unmodified Lighthouse implementation, including the link between peer ID and IP address, the current beacon epoch, and the current head slot."><img alt="Figure 4a) displays information from an unmodified Lighthouse implementation, including the link between peer ID and IP address, the current beacon epoch, and the current head slot." height="222" src="https://ethresear.ch/uploads/default/optimized/3X/b/c/bced7b1320231fc0916d3d6224dd817b7fc51674_2_690x222.jpeg" width="690" /></a></div><p></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/4/24afebff2692297daa070d33cd2d059d3198a0b0.jpeg" title="Figure 4b) features a table that calculates the correlation between public keys and peer IDs derived from attestations. Higher percentages in this table indicate a stronger probability of the peer ID being associated with the corresponding public key."><img alt="Figure 4b) features a table that calculates the correlation between public keys and peer IDs derived from attestations. Higher percentages in this table indicate a stronger probability of the peer ID being associated with the corresponding public key." height="448" src="https://ethresear.ch/uploads/default/optimized/3X/2/4/24afebff2692297daa070d33cd2d059d3198a0b0_2_690x448.jpeg" width="690" /></a></div><p></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/f/5f90b02eefc47810aa54e63130bf9e9cd3781580.jpeg" title="Figure 4c) uses the most probable associations between public keys and peer IDs to establish a mapping between the public key and the IP address of the validators' consensus layer (CL) client."><img alt="Figure 4c) uses the most probable associations between public keys and peer IDs to establish a mapping between the public key and the IP address of the validators' consensus layer (CL) client." height="267" src="https://ethresear.ch/uploads/default/optimized/3X/5/f/5f90b02eefc47810aa54e63130bf9e9cd3781580_2_690x267.jpeg" width="690" /></a></div><p></p>
<p><em><strong>Figure 4</strong> Grafana dashboard for MEV-Boost-Relay and their beacon nodes, labelled as a), b), and c) from top to bottom. Figure 4a) displays information from an unmodified Lighthouse implementation, including the link between peer ID and IP address, the current beacon epoch, and the current head slot. Figure 4b) features a table that calculates the correlation between public keys and peer IDs derived from attestations. Higher percentages in this table indicate a stronger probability of the peer ID being associated with the corresponding public key. Figure 4c) uses the most probable associations between public keys and peer IDs to establish a mapping between the public key and the IP address of the validators’ consensus layer (CL) client.</em></p>
<p>The other datastream is the HTTP API calls requested to MEV Relays.</p>
<p>Figure 5 shows the Metaclear Relay dashboard, which displays how the MEV-Boost Relay can track the IP address of mev-boost instances of validators through the ‘registerValidator’ HTTP call, which occurs at the outset of launching the mev-boost instance. It’s important to note that multiple relays can be assigned to mev-boost, allowing the IP address to be shared with all the provided MEV-Boost Relays.</p>
<p>Additionally, it monitors the frequency (per second) of each HTTP call to detect the submission pattern of block builders. The green line, representing the ‘POST /eth/v1/builder/blinded_blocks’ endpoint, reflects the rate of block builders delivering blocks. Moreover, the MEV-Boost Relay gathers the IP addresses of block builders.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/6/265c87a1d4dc52b8948f0d0556840c6a83195fd0.jpeg" title="Figure 5 Metaclear Relay dashboard displays the association between the validator’s public key, their peer ID, and the IP:port of their mev-boost instance at the launch of the mev-boost."><img alt="Figure 5 Metaclear Relay dashboard displays the association between the validator’s public key, their peer ID, and the IP:port of their mev-boost instance at the launch of the mev-boost." height="297" src="https://ethresear.ch/uploads/default/optimized/3X/2/6/265c87a1d4dc52b8948f0d0556840c6a83195fd0_2_690x297.jpeg" width="690" /></a></div><br />
<em><strong>Figure 5</strong> Metaclear Relay dashboard displays the association between the validator’s public key, their peer ID, and the IP:port of their mev-boost instance at the launch of the mev-boost. It also monitors the rate (per second, average over 1 minute) of each HTTP call to identify the submission pattern of block builders.</em><p></p>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-42-identifying-victim-validators-20" name="p-51319-h-42-identifying-victim-validators-20"></a>4.2. Identifying victim validators</h3>
<p>The Metaclear Relay dashboard also extracts the proposers of the next epoch, as shown in Figure 6. As proposers are unique once selected for a slot in a given epoch, they are the most obvious choice of victims for network-layer attacks after being deanonymized. However, we will outline possible attacks on other parties in the network later. Those proposers are computed by RANDAO <a href="https://eth2book.info/capella/part2/building_blocks/randomness/" rel="noopener nofollow ugc" title="B. Edgington, *Upgrading Ethereum | 2.9.2 Randomness*. 2023">[25]</a>.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/e/2e6e743461c5aa1039422f2bef6668a33e3371f2.jpeg" title="Figure 6 Metaclear Relay dashboard displays validators selected as proposers of the next epoch"><img alt="Figure 6 Metaclear Relay dashboard displays validators selected as proposers of the next epoch" height="314" src="https://ethresear.ch/uploads/default/optimized/3X/2/e/2e6e743461c5aa1039422f2bef6668a33e3371f2_2_690x314.jpeg" width="690" /></a></div><br />
<em><strong>Figure 6</strong> Metaclear Relay dashboard displays validators selected as proposers of the next epoch and historical RANDAO values.</em><p></p>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-43-viable-network-attacks-21" name="p-51319-h-43-viable-network-attacks-21"></a>4.3. Viable network attacks</h3>
<p>We conducted a memory stress attack on three different clients around 15:13 one validator client (“vc-1-geth-lighthouse”), one mev-boost instance (“mev-boost-2-lighthouse-geth”), and one beacon client (“cl-3-lighthouse-geth”), each belonging to a different beacon instance as shown in Figure 7.</p>
<p>The attack caused the respective clients to become overwhelmed. As a result, the validator client and the beacon client stopped producing blocks and processing slots. As shown in Figure 7, the block production metric came to a halt for client number 3 (green line in Figure 7), where the consensus client was under attack. Block production stopped for client number 1 (blue line in Figure 7), where the validator stopped functioning. Block production continued as usual for client number 2 (yellow line in Figure 7), where mev-boost was under attack. However, blocks were produced only locally but not from mev-boost, as shown in Figure 8.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/8/b889772db4e922230d02b5c3ff9c5f1b2192534b.png" title="Figure 7 Number of successfully produced blocks around the time of the attack."><img alt="Figure 7 Number of successfully produced blocks around the time of the attack." height="396" src="https://ethresear.ch/uploads/default/optimized/3X/b/8/b889772db4e922230d02b5c3ff9c5f1b2192534b_2_690x396.png" width="690" /></a></div><br />
<em><strong>Figure 7</strong> Number of successfully produced blocks around the time of the attack. Instances number 1 (blue) and 3 (green), where the validator client and beacon client were under attack respectively, stopped producing slots.  Instance number 2 (yellow), where the mev-boost was under attack, could still produce blocks locally.</em><p></p>
<p>Figure 8 shows the number of blocks constructed by the block builder instance and subsequently sent to proposers via the metaclear Relay instance. After the attack (after the dotted line), no blocks were delivered through the relay. This is due to instances 1 and 3 being unable to fulfill their proposer duties. Even though instance 2 was still able to propose blocks, they could not be proposed via the Relay because its MEV-boost functionality was not operational.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/4/544b07dc01aac7bfc5d8feb6a3bcbb9e711ba661.png" title="Figure 8* Rate per minute of slots containing blocks built by the block builder and delivered to proposers via meta-clear Relay."><img alt="Figure 8* Rate per minute of slots containing blocks built by the block builder and delivered to proposers via meta-clear Relay." height="322" src="https://ethresear.ch/uploads/default/optimized/3X/5/4/544b07dc01aac7bfc5d8feb6a3bcbb9e711ba661_2_690x322.png" width="690" /></a></div><br />
<em><strong>Figure 8</strong> Rate per minute of slots containing blocks built by the block builder and delivered to proposers via meta-clear Relay.</em><p></p>
<p>By observing the HTTP endpoint request rate in Figure 9, it is clear that block builders kept posting blocks to the Relay (continuous red and orange lines). However, the request rate of other endpoints (other lines) dropped to 0, indicating they were not called.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/5/a589596cf251feca4cef8a6cca127d71f3a674f0.png" title="Figure 9 Metaclear-Relay HTTP endpoint request rate (per minute) around the attack time"><img alt="Figure 9 Metaclear-Relay HTTP endpoint request rate (per minute) around the attack time" height="408" src="https://ethresear.ch/uploads/default/optimized/3X/a/5/a589596cf251feca4cef8a6cca127d71f3a674f0_2_690x408.png" width="690" /></a></div><br />
<em><strong>Figure 9</strong> Metaclear-Relay HTTP endpoint request rate (per minute) around the attack time</em><p></p>
<p>The block explorer gives us a more intuitive overview of slot production status, as seen in Figure 10. All slots were successfully proposed before the attack. After the attack, the only block proposer was client number 2, while clients 1 and 3 missed their slots.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/2/426d035252b87d4abd3e38bdfe6e2a6dfa8a70d5.jpeg" title="Figure 10 Explorer of block production before (left) and after (right) the attack."><img alt="Figure 10 Explorer of block production before (left) and after (right) the attack." height="410" src="https://ethresear.ch/uploads/default/optimized/3X/4/2/426d035252b87d4abd3e38bdfe6e2a6dfa8a70d5_2_690x410.jpeg" width="690" /></a></div><p></p>
<p><em><strong>Figure 10</strong> Explorer of block production before (left) and after (right) the attack.</em></p>
<h2><a class="anchor" href="https://ethresear.ch#p-51319-h-5-discussion-further-attack-scenarios-and-network-consequences-22" name="p-51319-h-5-discussion-further-attack-scenarios-and-network-consequences-22"></a>5. Discussion: Further Attack scenarios and Network Consequences</h2>
<p>Our Metaclear experiment focused on demonstrating the practicality of identifying and disrupting block proposers using leaked metadata. However, it is feasible that similar approaches to deanonymizing actors in the network could enable other attacks against other actors, including the Relay itself. The following sections outline briefly how the same metadata could be used to probabilistically identify the roles of different nodes and the kinds of attacks which could be conducted as a result, along with possible broader consequences for the entire network.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-51-network-topology-can-be-mapped-using-relay-metadata-23" name="p-51319-h-51-network-topology-can-be-mapped-using-relay-metadata-23"></a>5.1. Network topology can be mapped using Relay metadata</h3>
<p>The MEV-Boost Relay has access to ample metadata to provide an advantage in mapping the network topological and geographical distribution of validators and builders. Validators rely on MEV-Boost instances to produce blocks with higher returns. This incentivizes the deployment of mev-boost and beacon nodes in a network configuration that reduces latency, likely in the same geographical area. By cross-checking the IP addresses of mev-boost collected from ‘registerValidator’ calls with the beacon IP addresses correlated from observing attestations, the MEV-Boost Relay can map out the topology and location of validators more reliably.</p>
<p>By grouping validators by IP addresses, Relayers can use IP ranges to infer the network topology. For example, home stakers can be identified by residential IP addresses and they would likely experience higher latency due to multiple network layers before reaching the device. Cloud-hosted validators can easily be identified by finding providers from IP address registries. Staking pools generally exhibit a dense concentration of validators per IP address, which implies that they apply some network security rules similar to other nodes in the same pool. These are just some of the examples of inferences which might be drawn, with differing levels of confidence. Determining the nature of a particular validator’s hardware and relationship to other validators opens up the possibility of more nuanced targeted attacks.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-52-solo-stakers-are-more-vulnerable-to-attack-24" name="p-51319-h-52-solo-stakers-are-more-vulnerable-to-attack-24"></a>5.2. Solo stakers are more vulnerable to attack</h3>
<p>For reasons of security, decentralization and fairness, it is generally considered desirable for solo stakers to comprise a significant proportion of validators <a href="https://cointelegraph.com/news/vitalik-buterin-advocates-lowering-solo-staking-eth" rel="noopener nofollow ugc" title="“Vitalik Buterin supports lowering Ethereum solo staking requirement,” Cointelegraph.">[35]</a>, although there is debate about the extent to which stakers with less powerful hardware should be supported. However, even well-equipped solo stakers are disproportionately vulnerable to the attacks outlined in our research. They are generally easier to identify <a href="https://blog.rated.network/blog/solo-stakers" rel="noopener nofollow ugc" title="“Solo stakers: The backbone of Ethereum,” Rated blog.">[36]</a>, easier to attack, and would be less likely to notice that they have been attacked.</p>
<p>Solo stakers and home stakers generally do not have ample resources for Distributed Denial of Service (DDoS) protection, such as alternative fallback IP addresses. Due to the usage of consumer-grade hardware, physical limitations on memory and computation power, as well as the bandwidth on the router and switches, solo stakers are more exposed to DDoS attacks. Additionally, DDoS attacks against a single home staker can be targeted extremely well; the attack only needs to be effective for a few seconds to cause a slot to be missed, making such attacks cheap to execute and hard to detect. Attackers can identify solo stakers via attestations alone, but the extra data collected from Relayer would make the validator de-anonymization more efficient.</p>
<p>Since a DDoS attack can mimic the effects of other non-malicious errors, it is not always possible to identify when an attack has occurred. Solo stakers propose fewer blocks than other classes of proposer, and are much less likely to have robust hardware setups. Therefore, it will be harder for them to gather data to prove an attack, and they may be more likely to ascribe other causes such as a faulty setup or poor connection.</p>
<p>Attacking solo validators during block proposals can provide significant advantages to professional validators. These entities typically have the resources and expertise to carry out such attacks efficiently and profitably, aiming to repeatedly disrupt the reward-earning potential of solo stakers. Over time, this strategy can force solo validators to leave the network after repeatedly failing to produce blocks, allowing professional validators to capture a larger market share and exert greater control over block production.</p>
<p>The disproportionate effects of these attacks on solo stakers could harm attempts to encourage them to join and remain in the network.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-53-randao-can-be-exploited-and-manipulated-25" name="p-51319-h-53-randao-can-be-exploited-and-manipulated-25"></a>5.3. RANDAO can be exploited and manipulated</h3>
<p>RANDAO is the random number generator used by Ethereum in various parts of its consensus mechanism, including determining which validators will be selected to propose upcoming blocks. RANDAO values generated in a particular epoch are used to assign duties in two epochs’ time, allowing chosen validators enough time to prepare for their roles. This delay presents an opportunity for malicious actors, however: by observing when a victim is selected as proposer by the RANDAO, the attacker has a time advantage of one epoch to prepare a DDoS attack (although this will change if secret leader elections are implemented) <a href="https://ethereum.org/en/roadmap/secret-leader-election/" rel="noopener nofollow ugc" title="Ethereum Foundation, “Secret leader election,” ethereum.org.">[37]</a>.</p>
<p>But it is possible to more actively interfere with RANDAO to manipulate future outcomes. Missing a slot, either deliberately or through unresponsiveness or delay, does not introduce entropy in the next RANDAO calculation and thus gains potentially multiple slots for free without increasing its stake fraction. Researchers have already identified the possibility of choosing to forego block rewards in exchange for generating a favourable RANDAO result in two epochs’ time <a href="https://ethresear.ch/t/selfish-mixing-and-randao-manipulation/16081" title="T. Wahrstätter, “Selfish Mixing and RANDAO Manipulation - Consensus,” Ethereum Research.">[38]</a>. Our research suggests a further possibility: proposers could be deliberately targeted by competing proposers to miss their block in order to achieve the same result.</p>
<p>This could be part of a single attack to benefit from a known high-value upcoming event in a particular epoch, or part of a more concerted effort to accelerate the k-tailed RANDAO takeover [<a href="http://arxiv.org/abs/2409.19883" rel="noopener nofollow ugc" title="K. Alpturer and S. M. Weinberg, “Optimal RANDAO Manipulation in Ethereum,” Sep. 29, 2024, *arXiv*: arXiv:2409.19883.">26</a>, <a href="https://blog.rated.network/blog/solo-stakers" rel="noopener nofollow ugc" title="“Solo stakers: The backbone of Ethereum,” Rated blog.">36</a>].</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-54-blobs-can-be-disrupted-26" name="p-51319-h-54-blobs-can-be-disrupted-26"></a>5.4. Blobs can be disrupted</h3>
<p>By occupying network bandwidth, a validator may fail to send their attestation to data availability sampling (DAS) in time, which affects the result of attestation on blob data <a href="https://ethresear.ch/t/das-fork-choice/19578" title="F. Damato, L. Zanolini, and R. Saltini, “DAS fork-choice - Consensus,” Ethereum Research.">[39]</a>. Additionally, a node could selectively respond to sampling requests while selectively providing malicious responses to data requests by other nodes. This could be made feasible by identifying nodes uniquely based on their IP address as outlined above.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-55-recently-bootstrapped-sparsely-connected-beacon-nodes-are-more-vulnerable-to-covert-flash-attacks-27" name="p-51319-h-55-recently-bootstrapped-sparsely-connected-beacon-nodes-are-more-vulnerable-to-covert-flash-attacks-27"></a>5.5. Recently bootstrapped, sparsely connected beacon nodes are more vulnerable to “covert flash attacks”</h3>
<p>The higher the number of validators per beacon node, and the more sparse that beacon’s connections to the rest of the network, the more vulnerable it is to a “covert flash attack” <a href="https://arxiv.org/abs/2007.02754" rel="noopener nofollow ugc" title="D. Vyzovitis, Y. Napora, D. McCormick, D. Dias, and Y. Psaras, “GossipSub: Attack-Resilient Message Propagation in the Filecoin and ETH2.0 Networks,” 2019">[40]</a>. A covert flash attack happens when the sybils of an attacker connect to the victim and behave properly long enough to build up scores in GossipSub protocol before executing a coordinated eclipse attack when the victim needs to propose a block. Such a covert flash attack executed at the time when a validator proposes a block could be considered a new type of MEV, if as a result the attacker is able to extract the value which the attacked proposer would have otherwise claimed for themselves.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-56-selectively-attack-multi-block-mev-28" name="p-51319-h-56-selectively-attack-multi-block-mev-28"></a>5.6. Selectively attack multi-block MEV</h3>
<p>Even in the narrow definition, MEV is not always confined to individual blocks. It is possible to extract MEV across multiple neighbouring blocks, known as multi-block MEV (MMEV), by having two or more consecutive block proposers collude and jointly create blocks in a row <a href="https://doi.org/10.1109/ICBC54727.2022.9805499" rel="noopener nofollow ugc" title="T. Mackinga, T. Nadahalli, and R. Wattenhofer, “TWAP Oracle Attacks: Easier Done than Said?,” in *2022 IEEE International Conference on Blockchain and Cryptocurrency (ICBC)*, May 2022, pp. 1–8. doi: 10.1109/ICBC54727.2022.9805499.">[41]</a>. This is already known to be a high risk MEV tactic, but the disruptive attacks shown in our research increase this risk further. If a block proposer comes under attack and cannot produce the block at a given slot, this missing block will affect the MMEV, often with significant losses. For example, several proposers might collude to extract MMEV by manipulating on-chain time-weighted average price (TWAP) oracles. If a colluding proposer is attacked and fails to produce the promised block, the TWAP will not be manipulated, not only losing the MMEV but potentially resulting in a significant loss for MEV searchers. In an advanced attack scenario, the attacker would control the slot immediately after the targeted proposer. In this case, the attacker would be able to place a backrun order to exploit the original MMEV searcher.</p>
<p>Alternatively, MMEV could be de-risked by taking out other proposers. If, for example, a large validator controls slots 5 and 8 in an epoch, they could disrupt the nodes of proposers responsible for slots 6 and 7 in order to launch a multi block MEV strategy. This is especially viable if the proposers of slots 6 and 7 have been identified as home stakers which can likely be taken out successfully by cheap and highly targeted attacks.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-57-malicious-relay-can-more-easily-conduct-a-metadata-based-attack-without-leaving-traces-29" name="p-51319-h-57-malicious-relay-can-more-easily-conduct-a-metadata-based-attack-without-leaving-traces-29"></a>5.7. Malicious Relay can more easily conduct a metadata-based attack without leaving traces</h3>
<p>Despite Relays being monitored on the reliability of block delivery, there is no measure in place to prevent them from leaking metadata on validators. When such metadata leaks are exploited by network attacks to kick out block proposers, they leave no definitive trace that would incriminate them. As outlined above, in operation, validators, especially solo stakers who get to produce blocks at a lower frequency than professional node operators, cannot differentiate a purposeful attack from poor network conditions.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-58-decentralizing-relays-makes-attacks-more-likely-not-less-30" name="p-51319-h-58-decentralizing-relays-makes-attacks-more-likely-not-less-30"></a>5.8. Decentralizing Relays makes attacks more likely, not less</h3>
<p>It can be tempting to view issues like the ones outlined in this research as “teething troubles” which will be resolved once the network becomes more decentralized. But decentralization is unlikely to resolve the issue of high trust assumptions in Relays. In fact, it may make it worse: although validators may want to connect to multiple relays for more bids from more block builders and/or more reliable receipt of externally built blocks, connecting to multiple relays would leak their metadata to more parties in the network, each a potential attacker. It would also make it even harder to identify who the attacker was.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-59-valuable-metadata-can-make-relay-a-victim-31" name="p-51319-h-59-valuable-metadata-can-make-relay-a-victim-31"></a>5.9. Valuable metadata can make Relay a victim</h3>
<p>Much of the foregoing has focused on an untrustworthy Relay as an attacker, but Relays themselves are also exposed to attack. As a result of reducing latency, Relay, e.g. ultrasound relay, actively disabled Cloudflare service to gain 10 ms of latency advantage <a href="https://www.youtube.com/watch?v=fWRboyGk_lc" rel="noopener nofollow ugc" title="Bell Curve, *Shining A Light On MEV  | Tarun Chitra, Justin Drake*">[42]</a>; although it eliminates the time for round-trip traffic routing in Cloudflare’s network, it also exposes Relay to DDoS attacks. As the validator registration information is publicly available via the Relay’s Data API, attackers can scrape the latest registration data via public endpoints to obtain all the relays that a given validator connects to. Attackers can then prepare DDoS attacks on a validator that will soon become a block proposer, with a maximum of one epoch time ahead. Ideally, this validator only connects to one Relay, so that the target for a DDoS attack is minimal. When Relay fails to deliver a block in time, the victim proposer would need to build a block using only the local mempool and thus lose their MEV profit.</p>
<p>It would be even more beneficial for attackers to attack Relay in the window right after the proposer broadcasts the signed header but before the payload gets published. This would not only damage the reputation of the proposer but also push down the stable operation rate of Relay, gradually making block builders and validators abandon their service. If the attacker happened to be a competitor of Relay, they might attract alienated validators to their service, increasing revenues and getting access to even more valuable metadata.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-510-builders-can-be-made-to-underperform-32" name="p-51319-h-510-builders-can-be-made-to-underperform-32"></a>5.10. Builders can be made to underperform</h3>
<p>Last but not least, block builders also expose their IP addresses to Relays. A malicious attacker could target builders connected to profitable searchers, in the hope that those searchers would move from the now underperforming builders to other block builders.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51319-h-511-a-new-class-of-mev-33" name="p-51319-h-511-a-new-class-of-mev-33"></a>5.11.  A new class of MEV?</h3>
<p>Although the standard definition of MEV restricts itself to actions taken when constructing blocks, we would argue that many of the targeted attacks shown to be possible via this research also qualify as MEV. Metadata can be used to deanonymize other players in the cutthroat MEV game, identify their role in the network and target them with disrupt DDoS attacks which prevent them from extracting MEV they would otherwise expect. In addition to harming competitors, attackers can take advantage of favourable slot ordering to claim this MEV for themselves. Particularly resourceful (in every sense of the words) attackers could even manipulate RANDAO to engineer these favourable slot positions, rather than waiting for circumstances to align.</p>
<p>One key difference: the fairness and ethics of MEV extraction as usually defined is subject to much debate. However, the attacks identified in our research seem unambiguously negative for the health and security of the network. Smooth and reliable block production is a fundamental part of blockchain utility. Incentives to disrupt the block production process and even manipulate RANDAO generation can only be bad for all parties in the long run.</p>
<p>Not all of these attacks seem equally feasible or likely, and this is far from an exhaustive list of possible disruptive actions, but we hope this illustrates the broader point that failures in transport level metadata privacy leave all parties exposed. Even more concerning, many of these attacks can be carried out with no evidence of who the perpetrator is.</p>
<p>In general, all parties engaged in capturing MEV under the standard definition benefit from minimizing latency in the MEV pipeline <a href="https://frontier.tech/exploration-of-mev-latencies" rel="noopener nofollow ugc" title="0xTaker and Frontier Research, “Exploration of MEV Latencies,” Exploration of MEV Latencies.">[43]</a>. Some argue that the co-location of builders with relay and relay with validators may become a driver for a more distributed MEV infrastructure <a href="https://www.youtube.com/watch?v=fWRboyGk_lc" rel="noopener nofollow ugc" title="Bell Curve, *Shining A Light On MEV  | Tarun Chitra, Justin Drake*">[42]</a>. However, a distributed MEV infrastructure does not necessarily translate into a more distributed Ethereum network, hence there’s no direct contribution to a more robust Ethereum network. Due to the risk of leaking metadata privacy, bootstrapping a rather isolated node makes it susceptible to networking-level attacks before it establishes a stable and truthful connection with the rest of the Ethereum network. Therefore, protecting metadata privacy is particularly important to a more distributed Ethereum network with more sparsely-connected nodes.</p>
<p>To mitigate these risks, a promising approach involves integrating network-level metadata privacy protection protocols directly into the networking layer of Ethereum clients when designing PBS specifications. An effective protocol should be easily integrable with the Ethereum protocol, provide out-of-the-box transport layer anonymity for REST-JSON Builder API calls, maintain low latency to ensure that necessary communications can be completed within a slot time, and avoid any centralized points of failure that could become vulnerabilities for privacy leaks. Additionally, generalizing Ethereum’s peer-to-peer networking protocols to support an overlay of metadata-private protocols, such as a mixnet, would further enhance privacy protections across the network. These solutions are already viable and could be implemented to significantly bolster the security of Ethereum’s infrastructure against potential MEV exploits.</p>
<p>Privacy-critical API calls such as `registerValidators` are one-time requests which are not time-critical. Using metadata-private protocols to protect such API calls are given. However, protecting one single endpoint does not prevent metadata leaks because the link between validator public keys and IP addresses can be derived from other endpoints. Partial protection is no protection.</p>
<p>Granted that any overhead on the p2p networking layer would introduce additional latency, which is generally undesirable in MEV games, networking-level privacy protection makes the (re-)distribution of MEV fairer and enhances the resilience of the entire network by protecting solo stakers from network attacks.</p>
<p>While the community seems to be focusing on faster commitment, i.e., single slot finality, single slot secret leader election, should we leave some space for a fair and secure environment for solo stakers and home stakers, and consider a reasonably longer slot time and eventually a longer epoch time, such that large majority of transactions can still be gossiped through the network, even from one sparsely connected node to another sparsely connected node. When there’s sufficient time for transactions to be propagated across the network, the speed of propagation becomes less important as the information asymmetry eventually gets canceled out.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51319-h-6-limitations-34" name="p-51319-h-6-limitations-34"></a>6. Limitations</h2>
<p>While the “Metaclear” experiment effectively demonstrates the potential for network disruption and potential new kinds of MEV extraction through metadata analysis, it has limitations. The simulated memory exhaustion attack is not practically feasible in most real-world deployments due to robust resource allocations. Attacks such as botnet-driven DDoS could achieve the same or better results, but these are hard to test ethically. Such botnets are readily available and inexpensive at a cost of 200$/day <a href="https://www.statista.com/statistics/1350155/selling-price-malware-ddos-attacks-dark-web/" rel="noopener nofollow ugc" title="“Dark web price of malware/DDoS services 2023”">[44]</a>, which could make MEV extraction highly profitable: based on the average profit of 0.025 ETH/block at time of writing, up to $450,000 a day of value in total MEV is available.</p>
<p>Additionally, technical challenges prevented the simulation of network bandwidth and clock skew attacks, which are also known to exacerbate privacy leaks and disrupt block production. Despite these constraints, “Metaclear” highlights significant risks and underscores the need for stronger countermeasures.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51319-h-7-future-research-35" name="p-51319-h-7-future-research-35"></a>7. Future research</h2>
<p>Our current version of Metaclear ran in a contained environment with a limited number of proposers, relays, and block builders. We also assumed that validators had participated in the network for long periods and had gathered sufficient data to derive networking information on other parties.<br />
We would like to extend the experiment to:</p>
<ul>
<li>Run in a more realistic environment</li>
<li>Explore the possibilities for attacks when parties are more distributed yet still co-located and hold more computational resources.</li>
<li>Build a Proof-of-Concept (PoC) that integrates network-level metadata privacy protection protocol solutions like uHTTP <a href="https://github.com/hoprnet/uHTTP-lib/blob/fe38143e0b23ee7d81f8d8941f044261de74f320/ONBOARDING.md" rel="noopener nofollow ugc" title="Github: *u(nlinked)HTTP-lib*">[45]</a>.</li>
<li>Carry out experiments of network bandwidth attack when validators send attestations to blob in data availability sampling (DAS).</li>
</ul>
<p>We also hope to further explore questions such as:</p>
<ul>
<li>How effectively can we de-anonymize validators and builders in the real world?</li>
<li>How much additional latency can the MEV-Boost design tolerate while remaining fair?</li>
<li>How do we quantify the fairness of the distributed Ethereum network?</li>
<li>Are the attacks we have identified a new class of MEV, an extension of existing MEV, or something else?</li>
<li>How large an anonymity set can be obtained when applying network-level metadata privacy protection protocol solutions like uHTTP <a href="https://github.com/hoprnet/uHTTP-lib/blob/fe38143e0b23ee7d81f8d8941f044261de74f320/ONBOARDING.md" rel="noopener nofollow ugc" title="Github: *u(nlinked)HTTP-lib*">[45]</a>?</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-51319-bibliography-36" name="p-51319-bibliography-36"></a>Bibliography</h2>
<p>[1]	“Maximal extractable value (MEV) | <a href="http://ethereum.org" rel="noopener nofollow ugc">ethereum.org</a>.” Accessed: Oct. 09, 2024. [Online]. Available: <a class="inline-onebox" href="https://ethereum.org/en/developers/docs/mev/" rel="noopener nofollow ugc">Maximal extractable value (MEV) | ethereum.org</a><br />
[2]	G. Damalas and P. Ambrus, “An introduction to maximal extractable value on Ethereum,” Mar. 2023. [Online]. Available: <a href="https://assets.ey.com/content/dam/ey-sites/ey-com/en%5C_us/topics/financial-services/ey-an-introduction-to-maximal-extractable-value-on-ethereum.pdf" rel="noopener nofollow ugc">https://assets.ey.com/content/dam/ey-sites/ey-com/en\_us/topics/financial-services/ey-an-introduction-to-maximal-extractable-value-on-ethereum.pdf</a><br />
[3]	S. Bürgel and L. Pohanka, “Proof-of-Stake Validator Sniping Research,” HOPR. [Online]. Available: <a class="inline-onebox" href="https://medium.com/hoprnet/proof-of-stake-validator-sniping-research-8670c4a88a1c" rel="noopener nofollow ugc">Proof-of-Stake Validator Sniping Research | by Dr. Sebastian Bürgel | HOPR | Medium</a><br />
[4]	L. Heimbach, Y. Vonlanthen, J. Villacis, L. Kiffer, and R. Wattenhofer, “Deanonymizing Ethereum Validators: The P2P Network Has a Privacy Issue,” Sep. 06, 2024, <em>arXiv</em>: arXiv:2409.04366. doi: 10.48550/arXiv.2409.04366.<br />
[5]	Ethereum Foundation, “Ethereum Roadmap: PBS and MEV,” <a href="http://ethereum.org" rel="noopener nofollow ugc">ethereum.org</a>. [Online]. Available: <a class="inline-onebox" href="https://ethereum.org/en/roadmap/pbs/" rel="noopener nofollow ugc">Proposer-builder separation | ethereum.org</a><br />
[6]	V. Buterin, “Proposer/block builder separation-friendly fee market designs - Economics,” Ethereum Research. [Online]. Available: <a class="inline-onebox" href="https://ethresear.ch/t/proposer-block-builder-separation-friendly-fee-market-designs/9725">Proposer/block builder separation-friendly fee market designs</a><br />
[7]	V. Buterin, “Two-slot proposer/builder separation - Proof-of-Stake,” Ethereum Research. [Online]. Available: <a class="inline-onebox" href="https://ethresear.ch/t/two-slot-proposer-builder-separation/10980">Two-slot proposer/builder separation</a><br />
[8]	V. Buterin, “Single-slot PBS using attesters as distributed availability oracle - Proof-of-Stake,” Ethereum Research. [Online]. Available: <a class="inline-onebox" href="https://ethresear.ch/t/single-slot-pbs-using-attesters-as-distributed-availability-oracle/11877">Single-slot PBS using attesters as distributed availability oracle</a><br />
[9]	B. Monnot, “Unbundling PBS: Towards protocol-enforced proposer commitments (PEPC) - Economics,” Ethereum Research. [Online]. Available: <a class="inline-onebox" href="https://ethresear.ch/t/unbundling-pbs-towards-protocol-enforced-proposer-commitments-pepc/13879">Unbundling PBS: Towards protocol-enforced proposer commitments (PEPC)</a><br />
[10]	M. Neuder, “Payload-timeliness committee (PTC) – an ePBS design - Proof-of-Stake,” Ethereum Research. [Online]. Available: <a class="inline-onebox" href="https://ethresear.ch/t/payload-timeliness-committee-ptc-an-epbs-design/16054">Payload-timeliness committee (PTC) – an ePBS design</a><br />
[11]	T. Wahrstätter, “MEV-Boost Dashboard,” mevboost.pics. [Online]. Available: <a href="https://mevboost.pics/mevboost.pics" rel="noopener nofollow ugc">https://mevboost.pics/mevboost.pics</a><br />
[12]	Flashbots Ltd, “MEV-Boost in a Nutshell,” MEV-Boost in a Nutshell. [Online]. Available: <a href="https://boost.flashbots.net/" rel="noopener nofollow ugc">https://boost.flashbots.net/</a><br />
[13]	Ethereum Foundation, “Builder-API.” [Online]. Available: <a class="inline-onebox" href="https://ethereum.github.io/builder-specs/" rel="noopener nofollow ugc">Builder-API</a><br />
[14]	<em>Flashbot/MEV-Boost</em>. Go. Flashbots. [Online]. Available: <a class="inline-onebox" href="https://github.com/flashbots/mev-boost" rel="noopener nofollow ugc">GitHub - flashbots/mev-boost: MEV-Boost allows Ethereum validators to source high-MEV blocks from a competitive builder marketplace</a><br />
[15]	Flashbots Ltd, “Relay-API.” [Online]. Available: <a class="inline-onebox" href="https://flashbots.github.io/relay-specs/" rel="noopener nofollow ugc">Relay-API</a><br />
[16]	M. Neuder, “Relays in a post-ePBS world - Proof-of-Stake,” Ethereum Research. [Online]. Available: <a class="inline-onebox" href="https://ethresear.ch/t/relays-in-a-post-epbs-world/16278">Relays in a post-ePBS world</a><br />
[17]	C. Hager, “Post-mortem for a relay vulnerability leading to proposers falling back to local block production (Nov. 10, 2022) - Relays,” Nov. 2022. [Online]. Available: <a class="inline-onebox" href="https://collective.flashbots.net/t/post-mortem-for-a-relay-vulnerability-leading-to-proposers-falling-back-to-local-block-production-nov-10-2022/727" rel="noopener nofollow ugc">Post-mortem for a relay vulnerability leading to proposers falling back to local block production (Nov. 10, 2022) - Relays - The Flashbots Collective</a><br />
[18]	lotusbumi, “MEV-Boost Security Assessment (audit),” Jun. 2022. [Online]. Available: <a class="inline-onebox" href="https://github.com/flashbots/mev-boost/blob/4035cb3c8c8f9b0118a0170049203f0167c604a0/docs/audit-20220620.md" rel="noopener nofollow ugc">mev-boost/docs/audit-20220620.md at 4035cb3c8c8f9b0118a0170049203f0167c604a0 · flashbots/mev-boost · GitHub</a><br />
[19]	R. Miller, “Post mortem: April 3rd, 2023 mev-boost relay incident and related timing issue - The Flashbots Ship,” Apr. 2023. [Online]. Available: <a class="inline-onebox" href="https://collective.flashbots.net/t/post-mortem-april-3rd-2023-mev-boost-relay-incident-and-related-timing-issue/1540" rel="noopener nofollow ugc">Post mortem: April 3rd, 2023 mev-boost relay incident and related timing issue - The Flashbots Ship - The Flashbots Collective</a><br />
[20]	M. Sproul, “Unbundling attacks on MEV relays using RPC,” 12:00:00+10:00. [Online]. Available: <a class="inline-onebox" href="https://blog.sigmaprime.io/mev-unbundling-rpc.html" rel="noopener nofollow ugc">Unbundling attacks on MEV relays using RPC</a><br />
[21]	G. D. Bissias, M. Liberatore, D. Jensen, and B. N. Levine, “Privacy Vulnerabilities in Encrypted HTTP Streams,” in <em>Privacy Enhancing Technologies</em>, G. Danezis and D. Martin, Eds., Berlin, Heidelberg: Springer, 2006, pp. 1–11. doi: 10.1007/11767831_1.<br />
[22]	F. D. Smith, F. H. Campos, K. Jeffay, and D. Ott, “What TCP/IP Protocol Headers Can Tell Us About the Web,” <em>ACM SIGMETRICS Perform. Eval. Rev.</em>, vol. 29, no. 1, pp. 245–256, Jun. 2001, doi: 384268.378789.<br />
[23]	Ł. Miłkowski, “Realigning block building incentives and responsibilities - Block Negotiation Layer - Proof-of-Stake / Block proposer,” Ethereum Research. [Online]. Available: <a class="inline-onebox" href="https://ethresear.ch/t/realigning-block-building-incentives-and-responsibilities-block-negotiation-layer/16666">Realigning block building incentives and responsibilities - Block Negotiation Layer</a><br />
[24]	S. Gosselin, “MEV-Boost: Merge ready Flashbots Architecture - The Merge,” Ethereum Research. [Online]. Available: <a class="inline-onebox" href="https://ethresear.ch/t/mev-boost-merge-ready-flashbots-architecture/11177">MEV-Boost: Merge ready Flashbots Architecture</a><br />
[25]	B. Edgington, <em>Upgrading Ethereum | 2.9.2 Randomness</em>. 2023. [Online]. Available: <a class="inline-onebox" href="https://eth2book.info/capella/part2/building%5C_blocks/randomness/" rel="noopener nofollow ugc">Upgrading Ethereum | 2.9.2 Randomness</a><br />
[26]	K. Alpturer and S. M. Weinberg, “Optimal RANDAO Manipulation in Ethereum,” Sep. 29, 2024, <em>arXiv</em>: arXiv:2409.19883. [Online]. Available: <a class="inline-onebox" href="http://arxiv.org/abs/2409.19883" rel="noopener nofollow ugc">[2409.19883] Optimal RANDAO Manipulation in Ethereum</a><br />
[27]	M. Kalinin and D. Ryan, “EIP-4399: Supplant DIFFICULTY opcode with PREVRANDAO,” Ethereum Improvement Proposals. [Online]. Available: <a class="inline-onebox" href="https://eips.ethereum.org/EIPS/eip-4399" rel="noopener nofollow ugc">EIP-4399: Supplant DIFFICULTY opcode with PREVRANDAO</a><br />
[28]	B. Busa and P. Jayanthi, “Kurtosis: A Deep Dive to Local Devnets.” [Online]. Available: <a class="inline-onebox" href="https://ethpandaops.io/posts/kurtosis-deep-dive/" rel="noopener nofollow ugc">Kurtosis: A Deep Dive to Local Devnets · ethPandaOps</a><br />
[29]	Flashbots Ltd, “Relay API Documentation.” [Online]. Available: <a href="https://flashbots.notion.site/Relay-API-Documentation-5fb0819366954962bc02e81cb33840f5%5C#854339c909a042d0bbca6e8f8069674e" rel="noopener nofollow ugc">https://flashbots.notion.site/Relay-API-Documentation-5fb0819366954962bc02e81cb33840f5\#854339c909a042d0bbca6e8f8069674e</a><br />
[30]	Ethereum Foundation, <em>ethereum/beacon-APIs</em>. (Oct. 09, 2024). HTML. ethereum. [Online]. Available: <a class="inline-onebox" href="https://github.com/ethereum/beacon-APIs" rel="noopener nofollow ugc">GitHub - ethereum/beacon-APIs: Collection of RESTful APIs provided by Ethereum Beacon nodes</a><br />
[31]	<em>flashbots/mev-boost-relay</em>. (Apr. 03, 2024). Go. Flashbots. [Online]. Available: <a class="inline-onebox" href="https://github.com/flashbots/mev-boost-relay" rel="noopener nofollow ugc">GitHub - flashbots/mev-boost-relay: MEV-Boost Relay for Ethereum proposer/builder separation (PBS)</a><br />
[32]	H. Wang, D. Zhang, and K. G. Shin, “Detecting SYN flooding attacks,” in <em>Proceedings.Twenty-First Annual Joint Conference of the IEEE Computer and Communications Societies</em>, Jun. 2002, pp. 1530–1539. doi: 10.1109/INFCOM.2002.1019404.<br />
[33]	“crytic/attacknet: Tool and testing methodology for subjecting blockchain devnets to simulated network and side channel attacks.” [Online]. Available: <a class="inline-onebox" href="https://github.com/crytic/attacknet" rel="noopener nofollow ugc">GitHub - crytic/attacknet: Tool and testing methodology for subjecting blockchain devnets to simulated network and side channel attacks</a><br />
[34]	Flashbots Ltd, “Running MEV-Boost-Relay at scale.” [Online]. Available: <a class="inline-onebox" href="https://flashbots.notion.site/Running-MEV-Boost-Relay-at-scale-4040ccd5186c425d9a860cbb29bbfe09" rel="noopener nofollow ugc">Notion – The all-in-one workspace for your notes, tasks, wikis, and databases.</a><br />
[35]	“Vitalik Buterin supports lowering Ethereum solo staking requirement,” Cointelegraph. Accessed: Oct. 14, 2024. [Online]. Available: <a href="https://cointelegraph.com/news/vitalik-buterin-advocates-lowering-solo-staking-eth" rel="noopener nofollow ugc">https://cointelegraph.com/news/vitalik-buterin-advocates-lowering-solo-staking-eth</a><br />
[36]	“Solo stakers: The backbone of Ethereum,” Rated blog. [Online]. Available: <a class="inline-onebox" href="https://blog.rated.network/blog/solo-stakers" rel="noopener nofollow ugc">Solo stakers: The backbone of Ethereum — Rated blog</a><br />
[37]	Ethereum Foundation, “Secret leader election,” <a href="http://ethereum.org" rel="noopener nofollow ugc">ethereum.org</a>. [Online]. Available: <a href="https://ethereum.org/en/roadmap/secret-leader-election/" rel="noopener nofollow ugc">https://ethereum.org/en/roadmap/secret-leader-election/</a><br />
[38]	T. Wahrstätter, “Selfish Mixing and RANDAO Manipulation - Consensus,” Ethereum Research. [Online]. Available: <a class="inline-onebox" href="https://ethresear.ch/t/selfish-mixing-and-randao-manipulation/16081">Selfish Mixing and RANDAO Manipulation</a><br />
[39]	F. Damato, L. Zanolini, and R. Saltini, “DAS fork-choice - Consensus,” Ethereum Research. [Online]. Available: <a class="inline-onebox" href="https://ethresear.ch/t/das-fork-choice/19578">DAS fork-choice</a><br />
[40]	D. Vyzovitis, Y. Napora, D. McCormick, D. Dias, and Y. Psaras, “GossipSub: Attack-Resilient Message Propagation in the Filecoin and ETH2.0 Networks,” 2019.  <a class="inline-onebox" href="https://arxiv.org/abs/2007.02754" rel="noopener nofollow ugc">[2007.02754] GossipSub: Attack-Resilient Message Propagation in the Filecoin and ETH2.0 Networks</a><br />
[41]	T. Mackinga, T. Nadahalli, and R. Wattenhofer, “TWAP Oracle Attacks: Easier Done than Said?,” in <em>2022 IEEE International Conference on Blockchain and Cryptocurrency (ICBC)</em>, May 2022, pp. 1–8. doi: 10.1109/ICBC54727.2022.9805499.<br />
[42]	Bell Curve, <em>Shining A Light On MEV  | Tarun Chitra, Justin Drake</em>, (Apr. 05, 2023). [Online Video]. Available: <a href="https://www.youtube.com/watch?v=fWRboyGk%5C_lc" rel="noopener nofollow ugc">https://www.youtube.com/watch?v=fWRboyGk\_lc</a><br />
[43]	0xTaker and Frontier Research, “Exploration of MEV Latencies,” Exploration of MEV Latencies. [Online]. Available: <a class="inline-onebox" href="https://frontier.tech/exploration-of-mev-latencies" rel="noopener nofollow ugc">Exploration of MEV Latencies</a><br />
[44]	“Dark web price of malware/DDoS services 2023,” Statista. [Online]. Available: <a class="inline-onebox" href="https://www.statista.com/statistics/1350155/selling-price-malware-ddos-attacks-dark-web/" rel="noopener nofollow ugc">Dark web price of malware/DDoS services 2023 | Statista</a><br />
[45]	<em>u(nlinked)HTTP-lib</em>. (Oct. 03, 2024). TypeScript. HOPR. [Online]. Available: <a class="inline-onebox" href="https://github.com/hoprnet/uHTTP-lib/blob/fe38143e0b23ee7d81f8d8941f044261de74f320/ONBOARDING.md" rel="noopener nofollow ugc">uHTTP-lib/ONBOARDING.md at fe38143e0b23ee7d81f8d8941f044261de74f320 · hoprnet/uHTTP-lib · GitHub</a></p>
<hr class="footnotes-sep" />

<ol class="footnotes-list">
<li class="footnote-item" id="footnote-51319-1"><p>Source code of metaclear-ethereum package <a class="inline-onebox" href="https://github.com/hoprnet/metaclear-ethereum-package" rel="noopener nofollow ugc">GitHub - hoprnet/metaclear-ethereum-package: A Kurtosis package that deploys a private, portable, and modular Ethereum devnet</a> <a class="footnote-backref" href="https://ethresear.ch#footnote-ref-51319-1">↩︎</a></p>
</li>
<li class="footnote-item" id="footnote-51319-2"><p>Checking the source code within the Github repos of top Relays in <a href="https://www.relayscan.io/" rel="noopener nofollow ugc">https://www.relayscan.io/</a> indicates that they are variants of Flashbots’ MEV-Relay <a class="footnote-backref" href="https://ethresear.ch#footnote-ref-51319-2">↩︎</a></p>
</li>
<li class="footnote-item" id="footnote-51319-3"><p>Source code of metadata handling in metaclear-mev-relay <a href="https://github.com/hoprnet/metaclear-mev-relay/blob/a96c4b6e9df7e7788a308bd10e3efe0ea3b6316d/services/api/service.go%5C#L1003-L1042" rel="noopener nofollow ugc">https://github.com/hoprnet/metaclear-mev-relay/blob/a96c4b6e9df7e7788a308bd10e3efe0ea3b6316d/services/api/service.go\#L1003-L1042</a> <a class="footnote-backref" href="https://ethresear.ch#footnote-ref-51319-3">↩︎</a></p>
</li>
<li class="footnote-item" id="footnote-51319-4"><p>Source code of metaclear-ethereum package <a class="inline-onebox" href="https://github.com/hoprnet/metaclear-ethereum-package" rel="noopener nofollow ugc">GitHub - hoprnet/metaclear-ethereum-package: A Kurtosis package that deploys a private, portable, and modular Ethereum devnet</a> <a class="footnote-backref" href="https://ethresear.ch#footnote-ref-51319-4">↩︎</a></p>
</li>
<li class="footnote-item" id="footnote-51319-5"><p>Source code of metaclear-lighthouse <a class="inline-onebox" href="https://github.com/hoprnet/metaclear-lighthouse" rel="noopener nofollow ugc">GitHub - hoprnet/metaclear-lighthouse: Ethereum consensus client in Rust</a> <a class="footnote-backref" href="https://ethresear.ch#footnote-ref-51319-5">↩︎</a></p>
</li>
<li class="footnote-item" id="footnote-51319-6"><p>Source code of metaclear-attacknet <a class="inline-onebox" href="https://github.com/hoprnet/metaclear-attacknet" rel="noopener nofollow ugc">GitHub - hoprnet/metaclear-attacknet: Tool and testing methodology for subjecting blockchain devnets to simulated network and side channel attacks</a> <a class="footnote-backref" href="https://ethresear.ch#footnote-ref-51319-6">↩︎</a></p>
</li>
</ol>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/transport-privacy-exploration-of-the-validator-relayer-builder-api/21050">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 20 Nov 2024 05:34:09 +0000</pubDate>
</item>
<item>
<title>Overpass Channels - The Key to Unlock a New Era for All of Web3</title>
<link>https://ethresear.ch/t/overpass-channels-the-key-to-unlock-a-new-era-for-all-of-web3/21046</link>
<guid>https://ethresear.ch/t/overpass-channels-the-key-to-unlock-a-new-era-for-all-of-web3/21046</guid>
<content:encoded><![CDATA[
<div> 关键词: 网络验证、私有状态转换、稀疏默克尔树、加密证明、Layer 2可扩展性

总结:
网络验证私有状态转换是一种创新的Layer 2可扩展性方案，它不依赖传统的共识机制。该方法利用加密证明和稀疏默克尔树，确保了隐私、可扩展性和网络中的可验证性。具体实现步骤如下：

1. 使用私有稀疏默克尔树存储钱包数据（如余额和通道状态），网络仅知道代表钱包聚合状态的默克尔根。

2. 钱包执行状态更新时计算新默克尔根及其对应的证明，以展示新的状态如何从旧状态衍生出来。

3. 网络通过重新计算新根并验证提供的证明和约束条件来验证状态转换的有效性，而无需访问底层的私人数据。

4. 初始默克尔根在网络中被记录下来，后续每次状态转移都需要提交与之匹配的新根和证明，同时满足预定义的约束条件。

5. 这种方法保护了数据隐私，降低了网络负载，并通过即时最终确认提高了交易处理速度，为实现大规模可扩展的区块链应用提供了可能。 <div>
<h2><a class="anchor" href="https://ethresear.ch#p-51309-tldr-network-verification-of-private-state-transitions-1" name="p-51309-tldr-network-verification-of-private-state-transitions-1"></a>TL;DR: Network Verification of Private State Transitions</h2>
<p>Overpass Channels introduce an innovative approach to Layer 2 scalability by enabling network verification of private state transitions without relying on traditional consensus mechanisms. This is achieved through the use of cryptographic proofs and sparse Merkle trees, ensuring privacy, scalability, and verifiability across the network.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-how-it-works-2" name="p-51309-how-it-works-2"></a>How It Works</h3>
<h4><a class="anchor" href="https://ethresear.ch#p-51309-h-1-merkle-tree-structure-3" name="p-51309-h-1-merkle-tree-structure-3"></a>1. <strong>Merkle Tree Structure</strong></h4>
<ul>
<li><strong>Private Sparse Merkle Trees (SMTs)</strong>: Each wallet maintains a private sparse Merkle tree containing all its data, such as balances and channel states.</li>
<li><strong>Merkle Root (<span class="math">R</span>)</strong>: The network only knows the Merkle root <span class="math">R</span>, which represents the aggregated state of the wallet. The underlying data remains private on the client side.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-51309-h-2-state-transitions-4" name="p-51309-h-2-state-transitions-4"></a>2. <strong>State Transitions</strong></h4>
<ul>
<li><strong>Updating State</strong>: When a wallet performs a state transition (e.g., a channel update), it computes:
<ul>
<li>A new Merkle root <span class="math">R'</span> reflecting the updated state.</li>
<li>A <strong>proof <span class="math">P</span></strong> that demonstrates how <span class="math">R'</span> is derived from the previous state.</li>
</ul>
</li>
<li><strong>Proof Components</strong>: The proof includes sibling hashes necessary to recompute <span class="math">R'</span> from the modified leaf node in the Merkle tree.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-51309-h-3-networks-role-5" name="p-51309-h-3-networks-role-5"></a>3. <strong>Network’s Role</strong></h4>
<ul>
<li><strong>Verification without Accessing Private Data</strong>: The network verifies transitions by:
<ul>
<li>Recomputing <span class="math">R'</span> using the provided proof <span class="math">P</span> and new state data.</li>
<li>Ensuring the transition adheres to predefined <strong>constraints</strong> (e.g., balance integrity, nonce increments).</li>
</ul>
</li>
<li><strong>Constraints Enforcement</strong>: By checking these constraints, the network ensures the validity of each state transition without needing to see the underlying private data.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-51309-h-4-how-the-network-knows-what-to-expect-6" name="p-51309-h-4-how-the-network-knows-what-to-expect-6"></a>4. <strong>How the Network Knows What to Expect</strong></h4>
<ul>
<li><strong>Initial Merkle Root (<span class="math">R_0</span>)</strong>: At wallet creation, the network stores the initial Merkle root <span class="math">R_0</span>.</li>
<li><strong>Subsequent Transitions</strong>: For every state transition:
<ul>
<li>The network expects a proof <span class="math">P</span> that aligns with the new root <span class="math">R'</span>.</li>
<li>It verifies that the transition constraints are satisfied, ensuring validity.</li>
</ul>
</li>
<li><strong>Trustless Validation</strong>: Relying on the initial root and cryptographic proofs allows the network to detect tampering or invalid operations without accessing private client data.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-51309-h-5-privacy-and-security-7" name="p-51309-h-5-privacy-and-security-7"></a>5. <strong>Privacy and Security</strong></h4>
<ul>
<li><strong>Data Privacy</strong>: All underlying data remains on the client side within the SMT.</li>
<li><strong>Minimal Exposure</strong>: The network only checks the public roots (<span class="math">R_0</span>, <span class="math">R'</span>) and proofs (<span class="math">P</span>), along with transition constraints.</li>
<li><strong>Security Assurance</strong>: This approach ensures privacy, scalability, and verifiability across the network.</li>
</ul>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-why-this-approach-works-8" name="p-51309-why-this-approach-works-8"></a>Why This Approach Works</h2>
<p>By relying solely on the Merkle root and cryptographic proofs, the network can detect tampering or invalid operations without ever seeing private client data. Each transition must cryptographically align with the initial state and follow the rules, ensuring trustless validation. This design works exceptionally well without consensus because it leverages unidirectional cryptographic proofs and the inherent properties of sparse Merkle trees and unilateral channels.</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-formalized-explanation-with-latex-math-and-pseudocode-9" name="p-51309-formalized-explanation-with-latex-math-and-pseudocode-9"></a>Formalized Explanation with LaTeX Math and Pseudocode</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-merkle-tree-initialization-and-verification-10" name="p-51309-merkle-tree-initialization-and-verification-10"></a>Merkle Tree Initialization and Verification</h3>
<h4><a class="anchor" href="https://ethresear.ch#p-51309-h-1-merkle-tree-basics-11" name="p-51309-h-1-merkle-tree-basics-11"></a>1. <strong>Merkle Tree Basics</strong></h4>
<ul>
<li>
<p><strong>Initialization</strong>: The Merkle tree is initialized with a set of leaves <span class="math">\{L_1, L_2, \ldots, L_n\}</span>, where each leaf <span class="math">L_i</span> represents some data (e.g., channel state).</p>
</li>
<li>
<p><strong>Computing the Root</strong>: The tree computes hashes up to the Merkle root <span class="math">R</span>:</p>
<div class="math">
R = H(\ldots H(H(L_1, L_2), H(L_3, L_4)), \ldots)
</div>
</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-51309-h-2-state-transition-proof-12" name="p-51309-h-2-state-transition-proof-12"></a>2. <strong>State Transition Proof</strong></h4>
<ul>
<li>
<p><strong>Updating a Leaf</strong>: For a state transition, the client computes:</p>
<ul>
<li>The updated leaf <span class="math">L_i'</span> based on the channel operation.</li>
<li>A proof <span class="math">P_i</span> consisting of sibling hashes needed to recompute the root.</li>
</ul>
</li>
<li>
<p><strong>New Root Computation</strong>:</p>
<div class="math">
R' = \text{RecomputeRoot}(L_i', P_i)
</div>
</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-51309-h-3-networks-knowledge-13" name="p-51309-h-3-networks-knowledge-13"></a>3. <strong>Network’s Knowledge</strong></h4>
<ul>
<li><strong>Stored Information</strong>: The network only has:
<ul>
<li>The initial root <span class="math">R_0</span> shared during wallet creation.</li>
<li>Transition rules encoded into the system (e.g., balance integrity, nonce increments).</li>
<li>The proof <span class="math">P_i</span> submitted with each updated root <span class="math">R'</span>.</li>
</ul>
</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-51309-h-4-network-verification-14" name="p-51309-h-4-network-verification-14"></a>4. <strong>Network Verification</strong></h4>
<ul>
<li>
<p><strong>Proof Validity</strong>: The network verifies that the proof recomputes <span class="math">R'</span> correctly from <span class="math">L_i'</span> and <span class="math">P_i</span>:</p>
<div class="math">
R' \stackrel{?}{=} \text{RecomputeRoot}(L_i', P_i)
</div>
</li>
<li>
<p><strong>State Transition Validity</strong>: Ensures that transition constraints hold true:</p>
<div class="math">
\text{Constraints: } \left\{
      \begin{aligned}
          &amp;L_i' = L_i - \text{amount} &amp;\text{(balance update)} \\
          &amp;\text{nonce}_{i}' = \text{nonce}_i + 1 &amp;\text{(nonce update)}
      \end{aligned}
  \right.
</div>
</li>
</ul>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-latex-formalization-15" name="p-51309-latex-formalization-15"></a>LaTeX Formalization</h3>
<p>Define:</p>
<ul>
<li><span class="math">R_0</span>: Initial Merkle root shared during wallet creation.</li>
<li><span class="math">R_k</span>: Merkle root after the <span class="math">k</span>-th state transition.</li>
<li><span class="math">L_k^{(i)}</span>: Leaf <span class="math">i</span> after the <span class="math">k</span>-th state transition.</li>
<li><span class="math">P_k^{(i)}</span>: Proof for leaf <span class="math">L_k^{(i)}</span> for the <span class="math">k</span>-th state transition.</li>
<li><span class="math">\text{RecomputeRoot}(L_k^{(i)}, P_k^{(i)})</span>: Function to recompute the root from the updated leaf and proof.</li>
<li><span class="math">C(L_k^{(i)}, L_{k+1}^{(i)})</span>: Constraints for valid transitions.</li>
</ul>
<p><strong>Verification by the Network</strong>:</p>
<ol>
<li>
<p><strong>Initial State</strong>:</p>
<div class="math">
R_0 = \text{RecomputeRoot}(\{L_1, \ldots, L_n\}, \{\})
</div>
</li>
<li>
<p><strong>State Transition</strong>:</p>
<ul>
<li>
<p>The network receives <span class="math">R_{k+1}</span>, <span class="math">P_k^{(i)}</span>, and <span class="math">L_{k+1}^{(i)}</span>.</p>
</li>
<li>
<p>Verifies:</p>
<div class="math">
R_{k+1} = \text{RecomputeRoot}(L_{k+1}^{(i)}, P_k^{(i)})
</div>
</li>
<li>
<p>Ensures that:</p>
<div class="math">
C(L_k^{(i)}, L_{k+1}^{(i)})
</div>
</li>
</ul>
</li>
</ol>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-pseudocode-algorithm-16" name="p-51309-pseudocode-algorithm-16"></a>Pseudocode Algorithm</h3>
<pre><code class="lang-pseudocode"># Network Initialization
R_current = R_0  # Initial root stored during wallet creation

# Verification Function
function verify_state_transition(new_root, proof, updated_leaf, constraints):
    computed_root = recompute_root(updated_leaf, proof)
    if computed_root != new_root:
        return False  # Proof is invalid
    if not constraints(updated_leaf):
        return False  # Transition violates rules
    return True  # Valid transition

# State Transition Process
function on_state_transition(new_root, proof, updated_leaf, constraints):
    if verify_state_transition(new_root, proof, updated_leaf, constraints):
        R_current = new_root  # Update the stored root
        return True
    else:
        return False
</code></pre>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-how-the-network-knows-what-to-expect-17" name="p-51309-how-the-network-knows-what-to-expect-17"></a>How the Network Knows What to Expect</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-establishing-the-initial-state-18" name="p-51309-establishing-the-initial-state-18"></a>Establishing the Initial State</h3>
<ul>
<li><strong>Initial Root (<span class="math">R_0</span>)</strong>: Provided during wallet creation, it serves as the baseline for all future validations.</li>
<li><strong>Cryptographic Integrity</strong>: <span class="math">R_0</span> encapsulates the entire wallet state, ensuring that all subsequent transitions start from a provable and tamper-proof origin.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-incremental-updates-19" name="p-51309-incremental-updates-19"></a>Incremental Updates</h3>
<ul>
<li><strong>Client Computations</strong>:
<ul>
<li>Generates an updated Merkle root <span class="math">R'</span> representing the new wallet state.</li>
<li>Creates a cryptographic proof <span class="math">P</span> showing how <span class="math">R'</span> was derived from the prior state.</li>
</ul>
</li>
<li><strong>Proof Components</strong>:
<ul>
<li>The updated leaf (<span class="math">L_i'</span>).</li>
<li>Sibling hashes required to recompute the new Merkle root.</li>
</ul>
</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-network-verification-process-20" name="p-51309-network-verification-process-20"></a>Network Verification Process</h3>
<ul>
<li>
<p><strong>Proof Verification</strong>:</p>
<ul>
<li>Recomputes the new root <span class="math">R'</span> using the submitted proof <span class="math">P</span> and verifies that it matches the provided <span class="math">R'</span>.</li>
</ul>
</li>
<li>
<p><strong>Constraints Validation</strong>:</p>
<ul>
<li>
<p>Ensures compliance with predefined constraints, such as balance integrity and nonce progression:</p>
<div class="math">
\begin{aligned}
    &amp;L_i' = L_i - \text{amount} \\
    &amp;\text{nonce}_{i}' = \text{nonce}_i + 1
    \end{aligned}
</div>
</li>
</ul>
</li>
<li>
<p><strong>Trustless Validation</strong>: Any deviation from expected results invalidates the transition, ensuring that the network can trustlessly validate updates without needing access to underlying data.</p>
</li>
</ul>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-key-benefits-21" name="p-51309-key-benefits-21"></a>Key Benefits</h2>
<ul>
<li><strong>Data Privacy</strong>: Only cryptographic proofs and Merkle roots are shared; the network never accesses private data.</li>
<li><strong>Scalability</strong>: Lightweight proofs and the elimination of consensus mechanisms allow for massive scalability.</li>
<li><strong>Security</strong>: Tampering or unauthorized modifications result in invalid roots, ensuring network integrity.</li>
<li><strong>Simplicity</strong>: The network acts as a passive verifier, reducing complexity by removing consensus protocols.</li>
<li><strong>Instant Finality</strong>: Transactions are finalized upon proof verification, enhancing user experience.</li>
</ul>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-why-this-works-so-well-22" name="p-51309-why-this-works-so-well-22"></a>Why This Works So Well</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-the-power-of-cryptographic-proofs-23" name="p-51309-the-power-of-cryptographic-proofs-23"></a>The Power of Cryptographic Proofs</h3>
<ul>
<li><strong>Merkle Proofs</strong>: Each state transition is verified using cryptographic Merkle proofs linking the current state (<span class="math">R</span>) to the new state (<span class="math">R'</span>).</li>
<li><strong>Tamper Detection</strong>: Any unauthorized modification results in a mismatch of the expected root, making fraud immediately detectable.</li>
<li><strong>Zero-Knowledge Proofs (Optional)</strong>: Can confirm compliance with rules (e.g., balance integrity) without exposing private data.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-unidirectional-information-flow-24" name="p-51309-unidirectional-information-flow-24"></a>Unidirectional Information Flow</h3>
<ul>
<li><strong>Simplified Communication</strong>: Information flows from the source of truth (network) to clients, maintaining strict control and eliminating bidirectional dependencies.</li>
<li><strong>Independent Validation</strong>: Clients independently prove their updates to the network, removing the need for consensus among multiple nodes.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-elimination-of-consensus-25" name="p-51309-elimination-of-consensus-25"></a>Elimination of Consensus</h3>
<ul>
<li><strong>No Validators Required</strong>: The network retains the initial root (<span class="math">R_0</span>) and validates transitions using cryptographic proofs instead of consensus algorithms.</li>
<li><strong>Reduced Latency</strong>: Transactions can be validated and finalized quickly, as there’s no need for agreement across nodes.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-privacy-by-design-26" name="p-51309-privacy-by-design-26"></a>Privacy by Design</h3>
<ul>
<li><strong>Client-Side Data Management</strong>: All underlying data (e.g., wallet or channel states) remains private on the client side.</li>
<li><strong>Minimal Network Exposure</strong>: The network validates transitions without processing or storing user data, preserving privacy.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-scalability-27" name="p-51309-scalability-27"></a>Scalability</h3>
<ul>
<li><strong>Parallel Processing</strong>: Nodes independently validate proofs in parallel, avoiding bottlenecks.</li>
<li><strong>Efficient Verification</strong>: Cryptographic proofs are computationally efficient to verify.</li>
<li><strong>Sparse Merkle Trees</strong>: Proof sizes remain logarithmic in the tree size, ensuring consistent performance as the system scales.</li>
</ul>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-impact-on-throughput-tps-transactions-per-second-28" name="p-51309-impact-on-throughput-tps-transactions-per-second-28"></a>Impact on Throughput (TPS - Transactions Per Second)</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-removal-of-consensus-bottlenecks-29" name="p-51309-removal-of-consensus-bottlenecks-29"></a>Removal of Consensus Bottlenecks</h3>
<ul>
<li><strong>No Global State Synchronization</strong>: Eliminating consensus removes latency and limits on scalability.</li>
<li><strong>Instant Finality</strong>: Transactions are finalized upon proof verification, significantly increasing throughput.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-lightweight-validation-with-cryptographic-proofs-30" name="p-51309-lightweight-validation-with-cryptographic-proofs-30"></a>Lightweight Validation with Cryptographic Proofs</h3>
<ul>
<li><strong>Efficiency</strong>: Verifying cryptographic proofs involves minimal computation.</li>
<li><strong>Parallelism</strong>: Each transaction’s proof can be validated independently, allowing for parallel processing.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-unilateral-channels-and-unidirectional-information-flow-31" name="p-51309-unilateral-channels-and-unidirectional-information-flow-31"></a>Unilateral Channels and Unidirectional Information Flow</h3>
<ul>
<li><strong>Streamlined Transaction Flow</strong>: Unilateral channels remove bidirectional dependencies, simplifying the system.</li>
<li><strong>Concurrent Updates</strong>: Multiple state transitions can occur in parallel without interference.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-client-side-operations-32" name="p-51309-client-side-operations-32"></a>Client-Side Operations</h3>
<ul>
<li><strong>Offloading to Clients</strong>: Clients handle most computational overhead, including generating proofs and updating local Merkle trees.</li>
<li><strong>Network Efficiency</strong>: The network focuses on verifying proofs and maintaining roots, reducing its workload.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51309-scalable-state-management-33" name="p-51309-scalable-state-management-33"></a>Scalable State Management</h3>
<ul>
<li><strong>Sparse Merkle Trees</strong>: Ensure proof sizes remain constant (logarithmic in tree size), supporting scalability.</li>
<li><strong>Consistent Performance</strong>: Maintains high throughput as the system grows.</li>
</ul>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51309-theoretical-tps-potential-34" name="p-51309-theoretical-tps-potential-34"></a>Theoretical TPS Potential</h3>
<ul>
<li><strong>No Theoretical Limit</strong>: Throughput is primarily limited by hardware capabilities.</li>
<li><strong>High Efficiency</strong>: Proof verification can be performed in microseconds, allowing for millions of transactions per second in ideal conditions.</li>
<li><strong>Practical Scalability</strong>: Even with conservative estimates, Overpass can achieve hundreds of thousands of TPS in real-world scenarios.</li>
</ul>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-comparison-with-traditional-systems-35" name="p-51309-comparison-with-traditional-systems-35"></a>Comparison with Traditional Systems</h2>
<div class="md-table">
<table>
<thead>
<tr>
<th>Feature</th>
<th>Traditional Blockchain</th>
<th>Overpass Channels</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Consensus</strong></td>
<td>Required (validators)</td>
<td>Not required</td>
</tr>
<tr>
<td><strong>Transaction Finality</strong></td>
<td>Minutes (block time)</td>
<td>Instant (proof verification)</td>
</tr>
<tr>
<td><strong>Network Bandwidth</strong></td>
<td>High (global state sync)</td>
<td>Low (proof propagation)</td>
</tr>
<tr>
<td><strong>Throughput (TPS)</strong></td>
<td>10–1,000</td>
<td>100,000+</td>
</tr>
<tr>
<td><strong>Scalability</strong></td>
<td>Limited by consensus</td>
<td>Linear (client activity)</td>
</tr>
</tbody>
</table>
</div><hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51309-conclusion-36" name="p-51309-conclusion-36"></a>Conclusion</h2>
<p>Overpass Channels fundamentally transform Layer 2 scalability by leveraging cryptographic proofs, unidirectional information flow, and the elimination of consensus mechanisms. This architecture achieves:</p>
<ul>
<li><strong>Privacy</strong>: Client data stays private, with only proofs and roots shared for verification.</li>
<li><strong>Scalability</strong>: Parallel validation and efficient proof verification support millions of transactions.</li>
<li><strong>Security</strong>: Tampering is immediately detectable, and privacy-preserving techniques enhance user trust.</li>
<li><strong>Simplicity</strong>: The network’s role is reduced to passive verification, eliminating consensus complexities.</li>
<li><strong>Instant Finality</strong>: Transactions are finalized as soon as their proofs are verified.</li>
</ul>
<p>By addressing the fundamental limitations of traditional blockchain systems, Overpass Channels emerge as the next-generation Layer 2 solution for Web3, capable of scaling to meet the demands of global decentralized applications while maintaining privacy and security.</p>
<hr />
            <p><small>3 posts - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/overpass-channels-the-key-to-unlock-a-new-era-for-all-of-web3/21046">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 19 Nov 2024 18:21:41 +0000</pubDate>
</item>
<item>
<title>Vocdoni Protocol: Enabling Decentralized Voting for the Masses with ZK Technology</title>
<link>https://ethresear.ch/t/vocdoni-protocol-enabling-decentralized-voting-for-the-masses-with-zk-technology/21036</link>
<guid>https://ethresear.ch/t/vocdoni-protocol-enabling-decentralized-voting-for-the-masses-with-zk-technology/21036</guid>
<content:encoded><![CDATA[
<div> 关键词：去中心化投票、零知识证明（zk）、zkSNARKs、椭圆曲线加密（ElGamal）、分布式密钥生成（DKG）

总结：
我们是一家专注于构建去中心化投票解决方案的公司，已成功为足球俱乐部、市政府等组织执行高风险投票。过去六年里，我们依赖自定义的权威证明Layer 1网络，现在计划转向基于零知识证明（zk）的基础设施以实现完全去中心化，并解决数字投票系统的挑战。为此，我们提出了一种新的通用投票协议，采用zkSNARKs和阈值同态加密（ElGamal），确保端到端可验证性和用户隐私。该系统利用基于Ethereum区块链的zkSNARK层来提供审查抵抗性、完整性、无信任操作以及透明的审计。

关键设计包括使用分布式密钥生成（DKG）协调智能合约，使序列器能安全、分散地创建加密密钥而不依赖中心机构。投票者通过友好的界面进行匿名、安全的投票，并生成zkSNARK证明其加密选票符合规则但不透露选择。序列器负责收集、验证选票并更新共享状态，以保持投票过程的完整性和防止双重投票。

新协议着重于可扩展性和易访问性，支持高频低成本投票，适用于大规模采用。预计将在2025年第一季度至第二季度推出测试网。我们正在使用Circom和SnarkJS实现投票者侧的组件，而序列器则利用Gnark和BLS12-377及BW6-761曲线实现原生递归投票聚合，产生可在Ethereum上验证的BN254证明。为了保证参与门槛低，序列器可以在具有64GB内存的普通CPU系统上运行。

我们的投票协议通过椭圆曲线上的ElGamal同态加密维护投票隐私，利用一组分散的序列器和默克尔树确保投票完整性，并通过重加密和投票覆盖机制实现收据不可追溯性，防范胁迫和贿选行为。此外，还引入了zkSNARK证明确保状态转换的有效性，从而实现全程透明审计。 <div>
<p>At <a href="https://blog.vocdoni.io" rel="noopener nofollow ugc">Vocdoni</a>, we’ve spent the last six years advancing decentralized voting solutions, focusing on bridging web2 applications with web3 technologies. We’ve successfully executed high-stakes voting for organizations such as football clubs, city councils, associations, political parties, professional bodies, movements under prosecution and more.</p>
<p>Until today, we have been relying on a customized proof-of-authority Layer 1 network.<br />
But we believe it’s now time to transition to a zero-knowledge (zk) based infrastructure to achieve <strong>full decentralization</strong> and address the main challenges of digital voting systems</p>
<p>On the road to this new protocol implementation, we would like to receive feedback from the Ethereum community <img alt=":heartbeat:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/heartbeat.png?v=12" title=":heartbeat:" width="20" />.</p>
<hr />
<p>By taking ideas from our expertise, MACI, and others. We introduce a new universal voting protocol that tackles critical issues like receipt-freeness, voter privacy, scrutiny transparency, universal auditing, and eliminating the need for a trusted coordinator.</p>
<p>Designed for scalability and accessibility, the system enables high-frequency, low-cost voting suitable for mass adoption.  We laverage on zkSNARKs and threshold homomorphic encryption (ElGamal), to ensure end-to-end verifiability and anonymity for the end user.</p>
<p>A decentralized zkSNARK-based state machine, operating as specialized layer 2 on the Ethereum blockchain, provides censorship-resistance, integrity, trutless operation and a transparent scrutiny of results. A distributed key generation (DKG) among sequencers, coordinated via smart contracts, allows for secure and decentralized encryption key creation without reliance on a central authority.</p>
<p>Most components have been implemented using accessible technologies and have undergone proof-of-concept testing, confirming that the protocol is practical and ready for immediate deployment. We plan to launch the testnet in Q1–Q2 2025.</p>
<p>Our implementation uses Circom and SnarkJS on the voter side, enabling voting from any device, including smartphones and web browsers. For the sequencers, we use Gnark with curves BLS12-377 and BW6-761 for native recursion in vote aggregation. This setup produces a final BN254 proof that can be verified on Ethereum.</p>
<p>Focusing on decentralization, we designed the sequencer to operate on accessible machines—CPU-based systems with 64 GiB of memory—so that participation doesn’t require specialized hardware.</p>
<hr />
<h1><a class="anchor" href="https://ethresear.ch#p-51289-actors-1" name="p-51289-actors-1"></a>Actors</h1>
<p><strong>Organizers</strong> set up and manage voting processes, defining parameters such as voting options, duration, and voter registries (census).</p>
<p><strong>Voters</strong> interact with the system through user-friendly interfaces, enabling them to cast their votes securely and privately. Voters generate zkSNARKs to prove that their encrypted votes comply with voting rules without revealing their choices.</p>
<p><strong>Sequencers</strong> are specialized nodes responsible for collecting votes, verifying their validity, and updating the shared state. They participate in the Distributed Key Generation (DKG) protocol to collaboratively generate encryption keys without any single party controlling the private key.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/7/a7aa54df4abda59985545fef1ed402d388278ff8.png" title="image"><img alt="image" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/a/7/a7aa54df4abda59985545fef1ed402d388278ff8_2_642x500.png" width="642" /></a></div><p></p>
<h1><a class="anchor" href="https://ethresear.ch#p-51289-properties-2" name="p-51289-properties-2"></a>Properties</h1>
<p><strong>Privacy</strong> is maintained using homomorphic encryption. Votes are encrypted with the ElGamal cryptosystem over elliptic curves, allowing the aggregation of encrypted votes without decrypting individual ballots, thus keeping voter choices confidential.</p>
<p><strong>Integrity</strong> is ensured through the collaborative efforts of a <strong>decentralized network</strong> of sequencers, who maintain a shared state represented by a Merkle tree that summarizes the current status of the voting process, including accumulated votes and nullifiers to prevent double voting. Each time the state is updated with new votes, a sequencer generates a zkSNARK proof attesting to the validity of the state transition.</p>
<p><strong>Receipt-freeness</strong> is achieved by preventing voters from being able to prove to third parties how they voted, mitigating risks of coercion and vote-buying. This is accomplished through ballot re-encryption and vote overwriting mechanisms. When a voter submits an encrypted ballot, sequencers re-encrypt it before storing it in the state, making it computationally infeasible to link the original and re-encrypted ciphertexts. Voters are also allowed to overwrite their votes; if a voter casts a new vote, the sequencer subtracts the previous encrypted vote from the tally and adds the new one. Regular re-encryption of random ballots by sequencers conceals when overwrites occur, enhancing receipt-freeness by making it indistinguishable whether a ballot was overwritten or simply re-randomized.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51289-end-to-end-workflow-3" name="p-51289-end-to-end-workflow-3"></a>End-to-End Workflow</h1>
<ol>
<li><strong>Process Initialization</strong>: The organizer defines the voting parameters—including options, duration, and census—and registers the new process on the Ethereum blockchain via smart contracts.</li>
<li><strong>Distributed Key Generation (DKG)</strong>: Sequencers collaboratively execute the DKG protocol to generate the collective public encryption key without any single party knowing the private key. The generated public key is published on-chain for voters to use when encrypting their ballots.</li>
<li><strong>Voter Preparation</strong>: Voters retrieve the public encryption key and their inclusion proof (Merkle proof) from the census registry.</li>
<li><strong>Vote Casting</strong>: Voters select their choices and encrypt their ballots using the public key. Generate zkSNARK proofs to prove the validity of their encrypted votes without revealing their selections. Encrypted ballots and proofs are submitted to a sequencer for processing.</li>
<li><strong>Vote Collection and Verification</strong>: Sequencers verify the zkSNARK proofs to ensure each vote complies with the protocol rules. Confirm that the voter is eligible and has not already voted, or handle vote overwrites appropriately. Valid encrypted votes are aggregated homomorphically, allowing for tallying without decryption. The sequencer updates the State Merkle Tree to reflect new votes and nullifiers.</li>
<li><strong>State Transition and Proof Submission</strong>: Sequencers create zkProofs attesting to the validity of the state transition from the previous state to the new one. The new root and corresponding proof are submitted to the Ethereum smart contract. The contract verifies the data and updates the stored state root.</li>
<li><strong>Data Availability</strong>: Necessary data to reconstruct the state is published to the data availability layer (Ethereum data blobs), ensuring accessibility for verification and reconstruction of the new state.</li>
<li><strong>Process Finalization</strong>: At the end of the voting period, the process is finalized on-chain, and no further votes are accepted.</li>
<li><strong>Result Decryption</strong>: Sequencers collaborate to decrypt the aggregated vote totals using the threshold decryption protocol. Decrypted results are published on-chain, providing an immutable and transparent outcome.</li>
</ol>
<h1><a class="anchor" href="https://ethresear.ch#p-51289-threshold-homomorphic-encryption-4" name="p-51289-threshold-homomorphic-encryption-4"></a>Threshold Homomorphic Encryption</h1>
<p>The system utilizes the ElGamal threshold encryption scheme over the elliptic curve bn254, which provides additive homomorphic properties essential for securely aggregating votes.</p>
<p><strong>Encryption</strong></p>
<p>A voter’s choice is represented as a message <span class="math">m \in \mathbb{Z}_q</span>. To encrypt the vote:</p>
<ol>
<li>The voter encodes the message as a point on the elliptic curve: <span class="math">M = m G.</span></li>
<li>The voter selects a random scalar <span class="math">k \in \mathbb{Z}_q^*</span>.</li>
<li>The ciphertext is computed as: <span class="math">C = (C_1, C_2) = (k G, M + k H).</span></li>
</ol>
<p><strong>Homomorphic Addition</strong></p>
<p>The ElGamal cryptosystem over elliptic curves supports additive homomorphism for messages represented as points. Given two ciphertexts <span class="math">(C_1^{(1)}, C_2^{(1)})</span> and <span class="math">(C_1^{(2)}, C_2^{(2)})</span>, their component-wise addition yields:</p>
<ul>
<li><span class="math">C_1^{(\text{sum})} = C_1^{(1)} + C_1^{(2)}</span></li>
<li><span class="math">C_2^{(\text{sum})} = C_2^{(1)} + C_2^{(2)}</span></li>
</ul>
<p>The aggregated ciphertext decrypts to the sum of the messages:  <span class="math">M^{(\text{sum})} = M_1 + M_2</span></p>
<p><strong>Threshold Decryption</strong></p>
<p>After the voting period ends, the sequencers collaboratively decrypt the aggregated ciphertext. Each sequencer <span class="math">P_j</span> computes a partial decryption share:</p>
<ol>
<li>Compute:  <span class="math">D_j = s_j C_1.</span></li>
<li>The partial decryptions are combined using Lagrange interpolation coefficients <span class="math">\lambda_j</span>:<br />
<span class="math">D = \sum_{j \in T} \lambda_j D_j = s C_1</span><br />
where <span class="math">T</span> is a set of at least <span class="math">t</span> sequencers.</li>
<li>The plaintext message is recovered by computing:<br />
<span class="math">M = C_2 - D = M + k H - s k G = M</span></li>
<li>The final result <span class="math">m</span> is obtained by solving <span class="math">M = m G</span>, which yields <span class="math">m</span>.</li>
</ol>
<h1><a class="anchor" href="https://ethresear.ch#p-51289-distributed-key-generation-dkg-5" name="p-51289-distributed-key-generation-dkg-5"></a>Distributed Key Generation (DKG)</h1>
<p>To eliminate the need for a trusted authority, the encryption key used for ballot encryption is generated collaboratively by the sequencers through a Distributed Key Generation protocol, which proceeds as follows:</p>
<ol>
<li><strong>Initialization</strong>: Let <span class="math">G</span> be the generator of an elliptic curve group of prime order <span class="math">q</span>. The threshold <span class="math">t</span> and the number of sequencers <span class="math">n</span> are predefined, with <span class="math">t \leq n</span>.</li>
<li><strong>Secret Sharing</strong>: Each sequencer <span class="math">P_i</span> randomly selects a secret polynomial <span class="math">f_i(x)</span> of degree <span class="math">t - 1</span>, where <span class="math">f_i(0) = a_{i,0}</span>, and the coefficients <span class="math">a_{i,j}</span> are chosen uniformly at random from <span class="math">\mathbb{Z}_q</span>.</li>
<li><strong>Commitments</strong>: Each sequencer publishes commitments to their polynomial coefficients: <span class="math">C_{i,j} = a_{i,j} G \quad \text{for} \quad j = 0, \ldots, t - 1.</span></li>
<li><strong>Share Distribution</strong>: Sequencer <span class="math">P_i</span> computes shares for every other sequencer <span class="math">P_j</span>: <span class="math">s_{i,j} = f_i(j), \quad \text{for} \quad j = 1, \ldots, n</span><br />
These shares are securely transmitted to the respective sequencers using a simplified version of ECIES.</li>
<li><strong>Verification</strong>: Each sequencer <span class="math">P_j</span> verifies the received shares <span class="math">s_{i,j}</span> by checking: <span class="math">s_{i,j} G \stackrel{?}{=} \sum_{k=0}^{t - 1} C_{i,k} \cdot j^k</span></li>
<li><strong>Private Key Share Computation</strong>: Each sequencer computes their private key share: <span class="math">s_j = \sum_{i=1}^n s_{i,j} \mod q</span></li>
<li><strong>Public Key Computation</strong>: The collective public key is computed as: <span class="math">H = \sum_{i=1}^n C_{i,0} = s G</span>, where <span class="math">s = \sum_{i=1}^n a_{i,0} \mod q</span> is the aggregate private key known only in a distributed form among the sequencers.</li>
</ol>
<h1><a class="anchor" href="https://ethresear.ch#p-51289-the-vote-6" name="p-51289-the-vote-6"></a>The Vote</h1>
<p>The vote consists of several components and mechanisms listed below.</p>
<p><strong>Process Identifier</strong>: A unique identifier <span class="math">\text{ProcessId}</span> for the voting process. This ensures that votes are correctly associated with the specific voting event and prevents cross-process interference.</p>
<p><strong>Census Proof</strong>: A Merkle proof demonstrating the voter’s inclusion in the authorized voter registry (census). This proof allows the sequencers to verify the voter’s eligibility without revealing the entire voter list, preserving privacy and efficiency.</p>
<p><strong>Identity Commitment</strong>: The voter computes a commitment using a cryptographic hash function: <span class="math">C = Hash(\text{Address} \parallel \text{ProcessId} \parallel s)</span>, where <span class="math">H</span> is a cryptographic hash function, <span class="math">\text{Address}</span> is the voter’s unique identifier (such as a public key or an address), and <span class="math">s</span> is a secret known only to the voter.</p>
<p>By incorporating the secret <span class="math">s</span> into the commitment <span class="math">C</span>, we effectively detach the nullifier from any direct association with the voter’s identity in the publicly accessible data. This means that even if future quantum computing advancements were to compromise the ElGamal encryption and reveal the contents of encrypted ballots, there would be no practical method to link a decrypted ballot back to a specific voter’s address using the nullifier.</p>
<p><strong>Nullifier</strong>: To prevent double voting and handle vote overwriting, the voter computes a nullifier: <span class="math">N = Hash(C \parallel s)</span>. The nullifier acts as a one-time token that uniquely represents the voter’s participation without revealing their identity.</p>
<p>By avoiding the use of the private key or deterministic signatures when computing the nullifier, we ensure compatibility with hardware wallets and non-deterministic signature schemes.</p>
<p><strong>Ballot</strong>: The ballot represents the voter’s choices as an array of individual selections that must adhere to the ballot protocol rules defined by the organizer. This flexible approach allows the protocol to support various voting configurations, including range voting, ranking, quadratic voting and more.</p>
<p>Suppose an organizer wants to implement a quadratic voting system with the following parameters:</p>
<ul>
<li>Maximum Selections: Up to 5 options can be selected.</li>
<li>Value Range: Each selection must be a non-negative integer.</li>
<li>Total Cost Constraint: The sum of the squares of the selections must not exceed a budget of 100 credits.</li>
<li>Unique Values Constraint: Duplicate values are allowed.</li>
</ul>
<p>The protocol would enforce that for a ballot <span class="math">\mathbf{m} = (m_1, m_2, \ldots, m_5)</span>:</p>
<ul>
<li>Each <span class="math">m_i \geq 0</span>.</li>
<li><span class="math">\sum_{i=1}^5 m_i^2 \leq 100</span>.</li>
</ul>
<p><strong>Encryped Ballot</strong>: The voter encrypts their ballot using the homomorphic ElGamal encryption scheme, proceeding as follows:</p>
<ul>
<li>The voter’s choices are encoded into a message vector <span class="math">\mathbf{m} = (m_1, m_2, \ldots, m_n)</span>, where each <span class="math">m_i</span> corresponds to a selection in the ballot array.</li>
<li>Each element <span class="math">m_i</span> is encoded as a point on the elliptic curve: <span class="math">M_i = m_i G</span>, where <span class="math">G</span> is the generator of the curve.</li>
<li>The voter selects a random scalar <span class="math">k \in \mathbb{Z}_q^*</span>, where <span class="math">q</span> is the order of the curve.</li>
<li>The ciphertext is computed as: <span class="math">C = (C_1, C_2) = \left( k G,\; \sum_{i=1}^n M_i + k H \right)</span><br />
where <span class="math">H</span> is the public key obtained from the Distributed Key Generation protocol.</li>
</ul>
<p><strong>Zero-Knowledge Proof (zkSNARK)</strong>: The voter generates a zkSNARK that proves, without revealing the ballot content, that:</p>
<ul>
<li>Correct Encryption: The encrypted ballot is correctly formed according to the encryption scheme.</li>
<li>Protocol Compliance: The voter’s selections adhere to the ballot protocol rules defined by the organizer.</li>
<li>Correct Nullifier and Commitment: The nullifier and commitment are correctly computed using the voter’s secret <span class="math">s</span> and address.</li>
<li>Knowledge of Secrets: The voter knows the secret <span class="math">s</span> and the random scalar <span class="math">k</span> used in encryption.</li>
</ul>
<p><strong>Signature</strong>: The voter signs necessary components of the vote using their private key associated with their address. This authenticates the vote and binds it to the voter’s identity in a verifiable manner. Many signature schemes may be supported (ECDSA, EdDSA, RSA, etc.).</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51289-state-transitions-7" name="p-51289-state-transitions-7"></a>State Transitions</h1>
<p>The <strong>State Merkle Tree</strong> is a cryptographic data structure that allows efficient and secure verification of the data it contains. The tree is structured to store various types of information at predefined indices or addresses:</p>
<ul>
<li><strong>Process Parameters</strong>: Stored at static indices, containing essential information such as the <span class="math">\text{ProcessId}</span>, the root of the census Merkle tree (<span class="math">\text{censusRoot}</span>), ballot protocol configurations, and the public encryption key <span class="math">H</span> generated through the DKG protocol.</li>
<li><strong>Results Accumulators</strong>: Two accumulators are maintained to handle vote additions and subtractions:
<ul>
<li><strong>Addition Accumulator</strong> (<span class="math">C_{\text{add}}</span>): Stores the homomorphically aggregated encrypted votes that have been added to the tally.</li>
<li><strong>Subtraction Accumulator</strong> (<span class="math">C_{\text{sub}}</span>): Stores the homomorphically aggregated encrypted votes that have been subtracted due to vote overwrites.</li>
</ul>
</li>
<li><strong>Nullifiers</strong>: Stored to prevent double voting. Each nullifier <span class="math">N</span> is associated with a voter’s commitment and is unique to that voter for the specific voting process.</li>
<li><strong>Commitments</strong>: Stored to keep track of voter participation and to facilitate vote overwriting.</li>
</ul>
<p>Sequencers are responsible for processing new votes and updating the shared state. Each state transition involves the following steps:</p>
<ol>
<li>
<p><strong>Batch Collection of Votes</strong>: The sequencer collects a batch of up to <span class="math">N</span> new votes from voters. Batching votes enhances efficiency and scalability, allowing the sequencer to process multiple votes simultaneously. The value of <span class="math">N</span> is determined by system parameters that balance computational constraints and network throughput.</p>
</li>
<li>
<p><strong>Verification of Votes</strong>: For each vote in the batch, the sequencer performs:</p>
<ul>
<li><strong>zkSNARK Proof Verification</strong>: Ensures that the zero-knowledge proof submitted with each vote is valid and that the vote complies with the protocol rules, including correct encryption, adherence to ballot protocol constraints, and proper computation of the nullifier and commitment.</li>
<li><strong>Eligibility Check</strong>: Verifies the voter’s eligibility by checking the provided census Merkle proof against the stored <span class="math">\text{censusRoot}</span> in the state.</li>
<li><strong>Double Voting Prevention</strong>: Checks whether the nullifier <span class="math">N</span> already exists in the state. If it does not, the vote is processed as a new vote. If it does, the vote is considered a <strong>vote overwrite</strong>.</li>
</ul>
</li>
<li>
<p><strong>Handling Vote Overwrites</strong>: If a voter submits a new vote with the same nullifier <span class="math">N</span>, the sequencer:</p>
<ul>
<li>Subtracts the previous encrypted vote from the subtraction accumulator <span class="math">C_{\text{sub}}</span>.</li>
<li>Adds the new encrypted vote to the addition accumulator <span class="math">C_{\text{add}}</span>.</li>
<li>Replaces the new encrypted ballot within the State.</li>
</ul>
</li>
<li>
<p><strong>Random Re-encryptions</strong>: To enhance receipt-freeness and prevent linkage between votes and voters, the sequencer performs random re-encryptions of existing encrypted ballots:</p>
<ul>
<li>Selects a random subset of encrypted ballots in the state.</li>
<li>Re-encrypts each selected ballot by adding an encryption of zero, using a new random scalar.</li>
<li>Updates the encrypted ballots in the state with the re-encrypted versions.</li>
</ul>
</li>
<li>
<p><strong>Homomorphic Aggregation of Votes</strong>: The sequencer updates the accumulators using the homomorphic properties of the ElGamal encryption:</p>
</li>
<li>
<p><strong>Generation of State Transition zkSNARK</strong>: The sequencer generates a zkSNARK proof that attests to the validity of the state transition from the previous root <span class="math">\text{Root1}</span> to the new root <span class="math">\text{Root2}</span>. The zkSNARK proof verifies all previous constraints and operations.</p>
</li>
<li>
<p><strong>On-Chain Submission</strong>: The sequencer submits:, the updated state root <span class="math">\text{Root2}</span>. The proof attesting to the validity of the state transition. A hash commitment to the data blob containing the votes and state updates, ensuring data availability.</p>
</li>
</ol>
<h1><a class="anchor" href="https://ethresear.ch#p-51289-the-vocdoni-token-voc-8" name="p-51289-the-vocdoni-token-voc-8"></a>The Vocdoni Token (VOC)</h1>
<p>Vocdoni introduces a new token (VOC) to align incentives among participants and ensure the sustainability of the decentralized voting ecosystem. The token serves multiple essential functions: it incentivizes sequencers, facilitates payments for voting processes, and enables decentralized governance.</p>
<p>Sequencers must stake tokens as collateral to participate in the network, promoting honest behavior and network security. They earn rewards in VOC tokens based on their contributions to processing valid votes and maintaining the network’s integrity.</p>
<p>Organizers use tokens to pay for creating and managing voting processes. The costs depend on factors such as the maximum number of votes (<span class="math">\text{maxVotes}</span>), the voting duration (<span class="math">\text{processDuration}</span>), and the desired security level, which relates to the number of participating sequencers.</p>
<p>The total cost for a voting process is calculated using the formula:</p>
<p><span class="math">\text{totalCost} = \text{baseCost} + \text{capacityCost} + \text{durationCost} + \text{securityCost}</span></p>
<p><strong>Components of the Cost:</strong></p>
<ul>
<li>
<p><strong>Base Cost:</strong> <span class="math">\text{baseCost} = \text{fixedCost} + \text{maxVotes} \cdot p</span>, where <span class="math">\text{fixedCost}</span> is a fixed fee, and <span class="math">p</span> is a small linear factor.</p>
</li>
<li>
<p><strong>Capacity Cost:</strong> <span class="math">\text{capacityCost} = k_1 \left( \frac{\text{totalVotingProcesses}}{\text{totalSequencers} - \text{usedSequencers} + \epsilon} \cdot \text{maxVotes} \right)^a</span>, where <span class="math">k_1</span> is a scaling factor, <span class="math">a</span> controls non-linearity, and <span class="math">\epsilon</span> is a small number to prevent division by zero.</p>
</li>
<li>
<p><strong>Duration Cost:</strong> <span class="math">\text{durationCost} = k_2 \cdot \text{processDuration}^b</span>, with <span class="math">k_2</span> as a scaling factor and <span class="math">b</span> controlling the scaling based on duration.</p>
</li>
<li>
<p><strong>Security Cost:</strong> <span class="math">\text{securityCost} = k_3 \cdot e^{c \left( \frac{\text{numSequencers}}{\text{totalSequencers}} \right)^d}</span>, where <span class="math">k_3</span> is a scaling factor, and <span class="math">c</span>, <span class="math">d</span> control the exponential scaling related to the number of sequencers used.</p>
</li>
</ul>
<br />
<p><strong>Sequencers earn rewards</strong> based on the number of votes processed and the number of vote rewrites (including re-encryptions for receipt-freeness). The total reward for a sequencer <span class="math">i</span> is calculated as:</p>
<p><span class="math">\text{sequencerReward}_i = R \left( \frac{\text{votes}_i}{\text{maxVotes}} \right) + W \left( \frac{\text{voteRewrites}_i}{\text{totalRewrites}} \right)</span></p>
<p>This is subject to the constraints:</p>
<p><span class="math">\frac{\text{voteRewrites}_i}{\text{votes}_i} \leq T</span> and <span class="math">R &gt; W</span></p>
<p>Ensuring that sequencers prioritize processing new votes over rewrites. Here, <span class="math">R</span> and <span class="math">W</span> are parts of the reward pool allocated for processing votes and vote rewrites, respectively, and <span class="math">T</span> is a predefined constant limiting the number of rewrites per vote.</p>
<p>Sequencers who fail to meet their obligations may have their collateral slashed, calculated as:</p>
<p><span class="math">\text{SlashedAmount}_i = s \cdot \text{StakedCollateral}_i</span></p>
<p>where <span class="math">s</span> is the slashing coefficient (<span class="math">0 \leq s \leq 1</span>).</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51289-resources-9" name="p-51289-resources-9"></a>Resources</h1>
<ul>
<li>
<p>Full version of the whitepaper: <a href="https://whitepaper.vocdoni.io" rel="noopener nofollow ugc">https://whitepaper.vocdoni.io</a></p>
</li>
<li>
<p>List of repositories where the MVP version of the protocol is being implemented:</p>
<ul>
<li>Circom circuits for voter <a class="inline-onebox" href="https://github.com/vocdoni/z-ircuits" rel="noopener nofollow ugc">GitHub - vocdoni/z-ircuits: Vocdoni Z snark circuits.</a></li>
<li>Sequencer crypto primitives <a class="inline-onebox" href="https://github.com/vocdoni/gnark-crypto-primitives" rel="noopener nofollow ugc">GitHub - vocdoni/gnark-crypto-primitives: A set of custom circuits writted in Gnark that are required to support anonymous voting on Vocdoni.</a></li>
<li>Circom to Gnark parser <a class="inline-onebox" href="https://github.com/vocdoni/circom2gnark" rel="noopener nofollow ugc">GitHub - vocdoni/circom2gnark: Circom to Gnark Groth16 parser and recursion example</a></li>
<li>ElGamal and DKG sandbox: <a class="inline-onebox" href="https://github.com/vocdoni/vocdoni-z-sandbox" rel="noopener nofollow ugc">GitHub - vocdoni/vocdoni-z-sandbox</a></li>
</ul>
</li>
<li>
<p>Contact: pau [at] vocdoni [dot] io</p>
</li>
<li>
<p>Discord: <a href="https://chat.vocdoni.io" rel="noopener nofollow ugc">https://chat.vocdoni.io</a></p>
</li>
</ul>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/vocdoni-protocol-enabling-decentralized-voting-for-the-masses-with-zk-technology/21036">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 19 Nov 2024 12:01:04 +0000</pubDate>
</item>
<item>
<title>Proving multi-client Beam chain</title>
<link>https://ethresear.ch/t/proving-multi-client-beam-chain/21027</link>
<guid>https://ethresear.ch/t/proving-multi-client-beam-chain/21027</guid>
<content:encoded><![CDATA[
<div> 关键词: zkVM, 客户端语言, Nethermind, Teku/Besu, 网络协议, 证明大小, 性能, 出块时间, 证明聚合

总结:
关于zkVM团队对Beam链提案的支持，文章提及了以下几个要点：

1. 客户端实现将支持多种语言，包括C、Nim（通过C）、Rust，并计划增加Go、WASM（用于JavaScript客户端）以及Zig支持。同时询问Nethermind和Teku/Besu团队是否打算构建Beam客户端，他们是否会使用C#和Java。

2. 在网络协议方面，文章未明确提到具体的网络协议内容，但询问了目标证明大小是多少。

3. 对于性能需求，计划基准测试Grandine、Lighthouse和Nimbus当前状态转换函数的速度。考虑到若出块时间变为4秒，是否应将证明聚合在一个完整的“时代”（epoch）中进行。

请注意：以上信息为原文摘要，并非具体实施方案或最终决定。 <div>
<p>Hello teams,</p>
<p>I have a couple questions to make sure zkVM teams support the Beam chain proposal as best as possible.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51269-implementation-1" name="p-51269-implementation-1"></a>Implementation</h2>
<p>What languages will clients be built in. In our case at Lita/Valida, we can support C, Nim (through C), Rust. And we plan to add Go, WASM (for Javascript clients) and Zig support (seems to be a popular new language that Eth devs want to build in).</p>
<p>Are Nethermind and Teku/Besu team planning to build a Beam Client, will it be in C# and Java? What about LambdaClass and Elixir?</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51269-networking-2" name="p-51269-networking-2"></a>Networking</h2>
<p>What proof sizes are we targeting?</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51269-performance-3" name="p-51269-performance-3"></a>Performance</h2>
<p>What proving speed do we need? We plan to add some benchmarks of the current state transition function of Grandine, Lighthouse and Nimbus to get a baseline.</p>
<p>If block times become 4s, should the proof be aggregated over a whole “epoch”</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/proving-multi-client-beam-chain/21027">Read full topic</a></p>
]]></content:encoded>
<pubDate>Mon, 18 Nov 2024 11:43:02 +0000</pubDate>
</item>
<item>
<title>On Blob Markets, Base Fee Adjustments and Optimizations</title>
<link>https://ethresear.ch/t/on-blob-markets-base-fee-adjustments-and-optimizations/21024</link>
<guid>https://ethresear.ch/t/on-blob-markets-base-fee-adjustments-and-optimizations/21024</guid>
<content:encoded><![CDATA[
<div> 关键词: Blob Markets、Base Fee Adjustments、Pectra、Ethereum、Price Discovery

总结:

Blob Markets 是以太坊引入的一种新的费用市场机制，通过设置Blob基础费用来为数据提供专用空间，使得应用和Rollups无需执行EVM即可发布信息。随着需求增加，目前提议对Blob费用机制进行一系列调整：

1. 提高最小Blob基础费用：加快价格发现过程，减少波动，但需要重置累积的超额气体并可能导致短期的价格重新调整。
2. 自动化Blob基础费用更新比例：根据目标Blob数量动态调整更新比例，确保±12.5%的变化率不变，提高未来适应性。
3. 正常化超额气体计算：避免因目标Blob数量增加而导致的基础费用突然下降，增强费用调整的可预测性和平滑度。
4. 对称化Blob基础费用更新：确保在不同目标和最大值设定下，基础费用增减幅度保持一致。

建议在将目标/最大Blob数量从3/6调整至4/6或更高级别的同时，实施这些更改。需要注意的是，这些调整可能是微优化，需权衡是否改变尚未出现问题的现有协议。 <div>
<h1><a class="anchor" href="https://ethresear.ch#p-51263-on-blob-markets-base-fee-adjustments-and-optimizations-1" name="p-51263-on-blob-markets-base-fee-adjustments-and-optimizations-1"></a>On Blob Markets, Base Fee Adjustments and Optimizations</h1>
<blockquote>
<p>Special thanks to <a href="https://x.com/adietrichs">Ansgar</a>, <a href="https://x.com/barnabemonnot">Barnabé</a>, <a href="https://x.com/ralexstokes">Alex</a>, <a href="https://x.com/gakonst">Georgios</a>, <a href="https://x.com/r_krasiuk">Roman</a> and <a href="https://x.com/dankrad">Dankrad</a> for their input and discussions, as well as <a href="https://x.com/BertKellerman">Bert</a>, <a href="https://x.com/Gajpower">Gajinder</a> and <a href="https://x.com/MaxResnick1">Max</a> for their efforts on this topic!</p>
</blockquote>
<p><strong>The tl;dr:</strong><br />
All the suggested updates make sense in theory. Should we do them in Pectra - depends.</p>
<ol>
<li>Raising the min blob fee allows faster price discovery.</li>
<li>Automating the blob gas update fraction makes it future-proof.</li>
<li>Normalizing the excess gas prevents an edge gas where the blob base fee drops after a fork that increases the target, though, nothing bad can happen if we don’t do it.</li>
<li>Making the base fee scaling symmetric ensures the mechanism stays as-is (scales <span class="math">\pm 12.5%</span> at the extremes of 0 and 6 blobs)</li>
</ol>
<hr />
<p><strong>I would propose to summarize all those changes in <a href="https://github.com/ethereum/EIPs/blob/16a12dcf41251c592f74042b6aa5727097f60167/EIPS%2Feip-7762.md">EIP-7762</a> and ship them together when we change the target/max from 3/6 to 4/6. If we’re afraid that Pectra might grow too big with adding yet more changes, I’d propose them in the following order with decreasing importance:</strong></p>
<ol>
<li>Increase blob count.<br />
a) Do 4/6, being conservative, or something like 6/9 if we feel more confident.<br />
b) Ship <a href="https://github.com/ethereum/EIPs/blob/16a12dcf41251c592f74042b6aa5727097f60167/EIPS/eip-7623.md">EIP-7623</a> to ensure the EL payload size is significantly reduced to make room for more blobs.</li>
<li>Ship the outlined base fee changes.</li>
</ol>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51263-recap-of-the-blob-fee-mechanism-2" name="p-51263-recap-of-the-blob-fee-mechanism-2"></a>Recap of the Blob Fee Mechanism</h3>
<p>With the launch of <a href="https://github.com/ethereum/EIPs/blob/16a12dcf41251c592f74042b6aa5727097f60167/EIPS%2Feip-4844.md">EIP-4844</a>, Ethereum added a new dimension to its fee market. Blobs, coming with their own base fee, provide dedicated space for data, allowing applications and rollups to post information on-chain without requiring EVM execution.<br />
The blob fee structure is governed by a base fee update rule, which approximates the formula:</p>
<pre><code class="lang-python">base_fee_per_blob_gas = MIN_BASE_FEE_PER_BLOB_GAS * e**(excess_blob_gas / BLOB_BASE_FEE_UPDATE_FRACTION)
</code></pre>
<p>In this equation, <code>excess_blob_gas</code> represents the total surplus of blob gas usage compared to the target amount (<code>TARGET_BLOB_GAS_PER_BLOCK</code> per block). Like the <a href="https://github.com/ethereum/EIPs/blob/16a12dcf41251c592f74042b6aa5727097f60167/EIPS%2Feip-1559.md">EIP-1559</a> fee mechanism, this formula is self-adjusting: as the excess blob gas increases, the <code>base_fee_per_blob_gas</code> rises exponentially, which discourages excessive usage and nudges the excess back toward a level at which rollups perceive the base fee as a “fair” price.</p>
<p>The process operates as follows: if block <code>N</code> consumes <code>X</code> blob gas, then in block <code>N+1</code>, the <code>excess_blob_gas</code> increases by <code>X - TARGET_BLOB_GAS_PER_BLOCK</code>. Consequently, the <code>base_fee_per_blob_gas</code> for block <code>N+1</code> adjusts by a factor of <code>e**((X - TARGET_BLOB_GAS_PER_BLOCK) / BLOB_BASE_FEE_UPDATE_FRACTION)</code>.</p>
<p>The parameter <code>BLOB_BASE_FEE_UPDATE_FRACTION</code> controls the maximum possible rate of change for the blob base fee. This rate is set to achieve a target maximum increase and decrease of approximately <code>1.125</code> per block, based on <code>e**(TARGET_BLOB_GAS_PER_BLOCK / BLOB_BASE_FEE_UPDATE_FRACTION)</code>.</p>
<p>In the initial rollout, blob prices were expected to be low, with gradual increases until the market finds an equilibrium or “fair” price (i.e., price discovery). The blob fee market introduced by EIP-<a href="https://github.com/ethereum/EIPs/blob/16a12dcf41251c592f74042b6aa5727097f60167/EIPS%2Feip-4844.md">4844</a> follows a structure similar to <a href="https://github.com/ethereum/EIPs/blob/16a12dcf41251c592f74042b6aa5727097f60167/EIPS%2Feip-1559.md">EIP-1559</a>, with a base fee that adjusts dynamically based on demand.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-51263-reaching-the-point-of-price-discovery-3" name="p-51263-reaching-the-point-of-price-discovery-3"></a>Reaching the Point of “Price Discovery”</h4>
<p>As of November 2024, Ethereum has reached a level of demand where rollups would stop posting blobs no matter what the base fee is, but instead post an amount of blobs that keeps the base fee quite stable. People like calling that a phase of “price discovery”, even though it just means that at that specific point in time a certain base fee X is regarded as a fair price. At the time of “price discovery”, rollups would no longer consistently post 6 blobs per block without considering the blob base fee, as it is no longer negligible. However, an increasing demand for blobs without an increasing supply (i.e. more blobs available) will lead to higher blob fees. For the following, forgive me when using this simplified concept of “price discovery,” even though prices are discovered every 12 seconds with every slot.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51263-proposed-adjustments-and-their-pros-cons-4" name="p-51263-proposed-adjustments-and-their-pros-cons-4"></a>Proposed Adjustments, and their Pros &amp; Cons</h2>
<p>Looking ahead to the upcoming Pectra fork, there is a clear demand for scaling blobs (from 3/6 to 4/6 or, more sophisticated, 6/9), which could necessitate adjustments to the blob fee market.</p>
<p>In the following sections, I will outline 4 potential changes to the blob fee market and discuss the associated benefits and challenges for each.</p>
<ol>
<li><strong>Adjusting the Minimum Base Fee</strong>: One of the simplest adjustments is to modify the <code>MIN_BASE_FEE</code> parameter, as suggested by Max Resnick.</li>
<li><strong>Automating Blob Base Fee Update Fraction</strong>: A simple change to ensure the blob base fee update fraction scales with the target number of blobs.</li>
<li><strong>Normalization of Excess Gas</strong>: Another proposal from Bert Kellerman and Gajinder suggests “normalizing” the calculation for excess gas usage.</li>
<li><strong>Symmetrizing the Base Fee Updates</strong>: A proposal to adjust the base fee formula.</li>
</ol>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51263-h-1-increase-min_base_fee_per_blob_gas-eip-7762httpsgithubcomethereumeipsblob16a12dcf41251c592f74042b6aa5727097f60167eips2feip-7762md-5" name="p-51263-h-1-increase-min_base_fee_per_blob_gas-eip-7762httpsgithubcomethereumeipsblob16a12dcf41251c592f74042b6aa5727097f60167eips2feip-7762md-5"></a>1. Increase MIN_BASE_FEE_PER_BLOB_GAS (<a href="https://github.com/ethereum/EIPs/blob/16a12dcf41251c592f74042b6aa5727097f60167/EIPS%2Feip-7762.md">EIP-7762</a>)</h2>
<p>The blob base fee starts at 0 and then slowly increases until the point of price discovery. Every ~6 blocks (with 6 blobs) the base fee doubles but it’s a long way to go from 1 wei to a price that is more reasonable, like, for example, 5 gwei. Until we’re at that level, the price may fluctuate a lot and rollups basically get “overly” cheap DA.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/0/e/0e1ac5c0a500f95aea3f6eca8114744eae97c7aa.png" title="Screenshot from 2024-11-07 05-33-46"><img alt="Screenshot from 2024-11-07 05-33-46" height="324" src="https://ethresear.ch/uploads/default/optimized/3X/0/e/0e1ac5c0a500f95aea3f6eca8114744eae97c7aa_2_690x324.png" width="690" /></a></div><p></p>
<p>The mentioned EIP proposes to increase the minimum base fee from 1 wei to ~0.034 gwei. This would shorten the time until price discovery and thus quicker pushes rollups towards a more stable price range that is considered “fair”.</p>
<p>For the base fee to climb from 1 to 5 gwei, it takes…<br />
<span class="math">\frac{\ln\left(\frac{\text{base_fee_target}}{\text{base_fee_start}}\right)}{\text{growth_rate}} = \frac{\ln(5 \times 10^9 / 1)}{0.117} \approx 190 \text{ blocks}</span> and all of those blocks need to have 6 blobs. This equals to approx. 38 minutes.</p>
<p>With the new, increased <code>MIN_BASE_FEE_PER_BLOB_GAS</code>, we would lower this duration to…<br />
<span class="math">\frac{\ln(5 \times 10^9 / 2^{25})}{0.117} \approx  \text{42 blocks}</span>, equaling 8.4 minutes.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/1/417b2a602b0027c374bb9ba5aa1277a3e91fc684.png" title="Screenshot from 2024-11-07 05-41-23"><img alt="Screenshot from 2024-11-07 05-41-23" height="324" src="https://ethresear.ch/uploads/default/optimized/3X/4/1/417b2a602b0027c374bb9ba5aa1277a3e91fc684_2_690x324.png" width="690" /></a></div><p></p>
<p><strong>One highly important caveat/implementation detail:</strong></p>
<p><strong>We MUST reset the excess gas when updating the <code>MIN_BASE_FEE_PER_BLOB_GAS</code></strong>.</p>
<p>The reason for that is that otherwise we would see an unpredictable, extreme spike in the blob base fee right after the fork. This is because the min base fee acts as a multiplier to the base fee and a small adjustment to it can greatly impact the base fee if the excess gas accumulated until that point is not reset.<br />
Here’s an example of that: A apparently “meaningless” increase from 1 to 5 wei is enough to make the base fee skyrocketing and then, rollups would have to wait for a certain period of time until the price calms down again.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/8/8871037f542b273f175a81102502555816cc4f65.png" title="Screenshot from 2024-11-08 11-05-08"><img alt="Screenshot from 2024-11-08 11-05-08" height="305" src="https://ethresear.ch/uploads/default/optimized/3X/8/8/8871037f542b273f175a81102502555816cc4f65_2_690x305.png" width="690" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#p-51263-summarizing-6" name="p-51263-summarizing-6"></a>Summarizing</h3>
<h4><a class="anchor" href="https://ethresear.ch#p-51263-pros-7" name="p-51263-pros-7"></a>Pros:</h4>
<ul>
<li>Faster “price discovery”.</li>
<li>Less volatility in times of supply &lt; demand.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-51263-cons-8" name="p-51263-cons-8"></a>Cons:</h4>
<ul>
<li>Required touching the protocol and we usually aren’t “micro-optimizing” things that aren’t broken.</li>
<li>Since excess gas is reset, we have to go through a phase of price discovery again, though, starting at a higher level thus shortening that time.</li>
</ul>
<p>This can be simplified as “ossification vs. optimizations” and, imo, there’s no right or wrong, even though I personally think that NOT touching the protocol has value too.</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51263-h-2-automating-blob-base-fee-update-fraction-9" name="p-51263-h-2-automating-blob-base-fee-update-fraction-9"></a>2. Automating Blob Base Fee Update Fraction</h2>
<p>This change is super simple: As of now, the update fraction is set to 3338477, as specified in <a href="https://github.com/ethereum/EIPs/blob/16a12dcf41251c592f74042b6aa5727097f60167/EIPS%2Feip-4844.md">EIP-4844</a>. One gets to that number by doing <span class="math">\frac{target\_gas}{ln(1.125)}</span>. If we now replace the hardcoded number with this, we can make sure that we maintain the desired <span class="math">\pm 12.5%</span> change.</p>
<p>There is currently an <a href="https://github.com/ethereum/EIPs/pull/8994">open PR</a> for this change to be included in <a href="https://github.com/ethereum/EIPs/blob/16a12dcf41251c592f74042b6aa5727097f60167/EIPS%2Feip-7742.md">EIP-7742</a> but I’d argue we shouldn’t bundle it together and instead propose this change individually together with the other changes to the base fee mechanism that we might want to ship.</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-51263-h-3-normalization-of-excess-gas-10" name="p-51263-h-3-normalization-of-excess-gas-10"></a>3. Normalization of Excess Gas</h2>
<p>That’s a proposal that aims to avoid steep base fee drops after forks that increase the blob target.</p>
<p>To avoid a sudden drop in the base fee right after a fork that increases the blob target, this proposal suggests adjusting the way we calculate excess gas. Normally, the base fee should change smoothly—only up to 12.5% per slot. By normalizing excess gas, we can ensure the base fee remains stable and doesn’t drop sharply beyond the expected 12.5% limit when the target increases, keeping fee adjustments predictable and gradual.</p>
<p>Simply put, instead of accumulating excess gas, we accumulate normalized excess gas. This way the target blob number doesn’t matter.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/e/fea44d6615fabcf4e78b59a55a9edbdc57ecdb1b.png" title="Screenshot from 2024-11-14 04-14-09"><img alt="Screenshot from 2024-11-14 04-14-09" height="328" src="https://ethresear.ch/uploads/default/optimized/3X/f/e/fea44d6615fabcf4e78b59a55a9edbdc57ecdb1b_2_690x328.png" width="690" /></a></div><p></p>
<p>By introducing a normalization factor in the excess gas calculation, we can adjust the accumulated excess gas to remain proportionate to the new target. This ensures that the base fee adjusts smoothly, maintaining consistent fee dynamics even when the target changes.</p>
<p>Normalization involves scaling the excess gas using a constant factor, so that it reflects the deviation from the target in a consistent manner regardless of the target’s value. This approach keeps the base fee adjustments within the expected limits, preventing sudden drops or spikes that could disrupt the network’s fee market.</p>
<p>For more details on this change, check <a href="https://github.com/ethereum/EIPs/blob/4ce73bd55eacae560073622c1af2aac0ab362a52/EIPS/eip-7742.md">this commit</a>.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51263-summarizing-11" name="p-51263-summarizing-11"></a>Summarizing</h3>
<h4><a class="anchor" href="https://ethresear.ch#p-51263-pros-12" name="p-51263-pros-12"></a>Pros</h4>
<ul>
<li>Avoids base fee drop after fork with target increase.
<ul>
<li>more predictability for blob users</li>
</ul>
</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-51263-cons-13" name="p-51263-cons-13"></a>Cons</h4>
<ul>
<li>Another potential “micro-optimization”.</li>
</ul>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-51263-symmetrizing-blob-base-fee-updates-around-the-target-14" name="p-51263-symmetrizing-blob-base-fee-updates-around-the-target-14"></a>Symmetrizing Blob Base Fee Updates around the Target</h3>
<p>With changing the target such that <span class="math">target=\frac{max}{2}</span> doesn’t hold anymore, we change the distance from the target to the min/max. E.g., going to a blob target of 4 and a max of 6, there is room for 2 blobs up and 4 blobs down.</p>
<p><img alt="blob-target-max" height="216" src="https://ethresear.ch/uploads/default/original/3X/1/6/16bbe3da3a37a6134fe4878c7771e8aaff4dd555.png" width="380" /></p>
<p>Now, with more room on the negative side than on the positive side, the base fee can move down faster than it can move up.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/c/4ced4a84d32f81d3705402822fa4e527fdda158b.png" title="base_fee_symmetry"><img alt="base_fee_symmetry" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/4/c/4ced4a84d32f81d3705402822fa4e527fdda158b_2_404x500.png" width="404" /></a></div><p></p>
<p>A simple fix to this is the following:<br />
We first determine the delta between the gas used and the target (same as we do now). Then we apply a scaling factor that is simply <code>target/(max-target)</code> to the side that has <strong>less</strong> “room” (e.g., the “up” side when doing 4/6 target/max).</p>
<p>The <code>calc_excess_blob_gas</code> function would look like the following:</p>
<pre><code class="lang-python">def calc_excess_blob_gas(parent_header: Header) -&gt; int:
    scaling_factor = TARGET_BLOB_GAS_PER_BLOCK / (MAX_BLOB_GAS_PER_BLOCK - TARGET_BLOB_GAS_PER_BLOCK)
    blob_gas_delta = parent_header.blob_gas_used - TARGET_BLOB_GAS_PER_BLOCK
    if blob_gas_delta &gt; 0:
        scaled_delta = blob_gas_delta * scaling_factor
    else:
        scaled_delta = blob_gas_delta
    excess_blob_gas = max(0, parent_header.excess_blob_gas + scaled_delta)
    return excess_blob_gas
</code></pre>
<p>Of course, this only works in cases where <span class="math">max-target&lt;target</span> and one could generalize it even more, but I’d guess it’s not worth it.</p>
<p>To better understand the effects of this symmetrizing process, let’s walk through a simple example:</p>
<ol>
<li>Imagine we have three slots:
<ul>
<li>The first two slots contain 6 blobs each.</li>
<li>The last slot contains 0 blobs.</li>
</ul>
</li>
<li>This results in a total of 12 blobs across the 3 slots.</li>
<li>The target is set to 4 blobs per block, with a maximum of 6 blobs.</li>
</ol>
<h4><a class="anchor" href="https://ethresear.ch#p-51263-symmetric-vs-non-symmetric-base-fee-adjustments-15" name="p-51263-symmetric-vs-non-symmetric-base-fee-adjustments-15"></a>Symmetric vs. Non-Symmetric Base Fee Adjustments:</h4>
<ul>
<li>
<p><strong>Symmetric Base Fee</strong>:</p>
<ul>
<li>The base fee increases by approximately 12.5% twice (for the two 6-blob slots) and then decreases by 12.5% once (for the 0-blob slot).</li>
<li>After these adjustments, the base fee ends up <strong>higher</strong> than its initial value.</li>
</ul>
</li>
<li>
<p><strong>Non-Symmetric Base Fee</strong>:</p>
<ul>
<li>The base fee remains unchanged after the 6-6-0 blob sequence.</li>
</ul>
</li>
</ul>
<p>This example highlights the benefits of a symmetric adjustment:</p>
<ul>
<li>The 6-6-0 blob sequence pushes us toward extremes (max utilization followed by none), which is undesirable.</li>
<li>Ideally, the load should be more evenly distributed across the three slots (e.g., 4 blobs in each block).</li>
<li>A symmetric base fee discourages extreme behavior by “penalizing” uneven usage (6 blobs instead of 4) more with a higher base fee, promoting a more balanced load.</li>
<li>Of course, one could argue that blob users might not care about pushing the base fee up because they might only need to post blobs a few times in every epoch and therefore don’t care about the slot following their 6-blob posting. Though, this argument is short-sighted.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51263-summarizing-16" name="p-51263-summarizing-16"></a>Summarizing</h3>
<h4><a class="anchor" href="https://ethresear.ch#p-51263-pros-17" name="p-51263-pros-17"></a>Pros</h4>
<ul>
<li>Ensure “price discovery” happens as fast as it does now.</li>
<li>Ensure the “extremes” (0 and 6 blobs) cause the same percentage increases/decreases.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-51263-cons-18" name="p-51263-cons-18"></a>Cons</h4>
<ul>
<li>Yet another “micro-optimization” and everything might be fine without doing it.</li>
<li>Introducing some path dependency. E.g. jumping between <span class="math">gas\_used= target \pm 1\ blobs</span> causes the base fee to increase steadily over time. This behavior can be counteracted by avoiding posting more blobs than the target, which may even incentivize a more balanced posting strategy.</li>
</ul>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/on-blob-markets-base-fee-adjustments-and-optimizations/21024">Read full topic</a></p>
]]></content:encoded>
<pubDate>Mon, 18 Nov 2024 10:31:58 +0000</pubDate>
</item>
<item>
<title>The 1st mathematically, proven and mid development: FULLY, TRUSTLESS BTC bridge ~ "The Holy Grail"</title>
<link>https://ethresear.ch/t/the-1st-mathematically-proven-and-mid-development-fully-trustless-btc-bridge-the-holy-grail/20987</link>
<guid>https://ethresear.ch/t/the-1st-mathematically-proven-and-mid-development-fully-trustless-btc-bridge-the-holy-grail/20987</guid>
<content:encoded><![CDATA[
<div> 关键词: Overpass Channels, Bitcoin, 层2解决方案, 隐私保护, 扩展性

总结:<br />
本文介绍了Overpass Channels作为比特币的一种创新层2解决方案，旨在实现大规模交易的同时保持协议完整性和隐私性。该方案对比了Overpass Channels与BitVM2，强调Overpass的优势在于其对隐私的强化保护、网络完整性维护以及无需更改比特币核心协议即可实现高容量交易的能力。通过分布式存储、优化的状态管理和Plonky2基的zk-SNARKs，Overpass Channels提供了与比特币原生HTLC兼容的交易方式。文章证明了Overpass Channels在维持比特币的安全属性和货币政策方面的重要性，并通过一系列定理和证明确立了其在隐私、扩展性、经济中立性和安全性方面的优越性。总的来说，Overpass Channels为比特币扩展性问题提供了一种具有较强竞争力的解决方案，有望成为比特币和其他区块链网络的理想现金层。 <div>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-overpass-channel-sub-paper-1" name="p-51185-overpass-channel-sub-paper-1"></a>Overpass Channel Sub-paper:</h2>
<h1><a class="anchor" href="https://ethresear.ch#p-51185-bitcoin-bo-instant-private-massively-scalable-liquid-bitcoin-with-true-trustless-bridge-pro-maxi-choice-l1-heterogenous-2" name="p-51185-bitcoin-bo-instant-private-massively-scalable-liquid-bitcoin-with-true-trustless-bridge-pro-maxi-choice-l1-heterogenous-2"></a>Bitcoin (B²O): Instant, Private, Massively Scalable, Liquid Bitcoin with true trustless bridge - Pro Maxi Choice  - [L1 heterogenous]</h1>
<p><strong>Author</strong>: Brandon “Cryptskii” Ramsay<br />
<strong>Date</strong>: 2024-11-14</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-abstract-3" name="p-51185-abstract-3"></a>Abstract</h2>
<p>In response to the growing economic challenges faced by traditional financial systems, Bitcoin’s significance as a decentralized, censorship-resistant store of value continues to rise. Building on the Overpass Channels architecture, we propose a privacy-preserving, scalable Layer 2 solution that enables high-volume transactions on Bitcoin without altering its protocol or consensus model. This paper presents a comparative analysis of Overpass Channels and BitVM2, substantiating Overpass’s superiority in privacy, economic neutrality, and scalability. We formalize the system’s operational assumptions and provide rigorous theorems and proofs that validate Overpass’s ability to maintain Bitcoin’s security properties and monetary principles, setting a new benchmark for scalability on Bitcoin’s blockchain.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51185-h-1-introduction-4" name="p-51185-h-1-introduction-4"></a>1. Introduction</h1>
<p>The escalating volatility within traditional financial systems underscores Bitcoin’s foundational role as a decentralized store of value. As Bitcoin adoption grows, the need for scalable and private transaction mechanisms is evident. Leveraging the Overpass Channels architecture<br />
<a href="https://eprint.iacr.org/2024/1526" rel="noopener nofollow ugc">Overpass.2024</a>, we introduce a solution specifically designed to scale Bitcoin transactions without altering its consensus or core protocol. By contrasting Overpass Channels with BitVM2, we elucidate the distinct advantages of our approach in maintaining privacy and network integrity while ensuring economic neutrality.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-h-11-motivation-5" name="p-51185-h-11-motivation-5"></a>1.1 Motivation</h2>
<p>Given the limitations of traditional Layer 2 solutions—often requiring protocol adjustments or trust-based assumptions—the Overpass Channels approach offers a uniquely adaptable, non-invasive solution that enables Bitcoin to scale without compromising its decentralized ethos. While recent advancements like BitVM2 have made strides in SNARK-based verification, Overpass Channels address these challenges through its established hierarchical structure [Section 9.1] and privacy-focused mechanisms [Section 3].</p>
<ul>
<li><strong>Distributed Storage</strong>: Utilizes Overpass’s distributed storage model [Section 10] for efficient transaction handling.</li>
<li><strong>Optimized State Management</strong>: Employs hierarchical sparse Merkle trees [Section 12] for lightweight Bitcoin state management.</li>
<li><strong>Privacy-Enhanced zk-SNARKs</strong>: Integrates Plonky2-based zk-SNARKs [Section 3.8] to preserve transaction privacy.</li>
<li><strong>Compatibility with Bitcoin’s HTLC</strong>: Ensures seamless Bitcoin integration through HTLC adaptation [Section 8.2].</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-h-12-core-principles-6" name="p-51185-h-12-core-principles-6"></a>1.2 Core Principles</h2>
<p>Our design prioritizes the following principles to ensure Overpass Channels aligns with Bitcoin’s core properties:</p>
<ol>
<li><strong>Protocol Integrity</strong>: Achieves scalability without protocol modifications to Bitcoin.</li>
<li><strong>Economic Consistency</strong>: Preserves Bitcoin’s economic incentives and fee structure.</li>
<li><strong>Trustless Design</strong>: Implements trustless operation based on Overpass’s proven cryptographic assumptions [Section 6].</li>
<li><strong>Privacy Assurance</strong>: Enhances transaction privacy by default, following Overpass’s established privacy guarantees [Section 18].</li>
<li><strong>Decentralization Support</strong>: Maintains economic neutrality to avoid concentration of network power.</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-comparison-framework-7" name="p-51185-comparison-framework-7"></a>Comparison Framework</h3>
<p>To formalize the comparison between Overpass Channels and BitVM2, we establish a rigorous evaluation framework based on privacy, scalability, economic neutrality, and security. Each metric is substantiated through theorem-proof structures that quantify the systems’ respective capabilities.</p>
<p><strong>Definition (Layer 2 Security Preservation)</strong>: A Layer 2 solution <span class="math">S</span> preserves Bitcoin’s security model if and only if:<br />
<span class="math">
\forall t \in T, \; P(\text{attack} \mid S) \leq P(\text{attack} \mid \text{Bitcoin})
</span><br />
where <span class="math">T</span> is the set of all transaction types, and <span class="math">P(\text{attack})</span> represents the probability of a successful attack.</p>
<p><strong>Theorem (Security Preservation in Overpass Channels)</strong>: Overpass Channels maintain Bitcoin’s security properties with respect to consensus and decentralization by ensuring that no additional vulnerabilities are introduced in state management or transaction validation:<br />
<span class="math">
P(\text{attack} \mid \text{Overpass}) = P(\text{attack} \mid \text{Bitcoin}).
</span></p>
<p><strong>Proof</strong>: Let <span class="math">A</span> be an adversary aiming to compromise transactions in Overpass Channels. For any attack strategy <span class="math">\sigma</span>:</p>
<ol>
<li>
<p>The adversary must either:</p>
<ul>
<li>Break Bitcoin’s security assumptions, or</li>
<li>Exploit a flaw in Overpass’s zk-SNARK verification or channel closure mechanism.</li>
</ul>
</li>
<li>
<p>Overpass Channels enforce the following:</p>
<ul>
<li>zk-SNARK soundness guarantees transaction validity.</li>
<li>Channel closure requires a valid Bitcoin transaction, preserving the network’s security model.</li>
<li>No additional cryptographic assumptions beyond standard zk-SNARK soundness are introduced.</li>
</ul>
</li>
<li>
<p>Consequently, the security of Overpass Channels is bounded by Bitcoin’s own security assumptions and the integrity of zk-SNARK proofs:<br />
<span class="math">P(\text{attack} \mid \text{Overpass}) = P(\text{attack} \mid \text{Bitcoin})</span></p>
</li>
</ol>
<p>This completes the proof, showing that Overpass Channels do not degrade Bitcoin’s security guarantees.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-technical-architecture-8" name="p-51185-technical-architecture-8"></a>Technical Architecture</h2>
<p>The integration of Overpass Channels with Bitcoin leverages several technical mechanisms to achieve scalability and privacy while preserving security. We provide a structured comparison with BitVM2 to highlight Overpass’s unique advantages.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-unilateral-payment-channels-9" name="p-51185-unilateral-payment-channels-9"></a>Unilateral Payment Channels</h3>
<p>Overpass Channels introduce a unilateral payment channel structure specifically optimized for Bitcoin, distinct from BitVM2’s state model.</p>
<p><strong>Definition (Bitcoin-Compatible Unilateral Channel)</strong><br />
A Bitcoin-compatible unilateral channel <span class="math">C</span> is defined as a tuple <span class="math">(pk_s, pk_r, v, t, \sigma)</span> where:</p>
<ul>
<li><span class="math">pk_s</span>: Sender’s public key</li>
<li><span class="math">pk_r</span>: Receiver’s public key</li>
<li><span class="math">v</span>: Channel value in satoshis</li>
<li><span class="math">t</span>: Timelock value</li>
<li><span class="math">\sigma</span>: Channel signature</li>
</ul>
<p>satisfying the following property:<br />
<span class="math">{ValidChannel}(C) \iff {VerifyBitcoinSig}(sigma, (pk_s, pk_r, v, t)) = {true}</span></p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-cryptographic-constructions-for-bitcoin-channels-10" name="p-51185-cryptographic-constructions-for-bitcoin-channels-10"></a>Cryptographic Constructions for Bitcoin Channels</h3>
<p>Overpass Channels ensure privacy and security through cryptographic constructions designed to operate efficiently on Bitcoin’s existing infrastructure. This approach contrasts with BitVM2’s focus on sequential verification, yielding distinct privacy and efficiency advantages.</p>
<p><strong>Theorem (Channel State Privacy)</strong><br />
Given a channel state <span class="math">S</span> and its corresponding zk-SNARK proof <span class="math">\pi</span>, no adversary <span class="math">A</span> can determine the transaction history or current balances with probability greater than negligible, while still being able to verify the validity of the state.</p>
<p><strong>Proof</strong><br />
Let <span class="math">S</span> be a channel state and <span class="math">\pi</span> its corresponding zk-SNARK proof. Privacy is ensured through a series of games:</p>
<ol>
<li>
<p><strong>Game 0</strong>: The real privacy game, where an adversary <span class="math">A</span> attempts to learn information about the channel state <span class="math">S</span>.</p>
</li>
<li>
<p><strong>Game 1</strong>: Modify Game 0 by replacing the real zk-SNARK proof with a simulated proof.</p>
<p>By the zero-knowledge property of zk-SNARKs:<br />
<span class="math">\left| \Pr[A \text{ wins Game 0}] - \Pr[A \text{ wins Game 1}] \right| \leq \text{negl}(\lambda)</span><br />
where <span class="math">\text{negl}(\lambda)</span> is a negligible function in the security parameter <span class="math">\lambda</span>.</p>
</li>
<li>
<p><strong>Game 2</strong>: Replace the real channel state <span class="math">S</span> with a random, valid state.</p>
<p>By the hiding property of the commitment scheme:<br />
$\left| \Pr[A \text{ wins Game 1}] - \Pr[A \text{ wins Game 2}] \right| \leq \text{negl}(\lambda)$$</p>
</li>
</ol>
<p>In Game 2, the adversary receives no information about the actual channel state <span class="math">S</span>, resulting in:<br />
<span class="math">\Pr[A \text{ wins Game 2}] = \frac{1}{2}</span></p>
<p>Through this sequence of games, we conclude that <span class="math">A</span>'s advantage in the real game (Game 0) is negligible, establishing privacy for the Overpass Channels.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-channel-operations-and-bitcoin-script-integration-11" name="p-51185-channel-operations-and-bitcoin-script-integration-11"></a>Channel Operations and Bitcoin Script Integration</h3>
<p>Overpass Channels implement functionality through Bitcoin-compatible scripts, enabling secure channel operations without modifying Bitcoin’s protocol. This approach differs from BitVM2, which requires sequential verification stages, by focusing on privacy preservation and operational efficiency.</p>
<p><strong>Algorithm: Channel Opening on Bitcoin</strong></p>
<p><strong>Require:</strong> Sender keys <span class="math">sk_s</span>, <span class="math">pk_s</span>, Receiver public key <span class="math">pk_r</span>, Channel value <span class="math">v</span></p>
<ol>
<li>
<p>Generate funding transaction <span class="math">T_f</span> with the following script:</p>
<pre><code class="lang-auto">OP_IF
   OP_SHA256 H(revocation_key)
   OP_EQUALVERIFY
   pk_r OP_CHECKSIG
OP_ELSE
   timeout OP_CHECKLOCKTIMEVERIFY
   OP_DROP
   pk_s OP_CHECKSIG
OP_ENDIF
</code></pre>
</li>
<li>
<p>Broadcast <span class="math">T_f</span> to the Bitcoin network.</p>
</li>
<li>
<p>Generate zk-SNARK proof <span class="math">\pi</span> of the channel state validity.</p>
</li>
</ol>
<p><strong>Ensure:</strong> <span class="math">(T_f, \pi)</span></p>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-comparison-with-bitvm2-12" name="p-51185-comparison-with-bitvm2-12"></a>Comparison with BitVM2</h2>
<p>Overpass Channels and BitVM2 both utilize zk-SNARKs to enable advanced transaction verification on Bitcoin. However, their approaches to state management, privacy, and scalability vary significantly. This section provides a detailed comparison to illustrate the advantages of Overpass Channels over BitVM2.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-architectural-differences-13" name="p-51185-architectural-differences-13"></a>Architectural Differences</h3>
<p>The core architectural design of each system impacts their performance and scalability. Overpass Channels leverage distributed state management and privacy-preserving mechanisms, while BitVM2 emphasizes sequential verification stages.</p>
<div class="md-table">
<table>
<thead>
<tr>
<th>Feature</th>
<th>Overpass Channels</th>
<th>BitVM2</th>
</tr>
</thead>
<tbody>
<tr>
<td>State Model</td>
<td>Privacy-preserving off-chain</td>
<td>Off-chain with on-chain verification</td>
</tr>
<tr>
<td>Privacy</td>
<td>Full transaction privacy</td>
<td>Basic transaction privacy</td>
</tr>
<tr>
<td>Scalability</td>
<td><span class="math">O(n)</span> horizontal scaling</td>
<td><span class="math">O(n)</span> with verification overhead</td>
</tr>
<tr>
<td>Trust Model</td>
<td>Bitcoin-equivalent</td>
<td>Bitcoin-equivalent with setup</td>
</tr>
<tr>
<td>Impact on Miners</td>
<td>Neutral</td>
<td>Neutral with verification cost</td>
</tr>
<tr>
<td>Verification Method</td>
<td>Optimized SNARK proofs</td>
<td>Sequential SNARK-based verification</td>
</tr>
</tbody>
</table>
</div><h3><a class="anchor" href="https://ethresear.ch#p-51185-economic-implications-14" name="p-51185-economic-implications-14"></a>Economic Implications</h3>
<p>The economic implications of each approach significantly affect Bitcoin’s fee market and miner incentives. While both systems maintain Bitcoin’s security model, their respective costs and operational overhead differ.</p>
<p><strong>Theorem (Incentive Compatibility)</strong><br />
Let <span class="math">M</span> represent Bitcoin miners, and let <span class="math">I(m)</span> be the expected income of a miner <span class="math">m</span>. Under both Overpass Channels and BitVM2:<br />
<span class="math">\forall m \in M: E[I(m) \mid L2] \geq E[I(m) \mid Bitcoin]</span><br />
with system-specific overhead distributions as follows:<br />
<span class="math">O_{\text{Overpass}} = O_{\text{constant}}</span><br />
<span class="math">O_{\text{BitVM2}} = O_{\text{verification}} + O_{\text{setup}}</span></p>
<p><strong>Proof</strong><br />
For Overpass Channels:</p>
<ol>
<li>Channel operations rely on standard Bitcoin transactions.</li>
<li>Verification burden remains constant due to optimized SNARK proofs.</li>
<li>Mining decentralization and fee structures remain unaffected.</li>
</ol>
<p>For BitVM2:</p>
<ol>
<li>Similar reliance on standard Bitcoin transactions.</li>
<li>Initial setup and verification costs introduced.</li>
<li>Verification overhead potentially impacts miner fees due to increased computational requirements.</li>
</ol>
<p>Therefore, both systems preserve Bitcoin’s incentive model, although Overpass offers a more consistent and lower overhead for miners.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-network-effects-and-liquidity-15" name="p-51185-network-effects-and-liquidity-15"></a>Network Effects and Liquidity</h3>
<p>The liquidity distribution and network effects of each system are crucial for Bitcoin’s economic stability. Overpass Channels achieve liquidity efficiency with minimized operational costs, offering an advantage over BitVM2’s verification overhead.</p>
<p><strong>Theorem (Liquidity Preservation)</strong><br />
In a network with total liquidity <span class="math">L</span>, both systems preserve Bitcoin’s liquidity pool:<br />
<span class="math">L_{\text{effective}} = L_{\text{total}} - O_{\text{system}}</span><br />
where:<br />
<span class="math">O_{\text{Overpass}} &lt; O_{\text{BitVM2}}</span><br />
due to Overpass’s optimized state management and lack of setup costs.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-security-considerations-and-risk-analysis-16" name="p-51185-security-considerations-and-risk-analysis-16"></a>Security Considerations and Risk Analysis</h2>
<p>Layer 2 solutions must be carefully analyzed for security implications to ensure they do not compromise Bitcoin’s core properties. This section provides a comprehensive examination of the security models for Overpass Channels and BitVM2, focusing on privacy, attack surface, and resistance to double-spend attacks.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-attack-surface-analysis-17" name="p-51185-attack-surface-analysis-17"></a>Attack Surface Analysis</h3>
<p>The attack surface of each system represents the potential vulnerability points that could be exploited by adversaries. Overpass Channels and BitVM2 both introduce minimal attack surfaces, but their structural differences affect the composition of these surfaces.</p>
<p><strong>Definition (Attack Surface Extension)</strong><br />
For a Layer 2 solution <span class="math">L</span>, the attack surface extension <span class="math">E(L)</span> is defined as:<br />
<span class="math">E(L) = \{(v, p) \mid v \in V(L) \setminus V(Bitcoin), p &gt; 0\}</span><br />
where <span class="math">V(L)</span> is the set of potential vulnerability points in <span class="math">L</span> and <span class="math">p</span> is the probability of successful exploitation.</p>
<p><strong>Theorem (Equivalent Base Extension)</strong><br />
Both systems maintain minimal attack surface extension:<br />
<span class="math">|E(\text{Overpass})| = O(1)</span><br />
<span class="math">|E(\text{BitVM2})| = O(1)</span><br />
with different vulnerability classes:<br />
<span class="math">V_{\text{Overpass}} = \{V_{\text{privacy}}, V_{\text{state}}\}</span><br />
<span class="math">V_{\text{BitVM2}} = \{V_{\text{setup}}, V_{\text{verify}}\}</span></p>
<p><strong>Proof</strong><br />
For both Overpass Channels and BitVM2:</p>
<ol>
<li>State transitions and transaction validity are secured by zk-SNARKs.</li>
<li>Channel operations rely on standard Bitcoin transaction security.</li>
<li>No additional consensus requirements are introduced.</li>
</ol>
<p>Key distinctions include:</p>
<ol>
<li>
<p><strong>Privacy Mechanism</strong>:</p>
<ul>
<li>Overpass: Full privacy achieved through state channels.</li>
<li>BitVM2: Basic privacy limited by sequential verification.</li>
</ul>
</li>
<li>
<p><strong>Setup Requirements</strong>:</p>
<ul>
<li>Overpass: Direct channel initialization without additional setup.</li>
<li>BitVM2: Requires an initial verification setup phase.</li>
</ul>
</li>
</ol>
<p>Thus, both systems achieve minimal and comparable attack surface extensions, though the structure of vulnerability classes differs.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-double-spend-prevention-18" name="p-51185-double-spend-prevention-18"></a>Double-Spend Prevention</h3>
<p>Double-spend prevention is essential for maintaining Bitcoin’s integrity as a monetary system. Both Overpass Channels and BitVM2 implement robust mechanisms to prevent double-spend attacks.</p>
<p><strong>Theorem (Double-Spend Prevention)</strong><br />
For both systems, the probability of a successful double-spend attack <span class="math">P(DS)</span> is bounded by:<br />
<span class="math">P(DS) \leq \min(P(\text{Bitcoin\_DS}), P(\text{zk\_break}))</span><br />
where <span class="math">P(\text{Bitcoin\_DS})</span> represents the probability of a double-spend on Bitcoin and <span class="math">P(\text{zk\_break})</span> represents the probability of breaking the zk-SNARK system.</p>
<p><strong>Proof</strong><br />
Let <span class="math">A</span> be an adversary attempting a double-spend attack. For success, <span class="math">A</span> must either:</p>
<ol>
<li>Compromise Bitcoin’s underlying security model with probability <span class="math">P(\text{Bitcoin\_DS})</span>.</li>
<li>Generate a false zk-SNARK proof with probability <span class="math">P(\text{zk\_break})</span>.</li>
</ol>
<p>Additionally, both systems enforce a channel closure mechanism that ensures:<br />
<span class="math">\forall s_1, s_2 \in \text{States}: \text{Close}(s_1) \land \text{Close}(s_2) \implies s_1 = s_2</span></p>
<p>Thus, the probability of a successful double-spend attack is bounded by the minimum probability of either compromising Bitcoin’s security or breaking the zk-SNARK proof system, regardless of system-specific differences.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-impact-on-bitcoins-security-model-19" name="p-51185-impact-on-bitcoins-security-model-19"></a>Impact on Bitcoin’s Security Model</h3>
<p>Each Layer 2 solution must be assessed for its impact on Bitcoin’s core security properties, such as decentralization, censorship resistance, and immutability. Overpass Channels and BitVM2 maintain these properties, though their verification and state management differ.</p>
<p><strong>Definition (Security Model Preservation)</strong><br />
A Layer 2 solution <span class="math">S</span> preserves Bitcoin’s security model if:<br />
<span class="math">\forall p \in \text{Properties(Bitcoin)}: \text{Guarantee}(p \mid S) \geq \text{Guarantee}(p \mid \text{Bitcoin})</span><br />
where <span class="math">\text{Properties(Bitcoin)}</span> includes decentralization, censorship resistance, and immutability.</p>
<p><strong>Theorem (Security Model Impact)</strong><br />
Both Overpass Channels and BitVM2 maintain Bitcoin’s security model with distinct architectural trade-offs:<br />
<span class="math">\Delta_{\text{security}}(\text{Overpass}) = \Delta_{\text{security}}(\text{BitVM2}) = 0</span><br />
though they follow different verification pathways:<br />
<span class="math">\text{Path}_{\text{Overpass}} = \{\text{Privacy}, \text{StateManagement}\}</span><br />
<span class="math">\text{Path}_{\text{BitVM2}} = \{\text{Setup}, \text{VerificationFlow}\}</span></p>
<p><strong>Proof</strong><br />
To assess security preservation, consider the following for both systems:</p>
<ol>
<li>
<p><strong>Consensus Requirements</strong>:</p>
<ul>
<li>Both systems operate without modifying Bitcoin’s consensus.</li>
</ul>
</li>
<li>
<p><strong>Cryptographic Assumptions</strong>:</p>
<ul>
<li>Each system relies on zk-SNARKs, ensuring equivalent cryptographic strength.</li>
</ul>
</li>
<li>
<p><strong>State and Transaction Management</strong>:</p>
<ul>
<li>Overpass: Employs integrated, privacy-preserving state channels, minimizing exposure.</li>
<li>BitVM2: Utilizes a sequential verification process that introduces verification layers but maintains on-chain compatibility.</li>
</ul>
</li>
<li>
<p><strong>Implementation Distinctions</strong>:</p>
<ul>
<li>Overpass prioritizes direct state transitions, reducing operational overhead.</li>
<li>BitVM2 requires setup and verification sequences, increasing complexity.</li>
</ul>
</li>
</ol>
<p>Therefore, both systems preserve Bitcoin’s security model while following distinct approaches to verification and state management.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-liveness-and-availability-analysis-20" name="p-51185-liveness-and-availability-analysis-20"></a>Liveness and Availability Analysis</h3>
<p>The liveness and availability of transactions are critical for user experience and adoption. Overpass Channels and BitVM2 achieve comparable liveness guarantees through different transaction handling mechanisms.</p>
<p><strong>Theorem (Liveness Guarantee)</strong><br />
Under both systems, transaction liveness <span class="math">L(t)</span> for a transaction <span class="math">t</span> is guaranteed with probability:<br />
<span class="math">P(L(t)) \geq 1 - (1 - p)^k</span><br />
where <span class="math">p</span> is the probability of successful Bitcoin transaction inclusion and <span class="math">k</span> is the number of confirmation attempts.</p>
<p><strong>Proof</strong><br />
For both systems:</p>
<ol>
<li>
<p><strong>Channel Operations</strong>:</p>
<ul>
<li>Rely on standard Bitcoin transactions for channel creation and closure.</li>
</ul>
</li>
<li>
<p><strong>Verification Methodology</strong>:</p>
<ul>
<li>Both systems use zk-SNARK proofs for verification, enabling off-chain transaction finality.</li>
</ul>
</li>
<li>
<p><strong>Channel Closure Attempts</strong>:</p>
<ul>
<li>With <span class="math">k</span> attempts, the probability of successful closure is given by:<br />
<span class="math">P(\text{closure\_success}) = 1 - (1 - p)^k</span></li>
</ul>
</li>
</ol>
<p>Since each system relies on Bitcoin’s underlying liveness properties for final settlement, they both achieve equivalent liveness guarantees.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-long-term-security-implications-21" name="p-51185-long-term-security-implications-21"></a>Long-term Security Implications</h3>
<p>Both Overpass Channels and BitVM2 must be evaluated for their long-term security impacts, especially in terms of protocol longevity and resistance to future attack vectors.</p>
<p><strong>Theorem (Security Model Evolution)</strong><br />
The long-term security impact <span class="math">I(t)</span> of both Layer 2 solutions at time <span class="math">t</span> satisfies:<br />
<span class="math">\lim_{t \to \infty} I(t) = 0</span><br />
with differing composition vectors:<br />
<span class="math">V_{\text{Overpass}}(t) = \{v_{\text{privacy}}(t), v_{\text{state}}(t)\}</span><br />
<span class="math">V_{\text{BitVM2}}(t) = \{v_{\text{setup}}(t), v_{\text{verify}}(t)\}</span></p>
<p><strong>Proof</strong><br />
Consider the following security properties for both systems:</p>
<ol>
<li><strong>Longevity of Cryptographic Assumptions</strong>:</li>
</ol>
<ul>
<li>Both rely on zk-SNARKs with long-term security guarantees, ensuring consistency over time.</li>
</ul>
<ol start="2">
<li><strong>System-Specific Implications</strong>:</li>
</ol>
<ul>
<li>Overpass: Long-term stability due to privacy-preserving channels and minimal setup requirements.</li>
<li>BitVM2: Security preserved through on-chain verification, though with added complexity in setup and verification stages.</li>
</ul>
<ol start="3">
<li><strong>Impact on Bitcoin’s Security</strong>:</li>
</ol>
<ul>
<li>Neither system requires alterations to Bitcoin’s protocol, preserving the core security properties indefinitely.</li>
</ul>
<p>Thus, the long-term security impact remains neutral for both systems, with each maintaining minimal additional risk over time.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-privacy-guarantees-and-economic-implications-22" name="p-51185-privacy-guarantees-and-economic-implications-22"></a>Privacy Guarantees and Economic Implications</h2>
<p>The privacy and economic characteristics of a Layer 2 solution significantly affect Bitcoin’s fungibility and monetary stability. Overpass Channels and BitVM2 both employ zk-SNARKs, yet their approaches to privacy and economic neutrality are fundamentally different.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-privacy-model-23" name="p-51185-privacy-model-23"></a>Privacy Model</h3>
<p>Privacy within a Layer 2 solution is critical for ensuring that transactions are indistinguishable, preserving Bitcoin’s fungibility. Overpass Channels provide enhanced privacy over BitVM2 due to its integrated, privacy-preserving state channels.</p>
<p><strong>Definition (Transaction Privacy)</strong><br />
A transaction <span class="math">T</span> in a Layer 2 system provides <span class="math">\delta</span>-privacy if for any adversary <span class="math">A</span>:<br />
<span class="math">\left| \Pr[A(T) = 1] - \Pr[A(T') = 1] \right| \leq \delta</span><br />
where <span class="math">T'</span> is any other valid transaction with identical public parameters.</p>
<p><strong>Theorem (Privacy Guarantees)</strong><br />
Overpass Channels achieve an enhanced level of privacy, denoted <span class="math">\varepsilon</span>-privacy:<br />
<span class="math">\varepsilon_{\text{Overpass}} \leq \frac{1}{2^\lambda}</span><br />
compared to BitVM2’s basic transaction privacy:<br />
<span class="math">\varepsilon_{\text{BitVM2}} \leq \frac{1}{2^\lambda} + \delta_{\text{state}}</span><br />
where <span class="math">\delta_{\text{state}}</span> represents additional information leakage due to BitVM2’s state verification.</p>
<p><strong>Proof</strong><br />
Let <span class="math">A</span> be an adversary attempting to distinguish between transactions:</p>
<ol>
<li><strong>Base zk-SNARK Privacy</strong>:</li>
</ol>
<ul>
<li>By the zero-knowledge property of zk-SNARKs, for any input <span class="math">x</span> and witness <span class="math">w</span>:<br />
<span class="math">\{\text{Prove}(x, w)\} \approx_c \{\text{Sim}(x)\}</span></li>
</ul>
<ol start="2">
<li><strong>System-Specific Privacy Distinctions</strong>:</li>
</ol>
<ul>
<li>
<p>Overpass: Full state privacy, leading to negligible information leakage:<br />
<span class="math">\left| \Pr[A(\pi, P, U) = 1] - \Pr[A(\text{Sim}(\pi), P, U) = 1] \right| \leq \frac{1}{2^\lambda}</span></p>
</li>
<li>
<p>BitVM2: State verification introduces potential leakage:<br />
<span class="math">\left| \Pr[A(\pi, P, U) = 1] - \Pr[A(\text{Sim}(\pi), P, U) = 1] \right| \leq \frac{1}{2^\lambda} + \delta_{\text{state}}</span></p>
</li>
</ul>
<ol start="3">
<li><strong>Conclusion</strong>:<br />
While both systems provide robust privacy through zk-SNARKs, Overpass achieves stronger privacy guarantees due to its privacy-preserving state channels, resulting in reduced leakage.</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-economic-impact-analysis-24" name="p-51185-economic-impact-analysis-24"></a>Economic Impact Analysis</h3>
<p>The economic implications of each system on Bitcoin’s fee market and miner incentives are essential to maintaining a balanced ecosystem.</p>
<p><strong>Theorem (Fee Market Preservation)</strong><br />
Under both systems, Bitcoin’s fee market equilibrium <span class="math">E</span> remains stable:<br />
<span class="math">|E_{\text{L2}} - E_{\text{Bitcoin}}| \leq \epsilon</span><br />
where <span class="math">\epsilon</span> is a negligible factor, with differing overhead distributions:<br />
<span class="math">\epsilon_{\text{Overpass}} = O_{\text{channel}} + O_{\text{privacy}}</span><br />
<span class="math">\epsilon_{\text{BitVM2}} = O_{\text{setup}} + O_{\text{verify}}</span></p>
<p><strong>Proof</strong><br />
For a transaction <span class="math">t</span>, the fee function <span class="math">F(t)</span> can be expressed as:<br />
<span class="math">F(t) = \alpha \cdot s(t) + \beta \cdot p(t)</span><br />
where <span class="math">s(t)</span> is the transaction size, and <span class="math">p(t)</span> is the priority.</p>
<ol>
<li><strong>Overpass Channels</strong>:</li>
</ol>
<ul>
<li>Operations incur minimal overhead due to privacy-preserving channels.</li>
<li>Fee structure remains consistent with Bitcoin’s standard model.</li>
</ul>
<ol start="2">
<li><strong>BitVM2</strong>:</li>
</ol>
<ul>
<li>Additional setup and verification phases introduce operational overhead.</li>
<li>The fee model remains consistent but with added verification costs.</li>
</ul>
<p>Thus, while both systems preserve the equilibrium of Bitcoin’s fee market, Overpass offers a more efficient fee structure by minimizing extraneous costs.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-liquidity-efficiency-25" name="p-51185-liquidity-efficiency-25"></a>Liquidity Efficiency</h3>
<p>Efficient liquidity utilization is essential for a Layer 2 solution to scale while maintaining user accessibility and network sustainability. Overpass Channels provide a more optimized liquidity model than BitVM2 due to minimized verification and operational overhead.</p>
<p><strong>Theorem (Liquidity Utilization)</strong><br />
Both systems achieve efficient liquidity utilization <span class="math">U</span>, with different optimization paths:</p>
<p>For Overpass Channels:<br />
<span class="math">U_{\text{Overpass}} = \frac{L_{\text{active}}}{L_{\text{total}}} \cdot \prod_{i=1}^n r_i</span></p>
<p>For BitVM2:<br />
<span class="math">U_{\text{BitVM2}} = \frac{L_{\text{active}}}{L_{\text{total}}} \cdot \prod_{i=1}^n (r_i - \sigma_i)</span></p>
<p>where <span class="math">L_{\text{active}}</span> is the active channel liquidity, <span class="math">L_{\text{total}}</span> is the total liquidity, <span class="math">r_i</span> represents rebalancing factors, and <span class="math">\sigma_i</span> indicates verification overhead in BitVM2.</p>
<p><strong>Proof</strong><br />
Consider the set <span class="math">C</span> of all channels in the system. For each channel <span class="math">c \in C</span>:</p>
<ol>
<li>
<p><strong>Liquidity Utilization</strong>:<br />
<span class="math">u(c) = \frac{v(c)}{V(c)} \cdot r(c)</span><br />
where <span class="math">v(c)</span> is the value utilized and <span class="math">V(c)</span> is the channel capacity.</p>
</li>
<li>
<p><strong>System-Specific Utilization Factors</strong>:</p>
</li>
</ol>
<ul>
<li>
<p>Overpass Channels:<br />
<span class="math">U_{\text{Overpass}} = \frac{\sum_{c \in C} u(c) \cdot V(c)}{\sum_{c \in C} V(c)}</span><br />
indicating minimal operational costs and high liquidity efficiency.</p>
</li>
<li>
<p>BitVM2:<br />
<span class="math">U_{\text{BitVM2}} = \frac{\sum_{c \in C} (u(c) - \sigma(c)) \cdot V(c)}{\sum_{c \in C} V(c)}</span><br />
where <span class="math">\sigma(c)</span> reflects verification overhead, reducing effective liquidity.</p>
</li>
</ul>
<ol start="3">
<li><strong>Conclusion</strong>:<br />
Overpass Channels exhibit greater liquidity efficiency as they avoid the additional verification overhead imposed by BitVM2.</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-economic-centralization-resistance-26" name="p-51185-economic-centralization-resistance-26"></a>Economic Centralization Resistance</h3>
<p>Preserving decentralization within the economic model is crucial to avoid power concentration in a Layer 2 solution. Overpass Channels and BitVM2 maintain Bitcoin’s decentralization, but Overpass’s structure is inherently more resistant to centralization.</p>
<p><strong>Definition (Centralization Resistance)</strong><br />
A system <span class="math">S</span> is <span class="math">\rho</span>-centralization resistant if no entity <span class="math">e</span> can control more than <span class="math">\rho</span> fraction of the system’s economic activity:<br />
<span class="math">\forall e: \frac{\text{Control}(e)}{\text{Total}} \leq \rho</span></p>
<p><strong>Theorem (Decentralization Maintenance)</strong><br />
Both systems maintain Bitcoin’s centralization resistance bound <span class="math">\rho</span>:<br />
<span class="math">\rho_{\text{L2}} \leq \rho_{\text{Bitcoin}}</span><br />
though they differ in their resistance mechanisms:<br />
<span class="math">R_{\text{Overpass}} = \{R_{\text{privacy}}, R_{\text{state}}\}</span><br />
<span class="math">R_{\text{BitVM2}} = \{R_{\text{setup}}, R_{\text{verify}}\}</span></p>
<p><strong>Proof</strong><br />
For both systems, we examine centralization resistance as follows:</p>
<ol>
<li><strong>Architectural Aspects</strong>:</li>
</ol>
<ul>
<li>
<p>Overpass Channels:</p>
<ul>
<li>Privacy-preserving channels reduce reliance on trusted parties.</li>
<li>Distributed state management minimizes central control.</li>
</ul>
</li>
<li>
<p>BitVM2:</p>
<ul>
<li>Initial setup and verification dependencies may centralize certain operations.</li>
</ul>
</li>
</ul>
<ol start="2">
<li><strong>Economic Distribution</strong>:</li>
</ol>
<ul>
<li>Both systems employ decentralized transaction processing and verification to avoid reliance on centralized entities.</li>
<li>Dynamic rebalancing mechanisms distribute control across network participants.</li>
</ul>
<p>Thus, Overpass Channels provide a higher resistance to centralization due to minimized setup dependencies and enhanced privacy, while BitVM2 maintains resistance but with increased operational complexity.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-long-term-economic-stability-27" name="p-51185-long-term-economic-stability-27"></a>Long-term Economic Stability</h3>
<p>Ensuring economic stability over time is critical for the viability of a Layer 2 solution on Bitcoin. Both Overpass Channels and BitVM2 aim to preserve Bitcoin’s economic model; however, Overpass offers more consistent long-term stability due to its minimal operational overhead and direct transaction management.</p>
<p><strong>Theorem (Economic Model Preservation)</strong><br />
Both systems preserve Bitcoin’s long-term economic stability:<br />
<span class="math">\lim_{t \to \infty} |M_{\text{L2}}(t) - M_{\text{Bitcoin}}(t)| = 0</span><br />
where <span class="math">M(t)</span> represents the economic model at time <span class="math">t</span>. Each system has different stability vectors:<br />
<span class="math">S_{\text{Overpass}}(t) = \{S_{\text{privacy}}(t), S_{\text{channel}}(t)\}</span><br />
<span class="math">S_{\text{BitVM2}}(t) = \{S_{\text{verify}}(t), S_{\text{setup}}(t)\}</span></p>
<p><strong>Proof</strong><br />
To examine economic stability, we consider the following for each system:</p>
<ol>
<li><strong>Monetary Properties</strong>:</li>
</ol>
<ul>
<li>Both Overpass Channels and BitVM2:
<ul>
<li>Preserve Bitcoin’s fixed supply.</li>
<li>Maintain its issuance schedule.</li>
<li>Do not alter mining incentives or economic dynamics.</li>
</ul>
</li>
</ul>
<ol start="2">
<li><strong>System-Specific Characteristics</strong>:</li>
</ol>
<ul>
<li><strong>Overpass Channels</strong>:
<ul>
<li>The privacy-focused, channel-based structure ensures consistent fee and operational costs.</li>
<li>Direct state management minimizes fluctuations in transaction handling fees.</li>
</ul>
</li>
<li><strong>BitVM2</strong>:
<ul>
<li>Additional setup and verification stages introduce occasional cost spikes, which may lead to minor fee market adjustments over time.</li>
<li>The sequential verification process results in varying operational expenses.</li>
</ul>
</li>
</ul>
<ol start="3">
<li>
<p><strong>Network Effects</strong>:</p>
<ul>
<li>Both systems are designed to maintain decentralization and support censorship resistance, ensuring long-term usability and user accessibility.</li>
</ul>
<p>As <span class="math">t \to \infty</span>, both systems converge towards stable economic models with minor fluctuations for BitVM2 due to its additional verification overhead. Overpass Channels, however, offer a smoother economic trajectory with fewer cost variations.</p>
</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-comparative-analysis-of-trustless-mechanisms-28" name="p-51185-comparative-analysis-of-trustless-mechanisms-28"></a>Comparative Analysis of Trustless Mechanisms</h2>
<p>A fundamental requirement for Layer 2 solutions on Bitcoin is the minimization of trust assumptions. Overpass Channels and BitVM2 each establish distinct trust models, yet Overpass achieves stronger trust minimization due to its direct channel structure and privacy integration.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-trust-model-foundations-29" name="p-51185-trust-model-foundations-29"></a>Trust Model Foundations</h3>
<p>The level of trust required by a Layer 2 system impacts its alignment with Bitcoin’s trustless design. We formalize the trust minimization for each system.</p>
<p><strong>Theorem (Trust Minimization)</strong><br />
For both Layer 2 systems <span class="math">B</span>, the trust requirement <span class="math">T(B)</span> can be defined as:<br />
<span class="math">T(B) = \sum_{i=1}^n w_i \cdot t_i</span><br />
where <span class="math">w_i</span> represents trust weights and <span class="math">t_i</span> represents individual trust assumptions. Each system has unique trust vectors:<br />
<span class="math">T_{\text{Overpass}} = \{t_{\text{privacy}}, t_{\text{state}}\}</span><br />
<span class="math">T_{\text{BitVM2}} = \{t_{\text{setup}}, t_{\text{verify}}\}</span></p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-bridge-trust-models-30" name="p-51185-bridge-trust-models-30"></a>Bridge Trust Models</h3>
<p>Layer 2 solutions require secure bridging mechanisms with Bitcoin’s Layer 1 to facilitate interoperability while preserving trust assumptions.</p>
<p><strong>Definition (Bridge Security)</strong><br />
A bridge transaction maintains Bitcoin’s trust assumptions if:<br />
<span class="math">\forall \text{tx} \in \text{Transactions}: \text{Trust}(\text{tx}) \subseteq \text{Trust}(\text{Bitcoin})</span><br />
where <span class="math">\text{Trust(Bitcoin)}</span> encompasses Bitcoin’s base security assumptions.</p>
<p><strong>Theorem (Trust Preservation)</strong><br />
Both systems preserve Bitcoin’s trust model through different bridging mechanisms:<br />
<span class="math">T(\text{L2}) = T(\text{Bitcoin}) + T(\text{SNARK})</span><br />
where <span class="math">T(\text{SNARK})</span> represents the trust assumption introduced by zk-SNARKs. Distinct implementation paths are followed:<br />
<span class="math">\text{Path}_{\text{Overpass}} = \{\text{Privacy}, \text{StateTransition}\}</span><br />
<span class="math">\text{Path}_{\text{BitVM2}} = \{\text{Setup}, \text{VerificationFlow}\}</span></p>
<p><strong>Proof</strong><br />
The preservation of trust assumptions is achieved by both systems through:</p>
<ol>
<li>
<p><strong>zk-SNARK Trust Requirement</strong>:</p>
<ul>
<li>Both systems introduce SNARK-based proofs, which assume soundness and non-interactivity.</li>
</ul>
</li>
<li>
<p><strong>System-Specific Mechanisms</strong>:</p>
<ul>
<li>
<p><strong>Overpass Channels</strong>:</p>
<ul>
<li>Direct channel state transitions ensure trust minimization.</li>
<li>Integrated privacy reduces the reliance on trusted setups.</li>
</ul>
</li>
<li>
<p><strong>BitVM2</strong>:</p>
<ul>
<li>Requires an initial setup phase, adding a layer of trust for configuration integrity.</li>
<li>Sequential verification process may introduce dependencies on verification nodes.</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>In summary, both systems maintain Bitcoin’s trust model, but Overpass achieves a higher degree of trust minimization by avoiding setup requirements and emphasizing privacy-preserving operations.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-conclusion-31" name="p-51185-conclusion-31"></a>Conclusion</h2>
<p>This paper has provided a detailed comparative analysis of Overpass Channels and BitVM2 as Layer 2 solutions for Bitcoin, focusing on scalability, privacy, security, and economic neutrality. Through rigorous theorem-proof structures, we have demonstrated Overpass Channels’ unique advantages in privacy preservation, efficient liquidity utilization, and trust minimization, establishing it as a leading solution for scaling Bitcoin without altering its core protocol.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-summary-of-key-findings-32" name="p-51185-summary-of-key-findings-32"></a>Summary of Key Findings</h3>
<p>Overpass Channels emerge as a compelling choice for high-volume, privacy-preserving transactions on Bitcoin, offering the following distinct advantages:</p>
<ul>
<li><strong>Enhanced Privacy</strong>: Through integrated privacy-preserving state channels, Overpass ensures stronger privacy guarantees, minimizing information leakage compared to BitVM2.</li>
<li><strong>Scalability and Efficiency</strong>: Achieving <span class="math">O(n)</span> horizontal scaling with minimal verification overhead, Overpass efficiently supports high transaction throughput, whereas BitVM2 incurs higher verification and setup costs.</li>
<li><strong>Economic Neutrality and Stability</strong>: Closely aligned with Bitcoin’s fee market structure, Overpass preserves Bitcoin’s economic neutrality without introducing additional cost burdens.</li>
<li><strong>Trustless Design</strong>: Overpass Channels eliminate the need for trusted setups and emphasize zk-SNARK-based verification, achieving stronger trust minimization than BitVM2’s setup-dependent model.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-overpass-channels-as-the-cash-layer-for-layer-1-blockchains-33" name="p-51185-overpass-channels-as-the-cash-layer-for-layer-1-blockchains-33"></a>Overpass Channels as the Cash Layer for Layer 1 Blockchains</h3>
<p>While Bitcoin serves as an optimal reserve asset and “gold layer” of a decentralized financial network, Overpass Channels have the potential to become the “cash layer” not only for Bitcoin but for any Layer 1 blockchain that integrates with its architecture. By extending Overpass Channels as a universal Layer 2 solution, any compatible blockchain can benefit from instant, privacy-preserving transactions with high scalability, thus providing a cash layer capable of supporting everyday transactional demands across various blockchain ecosystems.</p>
<p>This analysis specifically highlights Overpass Channels in the context of Bitcoin as an extension of the original Overpass Channels research. However, the modular design of Overpass allows seamless integration with multiple blockchains, enhancing each one with Overpass’s advanced privacy and scalability benefits. This interoperability offers a transformative vision: a decentralized, multi-chain economy where Bitcoin and Overpass work symbiotically, with Bitcoin as the global reserve and Overpass as the universal, privacy-preserving cash layer.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-future-directions-34" name="p-51185-future-directions-34"></a>Future Directions</h3>
<p>Several areas of future research and development can help realize the full potential of Overpass Channels across multiple blockchain networks:</p>
<ol>
<li><strong>zk-SNARK Optimization</strong>: Further research into zk-SNARK efficiency can reduce computational overhead, making verification faster and more accessible across diverse Layer 1 blockchains.</li>
<li><strong>Expanding Integration Capabilities</strong>: Developing tools and protocols for seamless Overpass integration with other blockchains will extend its applicability as a cash layer beyond Bitcoin.</li>
<li><strong>Real-world Deployment and Audits</strong>: Comprehensive security audits and real-world testing will validate Overpass’s privacy and scalability claims, ensuring robust performance across different blockchain networks.</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-final-remarks-35" name="p-51185-final-remarks-35"></a>Final Remarks</h3>
<p>In conclusion, Overpass Channels represent a groundbreaking Layer 2 solution that enhances the scalability and privacy of Bitcoin and has the potential to serve as a universal cash layer across various Layer 1 blockchains. By offering a scalable, privacy-focused transaction layer, Overpass can redefine the usability and accessibility of decentralized finance. This cash layer for the Internet enables a flexible, interoperable financial system that respects user privacy and decentralization principles, positioning Bitcoin and Overpass as essential building blocks in the future of a decentralized global economy.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-references-36" name="p-51185-references-36"></a>References</h3>
<ol>
<li>
<p>Ramsay, B., “Overpass Channels: Horizontally Scalable, Privacy-Enhanced, with Independent Verification, Fluid Liquidity, and Robust Censorship Proof Payments,” Cryptology ePrint Archive, Paper 2024/1526, 2024.</p>
</li>
<li>
<p>Linus, R., Aumayr, L., Zamyatin, A., Pelosi, A., Avarikioti, Z., Maffei, M., “BitVM2: Bridging Bitcoin to Second Layers,” presented by ZeroSync, TU Wien, BOB, University of Pisa, University of Camerino, and Common Prefix, 2024.</p>
</li>
<li>
<p>Nakamoto, S., “Bitcoin: A Peer-to-Peer Electronic Cash System,” <a href="http://Bitcoin.org" rel="noopener nofollow ugc">Bitcoin.org</a>, 2008.</p>
</li>
</ol>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/the-1st-mathematically-proven-and-mid-development-fully-trustless-btc-bridge-the-holy-grail/20987">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 14 Nov 2024 23:54:37 +0000</pubDate>
</item>
<item>
<title>On trustless cross-chain asset transfers</title>
<link>https://ethresear.ch/t/on-trustless-cross-chain-asset-transfers/20992</link>
<guid>https://ethresear.ch/t/on-trustless-cross-chain-asset-transfers/20992</guid>
<content:encoded><![CDATA[
<div> 关键词：跨链、信任less、资产转移、智能合约、签名

总结:
该文提出了一种名为Viaduct的早期开发协议，旨在解决以太坊及其Layer 2区块链之间的碎片化和互操作性问题。该协议通过签名基交易系统实现无需信任的跨链资产转移，其核心依赖至少存在一个诚实节点以及网络能在合理时间内检测并转发交易。

首先，单部署阶段创建了一个智能合约系统，该系统可靠地处理基于签名的转账，分为检索、签名和执行三个步骤。

接下来，跨部署阶段使协议能够在相同区块链上的多个实例间运行，确保任意两个实例间存在直接或间接路径，从而避免双花问题并保持同步。

进一步的孤立跨部署阶段通过挑战窗口机制解决了双花问题，确保了只需要一个诚实中继节点即可防止双花并维持不同部署间的共识。

最后，通过将此协议应用于跨链场景，每个网络上部署一个实例，并利用中继节点复制交易，实现从一条链到另一条链的信任less资产转移。为了实现在不同区块链间的标准接口，设计了一个类似订单簿的去中心化交易所，使得用户能够进行跨链资产转移。

总之，该协议提供了一种标准化接口，实现了无需信任的跨链资产转移，为构建集成多种网络的新型区块链奠定了基础，但目前尚存在扩展性和高昂交易费用的问题，可通过使用L2网络存储交易数据或建立L3层等方式进行优化。 <div>
<p><strong>On trustless cross-chain asset transfers.</strong></p>
<p>NOTE: Early development of this protocol has begun under the name Viaduct.</p>
<p><em>Preface</em></p>
<p>In the state of Ethereum today, layer 2 blockchains experience fragmentation between each other and with the Ethereum mainnet. One core piece of solving this problem and achieving effective interoperability is cross-chain asset transfers. Many bridges implement this functionality, but they do so in a way that, to some extent, relies on trust.</p>
<p>But, by using a signature-based transaction system, it is possible to trustlessly mirror token transfers across multiple networks. The only trust assumption is that <strong>at least one</strong> honest node exists and that the network can detect and relay transactions in a reasonably short period of time.</p>
<p><em>Part I: Single-deployment</em></p>
<p>The first step is to create a system that can reliably handle signature-based transfers on a single smart contract. The protocol splits these transfers into three parts: retrieval, signing, and execution.</p>
<p>The retrieval step calls <code>getValidHash()</code> on the protocol’s core smart contract. This method will, given a sender, recipient, value, and nonce, calculate and return the requested transfer’s hash.</p>
<p>Then comes the signing step. Once the contract finishes its calculations, the transaction sender can sign the resulting hash to approve the transaction. Signatures are valid for one transfer only. If the sender wishes to repeat a transaction, it can calculate a new hash and signature using a new nonce.</p>
<p>Finally, in the execution step, anyone with access to the transaction signature can call <code>objectiveTransfer()</code> on the core contract. After checking for double spending and signature correctness, the contract transfers the correct number of tokens from the sender to the recipient. Either the signer or a so-called relay node could initiate the <code>objectiveTransfer()</code> call. Note that relay nodes may introduce fees into the transfer process.</p>
<p>This system allows for token transfers via signatures and relays. At this point, this design replicates the benefits and drawbacks of a system like Uniswap’s Permit2.</p>
<p><em>Part II: Cross-deployment</em></p>
<p>The next step in creating a cross-chain trustless signature transfer system is enabling this protocol to function across multiple deployments or instances of the core contract, all on the same blockchain. While it might not seem relevant now, it forms an important piece in the infrastructure for cross-chain transfers.</p>
<p>Each deployment can keep track of a list of other deployments. It is not necessary for every deployment to be aware of all others. The only requirement is that there is a route, direct or indirect, between any two given instances. Then, whenever a deployment receives a valid objective transfer, it can initiate an identical transfer call on each of its peer chains. Those chains can independently verify that the call is correct before executing the transfer and recalculating balances appropriately.</p>
<p>Attempts to double-spend tokens on multiple instances at once will fail. Because each instance is on the same blockchain, signers can’t execute multiple transactions at once. This means that all deployments will remain in sync, blocking all double-spending attempts.</p>
<p><em>Part III: Isolated cross-deployment</em></p>
<p>Now, the core contract deployments or instances need a way to run a secure version of the protocol without communicating with one another. To enable cross-deployment transaction execution, there can be a permissionless relay node network that monitors onchain objective transfers and relays them to each deployment.</p>
<p>However, at this point there remains a critical flaw in the protocol. If the same sender initiates two transactions on two different deployments simultaneously, both sending over 50% of the sender’s balance to two different addresses, the sender can double-spend. Both deployments will reject the transaction they did not receive initially, and their account balances will differ.</p>
<p>To resolve the double-spending issue, the core contracts use a <em>challenge window</em> . Each window lasts <span class="math">w_f</span> seconds long and consists of two periods: the proposal period and the challenge-only period, which last for <span class="math">w_p</span> and <span class="math">w_c</span> seconds, respectively. The core smart contract disables the <code>objectiveTransfer()</code> function during the challenge-only period.</p>
<p>When a signer submits an objective transfer, the contract checks it for double-spending and then stores it in the <code>challengeableTransfers</code> array. Note that the initial double-spending checks on <code>objectiveTransfer()</code> calls only check for double-spending against the address’ finalized balance. The protocol also checks for double-spending again when <code>cleanChallengeWindow()</code> is called [see below], where the total amount spent in all challangeable transfers are combined and <em>then</em> checked against the finalized balance.  The core contract can finalize the transfer once the active challenge period ends. Any address can call the <code>cleanChallengeWindow()</code> method, which will attempt to execute and delete all challengeable transfers that it can finalize. At this point, the transfer is complete.</p>
<p>Any address can call the <code>challengeAndRecord()</code> method on a core contract, which, given one or more transfers, will check them alongside all challengeable transfers for double-spending. If this process detects double-spending (the total amount of tokens transferred from one address in the challenge window period exceeds its balance), it will mark all challengeable transfers originating from double-spending addresses as problematic. During the <code>cleanChallengeWindow()</code> call, the core contract will delete but not execute problematic transactions.</p>
<p>These rules create a system that:</p>
<ul>
<li>Only needs one honest relay.</li>
<li>Prevents double spending.</li>
<li>Maintains consensus between deployments.</li>
</ul>
<p>But at this point, the protocol isn’t very useful.</p>
<p><em>Part IV: Cross-chain</em></p>
<p>There’s a surprisingly small gap between Part III and a cross-chain token. Because the isolated cross-deployment solution doesn’t require contracts to interact with each other, each deployment doesn’t need any awareness of or connection to any other deployments. <strong>They could be on entirely different blockchains, and the protocol would still function.</strong> We could deploy one instance on each network we want to connect to. Then, when an address initiates a transfer on one chain, relays will replicate it on every other chain.</p>
<p>A trustless exchange contract repackages the protocol into a more familiar form, where EOA addresses can transfer assets from one chain to another. This works by utilizing the swapon-sync-swapoff model, which functions as follows:</p>
<ol>
<li>Swap an ERC20 token on the source chain for the cross-chain token.</li>
<li>Wait for relays to sync cross-chain balances.</li>
<li>Swap the cross-chain token for an ERC20.</li>
</ol>
<p>However, standard liquidity pools like Uniswap can’t swap ERC20 tokens for cross-chain tokens. Instead of relying on the traditional model, we must take an orderbook-like approach.</p>
<p>For sellers:</p>
<ol>
<li>Deposit the ERC20 token into the exchange contract, specifying a price level for the trade.</li>
<li>Wait for a buyer to fulfill the order.</li>
</ol>
<p>For buyers:</p>
<ol>
<li>Fetch unfulfilled trades for the specified price level.</li>
<li>Create and sign transactions to transfer the cross-chain token to the appropriate sellers.</li>
<li>Send the transactions and signatures to the exchange contract, which will verify them.</li>
<li>The exchange contract executes cross-chain token transfers using <code>objectiveTransfer()</code> calls.</li>
<li>The exchange contract releases sellers’ deposited funds to the buyer.</li>
</ol>
<p>This design allows for a standard asset bridging interface between blockchains.</p>
<p><em>Conclusion</em></p>
<p>This protocol enables trustless cross-chain asset transfers using a standardized interface. It would also be possible to expand the protocol to execute arbitrary transactions on an EVM instance. This expanded version of the protocol could be the first step in building a class of blockchain that integrates a vast array of networks into its fundamental design, enabling trustless interoperability between all Ethereum-based blockchains in existence. It would be similar to an L2 blockchain in the sense that transaction data is stored onto another blockchain, but different in the sense that it stores data on multiple blockchains all at once to enhance interoperability.</p>
<p>By itself, this system is not very scalable. High fees are expected as there are multiple transactions required per transfer. However, this issue can be mitigated by either only using L2 networks to store transfer data or building L3s on top of the protocol. Alternatively, transactions could be bundled and their Merkle root submitted onchain alongside a ZK proof in a manner similar to that of a zero-knowledge rollup. In this way, multiple objective transfers could be submitted in only a few transactions.</p>
<p>Finally, any and all feedback is welcome! The project is in its very early stages but I’ll attach the Github repository once I’ve made some more progress.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/on-trustless-cross-chain-asset-transfers/20992">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 15 Nov 2024 16:01:18 +0000</pubDate>
</item>
<item>
<title>[ B²O] Digital Cash Properties for Bitcoin [The Real Satoshi Vision] - ! not a CW Deepfake!</title>
<link>https://ethresear.ch/t/b-o-digital-cash-properties-for-bitcoin-the-real-satoshi-vision-not-a-cw-deepfake/20987</link>
<guid>https://ethresear.ch/t/b-o-digital-cash-properties-for-bitcoin-the-real-satoshi-vision-not-a-cw-deepfake/20987</guid>
<content:encoded><![CDATA[
<div> 关键词：Overpass Channels、比特币、Layer 2解决方案、隐私、可扩展性

总结:<br />
本文提出了一种基于Overpass Channels架构的Layer 2解决方案，旨在为比特币提供高容量交易、隐私保护和无需更改其核心协议的可扩展性。通过对比分析Overpass Channels与BitVM2，文章强调了Overpass的优势在于更好的隐私保护、经济中立性和更强的可扩展性。Overpass Channels通过分布式存储、优化的状态管理和采用Plonky2为基础的zk-SNARKs来实现这些目标，并保证了与比特币HTLC的无缝集成。此外，论文通过形式化证明表明，Overpass Channels维持了比特币的安全属性和货币原则，为比特币区块链上的可扩展性设定了新标准。总体而言，Overpass Channels提供了一个无须信任设置、高效且隐私性强的Layer 2选择，适用于大规模拓展比特币网络交易能力。 <div>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-overpass-channel-sub-paper-1" name="p-51185-overpass-channel-sub-paper-1"></a>Overpass Channel Sub-paper:</h2>
<h1><a class="anchor" href="https://ethresear.ch#p-51185-bitcoin-bo-instant-private-massively-scalable-liquid-bitcoin-with-true-trustless-bridge-pro-maxi-choice-l1-heterogenous-2" name="p-51185-bitcoin-bo-instant-private-massively-scalable-liquid-bitcoin-with-true-trustless-bridge-pro-maxi-choice-l1-heterogenous-2"></a>Bitcoin (B²O): Instant, Private, Massively Scalable, Liquid Bitcoin with true trustless bridge - Pro Maxi Choice  - [L1 heterogenous]</h1>
<p><strong>Author</strong>: Brandon “Cryptskii” Ramsay<br />
<strong>Date</strong>: 2024-11-14</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-abstract-3" name="p-51185-abstract-3"></a>Abstract</h2>
<p>In response to the growing economic challenges faced by traditional financial systems, Bitcoin’s significance as a decentralized, censorship-resistant store of value continues to rise. Building on the Overpass Channels architecture, we propose a privacy-preserving, scalable Layer 2 solution that enables high-volume transactions on Bitcoin without altering its protocol or consensus model. This paper presents a comparative analysis of Overpass Channels and BitVM2, substantiating Overpass’s superiority in privacy, economic neutrality, and scalability. We formalize the system’s operational assumptions and provide rigorous theorems and proofs that validate Overpass’s ability to maintain Bitcoin’s security properties and monetary principles, setting a new benchmark for scalability on Bitcoin’s blockchain.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51185-h-1-introduction-4" name="p-51185-h-1-introduction-4"></a>1. Introduction</h1>
<p>The escalating volatility within traditional financial systems underscores Bitcoin’s foundational role as a decentralized store of value. As Bitcoin adoption grows, the need for scalable and private transaction mechanisms is evident. Leveraging the Overpass Channels architecture<br />
<a href="https://eprint.iacr.org/2024/1526" rel="noopener nofollow ugc">Overpass.2024</a>, we introduce a solution specifically designed to scale Bitcoin transactions without altering its consensus or core protocol. By contrasting Overpass Channels with BitVM2, we elucidate the distinct advantages of our approach in maintaining privacy and network integrity while ensuring economic neutrality.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-h-11-motivation-5" name="p-51185-h-11-motivation-5"></a>1.1 Motivation</h2>
<p>Given the limitations of traditional Layer 2 solutions—often requiring protocol adjustments or trust-based assumptions—the Overpass Channels approach offers a uniquely adaptable, non-invasive solution that enables Bitcoin to scale without compromising its decentralized ethos. While recent advancements like BitVM2 have made strides in SNARK-based verification, Overpass Channels address these challenges through its established hierarchical structure [Section 9.1] and privacy-focused mechanisms [Section 3].</p>
<ul>
<li><strong>Distributed Storage</strong>: Utilizes Overpass’s distributed storage model [Section 10] for efficient transaction handling.</li>
<li><strong>Optimized State Management</strong>: Employs hierarchical sparse Merkle trees [Section 12] for lightweight Bitcoin state management.</li>
<li><strong>Privacy-Enhanced zk-SNARKs</strong>: Integrates Plonky2-based zk-SNARKs [Section 3.8] to preserve transaction privacy.</li>
<li><strong>Compatibility with Bitcoin’s HTLC</strong>: Ensures seamless Bitcoin integration through HTLC adaptation [Section 8.2].</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-h-12-core-principles-6" name="p-51185-h-12-core-principles-6"></a>1.2 Core Principles</h2>
<p>Our design prioritizes the following principles to ensure Overpass Channels aligns with Bitcoin’s core properties:</p>
<ol>
<li><strong>Protocol Integrity</strong>: Achieves scalability without protocol modifications to Bitcoin.</li>
<li><strong>Economic Consistency</strong>: Preserves Bitcoin’s economic incentives and fee structure.</li>
<li><strong>Trustless Design</strong>: Implements trustless operation based on Overpass’s proven cryptographic assumptions [Section 6].</li>
<li><strong>Privacy Assurance</strong>: Enhances transaction privacy by default, following Overpass’s established privacy guarantees [Section 18].</li>
<li><strong>Decentralization Support</strong>: Maintains economic neutrality to avoid concentration of network power.</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-comparison-framework-7" name="p-51185-comparison-framework-7"></a>Comparison Framework</h3>
<p>To formalize the comparison between Overpass Channels and BitVM2, we establish a rigorous evaluation framework based on privacy, scalability, economic neutrality, and security. Each metric is substantiated through theorem-proof structures that quantify the systems’ respective capabilities.</p>
<p><strong>Definition (Layer 2 Security Preservation)</strong>: A Layer 2 solution <span class="math">S</span> preserves Bitcoin’s security model if and only if:<br />
<span class="math">
\forall t \in T, \; P(\text{attack} \mid S) \leq P(\text{attack} \mid \text{Bitcoin})
</span><br />
where <span class="math">T</span> is the set of all transaction types, and <span class="math">P(\text{attack})</span> represents the probability of a successful attack.</p>
<p><strong>Theorem (Security Preservation in Overpass Channels)</strong>: Overpass Channels maintain Bitcoin’s security properties with respect to consensus and decentralization by ensuring that no additional vulnerabilities are introduced in state management or transaction validation:<br />
<span class="math">
P(\text{attack} \mid \text{Overpass}) = P(\text{attack} \mid \text{Bitcoin}).
</span></p>
<p><strong>Proof</strong>: Let <span class="math">A</span> be an adversary aiming to compromise transactions in Overpass Channels. For any attack strategy <span class="math">\sigma</span>:</p>
<ol>
<li>
<p>The adversary must either:</p>
<ul>
<li>Break Bitcoin’s security assumptions, or</li>
<li>Exploit a flaw in Overpass’s zk-SNARK verification or channel closure mechanism.</li>
</ul>
</li>
<li>
<p>Overpass Channels enforce the following:</p>
<ul>
<li>zk-SNARK soundness guarantees transaction validity.</li>
<li>Channel closure requires a valid Bitcoin transaction, preserving the network’s security model.</li>
<li>No additional cryptographic assumptions beyond standard zk-SNARK soundness are introduced.</li>
</ul>
</li>
<li>
<p>Consequently, the security of Overpass Channels is bounded by Bitcoin’s own security assumptions and the integrity of zk-SNARK proofs:<br />
<span class="math">P(\text{attack} \mid \text{Overpass}) = P(\text{attack} \mid \text{Bitcoin})</span></p>
</li>
</ol>
<p>This completes the proof, showing that Overpass Channels do not degrade Bitcoin’s security guarantees.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-technical-architecture-8" name="p-51185-technical-architecture-8"></a>Technical Architecture</h2>
<p>The integration of Overpass Channels with Bitcoin leverages several technical mechanisms to achieve scalability and privacy while preserving security. We provide a structured comparison with BitVM2 to highlight Overpass’s unique advantages.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-unilateral-payment-channels-9" name="p-51185-unilateral-payment-channels-9"></a>Unilateral Payment Channels</h3>
<p>Overpass Channels introduce a unilateral payment channel structure specifically optimized for Bitcoin, distinct from BitVM2’s state model.</p>
<p><strong>Definition (Bitcoin-Compatible Unilateral Channel)</strong><br />
A Bitcoin-compatible unilateral channel <span class="math">C</span> is defined as a tuple <span class="math">(pk_s, pk_r, v, t, \sigma)</span> where:</p>
<ul>
<li><span class="math">pk_s</span>: Sender’s public key</li>
<li><span class="math">pk_r</span>: Receiver’s public key</li>
<li><span class="math">v</span>: Channel value in satoshis</li>
<li><span class="math">t</span>: Timelock value</li>
<li><span class="math">\sigma</span>: Channel signature</li>
</ul>
<p>satisfying the following property:<br />
<span class="math">{ValidChannel}(C) \iff {VerifyBitcoinSig}(sigma, (pk_s, pk_r, v, t)) = {true}</span></p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-cryptographic-constructions-for-bitcoin-channels-10" name="p-51185-cryptographic-constructions-for-bitcoin-channels-10"></a>Cryptographic Constructions for Bitcoin Channels</h3>
<p>Overpass Channels ensure privacy and security through cryptographic constructions designed to operate efficiently on Bitcoin’s existing infrastructure. This approach contrasts with BitVM2’s focus on sequential verification, yielding distinct privacy and efficiency advantages.</p>
<p><strong>Theorem (Channel State Privacy)</strong><br />
Given a channel state <span class="math">S</span> and its corresponding zk-SNARK proof <span class="math">\pi</span>, no adversary <span class="math">A</span> can determine the transaction history or current balances with probability greater than negligible, while still being able to verify the validity of the state.</p>
<p><strong>Proof</strong><br />
Let <span class="math">S</span> be a channel state and <span class="math">\pi</span> its corresponding zk-SNARK proof. Privacy is ensured through a series of games:</p>
<ol>
<li>
<p><strong>Game 0</strong>: The real privacy game, where an adversary <span class="math">A</span> attempts to learn information about the channel state <span class="math">S</span>.</p>
</li>
<li>
<p><strong>Game 1</strong>: Modify Game 0 by replacing the real zk-SNARK proof with a simulated proof.</p>
<p>By the zero-knowledge property of zk-SNARKs:<br />
<span class="math">\left| \Pr[A \text{ wins Game 0}] - \Pr[A \text{ wins Game 1}] \right| \leq \text{negl}(\lambda)</span><br />
where <span class="math">\text{negl}(\lambda)</span> is a negligible function in the security parameter <span class="math">\lambda</span>.</p>
</li>
<li>
<p><strong>Game 2</strong>: Replace the real channel state <span class="math">S</span> with a random, valid state.</p>
<p>By the hiding property of the commitment scheme:<br />
$\left| \Pr[A \text{ wins Game 1}] - \Pr[A \text{ wins Game 2}] \right| \leq \text{negl}(\lambda)$$</p>
</li>
</ol>
<p>In Game 2, the adversary receives no information about the actual channel state <span class="math">S</span>, resulting in:<br />
<span class="math">\Pr[A \text{ wins Game 2}] = \frac{1}{2}</span></p>
<p>Through this sequence of games, we conclude that <span class="math">A</span>'s advantage in the real game (Game 0) is negligible, establishing privacy for the Overpass Channels.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-channel-operations-and-bitcoin-script-integration-11" name="p-51185-channel-operations-and-bitcoin-script-integration-11"></a>Channel Operations and Bitcoin Script Integration</h3>
<p>Overpass Channels implement functionality through Bitcoin-compatible scripts, enabling secure channel operations without modifying Bitcoin’s protocol. This approach differs from BitVM2, which requires sequential verification stages, by focusing on privacy preservation and operational efficiency.</p>
<p><strong>Algorithm: Channel Opening on Bitcoin</strong></p>
<p><strong>Require:</strong> Sender keys <span class="math">sk_s</span>, <span class="math">pk_s</span>, Receiver public key <span class="math">pk_r</span>, Channel value <span class="math">v</span></p>
<ol>
<li>
<p>Generate funding transaction <span class="math">T_f</span> with the following script:</p>
<pre><code class="lang-auto">OP_IF
   OP_SHA256 H(revocation_key)
   OP_EQUALVERIFY
   pk_r OP_CHECKSIG
OP_ELSE
   timeout OP_CHECKLOCKTIMEVERIFY
   OP_DROP
   pk_s OP_CHECKSIG
OP_ENDIF
</code></pre>
</li>
<li>
<p>Broadcast <span class="math">T_f</span> to the Bitcoin network.</p>
</li>
<li>
<p>Generate zk-SNARK proof <span class="math">\pi</span> of the channel state validity.</p>
</li>
</ol>
<p><strong>Ensure:</strong> <span class="math">(T_f, \pi)</span></p>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-comparison-with-bitvm2-12" name="p-51185-comparison-with-bitvm2-12"></a>Comparison with BitVM2</h2>
<p>Overpass Channels and BitVM2 both utilize zk-SNARKs to enable advanced transaction verification on Bitcoin. However, their approaches to state management, privacy, and scalability vary significantly. This section provides a detailed comparison to illustrate the advantages of Overpass Channels over BitVM2.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-architectural-differences-13" name="p-51185-architectural-differences-13"></a>Architectural Differences</h3>
<p>The core architectural design of each system impacts their performance and scalability. Overpass Channels leverage distributed state management and privacy-preserving mechanisms, while BitVM2 emphasizes sequential verification stages.</p>
<div class="md-table">
<table>
<thead>
<tr>
<th>Feature</th>
<th>Overpass Channels</th>
<th>BitVM2</th>
</tr>
</thead>
<tbody>
<tr>
<td>State Model</td>
<td>Privacy-preserving off-chain</td>
<td>Off-chain with on-chain verification</td>
</tr>
<tr>
<td>Privacy</td>
<td>Full transaction privacy</td>
<td>Basic transaction privacy</td>
</tr>
<tr>
<td>Scalability</td>
<td><span class="math">O(n)</span> horizontal scaling</td>
<td><span class="math">O(n)</span> with verification overhead</td>
</tr>
<tr>
<td>Trust Model</td>
<td>Bitcoin-equivalent</td>
<td>Bitcoin-equivalent with setup</td>
</tr>
<tr>
<td>Impact on Miners</td>
<td>Neutral</td>
<td>Neutral with verification cost</td>
</tr>
<tr>
<td>Verification Method</td>
<td>Optimized SNARK proofs</td>
<td>Sequential SNARK-based verification</td>
</tr>
</tbody>
</table>
</div><h3><a class="anchor" href="https://ethresear.ch#p-51185-economic-implications-14" name="p-51185-economic-implications-14"></a>Economic Implications</h3>
<p>The economic implications of each approach significantly affect Bitcoin’s fee market and miner incentives. While both systems maintain Bitcoin’s security model, their respective costs and operational overhead differ.</p>
<p><strong>Theorem (Incentive Compatibility)</strong><br />
Let <span class="math">M</span> represent Bitcoin miners, and let <span class="math">I(m)</span> be the expected income of a miner <span class="math">m</span>. Under both Overpass Channels and BitVM2:<br />
<span class="math">\forall m \in M: E[I(m) \mid L2] \geq E[I(m) \mid Bitcoin]</span><br />
with system-specific overhead distributions as follows:<br />
<span class="math">O_{\text{Overpass}} = O_{\text{constant}}</span><br />
<span class="math">O_{\text{BitVM2}} = O_{\text{verification}} + O_{\text{setup}}</span></p>
<p><strong>Proof</strong><br />
For Overpass Channels:</p>
<ol>
<li>Channel operations rely on standard Bitcoin transactions.</li>
<li>Verification burden remains constant due to optimized SNARK proofs.</li>
<li>Mining decentralization and fee structures remain unaffected.</li>
</ol>
<p>For BitVM2:</p>
<ol>
<li>Similar reliance on standard Bitcoin transactions.</li>
<li>Initial setup and verification costs introduced.</li>
<li>Verification overhead potentially impacts miner fees due to increased computational requirements.</li>
</ol>
<p>Therefore, both systems preserve Bitcoin’s incentive model, although Overpass offers a more consistent and lower overhead for miners.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-network-effects-and-liquidity-15" name="p-51185-network-effects-and-liquidity-15"></a>Network Effects and Liquidity</h3>
<p>The liquidity distribution and network effects of each system are crucial for Bitcoin’s economic stability. Overpass Channels achieve liquidity efficiency with minimized operational costs, offering an advantage over BitVM2’s verification overhead.</p>
<p><strong>Theorem (Liquidity Preservation)</strong><br />
In a network with total liquidity <span class="math">L</span>, both systems preserve Bitcoin’s liquidity pool:<br />
<span class="math">L_{\text{effective}} = L_{\text{total}} - O_{\text{system}}</span><br />
where:<br />
<span class="math">O_{\text{Overpass}} &lt; O_{\text{BitVM2}}</span><br />
due to Overpass’s optimized state management and lack of setup costs.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-security-considerations-and-risk-analysis-16" name="p-51185-security-considerations-and-risk-analysis-16"></a>Security Considerations and Risk Analysis</h2>
<p>Layer 2 solutions must be carefully analyzed for security implications to ensure they do not compromise Bitcoin’s core properties. This section provides a comprehensive examination of the security models for Overpass Channels and BitVM2, focusing on privacy, attack surface, and resistance to double-spend attacks.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-attack-surface-analysis-17" name="p-51185-attack-surface-analysis-17"></a>Attack Surface Analysis</h3>
<p>The attack surface of each system represents the potential vulnerability points that could be exploited by adversaries. Overpass Channels and BitVM2 both introduce minimal attack surfaces, but their structural differences affect the composition of these surfaces.</p>
<p><strong>Definition (Attack Surface Extension)</strong><br />
For a Layer 2 solution <span class="math">L</span>, the attack surface extension <span class="math">E(L)</span> is defined as:<br />
<span class="math">E(L) = \{(v, p) \mid v \in V(L) \setminus V(Bitcoin), p &gt; 0\}</span><br />
where <span class="math">V(L)</span> is the set of potential vulnerability points in <span class="math">L</span> and <span class="math">p</span> is the probability of successful exploitation.</p>
<p><strong>Theorem (Equivalent Base Extension)</strong><br />
Both systems maintain minimal attack surface extension:<br />
<span class="math">|E(\text{Overpass})| = O(1)</span><br />
<span class="math">|E(\text{BitVM2})| = O(1)</span><br />
with different vulnerability classes:<br />
<span class="math">V_{\text{Overpass}} = \{V_{\text{privacy}}, V_{\text{state}}\}</span><br />
<span class="math">V_{\text{BitVM2}} = \{V_{\text{setup}}, V_{\text{verify}}\}</span></p>
<p><strong>Proof</strong><br />
For both Overpass Channels and BitVM2:</p>
<ol>
<li>State transitions and transaction validity are secured by zk-SNARKs.</li>
<li>Channel operations rely on standard Bitcoin transaction security.</li>
<li>No additional consensus requirements are introduced.</li>
</ol>
<p>Key distinctions include:</p>
<ol>
<li>
<p><strong>Privacy Mechanism</strong>:</p>
<ul>
<li>Overpass: Full privacy achieved through state channels.</li>
<li>BitVM2: Basic privacy limited by sequential verification.</li>
</ul>
</li>
<li>
<p><strong>Setup Requirements</strong>:</p>
<ul>
<li>Overpass: Direct channel initialization without additional setup.</li>
<li>BitVM2: Requires an initial verification setup phase.</li>
</ul>
</li>
</ol>
<p>Thus, both systems achieve minimal and comparable attack surface extensions, though the structure of vulnerability classes differs.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-double-spend-prevention-18" name="p-51185-double-spend-prevention-18"></a>Double-Spend Prevention</h3>
<p>Double-spend prevention is essential for maintaining Bitcoin’s integrity as a monetary system. Both Overpass Channels and BitVM2 implement robust mechanisms to prevent double-spend attacks.</p>
<p><strong>Theorem (Double-Spend Prevention)</strong><br />
For both systems, the probability of a successful double-spend attack <span class="math">P(DS)</span> is bounded by:<br />
<span class="math">P(DS) \leq \min(P(\text{Bitcoin\_DS}), P(\text{zk\_break}))</span><br />
where <span class="math">P(\text{Bitcoin\_DS})</span> represents the probability of a double-spend on Bitcoin and <span class="math">P(\text{zk\_break})</span> represents the probability of breaking the zk-SNARK system.</p>
<p><strong>Proof</strong><br />
Let <span class="math">A</span> be an adversary attempting a double-spend attack. For success, <span class="math">A</span> must either:</p>
<ol>
<li>Compromise Bitcoin’s underlying security model with probability <span class="math">P(\text{Bitcoin\_DS})</span>.</li>
<li>Generate a false zk-SNARK proof with probability <span class="math">P(\text{zk\_break})</span>.</li>
</ol>
<p>Additionally, both systems enforce a channel closure mechanism that ensures:<br />
<span class="math">\forall s_1, s_2 \in \text{States}: \text{Close}(s_1) \land \text{Close}(s_2) \implies s_1 = s_2</span></p>
<p>Thus, the probability of a successful double-spend attack is bounded by the minimum probability of either compromising Bitcoin’s security or breaking the zk-SNARK proof system, regardless of system-specific differences.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-impact-on-bitcoins-security-model-19" name="p-51185-impact-on-bitcoins-security-model-19"></a>Impact on Bitcoin’s Security Model</h3>
<p>Each Layer 2 solution must be assessed for its impact on Bitcoin’s core security properties, such as decentralization, censorship resistance, and immutability. Overpass Channels and BitVM2 maintain these properties, though their verification and state management differ.</p>
<p><strong>Definition (Security Model Preservation)</strong><br />
A Layer 2 solution <span class="math">S</span> preserves Bitcoin’s security model if:<br />
<span class="math">\forall p \in \text{Properties(Bitcoin)}: \text{Guarantee}(p \mid S) \geq \text{Guarantee}(p \mid \text{Bitcoin})</span><br />
where <span class="math">\text{Properties(Bitcoin)}</span> includes decentralization, censorship resistance, and immutability.</p>
<p><strong>Theorem (Security Model Impact)</strong><br />
Both Overpass Channels and BitVM2 maintain Bitcoin’s security model with distinct architectural trade-offs:<br />
<span class="math">\Delta_{\text{security}}(\text{Overpass}) = \Delta_{\text{security}}(\text{BitVM2}) = 0</span><br />
though they follow different verification pathways:<br />
<span class="math">\text{Path}_{\text{Overpass}} = \{\text{Privacy}, \text{StateManagement}\}</span><br />
<span class="math">\text{Path}_{\text{BitVM2}} = \{\text{Setup}, \text{VerificationFlow}\}</span></p>
<p><strong>Proof</strong><br />
To assess security preservation, consider the following for both systems:</p>
<ol>
<li>
<p><strong>Consensus Requirements</strong>:</p>
<ul>
<li>Both systems operate without modifying Bitcoin’s consensus.</li>
</ul>
</li>
<li>
<p><strong>Cryptographic Assumptions</strong>:</p>
<ul>
<li>Each system relies on zk-SNARKs, ensuring equivalent cryptographic strength.</li>
</ul>
</li>
<li>
<p><strong>State and Transaction Management</strong>:</p>
<ul>
<li>Overpass: Employs integrated, privacy-preserving state channels, minimizing exposure.</li>
<li>BitVM2: Utilizes a sequential verification process that introduces verification layers but maintains on-chain compatibility.</li>
</ul>
</li>
<li>
<p><strong>Implementation Distinctions</strong>:</p>
<ul>
<li>Overpass prioritizes direct state transitions, reducing operational overhead.</li>
<li>BitVM2 requires setup and verification sequences, increasing complexity.</li>
</ul>
</li>
</ol>
<p>Therefore, both systems preserve Bitcoin’s security model while following distinct approaches to verification and state management.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-liveness-and-availability-analysis-20" name="p-51185-liveness-and-availability-analysis-20"></a>Liveness and Availability Analysis</h3>
<p>The liveness and availability of transactions are critical for user experience and adoption. Overpass Channels and BitVM2 achieve comparable liveness guarantees through different transaction handling mechanisms.</p>
<p><strong>Theorem (Liveness Guarantee)</strong><br />
Under both systems, transaction liveness <span class="math">L(t)</span> for a transaction <span class="math">t</span> is guaranteed with probability:<br />
<span class="math">P(L(t)) \geq 1 - (1 - p)^k</span><br />
where <span class="math">p</span> is the probability of successful Bitcoin transaction inclusion and <span class="math">k</span> is the number of confirmation attempts.</p>
<p><strong>Proof</strong><br />
For both systems:</p>
<ol>
<li>
<p><strong>Channel Operations</strong>:</p>
<ul>
<li>Rely on standard Bitcoin transactions for channel creation and closure.</li>
</ul>
</li>
<li>
<p><strong>Verification Methodology</strong>:</p>
<ul>
<li>Both systems use zk-SNARK proofs for verification, enabling off-chain transaction finality.</li>
</ul>
</li>
<li>
<p><strong>Channel Closure Attempts</strong>:</p>
<ul>
<li>With <span class="math">k</span> attempts, the probability of successful closure is given by:<br />
<span class="math">P(\text{closure\_success}) = 1 - (1 - p)^k</span></li>
</ul>
</li>
</ol>
<p>Since each system relies on Bitcoin’s underlying liveness properties for final settlement, they both achieve equivalent liveness guarantees.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-long-term-security-implications-21" name="p-51185-long-term-security-implications-21"></a>Long-term Security Implications</h3>
<p>Both Overpass Channels and BitVM2 must be evaluated for their long-term security impacts, especially in terms of protocol longevity and resistance to future attack vectors.</p>
<p><strong>Theorem (Security Model Evolution)</strong><br />
The long-term security impact <span class="math">I(t)</span> of both Layer 2 solutions at time <span class="math">t</span> satisfies:<br />
<span class="math">\lim_{t \to \infty} I(t) = 0</span><br />
with differing composition vectors:<br />
<span class="math">V_{\text{Overpass}}(t) = \{v_{\text{privacy}}(t), v_{\text{state}}(t)\}</span><br />
<span class="math">V_{\text{BitVM2}}(t) = \{v_{\text{setup}}(t), v_{\text{verify}}(t)\}</span></p>
<p><strong>Proof</strong><br />
Consider the following security properties for both systems:</p>
<ol>
<li><strong>Longevity of Cryptographic Assumptions</strong>:</li>
</ol>
<ul>
<li>Both rely on zk-SNARKs with long-term security guarantees, ensuring consistency over time.</li>
</ul>
<ol start="2">
<li><strong>System-Specific Implications</strong>:</li>
</ol>
<ul>
<li>Overpass: Long-term stability due to privacy-preserving channels and minimal setup requirements.</li>
<li>BitVM2: Security preserved through on-chain verification, though with added complexity in setup and verification stages.</li>
</ul>
<ol start="3">
<li><strong>Impact on Bitcoin’s Security</strong>:</li>
</ol>
<ul>
<li>Neither system requires alterations to Bitcoin’s protocol, preserving the core security properties indefinitely.</li>
</ul>
<p>Thus, the long-term security impact remains neutral for both systems, with each maintaining minimal additional risk over time.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-privacy-guarantees-and-economic-implications-22" name="p-51185-privacy-guarantees-and-economic-implications-22"></a>Privacy Guarantees and Economic Implications</h2>
<p>The privacy and economic characteristics of a Layer 2 solution significantly affect Bitcoin’s fungibility and monetary stability. Overpass Channels and BitVM2 both employ zk-SNARKs, yet their approaches to privacy and economic neutrality are fundamentally different.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-privacy-model-23" name="p-51185-privacy-model-23"></a>Privacy Model</h3>
<p>Privacy within a Layer 2 solution is critical for ensuring that transactions are indistinguishable, preserving Bitcoin’s fungibility. Overpass Channels provide enhanced privacy over BitVM2 due to its integrated, privacy-preserving state channels.</p>
<p><strong>Definition (Transaction Privacy)</strong><br />
A transaction <span class="math">T</span> in a Layer 2 system provides <span class="math">\delta</span>-privacy if for any adversary <span class="math">A</span>:<br />
<span class="math">\left| \Pr[A(T) = 1] - \Pr[A(T') = 1] \right| \leq \delta</span><br />
where <span class="math">T'</span> is any other valid transaction with identical public parameters.</p>
<p><strong>Theorem (Privacy Guarantees)</strong><br />
Overpass Channels achieve an enhanced level of privacy, denoted <span class="math">\varepsilon</span>-privacy:<br />
<span class="math">\varepsilon_{\text{Overpass}} \leq \frac{1}{2^\lambda}</span><br />
compared to BitVM2’s basic transaction privacy:<br />
<span class="math">\varepsilon_{\text{BitVM2}} \leq \frac{1}{2^\lambda} + \delta_{\text{state}}</span><br />
where <span class="math">\delta_{\text{state}}</span> represents additional information leakage due to BitVM2’s state verification.</p>
<p><strong>Proof</strong><br />
Let <span class="math">A</span> be an adversary attempting to distinguish between transactions:</p>
<ol>
<li><strong>Base zk-SNARK Privacy</strong>:</li>
</ol>
<ul>
<li>By the zero-knowledge property of zk-SNARKs, for any input <span class="math">x</span> and witness <span class="math">w</span>:<br />
<span class="math">\{\text{Prove}(x, w)\} \approx_c \{\text{Sim}(x)\}</span></li>
</ul>
<ol start="2">
<li><strong>System-Specific Privacy Distinctions</strong>:</li>
</ol>
<ul>
<li>
<p>Overpass: Full state privacy, leading to negligible information leakage:<br />
<span class="math">\left| \Pr[A(\pi, P, U) = 1] - \Pr[A(\text{Sim}(\pi), P, U) = 1] \right| \leq \frac{1}{2^\lambda}</span></p>
</li>
<li>
<p>BitVM2: State verification introduces potential leakage:<br />
<span class="math">\left| \Pr[A(\pi, P, U) = 1] - \Pr[A(\text{Sim}(\pi), P, U) = 1] \right| \leq \frac{1}{2^\lambda} + \delta_{\text{state}}</span></p>
</li>
</ul>
<ol start="3">
<li><strong>Conclusion</strong>:<br />
While both systems provide robust privacy through zk-SNARKs, Overpass achieves stronger privacy guarantees due to its privacy-preserving state channels, resulting in reduced leakage.</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-economic-impact-analysis-24" name="p-51185-economic-impact-analysis-24"></a>Economic Impact Analysis</h3>
<p>The economic implications of each system on Bitcoin’s fee market and miner incentives are essential to maintaining a balanced ecosystem.</p>
<p><strong>Theorem (Fee Market Preservation)</strong><br />
Under both systems, Bitcoin’s fee market equilibrium <span class="math">E</span> remains stable:<br />
<span class="math">|E_{\text{L2}} - E_{\text{Bitcoin}}| \leq \epsilon</span><br />
where <span class="math">\epsilon</span> is a negligible factor, with differing overhead distributions:<br />
<span class="math">\epsilon_{\text{Overpass}} = O_{\text{channel}} + O_{\text{privacy}}</span><br />
<span class="math">\epsilon_{\text{BitVM2}} = O_{\text{setup}} + O_{\text{verify}}</span></p>
<p><strong>Proof</strong><br />
For a transaction <span class="math">t</span>, the fee function <span class="math">F(t)</span> can be expressed as:<br />
<span class="math">F(t) = \alpha \cdot s(t) + \beta \cdot p(t)</span><br />
where <span class="math">s(t)</span> is the transaction size, and <span class="math">p(t)</span> is the priority.</p>
<ol>
<li><strong>Overpass Channels</strong>:</li>
</ol>
<ul>
<li>Operations incur minimal overhead due to privacy-preserving channels.</li>
<li>Fee structure remains consistent with Bitcoin’s standard model.</li>
</ul>
<ol start="2">
<li><strong>BitVM2</strong>:</li>
</ol>
<ul>
<li>Additional setup and verification phases introduce operational overhead.</li>
<li>The fee model remains consistent but with added verification costs.</li>
</ul>
<p>Thus, while both systems preserve the equilibrium of Bitcoin’s fee market, Overpass offers a more efficient fee structure by minimizing extraneous costs.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-liquidity-efficiency-25" name="p-51185-liquidity-efficiency-25"></a>Liquidity Efficiency</h3>
<p>Efficient liquidity utilization is essential for a Layer 2 solution to scale while maintaining user accessibility and network sustainability. Overpass Channels provide a more optimized liquidity model than BitVM2 due to minimized verification and operational overhead.</p>
<p><strong>Theorem (Liquidity Utilization)</strong><br />
Both systems achieve efficient liquidity utilization <span class="math">U</span>, with different optimization paths:</p>
<p>For Overpass Channels:<br />
<span class="math">U_{\text{Overpass}} = \frac{L_{\text{active}}}{L_{\text{total}}} \cdot \prod_{i=1}^n r_i</span></p>
<p>For BitVM2:<br />
<span class="math">U_{\text{BitVM2}} = \frac{L_{\text{active}}}{L_{\text{total}}} \cdot \prod_{i=1}^n (r_i - \sigma_i)</span></p>
<p>where <span class="math">L_{\text{active}}</span> is the active channel liquidity, <span class="math">L_{\text{total}}</span> is the total liquidity, <span class="math">r_i</span> represents rebalancing factors, and <span class="math">\sigma_i</span> indicates verification overhead in BitVM2.</p>
<p><strong>Proof</strong><br />
Consider the set <span class="math">C</span> of all channels in the system. For each channel <span class="math">c \in C</span>:</p>
<ol>
<li>
<p><strong>Liquidity Utilization</strong>:<br />
<span class="math">u(c) = \frac{v(c)}{V(c)} \cdot r(c)</span><br />
where <span class="math">v(c)</span> is the value utilized and <span class="math">V(c)</span> is the channel capacity.</p>
</li>
<li>
<p><strong>System-Specific Utilization Factors</strong>:</p>
</li>
</ol>
<ul>
<li>
<p>Overpass Channels:<br />
<span class="math">U_{\text{Overpass}} = \frac{\sum_{c \in C} u(c) \cdot V(c)}{\sum_{c \in C} V(c)}</span><br />
indicating minimal operational costs and high liquidity efficiency.</p>
</li>
<li>
<p>BitVM2:<br />
<span class="math">U_{\text{BitVM2}} = \frac{\sum_{c \in C} (u(c) - \sigma(c)) \cdot V(c)}{\sum_{c \in C} V(c)}</span><br />
where <span class="math">\sigma(c)</span> reflects verification overhead, reducing effective liquidity.</p>
</li>
</ul>
<ol start="3">
<li><strong>Conclusion</strong>:<br />
Overpass Channels exhibit greater liquidity efficiency as they avoid the additional verification overhead imposed by BitVM2.</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-economic-centralization-resistance-26" name="p-51185-economic-centralization-resistance-26"></a>Economic Centralization Resistance</h3>
<p>Preserving decentralization within the economic model is crucial to avoid power concentration in a Layer 2 solution. Overpass Channels and BitVM2 maintain Bitcoin’s decentralization, but Overpass’s structure is inherently more resistant to centralization.</p>
<p><strong>Definition (Centralization Resistance)</strong><br />
A system <span class="math">S</span> is <span class="math">\rho</span>-centralization resistant if no entity <span class="math">e</span> can control more than <span class="math">\rho</span> fraction of the system’s economic activity:<br />
<span class="math">\forall e: \frac{\text{Control}(e)}{\text{Total}} \leq \rho</span></p>
<p><strong>Theorem (Decentralization Maintenance)</strong><br />
Both systems maintain Bitcoin’s centralization resistance bound <span class="math">\rho</span>:<br />
<span class="math">\rho_{\text{L2}} \leq \rho_{\text{Bitcoin}}</span><br />
though they differ in their resistance mechanisms:<br />
<span class="math">R_{\text{Overpass}} = \{R_{\text{privacy}}, R_{\text{state}}\}</span><br />
<span class="math">R_{\text{BitVM2}} = \{R_{\text{setup}}, R_{\text{verify}}\}</span></p>
<p><strong>Proof</strong><br />
For both systems, we examine centralization resistance as follows:</p>
<ol>
<li><strong>Architectural Aspects</strong>:</li>
</ol>
<ul>
<li>
<p>Overpass Channels:</p>
<ul>
<li>Privacy-preserving channels reduce reliance on trusted parties.</li>
<li>Distributed state management minimizes central control.</li>
</ul>
</li>
<li>
<p>BitVM2:</p>
<ul>
<li>Initial setup and verification dependencies may centralize certain operations.</li>
</ul>
</li>
</ul>
<ol start="2">
<li><strong>Economic Distribution</strong>:</li>
</ol>
<ul>
<li>Both systems employ decentralized transaction processing and verification to avoid reliance on centralized entities.</li>
<li>Dynamic rebalancing mechanisms distribute control across network participants.</li>
</ul>
<p>Thus, Overpass Channels provide a higher resistance to centralization due to minimized setup dependencies and enhanced privacy, while BitVM2 maintains resistance but with increased operational complexity.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-long-term-economic-stability-27" name="p-51185-long-term-economic-stability-27"></a>Long-term Economic Stability</h3>
<p>Ensuring economic stability over time is critical for the viability of a Layer 2 solution on Bitcoin. Both Overpass Channels and BitVM2 aim to preserve Bitcoin’s economic model; however, Overpass offers more consistent long-term stability due to its minimal operational overhead and direct transaction management.</p>
<p><strong>Theorem (Economic Model Preservation)</strong><br />
Both systems preserve Bitcoin’s long-term economic stability:<br />
<span class="math">\lim_{t \to \infty} |M_{\text{L2}}(t) - M_{\text{Bitcoin}}(t)| = 0</span><br />
where <span class="math">M(t)</span> represents the economic model at time <span class="math">t</span>. Each system has different stability vectors:<br />
<span class="math">S_{\text{Overpass}}(t) = \{S_{\text{privacy}}(t), S_{\text{channel}}(t)\}</span><br />
<span class="math">S_{\text{BitVM2}}(t) = \{S_{\text{verify}}(t), S_{\text{setup}}(t)\}</span></p>
<p><strong>Proof</strong><br />
To examine economic stability, we consider the following for each system:</p>
<ol>
<li><strong>Monetary Properties</strong>:</li>
</ol>
<ul>
<li>Both Overpass Channels and BitVM2:
<ul>
<li>Preserve Bitcoin’s fixed supply.</li>
<li>Maintain its issuance schedule.</li>
<li>Do not alter mining incentives or economic dynamics.</li>
</ul>
</li>
</ul>
<ol start="2">
<li><strong>System-Specific Characteristics</strong>:</li>
</ol>
<ul>
<li><strong>Overpass Channels</strong>:
<ul>
<li>The privacy-focused, channel-based structure ensures consistent fee and operational costs.</li>
<li>Direct state management minimizes fluctuations in transaction handling fees.</li>
</ul>
</li>
<li><strong>BitVM2</strong>:
<ul>
<li>Additional setup and verification stages introduce occasional cost spikes, which may lead to minor fee market adjustments over time.</li>
<li>The sequential verification process results in varying operational expenses.</li>
</ul>
</li>
</ul>
<ol start="3">
<li>
<p><strong>Network Effects</strong>:</p>
<ul>
<li>Both systems are designed to maintain decentralization and support censorship resistance, ensuring long-term usability and user accessibility.</li>
</ul>
<p>As <span class="math">t \to \infty</span>, both systems converge towards stable economic models with minor fluctuations for BitVM2 due to its additional verification overhead. Overpass Channels, however, offer a smoother economic trajectory with fewer cost variations.</p>
</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-comparative-analysis-of-trustless-mechanisms-28" name="p-51185-comparative-analysis-of-trustless-mechanisms-28"></a>Comparative Analysis of Trustless Mechanisms</h2>
<p>A fundamental requirement for Layer 2 solutions on Bitcoin is the minimization of trust assumptions. Overpass Channels and BitVM2 each establish distinct trust models, yet Overpass achieves stronger trust minimization due to its direct channel structure and privacy integration.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-trust-model-foundations-29" name="p-51185-trust-model-foundations-29"></a>Trust Model Foundations</h3>
<p>The level of trust required by a Layer 2 system impacts its alignment with Bitcoin’s trustless design. We formalize the trust minimization for each system.</p>
<p><strong>Theorem (Trust Minimization)</strong><br />
For both Layer 2 systems <span class="math">B</span>, the trust requirement <span class="math">T(B)</span> can be defined as:<br />
<span class="math">T(B) = \sum_{i=1}^n w_i \cdot t_i</span><br />
where <span class="math">w_i</span> represents trust weights and <span class="math">t_i</span> represents individual trust assumptions. Each system has unique trust vectors:<br />
<span class="math">T_{\text{Overpass}} = \{t_{\text{privacy}}, t_{\text{state}}\}</span><br />
<span class="math">T_{\text{BitVM2}} = \{t_{\text{setup}}, t_{\text{verify}}\}</span></p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-bridge-trust-models-30" name="p-51185-bridge-trust-models-30"></a>Bridge Trust Models</h3>
<p>Layer 2 solutions require secure bridging mechanisms with Bitcoin’s Layer 1 to facilitate interoperability while preserving trust assumptions.</p>
<p><strong>Definition (Bridge Security)</strong><br />
A bridge transaction maintains Bitcoin’s trust assumptions if:<br />
<span class="math">\forall \text{tx} \in \text{Transactions}: \text{Trust}(\text{tx}) \subseteq \text{Trust}(\text{Bitcoin})</span><br />
where <span class="math">\text{Trust(Bitcoin)}</span> encompasses Bitcoin’s base security assumptions.</p>
<p><strong>Theorem (Trust Preservation)</strong><br />
Both systems preserve Bitcoin’s trust model through different bridging mechanisms:<br />
<span class="math">T(\text{L2}) = T(\text{Bitcoin}) + T(\text{SNARK})</span><br />
where <span class="math">T(\text{SNARK})</span> represents the trust assumption introduced by zk-SNARKs. Distinct implementation paths are followed:<br />
<span class="math">\text{Path}_{\text{Overpass}} = \{\text{Privacy}, \text{StateTransition}\}</span><br />
<span class="math">\text{Path}_{\text{BitVM2}} = \{\text{Setup}, \text{VerificationFlow}\}</span></p>
<p><strong>Proof</strong><br />
The preservation of trust assumptions is achieved by both systems through:</p>
<ol>
<li>
<p><strong>zk-SNARK Trust Requirement</strong>:</p>
<ul>
<li>Both systems introduce SNARK-based proofs, which assume soundness and non-interactivity.</li>
</ul>
</li>
<li>
<p><strong>System-Specific Mechanisms</strong>:</p>
<ul>
<li>
<p><strong>Overpass Channels</strong>:</p>
<ul>
<li>Direct channel state transitions ensure trust minimization.</li>
<li>Integrated privacy reduces the reliance on trusted setups.</li>
</ul>
</li>
<li>
<p><strong>BitVM2</strong>:</p>
<ul>
<li>Requires an initial setup phase, adding a layer of trust for configuration integrity.</li>
<li>Sequential verification process may introduce dependencies on verification nodes.</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>In summary, both systems maintain Bitcoin’s trust model, but Overpass achieves a higher degree of trust minimization by avoiding setup requirements and emphasizing privacy-preserving operations.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51185-conclusion-31" name="p-51185-conclusion-31"></a>Conclusion</h2>
<p>This paper has provided a detailed comparative analysis of Overpass Channels and BitVM2 as Layer 2 solutions for Bitcoin, focusing on scalability, privacy, security, and economic neutrality. Through rigorous theorem-proof structures, we have demonstrated Overpass Channels’ unique advantages in privacy preservation, efficient liquidity utilization, and trust minimization, establishing it as a leading solution for scaling Bitcoin without altering its core protocol.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-summary-of-key-findings-32" name="p-51185-summary-of-key-findings-32"></a>Summary of Key Findings</h3>
<p>Overpass Channels emerge as a compelling choice for high-volume, privacy-preserving transactions on Bitcoin, offering the following distinct advantages:</p>
<ul>
<li><strong>Enhanced Privacy</strong>: Through integrated privacy-preserving state channels, Overpass ensures stronger privacy guarantees, minimizing information leakage compared to BitVM2.</li>
<li><strong>Scalability and Efficiency</strong>: Achieving <span class="math">O(n)</span> horizontal scaling with minimal verification overhead, Overpass efficiently supports high transaction throughput, whereas BitVM2 incurs higher verification and setup costs.</li>
<li><strong>Economic Neutrality and Stability</strong>: Closely aligned with Bitcoin’s fee market structure, Overpass preserves Bitcoin’s economic neutrality without introducing additional cost burdens.</li>
<li><strong>Trustless Design</strong>: Overpass Channels eliminate the need for trusted setups and emphasize zk-SNARK-based verification, achieving stronger trust minimization than BitVM2’s setup-dependent model.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-overpass-channels-as-the-cash-layer-for-layer-1-blockchains-33" name="p-51185-overpass-channels-as-the-cash-layer-for-layer-1-blockchains-33"></a>Overpass Channels as the Cash Layer for Layer 1 Blockchains</h3>
<p>While Bitcoin serves as an optimal reserve asset and “gold layer” of a decentralized financial network, Overpass Channels have the potential to become the “cash layer” not only for Bitcoin but for any Layer 1 blockchain that integrates with its architecture. By extending Overpass Channels as a universal Layer 2 solution, any compatible blockchain can benefit from instant, privacy-preserving transactions with high scalability, thus providing a cash layer capable of supporting everyday transactional demands across various blockchain ecosystems.</p>
<p>This analysis specifically highlights Overpass Channels in the context of Bitcoin as an extension of the original Overpass Channels research. However, the modular design of Overpass allows seamless integration with multiple blockchains, enhancing each one with Overpass’s advanced privacy and scalability benefits. This interoperability offers a transformative vision: a decentralized, multi-chain economy where Bitcoin and Overpass work symbiotically, with Bitcoin as the global reserve and Overpass as the universal, privacy-preserving cash layer.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-future-directions-34" name="p-51185-future-directions-34"></a>Future Directions</h3>
<p>Several areas of future research and development can help realize the full potential of Overpass Channels across multiple blockchain networks:</p>
<ol>
<li><strong>zk-SNARK Optimization</strong>: Further research into zk-SNARK efficiency can reduce computational overhead, making verification faster and more accessible across diverse Layer 1 blockchains.</li>
<li><strong>Expanding Integration Capabilities</strong>: Developing tools and protocols for seamless Overpass integration with other blockchains will extend its applicability as a cash layer beyond Bitcoin.</li>
<li><strong>Real-world Deployment and Audits</strong>: Comprehensive security audits and real-world testing will validate Overpass’s privacy and scalability claims, ensuring robust performance across different blockchain networks.</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-final-remarks-35" name="p-51185-final-remarks-35"></a>Final Remarks</h3>
<p>In conclusion, Overpass Channels represent a groundbreaking Layer 2 solution that enhances the scalability and privacy of Bitcoin and has the potential to serve as a universal cash layer across various Layer 1 blockchains. By offering a scalable, privacy-focused transaction layer, Overpass can redefine the usability and accessibility of decentralized finance. This cash layer for the Internet enables a flexible, interoperable financial system that respects user privacy and decentralization principles, positioning Bitcoin and Overpass as essential building blocks in the future of a decentralized global economy.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51185-references-36" name="p-51185-references-36"></a>References</h3>
<ol>
<li>
<p>Ramsay, B., “Overpass Channels: Horizontally Scalable, Privacy-Enhanced, with Independent Verification, Fluid Liquidity, and Robust Censorship Proof Payments,” Cryptology ePrint Archive, Paper 2024/1526, 2024.</p>
</li>
<li>
<p>Linus, R., Aumayr, L., Zamyatin, A., Pelosi, A., Avarikioti, Z., Maffei, M., “BitVM2: Bridging Bitcoin to Second Layers,” presented by ZeroSync, TU Wien, BOB, University of Pisa, University of Camerino, and Common Prefix, 2024.</p>
</li>
<li>
<p>Nakamoto, S., “Bitcoin: A Peer-to-Peer Electronic Cash System,” <a href="http://Bitcoin.org" rel="noopener nofollow ugc">Bitcoin.org</a>, 2008.</p>
</li>
</ol>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/b-o-digital-cash-properties-for-bitcoin-the-real-satoshi-vision-not-a-cw-deepfake/20987">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 14 Nov 2024 23:54:37 +0000</pubDate>
</item>
<item>
<title>AI-Powered Ethereum: From Rules-Based Smart Contracts to AI-Powered Smart Contracts</title>
<link>https://ethresear.ch/t/ai-powered-ethereum-from-rules-based-smart-contracts-to-ai-powered-smart-contracts/20981</link>
<guid>https://ethresear.ch/t/ai-powered-ethereum-from-rules-based-smart-contracts-to-ai-powered-smart-contracts/20981</guid>
<content:encoded><![CDATA[
<div> 关键词：AI Kernel、智能合约、以太坊、Proof-of-Compute、去中心化应用

总结:
本文提出了一种将大型语言模型（LLM）融入智能合约的新框架——AI Kernel，用于构建能够自主学习和适应环境变化的AI驱动的智能合约。AI Kernel由用户空间、内核空间、模型空间和硬件空间四个主要部分组成，其中用户空间提供与AI Kernel交互的简单接口，内核空间通过一系列核心智能合约管理资源并实现通信；模型空间负责调整开源AI模型以适应区块链环境；硬件空间则利用全球GPU节点执行AI计算任务。通过创新性的Proof-of-Compute共识算法，AI Kernel不仅保证了网络的安全性，而且确保了计算能量的实际效用，为开发者提供了构建具备自适应能力的去中心化应用新途径，如对话式AI代理、AI驱动的钱包、预言机以及DAO等。 <div>
<p><em>Hi! We just released a whitepaper on adding onchain LLM to smart contracts. Any feedback is much appreciated. We have also implemented it already. You can play around with it at <a href="http://eternalai.org" rel="noopener nofollow ugc">eternalai.org</a></em></p>
<h1><a class="anchor" href="https://ethresear.ch#p-51170-ai-powered-ethereum-1" name="p-51170-ai-powered-ethereum-1"></a>AI-Powered Ethereum</h1>
<h2><a class="anchor" href="https://ethresear.ch#p-51170-from-rules-based-smart-contracts-to-ai-powered-smart-contracts-2" name="p-51170-from-rules-based-smart-contracts-to-ai-powered-smart-contracts-2"></a>From Rules-Based Smart Contracts to AI-Powered Smart Contracts</h2>
<p><strong>Abstract.</strong> One of the most fascinating aspects of Ethereum is its ability to create decentralized systems based on a set of smart contracts that can operate without human intervention. However, these smart contracts are still limited by their reliance on pre-programmed rules and logic. By integrating AI, we can begin to create systems that are not only decentralized but also autonomous, adaptive, and self-aware. This raises a range of interesting questions about the combined potential of blockchain technology and the role of AI in decentralized systems. To explore these questions, we propose the development of an AI Kernel that can be used to build AI-powered smart contracts on Ethereum. Eternal AI presents the architecture of the AI Kernel and examines the implications of integrating AI into smart contracts.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51170-h-1-truly-smart-contracts-3" name="p-51170-h-1-truly-smart-contracts-3"></a>1. Truly Smart Contracts</h1>
<p>Let’s take a step back and think about what we’re trying to achieve with smart contracts. We want to create systems that are not only decentralized but also autonomous, able to make decisions and respond to changing circumstances.</p>
<p>But if we look at the current state of dapps, we’re still a long way from achieving that vision. Most dapps today are simply rule-based programs coded in smart contracts with no ability to integrate AI capabilities. They’re like rigid machines, unable to adapt or learn from their environment.</p>
<p>Meanwhile, in the Web2 world, we’re seeing a proliferation of AI-powered applications capable of making decisions in real-time. So, what’s holding us back from bringing this same level of sophistication to decentralized applications?</p>
<p>To address this challenge, we need to rethink the way we approach decentralized software development. We need to create a framework that allows developers to incorporate AI capabilities into their smart contracts, enabling the creation of truly smart contracts that can adapt and evolve over time.</p>
<p>We propose to achieve this by developing an AI Kernel for Ethereum.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51170-h-2-a-new-programming-model-4" name="p-51170-h-2-a-new-programming-model-4"></a>2. A New Programming Model</h1>
<p>Consider a smart contract that manages a decentralized fantasy sports league. The contract needs to determine the winner of a matchup between two teams.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51170-rule-based-approach-5" name="p-51170-rule-based-approach-5"></a>Rule-Based Approach</h2>
<p>With a traditional rule-based approach, the contract might use a complex set of if-else statements to analyze each player’s performance and determine the matchup’s winner.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/d/7d170c152ba23645d0972c413a844be7acfb7204.jpeg" title=""><img alt="" height="256" src="https://ethresear.ch/uploads/default/optimized/3X/7/d/7d170c152ba23645d0972c413a844be7acfb7204_2_624x256.jpeg" width="624" /></a></div><p></p>
<p>Figure 1. Rule-based smart contract.</p>
<p>This approach would be rigid and inflexible and unable to capture the game’s nuances and complexities.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51170-ai-powered-approach-6" name="p-51170-ai-powered-approach-6"></a>AI-Powered Approach</h2>
<p>In contrast, the AI Kernel enables a new programming model that uses large language models (LLMs) to make decisions dynamically in real time. With the AI Kernel, the fantasy sports league contract could be written as providing a prompt to the LLM and receiving a structured response.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/9/294993a536f3c8fa3c46bfa19ace693fdd42efe8.jpeg" title=""><img alt="" height="301" src="https://ethresear.ch/uploads/default/optimized/3X/2/9/294993a536f3c8fa3c46bfa19ace693fdd42efe8_2_624x301.jpeg" width="624" /></a></div><p></p>
<p>Figure 2. AI-powered smart contract.</p>
<p>In this example, the contract prompts the AI Kernel to analyze the two teams’ performance and determine the matchup winner. This approach allows for much more flexibility and dynamic decision-making. It could capture the nuances and complexities of the game in a way that a traditional rule-based approach would not.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51170-h-3-ai-kernel-architecture-7" name="p-51170-h-3-ai-kernel-architecture-7"></a>3. AI Kernel Architecture</h1>
<p>To build truly smart contracts and AI-powered dapps, we need a decentralized framework that can facilitate the integration of AI inference, AI models, and GPU resources. This is where the AI Kernel comes in – the central component of the AI-powered Ethereum.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/e/6eb7da57185b5fcef0a29ac3798d87e42f708857.jpeg" title=""><img alt="" height="430" src="https://ethresear.ch/uploads/default/optimized/3X/6/e/6eb7da57185b5fcef0a29ac3798d87e42f708857_2_478x430.jpeg" width="478" /></a></div><p></p>
<p>Figure 3. The AI Kernel architecture.</p>
<p>At a high level, the AI Kernel can be broken down into four main components. Let’s explore each of these in turn, and think about how they fit together to enable decentralized AI.</p>
<p>First, we have the User space – the domain where dapps operate. In this space, developers can build applications that interact with AI models, but they don’t have direct access to the underlying AI models or compute resources. Instead, they connect to the AI models via the Kernel space.</p>
<p>The Kernel space is where the magic happens. This component provides a simple programming interface for developers to interact with AI models, making it easier to build AI-powered dapps. Under the hood, the kernel space is divided into two sub-components: Decentralized Inference and Core AI Kernel. The Decentralized Inference provides a simple programming interface for developers to interact with AI models. At the same time, the Core AI Kernel takes care of the complex task of executing AI models on decentralized compute resources.</p>
<p>Next, we have the Model space – a domain dedicated to managing AI models. Here, we take existing open-source models like Llama and FLUX, and adjust them to work onchain, enabling decentralized inference. By making these models available onchain, we can create a shared resource that developers can tap into, without having to replicate effort or manage complex model deployments.</p>
<p>Finally, we have the Hardware space – the component that interacts with physical hardware, such as GPU nodes worldwide. This is where the compute resources are provisioned and the AI models are executed. By leveraging decentralized compute resources, we can create a scalable and flexible platform that can handle complex AI workloads.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51170-h-4-user-space-8" name="p-51170-h-4-user-space-8"></a>4. User Space</h1>
<p>Let’s consider the user’s journey to interacting with the AI Kernel. It starts with a prompt — a request for the AI Kernel to generate an output. This prompt can come from a regular user or smart contract accounts. The prompt is sent to the Decentralized Inference smart contract.</p>
<p>The prompt itself is a simple data structure consisting of four fields:</p>
<ul>
<li>The account: either a regular user account or a smart contract account</li>
<li>The topic: one of the many unique contexts between the account and the AI Kernel</li>
<li>The input: a question or a message to elicit an AI-generated output</li>
<li>The extra piece of context (optional)</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/b/1b40db0cb07ca808f6a97707905bd2415f1b5766.png" title=""><img alt="" height="277" src="https://ethresear.ch/uploads/default/optimized/3X/1/b/1b40db0cb07ca808f6a97707905bd2415f1b5766_2_624x277.png" width="624" /></a></div><p></p>
<p>Figure 4. A chain of prompts for a specific account and topic.</p>
<p>The topic is an interesting concept – it’s a unique context that’s shared between the account and the AI Kernel. This context is crucial for the AI Kernel to generate a meaningful output, and it’s something that evolves over time. The Context Manager smart contract is responsible for constructing and updating this context, based on the previous prompts, the input, and any extra context provided.</p>
<p>Once the prompt is submitted, the AI Kernel generates an output, and the Context Manager updates the prompt context with the new output. The prompt data is stored onchain, meaning anyone can verify the output by rerunning the prompt. This transparency is a key feature of the AI Kernel, which sets it apart from traditional AI systems.</p>
<p>Developers have a choice regarding storing the prompt data — they can either store it directly on its native blockchain or store a hash that points to the raw data on an external decentralized storage network like Filecoin. This flexibility is important, as it allows developers to balance the trade-offs between cost, scalability, and security.</p>
<p>Overall, the User Space is designed to provide a simple and intuitive interface for users to interact with the AI Kernel. By abstracting away the complexity of the underlying AI models and compute resources, we can create a seamless experience that allows users to focus on what matters - generating insights and solving problems.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51170-h-5-kernel-space-9" name="p-51170-h-5-kernel-space-9"></a>5. Kernel Space</h1>
<p>The AI Kernel is the heart of our decentralized AI architecture. It is designed to be modular and flexible. At its core, the AI Kernel consists of a set of smart contracts that work together to manage resources and facilitate communication between different parts of the protocol.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/2/92e714e8e34cbdb94d71ef227ee07fecc1247439.jpeg" title=""><img alt="" height="290" src="https://ethresear.ch/uploads/default/optimized/3X/9/2/92e714e8e34cbdb94d71ef227ee07fecc1247439_2_480x290.jpeg" width="480" /></a></div><p></p>
<p>Figure 5. The core smart contracts of the AI Kernel.</p>
<p>Let’s take a look at the five main smart contracts that make up the AI Kernel.</p>
<p>First, the Decentralized Inference contract provides a standardized interface for dapps to interact with the AI Kernel. This contract offers a set of “inference calls” that allow developers to tap into the AI Kernel’s capabilities in a simple and intuitive way.</p>
<p>Next, we have the Prompt Scheduler contract, which is responsible for distributing GPU time and resources among all prompts in a fair and efficient manner. This is a critical component of the AI Kernel, ensuring that all prompts can be handled efficiently and simultaneously. The Prompt Scheduler uses various scheduling algorithms, such as round-robin and fee-based, to manage the flow of prompts and ensure that the system remains responsive and scalable.</p>
<p>The GPU Management contract is another key component of the AI Kernel. This contract manages the staking, status, and configurations of GPU nodes, which are the workhorses of the decentralized AI system. By providing a standardized interface for managing GPU nodes, we can ensure that the system remains flexible and scalable.</p>
<p>The Model File System contract provides access to stored models on various file systems, such as Filecoin and Arweave. This contract abstracts the details of different file systems, providing a consistent model I/O interface to GPU nodes. This allows developers to focus on building their dapps without worrying about the underlying complexities of model storage and retrieval.</p>
<p>Finally, the Context Manager contract organizes and makes various user contexts accessible to GPU nodes. This contract is critical for ensuring the AI Kernel can provide personalized and context-dependent responses to user queries.</p>
<p>These five smart contracts form the core of the AI Kernel, working together to provide a decentralized and scalable AI system that can support a wide range of applications.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51170-h-6-model-space-10" name="p-51170-h-6-model-space-10"></a>6. Model Space</h1>
<p>The Model Space is a critical component of the AI Kernel, where we adapt popular open-source AI models to the blockchain environment. At its core, the Model Space consists of two key components: AI Models and AI Model Drivers.</p>
<p>The AI Models are well-known open-source models like Llama, FLUX, and Hermes. These models have been widely adopted in the AI community and provide a solid foundation for our decentralized AI system.</p>
<p>However, these models were not designed with the blockchain in mind, which is where the AI Model Drivers come in. These drivers play a crucial role in adapting the models to the blockchain environment, ensuring they can operate effectively in a decentralized setting.</p>
<p>One key challenge in adapting AI models to the blockchain is ensuring deterministic. In other words, we must ensure that the models produce the same results given the same input. This is critical for maintaining the integrity of the decentralized AI system, and the AI Model Drivers are designed to handle it.</p>
<p>Another important aspect of adapting AI models to the blockchain is quantization. By reducing the precision of model weights and activations, we can improve performance and reduce storage requirements. This is especially important in a decentralized setting, where storage and computational resources may be limited.</p>
<p>The AI Model Drivers are designed to be modular and extensible, allowing new models to be integrated into the AI Kernel easily. This means that developers can simply plug in new models as they become available without worrying about the underlying complexities of the blockchain environment via a standardized interface.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51170-h-7-hardware-space-11" name="p-51170-h-7-hardware-space-11"></a>7. Hardware Space</h1>
<p>The Hardware Space is where the actual computation happens in the AI Kernel. At its core, this space consists of GPU nodes that serve as the atomic compute unit of the system. These nodes are responsible for taking user prompts, running inference, and returning outputs.</p>
<p>But what makes these GPU nodes tick? The answer lies in the GPU Management smart contract, which plays a critical role in managing the nodes and ensuring that they’re qualified to work. To participate in the system, nodes must stake EAI, which provides a level of accountability and ensures that nodes are invested in the success of the AI Kernel.</p>
<p>In addition to managing nodes, the GPU Management contract also tracks hardware configurations, such as GPU device models. This information is used by the Prompt Scheduler contract to assign prompts to nodes for processing.</p>
<p>But how do nodes get incentivized to participate in the system? That’s where the Proof-of-Compute mechanism comes in. This novel approach to node participation rewards nodes for generating outputs for prompts. The first node to generate an output for a prompt is rewarded with EAI, creating a built-in incentive mechanism that encourages nodes to support the AI Kernel.</p>
<p>Think of it like gold mining. Miners expend resources to add gold to circulation, and in return, they’re rewarded with a piece of that gold. In our case, GPU nodes expend resources (electricity and GPU time) to process prompts, and in return, they’re rewarded with EAI. This mechanism creates a self-sustaining ecosystem where nodes are incentivized to participate and support the AI Kernel.</p>
<p>Eventually, as the system matures, the reward mechanism will transition to prompt fees, making it completely inflation-free. This approach ensures that the AI Kernel’s ecosystem is sustainable and self-sufficient, with nodes incentivized to participate and users paying for value-added services.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51170-h-8-proof-of-compute-12" name="p-51170-h-8-proof-of-compute-12"></a>8. Proof-of-Compute</h1>
<p>Traditional consensus algorithms like Proof-of-Work (PoW) have been criticized for their lack of real-world utility. In contrast, our AI Kernel runs on a novel consensus algorithm called Proof-of-Compute (PoC), which challenges this paradigm by repurposing the computational energy expended in the network.</p>
<p>Instead of solely solving a complex mathematical puzzle, GPU nodes in the PoC network perform meaningful computations on prompts requested by real users and real dapps. This generates valuable outputs for them, creating a self-sustaining ecosystem where nodes are incentivized to participate, and users receive tangible benefits.</p>
<p>So, how does Proof-of-Compute work? The process is straightforward:</p>
<p>First, users or dapps submit a prompt to the Decentralized Inference smart contract. This prompt can be anything from a simple question to a complex AI task.</p>
<p>Next, the Prompt Scheduler smart contract randomly assigns the prompt to a subset of available GPU nodes managed by the GPU Management smart contract. This ensures a decentralized and resilient computation process where no single node has too much control.</p>
<p>Once assigned, the GPU nodes process the prompt and generate outputs, competing to be the first to return a valid result. This competition incentivizes nodes to invest in computational resources and participate honestly in the network.</p>
<p>The first GPU node to return a valid output receives a reward comprising the prompt fee and the block reward. This reward mechanism incentivizes nodes to participate in the network and maintain its integrity.</p>
<p>But what about malicious behavior? To address this, other GPU nodes verify the output for accuracy, detecting and penalizing malicious behavior. This validation and penalty mechanism ensures the integrity of the computation process and maintains trust within the network.</p>
<p>By combining these elements, Proof-of-Compute creates a novel consensus algorithm that not only secures the network but also provides tangible benefits to users. It’s a new blockchain consensus paradigm that prioritizes utility and efficiency over mere security.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/1/71d94a277ec8d80dc8fc554a62b17eebba218205.png" title=""><img alt="" height="436" src="https://ethresear.ch/uploads/default/optimized/3X/7/1/71d94a277ec8d80dc8fc554a62b17eebba218205_2_624x436.png" width="624" /></a></div><p></p>
<p>Figure 6. Proof-of-Compute</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51170-h-9-ai-powered-decentralized-applications-13" name="p-51170-h-9-ai-powered-decentralized-applications-13"></a>9. AI-Powered Decentralized Applications</h1>
<p>As we integrate the AI Kernel into Ethereum, a new paradigm for decentralized applications begins to take shape. No longer limited by rigid, rule-based programming constraints, developers can now create dapps capable of adapting, learning, and evolving over time.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51170-onchain-conversational-ai-agents-14" name="p-51170-onchain-conversational-ai-agents-14"></a>Onchain Conversational AI Agents</h2>
<p>With just a few lines of code, you can build an autonomous agent on top of the AI Kernel and earn passive income by charging a service fee when someone uses your agent.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/3/637ed842df63c9071312905a4b1c344002439a6d.png" title=""><img alt="" height="196" src="https://ethresear.ch/uploads/default/optimized/3X/6/3/637ed842df63c9071312905a4b1c344002439a6d_2_624x196.png" width="624" /></a></div><p></p>
<p>Figure 7. Onchain AI agent.</p>
<p>When someone chats with your agent, simply call the AI Kernel.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/3/d3b780212fa26ca57d8c452931a0b5c7597f4f21.png" title=""><img alt="" height="196" src="https://ethresear.ch/uploads/default/optimized/3X/d/3/d3b780212fa26ca57d8c452931a0b5c7597f4f21_2_624x196.png" width="624" /></a></div><p></p>
<p>Figure 8. Conversational onchain AI agent.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51170-ai-powered-crypto-wallet-15" name="p-51170-ai-powered-crypto-wallet-15"></a>AI-Powered Crypto Wallet</h2>
<p>In this example, we’re building an AI-powered wallet. Before sending the funds to an address, the wallet will call the suspiciousTransaction function.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/b/eb462ae970d3c066fb40b8c20e8946f4e139c6f5.jpeg" title=""><img alt="" height="344" src="https://ethresear.ch/uploads/default/optimized/3X/e/b/eb462ae970d3c066fb40b8c20e8946f4e139c6f5_2_624x344.jpeg" width="624" /></a></div><p></p>
<p>Figure 9. AI-powered crypto wallet.</p>
<p>By providing the AI Kernel with a rich context of transaction history, the model can learn to identify potential red flags such as:</p>
<ul>
<li>Large or unusual transaction amounts</li>
<li>Unusual frequencies within a short period</li>
<li>Transactions that are not consistent with the user’s typical wallet behavior</li>
<li>Transactions to known flagged addresses</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-51170-ai-powered-oracles-16" name="p-51170-ai-powered-oracles-16"></a>AI-Powered Oracles</h2>
<p>In this example, we’re building an AI-powered oracle for BTC prices.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/b/abf968b8b5d9d498e7a1c1871849c14f6bceb4d7.png" title=""><img alt="" height="364" src="https://ethresear.ch/uploads/default/optimized/3X/a/b/abf968b8b5d9d498e7a1c1871849c14f6bceb4d7_2_624x364.png" width="624" /></a></div><p></p>
<p>Figure 10. AI-powered oracles.</p>
<p>By providing the AI Kernel with a rich context of BTC price feeds added continuously by Oracle feeders, the AI Kernel can learn to return the current BTC price by aggregating the fed prices and determining the most accurate value.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51170-ai-powered-daos-17" name="p-51170-ai-powered-daos-17"></a>AI-Powered DAOs</h2>
<p>In this example, we’re building an AI-powered DAO.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/6/e6ac324c400a255838f8ad60293bc3f329eb9b33.jpeg" title=""><img alt="" height="347" src="https://ethresear.ch/uploads/default/optimized/3X/e/6/e6ac324c400a255838f8ad60293bc3f329eb9b33_2_624x347.jpeg" width="624" /></a></div><p></p>
<p>Figure 11. AI-powered DAOs.</p>
<p>By feeding the AI Kernel a continuously updated context of proposal result history, the AI Kernel can develop an understanding of successful and unsuccessful proposals. This enables it to assess and predict the viability of new proposals and make informed decisions on whether to approve or reject a proposal.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51170-ai-powered-wallet-credit-scores-18" name="p-51170-ai-powered-wallet-credit-scores-18"></a>AI-Powered Wallet Credit Scores</h2>
<p>In this example, we’re building an AI-powered Wallet Credit Scoring system.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/c/7c956f1f8bb8ceb00ed8cbd56f9f175c620cbe02.jpeg" title=""><img alt="" height="328" src="https://ethresear.ch/uploads/default/optimized/3X/7/c/7c956f1f8bb8ceb00ed8cbd56f9f175c620cbe02_2_624x328.jpeg" width="624" /></a></div><p></p>
<p>Figure 12. AI-powered credit scoring.</p>
<p>By providing the AI kernel with a comprehensive context of a given address’s transaction history, including details such as transaction amounts, contract interactions (e.g., swaps, lending, borrowing), and other relevant data, the model can learn to accurately assess the address’s creditworthiness and provide a corresponding credit score.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51170-ai-powered-ens-19" name="p-51170-ai-powered-ens-19"></a>AI-Powered ENS</h2>
<p>In this example, we’re building an AI-powered ENS generator.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/2/e2baa9e8e7f688d9aaac20bacb4f678077297ffe.jpeg" title=""><img alt="" height="371" src="https://ethresear.ch/uploads/default/optimized/3X/e/2/e2baa9e8e7f688d9aaac20bacb4f678077297ffe_2_624x371.jpeg" width="624" /></a></div><p></p>
<p>Figure 13. AI-powered ENS.</p>
<p>The model can generate an available ENS domain that best fits the given description. If a suggested domain has already been taken, it will continue to retry until it finds a suitable one.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51170-h-10-conclusion-20" name="p-51170-h-10-conclusion-20"></a>10. Conclusion</h1>
<p>The integration of AI and blockchain technology represents a significant paradigm shift in the way we approach decentralized systems. Our work on the AI Kernel provides a framework for executing AI computations on the blockchain, opening up new possibilities for decentralized applications.</p>
<p>As AI is such an important technology and is growing into every fabric of our lives, the success of decentralized AI will depend on our ability to design systems that are not only technically robust but also socially and philosophically sound. This requires a deep understanding of the complex relationships between technology, society, and the individual.</p>
<p>Ultimately, our work on the AI Kernel is a starting point for a broader conversation about the future of AI-powered decentralized systems and their potential to reshape our world.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/ai-powered-ethereum-from-rules-based-smart-contracts-to-ai-powered-smart-contracts/20981">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 14 Nov 2024 10:36:43 +0000</pubDate>
</item>
<item>
<title>Ethereum Needs to Dream Bigger</title>
<link>https://ethresear.ch/t/ethereum-needs-to-dream-bigger/20963</link>
<guid>https://ethresear.ch/t/ethereum-needs-to-dream-bigger/20963</guid>
<content:encoded><![CDATA[
<div> 关键词：Ethereum、竞争、区块链市场、愿景、技术进步<br /><br />总结: 以太坊自2013年创立以来，其作为全球金融新秩序基石的原始愿景因开发难题和核心开发者政治而受挫。如今，以太坊面临着来自具有不同愿景的区块链竞争对手的压力，并需要接受技术快速发展的现实，五年规划的方式已不再适用。为了重振雄风，首先应明确并阐述以太坊的发展愿景：成为全球计算平台，为解决世界上最复杂的协调问题提供开发者所需的所有工具。接下来的五年目标包括实现1秒区块时间、单槽最终性、大幅提高吞吐量（超过1000TPS）以及实现多个并发提议者。这些改变将有助于提升以太坊的竞争力、降低交易成本、增强实时抗审查能力，并使其更贴近最初的理念。 <div>
<p><strong>Ethereum Needs to Dream Bigger</strong></p>
<p>Ethereum was conceived in 2013 to change the world. Since then, the original vision of a blockchain that would serve as the backbone for a new world order—a permissionless financial substrate with endless possibilities—has been ground to dust by the harsh realities of developing bulletproof production software and navigating core developer politics. In other words, Ethereum is jaded and burnt out. Our most “ambitious” proposals are five-year megaprojects to ship marginal improvements, and progress is slowing incrementally with each hard fork.</p>
<p>Reversing these trends requires accepting two hard truths:</p>
<ol>
<li>
<p><strong>We are no longer the only game in town; Ethereum is now competing in a marketplace of blockchains.</strong> Other teams with different visions are credible threats to Ethereum’s dominance.</p>
</li>
<li>
<p><strong>Making decisions on five-year timescales doesn’t make sense when cryptography, consensus, engineering, and mechanism design are advancing at an extreme pace.</strong> State-of-the-art technology today may not be state-of-the-art in five years.</p>
</li>
</ol>
<p>But enough with the diagnosis; how can we treat this ailment? First, we need to agree on and articulate a clear vision for what we are trying to build. In my opinion, this doesn’t require changing course; it requires doubling down on the core values that we started with in 2013:</p>
<blockquote>
<p><strong>“Ethereum should be a global compute platform that provides developers all the tools they need to solve the world’s hardest coordination problems.”</strong></p>
</blockquote>
<p>In many ways, we have achieved, at least partially, this vision already, but to complete the project we have a long way to go. Smart contracts on Ethereum are Turing complete, but fees on the blockchain are too high for many applications, and block times are long enough that Ethereum isn’t the ideal place to host financial applications. Moreover, Turing completeness means we can write arbitrary smart contract programs that execute logic on their inputs, but if app developers can’t trust the quality of their inputs, then they can’t trust the outputs either.</p>
<p>Ethereum today is credibly neutral and censorship-resistant if given enough time, but when you zoom in to millisecond timescales, it looks a lot more centralized. Within each 12-second block, a single proposer must approve every transaction. This temporary proposer monopoly distorts markets and makes many mechanisms that app developers want to build completely intractable on Ethereum.</p>
<p>With that said, here are the goals for the next five years on Ethereum:</p>
<ul>
<li><strong>1-second block times</strong></li>
<li><strong>Single-slot finality</strong></li>
<li><strong>Vastly increased throughput (&gt;1000 TPS)</strong></li>
<li><strong>Multiple concurrent proposers</strong></li>
</ul>
<p><strong>1-Second Block Times</strong></p>
<p>A round trip from NYC to Tokyo takes less than 200 ms. You can round-trip a message from NYC to Tokyo five times in one second. If anything, 1-second block times are not ambitious enough. Implementing them would be a major unlock for applications on the L1 and would improve the speed to L1 confirmation for L2 transactions.</p>
<p><strong>Single-Slot Finality</strong></p>
<p>Single-slot finality is important for rollup interoperability. In five years, all rollups will be ZK rollups. Bridging will happen in the span of two slots on the L1. But this requires single-slot finality because a ZK rollup cannot allow funds to be withdrawn before finality, even with a valid proof.</p>
<p><strong>Vastly Increased Throughput</strong></p>
<p>Fees on Ethereum L1 are too high. Even if we think rollups are going to be where Ethereum activity takes place, L1 fees will still be a multiplier in those fees. We also need to credibly signal to app developers that Ethereum is a place they can safely build their apps—that we will continue to scale as new technologies become available—so there is no possibility that their app, if it becomes successful, will outgrow the capacity of Ethereum.</p>
<p><strong>Multiple Concurrent Proposers</strong></p>
<p>Of all these proposed changes, Multiple Concurrent Proposers may be the highest lift but also the highest potential reward. While the other items on the list are parameter changes (which will require a lot of work to accomplish, no doubt), MCP unlocks capabilities that no blockchain has had ever before: real-time censorship resistance guarantees. The types of applications that can be built with this technology are purely theoretical today. MCP is the capstone that brings Ethereum’s market microstructure—which today is extremely centralized and extractive—in line with its broader macro vision of credible neutrality and decentralization.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/ethereum-needs-to-dream-bigger/20963">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 12 Nov 2024 22:46:51 +0000</pubDate>
</item>
<item>
<title>Crawling the Ethereum discv5 network - fast</title>
<link>https://ethresear.ch/t/crawling-the-ethereum-discv5-network-fast/20962</link>
<guid>https://ethresear.ch/t/crawling-the-ethereum-discv5-network-fast/20962</guid>
<content:encoded><![CDATA[
<div> 关键词：FastEthereumCrawler、Portal Network、DHT性能优化、FINDNODE请求、网络测量

总结:
本文介绍了名为FastEthereumCrawler的新爬虫，该爬虫用于以太坊、Portal Network等网络，能在不到一分钟的时间里快速抓取大量网络数据。该爬虫是从针对DHT性能优化的工作中衍生出来的，并受到与EF研究人员关于DAS讨论的启发。FastEthereumCrawler基于discv5实现，能够在55秒内从单个引导ENR开始，发现并验证大约50000个ENR和20000个节点。其通过巧妙构造的FINDNODE请求直接遍历网络，同时进行发现和测量，而不是重建单个节点的完整路由表。此外，该爬虫还利用UDP包进行网络测量，包括RTT和带宽估计，并展示了相关的分布图。尽管有约一半的ENR未响应，但研究显示，该爬虫在首次运行中可能已经发现了几乎所有可连接的节点。最后，文章探讨了加快或减缓爬取速度的可能性，并提供了Nim语言编写的源代码供用户尝试。 <div>
<p>In this post we introduce <strong>FastEthereumCrawler</strong>, a new crawler for the <a href="https://github.com/ethereum/devp2p/blob/5713591d0366da78a913a811c7502d9ca91d29a8/discv5/discv5.md" rel="noopener nofollow ugc">Node Discovery v5 (discv5)</a> network used by Ethereum, the Portal Network, and a few others, allowing the crawling of large portions of the network in less than a minute.</p>
<p>The crawler is a side-product of our DHT performance optimization work at <a href="https://codex.storage/" rel="noopener nofollow ugc">Codex.storage Research</a>, inspired also by discussions about DAS with EF researchers. Of course, this is not the first crawler, see other works from <a href="https://migalabs.io/blog/post/presenting-the-ant-crawler-release" rel="noopener nofollow ugc">MigaLabs</a>, from <a href="https://ethresear.ch/t/nebula-a-novel-discv5-dht-crawler/17488">Protocol Labs</a>, and from the Geth team. Our crawler is based on the discv5 implementation in <a href="https://github.com/status-im/nim-eth" rel="noopener nofollow ugc">nim-eth</a>.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51134-cutting-to-the-chase-1" name="p-51134-cutting-to-the-chase-1"></a>Cutting to the chase</h2>
<p>Below is the progress graph of a crawl of the entire network, mapping out approx. <strong>50000 ENRs</strong>, while also verifying connectivity to approx. <strong>20000 nodes</strong>, in <strong>55 seconds</strong>. This is from a fiber connected but otherwise home node. A small NUC, nothing special.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/7/d72f63522e40b4f2c63cd58a1b6699ec62e3c399.png" title=""><img alt="" height="345" src="https://ethresear.ch/uploads/default/optimized/3X/d/7/d72f63522e40b4f2c63cd58a1b6699ec62e3c399_2_690x345.png" width="690" /></a></div><br />
<em>Figure 1: Crawling the Discovery v5 network, progress in time</em><p></p>
<p>The crawl started from knowing a single bootstrap ENR, and took 55 seconds, after which eventually new iterations could start. The figure shows the number of nodes discovered as a function of time, in different categories:</p>
<ul>
<li><strong>discovered</strong>: unique nodes discovered till that point in time. Uniqueness is defined by discv5 nodeID here (although we could also use IP:port pairs, which should lead to similar, although not identical, results).</li>
<li><strong>measured</strong>: nodes that already responded to a direct request message.</li>
<li><strong>pending</strong>: nodes to which we’ve sent a request, but haven’t responded (or timed out) yet.</li>
<li><strong>failed</strong>: nodes to which we’ve sent a request, which already timed-out.</li>
<li><strong>queued</strong>: nodes that are discovered, but we haven’t sent a request yet.</li>
</ul>
<p>Note that by definition</p>
<div class="math">
\text{measured} + \text{pending} + \text{failed} + \text{queued} = \text{discovered}
</div>
<p>Interesting to note also how in the <strong>first 10 seconds</strong> we learn almost <strong>30000 ENRs.</strong> Besides identities and IP:port pairs, these ENRs also contain service related metadata published by the nodes. Zooming in on the first 10 seconds shows an interesting perspective on finding nodes providing services, even rare services, surprisingly fast.</p>
<p>Soon, with <a href="https://ethresear.ch/t/peerdas-a-simpler-das-approach-using-battle-tested-p2p-components/16541">PeerDAS</a>, the metadata in the ENR will include the custody count required for identifying peers having specific columns in custody. If we are able to get many ENRs fast, like we show here, we can also search for custody nodes for specific rows/columns fast, which was a concern for DAS.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/1/d1f480819782d7e31bed244d787fcb07bc93120c.png" title=""><img alt="" height="345" src="https://ethresear.ch/uploads/default/optimized/3X/d/1/d1f480819782d7e31bed244d787fcb07bc93120c_2_690x345.png" width="690" /></a></div><br />
<em>Figure 2: Discovery progress in the first 10 secondds of the crawl</em><p></p>
<p>Not all the discovered nodes are Ethereum nodes, and not all are on Mainnet. Below we give the distribution of discovered nodes belonging to different networks, showing only the nodes that we actually managed to contact, the “measured” nodes in the plots above. For Ethereum consensus nodes, we also show the breakdown based on their published forkDigest.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/1/e19d89d918ab4f8680b7a9c22404570c0a1f1991.png" title=""><img alt="" height="413" src="https://ethresear.ch/uploads/default/optimized/3X/e/1/e19d89d918ab4f8680b7a9c22404570c0a1f1991_2_690x413.png" width="690" /></a></div><br />
<em>Figure 3: Ratio of connectable nodes belonging to different networks.</em><br />
<em>Deneb’s fork digest is 0x6a95a1a9, accounting for 35% of the nodes.</em><p></p>
<p>Note that we discover all these nodes belonging to different networks in one crawl, since they all use a joint Discovery v5 DHT. Nothing forbids other networks to use this DHT. Some examples are the SSV, C, and Opera categories, each indicating nodes distinguished by publishing fields with these names in their ENR, but not belonging to the Ethereum network. “C”, for example, identifies Portal Network nodes. Services could also start their own isolated discv5 network, in which case we would have to bootstrap the crawl from a node belonging to that network.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51134-why-is-our-crawler-so-fast-2" name="p-51134-why-is-our-crawler-so-fast-2"></a>Why is our crawler so fast?</h2>
<p>Ethereum relies on the Node Discovery v5 protocol for bootstrap and peer discovery. This is a Kademlia-style DHT in which most Ethereum nodes participate, providing their Ethereum Node Record (ENR) to their neighbors, while keeping ENRs of all their neighbor nodes in their Kademlia routing table. We use this DHT to crawl the network.</p>
<p>The typical operation on a Kademlia DHT is a so-called ‘Lookup’, which searches in the DHT for nodes close to a target address, iteratively getting closer-and-closer to this address, until reaching the nodes closest to it. Under the hood, the iterative step uses the FINDNODE request-response pair.</p>
<p>Instead of using high-level DHT primitives like lookups, our crawler directly uses <strong>cleverly crafted individual FINDNODE requests</strong> to crawl the network fast.</p>
<p>Our first FINDNODE returns a set of ENRs from the bootstrap node. In most cases, 16 ENRs are returned, containing IP:port pairs for 16 new nodes. We can then start new findNode requests to all these nodes, which will similarly return new ENRs. The overlap between these sets will be statistically negligible at the beginning, hence we have a nice exponential start. Of course later, when we have already discovered a large part of the network, we will get less and less new ENRs, however this is not a problem, since we can’t scale our discovery exponentially anyway to avoid using too much bandwidth. We <strong>rate-limit FINDNODE requests</strong>, while queuing newly discovered nodes for sending requests.</p>
<p>At the end of the process, we’ve sent a <strong>single FINDNODE to every single discovered node</strong>. By this we verify every single node, checking if there is a real reachable node behind the ENR.</p>
<p>By using FINDNODE requests and queuing discovered nodes, we get both crawling progress (discovering new ENRs) and measurements (verifying that there is a node behind the ENR, but also more) at the same time. Contrary to some other crawlers, we do not try to reconstruct the entire routing table of single nodes. We instead settle for getting only 16 ENRs out of the routing table of a single node, and focus on mapping the whole network fast.</p>
<p>When crafting FINDNODE requests, we also take into account how implementations select the 16 ENRs for their response. This is needed for fast crawling since the discv5 specification does not mandate this behavior, leaving it to the implementation. In more detail, we <strong>diversify between the requests sent to the nodes</strong>, by flipping one of the first 16 prefix bits. In discv5, where there is no explicit target address in the FINDNODE request, this is achieved by specifying requested distances between 256 and 240. With this logic, we expect to explore sub-spaces belonging to each combinations of the first 16 bits quite fast, which is very likely to provide us the whole network (nodes from each of the <span class="math"> 2^{16} </span> sub-spaces explored).</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51134-what-else-can-we-measure-3" name="p-51134-what-else-can-we-measure-3"></a>What else can we measure?</h2>
<p>When sending a FINDNODE request to a node, under the hood, several things happen. First, discv5 performs a key exchange. Then, the 16 ENRs are sent back, but an ENR is up to 300 bytes long, so to avoid large UDP packets, discv5 sends the response in multiple smaller UDP packets, sending only 3-4 ENRs per packet.</p>
<p>We can use these UDP packets to perform additional network measurements. Even better, these measurements are free, without requiring additional network traffic.</p>
<p>First, we can <strong>measure Round Trip Time (RTT)</strong>. We have modified the underlying code to measure RTT already during the key exchange, with the added benefit of measuring RTT on first contact independent of the actual Discv5 message sent. Below we show the RTT distribution to all the nodes found.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/a/7a69a5f4e062287e81648488c669d561db7cc0ff.png" title=""><img alt="" height="345" src="https://ethresear.ch/uploads/default/optimized/3X/7/a/7a69a5f4e062287e81648488c669d561db7cc0ff_2_690x345.png" width="690" /></a></div><br />
<em>Figure 4: Rount Trip Time (RTT) distribution, measured from Italy, to all nodes, and to nodes belonging to Ethereum mainnet (Deneb)</em><p></p>
<p>We can also <strong>estimate bandwidth</strong> using the packet-pair technique based on the time-difference of received response packets. We have an initial implementation of this, although it needs a bit more work to return reliable results. Below we show what we get now, we plan to update this soon with better results.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/c/3c507bb0acb128fd4183ab89e85c14ac252d1e04.png" title=""><img alt="" height="345" src="https://ethresear.ch/uploads/default/optimized/3X/3/c/3c507bb0acb128fd4183ab89e85c14ac252d1e04_2_690x345.png" width="690" /></a></div><br />
<em>Figure 5: distribution of estimated bandwidth from other nodes to our node, measured from Italy.</em><p></p>
<h2><a class="anchor" href="https://ethresear.ch#p-51134-how-much-traffic-does-this-use-4" name="p-51134-how-much-traffic-does-this-use-4"></a>How much traffic does this use?</h2>
<p>For each findnode, we send 2 packets (one for key negotiation, one with the actual message), and receive 5-6 packets. Approximately 500 bytes egress, 3600 bytes ingress, on average. With 20000 nodes responding and another 30000 not responding, that’s around 87 MB of traffic. Below we show the traffic trace of the crawler, obtained with tcpdump.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/a/aa575538514c428e1e6e2c3cb5f43a87d3272b5d.png" title=""><img alt="" height="295" src="https://ethresear.ch/uploads/default/optimized/3X/a/a/aa575538514c428e1e6e2c3cb5f43a87d3272b5d_2_690x295.png" width="690" /></a></div><br />
<em>Figure 6: Traffic amount consumed by a full crawl cycle.</em><p></p>
<p>By rate-limiting FINDNODE requests, we control egress bandwidth consumption directly. Since replies are limited in size, we also keep ingress under control with this. For RTT and bandwidth measurements to be meaningful, it is important to keep these limits well below the uplink and the downlink bandwidth of the crawler node, but as shown above, this is hardly an issue, as the top usage is 20 Mbits/sec downlink and 4 Mbits/sec uplink with the rate we’ve used here.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51134-what-about-all-those-failed-nodes-5" name="p-51134-what-about-all-those-failed-nodes-5"></a>What about all those failed nodes?</h2>
<p>Our crawler checked all discovered ENRs by sending a FINDNODE request, but about half of them failed to respond. This could be due to several reasons:</p>
<ul>
<li>Lost UDP packets</li>
<li>Stale ENRs of nodes long gone</li>
<li>Connectivity limitations due to NAT and/or firewall</li>
</ul>
<p>We have added an option to the crowler to have multiple retries, but that did not change much, as it can be seen below. Even if retrying failed nodes 3 times, the “measured” curve showing connectable nodes flatline, showing that we can rule out random UDP losses. Whether these nodes left the network, or just can’t be connected, is something to investigate. The crawler itself can’t distinguish.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/a/ea02c5ca6b26bc204906b01f526ad61faaa02c00.jpeg" title=""><img alt="" height="328" src="https://ethresear.ch/uploads/default/optimized/3X/e/a/ea02c5ca6b26bc204906b01f526ad61faaa02c00_2_624x328.jpeg" width="624" /></a></div><br />
<em>Figure 7: Retrying failed FINDNODE queries 3 times.</em><p></p>
<p>It might also happend that nodes avoided responding if they have blacklisted our node for some reason. Our crawler is basically undetectable, since it sends only a single message to every node. However, during development we’ve run it a many times, so there is a slight chance that some of the nodes did not respond because they’ve blacklisted our IP. We tried to double-check this by running the crawler from a different location, and the results matched, so we don’t think blacklisting was an issue.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51134-do-we-discover-the-whole-network-6" name="p-51134-do-we-discover-the-whole-network-6"></a>Do we discover the whole network?</h2>
<p>One of the most interesting questions is whether we discover the whole network. It is also a question that does not have a single answer.<br />
The answer is: <strong>Yes or No</strong>, depending on how we interpret the questions.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51134-answer-from-the-enr-point-of-view-no-there-are-more-enrs-to-discover-7" name="p-51134-answer-from-the-enr-point-of-view-no-there-are-more-enrs-to-discover-7"></a>Answer from the ENR point of view: No, there are more ENRs to discover</h3>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/8/c8f19fec590556609a2cc78ec1b5970048990800.png" title=""><img alt="" height="345" src="https://ethresear.ch/uploads/default/optimized/3X/c/8/c8f19fec590556609a2cc78ec1b5970048990800_2_690x345.png" width="690" /></a></div><br />
<em>Figure 8: Running 50 crawl cycles while accumulating results.</em><p></p>
<p>If we run more iterations, we can discover much more ENRs, as the red line in the figure below shows. After 50 cycles, we discovered almost 230000 ENRs belonging to unique nodeIDs! More specifically, we see 228852 unique nodeIDs, but only 145512 unique IP:port combinations, which is already a bit suspicious.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51134-answer-from-the-nodes-point-of-view-yes-all-connectable-nodes-were-discovered-in-55-seconds-8" name="p-51134-answer-from-the-nodes-point-of-view-yes-all-connectable-nodes-were-discovered-in-55-seconds-8"></a>Answer from the Nodes point of view: Yes, all connectable nodes were discovered in 55 seconds</h3>
<p>As the figure above also shows, the “measured” line remains constant after the first cycle. We did not manage to connect to almost any of these ENRs discovered later. We are still investigating whether this is an error in our code, or the reality, but currently it seems our crawler manages to find almost all connectable nodes in the first cycle, in less than 55 seconds.</p>
<p>We could still discover more nodes in a few ways:</p>
<ul>
<li>Staying longer in the network, we could wait for nodes behind NAT to connect to us</li>
<li>We could also try to connect to nodes using other protocols, e.g. libP2P, hoping that that would succeed.</li>
</ul>
<p>Our crawler is not doing these at the moment, but these are obvious potential extensions.</p>
<p>The relative ratios of discovered ENRs is slightly different, but similar to those of measured nodes.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/0/401af386e00226a2bfc4e932a646998e701fd996.png" title=""><img alt="" height="413" src="https://ethresear.ch/uploads/default/optimized/3X/4/0/401af386e00226a2bfc4e932a646998e701fd996_2_690x413.png" width="690" /></a></div><br />
<em>Figure 9: Ratio of ENRs belonging to different networks.</em><br />
<em>Deneb’s fork digest is 0x6a95a1a9, accounting for 35% of all ENRs.</em><p></p>
<h2><a class="anchor" href="https://ethresear.ch#p-51134-can-we-do-the-crawl-even-faster-9" name="p-51134-can-we-do-the-crawl-even-faster-9"></a>Can we do the crawl even faster?</h2>
<p>There are still a few ways to crawl faster, which we haven’t explored yet. A few that comes to mind:</p>
<ul>
<li>The trivial: start from more Bootstrap nodes, or actually from any list of nodes belonging to the network.</li>
<li>The obvious: allow more requests per second. The fastest we’ve tried is 1000 FINDNODE requests/second, which is far from maxing out our uplink or downlink. However, it is already maxing out a single CPU core. We’ve yet to streamline the code to use less CPU or use multiple cores.</li>
<li>The UDP level API: As we’ve said, responses to FINDNODE are coming in multiple UDP packets. Yet, we are using the “findnode” API, which waits for all the response UDP packets, aggregates all 16 ENRs into one response, and only then handles it back to the caller. We could clearly be more reactive here.</li>
<li>The ID space conscious: our crawler explores the node ID space randomly, without looking into the distribution of what IDs we find, and where are the “holes” in the ID space where we could probably find more nodes. If we would store discovered nodeIDs in a “B+ tree”, we could explore the ID space in a structured way, probably leading to better crawl performance.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-51134-can-we-do-the-crawl-slower-10" name="p-51134-can-we-do-the-crawl-slower-10"></a>Can we do the crawl slower?</h2>
<p>Of course, crawling could also be done slower. In fact, most DHT implementations perform random lookups every now-and-then to explore the network. Slow, or repeated crawls can also be useful to give historical perspective on the lifetime of records and nodes, on node-churn, etc.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51134-finally-can-i-try-the-crawler-11" name="p-51134-finally-can-i-try-the-crawler-11"></a>Finally, can I try the crawler?</h2>
<p>Sure, <a href="https://github.com/cskiraly/fast-ethereum-crawler" rel="noopener nofollow ugc">here’s the code on GitHub</a>. Although written in the Nim language, it is self-contained, building also the compiler itself. Just git clone, and make. Enjoy! Even during Devcon SEA <img alt=":slight_smile:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/slight_smile.png?v=12" title=":slight_smile:" width="20" /></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/9/9941dee4335839c6866ba9d5a28c1e575597b6b6.png" title=""><img alt="" height="345" src="https://ethresear.ch/uploads/default/optimized/3X/9/9/9941dee4335839c6866ba9d5a28c1e575597b6b6_2_690x345.png" width="690" /></a></div><br />
<em>Figure 10: RTT distribution from Devcon SEA</em><p></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/crawling-the-ethereum-discv5-network-fast/20962">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 12 Nov 2024 19:06:21 +0000</pubDate>
</item>
<item>
<title>ZKMPC: Publicly Auditable MPC for general-purpose computations</title>
<link>https://ethresear.ch/t/zkmpc-publicly-auditable-mpc-for-general-purpose-computations/20956</link>
<guid>https://ethresear.ch/t/zkmpc-publicly-auditable-mpc-for-general-purpose-computations/20956</guid>
<content:encoded><![CDATA[
<div> 关键词: 多方计算(MPC), 公开可验证性, SPDZ协议, 位操作, 狼人杀游戏实现

总结:
本文介绍了一个基于SPDZ协议的公开可验证多方计算扩展版本——ZKMPC。该协议支持第三方对输入约束和计算结果进行验证，不仅限于算术运算，还新增了位操作、比较及条件分支等能力，拓宽了应用场景。项目采用Rust语言实现，针对 Arkworks 库进行了优化，增加了对位分解和相关位操作的支持，例如 AND、OR、XOR、NOT 和 EQZ 等。通过引入零知识证明（ZKP），实现了对MPC计算结果的第三方公开验证，同时确保输入值满足预设条件，如区块链应用中的承诺一致性检查。文章以无Game Master的狼人杀游戏为例展示了ZKMPC的应用潜力。未来需关注计算成本等方面的挑战。 <div>
<h2><a class="anchor" href="https://ethresear.ch#p-51120-tldr-1" name="p-51120-tldr-1"></a>TL;DR</h2>
<ul>
<li>We extended the MPC protocol SPDZ to create a publicly auditable version that enables third-party verification of input constraints and calculation results.</li>
<li>SPDZ is well-suited for arithmetic operations but less effective for bitwise operations. We now support both types of computations, along with comparisons and conditional branching, broadening the range of possible applications.</li>
<li>As an example, we implemented a Game Master (GM)-free werewolf game using MPC.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-51120-h-1-introduction-2" name="p-51120-h-1-introduction-2"></a>1. Introduction</h2>
<p>Our project enables third-party verification of the correctness of MPC computation results.</p>
<p>Multi-Party Computation (MPC) is a protocol that allows multiple parties to perform computations on their secret inputs without revealing those inputs to others. For instance, the SPDZ protocol [1] facilitates secure computations even in the presence of malicious actors.</p>
<p>“Publicly auditable MPC” is a concept that refers to an MPC system where the results of computations can be verified by a third party. We have developed “<strong>ZKMPC</strong>,” an extension of SPDZ-based publicly auditable MPC (PA-MPC), tailored for scenarios where specific constraints are placed on the input values.</p>
<p>Some of the computations and circuits we aim to support with MPC extend beyond simple arithmetic—they include binary-oriented operations like comparisons, bit decomposition, and more. Adding support for these operations significantly broadens the range of calculations that MPC can handle. For example, efficient calculations involving Pedersen commitments require a bit-decomposition step to break down the exponential component into separate powers for further combination.</p>
<p>The objective of this project is to create a general-purpose PA-MPC with potential applications in blockchain technology.</p>
<p>The code for this project can be found here.</p><aside class="onebox githubrepo">
  <header class="source">

      <a href="https://github.com/Yoii-Inc/zk-mpc" rel="noopener nofollow ugc" target="_blank">github.com</a>
  </header>

  <article class="onebox-body">
    <div class="github-row">
  <img class="thumbnail" height="344" src="https://ethresear.ch/uploads/default/optimized/3X/c/a/ca3e159831f2f4fb6dd534391b2a2691a020001f_2_690x344.png" width="690" />

  <h3><a href="https://github.com/Yoii-Inc/zk-mpc" rel="noopener nofollow ugc" target="_blank">GitHub - Yoii-Inc/zk-mpc</a></h3>

    <p><span class="github-repo-description">Contribute to Yoii-Inc/zk-mpc development by creating an account on GitHub.</span></p>
</div>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both;"></div>
</aside>

<p><em>This project has received funding from the PSE Grants Program and is a continuation of the previous topic.</em></p><aside class="quote quote-modified">
  <div class="title">
    <div class="quote-controls"></div>
    <img alt="" class="avatar" height="24" src="https://ethresear.ch/user_avatar/ethresear.ch/sheagrief/48/15040_2.png" width="24" />
    <a href="https://ethresear.ch/t/zkmpc-publicly-verifiable-spdz-with-constraints-on-secret-inputs/18661">ZKMPC: Publicly Verifiable SPDZ with Constraints on Secret Inputs</a> <a class="badge-category__wrapper " href="https://ethresear.ch/c/cryptography/mpc/14"><span class="badge-category --has-parent"><span class="badge-category__name">Multiparty Computation</span></span></a>
  </div>
  <blockquote>
    <a class="anchor" href="https://ethresear.ch#h-1-project-overview-1" name="h-1-project-overview-1"></a>1. Project Overview
<a class="anchor" href="https://ethresear.ch#h-11-overview-2" name="h-11-overview-2"></a>1.1 Overview
Multi-Party Computation (MPC) allows protocol participants to collaborate without exposing their own private input values to other participants. 
This project extends one of the MPC protocols, SPDZ [1], and adding following functionalities: 

Proof that the input values satisfy the conditions when there are restrictions on the input values of the protocol participants
Third-party verification that the MPC calculation results are correct

The SPDZ is an extension …
  </blockquote>
</aside>

<h2><a class="anchor" href="https://ethresear.ch#p-51120-h-2-background-3" name="p-51120-h-2-background-3"></a>2. Background</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51120-h-21-what-is-mpc-4" name="p-51120-h-21-what-is-mpc-4"></a>2.1 What is MPC?</h3>
<p>Multi-Party Computation (MPC) allows multiple parties to compute a function over their private inputs without revealing those inputs to each other. It ensures privacy while enabling collaborative computations.</p>
<p>For example, the SPDZ protocol is an MPC protocol that ensures security even against malicious adversaries. It leverages techniques such as secret sharing and homomorphic encryption to enable secure collective computations.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51120-h-22-the-role-of-zero-knowledge-proofs-and-public-auditability-in-mpc-5" name="p-51120-h-22-the-role-of-zero-knowledge-proofs-and-public-auditability-in-mpc-5"></a>2.2 The Role of Zero-Knowledge Proofs and Public Auditability in MPC</h3>
<p>Public auditability is essential in many MPC applications to ensure trust in computed results, especially when stakeholders are not directly involved in the computation. Traditional MPC protocols only allow participants to verify correctness, but in cases like regulatory compliance or decentralized systems, third-party verification is crucial for transparency.</p>
<p>Zero-Knowledge Proofs (ZKPs) enable this public verifiability by allowing a prover to convince a verifier that a computation was done correctly without revealing any underlying data.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51120-h-3-project-details-6" name="p-51120-h-3-project-details-6"></a>3. Project Details</h2>
<p>We extended the SPDZ protocol, which is based on additive secret sharing, to integrate zero-knowledge proofs for verifiable computations.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51120-h-31-project-architecture-7" name="p-51120-h-31-project-architecture-7"></a>3.1 Project Architecture</h3>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/2X/7/7210b86361b9452a368a1346c0591112b5e060c5.png" title="project_architecture"><img alt="project_architecture" height="500" src="https://ethresear.ch/uploads/default/optimized/2X/7/7210b86361b9452a368a1346c0591112b5e060c5_2_601x500.png" width="601" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#p-51120-h-32-technical-specifications-8" name="p-51120-h-32-technical-specifications-8"></a>3.2 Technical specifications</h3>
<p>We extend the MPC protocol called SPDZ, which is based on additive secret sharing, due to its affinity with zero-knowledge proofs, among other factors.</p>
<p>MPC libraries are predominantly implemented in C++, but given the increasing adoption of Rust for ZKP and blockchain projects, we chose Rust for this implementation. Rust’s safety features and growing developer community make it a strong fit for our needs.</p>
<p><strong>Extension of SPDZ</strong></p>
<p>We’ve implemented the above features as an extension of SPDZ.</p>
<ul>
<li>We could not find any prior research on Bitwise MPC that is publicly auditable.
<ul>
<li>QuickSilver (using zk but not PA)</li>
<li>MPC that does not use zk
<ul>
<li>Yao’s Garbled Circuit (2PC)</li>
<li>BMR (MPC extension of Yao’s Garbled Circuit)</li>
<li>BGW</li>
<li>etc…</li>
</ul>
</li>
</ul>
</li>
<li>Good extensions from previous studies of collaborative proof.</li>
<li>Even with these updates, SPDZ retains its malicious security. In fact, the following features are achieved through a combination of MPC-secure additions and multiplications.</li>
</ul>
<p>We referred to [3,4]. [3] proposes a protocol for primitive computation in MPC without bit-decomposition as much as possible. And [4] implies the possibility of a slightly more efficient implementation.</p>
<p><strong>Modifications to arkworks library</strong></p>
<p>For example, consider constraints on bit decomposition.</p>
<ul>
<li>In the part that generates the R1CS constraint, the following is done:
<ul>
<li>Bit-decomposition operation</li>
<li>Generate constraints to ensure that the bit-decomposed value is equal to the original value</li>
</ul>
</li>
<li>Contents to be implemented:
<ul>
<li>The bit-decomposed value was originally stored in a boolean type, so the library is modified so that it becomes a finite share.</li>
</ul>
</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51120-h-33-third-party-verification-of-calculation-and-output-9" name="p-51120-h-33-third-party-verification-of-calculation-and-output-9"></a>3.3 <strong>Third-party verification of calculation and output</strong></h3>
<p>General MPC does not allow third-party verification of output values. A previous study that made this possible using zero-knowledge proofs is called Collaborative zk-SNARKs [2].</p>
<p>The output results should also be verifiable by a third party.</p>
<p>By adding ZKP to the MPC, the output of MPC can be publicly verified by a third party, enabling more secure and correct multi-party computation while keeping the input information confidential.</p>
<p><strong>Verification of secret inputs</strong><br />
In many practical applications, it cannot be guaranteed that protocol participants will use appropriate input values, and there may be conditions on the input values, such as being within a specific range. In blockchain applications, it is often necessary to ensure consistency between a “pre-committed value on the chain” and the “input in MPC calculation.” To address this, ZKMPC verifies that commitments published on the blockchain match the commitments of the inputs used during MPC. This verification is facilitated by adding ZKPs to the input-sharing phase.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51120-h-34-supporting-bitwise-operations-10" name="p-51120-h-34-supporting-bitwise-operations-10"></a>3.4 Supporting Bitwise Operations</h3>
<p>Previously, ZKMPC did not support operations involving zero-knowledge circuits with bitwise operations, nor did it handle multi-party computation (MPC) for bit decomposition and comparison. Our ZKMPC now incorporates support for these critical calculations.</p>
<p>Specifically, we’ve implemented the following:</p>
<ul>
<li>Fixed arkworks libraries with respect to R1CS</li>
<li>Fundamental bit operations (AND, OR, XOR, NOT, etc.)</li>
<li>EQZ (equality zero test)</li>
<li>Bit decomposition</li>
<li>Comparison</li>
<li>Conditional branching (IF) functions</li>
</ul>
<p>We implemented these functions based on [Nishide, Ohta, 2007]. This research pointed out that bit decomposition itself is a rather heavy computation, and an efficient method is proposed to perform functions such as EQZ and comparison without directly using bit decomposition.</p>
<ul>
<li>
<p>Equality zero (computation of <span class="math">[x==0]</span>)</p>
<ul>
<li>Overview:
<ul>
<li>Generate <span class="math">[r]</span> randomly.</li>
<li>Reveal <span class="math">[x+r]</span>.</li>
<li>Create a share such that if all bit decompositions of <span class="math">[r]</span> and <span class="math">[x+r]</span> match, the share is <span class="math">1</span>; if none match, the share is <span class="math">0</span>.</li>
</ul>
</li>
<li>Details: <div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/3/e3fd25ba87c6f6d811f065e80627619aa73defa1.jpeg" title="equality_zero_test_algorithm"><img alt="equality_zero_test_algorithm" height="265" src="https://ethresear.ch/uploads/default/optimized/3X/e/3/e3fd25ba87c6f6d811f065e80627619aa73defa1_2_690x265.jpeg" width="690" /></a></div></li>
</ul>
</li>
<li>
<p>Bit Decomposition (<span class="math">[a]_p</span> to <span class="math">[a]_B</span>)</p>
<ul>
<li>Overview:
<ul>
<li><span class="math">a</span> is an integer from <span class="math">0</span> to <span class="math">p-1</span></li>
<li>Generate <span class="math">[r]</span> randomly.</li>
<li>Reveal <span class="math">[a-r]</span>.</li>
<li>Calculate the bit decomposition of <span class="math">[a-r]</span>, paying special attention to carry-over.</li>
<li>Let <span class="math">[a]_B=[r]_B+[a-r]_B</span></li>
</ul>
</li>
<li>Details:
<ul>
<li>Here, <span class="math">p</span> represents the modulus of the field, and <span class="math">l</span> is the bit length of <span class="math">p</span>.</li>
<li><img alt="simplified bit-decomposition" height="406" src="https://ethresear.ch/uploads/default/original/3X/9/8/987add4ec752cfacd2bf4cd9ce68557ddc81983b.png" width="640" /></li>
</ul>
</li>
</ul>
</li>
<li>
<p>Comparison (<span class="math">[a&lt;b]</span> calculation)</p>
<ul>
<li>Overview:
<ul>
<li>Compute <span class="math">[a&lt;p/2], [b&lt;p/2], [a-b &lt; p/2]</span> by interval test, where <span class="math">p</span> is the modulus.</li>
<li>An interval test is a function that securely determines if a shared value is less than a constant, and outputs a shared true or false value.</li>
<li>Since the interval test involves comparing a shared value to a constant, it is more efficient than comparing two shared values.</li>
</ul>
</li>
<li>Details: <div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/6/86c53fc7dc95faa664b6de75fd07757d59d00a6b.jpeg" title="upload_6fca0a521763f0c1f3ae4b9258d25089"><img alt="upload_6fca0a521763f0c1f3ae4b9258d25089" height="212" src="https://ethresear.ch/uploads/default/optimized/3X/8/6/86c53fc7dc95faa664b6de75fd07757d59d00a6b_2_690x212.jpeg" width="690" /></a></div></li>
</ul>
</li>
<li>
<p>Conditional Branching (IF) Function</p>
<ul>
<li><span class="math">\mathrm{IF}([a], [b], [c])</span>
<ul>
<li>Returns <span class="math">[b]</span> when <span class="math">[a]</span> is non-zero, returns <span class="math">[c]</span> when <span class="math">[a]</span> is zero</li>
<li><span class="math">\mathrm{IF}([a], [b], [c])=[b]+([c]-[b])*[a==0]</span></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51120-h-35-others-11" name="p-51120-h-35-others-11"></a>3.5 Others</h3>
<p><strong>Preprocessing in MPC</strong></p>
<p>In the context of ZKMPC, we extended the preprocessing phase by adding constraints to ensure guarantees on secret inputs. This preprocessing includes the generation of zero-knowledge proofs alongside traditional SPDZ preprocessing elements. Specifically, the preprocessing includes:</p>
<ul>
<li><strong>Multiplication Triples</strong>: Pre-computed shares that facilitate efficient secure multiplications during the online phase.</li>
<li><strong>Correlated Randomness</strong>: Used to ensure that operations such as addition, multiplication, and comparison can be executed without additional interaction.</li>
<li><strong>Zero-Knowledge Proof Setup</strong>: Preparing the cryptographic material needed to provide zero-knowledge proofs for correctness and constraints on secret inputs.</li>
<li><strong>Commitment Scheme Preparation</strong>: Preprocessing also involves setting up Pedersen commitments to ensure that inputs remain consistent between the preprocessing and online phases.</li>
</ul>
<p>By adding these preprocessing constraints, we provide stronger guarantees regarding secret inputs, enhancing both security and efficiency during the online phase.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51120-h-36-cost-analysis-12" name="p-51120-h-36-cost-analysis-12"></a>3.6 Cost Analysis</h3>
<p>The cost analysis is based on the paper, [3] (Table 1), where <span class="math">l</span> is the bit length of a modulus of the finite field.</p>
<p>“Round” refers to the number of times communication takes place (i.e., the number of times multiplication is invoked), and “Comm” is the communication complexity (i.e., the amount of data exchanged during communication).</p>
<p>We adopt the proposed implementation shown in Table 1, which achieves fewer rounds of MPC and less communication complexity than a primitive implementation using bit decomposition (BD-based).</p>
<p><img alt="cost analysis" height="435" src="https://ethresear.ch/uploads/default/original/3X/b/5/b595e44a266191a64fae0830615cf651a83bf7be.png" width="660" /></p>
<h2><a class="anchor" href="https://ethresear.ch#p-51120-h-4-example-implementation-of-game-master-free-werewolf-game-components-13" name="p-51120-h-4-example-implementation-of-game-master-free-werewolf-game-components-13"></a>4. Example: Implementation of Game Master-Free Werewolf Game Components</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-51120-h-41-overview-14" name="p-51120-h-41-overview-14"></a>4.1 Overview</h3>
<p>We will demonstrate the application of ZKMPC by implementing a version of the Werewolf game that operates without a Game Master.</p>
<p>The components are as follows:</p>
<ul>
<li>Generating the Fortune Teller’s Public Key</li>
<li>Divination (Fortune-telling)</li>
<li>Anonymous Voting</li>
<li>Winning Judgment</li>
<li>Role Assignment</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51120-h-42-why-the-werewolf-game-15" name="p-51120-h-42-why-the-werewolf-game-15"></a>4.2 Why the Werewolf Game?</h3>
<p>The Werewolf game is a particularly well-suited example for several reasons:</p>
<ul>
<li>It incorporates several operations, including bitwise operations, that require confidential inputs from multiple participants. Removing the need for a Game Master (GM) demonstrates the strength of MPC in handling complex, decentralized interactions without a trusted authority. These operations have potential real-world applications beyond gaming.</li>
<li>The game involves a moderate level of complexity.
<ul>
<li>Only certain participants know the results of specific calculations, adding an element of secrecy.</li>
<li>The roles of the participants vary, contributing to dynamic gameplay.</li>
</ul>
</li>
<li>This implementation is innovative as there is no existing precedent for a Game Master-free version of the game.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51120-h-43-technical-details-16" name="p-51120-h-43-technical-details-16"></a>4.3 Technical Details</h3>
<p>In this section, we explain how they can be expressed as mathematical formulas. We can make ZKP circuits based on these expressions.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-51120-circuit-1-generating-the-fortune-tellers-public-key-17" name="p-51120-circuit-1-generating-the-fortune-tellers-public-key-17"></a>Circuit 1: Generating the Fortune Teller’s Public Key</h4>
<p>For each participant who is a fortune teller, a public key and a private key associated with that person are needed. Therefore, the key pair is first generated, and then MPC is performed by the participants to disclose the public key without revealing who the public key belongs to.</p>
<ol>
<li>A fortune teller generates a secret key <span class="math">sk</span> and corresponding public key <span class="math">pk</span> locally.</li>
<li>Each participant calculates the following by MPC, with <span class="math">F_i</span> indicating whether one is a fortune teller, and <span class="math">pk_i</span> being an input (fortune teller: public key generated in step 1, others: any value):</li>
</ol>
<p>The constraint is:</p>
<div class="math">
\sum_{i=1}^n F_i pk_i
</div>
<ul>
<li>The condition for <span class="math">F_i</span> is that it should be consistent with the commitment of the position.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-51120-circuit-2-divination-fortune-telling-18" name="p-51120-circuit-2-divination-fortune-telling-18"></a>Circuit 2: Divination (Fortune-telling)</h4>
<p>Using whether player <span class="math">i</span> is a Werewolf (<span class="math">W_i \in {0,1}</span>) and whether the fortune teller wants to know the result for player <span class="math">i</span> (<span class="math">C_i \in {0,1}</span>, all but one are 0), whether player <span class="math">i</span> is a werewolf (=1) or not (=0) is calculated by</p>
<div class="math">
\sum_{i=1}^{n} W_i C_i
</div>
<p><span class="math">W_i</span> and <span class="math">C_i</span> are distributed to each player as shares, and the final result is encrypted so that only the fortune teller can see it.</p>
<p>At this time:</p>
<ul>
<li>The condition regarding <span class="math">C_i</span> is that <span class="math">C_i</span> should be 1 for only one value of <span class="math">i</span>, and 0 for all other values.</li>
<li>The condition for <span class="math">W_i</span> is that it should be consistent with the commitment of the position.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-51120-circuit-3-anonymous-voting-determination-of-who-gets-the-most-votes-19" name="p-51120-circuit-3-anonymous-voting-determination-of-who-gets-the-most-votes-19"></a>Circuit 3: Anonymous Voting (Determination of Who Gets the Most Votes)</h4>
<ul>
<li>Naive idea:
<ul>
<li>It can be expressed as “the number of votes cast for a player is greater than or equal to the number of votes cast for all other players.”</li>
</ul>
</li>
<li>The formula for determining the maximum voter is as follows:<div class="math">
b_i=\prod_{j=1}^n \mathrm{GTE}(a_i, a_j)
</div>
<ul>
<li><span class="math">b_i</span>: 1 when player <span class="math">i</span> has the most votes, 0 otherwise</li>
<li><span class="math">a_i</span>: Number of votes cast for player <span class="math">i</span>.</li>
<li><span class="math">\mathrm{GTE}(x,y)</span>: (Abbreviation for Greater Than or Equal). 1 when <span class="math">x \geq y</span>, 0 otherwise.</li>
</ul>
</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-51120-circuit-4-winning-judgment-20" name="p-51120-circuit-4-winning-judgment-20"></a>Circuit 4: Winning Judgment</h4>
<ul>
<li>Naive idea:
<ul>
<li>It can be expressed as “if the number of werewolves <span class="math">\geq</span> the number of villagers, the wolves win”, “if the number of werewolves = 0, the villagers win”, or “if <span class="math">1 \leq</span> the number of werewolves &lt; the number of villagers, the game continues”.</li>
</ul>
</li>
<li>The formula for determining the victory of a game is, for example, as follows:<div class="math">
\begin{aligned}
    f =&amp;IF(\\
    &amp;&amp;&amp;N_w,\\
    &amp;&amp;&amp;IF(LT(N_w, N_v),3,1) ,\\
    &amp;&amp;&amp;2\\
    &amp;)
    \end{aligned}
</div>
<ul>
<li><span class="math">f</span>: game state, 1 when the wolves win, 2 when the villagers win, 3 when the game continues</li>
<li><span class="math">N_w</span>: Number of living werewolves</li>
<li><span class="math">N_v</span>: Number of living villagers</li>
<li><span class="math">\mathrm{EQZ}(x)</span>: (Abbreviation for Equal Zero). 1 when <span class="math">x=0</span>, 0 otherwise.</li>
<li><span class="math">\mathrm{LT}(x,y)</span>: (Abbreviation for Less Than). 1 when <span class="math">x &lt; y</span>, 0 otherwise.</li>
</ul>
</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-51120-circuit-5-role-assignment-21" name="p-51120-circuit-5-role-assignment-21"></a>Circuit 5: Role Assignment</h4>
<ul>
<li>Concept:
<ul>
<li>Each participant makes a random substitution, and the grouping protocol is implemented by MPC.</li>
</ul>
</li>
<li>Requirement:
<ul>
<li>We would like to assign a position randomly and disclose it only to the player themselves.</li>
<li>For a specific position, such as werewolf, we want to know which members belong to the group.</li>
<li>We want to create a commitment for each member’s position and make it publicly auditable.</li>
</ul>
</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/3/e30c581add23f8bc20bc619cb1281c579118c7e6.png" title="the_role_assignment_protocol_diagram"><img alt="the_role_assignment_protocol_diagram" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/e/3/e30c581add23f8bc20bc619cb1281c579118c7e6_2_666x500.png" width="666" /></a></div><p></p>
<ul>
<li><strong>For grouping protocols</strong>:
<ul>
<li>ref. [5]</li>
<li>ZKMPC creates the following proof:
<ul>
<li>Public parameter
<ul>
<li><span class="math">n</span>: number of participants</li>
<li><span class="math">m</span>: number of groups</li>
<li><span class="math">\tau</span>: substitution matrix of a particular type of order </li>
</ul>
</li>
<li>Witness
<ul>
<li><span class="math">M_i (i\in \{1,\dots, n\})</span>: shuffle matrix of player <span class="math">i</span></li>
<li><span class="math">r_i</span>: randomness of player <span class="math">i</span></li>
</ul>
</li>
<li>Instance
<ul>
<li>Commitment <span class="math">c_i</span></li>
</ul>
</li>
<li>Constraint
<ul>
<li><span class="math">M_i</span> for each <span class="math">i</span> is a substitution matrix</li>
<li>For each <span class="math">M_i</span>, the submatrix consisting of rows <span class="math">n+1</span> to <span class="math">n+m</span> and columns <span class="math">n+1</span> to <span class="math">n+m</span> forms an identity matrix.</li>
<li><span class="math">\rho = M^{-1}\tau M</span> where <span class="math">M=\prod_{i=1}^n M_i</span></li>
<li>For each <span class="math">j</span>, <span class="math">\rho^{i}(j) = x_{i,j}</span></li>
<li>Where <span class="math">x_i</span> is the group number contained in <span class="math">x_{i,j}</span></li>
<li>For each <span class="math">i</span>, <span class="math">c_i = Commitment(x_i, r_i)</span></li>
</ul>
</li>
<li>Third-party verification is done by including the commitment <span class="math">c_i = Commitment(x_i, r_i)</span> in the proof</li>
</ul>
</li>
<li>Compute the following in MPC:
<ul>
<li>Input:
<ul>
<li><span class="math">[M_i]</span>: Shuffle matrix of player <span class="math">i</span>.</li>
<li><span class="math">(1, \dots, n)</span>: Constant number vec from 1 to <span class="math">n</span>.</li>
</ul>
</li>
<li>Output:
<ul>
<li><span class="math">[x_i] = ([x_{i,1}], \dots, [x_{i,n}])</span>: a shuffled shared vector representing each user’s group membership, including both the group that the <span class="math">i</span>-th user belongs to and their fellow members within that group.</li>
</ul>
</li>
</ul>
</li>
<li>Reveal <span class="math">[x_i]</span> to player <span class="math">i</span>.
<ul>
<li>Player <span class="math">i</span> now knows their role.</li>
</ul>
</li>
<li>Player <span class="math">i</span> commits <span class="math">x_i</span>, where <span class="math">r_i</span> is randomness
<ul>
<li><span class="math">c_i = Commitment(x_i, r_i)</span></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-51120-h-5-benchmark-22" name="p-51120-h-5-benchmark-22"></a>5. Benchmark</h2>
<p>256-bit field is used in the ZKMPC benchmark below.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51120-h-51-online-phase-23" name="p-51120-h-51-online-phase-23"></a>5.1 Online Phase</h3>
<p>run_online.zsh (Marlin): 7s</p>
<p>Since the proving and verifying times of circuits depend on the number of constraints, and the dependency is roughly the same as in the previous study, Collaborative zk-SNARKs, only the number of constraints will be recorded here.</p>
<p>For example, when the constraints <span class="math">\approx 2^{10}</span> and executing in Marlin, the proving time <span class="math">\approx 2</span> s as follows.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/2X/4/4fa337877d5de3a131f318d6a2a2d9f9f981d49e.png" title="relation_of_constraints_and_proving_time"><img alt="relation_of_constraints_and_proving_time" height="358" src="https://ethresear.ch/uploads/default/optimized/2X/4/4fa337877d5de3a131f318d6a2a2d9f9f981d49e_2_690x358.png" width="690" /></a></div><br />
cited from [2].<p></p>
<h3><a class="anchor" href="https://ethresear.ch#p-51120-h-52-mpc-calculation-bitwise-operation-components-24" name="p-51120-h-52-mpc-calculation-bitwise-operation-components-24"></a>5.2 MPC Calculation: Bitwise Operation Components</h3>
<div class="md-table">
<table>
<thead>
<tr>
<th>Calculation Name</th>
<th>Total Time</th>
<th>Communications</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr>
<td>EqualityZeroTest</td>
<td>211ms ~ 226ms</td>
<td>3,795</td>
<td>Input: 1 field element. Output: is zero value</td>
</tr>
<tr>
<td>BitDecomposition</td>
<td>479ms ~ 652ms</td>
<td>8,337</td>
<td>Input: 1 field element. Output: bitwise share.</td>
</tr>
<tr>
<td>LessThan</td>
<td>1.092s ~ 1.178s</td>
<td>20,529</td>
<td>Input: 2 field elements. Output: comparison result value</td>
</tr>
</tbody>
</table>
</div><p>Where communications refer to the number of broadcasts in the protocols.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51120-h-53-circuits-constraints-main-circuits-1-25" name="p-51120-h-53-circuits-constraints-main-circuits-1-25"></a>5.3 Circuits Constraints: Main Circuits 1</h3>
<div class="md-table">
<table>
<thead>
<tr>
<th>Circuit Name</th>
<th>Total Constraints</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr>
<td>MySecretInputCircuit</td>
<td>6,574</td>
<td>1 secret &amp; its Pedersen commitment &amp; additional range constraints</td>
</tr>
<tr>
<td>PedersenComCircuit</td>
<td>2,544</td>
<td>1 secret &amp; its Pedersen commitment</td>
</tr>
<tr>
<td>MyCircuit</td>
<td>5,094</td>
<td>2 committed secrets &amp; their multiplication</td>
</tr>
</tbody>
</table>
</div><h3><a class="anchor" href="https://ethresear.ch#p-51120-h-54-circuits-constraints-main-circuits-2-mpc-bitwise-operations-26" name="p-51120-h-54-circuits-constraints-main-circuits-2-mpc-bitwise-operations-26"></a>5.4 Circuits Constraints: Main Circuits 2 (MPC Bitwise Operations)</h3>
<div class="md-table">
<table>
<thead>
<tr>
<th>Circuit Name</th>
<th>Total Constraints</th>
<th>Proving Time</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr>
<td>BitDecompositionCircuit</td>
<td>672</td>
<td>2.028s</td>
<td>1 secret</td>
</tr>
<tr>
<td>SmallerEqThanCircuit</td>
<td>621~625</td>
<td>1.468s~1.483s</td>
<td>1 bitwise secret &amp; 1 comparison</td>
</tr>
<tr>
<td>EqualityZeroCircuit</td>
<td>4</td>
<td>368.426ms</td>
<td>1 secret</td>
</tr>
<tr>
<td>PedersenComCircuit</td>
<td>2,543</td>
<td>6.572s</td>
<td>1 secret &amp; its Pedersen commitment</td>
</tr>
<tr>
<td>SmallerThanCircuit</td>
<td>2,016</td>
<td>4.039s</td>
<td>2 secrets and ordering</td>
</tr>
</tbody>
</table>
</div><h3><a class="anchor" href="https://ethresear.ch#p-51120-h-55-circuits-constraints-werewolfs-circuits-27" name="p-51120-h-55-circuits-constraints-werewolfs-circuits-27"></a>5.5 Circuits Constraints: Werewolf’s Circuits</h3>
<div class="md-table">
<table>
<thead>
<tr>
<th>Circuit Name</th>
<th>Total Constraints</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr>
<td>KeyPublicizeCircuit (3 parties)</td>
<td>15,266</td>
<td>3 committed secrets &amp; their sum</td>
</tr>
<tr>
<td>DivinationCircuit (3 parties)</td>
<td>22,249</td>
<td>Many committed secrets &amp; ElGamal encryption</td>
</tr>
<tr>
<td>AnnonymousVotingCircuit (3 parties)</td>
<td>8,065</td>
<td>3 committed secrets &amp; some bit operations</td>
</tr>
<tr>
<td>WinningJudgementCircuit (3 parties)</td>
<td>9,648</td>
<td>3 committed secrets &amp; some bit operations</td>
</tr>
<tr>
<td>RoleAssignmentCircuit (3 parties)</td>
<td>about 65,644</td>
<td>3 committed secrets &amp; grouping protocol</td>
</tr>
</tbody>
</table>
</div><h2><a class="anchor" href="https://ethresear.ch#p-51120-h-6-impact-28" name="p-51120-h-6-impact-28"></a>6. Impact</h2>
<p>If third-party verification becomes possible in multiparty computation, it can be applied to blockchain.</p>
<p>If collaborative proof can be created, including bitwise operations, the versatility of ZKMPC will increase and it can be used in a wider range of situations. In addition, secret computation in two or three parties has a wide range of applications. Recently, re-staking techniques such as EigenLayer have been developed, and ZKMPC may be able to efficiently perform re-staking while ensuring security in such situations. In addition, in the convolution layer of machine learning, linear and nonlinear operations are mixed, and it is important to be able to convert between the two. This will be a pioneering example of such a case.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51120-h-7-future-issues-29" name="p-51120-h-7-future-issues-29"></a>7. Future Issues</h2>
<ul>
<li>Computational cost
<ul>
<li>Since the number of constraints in the circuit to verify PedersenCommitment is about 7000, it is expected that the proof time will increase by 1 to 10 seconds even if we increase the number of committed inputs by one.</li>
</ul>
</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-51120-bibliography-30" name="p-51120-bibliography-30"></a>Bibliography</h2>
<ul>
<li>
<p>[1] SPDZ original paper</p>
<ul>
<li>title: Multiparty Computation from Somewhat Homomorphic Encryption</li>
<li><a href="https://eprint.iacr.org/2011/535" rel="noopener nofollow ugc">https://eprint.iacr.org/2011/535</a></li>
</ul>
</li>
<li>
<p>[2] Collaborative zk-SNARKs</p>
<ul>
<li>title: Experimenting with Collaborative zk-SNARKs: Zero-Knowledge Proofs for Distributed Secrets</li>
<li><a href="https://eprint.iacr.org/2021/1530" rel="noopener nofollow ugc">https://eprint.iacr.org/2021/1530</a></li>
</ul>
</li>
<li>
<p>[3] Takashi Nishide, Kazuo Ohta 2007 Multiparty computation for interval, equality, and comparison without bit-decomposition protocol</p>
<ul>
<li><a href="https://link.springer.com/chapter/10.1007/978-3-540-71677-8_23" rel="noopener nofollow ugc">https://link.springer.com/chapter/10.1007/978-3-540-71677-8_23</a></li>
<li>This paper describes efficient MPC primitives without bit-decomposition.</li>
</ul>
</li>
<li>
<p>[4] Daniel Escudero, Satrajit Ghosh, Marcel Keller, Rahul Rachuri, Peter Scholl 2020 Improved primitives for MPC over mixed Arithmetic-Binary circuits</p>
<ul>
<li><a href="https://eprint.iacr.org/2020/338" rel="noopener nofollow ugc">https://eprint.iacr.org/2020/338</a></li>
</ul>
</li>
<li>
<p>[5] Secure Grouping Protocol Using a Deck of Cards</p>
<ul>
<li><a href="https://arxiv.org/abs/1709.07785" rel="noopener nofollow ugc">https://arxiv.org/abs/1709.07785</a> (en)</li>
<li><a href="https://ipsj.ixsq.nii.ac.jp/ej/?action=repository_uri&amp;item_id=175767&amp;file_id=1&amp;file_no=1" rel="noopener nofollow ugc">https://ipsj.ixsq.nii.ac.jp/ej/?action=repository_uri&amp;item_id=175767&amp;file_id=1&amp;file_no=1</a> (jp)</li>
</ul>
</li>
</ul>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/zkmpc-publicly-auditable-mpc-for-general-purpose-computations/20956">Read full topic</a></p>
]]></content:encoded>
<pubDate>Mon, 11 Nov 2024 03:31:05 +0000</pubDate>
</item>
<item>
<title>Fusion Module - 7702 alternative with no protocol changes</title>
<link>https://ethresear.ch/t/fusion-module-7702-alternative-with-no-protocol-changes/20949</link>
<guid>https://ethresear.ch/t/fusion-module-7702-alternative-with-no-protocol-changes/20949</guid>
<content:encoded><![CDATA[
<div> 关键词：Externally Owned Accounts (EOAs)，Smart Contract Accounts (SCAs)，Pectra 协议升级，Fusion 模块，Supertransactions

总结:<br />
本文提出了一种创新机制，使得Externally Owned Accounts（EOAs）可以像智能账户（SCAs）那样运作，无需等待Pectra协议升级并且与现有系统兼容。通过创建每个EOA的预计算智能账户地址作为伴侣账户，用户可以在仅需一次签名的情况下利用其EOA执行复杂的操作，如交易批处理、跨链交互等，从而解决了SCAs用户体验和资产管理复杂性的问题。该方案的关键技术包括利用EVM交易数据字段附加额外信息以及通过ERC20Permit标准实现无gas费用的EOA操作。Fusion模块作为一个标准化的7579智能账户模块，支持这一融合模式，并能与其他智能账户提供商实现兼容。此外，文章还探讨了Fusion模块与未来EIP-7702协议升级的关系及差异，展示了两种方法各自的优势和适用场景。总之，Fusion模块为Web3账户进化提供了一个过渡解决方案，允许用户立即使用智能账户功能，同时保持对未来发展路径的兼容性。 <div>
<p>During the development and R&amp;D of <a href="https://www.biconomy.io/post/modular-execution-environment-supertransactions" rel="noopener nofollow ugc">Modular Execution Environments and Supertransactions</a>, we uncovered an intriguing mechanism that enables Externally Owned Accounts (EOAs) to function similarly to smart accounts. This innovation can be implemented immediately, without waiting for the 7702 Pectra protocol upgrade whilst at same time being compatible. We’ve conducted testing and developed a proof-of-concept. Let’s delve into the details!</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51097-background-1" name="p-51097-background-1"></a>Background</h2>
<p>The blockchain community has engaged in extensive discussions, particularly within the Chain Abstraction and Account Abstraction groups, regarding the role of smart account models in the ecosystem and the potential impact of EIP-7702 on user experience.</p>
<p>EOAs remain a fundamental element in blockchain infrastructure and while they represent one of the most prevalent wallet types in current use, EOAs lack several essential features that modern Web3 users consider standard requirements for day-to-day operations.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51097-motivation-2" name="p-51097-motivation-2"></a>Motivation</h3>
<p>Smart Contract Accounts (SCAs) represent a significant advancement in blockchain wallet technology, offering programmable functionality that extends far beyond the capabilities of traditional EOAs. The enhanced feature set of SCAs includes:</p>
<ul>
<li>Transaction batching</li>
<li>Spending limits</li>
<li>Account recovery mechanisms</li>
<li>Gas fee abstraction</li>
<li>Cross-chain interoperability</li>
<li>Automation and scheduled transactions</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51097-challenges-3" name="p-51097-challenges-3"></a>Challenges</h3>
<p>The current implementation of SCAs presents several operational challenges:<br />
First, the requirement to transfer assets from an EOA to utilize an SCA creates unnecessary friction in the user experience and leads to liquidity fragmentation across accounts.<br />
Second, the proliferation of different smart account providers across Apps fragments the unified wallet experience users have come to expect. This stands in contrast to the traditional EOA model, where users maintain a single wallet address and consolidated balance across multiple blockchain networks.</p>
<p>As illustrated below, the user would have to sign twice to fund their smart account and then start interacting with it. And to make it even worse, the first signature forces user to pay gas themselves, meaning they need to own some native coin. The common 2-step SCA onboarding flow is shown below:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/0/0/001321d7cef3b7f46547ba624b0d2f7972ba3202.png" title="test"><img alt="Two user signatures, shown in blue, are required for usingthe smart accoun for the first time." height="500" src="https://ethresear.ch/uploads/default/optimized/3X/0/0/001321d7cef3b7f46547ba624b0d2f7972ba3202_2_669x500.png" title="test" width="669" /></a></div><br />
<em>Two user signatures, shown in blue, are required for usingthe smart accoun for the first time.</em><p></p>
<p>Furthermore, the implementation of embedded SCAs within dApps introduces additional complexity to asset management. Users must track and manage assets on an application-by-application basis, while placing trust in each application to maintain accessible interfaces for manual asset management.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51097-solution-fusing-eoa-sca-4" name="p-51097-solution-fusing-eoa-sca-4"></a>Solution - Fusing EOA &amp; SCA!</h3>
<p>Our innovation, which we’ll present shortly, introduces a mechanism that can be utilized today to achieve a virtual merging of EOA and SCA into a single entity. This is accomplished without requiring users to explicitly upgrade their EOA or manually transfer funds to a new address. This breakthrough approach fundamentally resolves the fragmentation, complexity, and user experience challenges that have historically impeded SCA adoption, while maintaining backward compatibility with existing infrastructure.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51097-eoa-fused-5" name="p-51097-eoa-fused-5"></a>EOA Fused</h2>
<p>We’ve developed a method that allows users to utilize their EOAs as if they were interacting with a smart account. This is achieved while only requiring a single signature for each action, eliminating the need for manual pre-activations, upgrades, or fund transfers to new addresses.</p>
<p>Our proposal involves precalculating a Smart Account address for each EOA and utilizing it as a <em>companion</em> account. This approach builds upon the established mechanism for generating ERC4337 accounts. The innovation in our solution lies in the ability to transfer tokens to the companion Smart Contract Account <em>and</em> provide bundlers with all necessary additional instructions (UserOps) to be executed - all with a single signature.</p>
<p>Furthermore, when a user first interacts with our system, the smart contract account is deployed on-demand (lazy deployment), ensuring that even the initial interaction only requires one user signature.</p>
<p>Key concepts of our approach include:</p>
<ol>
<li>Users need not be aware of their smart account wallet.</li>
<li>Users continue to use their EOA as their primary wallet, eliminating the need to explicitly move funds elsewhere.</li>
<li>Users never grant access to their entire EOA portfolio to execute complex operations on a small amount of tokens they hold.</li>
<li>Users sign <strong>ONCE</strong> for every interaction, providing a full EOA-like experience while utilizing funds from their EOA as if they were directly using the smart account.</li>
</ol>
<p>The companion smart account processes complex user requests that would not be possible with a standard EOA alone. These include batching multiple operations, interacting with several DeFi protocols simultaneously, and distributing assets to multiple recipients in a single transaction.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51097-live-demo-6" name="p-51097-live-demo-6"></a>Live Demo</h3>
<p>Demo video</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51097-technical-implementation-7" name="p-51097-technical-implementation-7"></a>Technical Implementation</h3>
<p><em>…how is it possible to use EOA as if it was an SCA, before Pectra upgrade?</em></p>
<p>We’ve leveraged the fact that appending arbitrary bytes to a valid EVM transaction maintains its validity while including all the appended data on-chain.</p>
<p>For instance, if we’re executing a standard ERC20 transfer function:</p>
<pre><code class="lang-auto">transfer(address to, uint256 amount)
</code></pre>
<p>We would typically encode this function as:</p>
<pre><code class="lang-auto">callData = abi.encode("transfer", [ address, amount ])
</code></pre>
<p>resulting in a transaction data structure:</p>
<pre><code class="lang-auto">tx = {
	to: "0xtokenaddress",
	value: 0,
	data: callData
}
</code></pre>
<p>However, we can append any arbitrary data to the callData field, and the transaction remains valid. The USDC transfer will execute correctly, while the additional appended hex data is simply ignored by the EVM.</p>
<p>This is a fundamental aspect of EVM design and how function selectors operate. For illustration, consider this <a href="https://sepolia.etherscan.io/tx/0xb25e84d0530e30fccd47ef82e08532927f727dc8093c2682682501459b971ad8" rel="noopener nofollow ugc">test USDC transfer on Sepolia</a>. The transaction transferred 1 USDC, but examining the input data field (viewed as UTF-8) reveals an additional message (image below).</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/4/d44022dce5256a8247934b5acedd38258c47c766.jpeg" title="Appending ASCII text to a valid ERC20 transaction."><img alt="Appending ASCII text to a valid ERC20 transaction." height="431" src="https://ethresear.ch/uploads/default/optimized/3X/d/4/d44022dce5256a8247934b5acedd38258c47c766_2_690x431.jpeg" width="690" /></a></div><br />
<em>Appending ASCII text to a valid ERC20 transaction.</em><p></p>
<p>An important consideration is that the user’s signature covers the entire transaction, including all appended extra data. This approach allows us to execute multiple actions simultaneously:</p>
<ol>
<li>Transfer funds (ETH or other ERC-20 tokens) from an EOA to a new address.</li>
<li>Include extra data in this action, which is part of the transaction fully signed by the EOA and verifiable on-chain. For example, this extra data could be the UserOp(s) hash to be executed by bundlers.</li>
</ol>
<p>By setting the destination of transferred funds to the user’s smart account address and including extra data (e.g., a userOp hash for operations to execute on the smart account), we achieve both actions with a single signature.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/8/5898b020bd637d123be31df4ea4318ddac0f94f5.png" title="Fusing EOA with companion SCA bundles both deposit and execute operations into a single user signature shown in blue."><img alt="Fusing EOA with companion SCA bundles both deposit and execute operations into a single user signature shown in blue." height="418" src="https://ethresear.ch/uploads/default/optimized/3X/5/8/5898b020bd637d123be31df4ea4318ddac0f94f5_2_690x418.png" width="690" /></a></div><br />
<em>Fusing EOA with companion SCA bundles both deposit and execute operations into a single user signature shown in blue.</em><p></p>
<p>The only requirement for this to function is that the user’s companion smart account contains a module capable of verifying a userOp given the fully signed &amp; serialized EVM transactions - which we have developed.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51097-fusion-module-7579-8" name="p-51097-fusion-module-7579-8"></a>Fusion Module (7579)</h2>
<p>The <a href="https://github.com/0xPolycode/mee-contracts/blob/901a13962f3becc0afe90c61f720fc6f02464ef8/contracts/validators/FusionValidator.sol" rel="noopener nofollow ugc">Fusion Module</a> is a standardized 7579 smart account module capable of validating userOps not only by validating userOpHash signatures but also by validating userOps given the fully signed and serialized EVM transaction.</p>
<p>The module maintains universal compatibility with existing ERC-7579 smart account providers while introducing enhanced validation mechanisms, as demonstrated in our reference implementation.</p>
<p><img alt="Fusion Module" height="401" src="https://ethresear.ch/uploads/default/original/3X/4/0/40fe4c193681061f3bc16d33ba70388cad46bb09.png" width="581" /></p>
<p>When processing a signed EVM transaction as userOp signature, the module executes the following validation sequence:</p>
<ol>
<li><strong>Transaction Parsing</strong><br />
Deconstructs the signed EVM transaction into its constituent components</li>
<li><strong>Cryptographic Verification</strong><br />
Performs cryptographic recovery of the transaction signer and validates against the smart account owner’s signature</li>
<li><strong>Data Extraction</strong><br />
Retrieves extraData parameters from the transaction data field</li>
<li><strong>Hash Verification</strong><br />
Validates the correlation between userOp hash and extraData field contents</li>
</ol>
<p>Upon successful completion of all validation checks, the userOp achieves on-chain execution eligibility, permitting execution by any entity willing to pay for gas costs.</p>
<p>By utilizing this standardized module, we maintain compatibility with the 4337 infrastructure and enable any smart account provider to implement one-click onboarding and execution, providing users with an experience akin to direct EOA interaction.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51097-infrastructure-for-fusion-modules-9" name="p-51097-infrastructure-for-fusion-modules-9"></a>Infrastructure for Fusion Modules</h2>
<p><em>… or, who’s paying for gas?</em></p>
<p>When a user executes an on-chain transaction that approves a userOp, there needs to be an entity willing to submit that userOp to the blockchain, knowing it’s now authorized and ready for execution. This entity must also have a mechanism to collect payment from the user for processing their complex request initiated from the EOA.</p>
<p>The current 4337 infrastructure can’t handle this efficiently due to limitations in how paymasters and bundlers are configured. The existing system expects users to either:</p>
<ol>
<li>Have native coins pre-staked in the EntryPoint contract</li>
<li>Receive sponsorship from a paymaster for their operation</li>
</ol>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/8/988c42cf30c080899e8ca516dcdf1c0d79c90c7a.png" title="Regular 4337 bundler will reject companion userOp if funds are on the EOA - bundler doesn’t know EOA &amp; SCA are companions!"><img alt="Regular 4337 bundler will reject companion userOp if funds are on the EOA - bundler doesn’t know EOA &amp; SCA are companions!" height="452" src="https://ethresear.ch/uploads/default/optimized/3X/9/8/988c42cf30c080899e8ca516dcdf1c0d79c90c7a_2_690x452.png" width="690" /></a></div><br />
<em>Regular 4337 bundler will reject companion userOp if funds are on the EOA - bundler doesn’t know EOA &amp; SCA are companions!</em><p></p>
<p>Neither option works effectively for Fusion because bundlers and/or paymasters aren’t aware that the companion SCA has got the EOA address linked to it - who’s funds could be used for paying for fees and onboarding assets.</p>
<p>On the other hand, user shouldn’t need to manually stake ETH or fund their SCA with paymaster-accepted tokens - after all, we’re aiming for seamless user experience.</p>
<p>This is where <strong>Modular Execution Environments (MEE)</strong> become crucial. Fusion Modules are integrated as first-class citizens within the MEE stack. The MEE infrastructure, which is briefly introduced <a href="https://www.biconomy.io/post/modular-execution-environment-supertransactions" rel="noopener nofollow ugc">here</a>, enables developers to build applications without concerning themselves with the complexities of transaction execution, whether across one chain or many.</p>
<p>MEE stack supports diverse gas payment models, including scenarios where Fusion Module manages EOA assets through companion SCA accounts.</p>
<p>Rather than standard blockchain transactions, at the heart of MEE, lies a new primitive called S<strong>upertransactions</strong>. Supertransactions can contain multiple userOps that manage a single SCA, and every Supertransaction is uniquely identified by its hash. This architecture aligns perfectly with the Fusion Modules, enabling users to execute complex cross-chain interactions by signing one root Supertransaction hash. For example, users can sign once to approve Supertransaction which:</p>
<ol>
<li>Transfers USDC from their EOA to SCA</li>
<li>Transfers a portion of USDC from the SCA to the MEE Node (as gas payment)</li>
<li>Swaps remaining USDC for ETH</li>
<li>Bridges ETH to another chain and transfer it back to the user’s EOA</li>
</ol>
<p>These and many other workflows become one-click experiences through the EOA+SCA pass-through model.</p>
<p>To illustrate: consider a user leveraging their EOA as if it were an SCA, without needing to know about the SCA’s existence. They can execute cross-chain swaps, directly approving and spending funds from their EOA, while companion SCA accounts handle the complex operations across multiple chains behind the scenes. The transaction lifecycle of such an operation is laid out below:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/9/8990a9f0e3cccb3c3256b6d1ae2c83eaf4f92f63.png" title="Single signature user interaction is again shown in blue - where user signed one EVM transaction to both fund their companion SCA AND execute multiple userOps once funded."><img alt="Single signature user interaction is again shown in blue - where user signed one EVM transaction to both fund their companion SCA AND execute multiple userOps once funded." height="500" src="https://ethresear.ch/uploads/default/optimized/3X/8/9/8990a9f0e3cccb3c3256b6d1ae2c83eaf4f92f63_2_205x500.png" width="205" /></a></div><br />
<em>Single signature user interaction is again shown in blue - where user signed one EVM transaction to both fund their companion SCA AND execute multiple userOps once funded.</em><p></p>
<p>After the dApp prepares the Supertransaction, users only need to sign a single ERC20 transfer transaction (highlighted in blue in the diagram above). The MEE stack then handles all operation orchestration and execution across different blockchain networks. Users simply see the end result: the received asset credited to their EOA on the destination chain.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51097-advanced-signing-schemes-10" name="p-51097-advanced-signing-schemes-10"></a>Advanced Signing Schemes</h2>
<p>The upgrade process described above enables an EOA to gain SCA capabilities. However, users must still maintain a balance of native coins in their EOA to execute Supertransactions with a single signature. This limitation stems from the core design of the EVM, where gas fees must be paid from the public address associated with the transaction-signing private key.</p>
<p>However, there’s an interesting workaround. Through extensive exploration of different approaches to appending Supertransaction hash, we’ve discovered several alternative methods that allow users to spend assets from their EOA without paying gas fees directly.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51097-erc20permit-gasless-eoa-11" name="p-51097-erc20permit-gasless-eoa-11"></a>ERC20Permit (Gasless EOA)</h3>
<p>We leverage the ERC20Permit standard for supported tokens (such as USDC) to deliver a seamless, gasless experience where EOAs can execute Supertransactions without even having any ETH on their wallet, while still requiring only one off-chain singature!</p>
<p>ERC20Permit standard allows EOAs to approve spending of the tokens off-chain, by only signing an off-chain message of the following structure:</p>
<pre><code class="lang-auto">PermitMessage = {
	owner,
	spender,
	amount,
	nonce,
	deadline,
};
</code></pre>
<p>Since the <strong>deadline</strong> parameter is already validated through the smart account/ERC-4337 model during userOp validation and execution (via validAfter and validUntil fields), we can repurpose it as a data carrier. This field stores the Supertransaction hash that fully describes the companion SCA operations that will be approved with the signed Permit Message. The deadline field’s uint256 type conveniently matches bytes32, allowing us to accomplish two objectives with a single off-chain signature:</p>
<ol>
<li>Authorize the smart account to spend ERC20 assets by setting the spender address to be equal to the user’s companion SCA address (limited by the <strong>amount</strong> parameter) - which is what the ERC20Permit standard enables.</li>
<li>Approve smart account operations that can use up to the specified <strong>amount</strong> of funds from the EOA by setting the deadline value to the given Supertransaction hash.</li>
</ol>
<p>When using the ERC20Permit signature, Fusion Module SCA module must:</p>
<ol>
<li>Parse the ERC20Permit signature</li>
<li>Call the token’s permit() function when necessary</li>
<li>Execute the operations defined in the userOp - transferring funds from EOA to SCA and performing any additional specified actions</li>
</ol>
<p>The paymaster can handle gas fees conventionally, deducting them from the same tokens transferred from EOA to SCA. Importantly, all operations occur effectively as a single atomic step - the user signs once, and this signature covers both gas payment and all user-defined actions.</p>
<p>As an example: let’s look at the example where user wants to transfer USDC from their EOA to some other address, but has no native coin on their wallet. In this case, they could leverage Fusion Module with the MEE stack and perform this operation with a single off-chain signature. The transaction lifecycle of such an operation is laid out below:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/e/de3f9e4a5f6f2db10f250cd083176fe7f11f1887.png" title="Single signature user interaction show in blue - where user signed an off-chain ERC20Permit message to both fund their SCA in a gasless way, and then execute userOp once funded."><img alt="Single signature user interaction show in blue - where user signed an off-chain ERC20Permit message to both fund their SCA in a gasless way, and then execute userOp once funded." height="500" src="https://ethresear.ch/uploads/default/optimized/3X/d/e/de3f9e4a5f6f2db10f250cd083176fe7f11f1887_2_259x500.png" width="259" /></a></div><br />
<em>Single signature user interaction show in blue - where user signed an off-chain ERC20Permit message to both fund their SCA in a gasless way, and then execute userOp once funded.</em><p></p>
<p>The only interaction performed by the user is highlighted in blue color in flow from above. This means the EOA still feels like an EOA while more complex flow is being approved and executed by the MEE stack.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51097-cross-chain-intent-signature-erc-7683-12" name="p-51097-cross-chain-intent-signature-erc-7683-12"></a>Cross-chain Intent Signature (ERC-7683)</h3>
<p>Another way of fusing EOA with a companion SCA is by packaging Supertransaction hash together with a signed cross-chain intent which is defined by ERC-7683 standard to have the following off-chain message structure:</p>
<pre><code class="lang-auto">/// @title CrossChainOrder type
/// @notice Standard order struct to be signed by swappers, disseminated to fillers, and submitted to settlement contracts
struct CrossChainOrder {
	/// @dev The contract address that the order is meant to be settled by.
	/// Fillers send this order to this contract address on the origin chain
	address settlementContract;
	/// @dev The address of the user who is initiating the swap,
	/// whose input tokens will be taken and escrowed
	address swapper;
	/// @dev Nonce to be used as replay protection for the order
	uint256 nonce;
	/// @dev The chainId of the origin chain
	uint32 originChainId;
	/// @dev The timestamp by which the order must be initiated
	uint32 initiateDeadline;
	/// @dev The timestamp by which the order must be filled on the destination chain
	uint32 fillDeadline;
	/// @dev Arbitrary implementation-specific data
	/// Can be used to define tokens, amounts, destination chains, fees, settlement parameters,
	/// or any other order-type specific information
	bytes orderData;
}
</code></pre>
<p>By using the same approach as with the ERC20Permit, we can store the Supertransaction hash inside the <code>bytes orderData</code> field and have the signed off-chain intent be used for both the actual intent settlement and the subsequent operations performed once the intent has been settled.</p>
<p>By making Fusion Module compatible with the ERC-7683 standard, we’ve created a powerful synergy that bridges traditional smart contract interactions with intent-based systems. This integration enables users to sign a single intent that can both settle cross-chain operations and trigger complex subsequent actions across multiple chains. The result is a more streamlined, user-friendly experience that maintains the security and flexibility of both systems while significantly reducing complexity for end users and developers alike.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51097-implications-for-smart-accounts-today-13" name="p-51097-implications-for-smart-accounts-today-13"></a>Implications for Smart Accounts Today</h2>
<p>The signature schemes described above demonstrate how single-signature onboarding enables users to interact exclusively with their EOA while utilizing Smart Accounts as a pass-through mechanism.</p>
<p>This architecture enhances the user experience for smart accounts broadly, even when they serve as primary accounts, by enabling one-signature onboarding of EOA assets and operation execution. In essence, the smart account address becomes a true extension of the EOA - any asset in the EOA becomes seamlessly accessible through the Smart Account address.</p>
<p>Our Fusion Module validator currently supports three signature schemes:</p>
<ol>
<li>Plain userOpHash signature</li>
<li>Signed EVM transaction signature</li>
<li>Signed ERC20Permit message signature</li>
</ol>
<p>The plain userOpHash signature maintains backwards compatibility, allowing the SCA to work with existing 4337 infrastructure, including bundlers and paymasters processing userOps. The EVM and ERC20Permit schemes extend this functionality, enabling the same SCA to act as a companion account and support Fusion Transactions.</p>
<p>We’ve made the module implementation publicly available in our <a href="https://github.com/0xPolycode/mee-contracts/blob/901a13962f3becc0afe90c61f720fc6f02464ef8/contracts/validators/FusionValidator.sol" rel="noopener nofollow ugc">GitHub repository.</a> Through our collaboration with the Rhinestone team, we’re working to have the standardized ERC-7579 module audited and published in the whitelisted modules repository, making it accessible to all smart account providers.</p>
<p>Integration of these modules is straightforward, varying based on the dApp’s specific needs. Developers simply need to modify the userOp signature field in their frontend to enable single-signature EOA interactions that function like native smart account operations.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51097-h-7702-fusion-different-approaches-to-the-same-problem-14" name="p-51097-h-7702-fusion-different-approaches-to-the-same-problem-14"></a>7702 &amp; Fusion - Different Approaches to the Same Problem</h2>
<p>We propose this model as a solution that’s implementable today, without waiting for the Pectra upgrade, while supporting almost all features that 7702 will enable as well as being compatible with each other.</p>
<p>There are slight differences in capabilities between these models. Here’s a comparison of the pros and cons:</p>
<div class="md-table">
<table>
<thead>
<tr>
<th>7702</th>
<th>Fusion</th>
</tr>
</thead>
<tbody>
<tr>
<td><img alt=":red_circle:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/red_circle.png?v=12" title=":red_circle:" width="20" /> Converts an EOA to SCA (code gets an access to an EOA in full)</td>
<td><img alt=":sparkle:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/sparkle.png?v=12" title=":sparkle:" width="20" /> Keeps EOA &amp; SCA separated.</td>
</tr>
<tr>
<td><img alt=":red_circle:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/red_circle.png?v=12" title=":red_circle:" width="20" /> Requires an initial signature to activate SCA.</td>
<td><img alt=":sparkle:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/sparkle.png?v=12" title=":sparkle:" width="20" /> Works immediately - first signature for first action.</td>
</tr>
<tr>
<td><img alt=":sparkle:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/sparkle.png?v=12" title=":sparkle:" width="20" /> Supports spending multiple tokens from EOA at once.</td>
<td><img alt=":red_circle:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/red_circle.png?v=12" title=":red_circle:" width="20" /> Supports spending one token at a time from an EOA.</td>
</tr>
<tr>
<td><img alt=":sparkle:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/sparkle.png?v=12" title=":sparkle:" width="20" /> Supports gasless EOA transactions for any token.</td>
<td><img alt=":red_circle:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/red_circle.png?v=12" title=":red_circle:" width="20" /> Supports spending gasless transactions for ERC20Permit tokens.</td>
</tr>
<tr>
<td><img alt=":man_shrugging:t2:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/man_shrugging/2.png?v=12" title=":man_shrugging:t2:" width="20" /> Always relatively cheap (code is already deployed).</td>
<td><img alt=":man_shrugging:t2:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/man_shrugging/2.png?v=12" title=":man_shrugging:t2:" width="20" /> Initially expensive (SCA deployment) and then cheap for subsequent actions.</td>
</tr>
<tr>
<td><img alt=":red_circle:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/red_circle.png?v=12" title=":red_circle:" width="20" /> Doesn’t support single-signature multichain operations.</td>
<td><img alt=":sparkle:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/sparkle.png?v=12" title=":sparkle:" width="20" /> Supports single-signature multichain operations.</td>
</tr>
</tbody>
</table>
</div><p><img alt=":sparkle:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/sparkle.png?v=12" title=":sparkle:" width="20" /> indicates an advantage over <img alt=":red_circle:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/red_circle.png?v=12" title=":red_circle:" width="20" /> according to our assessment.<br />
<img alt=":man_shrugging:t2:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/man_shrugging/2.png?v=12" title=":man_shrugging:t2:" width="20" /> indicates: “Uncertain, further testing required”</p>
<p>Evidently, some actions are only possible with 7702, while others can only be performed using the Fusion Module approach. However, at a high level, both approaches address the same issue - how to safely enable EOA to have a SCA functionality.</p>
<p>Importantly, both models are forward-compatible with the general AA roadmap.</p>
<p>It’s often noted that 7702 will enable a gradual transition towards a fully account-abstracted future, as users can upgrade to increasingly complex account implementations on-the-fly.</p>
<p>7702 and the Fusion module differ in that Fusion module maintain separate account spaces: the EOA remains an EOA, while the SCA exists as a companion account, used only to spend funds that the user explicitly approves at the moment of execution.</p>
<p>Conversely, 7702 fully converts the EOA into an SCA - a stateful change. The EOA remains upgraded and points to an SCA implementation until an upgrade to a new account implementation is executed.</p>
<p>While both approaches have their merits and enable EOA accounts to function like SCAs, Fusion Modules allow users to switch between implementations on every request, eliminating the need to store the active implementation.</p>
<p>They can be viewed as a pathway to adopting smart accounts by enabling:</p>
<ol>
<li>Level 1 adoption: Using SCAs as a pass-through only - retaining assets in the EOA unless necessary, and transferring only what’s required to process the request</li>
<li>Level 2 adoption: Using SCAs as the primary wallet and onboarding funds by bundling the deposit transaction with the first userOp</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#p-51097-conclusion-15" name="p-51097-conclusion-15"></a>Conclusion</h2>
<p>The Fusion Module is a step forward in web3 account evolution, offering immediate benefits while staying compatible with future upgrades.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51097-immediate-benefits-16" name="p-51097-immediate-benefits-16"></a>Immediate Benefits</h3>
<ul>
<li>Users can start using smart account features today</li>
<li>No need to wait for protocol upgrades</li>
<li>Simple one-signature experience</li>
<li>Works with existing infrastructure</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51097-future-compatibility-17" name="p-51097-future-compatibility-17"></a>Future Compatibility</h3>
<p>The Fusion Module is designed to work alongside EIP-7702 when it launches. While 7702 will transform EOAs into SCAs directly, Fusion Module offers a complementary approach by keeping EOAs and SCAs separate but connected. This means:</p>
<ol>
<li>Users can start with Fusion Module today and seamlessly transition to 7702 when ready</li>
<li>Projects can build with Fusion Module now, knowing their implementations will remain valid post-7702</li>
<li>Some features unique to Fusion (like single-signature multichain operations) will complement 7702’s capabilities</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-51097-path-forward-18" name="p-51097-path-forward-18"></a>Path Forward</h3>
<p>We see Fusion Module as an important bridge to the future of account abstraction. It enables immediate adoption of smart account features while the ecosystem continues to evolve. Whether users eventually choose to fully upgrade their EOAs with 7702 or continue using the Fusion Module approach, they’ll have the flexibility to choose what works best for their needs.</p>
<p>The module is open source and ready for integration. We invite developers to try it out and help shape the future of wallet interactions in Web3. If you’d like  to enable smart contract capabilities on the EOA for your dapp or wallet, or just test it out, feel free to <a href="https://hlp8w9pp2h3.typeform.com/biconomy?typeform-source=ethresearch" rel="noopener nofollow ugc">reach out here</a> and we’ll help!</p>
            <p><small>2 posts - 2 participants</small></p>
            <p><a href="https://ethresear.ch/t/fusion-module-7702-alternative-with-no-protocol-changes/20949">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 08 Nov 2024 13:00:04 +0000</pubDate>
</item>
<item>
<title>Multilane Sequencing</title>
<link>https://ethresear.ch/t/multilane-sequencing/20946</link>
<guid>https://ethresear.ch/t/multilane-sequencing/20946</guid>
<content:encoded><![CDATA[
<div> 关键词: 多车道序列化、Layer 2 可互操作性、安全性、交易原子性、成本

总结:
多车道序列化是一种针对以太坊Layer 2扩展方案的模型，旨在解决Layer 2之间的可互操作性、交易安全性和资产转移成本等问题。该模型通过将Layer 2分类为基于与以太坊主网结算频率的“车道”，如Lane-1、Lane-2等。车道频率越高（更慢），结算越不频繁；反之则更频繁。

任何位于Lane-M的链可以作为两个Lane-N之间（N>M）的桥梁。在Lane-N上，每N个块都会利用以太坊的安全性锁定整个Lane的状态，特殊车道结算合约和原子操作可以在区块编号为N的倍数时执行。

Rollup在Lane-N上的生产速度自然会超过N，这意味着Rollup的序列器或预确认者群体在L1结算上下文中扮演着未来将在L1上结算用户交易的角色。独立的预确认者可以提供更细粒度的保证，进一步分割Rollup块。

每个Rollup状态在任何时候都可能在具有不同安全保证的不同车道上得到结算。这创造了一个安全性、可组合性和价格之间的权衡——可组合性安全性价格权衡（CST）。Rollup选择更高的车道可以降低安全性和原子性成本，但也会限制其与更低车道的直接交互。

此外，Rollup还可以支持原生的多车道序列化，允许资产在不同车道间移动，遵循仅能在车道结算时间进行跨车道移动的规则。这种机制使得应用开发者可以根据智能合约对安全性及交易成本的需求，选择部署到特定车道的分片上。例如，Rollup X可以为游戏提供16天结算周期的车道、为资产转移提供6分钟结算周期的车道，以及为大规模跨Rollup交换提供1分钟结算周期的车道。 <div>
<h1><a class="anchor" href="https://ethresear.ch#p-51091-multilane-sequencing-1" name="p-51091-multilane-sequencing-1"></a>Multilane Sequencing</h1>
<p>Thanks to Conor McMenamin for the review and comments (and yes, he said it needs to be better before I post it here but there is a limit to my research skills)</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51091-intro-2" name="p-51091-intro-2"></a>Intro</h3>
<p>L2 interoperability / L2 bridging is the subject of major discussion now. It is worth noting that L2 interop touches all the important aspects of Ethereum scaling—security of L2s, atomicity of transactions, cost of moving assets between chains, etc.</p>
<p>Any solutions bringing us closer to a standardization of the bridging, or at least a standardization of how we describe them, would likely help with cross-chain risk management, better comparability of L2s, and better scaling UX.</p>
<p>Below, I am proposing a multilane sequencing model for L2s on Ethereum that aims at addressing these problems (or rather not really solving them but creating ways of talking and thinking about tradeoffs and classification).</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51091-how-do-we-construct-lanes-3" name="p-51091-how-do-we-construct-lanes-3"></a>How do we construct Lanes?</h3>
<p>First, we construct the lanes based on their settlement frequency measured as an L1 blocks multiplier. We make the construction binary, so the lanes are Lane-1, Lane-2, Lane-4, Lane-8, Lane-16, and so on. Lane-8 settles to Ethereum every 8 blocks. Lane-1 settles to Ethereum on every Ethereum block. For every chain, we place it on a corresponding lane depending on how often it settles on L1. And so, Ethereum mainnet trivially lands on Lane-1. Any rollup that produces a block every N Ethereum blocks lands on Lane-N. A rollup targeting Lane-256 settles on Ethereum L1 every ~50 minutes (12 seconds × 256 blocks / 60 seconds is 51.2).</p>
<p>We call Lanes ‘lower’ or ‘faster’ if they settle more often. We call Lanes ‘higher’ or ‘slower’ if they settle less often.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/d/4d021fa96f3e6052958ff4bf3860fab4c1b38d09.png" title="image"><img alt="image" height="480" src="https://ethresear.ch/uploads/default/optimized/3X/4/d/4d021fa96f3e6052958ff4bf3860fab4c1b38d09_2_690x480.png" width="690" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#p-51091-details-4" name="p-51091-details-4"></a>Details</h3>
<p>Any chain on Lane-M can serve as a bridge between chains on Lane-N as long as N &gt; M. So any of the below can be a bridge:</p>
<ul>
<li>L1 for any two L2s</li>
<li>Any generic L2 for other L2s that are settling less often
<ul>
<li>specifically any ‘bridge rollup’ that settles on Ethereum more often than the rollups that it facilitates bridging between</li>
</ul>
</li>
</ul>
<p>Why so? On Lane N, every N blocks we lock the state of the entire Lane N using Ethereum security. Any special lane-settling contracts, atomic operations, etc., can be executed on blocks with numbers being a multiple of N.</p>
<p>Rollups on Lane-N naturally produce blocks more often than N (which is practically the rollup sequencer or group of sequencers acting as preconfirmers in the context of L1 settlement—the user trusts the L2 sequencer layer that it will settle user transactions on L1 in the future).</p>
<p>Separately, any independent preconfirmer can offer even more fine-grained guarantees and split rollup blocks even further (for example, rollup-boost can act as the Unichain block preconfirmer by splitting the block into 250 ms chunks).</p>
<p>At any Lane-N, there may exist alternative settlement chains with their own security guarantees. As such, you can imagine a Layer-1/8th which allows for settlements up to every 1.5 seconds. These may include, for example, DA layers.</p>
<p>Any state of a rollup can, at a given time, be settled at various different lanes with varying security guarantees. If I have an account entry on a Rollup X on Lane-N, then I can take timestamps of the state at times modulo various multiples of 12 seconds (1.5 seconds, 3 seconds, 6 seconds, 12 seconds, etc.) and find any settlement layer to which the rollup settles at the given lane. Then I can say that my state at that time is secured to that layer’s security level. For example, if Ethereum’s security level is $100 B, and there is a DA layer secured by $10 B that the rollup settles to every 3 seconds, and my account had $1000 at time 12k and $1500 at time 12k + 3, then I have $1000 secured by $100 B and an additional $500 ($1500 − $1000) secured by a $10 B settlement layer until time 12(k+1) when (assuming the balance does not change) it will get secured by $100B as well.</p>
<p>Now, security itself may actually be seen as a bit more tricky, as it is affected by the security of the layer that the rollup settles to, as well as the security of the rollup’s enshrined bridge. It defines how likely the rollup may be seen as a pure security settlement route from higher lanes all the way to Lane-1 (Ethereum mainnet). Simply speaking, if you have a pure security route (absolutely no-risk bridge—theoretical), then bridging through a Lane-256 rollup between two Lane-512 rollups is as secure as bridging directly via L1.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/a/5aa4e2fdbcd321b475f68565850baf3ca3cd8b56.jpeg" title="image"><img alt="image" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/5/a/5aa4e2fdbcd321b475f68565850baf3ca3cd8b56_2_642x500.jpeg" width="642" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#p-51091-composability-security-price-trade-off-5" name="p-51091-composability-security-price-trade-off-5"></a>Composability-Security-Price Trade-off</h3>
<p>Why would a rollup ever choose to be placed on higher lanes? Because lower lanes (Lane-1, Lane-2, etc.) are much more expensive—settling on Ethereum mainnet is expensive as it has to pay for the borrowed security. Also, the higher the Lane level, the higher the price of atomic composability—I can atomically compose with any other rollup at the same or higher Lane, but to compose with a rollup on a lower Lane I need to settle to at least one of the lower Lane rollups—borrowing security from Ethereum mainnet for that Lane settlement or finding another settlement layer on the Lane. Hence, we have a trade-off between the price of composability and the price of security—Composability-Security Trade-off (CST). This seems to be an intuition from all economic activity—the more different markets I want to access, the more I have to pay for access to the markets that everyone uses for settlement or to access alternative, more shady settlement venues.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51091-multilane-sequencing-6" name="p-51091-multilane-sequencing-6"></a>Multilane Sequencing</h3>
<p>Now, interestingly, any rollup may participate in native multilane sequencing, allowing for some kind of Lane sharding. The rollup allows assets to move between the Lane shards with the following rules—you can move assets from Lane-N to another Lane only at the time when Lane-N settles. This means that I can move assets quite often to slower settling lanes, but I can only move assets back to the faster lanes every now and then. And so, if Rollup X supports sequencing on Lane-8 and Lane-32, then I can move assets from Lane-32 to Lane-8 at blocks 32k, but I can move assets from Lane-8 to Lane-32 at blocks 32k, 32k+8, 32k+16, 32k+24. Moving assets to a faster lane means that I will be ready to pay more for any transactions involving these assets (I settle more often to Ethereum mainnet), but I am getting more composability natively. Solutions like this should be a relatively simple state sharding for L2s, and application developers could even choose which Lane shard they deploy their smart contract to, hence deciding the level of security the application guarantees and the cost of transactions. So, for example, Rollup X could offer a 16-day Lane for gaming, a 6-minute Lane for asset transfers, or a 1-minute Lane for large cross-rollup swaps.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/b/5bdbad3ebd2cea676dc51e6f20ec41f9b7a6db65.png" title="image"><img alt="image" height="318" src="https://ethresear.ch/uploads/default/optimized/3X/5/b/5bdbad3ebd2cea676dc51e6f20ec41f9b7a6db65_2_689x318.png" width="689" /></a></div><p></p>
            <p><small>2 posts - 2 participants</small></p>
            <p><a href="https://ethresear.ch/t/multilane-sequencing/20946">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 08 Nov 2024 07:44:38 +0000</pubDate>
</item>
<item>
<title>Orbit SSF in Practice</title>
<link>https://ethresear.ch/t/orbit-ssf-in-practice/20943</link>
<guid>https://ethresear.ch/t/orbit-ssf-in-practice/20943</guid>
<content:encoded><![CDATA[
<div> 关键词：Orbit SSF、单槽最终性（SSF）、委员会选择、ETH_CEIL、epoch 过程优化

总结：
本文提出了一种针对以太坊协议实施Orbit SSF方法的方案，该方案通过仅使用一个控制参数ETH_CEIL简化了验证者抽样形成的委员会过程，旨在实现单槽最终性，提高网络的最终性和安全性。文章介绍MaxEB背景，它限制每个验证者的最大有效余额为2048 ETH，以平衡经济权重和网络效率。

在委员会选择方面，Orbit SSF采用基于余额权重的随机性方法，根据每个验证者的余额（上限为ETH_CEIL）调整其入选概率。具体操作包括生成每个验证者的独特随机数，并根据此确定其是否应被纳入委员会。同时，提议仅对委员会成员更新奖励、惩罚及不活动分数，以降低处理开销。

在epoch过程中，为适应Orbit SSF，提议客户端可以预计算下一槽位的随机数，在当前槽位的空闲时段进行，以便顺利进行epoch过渡。

最后，SSF带来的潜在优化可能在于减少与见证相关的网络流量并去除聚合层，从而简化协议并释放更多时间用于区块验证。未来工作需要评估随着验证者集中的变化，这些改进如何影响协议的健壮性和韧性，以及保持经济安全和强最终性保证所需的任何调整。 <div>
<p>I would like to thank <a class="mention" href="https://ethresear.ch/u/fradamt">@fradamt</a> and <a class="mention" href="https://ethresear.ch/u/potuz">@potuz</a> for their review and insights</p>
<p>We propose an approach for implementing the Orbit method for Single Slot Finality (SSF) with minimal adjustments to the base Ethereum protocol. This proposal simplifies committee formation by using a single control parameter, <code>ETH_CEIL</code>, for validator sampling.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51085-introduction-to-orbit-ssf-1" name="p-51085-introduction-to-orbit-ssf-1"></a>Introduction to Orbit SSF</h2>
<p>Orbit SSF aims to achieve single-slot finality (SSF) in the Ethereum network by optimally selecting validators for committee formation. SSF allows Ethereum to finalize blocks within a single slot, significantly enhancing finality speed and network security. As the validator count and concentration of high-balance validators (e.g., from entities like Coinbase or Lido) increase, Orbit offers an efficient and balanced way to sample validators, reducing bandwidth requirements while maintaining economic security and robustness.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51085-orbit-ssf-and-maxeb-2" name="p-51085-orbit-ssf-and-maxeb-2"></a>Orbit SSF and MaxEB</h3>
<p>The protocol operates under the assumption that MAXEB (Maximum Effective Balance) has been implemented, creating a predictable validator landscape. MAXEB establishes a balance cap of 2048 ETH per validator, concentrating economic weight among fewer, larger validators. By leveraging the security of high-balance validators, the protocol achieves robust finality without imposing high network overhead.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51085-idea-of-a-specification-3" name="p-51085-idea-of-a-specification-3"></a>Idea of a Specification</h2>
<p>Our goal is to start thinking about a minimally invasive implementantion of Orbit SSF within the Ethereum 2.0’s consensus framework. Thus, We will focus on modifying:</p>
<ul>
<li>Committee Selection</li>
<li>Attestation Format</li>
</ul>
<p>Additionally, we explore how clients can perform an epoch transition every slot without falling out of sync.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51085-changes-in-committee-selection-4" name="p-51085-changes-in-committee-selection-4"></a>Changes in Committee Selection</h3>
<p>Orbit’s committee selection method deviates from purely random sampling, employing a “weighted-by-balance” randomness approach. This method adjusts validator inclusion probability based on each validator’s balance, capped by <code>ETH_CEIL</code>.</p>
<div class="md-table">
<table>
<thead>
<tr>
<th>Constant</th>
<th>Description</th>
<th>Value/Unit</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>ETH_CEIL</code></td>
<td>Balance ceiling used to calculate validator inclusion probability</td>
<td>2048 ETH</td>
</tr>
</tbody>
</table>
</div><h4><a class="anchor" href="https://ethresear.ch#p-51085-committee-selection-process-5" name="p-51085-committee-selection-process-5"></a>Committee Selection Process</h4>
<ol>
<li>
<p><strong>Generate random numbers for each validator</strong>:<br />
Generate a unique random number for each validator’s inclusion decision. This random number, limited to a 16-bit space, is based on the current epoch’s mix.</p>
<pre><code class="lang-python">def get_random_numbers_for_orbit_sampling(state: State) -&gt; Sequence[uint64]:
    epoch = get_current_epoch(state)
    seed = get_seed(state, epoch, DOMAIN_ORBIT_COMMITTEE)
    expanded_seed = b''.join(
        hash(seed + to_bytes(i))
        for i in range((len(validators) + 15) // 16)
    )
    return [from_bytes(expanded_seed[i:i+2]) for i in range(0, len(validators)*2, 2)]
</code></pre>
</li>
<li>
<p><strong>Calculate inclusion probability based on balance</strong>:<br />
Using <code>ETH_CEIL</code> as the sole parameter, we calculate the probability for each validator to be included in the committee.</p>
<p>By adjusting <code>ETH_CEIL</code>, we control the maximum inclusion probability. A higher <code>ETH_CEIL</code> decreases the probability of validators with balances near the <code>MaxEB</code>cap, whereas a lower <code>ETH_CEIL</code> increases the committee size by increasing the inclusion of validators.</p>
</li>
<li>
<p><strong>Determine committee inclusion</strong>:<br />
A validator’s inclusion is determined by the following inequality:</p>
<div class="math">
\frac{M_{i} \times \text{ETH}_{\text{ceil}}}{\min(\text{ETH}_{\text{ceil}}, \text{validator}_{\text{balance}})} &lt; 2^{16}
</div>
<p>Where:</p>
<ul>
<li><span class="math">M_{i}</span> is the random 64-bit number generated for the validator.</li>
<li><span class="math">2^{16}</span> is the threshold based on the 16-bit space.</li>
<li><span class="math">\text{ETH}_{\text{ceil}}</span> is the balance cap that determines the maximum inclusion probability.</li>
</ul>
<p>If this inequality holds true, the validator is included in the committee; otherwise, they are excluded. Adjusting <code>ETH_CEIL</code> allows us to control the committee size without additional ratio parameters.</p>
</li>
</ol>
<h4><a class="anchor" href="https://ethresear.ch#p-51085-pseudo-code-for-committee-selection-6" name="p-51085-pseudo-code-for-committee-selection-6"></a>Pseudo-code for Committee Selection</h4>
<pre><code class="lang-python"># Constants
ETH_CEIL = 2048 * ETH  # Control parameter for inclusion probability

def calculate_probability_factors(balance):
    numerator = min(ETH_CEIL, balance)
    denominator = ETH_CEIL
    return numerator, denominator

def check_committee_inclusion(rng_u64, numerator, denominator):
    return (rng_u64 * denominator) / numerator &lt; 2**16

def select_committee(state):
    random_numbers = get_random_numbers_for_orbit_sampling(state)
    committee = []
    for i, validator in enumerate(state.validators):
        numerator, denominator = calculate_probability_factors(validator['balance'])
        if check_committee_inclusion(random_numbers[i], numerator, denominator):
            committee.append(validator)
    return committee
</code></pre>
<h3><a class="anchor" href="https://ethresear.ch#p-51085-changes-in-epoch-processing-7" name="p-51085-changes-in-epoch-processing-7"></a>Changes in Epoch Processing</h3>
<h4><a class="anchor" href="https://ethresear.ch#p-51085-update-rewards-penalties-and-inactivity-scores-only-for-committee-members-8" name="p-51085-update-rewards-penalties-and-inactivity-scores-only-for-committee-members-8"></a>Update Rewards, Penalties, and Inactivity Scores Only for Committee Members</h4>
<p>To further optimize, we modify the <code>get_unslashed_participating_indices</code> function to return only validators included in the committee. This ensures rewards, penalties, and inactivity scores apply only to committee members, reducing processing overhead.</p>
<pre><code class="lang-python">def get_unslashed_participating_indices(state: BeaconState, flag_index: int, epoch: Epoch) -&gt; Set[ValidatorIndex]:
    assert epoch in (get_previous_epoch(state), get_current_epoch(state))
    epoch_participation = (
        state.current_epoch_participation if epoch == get_current_epoch(state) else state.previous_epoch_participation
    )
    active_validator_indices = get_active_validator_indices(state, epoch)
    participating_indices = [
        i for i in select_committee(state) if has_flag(epoch_participation[i], flag_index)
    ]
    return set(filter(lambda index: not state.validators[index].slashed, participating_indices))
</code></pre>
<p>With this change, only committee members are processed for inactivity scores, penalties, and rewards. Similarly, only committee members are considered in justification bits and related processes.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-51085-finality-condition-9" name="p-51085-finality-condition-9"></a>Finality Condition</h4>
<p>Finality is achieved when <code>66%</code> of the Orbit committee attests to a slot. In case of non-finality, the protocol falls back to LMD-GHOST.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51085-epoch-transition-every-slot-10" name="p-51085-epoch-transition-every-slot-10"></a>Epoch Transition Every Slot</h3>
<p>In a post-MAXEB world, where the active validator set is expected to decrease significantly, epoch transitions may have reduced overhead. However, generating fresh random numbers each slot remains essential to maintain committee integrity. To achieve this efficiently, clients can precompute random numbers for the next slot during idle periods within the current slot, ensuring they are ready for the transition.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51085-potential-optimization-with-ssf-11" name="p-51085-potential-optimization-with-ssf-11"></a>Potential Optimization with SSF</h3>
<p>SSF allows for protocol simplifications depending on the post-MAXEB validator set structure. as a matter of fact, by sampling only a subset of validators, the protocol can reduce attestation-related traffic and get rid of the aggregation layer. This enables the potential removal of aggregators, streamlining the protocol and freeing up additional slot time for block verification.</p>
<p><strong>NOTE:</strong> at this time this is quite unclear if this is a feasible path.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51085-future-work-12" name="p-51085-future-work-12"></a>Future Work:</h2>
<p>As MAXEB concentrates validator balances, we will need to conduct an analysis to assess how these changes impact the robustness and resilience of the protocol. Such as how, as the validator set consolidates,  how effectively Orbit SSF can maintain economic security and robust finality guarantees. This includes examining any adjustments needed to preserve Ethereum’s security standards.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/orbit-ssf-in-practice/20943">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 07 Nov 2024 14:57:34 +0000</pubDate>
</item>
<item>
<title>Improving connectivity and preventing censorship in networks</title>
<link>https://ethresear.ch/t/improving-connectivity-and-preventing-censorship-in-networks/20942</link>
<guid>https://ethresear.ch/t/improving-connectivity-and-preventing-censorship-in-networks/20942</guid>
<content:encoded><![CDATA[
<div> 关键词: 区块链节点、NAT、火墙、Libp2p、连接策略

总结:
本文提出了一个针对当前区块链节点及P2P应用连通性问题的解决方案。现有的P2P库如Libp2p依赖于分布式中继网络，但这种做法并不利于增强网络资源并实现真正的点对点通信。作者设计了一个系统，支持包括UPnP端口转发、UPnP打洞、TCP直接连接、TCP反向连接、TCP打洞以及UDP TURN代理等多种常见的连接策略，并构建了一个框架用于新的连接策略开发，支持NAT枚举、多接口和改进的服务跨网络寻址设计。

作者已经编写了一个Python 3.6+异步库，并进行了多平台测试，虽仍处于Beta阶段，但提供了简单的文本演示Demo。感兴趣的读者可以通过指定命令尝试运行Demo，并阅读作者的博客文章以获取更多信息。作者正在寻求反馈，以判断是否值得继续推进该项目。 <div>
<p><img alt="demo_small (1)" class="animated" height="437" src="https://ethresear.ch/uploads/default/original/3X/c/0/c0a496d5821b840a46b956450fbab0271213cb35.gif" width="690" /></p>
<p>Imagine if you could easily run a blockchain node and anyone could connect. Whether on a phone, desktop, or even locked-down IoT connection. Imagine that there was no way to filter such connections. Imagine peer-to-peer apps like chat programs that worked effortlessly. Where connections always succeeded.</p>
<p>You can’t do this today because of NATs and firewalls used in the networks most people have at home. But this is an important part of decentralization. If you can improve connectivity in general you can also help bypass censorship. And improved connectivity isn’t just useful for censorship edge-cases. It solves a practical problem experienced by many programs…</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51084-what-options-are-there-to-do-this-already-1" name="p-51084-what-options-are-there-to-do-this-already-1"></a>What options are there to do this already?</h1>
<p>Well, the design favored today by ‘P2P libraries’ like Libp2p is to use a ‘decentralized’ network of relays. The logic behind this is easy to understand. There are many different types of NATs with sub-types of external mapping allocation behaviors. An effective algorithm for punching holes through NATs has to be designed to take into account both sides – which some may consider an unnecessary complexity. But this approach isn’t compatible with the scenarios in the introduction.</p>
<p><strong>If you have to consume relay bandwidth to reach a new node in the network then you’re not strengthening the network’s resources. Likewise, relying on a proxy to do ‘direct’ connections in say – a chat program – is a bit of an oxymoron.</strong></p>
<p>Relays work well as a fallback for worse-case scenarios. But they don’t fundamentally contribute to the technological development of a model to improve connectivity. Perhaps useful if we want to start to think about network censorship or even just useability improvements.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51084-my-approach-to-the-problem-2" name="p-51084-my-approach-to-the-problem-2"></a>My approach to the problem:</h1>
<p>Relay servers are a short-cut and I don’t take shortcuts. I designed a system that supports every common strategy for connectivity. They include all the approaches you’re familiar with and introduce a framework for building new ones. The framework includes comprehensive support for NAT enumeration, multi-interface support, and a unique address design that improves pathing to services across networks. Here is a list of the connectivity strategies supported:</p>
<ul>
<li>
<p>UPnP port forwarding (IPv4)</p>
</li>
<li>
<p>UPnP pin holing (IPv6)</p>
</li>
<li>
<p>TCP direct connect</p>
</li>
<li>
<p>TCP reverse connect</p>
</li>
<li>
<p>TCP hole punching</p>
</li>
<li>
<p>UDP TURN proxy (fallback, non-default)</p>
</li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#p-51084-status-3" name="p-51084-status-3"></a>Status</h1>
<p>Currently I have a Python 3.6 &gt;= async library written. It’s portable (avoids C extensions) and tested on many OS. The software is currently a beta release and may contain bugs.<br />
If you want to try out a simple, text-based demo:</p>
<p>python3 -m pip install p2pd<br />
python3 -m p2pd.demo</p>
<p>Also, here’s my blog post on the project with more information:<br />
<a class="onebox" href="https://roberts.pm/index.php/2024/11/05/p2pd/" rel="noopener nofollow ugc" target="_blank">https://roberts.pm/index.php/2024/11/05/p2pd/</a></p>
<p>Let me know what your thoughts are on the above and whether it’s worth continuing.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/improving-connectivity-and-preventing-censorship-in-networks/20942">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 07 Nov 2024 07:22:57 +0000</pubDate>
</item>
<item>
<title>Proposers do play dice: Introducing random Execution Auctions (randEAs)</title>
<link>https://ethresear.ch/t/proposers-do-play-dice-introducing-random-execution-auctions-randeas/20938</link>
<guid>https://ethresear.ch/t/proposers-do-play-dice-introducing-random-execution-auctions-randeas/20938</guid>
<content:encoded><![CDATA[
<div> 关键词: 执行拍卖（EAs）、最大可提取价值（MEV）、提前分配、多块控制影响（MMEV）、随机执行拍卖（randEAs）

总结:
本文探讨了执行拍卖（EAs）作为实现最大可提取价值（MEV）治理方案的一种候选机制，旨在通过提前分配和区块执行提议权来解决提案者的时间游戏问题以及提议者收益的波动性。然而，EAs可能存在集中化风险，例如导致特定参与者更易获取执行提议权，并可能催生多块控制影响（MMEV）。为了解决MMEV问题，文章提出了随机执行拍卖（randEAs）的新方法。在randEAs中，中标者虽然获得了一定时间段内的执行提议权，但具体的提议槽位是在临近提议前才随机确定并揭晓。randEAs通过这种方式减少了中标者能够提前知晓并控制连续多个区块的可能性。同时，文章还指出了randEAs可能会带来的市场结构变化及不平等现象等问题，需要谨慎考虑实施影响。 <div>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/a/1ad5e515fee1d377e9c25de34aa072469479e79e.jpeg" title="Screenshot 2024-10-09 at 10.25.42"><img alt="Screenshot 2024-10-09 at 10.25.42" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/1/a/1ad5e515fee1d377e9c25de34aa072469479e79e_2_498x500.jpeg" width="498" /></a></div><p></p>
<p><em>by <a href="https://x.com/soispoke" rel="noopener nofollow ugc">Thomas Thiery</a></em> - November 6th, 2024</p>
<p><em>Thanks to <a href="https://x.com/_julianma" rel="noopener nofollow ugc">Julian</a>, <a href="https://x.com/barnabemonnot" rel="noopener nofollow ugc">Barnabé</a>, <a href="https://x.com/weboftrees" rel="noopener nofollow ugc">Anders</a> and <a href="https://x.com/_JonahB_" rel="noopener nofollow ugc">Jonah Burian </a> for discussions, feedback and comments on this post.</em></p>
<h2><a class="anchor" href="https://ethresear.ch#p-51078-introduction-1" name="p-51078-introduction-1"></a>Introduction</h2>
<p><a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ" rel="noopener nofollow ugc">Execution Auctions</a> (EAs) have been proposed as a candidate implementation of <a href="https://youtu.be/MtvbGuBbNqI?si=YXgH0JkF8EZCMvPg" rel="noopener nofollow ugc">Attester-Proposer Separation</a> (APS). EAs aim to prevent the negative externalities of Maximal Extractable Value (MEV) by further <a href="https://collective.flashbots.net/t/isolating-attesters-from-mev/3837" rel="noopener nofollow ugc">separating the roles of attesting/validating</a> and block execution proposing. Specifically, EAs address two key issues: the incentive to engage in beacon proposer timing games and the variance in execution proposer payoffs.</p>
<p>By auctioning the right to propose execution payloads well in advance (e.g., 32 slots ahead), EAs potentially mitigate timing games by removing the immediate incentive to delay block proposals. Additionally, by burning the value of the winning bid, EAs could reduce the variance in proposer payoffs and help <a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590">improve both micro and macro consensus stability</a>. In essence, EAs involve the protocol running an auction to sell future execution proposing rights, with attesters monitoring bids to ensure the highest bid is selected and the proceeds are <a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590">burned</a>.</p>
<p>The simplicity of EAs and some of their <a href="https://ethresear.ch/t/execution-auctions-as-an-alternative-to-execution-tickets/19894">economic properties</a> make them a potential candidate for implementation in Ethereum. By running auctions to allocate proposing rights, EAs (1) effectively separate block execution proposing from attesting/validating, (2) enforce a new competition for every slot, and (3) aim to resolve the “value-in-flight problem” by removing reliance on time-sensitive block value fluctuations (e.g., from CEX-DEX arbitrages).</p>
<p>However, concerns have been raised regarding the centralizing forces inherent in Execution Auctions (EAs) and other Attester-Proposer Separation (APS) designs. These concerns specifically include:</p>
<ul>
<li>Moving from relying on ex post to ex ante MEV valuations for winning execution proposing rights can influence which actors are able to secure these rights (see papers from <a href="https://arxiv.org/abs/2408.11255" rel="noopener nofollow ugc">Burian, Crapis, Saleh</a> and <a href="https://arxiv.org/abs/2408.03116" rel="noopener nofollow ugc">Pai and Resnick</a>).</li>
<li>Allowing proposers to know in advance if they have won proposing rights for multiple consecutive slots may lead to the emergence of toxic <a href="https://ethresear.ch/t/does-multi-block-mev-exist-analysis-of-2-years-of-mev-data/20345">multi-block MEV</a> (MMEV), where a single actor can exert excessive control over multiple blocks in succession.</li>
</ul>
<p>Importantly, randEAs solely focus on proposing a candidate design to mitigate problems related to MMEV. One potential solution to mitigate MMEV is to introduce a second auction—conducted by the execution proposer (i.e., the winner of the initial auction)—just in time (e.g., during slot <code>n+32</code>). In this second auction, the right to insert the first set of transactions—the Top-of-Block—would be sold to a party potentially different from the winner of the first auction (see <a href="https://x.com/barnabemonnot/status/1808444733305258047" rel="noopener nofollow ugc">Barnabé’s thread</a> and <a href="https://ethresear.ch/t/mechan-stein-alt-franken-ism/20321">Mike’s post</a>). Introducing this additional auction during the slot eliminates the certainty of controlling the full content of multiple blocks in a row ahead of time, thus mitigating MMEV. Additionally, inclusion list (IL) designs like <a href="https://ethresear.ch/t/fork-choice-enforced-inclusion-lists-focil-a-simple-committee-based-inclusion-list-proposal/19870">FOCIL</a>, which require the execution proposer to include transactions from multiple parties (e.g., IL committee members) in the block, could further alleviate some concerns related to MMEV by adding constraints to the execution proposer.</p>
<p>In this post, we propose an alternative way to address MMEV concerns without the need for an additional auction. In randEAs, similar to <a href="https://arxiv.org/abs/2408.11255" rel="noopener nofollow ugc">execution tickets</a> (ETs), winning the initial auction guarantees execution proposing rights within a specific slot window (e.g., from slot <code>n+16</code> to <code>n+32</code>), but the exact slot for which the proposing rights are won is only <strong>assigned and revealed one slot in advance</strong>. However, unlike ETs, the mechanism to assign execution proposing rights is an auction. This simple approach allows participants to secure future execution proposing rights if they win the auction without knowing exactly if or when they would control multiple consecutive slots.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51078-design-2" name="p-51078-design-2"></a>Design</h2>
<p>We begin with a high-level overview of the randEAs design (see <strong>Figure 1</strong>), intentionally omitting some implementation details for simplicity.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/5/c55ca434612e45eedafd29681755b1f276ce871f.png" title="Sept 23 Screenshot from Notion"><img alt="Sept 23 Screenshot from Notion" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/c/5/c55ca434612e45eedafd29681755b1f276ce871f_2_689x500.png" width="689" /></a></div><p></p>
<ol>
<li>
<p><strong>Auctioning Execution Proposing Rights</strong>: During slot <code>n</code>, the beacon proposer conducts an auction for execution proposing rights within a future slot window (e.g., slots <code>n+16</code> to <code>n+32</code>). Participants submit bids to acquire the right to propose an execution payload in one of these future slots. Attesters of slot <code>n</code> monitor these bids and ensure that the beacon proposer commits to the highest bid (e.g., <code>0.2 ETH</code>) and the execution proposing right winner’s associated public key (e.g., execution proposer <code>0xA12</code>). They will only attest to the beacon block if the committed bid matches the highest bid they have seen, using a <a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590">MEV burn</a> like mechanism.</p>
</li>
<li>
<p><strong>Revealing the Exact Execution Slot</strong>: The exact slot assignment is revealed by the beacon proposer right before the execution slot occurs. For example, during beacon slot <code>n+22</code>, the beacon proposer discloses that the winner of the auction in slot <code>n</code>, Execution Proposer <code>0xA12</code>, will have to provide the execution payload for execution slot <code>n+22</code>. This means that although the auction winner knows they will propose within the window <code>n+16</code> to <code>n+32</code>, they only learn their specific execution slot (e.g., <code>n+22</code>) right before they have to propose.</p>
</li>
</ol>
<p>By randomizing the exact slot assignment within a known window and revealing it just in time, randEAs prevent proposers from knowing far in advance whether they will control multiple consecutive slots.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51078-beacon-proposer-slot-assignment-mechanism-3" name="p-51078-beacon-proposer-slot-assignment-mechanism-3"></a><strong>Beacon Proposer Slot Assignment Mechanism</strong></h3>
<p>To ensure that beacon proposers exclusively and securely assign execution slots to winners, we incorporate a mechanism inspired by <a href="https://github.com/ethereum/consensus-specs/blob/a09d0c321550c5411557674a981e2b444a1178c0/specs/phase0/validator.md#aggregation-selection" rel="noopener nofollow ugc">**attestation aggregation</a>.** We use cryptographic signatures and deterministic selection functions to maintain randomness and fairness in execution slot assignments.</p>
<p><strong>1. Key Components</strong></p>
<ul>
<li><strong>Execution Proposer Pool</strong>: A list of execution proposers who have won the initial randEA auction for a specific slot window.</li>
<li><strong>Slot Signature</strong>: A unique BLS signature generated by the beacon proposer for each execution slot to introduce randomness.</li>
<li><strong>Selection Function</strong>: A deterministic method that uses the slot signature to select an execution proposer from the pool.</li>
</ul>
<p><strong>2. Generating Slot Signatures</strong></p>
<p>For each execution slot within the auction window (e.g., <code>n+16</code> to <code>n+32</code>), the beacon proposer of that specific slot generates a unique <strong>slot signature</strong>. This signature serves as a cryptographic source of randomness for selecting the execution proposer for that specific slot.</p>
<pre><code class="lang-python">def get_slot_signature(state: BeaconState, slot: Slot, privkey: int) -&gt; BLSSignature:
    domain = get_domain(state, DOMAIN_EXECUTION_SLOT_SELECTION, compute_epoch_at_slot(slot))
    signing_root = compute_signing_root(slot, domain)
    return bls.Sign(privkey, signing_root)
</code></pre>
<ul>
<li><strong>Function Breakdown</strong>:
<ul>
<li><strong><code>get_domain</code></strong>: Defines the context for slot selection.</li>
<li><strong><code>compute_signing_root</code></strong>: Combines slot and domain to create a unique message for signing.</li>
<li><strong><code>bls.Sign</code></strong>: Generates the BLS signature using the Beacon Proposer’s private key.</li>
</ul>
</li>
</ul>
<p><strong>3. Selecting an Execution Proposer</strong></p>
<p>Using the generated <strong>slot signature</strong>, the beacon proposer of slot <code>n+22</code> deterministically selects an <strong>Execution Proposer</strong> from the <strong>Execution Proposer Pool</strong> for execution slot <code>n+22</code>.</p>
<pre><code class="lang-python">def select_execution_proposer(proposer_pool: List[str], slot_signature: BLSSignature) -&gt; str:
    signature_bytes = bls.Signature.to_bytes(slot_signature)
    hashed_signature = hashlib.sha256(signature_bytes).digest()
    hashed_int = int.from_bytes(hashed_signature[:8], 'big')
    proposer_index = hashed_int % len(proposer_pool)
    selected_proposer = proposer_pool[proposer_index]
    return selected_proposer
</code></pre>
<p>This function <strong>(1)</strong> converts the slot signature into a hash using SHA-256 to ensure unpredictability, <strong>(2)</strong> maps the hashed value to an index within the Execution Proposer Pool, and <strong>(3)</strong> chooses the proposer at the determined index, ensuring each proposer has an equal chance of being selected.</p>
<p><strong>4. Delayed Reveal</strong></p>
<p>The selected execution proposer for a specific slot is revealed during the beacon slot <strong>immediately preceding</strong> to the execution slot.</p>
<ul>
<li><strong>Example</strong>: For execution slot <code>n+22</code>, the assignment is revealed during beacon slot <code>n+22</code> by the beacon proposer of slot <code>n+22</code>.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-51078-model-4" name="p-51078-model-4"></a><strong>Model</strong></h2>
<p>To demonstrate how randEAs could work, we present a simple model based on the following assumptions:</p>
<ul>
<li><strong>Slot Windows</strong>: Each proposer is assigned a slot from a predefined window of slots.</li>
<li><strong>Random, Non-Reassignable Slots</strong>: Slots within the window are assigned randomly, and once assigned, a slot cannot be taken by another proposer.</li>
<li><strong>Window Progression</strong>: Once all slots in a window are assigned, the process moves to the next window.</li>
<li><strong>Delayed Slot Reveal</strong>: Although execution proposers are pre-assigned slots, the exact slot is revealed only one slot before they are scheduled to propose.</li>
</ul>
<p>We define the following variables:</p>
<ul>
<li><span class="math">W</span>: Slot window size (e.g., <span class="math">W = 16</span>). Note that the size of the window is a parameter that should be carefully set after conducting further analyses.</li>
<li><span class="math">S_i</span>: Slot assigned to execution proposer <span class="math">i</span>.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51078-slot-window-5" name="p-51078-slot-window-5"></a><strong>Slot window</strong></h3>
<p>Each execution proposer <span class="math">i</span> is assigned a slot from the following window:</p>
<p><span class="math">
W_i = \{ \text{slot}_{n+16}, \dots, \text{slot}_{n+15+W} \}
</span></p>
<p>Execution proposer $is slot is chosen randomly from the available slots in <span class="math">W_i</span>.</p>
<p>The slot assignment follows these rules:</p>
<ul>
<li>
<p><strong>Random Selection</strong>: A slot is randomly allocated to execution proposer <span class="math">i</span> from <span class="math">W_i</span>, and once assigned, it cannot be reassigned.</p>
</li>
<li>
<p><strong>Window Progression</strong>: Once all slots in <span class="math">W_n</span> are assigned, the next window,<br />
<span class="math">W_{i+W} = \{ \text{slot}_{n+32}, \dots, \text{slot}_{i+31+W} \},</span> is opened for the next set of execution proposers.</p>
</li>
<li>
<p><strong>Delayed Slot Reveal:</strong> Although execution proposer <span class="math">i</span> has a slot assigned within their window, the exact slot  <span class="math">S_i</span> is revealed to them is revealed to them only one slot before they are scheduled to propose.</p>
</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51078-probability-of-knowing-the-exact-slot-6" name="p-51078-probability-of-knowing-the-exact-slot-6"></a>Probability of Knowing the Exact Slot</h3>
<p>As more slots within a window are assigned, the probability that an execution proposer knows their exact slot increases because they can observe which slots have already been allocated. This probability reaches certainty when only one slot remains unassigned (see <strong>Figure 2</strong>).</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/6/16eb5fe32bebf09ff9d5e96f929ba24cee2c58c4.png" title="Slot Allocation Mechanism output (1) (1)"><img alt="Slot Allocation Mechanism output (1) (1)" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/1/6/16eb5fe32bebf09ff9d5e96f929ba24cee2c58c4_2_601x500.png" width="601" /></a></div><p></p>
<p>Let <span class="math">k</span> represent the number of slots already allocated in proposer $is window <span class="math">W_i</span>. The probability that proposer <span class="math">n</span> knows their exact slot prior to the final reveal is:</p>
<div class="math">
P_i(k) = \frac{1}{W - k}
</div>
<p>Here’s why:</p>
<p><strong>Total Remaining Slots:</strong> There are <span class="math">W - k</span> unassigned slots remaining in <span class="math">W_i</span>:</p>
<div class="math">
\text{Total Remaining Slots} = W - k
</div>
<p><strong>Equal Probability:</strong> Since slots are assigned randomly, each of the remaining slots has an equal chance of being assigned to proposer <span class="math">n</span>.</p>
<p><strong>Certainty with One Slot Left:</strong> When only one slot remains unassigned <span class="math">W - k = 1</span>, the probability becomes:</p>
<div class="math">
P_i(W - 1) = \frac{1}{W - (W - 1)} = \frac{1}{1} = 1
</div>
<p>As the window fills up, the probability of knowing the exact slot increases, ultimately reaching certainty when only one slot remains unassigned.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51078-pros-7" name="p-51078-pros-7"></a>Pros</h2>
<ul>
<li>
<p><strong>Simplicity</strong>: Employing slot windows with random slot allocation, instead of introducing a second <a href="https://hackmd.io/@mikeneuder/mechan-stein" rel="noopener nofollow ugc">just-in-time (JIT) auction</a>, simplifies implementation and requires fewer resources from protocol actors like attesters, who would otherwise need to ensure that the additional auction is conducted properly and that participants adhere to the rules.</p>
</li>
<li>
<p>Random EAs effectively differentiate between <strong>knowing that you will propose</strong> and <strong>knowing the exact slot in which you will propose</strong>. This allows proposers to leverage the certainty of having won the right to propose in the near future to price the value of winning the auction while limiting potential abuses  that could arise from knowing the exact slots they will control well in advance (e.g., MMEV).</p>
</li>
<li>
<p><strong>Reduced edge for sophisticated bidders</strong>: Not knowing the exact slot you will control may also reduce the edge that sophisticated parties gain when bidding during the execution proposing rights auction. For example, if a token or NFT is scheduled to launch at a specific slot, the uncertainty in slot allocation might prevent sophisticated actors from exploiting this information to extract more MEV by pricing the expected block value ahead of time.</p>
</li>
<li>
<p>Random EAs allow execution proposers to provide both short-lived (lookahead of 1) and long-lived (guaranteed to get the execution proposing rights in a given slot window) <strong>preconfirmations</strong>. We could also envision proposers who have won the right to propose within a given time window coordinating off-chain to offer better guarantees to users.</p>
</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-51078-things-to-look-out-for-8" name="p-51078-things-to-look-out-for-8"></a>Things to look out for</h2>
<ul>
<li>
<p>The current market structure, which employs out-of-protocol Proposer-Builder Separation (PBS), tends to centralize block proposals among a few actors with the highest ex post MEV (Maximal Extractable Value) extraction abilities. This concentration leads to significant variability and results in an oligopoly, where only two builders are responsible for producing more than 85% of all blocks. In contrast, random Execution Auctions (randEAs), similar to Execution Tickets (ETs), may shift this concentration toward actors with the highest ex ante value for MEV extraction (see <a href="https://arxiv.org/abs/2408.03116" rel="noopener nofollow ugc">Pai and Resnick, 2024</a>). Under randEAs, the winners of execution proposing rights might have limited MEV extraction capabilities themselves and instead outsource block construction. This outsourcing causes execution proposing rights to concentrate among those with the lowest capital costs. If capital costs are uniform across participants, the concentration instead favors those with superior MEV extraction abilities. In more realistic scenarios, when both capital costs and MEV extraction abilities vary among participants, the interplay between these two factors might lead to further concentration, potentially resulting in a monopoly (see <a href="https://arxiv.org/abs/2408.11255" rel="noopener nofollow ugc">Burian, Crapis, Saleh, 2024</a>). This shift in market structure could cause greater centralization than the current PBS system, and the possibility of increased concentration with randEAs must be carefully considered.</p>
</li>
<li>
<p>If slot windows are relatively small (e.g., 16 slots), random EAs may introduce some inequality among proposers within a given window. Proposers assigned to slots towards the end of the window have a higher probability of knowing their exact slot in advance, as more slots are allocated and revealed. Conversely, an infinitely long time window would provide a memoryless property (i.e., the probability of knowing the exact slot remains constant), but it would make pricing highly complex: Without a bounded window, assigning a fair value to execution proposing rights is inherently difficult, as there are no natural limits or constraints to guide pricing decisions.</p>
</li>
<li>
<p>Bidders that win multiple auctions gain an increased ability to predict the exact slots they will propose, especially as the number of unassigned slots decreases and if their own slots haven’t been assigned yet. For example, if a single party has won three auctions in a row and their slots have not been assigned while only three slots remain unassigned in the window, they are certain to be allocated those three remaining slots. This certainty allows them to control multiple consecutive slots, potentially giving them an unfair advantage and enabling them to exploit MMEV opportunities.</p>
</li>
</ul>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/proposers-do-play-dice-introducing-random-execution-auctions-randeas/20938">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 06 Nov 2024 14:58:56 +0000</pubDate>
</item>
<item>
<title>PeerDAS with significantly less bandwidth consumption</title>
<link>https://ethresear.ch/t/peerdas-with-significantly-less-bandwidth-consumption/20932</link>
<guid>https://ethresear.ch/t/peerdas-with-significantly-less-bandwidth-consumption/20932</guid>
<content:encoded><![CDATA[
<div> 关键词: PeerDAS、GossipSub、带宽消耗、IDONTWANT、订阅策略

总结:
本文提出了一种改进方案，旨在降低PeerDAS的带宽消耗。当前方案中，GossipSub协议导致了带宽的放大因子浪费，因为多个节点可能向单个节点发送相同的列。此前已实现IDONTWANT机制来减少接收副本的数量，但不能确保确切数量。新提案将每个节点订阅的GossipSub主题数从S减至K（如设为2），从而减少了带宽消耗。虽然仍需要订阅一定数量的主题以保证稳定性，但节点仅需从其余未订阅的主题中获取S-K列。通过观察并请求其他节点的通知来获取缺失的列，利用超时机制选择下一个源节点，从而有效地减少副本的接收。新设计下，带宽消耗从原来的64C降至22C，减少了65.6%。此外，该方案与IDONTWANT机制结合使用可进一步节省带宽，同时与“对等体采样”方法相比，本方案在请求列的时间和条件上具有不同优势。 <div>
<p><em>Authors: <a href="https://github.com/ppopth" rel="noopener nofollow ugc">pop</a></em></p>
<p><em>tldr; this proposal reduces the bandwidth consumption of PeerDAS by 65.6%</em></p>
<ul>
<li><em>Since this proposal is an improvement to PeerDAS. <a href="https://github.com/ethereum/consensus-specs/tree/dev/specs/_features/eip7594" rel="noopener nofollow ugc">Familiarity with PeerDAS</a> is required.</em></li>
<li><em><a href="https://ethresear.ch/t/gossipsub-topic-observation-proposed-gossipsub-1-3/20907">Topic observation</a> is a building block of this design so it would be helpful if you read it first.</em></li>
</ul>
<p>Currently GossipSub imposes an amplification factor on the bandwidth consumption to PeerDAS, since more than one peers can send you the same columns. In fact, you need only one copy, so this amplification wastes your bandwidth.</p>
<p>Previously we have <code>IDONTWANT</code> implemented which reduces the number of copies you will receive, but it doesn’t guarantee exactly how many.</p>
<p>This proposal enables nodes to receive only one copy of most of their sampled columns.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51060-current-bandwidth-consumption-1" name="p-51060-current-bandwidth-consumption-1"></a>Current bandwidth consumption</h1>
<p><em>(For simplicity, let’s assume that <code>DATA_COLUMN_SIDECAR_SUBNET_COUNT</code> and <code>NUMBER_OF_CUSTODY_GROUPS</code> are equal to <code>NUMBER_OF_COLUMNS</code>)</em></p>
<p>Let <span class="math">S</span> be <code>SAMPLES_PER_SLOT</code>, <span class="math">C</span> be the size of a column, and <span class="math">D</span> be the amplification factor of GossipSub (aka the mesh degree).</p>
<p>Nodes are required to subscribe to and sample <span class="math">S</span> columns, so each node has to consume the bandwidth about <span class="math">D*S*C</span> bytes per slot.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51060-new-design-2" name="p-51060-new-design-2"></a>New design</h1>
<p>Previously, we have each node subscribe to <span class="math">S</span> GossipSub topics. Now, we subscribe to fewer topics than that. We have each node subscribe to <span class="math">K=2</span> topics which is lower than <span class="math">S</span>. Nodes will still receive or forward <span class="math">D</span> copies in these <span class="math">K</span> topics, but they will receive only one copy and forward no copy for the remaining <span class="math">S-K</span> topics.</p>
<p>The reason that we still need to subscribe to <span class="math">K</span> topics is because we need to provide backbones for the topics as required by <a href="https://ethresear.ch/t/gossipsub-topic-observation-proposed-gossipsub-1-3/20907">topic observations</a> (aka stability of the topics).</p>
<p>The bandwidth consumption of <span class="math">K</span> subscriptions is <span class="math">D*K*C</span> bytes per slot.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/a/1aafa726959d5acdd03d61f0201e9c2e0effdaf9.png" title="slot-breakdown"><img alt="slot-breakdown" height="210" src="https://ethresear.ch/uploads/default/optimized/3X/1/a/1aafa726959d5acdd03d61f0201e9c2e0effdaf9_2_690x210.png" width="690" /></a></div><p></p>
<p>Now, the remaining question is how the node can get the remaining <span class="math">S-K</span> columns that it needs.</p>
<p>Firstly, you start observing the topic at the beginning of the slot (shown as a blue line).</p>
<p>After that, your peers will notify you when there is a new message in the topic. Orange lines show when your peers notify you. Notice that peer 2 is the first one who gets the message (column) and notifies you first.</p>
<p>Since peer 2 notifies you first, you request the actual column from peer 2 with the timeout <span class="math">T</span> (400 ms). After the timeout, if you don’t get it from peer 2, you request it from the peer that notifies you second which is peer 4. If you still don’t get it, you keep going on. Red lines show when you request the column from each peer. The further lines are lighter to indicate that it’s less probable. Consecutive lines are 400ms apart indicating the timeout.</p>
<p>It looks like timeouts will delay the column reception a lot because with the current PeerDAS you will get the column right at the orange lines which are faster. In fact, it’s not that bad for the following reasons.</p>
<ol>
<li>It saves a lot of bandwidth. Imagine that you get a copy of the column at each orange line. That looks very wasteful. With this proposal, you get only one copy at one of the red lines.</li>
<li>Timeouts are rare. You don’t expect to get many timeouts for the following reasons.
<ul>
<li>The network condition is already good. If not, how could your peer notify you that it gets a message?. If you could notify me, so you could also send me the column. If it doesn’t, you can probably de-score it.</li>
<li>Your peer can send you an early rejection without waiting for the timeout. For instance, if your peer is overloaded and doesn’t want to waste the bandwidth sending you the column, it can just send a rejection to you and you can move forward to another peer quickly.</li>
</ul>
</li>
</ol>
<h1><a class="anchor" href="https://ethresear.ch#p-51060-new-bandwidth-consumption-3" name="p-51060-new-bandwidth-consumption-3"></a>New bandwidth consumption</h1>
<ul>
<li>The bandwidth consumption due to subscribing to <span class="math">K</span> topics is <span class="math">D*K*C</span> bytes per slot.</li>
<li>The bandwdith consumption due to observing and downloading the remaining <span class="math">S-K</span> columns is <span class="math">(S-K)*C</span> bytes per slot.</li>
</ul>
<p>The total bandwidth consumption would be <span class="math">(D*K+S-K)*C</span> bytes per slot.</p>
<p>Assign the parameters with the current assignments in the spec: <span class="math">D = 8</span>, <span class="math">K = 2</span>, and <span class="math">S = 8</span>.</p>
<ul>
<li>The bandwidth consumption of the current PeerDAS is <span class="math">64*C</span>.</li>
<li>The bandwidth consumption of the new one is <span class="math">22*C</span> which is 65.6% reduction.</li>
</ul>
<p>The reason I assign <span class="math">K=2</span> is because, with 8k nodes and the number of columns of 128, there will be at least 100 nodes in each topic.</p>
<p>Pessimistically, if you think <span class="math">K=2</span> doesn’t make the topics stable enough, we can go to <span class="math">K=4</span> and the bandwidth consumption would be <span class="math">36*C</span> which is still 43.8% reduction.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51060-comparison-to-idontwant-4" name="p-51060-comparison-to-idontwant-4"></a>Comparison to IDONTWANT</h1>
<p>You can note that the analysis in the previous sections assumes that you will receive or forward exactly <span class="math">D</span> copies of messages when subscribing to topics.</p>
<p>This is not true with <code>IDONTWANT</code> since it can reduce the number of copies you will receive by sending <code>IDONTWANT</code> to your peers before they send you a copy.</p>
<p>There is a still corner case that <code>IDONTWANT</code> doesn’t help reduce the number of copies at all. Imagine that all of your peers send you the message at the same time (the orange lines are very close to each other), so you don’t have a chance to send <code>IDONTWANT</code> to them in time. So, in this case, you still receive the same number of copies as before. While in this proposal, it’s guaranteed that you will receive only one copy.</p>
<p>However, we can combine this proposal with <code>IDONTWANT</code> to get an even better protocol. Since nodes still subscribe to <span class="math">K</span> topics. <code>IDONTWANT</code> can reduce a lot of bandwidth consumption there.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51060-comparison-to-peer-sampling-5" name="p-51060-comparison-to-peer-sampling-5"></a>Comparison to Peer Sampling</h1>
<p>Peer sampling is a process that after all the columns are disseminated through the network, you request a column from your peer that’s supposed to have it. If you get the column back, it means that column is available. If not, you may either request another peer or decide that the column is not available.</p>
<p>You can see that you always request for a column no matter what which is different from this proposal. In this proposal, you will request a column only if your peer notifies you that it has one. So peer sampling and this proposal are fundamentally different.</p>
<p>Another difference is, in peer sampling, you aren’t sure when to request a column. In other words, you don’t know when the column dissemination is finished so that you can start requesting the column. What you can do is to set an exact timestamp that you will assume the dissemination is already finished and start requesting. This sometimes waste you some time since the dissemination is finished far before the timestamp. In this proposal, you don’t get this problem since you’re notified when you can request.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/peerdas-with-significantly-less-bandwidth-consumption/20932">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 19:33:20 +0000</pubDate>
</item>
<item>
<title>Economic Implications of a Competitive Blob Market</title>
<link>https://ethresear.ch/t/economic-implications-of-a-competitive-blob-market/20931</link>
<guid>https://ethresear.ch/t/economic-implications-of-a-competitive-blob-market/20931</guid>
<content:encoded><![CDATA[
<div> 关键词：blob市场、竞争、费用、利润边际、L2扩容

总结:<br />
随着最近blob市场的竞争加剧，其交易费用持续超过目标计数，标志着从低竞争环境向活跃竞争阶段的转变。这可能导致更高的费用，并需要更具有竞争力的blob发布策略以减轻fee slippage的影响。目前，priority费用仅占总费用的11.8%，而rollups通过转向blob市场已获得了显著的利润提升，但未来市场竞争将可能压缩这些利润边际。为应对竞争，rollups可能需要支付更高优先级费用，这将增加构建者和验证者的收入。此外，blob市场的扩容，如增大最大blob数量，可能会进一步影响收入分布与市场动态。然而，如何平衡竞争和盈利能力仍然是未来的挑战。同时，文章提出了对新进入的rollup经济模型、EIP-7762提案对blob市场经济学影响以及除提高优先级费用和预确认之外，L2还可以采取哪些其他方法来优化其blob包含流程和成本结构等开放性问题的探讨。 <div>
<h1><a class="anchor" href="https://ethresear.ch#p-51058-intro-1" name="p-51058-intro-1"></a>Intro</h1>
<p>Until recently, the blob market has seen minimal competition, with fees hovering near the base minimum since its inception, interrupted only several times briefly by blob market volatility. However, this past week marked a shift: the blob market has consistently exceeded the target blob count, signaling a tipping point toward prolonged competition.</p>
<p>This shift transitions the blob market from its initial bootstrapping phase to an arena of active competition. We briefly saw the impact at peak blob volatility: inclusion times spiked to over 10 minutes, with rollup costs surging past 15 ETH due to blob slippage. <a href="https://mirror.xyz/preconf.eth/xX2wu_3DC76qVYy1GHi1WVeOV2cXDF_rtDh7GlD6ZEU" rel="noopener nofollow ugc">Blob slippage</a> occurs when the blob fee paid exceeds the fee that would have applied if the blob had been included sooner, at a lower rate. The below chart shows thelayer zero airdrop caused severe blob market congestion.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/3/1326a1281287785a2caf0035248d50711625c910.jpeg" title="image"><img alt="image" height="345" src="https://ethresear.ch/uploads/default/optimized/3X/1/3/1326a1281287785a2caf0035248d50711625c910_2_690x345.jpeg" width="690" /></a></div><p></p>
<p>This shift from low competition to a competitive environment not only signals potentially higher fees  for builders and providers, but also requires more competitive blob posting strategies to mitigate blob fee slippage. These strategies include increasing priority fees or <a href="https://mirror.xyz/preconf.eth/cxUO8pPBfqnqAlzFUzoEUa6sgnr68DRmsNhBWPb2u-c" rel="noopener nofollow ugc">utilizing preconfirmations</a> to ensure timely blob inclusion.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51058-blob-market-fees-2" name="p-51058-blob-market-fees-2"></a>Blob Market Fees</h1>
<p>Builders and Validators profit from blobs only through priority fees, as base fees for execution and consensus are burned. Since Dencun, the blob market has generated approximately $11 million in fees. Priority fees account for only 11.8% of total fees, around $1.3 million cumulative fees. To put this number in perspective, MEV activity generated $2.4 million in tipped fees in the past 7 days, according to <a href="https://libmev.com/?presetRange=%227D%22" rel="noopener nofollow ugc">libMEV</a>. This comparison highlights that fees generated by the blob market are still nascent to builder and validator revenues.</p>
<p>The chart below shows the cumulative fees generated in the blob market.<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/9/c956fe4bf5e9cf5e83a5e557426a1ecaeec4bc2f.png" title="image"><img alt="image" height="210" src="https://ethresear.ch/uploads/default/optimized/3X/c/9/c956fe4bf5e9cf5e83a5e557426a1ecaeec4bc2f_2_690x210.png" width="690" /></a></div><p></p>
<p>The majority of fees, ~89% have come from the base execution and consensus fees, which are burned. As blob market fees influence builder and validator revenue, they also affect the profit margins of major market users, especially L2s.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51058-l2-margins-3" name="p-51058-l2-margins-3"></a>L2 Margins</h1>
<p>The largest Blob market users have been rollups. Transitioning from calldata to the blob market has had an overwhelmingly positive effect on L2 profit margins. The largest L2s have collectively profited $ 19.8 million from switching to the blob market.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/0/f080ee7a775a70e5e82ef3802ccf89041233278d.jpeg" title="image"><img alt="image" height="222" src="https://ethresear.ch/uploads/default/optimized/3X/f/0/f080ee7a775a70e5e82ef3802ccf89041233278d_2_690x222.jpeg" width="690" /></a></div><br />
The chart below shows that immediately after Dencun, some rollup margins shot up to approximately 99% profit margins with all the rollups in the dataset seeing over 70% profit margins in the past 3 months alone.<p></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/0/f00284c3f86af8739714bcaead91fd7d046e09de.jpeg" title="image"><img alt="image" height="437" src="https://ethresear.ch/uploads/default/optimized/3X/f/0/f00284c3f86af8739714bcaead91fd7d046e09de_2_690x437.jpeg" width="690" /></a></div><p></p>
<p>In competitive markets, such high profit margins are rare. As more rollups enter the blob market, competition will intensify, compelling rollups to adopt more competitive inclusion strategies to defend their profit margins by securing blob space. This heightened competition is expected to drive down profit margins and generated increased blob market fees.</p>
<p>In contrast, Alphabet’s <a href="https://www.alphaspread.com/security/nasdaq/googl/earnings-calls/q2-2024" rel="noopener nofollow ugc">operating margin is ‘only’ 32%</a>. <a href="https://www.fool.com/investing/2024/08/25/3-things-investors-interested-in-alphabet-need-to/" rel="noopener nofollow ugc">Operating margins</a> for AWS was 36% and Microsoft Azure was 45% in 2024.</p>
<p>If rollups paid 20% of their historical margins in the form of a higher priority fee, that would have contributed $3.96 million in additional priority fees, increasing builder and validator revenue by 110% while still keeping L2 margins above 50%. This would bring the <a href="https://blobs.primev.xyz/dashboard" rel="noopener nofollow ugc">average priority fee</a> up from 2.8 gwei to 3.36 gwei.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51058-blob-market-expansion-4" name="p-51058-blob-market-expansion-4"></a>Blob Market Expansion</h1>
<p>Efforts to expand the blob market include increasing the max blob count. Based on the current fee breakdown, we can project how a larger blob market—such as an increase from 6 to 12 blobs—might impact revenue. Since 50% of blob market fees come from the execution base fee, which remains steady regardless of congestion, revenue from execution fees could rise from $6 million to $12 million if gas usage stays consistent.</p>
<p>However, the consensus blob base fee and the priority fee, which are more volatile, may behave differently. With the blob base fee remaining at minimal levels for most of the year, it’s likely at its lowest point. Furthermore, <a href="https://ethereum-magicians.org/t/eip-7762-increase-min-base-fee-per-blob-gas/20949/6" rel="noopener nofollow ugc">EIP 7762</a> is advocating for a higher minimum base fee, and increased participation from L2s could drive blob fees higher. Already, the blob market <a href="https://x.com/hildobby_/status/1851992944208941323" rel="noopener nofollow ugc">has begun approaching</a> a new equilibrium above the minimum fee.</p>
<p>In a very optimistic scenario where the blob market increases to 12 blobs, blobs pay higher priority fees, and there is a higher minimum fee set, we might see blob market fees increase from ~11m to $25 million.</p>
<p>With these factors in play, the future of the blob market points toward greater scalability, yet with possible challenges in balancing competition and profitability.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51058-data-5" name="p-51058-data-5"></a>Data</h2>
<p>Data was taken from queries used to create the <a href="https://dune.com/glxyresearch_team/eip-4844-blobs" rel="noopener nofollow ugc">blob dashboard</a> by the Galaxy Research team and <a href="https://github.com/paradigmxyz/spice" rel="noopener nofollow ugc">dune-spice</a> by <a href="https://x.com/notnotstorm" rel="noopener nofollow ugc">Storm</a> from Paradigm . The notebook used to create the charts can be found <a href="https://github.com/Evan-Kim2028/spice_queries/blob/35b6f7e94c58a9e00e13592a7f1a3e65f8a7d294/blob_market_economics.ipynb" rel="noopener nofollow ugc">here</a>.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51058-open-questions-6" name="p-51058-open-questions-6"></a>Open Questions</h2>
<ul>
<li>Would validators be more motivated to increase <a href="https://ethresear.ch/t/blob-eips-and-minimum-validator-requirements/20679">their requirements</a> if they were better compensated? For example, if rollups paid 20% higher priority fees in a competitive market, historical priority fees would have increased from $1.3 million to $3.96 million.</li>
<li>What do the rollup economics such as profit margins look like for newer rollups such as Taiko?</li>
<li>What would <a href="https://ethereum-magicians.org/t/eip-7762-increase-min-base-fee-per-blob-gas/20949/6" rel="noopener nofollow ugc">increasing the min blob fee</a> (EIP 7762) have on blob market economics? Would it create an unfair advantage for rollups that currently have 99% profit margins over others?</li>
<li>How effectively can preconfirmations mitigate blob fee slippage compared to priority fees?</li>
<li>Beyond increasing priority fees and preconfirmations, what other methods can L2s employ to optimize their blob inclusion processes and cost structures?</li>
</ul>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/economic-implications-of-a-competitive-blob-market/20931">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 12:32:08 +0000</pubDate>
</item>
<item>
<title>3-Slot-Finality: SSF is not about "Single" Slot</title>
<link>https://ethresear.ch/t/3-slot-finality-ssf-is-not-about-single-slot/20927</link>
<guid>https://ethresear.ch/t/3-slot-finality-ssf-is-not-about-single-slot/20927</guid>
<content:encoded><![CDATA[
<div> 关键词: 3-Slot-Finality, 单槽最终性, 网络延迟, 验证者集合管理, 区块确认时间

总结:
本文介绍了3-Slot-Finality（3SF）协议，该协议在网络延迟被限制在一个已知值\Delta的情况下，能够在诚实提议者提出的区块在三个槽位内实现最终确定，即使后续提议者可能不诚实，并且只需要每个槽位进行一轮投票。相较于单槽最终性协议，3SF减少了投票阶段的数量，从而缩短了槽位时间并降低了预期确认时间，这对基于智能合约区块链的金融应用尤其有利，因为它们可以减少经济泄漏和提高流动性提供者的安全性。虽然3SF的区块最终确定时间相比单槽最终性有所延长，但这种延迟能通过增加到两个投票阶段并在不影响实际槽位长度的情况下进一步优化至只需两个槽位的时间来平衡。总体而言，3SF通过权衡区块最终确定时间和确认时间，为大多数用户提供了一个更高效、实用的选择。 <div>
<h1><a class="anchor" href="https://ethresear.ch#p-51049-h-3-slot-finality-ssf-is-not-about-single-slot-1" name="p-51049-h-3-slot-finality-ssf-is-not-about-single-slot-1"></a>3-Slot-Finality: SSF is not about “Single” Slot</h1>
<p>Authors: <a href="https://twitter.com/fradamt" rel="noopener nofollow ugc">Francesco D’Amato</a>, <a href="https://twitter.com/robsaltini" rel="noopener nofollow ugc">Roberto Saltini</a>, <a href="https://x.com/thanh_hai_tran" rel="noopener nofollow ugc">Thanh-Hai Tran</a>, <a href="https://twitter.com/luca_zanolini" rel="noopener nofollow ugc">Luca Zanolini</a></p>
<p><em>TL;DR; In this post, we introduce 3-Slot Finality (3SF), a protocol designed to finalize blocks proposed by honest proposer within 3 slots when network latency is bounded by a known value <span class="math">\Delta</span> – even if subsequent proposers might be dishonest – while requiring only <em>one</em> voting phase per slot. This approach contrasts with <a href="https://ethresear.ch/t/a-simple-single-slot-finality-protocol/14920">previously proposed protocols for Single-Slot Finality</a>, which require <em>three voting phases per slot</em> to finalize an honestly proposed block within a single slot resulting in longer slot time. We also show that 3SF guarantees all the key properties expected from SSF, offering an efficient and practical alternative that reduces overhead while ensuring fast and predictable block finalization within a few slots, and a shorter practical slot time (as voting phases take practically longer than other phases). As a result, our protocol achieves a shorter expected confirmation time compared to the previously proposed protocol, at the expense of a slight delay in block finalization, which extends to the time required to propose three additional blocks, rather than finalizing before the next block proposal. However, we believe that a shorter expected confirmation time could be sufficient for most users. Also, slot time is a crucial parameter affecting economic leakage in on-chain automated market makers (AMMs) due to arbitrage. Specifically, arbitrage profits – and consequently, liquidity providers’ (LP) losses – are proportional to the square root of slot time. Therefore, reducing slot time is highly desirable for financial applications on smart contract blockchains. Finally, we show that we can make a further trade-off: we can increase the number of voting phases to two, without this affecting the actual slot length, and achieve finalization in only two slots.</em></p>
<p>This post represents a summary of our extended technical paper that you can find <a href="https://arxiv.org/abs/2411.00558" rel="noopener nofollow ugc">here</a>.</p>
<p>Observe that in this post, we will focus exclusively on the consensus protocol, setting aside the issues of validator set management. This aspect is independent of the design of an effective consensus protocol and can be <a href="https://ethresear.ch/t/orbit-ssf-solo-staking-friendly-validator-set-management-for-ssf/19928">addressed separately</a>.</p>
<p><a href="https://notes.ethereum.org/@vbuterin/single_slot_finality" rel="noopener nofollow ugc">Single-Slot-Finality</a>, or SSF, is a highly expected upgrade to the Ethereum consensus layer that is often associated with being able to finalize blocks within the same slot where they are proposed, a significant improvement over the current protocol, which finalizes blocks within 64 to 95 slots. This advancement would eliminate the trade-off between economic security and faster transaction confirmation.</p>

<p>However, do we really need <strong>Single</strong>-Slot-Finality?</p>
<p>To answer this question, let us take a step back and ask ourselves another question: What are the properties not guaranteed by the current protocol (<strong>Gasper</strong>) that we would like SSF to give us?</p>
<blockquote>
<ul>
<li><strong>Property 1.</strong> If the network latency is lower than a known value <span class="math">\Delta</span> and <span class="math">\geq</span> 2/3 of the validator set (by stake) is honest and actively participate, then a block proposed by an honest node is finalized in a short and predictable amount of time.</li>
<li><strong>Property 2.</strong> If the network latency is lower than <span class="math">\Delta</span> and &gt; 1/2 of the validator set (by stake) is honest, even though &lt; 2/3 of the validators (by stake) are actively participating, blocks proposed by honest nodes will never leave the canonical chain and, once participation of honest validators is back up to <span class="math">\geq</span> 2/3 (by stake), then such a block will be finalized within a short and predictable amount of time.</li>
<li><strong>Property 3.</strong> Using the same assumptions of the point above, there exists a way to <em>confirm</em> a block, that is, to determine whether such a block will always be part of the canonical chain and, consequentially, once participation of honest validators is back up to &gt;= 2/3 (by stake), it will be finalized within a short and predictable amount of time.</li>
</ul>
</blockquote>
<p>As we can see, none of the properties listed above is necessarily about being able to finalize blocks within the same slot they are proposed. However, arguably, the sooner a block is finalized, the better. So, why don’t we just stick with Single-Slot-Finality given that we already have <a href="https://ethresear.ch/t/a-simple-single-slot-finality-protocol/14920">one such protocol</a> (which, from here on, we refer to as SSF) that guarantees all of our desired properties above, including being able to finalize blocks within the same slot?</p>
<p>The reason is that SSF requires more than one voting phase per slot. The general issue with this is that, because of the large amount of data flooding the network when voting happens, in practice, extra latency must be accounted for each voting phase. For example, to reduce overall network bandwidth, Ethereum currently employs a signature aggregation scheme for each voting phase by which votes are first sent to aggregators who then distribute the aggregated signatures. This means that voting phases require <span class="math">2\Delta</span> time, that is, <em>double</em> the normal network latency. Even if in the future it is found that we can do away with aggregators, it is reasonable to expect that voting phases take longer than other phase. In the rest of this section, we assume that a voting phase lasts <span class="math">2\Delta</span> as per current Ethereum protocol.</p>
<p>So, the more voting phases, the longer the block time tends to be. Specifically, as detailed later, SSF divides each slot in 4 phases: (1) block proposing, (2) <code>Head-vot</code>ing, (3) <code>FFG-vot</code>ing and (4) <code>Acknowledgment</code> voting plus view freezing (view-freezing is not important for the argument here). Phase (3) and (4) must wait for phases (2) and (3) to complete, respectively. However, the start of the next slot and any of its phases <em>do not</em> depend on phase (4). So, the slot length in SSF is <span class="math">\Delta + 2\Delta + 2\Delta + \Delta = 6 \Delta</span>.</p>
<p>In our protocol (3SF) slots are also composed of 4 phases, but only one of them is a voting phase. Specifically, we have (1) block proposing, (2) <code>Head/FFG-vot</code>ing where both <code>Head-vote</code>s and <code>FFG-vote</code>s are cast, (3) fast-confirmation and (4) <a href="https://ethresear.ch/t/view-merge-as-a-replacement-for-proposer-boost/13739">view-merging</a>. Then, the slot length becomes <span class="math">\Delta + 2\Delta + \Delta + \Delta = 5\Delta</span>.</p>
<p>First, block time <a href="https://arxiv.org/abs/2208.06046" rel="noopener nofollow ugc">has been shown</a> to be an important parameter in determining the <em>economic leakage</em> of on-chain AMMs to arbitrage. For instance, arbitrage profits (and equivalently LP losses) are proportional to the square root of block time, so that a lower block time is very desirable by financial applications built on top of a smart contract blockchain.</p>
<p>Second, with a shorter block time, we can achieve a shorter <em>expected confirmation time</em> which is the expected delay from when a user submit a transaction to when such a transaction is <em>confirmed</em>, that is, to when it is included in a confirmed block. This corresponds to the time taken to confirm a block proposed by an honest proposer + <span class="math">\frac{(1+\beta) \cdot \text{slot-time}}{2(1 - \beta)}</span> where <span class="math">\beta</span> represents the adversarial power in the network. In both 3SF and SSF the time taken to confirm a block proposed by an honest validator is <span class="math">3\Delta</span>. So, for <span class="math">\beta = \frac{1}{3}</span>, the expected confirmation time for SSF is <span class="math">9\Delta</span> whereas for our protocol is <span class="math">8\Delta</span>, meaning an <span class="math">\approx 11\%</span> improvement. For <span class="math">\beta = 0</span>, the expected confirmation time for SSF is <span class="math">6\Delta</span> whereas for our protocols is <span class="math">5.5\Delta</span>, meaning an <span class="math">\approx 8\%</span> improvement.</p>
<p>However, as one would expect, while the expected confirmation time of 3SF is shorter, the expected finalization time (that is, the expected time take to finalize a transaction) is longer. The expected finalization time is computed similarly to the expected confirmation time, namely, the time taken to finalize a block proposed by an honest proposer + <span class="math">\frac{(1+\beta) \cdot \text{slot-time}}{2(1 - \beta)}</span>. In SSF the time taken to finalize an honest block is <span class="math">5\Delta</span>, in 3SF is <span class="math">11\Delta</span> and the two-slot variant of 3SF is <span class="math">8\Delta</span>. So, for <span class="math">\beta = \frac{1}{3}</span>, the expected finalization time for SSF is <span class="math">11\Delta</span>, for 3SF is <span class="math">16\Delta</span>, and for the two-slot variant of 3SF is <span class="math">13\Delta</span>, meaning that the expected finalization time for 3SF is <span class="math">\approx 46\%</span> higher than that of SSF, but this reduces to <span class="math">\approx 18\%</span> for its two-slot variant. For <span class="math">\beta = 0</span>, the expected finalization time for SSF is <span class="math">8\Delta</span>, for 3SF is <span class="math">13.5\Delta</span>, and for the two-slot variant of 3SF is <span class="math">10.5\Delta</span>, meaning that the expected finalization time for 3SF is <span class="math">\approx 69\%</span> higher than that of SSF, but this reduces to <span class="math">\approx 31\%</span> for its two-slot variant.</p>
<p>Overall, 3SF achieves a balance by trading a higher expected finalization time for a shorter expected confirmation time, which could be sufficient for most users. At the same time, it offers shorter slot time. As additional benefit, the slot structure of 3SF more closely resemble the current Ethereum’s slot structure.</p>
<p>A <a href="https://ethresear.ch/t/streamlining-fast-finality/16591">previous research post</a> already highlighted the issue about the number of voting phases in 3SF and put forward a solution for that. However, this solution could guarantee the desired properties only if the proposers in the <em>two subsequent slots</em> are also honest.</p>
<p>This takes a step in the right direction by reducing the protocol to <em>only one voting phase per slot</em>, but it also introduces a drawback: a <em>lower probability</em> of meeting the required conditions for finality. However, we now show that it is possible to design a protocol that can reduce the number of voting phase to one without requiring that proposers in subsequent slots are honest.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51049-recalling-the-ssf-protocolhttpsarxivorgabs230212745-2" name="p-51049-recalling-the-ssf-protocolhttpsarxivorgabs230212745-2"></a>Recalling the <a href="https://arxiv.org/abs/2302.12745" rel="noopener nofollow ugc">SSF protocol</a></h2>
<p>We build our 3SF protocol by using the SSF protocol as starting point, which is summarized by the following picture.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/0/6/06ae66d220c91d79d61587390045bed544c39ecd.png" title="SSF"><img alt="SSF" height="328" src="https://ethresear.ch/uploads/default/optimized/3X/0/6/06ae66d220c91d79d61587390045bed544c39ecd_2_690x328.png" width="690" /></a></div><p></p>
<p>Let us now recall some of the most important concepts.<br />
(The reader is however encouraged to read the <a href="https://ethresear.ch/t/a-simple-single-slot-finality-protocol/14920">original post</a>.)</p>
<ol>
<li><strong>Ebb-and-flow construction.</strong> SSF consists of two sub-protocols. The first is a <em>PBFT-style</em> (name that comes from this <a href="https://pmg.csail.mit.edu/papers/osdi99.pdf" rel="noopener nofollow ugc">seminal paper</a>) sub-protocol that always ensures <em>safety</em>, even in the case of network partitioning, but it requires at least 2/3 honest stake. The second is the <em>dynamically-available (DA)</em> sub-protocol, which guarantees <em>progress</em> even if some honest validators stop participating, under the assumption of a simple <em>honest majority</em>. A protocol combining these two types of sub-protocols is referred to as an <em><a href="https://arxiv.org/abs/2009.04987" rel="noopener nofollow ugc">ebb-and-flow protocol</a></em>.</li>
<li><strong>Casper-based BFT sub-protocol.</strong> The PBFT-style sub-protocol is heavily based on <a href="https://arxiv.org/abs/1710.09437" rel="noopener nofollow ugc">Casper</a>, the Finality <em>Gadget</em> currently used by Gasper. Compared to Gasper, in SSF checkpoints are pairs of blocks and slots, rather than blocks and epochs. This is because, in SSF, <em>in some sense</em>, epochs and slot are the same thing. However, the general rules for <em>checkpoint justification and finalization</em> are the same. Let us briefly recall such rules. First, block finalization is achieved through checkpoint finalization. Specifically, to finalize a block <span class="math">B</span>, we need to finalize some checkpoint <span class="math">(B_\mathsf{d},s)</span> where <span class="math">B_\mathsf{d}</span> is a descendant of <span class="math">B</span> and <span class="math">s \geq \mathsf{slot}(B_\mathsf{d})</span>. Second, checkpoint finalization is achieved in two stages: first checkpoints are <em>justified</em>, then a justified checkpoint is <em>finalized</em>. Let us now explain how this process works. The genesis checkpoint <span class="math">(B_\mathsf{genesis},0)</span> is by definition both justified and finalized. Justification and finalization of any other checkpoint is achieved through <code>FFG-vote</code>s which are votes for links between checkpoints. They are of the form <span class="math">C_\mathsf{s} \to C_\mathsf{t}</span>, where <span class="math">C_\mathsf{s}</span> and <span class="math">C_\mathsf{t}</span> are called the <em>source</em> and <em>target</em> checkpoints, respectively.  A checkpoint <span class="math">(B,s)</span> is justified if there are <code>FFG-vote</code>s  from <span class="math">\geq</span> 2/3 of the validators (weighted by stake) for a link <span class="math">C_\mathsf{s} \to (B,s)</span> where <span class="math">C_\mathsf{s}</span> is a justified checkpoint. A checkpoint <span class="math">(B,s)</span> is then <em>finalized</em> if <span class="math">(B,s)</span> is justified and there are <code>FFG-vote</code>s  from <span class="math">\geq</span> 2/3 of the validators (weighted by stake) for a link <span class="math">(B,s) \to (B',s+1)</span> with <span class="math">B'</span> descendant of <span class="math">B</span>. In addition to this, in order to be able to finalize blocks within one slot, SSF introduces <code>Acknowledgment</code> votes <span class="math">((B,s),s)</span> that are, roughly speaking, a “compressed” <code>FFG-vote</code> saying that a validator saw a given checkpoint <span class="math">(B,s)</span> justified at the end of slot <span class="math">s</span>. Anyone who sees checkpoint <span class="math">(B,s)</span> as justified and receives <code>Acknowledgment</code> votes <span class="math">((B,s),s)</span> from <span class="math">\geq</span> 2/3 of the validators (weighted by stake) can safely consider checkpoint <span class="math">(B,s)</span> finalized.</li>
<li><strong><a href="https://arxiv.org/pdf/2302.11326.pdf" rel="noopener nofollow ugc">RLMD-GHOST</a> as the DA sub-protocol.</strong> SSF leverages RLMD-GHOST as the DA sub-protocol to achieve Properties 2 and 3 above. In RLMD-GHOST, validators votes for blocks. We use the term <code>Head-vote</code>s to refer to such votes. RLMD-GHOST works in the <a href="https://eprint.iacr.org/2016/918.pdf" rel="noopener nofollow ugc">sleepy model</a> where validators may <em>fall asleep</em> and stop participating for a period. A key property of RLMD-GHOST is that as long as the network latency is less than <span class="math">\Delta</span> and less than 1/2 of the validators (weighted by stake) are dishonest, the block proposed by an honest proposer will receive and keep receiving the <code>Head-vote</code>s of all active validators. Importantly, RLMD-GHOST comes with the following <em>confirmation</em> rule that allows determining whether a block is <em>confirmed</em>, that is, it will always be part of the canonical chain of any honest valiadator: Any block that is at least <span class="math">\kappa</span>-deep with respect to the the current canonical chain is <em>confirmed</em> (the head of the canonical chain is 0-deep, its parent is 1-deep and so on). We refer to this rule as the <em><span class="math">\kappa</span>-deep confirmation rule</em>. The value <span class="math">\kappa</span> represents the number of slots that we need in order to be sure, except for a negligible probability, that at least one of the proposers in these slots is honest. Clearly this means that <span class="math">\kappa &gt;&gt; 1</span>.</li>
<li><strong>Fast confirmation.</strong> Compared to the <span class="math">\kappa</span>-deep confirmation rule, fast confirmation allows <em>confirming</em> a block within the same slot it has been proposed. However, it requires at least 2/3 of the validators (weighted by stake) to be honest and active. Fast confirmation of a block occurs when more than <em>2/3 of the <code>Head-vote</code>s</em> from all active validators (weighted by stake) are received after a time delay of <span class="math">\Delta</span> from when they were sent. In the diagram above, the <code>Head-vote</code>s are cast at 4$\Delta s$ + <span class="math">\Delta</span>, meaning they are sent <span class="math">\Delta</span> time within slot <span class="math">s</span>. If by 4$\Delta s  + 2\Delta$ an honest validator receives more than 2/3 of the <code>Head-votes</code> (weighted by stake) for a specific block <span class="math">B</span>, then <span class="math">B</span> is considered <em>fast confirmed</em>.</li>
<li><strong>Integration between the BFT and DA protocols.</strong> In SSF, honest validators determine the <code>FFG-vote</code> to cast as follows. The source checkpoint corresponds to the <em>greatest justified checkpoints</em> in their view, which is the justified checkpoint with the highest slot (no two different justified checkpoints for the same slot can every be justified unless &gt; 1/3 of the stake is slashed). The slot of the target checkpoint corresponds to the current slot while the block corresponds to the block fast confirmed by the DA protocol, if there exists one, or the highest block confirmed by the DA protocol via the <span class="math">\kappa</span>-deep confirmation rule, otherwise.</li>
<li><strong>Slots of length <span class="math">4\Delta</span>.</strong> Except when specifically stated, to align with previous literature, in the rest of this post, we assume that voting phases take just <span class="math">\Delta</span> time, that is, we do not explicitly account  for the extra latency introduced by voting phases. Then, in SSF, each slot has length <span class="math">4\Delta</span>.</li>
</ol>


 
<h2><a class="anchor" href="https://ethresear.ch#p-51049-building-our-3sf-protocol-3" name="p-51049-building-our-3sf-protocol-3"></a>Building our 3SF Protocol</h2>
<p>We now start building our 3SF protocol by taking the SSF protocol and applying the following modifications to have one single voting phase per slot.</p>
<ol>
<li><code>FFG-vote</code>s are cast together with the <code>Head-vote</code>s at <span class="math">\Delta</span> time into a slot, where <span class="math">\Delta</span> represents the network latency.</li>
<li>Remove fast-confirmations.<br />
Hence, the target of the <code>FFG-vote</code>s is the longest chain confirmed by the DA protocol. Under this condition, the DA protocol ensures that if the proposer is honest, all honest validators see the same chain as confirmed by the DA protocol.</li>
<li>No <code>Acknowledgment</code> votes.</li>
</ol>
<p>By removing one phase, the length of the slot then decreases to <span class="math">3\Delta</span>. The resulting protocol can be schematized as follows.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/9/f957914ecef4f75b33bf7720165291a2a8ab6285.png" title="attempt1"><img alt="attempt1" height="283" src="https://ethresear.ch/uploads/default/optimized/3X/f/9/f957914ecef4f75b33bf7720165291a2a8ab6285_2_690x283.png" width="690" /></a></div><p></p>
<p>Let us now take a look at how a run of such a protocol would look like even under the simplifying assumption that we have two consecutive honest proposers and the usual assumptions that (i) &lt; 1/3 of the validators (weighted by stake) are dishonest and the (ii) network delay is less than <span class="math">\Delta</span>.</p>
<ol>
<li>The proposer in slot <span class="math">s</span> proposes block <span class="math">B</span>.</li>
<li>Due to the DA protocol’s properties, all honest validators in slot <span class="math">s</span> cast an <code>Head-vote</code> for <span class="math">B</span>. However, given that <span class="math">B</span> has just been proposed, <span class="math">B</span> cannot also already be part of the chain confirmed by the DA protocol. This means that the <code>FFG-vote</code>s sent by honest validators have as target a strict ancestor of <span class="math">B</span>. All validators have the same view of the chain confirmed by the DA protocol. This means that they all send <code>FFG-vote</code>s with the same target <span class="math">(B_\mathsf{a},s)</span> with <span class="math">B_\mathsf{a}</span> ancestor of <span class="math">B</span>. (From now on, let us use the notation <span class="math">B_\mathsf{a} \preceq B</span> to mean that <span class="math">B_\mathsf{a}</span> is a non-strict ancestor of <span class="math">B</span>, and <span class="math">B_\mathsf{a} \prec B</span> to mean that it is a strict ancestor, that is, <span class="math">B_\mathsf{a} \preceq B</span> means <span class="math">B_\mathsf{a} \prec B</span> or <span class="math">B_\mathsf{a} = B</span>.)</li>
<li>The proposer in slot <span class="math">s+1</span> proposes a block <span class="math">B'</span> child of <span class="math">B</span> and packs into it all the <code>FFG-vote</code>s sent in slot <span class="math">s</span>.</li>
<li>Validators voting in slot <span class="math">s+1</span>, then see <span class="math">(B_\mathsf{a},s)</span> as justified. This means that they cast an <code>FFG-vote</code> <span class="math">(B_\mathsf{a},s) \to (B'_\mathsf{a},s+1)</span> (we use the notation <span class="math">C_\mathsf{s} \to C_\mathsf{t}</span> to indicate an <code>FFG-vote</code> with source <span class="math">C_\mathsf{s}</span> and target <span class="math">C_\mathsf{t}</span>), where <span class="math">B'_\mathsf{a} \prec B'</span>. However, given that we have dropped fast-confirmation, the DA protocol only confirms via the <span class="math">\kappa</span>-deep confirmation rule. Given that <span class="math">\kappa &gt; 1</span>, this implies that <span class="math">B'_\mathsf{a} \prec B</span>. As a consequence of this, we are at most able to justify a strict ancestor of <span class="math">B</span>, not <span class="math">B</span>, with the votes sent in this slot, which makes impossible to finalize <span class="math">B</span> in the next slot.</li>
</ol>
<p>The above shows us that we do need fast-confirmations. Therefore, let’s reintroduce them into the protocol just presented. Specifically, for <code>FFG-vote</code>s, we use the chain fast-confirmed in the previous slot, rather than the one from the current slot, to determine the target block. This is because, given that <code>Head-vote</code>s and <code>FFG-vote</code>s are cast at the same time, clearly we cannot use the <code>Head-vote</code>s cast at a given slot to determine the target of the <code>FFG-vote</code>s cast in the same slot. We just need to wait <span class="math">\Delta</span> from the voting time to perform fast-confirmation. As a consequence of the above, we are back to a slot of length <span class="math">4\Delta</span> as shown by the following picture illustrating this last protocol.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/7/277aa6b17ef4ceb670f32ea71d3f03bbb5ab0cbb.png" title="attempt2"><img alt="attempt2" height="241" src="https://ethresear.ch/uploads/default/optimized/3X/2/7/277aa6b17ef4ceb670f32ea71d3f03bbb5ab0cbb_2_690x241.png" width="690" /></a></div><p></p>
<p>Let us now take a look at whether this is going to work.</p>
<ol start="2">
<li>In slot <span class="math">s</span>, the block in the target checkpoint of <code>FFG-vote</code>s cast by honest validators is either the chain confirmed by the DA protocol (via the <span class="math">\kappa</span>-deep rule mentioned above), or the chain fast-confirmed at the slot before. This turns out to be an issue as some validators may have fast-confirmed a block <span class="math">B^{\mathsf{fastconf}}</span> in the slot before, some others might have not. So, it is very well possible that some validators cast an <code>FFG-vote</code> with target block <span class="math">B^{\mathsf{conf}}</span>, others an <code>FFG-vote</code> with target block <span class="math">B^{\mathsf{fastconf}}_\mathsf{a}</span>. The problem is that, even though <span class="math">B^{\mathsf{conf}}_\mathsf{a} \prec B^{\mathsf{fastconf}}</span>, by using the justification rules of SSF/Gasper this leads to a situation where we do not justify any checkpoint for slot <span class="math">s</span>, which seems like a step backwards rather then forward.</li>
</ol>
<p>How do we fix this?</p>
<p>Note that when we finalize a checkpoint with block <span class="math">B</span>, we finalize the entire chain, not only <span class="math">B</span>. We could do the same for justified checkpoints. That is, if we justify <span class="math">(B,s)</span> we also consider any checkpoint <span class="math">(B_\mathsf{a},s)</span> where <span class="math">B_\mathsf{a} \preceq B</span> as justified. However, this is not enough to address the issue highlighted from the last example, as different validators might <code>FFG-vote</code> for different checkpoints, even though the blocks included in such checkpoints are on the same chain.</p>
<p>We need to take this a step further:</p>
<blockquote>
<p>An <code>FFG-vote</code> <span class="math">(B_\mathsf{s},s_\mathsf{s}) \to(B_\mathsf{t},s_\mathsf{t})</span>, provided that the source checkpoint <span class="math">(B_\mathsf{s},s_\mathsf{s})</span> is justified, is considered contributing to the justification of any checkpoint <span class="math">(B',s_\mathsf{t})</span> where <span class="math">B_\mathsf{s}\preceq B' \preceq B_\mathsf{t}</span>,that is, <span class="math">B</span>’ is any block between <span class="math">B_\mathsf{s}</span> and <span class="math">B_\mathsf{t}</span> included. Note that the slot in the target checkpoint and the justified checkpoint are the same.<br />
If we have <code>FFG-vote</code>s from <span class="math">\geq</span> 2/3 of the validators (weighted by stake) contributing to the justification of <span class="math">(B',s_\mathsf{t})</span>, then <span class="math">(B',s_\mathsf{t})</span> is considered justified.</p>
</blockquote>
<p>Before proceeding, let us go over the example below to ensure that this new justification rule is clear. We consider 3 slots, slot 0, 4, and 6. For each of these slots, say <span class="math">s</span>, the Figure below shows the list of possible target checkpoints for <code>FFG-vote</code>s cast in slot <span class="math">s</span>. This includes all the checkpoints comprising any of the blocks received up to the voting time in slot <span class="math">s</span>, except for the block proposed in slot <span class="math">s</span>, paired with slot <span class="math">s</span>.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/0/b/0bbd08685b1abd9f7bdcf1f8b38088f57ae3ba59.jpeg" title="justification2"><img alt="justification2" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/0/b/0bbd08685b1abd9f7bdcf1f8b38088f57ae3ba59_2_367x500.jpeg" width="367" /></a></div><p></p>
<p>We assume just 4 validators, all with the same stake.<br />
Let us consider first the <code>FFG-vote</code>s between <span class="math">(B0,0)</span> and checkpoints for slot 4. If we take <span class="math">B</span> to be such that <span class="math">B0 \preceq B \preceq B2</span>, then we have <span class="math">3 \geq \frac{2}{3} 4</span> <code>FFG-vote</code>s between <span class="math">(B0,0)</span> and a checkpoint <span class="math">(B_\mathsf{d},4)</span> such that <span class="math">B \preceq B_\mathsf{d}</span>.<br />
Given that <span class="math">B0 \preceq B</span>, as per our rule, <span class="math">(B0,4)</span>, <span class="math">(B1,4)</span> and <span class="math">(B2,4)</span> are justified. However, <span class="math">(B3,4)</span> is not justified as only the <code>FFG-vote</code> <span class="math">(B0,0) \to (B3,4)</span> is in support of its justification.<br />
Now we move to the <code>FFG-vote</code>s between checkpoints for slot 4 and checkpoints for slot 6. Note that by the reasoning above all of these <code>FFG-vote</code>s have a justified checkpoint as source. Now, take <span class="math">B</span> to be such that <span class="math">B2 \preceq B \preceq B4</span>. Note that we have 3 <code>FFG-vote</code>s between a justified checkpoint <span class="math">(B_\mathsf{a},4)</span> and a checkpoint <span class="math">(B_\mathsf{d}.6)</span> such that <span class="math">B_\mathsf{a}\preceq B\preceq B_\mathsf{d}</span>. As per our rule, this means that <span class="math">(B2,6)</span>, <span class="math">(B3,6)</span>, and <span class="math">(B4,6)</span> are justified. Importantly, <span class="math">(B0,6)</span> and <span class="math">(B1,6)</span> are not justified, despite <span class="math">(B0,4)</span> and <span class="math">(B1,4)</span> being justified. For <span class="math">(B1,6)</span>, this is because we only have the two <code>FFG-vote</code>s <span class="math">(B1,4) \to (B4,6)</span> in support of its justification. Whereas, for <span class="math">(B0,6)</span> we have no <code>FFG-vote</code> is support of its justification.</p>
<p>This new rule clearly allows having more than one justified checkpoint for a given slot. This is OK, as they would be on the same chain. However, when computing the best justified checkpoint, using the checkpoint slot is not sufficient anymore. Then, we naturally define the ordering between checkpoints that have the same slot number to be according to the block’s proposed slot so that if <span class="math">\mathsf{slot}(B) &lt; \mathsf{slot}(B')</span>, then <span class="math">(\mathsf{slot}(B'),s)</span> is greater than <span class="math">(\mathsf{slot}(B),s)</span>.</p>
<p>Then, let’s resume the previous example with this new protocol assuming that only the proposer of slot <span class="math">s</span> is honest (in addition to the usual assumptions):</p>
<ol start="2">
<li>In slot <span class="math">s</span>, any honest validator cast an <code>FFG-vote</code> <span class="math">C_\mathsf{s} = (B_{\mathsf{aa}},s') \to C_d = (B_\mathsf{a},s)</span>, with <span class="math">C_\mathsf{s}</span> and <span class="math">C_d</span> potentially different for each validator, but with the property that <span class="math">B_\mathsf{aa} \preceq B_\mathsf{a} \prec B</span>. This set of <code>FFG-vote</code>s then leads to justifying <em>at least</em> a checkpoint <span class="math">(B_\mathsf{j},s)</span> with <span class="math">B_\mathsf{j} \prec B</span>.<br />
Also, all honest validators fast-confirm <span class="math">B</span>.</li>
<li>The proposer of slot <span class="math">s+1</span> can propose whatever they want, or nothing at all.</li>
<li>In slot <span class="math">s+1</span>, any honest validator casts an <code>FFG-vote</code> <span class="math">(B_\mathsf{j},s) \to (B,s+1)</span>, where <span class="math">B_\mathsf{j}</span> can be potentially different for each validator, but <span class="math">B_\mathsf{j} \prec B</span>. This is because, from the point above, we know that there is at least a justified checkpoint for slot <span class="math">s</span>. Overall, this means that this set of <code>FFG-vote</code>s justifies <span class="math">(B,s+1)</span>. So far so good! It looks like we are headed in the right direction!</li>
<li>The proposer of slot <span class="math">s+2</span> can propose whatever they want, or nothing at all.</li>
<li>Given the rules on checkpoint ordering that we have established above, any honest validator sees <span class="math">(B,s+1)</span> as the greatest justified checkpoint. Then, any honest validator cast an <code>FFG-vote</code> <span class="math">(B,s+1) \to (B'',s+2)</span> where <span class="math">B''</span> can be potentially different for each validator, but, due to the DA protocol’s properties, <span class="math">B \prec B''</span>. Using the Gasper/SSF finalization-rule this would not necessarily yield the  finalization <span class="math">(B,s+1)</span>. However, this can be easily fixed. Let’s see how!</li>
</ol>
<p>Note that in Gasper/SSF, in an <code>FFG-vote</code> <span class="math">(B,s+1) \to (B',s+2)</span>, block <span class="math">B'</span> doesn’t carry any meaningful information about whether it is safe to finalize <span class="math">(B, s+1)</span>. Such a vote is saying that the validator signing it saw <span class="math">(B,s+1)</span> as the greatest justified checkpoint by the time they vote in slot <span class="math">s+2</span> (which is exactly what an <code>Acknowledgment</code> vote does in the SSF protocol). Hence, such a validator would then be slashed if it in a later slot (that is, in a slot <span class="math">&gt; s + 2</span>) it signs an <code>FFG-vote</code> with source a checkpoint lower than <span class="math">(B,s+1)</span>. This means that we can modify the finalization rule as follows.</p>
<blockquote>
<p>If we have <span class="math">\geq</span> 2/3 of the validators (weighted by stake) FFG-voting <span class="math">(B,s+1) \to (B',s+2)</span>, with <span class="math">B'</span> potentially different for each validator, but such that <span class="math">B \preceq B'</span>, then <span class="math">(B,s+1)</span> is finalized.</p>
</blockquote>
<p>With this last modification, at step 6 above, we finalize block <span class="math">B</span> as we were hoping to do!</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51049-is-this-protocol-safe-4" name="p-51049-is-this-protocol-safe-4"></a>Is this protocol safe?</h3>
<p>Fair question! In order to make this protocol safe we only need to slightly modify the slashing rules.</p>
<p>Let us start by recalling the slashing rules employed by Gasper which are the same used by SSF (to be precise SSF has an additional rule that uses for <code>Acknowledgment</code> votes, but we can ignore it as we do not use that type of vote).<br />
First, a validator is slashed if it sends two different <code>FFG-vote</code>s but with target checkpoints for the same slot.<br />
Second, a validator is slashed if it sends two <code>FFG-vote</code>s <span class="math">(*,\mathit{s1}_\mathsf{s})\to(*,\mathit{s1}_\mathsf{t})</span> and <span class="math">(*,\mathit{s2}_\mathsf{s})\to(*,\mathit{s2}_\mathsf{t})</span> such that <span class="math">\mathit{s1}_\mathsf{s} &lt; \mathit{s2}_\mathsf{s}</span> and <span class="math">\mathit{s2}_\mathsf{t} &lt; \mathit{s1}_\mathsf{t}</span>. This is called a <em>surround vote</em> as the first vote surrounds the second one.<br />
Gasper/SSF ensures that if two conflicting blocks are finalized, then we can slash at least 1/3 of the validator set.</p>
<p>We now show that the two rules above are not sufficient for 3SF.<br />
Note that in Gasper/SSF, it is perfectly legal for a validator to cast two <code>FFG-vote</code>s with source checkpoints from the same slot. This fine as Gasper/SSF ensures that no two different checkpoints for the same slot can ever be justified (unless &gt; 1/3 of the validators, weighted by stake, are dishonest). However, with our modified protocol, this is not true any more. This means that in our protocol, it is currently possible to cast two <code>FFG-vote</code>s <span class="math">(B,s) \to (B_d,s+1)</span> and <span class="math">(B_\mathsf{a},s) \to (B',s+2)</span>, where both <span class="math">(B,s)</span> and <span class="math">(B_\mathsf{a},s)</span> are justified, with <span class="math">B_\mathsf{a} \preceq B</span>, and <span class="math">B'</span> conflicting with <span class="math">B_d</span>. This means that the first <code>FFG-vote</code> can contribute to finalizing checkpoint <span class="math">(B,s)</span> and the second one to justifying the higher, but conflicting checkpoint, <span class="math">(B',s+2)</span> from which then it is possible to finalize such a checkpoint without committing any slashsable offense.</p>
<p>Fortunately, the fix to this issue is straightforward. We extend the Gasper/FFG slashing rules with the following one.</p>
<blockquote>
<p>Two <code>FFG-vote</code>s <span class="math">(\mathit{B1},s) \to (*,\mathit{s1}_\mathsf{t})</span> and <span class="math">(\mathit{B2},s) \to (*,\mathit{s2}_\mathsf{t})</span> with <span class="math">\mathsf{slot}(\mathit{B1}) &lt; \mathsf{slot}(\mathit{B2})</span> and <span class="math">\mathit{s2}_\mathsf{t} &lt; \mathit{s1}_\mathsf{t}</span> constitutes a slashable offense (surrounding vote).</p>
</blockquote>
<p>As we can see, with this rule, the situation described above is prevented.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-51049-implementation-considerations-5" name="p-51049-implementation-considerations-5"></a>Implementation Considerations</h4>
<p>The newly introduced slashing rule requires accessing the block’s slot number. This poses a slight problem as checkpoint <code>FFG-vote</code>s do not include the full block, but only the hash and we need to be able to determine validators’ slashability just by looking at the <code>FFG-vote</code>s that they cast, without needing to have received the actual blocks that the refer to via their hashes.</p>
<p>This is easily solved by extending checkpoint to be triples <span class="math">(H,s,p)</span>, rather than just tuples <span class="math">(H,s)</span>, where <span class="math">H</span> is the hash of a block (say <span class="math">B</span>), <span class="math">s</span> is the checkpoint’s slot and <span class="math">p</span> is <span class="math">B</span>'s block slot, that is, <span class="math">p = \mathsf{slot}(B)</span>. Naturally, this also means that before accepting an <code>FFG-vote</code> <span class="math">(H,s,p) \to C_\mathsf{t}</span> as valid and accounting it for justifications/finalizations, we need to check that <span class="math">B.\mathsf{slot}=p</span>, where <span class="math">B</span> is the block with hash <span class="math">H</span>.</p>

<h2><a class="anchor" href="https://ethresear.ch#p-51049-what-about-the-other-properties-6" name="p-51049-what-about-the-other-properties-6"></a>What about the other properties?</h2>
<p>So far, we have just addressed the first property of our interest: If the network latency is lower than <span class="math">\Delta</span> and &gt;= 2/3 of the validator set (by stake) is honest and actively participate, then a block proposed by an honest node is finalized in a short and predictable amount of time.</p>
<p>What about the other two properties that we have listed at the beginning?</p>
<p>Those properties are already guaranteed by the DA protocol employed by SSF, namely RLMD-GHOST. So, we can either use RLMD-GHOST as our DA protocol or another DA protocol, like <a href="https://arxiv.org/abs/2310.11331" rel="noopener nofollow ugc">TOB-SVD</a>.</p>
<p>In <a href="https://arxiv.org/abs/2411.00558" rel="noopener nofollow ugc">our extended technical report</a>, we show in detail how both RLMD-GHOST and TOB-SVD can be used to achieve 3-Single-Slot Finality following the description we just presented.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51049-can-we-do-better-than-3-slot-finality-7" name="p-51049-can-we-do-better-than-3-slot-finality-7"></a>Can we do better than 3-Slot-Finality?</h2>
<p>It turns our that we can actually reduce finalization to just two slots by re-introducing <code>Acknowledgment</code> votes.<br />
Of course, this mean two voting phases per slot, rather than one, but, as discussed above, this does not impact the actual slot length (that is, when we consider the extra latency required by voting phases), as the distribution of <code>Acknowledgment</code> votes can proceed in parallel to any subsequent phase.</p>
<p>In 3SF, <code>Acknowledgment</code> votes would be cast at the fast confirmation time, that is, at <span class="math">2\Delta</span> in a slot.<br />
As in the SSF protocol, to keep things safe, we would need to add the following slashing rule.</p>
<blockquote>
<p>An <code>FFG-vote</code> <span class="math">(\mathit{B1},s1_\mathsf{s}) \to (*,s1_\mathsf{t})</span> and an <code>Acknowledgment</code> vote <span class="math">((\mathit{B2},\mathit{sa}),\mathit{sa})</span> with <span class="math">\mathit{sa} &lt; s1_\mathsf{t}</span> and either <span class="math">s1_\mathsf{s} &lt; \mathit{sa}</span>,  or <span class="math">s1_\mathsf{s} = \mathit{sa}</span> and <span class="math">\mathsf{slot}(\mathit{B1}) &lt; \mathsf{slot}(\mathit{B2})</span> constitutes a slashable offense.</p>
</blockquote>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/3-slot-finality-ssf-is-not-about-single-slot/20927">Read full topic</a></p>
]]></content:encoded>
<pubDate>Mon, 04 Nov 2024 12:10:04 +0000</pubDate>
</item>
<item>
<title>Censorable Tornado Cash</title>
<link>https://ethresear.ch/t/censorable-tornado-cash/20920</link>
<guid>https://ethresear.ch/t/censorable-tornado-cash/20920</guid>
<content:encoded><![CDATA[
<div> 关键词：Tornado Cash、零知识证明（ZK技术）、隐私保护、监管审计、选择性解密

总结:
Tornado Cash是一个基于以太坊虚拟机兼容网络的开源、非托管、完全去中心化的加密货币搅拌器，通过使用零知识证明（ZK技术）提供服务，混淆可识别或“污染”的数字货币资金来源。然而，这也使其成为洗钱活动的工具，例如美国财政部因涉及数亿美元的黑客攻击洗钱行为制裁了Tornado Cash。

为了解决隐私与监管之间的矛盾，未来可能发展出带有选择性审计功能的隐私交易方案。选择性解密的零知识证明（SA-ZKP）算法为此提供了可能性，它可以在保持用户匿名的同时，允许授权监管机构在必要时选择性地解密交易数据进行调查。若推出这样一个新版本的混合器，即在法律执行部门要求时使用陷阱门密钥解密特定交易证据，用户是否会继续使用，则取决于他们对隐私和合规性的权衡考虑。 <div>
<p>I’m currently researching Tornado Cash, mainly because I believe it’s a proven application of ZK technology and has broad privacy-oriented uses for community members. I’ve created this thread to discuss it with everyone.</p>
<p><strong>Tornado Cash</strong> (also stylized as <strong>TornadoCash</strong> ) is an <a href="https://en.wikipedia.org/wiki/Open_source" rel="noopener nofollow ugc">open source</a>, <a href="https://en.wikipedia.org/wiki/Custodian_bank" rel="noopener nofollow ugc">non-custodial</a>, fully <a href="https://en.wikipedia.org/wiki/Decentralised_system" rel="noopener nofollow ugc">decentralized</a> <a href="https://en.wikipedia.org/wiki/Cryptocurrency_tumbler" rel="noopener nofollow ugc">cryptocurrency tumbler</a> that runs on <a href="https://en.wikipedia.org/wiki/Ethereum_Virtual_Machine" rel="noopener nofollow ugc">Ethereum Virtual Machine</a>-compatible networks. It offers a service that mixes potentially identifiable or “tainted” cryptocurrency funds with others, so as to obscure the trail back to the fund’s original source. This is a privacy tool used in EVM networks where all transactions are public by default.</p><aside class="onebox wikipedia">
  <header class="source">

      <a href="https://en.wikipedia.org/wiki/Tornado_Cash" rel="noopener nofollow ugc" target="_blank">en.wikipedia.org</a>
  </header>

  <article class="onebox-body">
    

<h3><a href="https://en.wikipedia.org/wiki/Tornado_Cash" rel="noopener nofollow ugc" target="_blank">Tornado Cash</a></h3>

<p>

 Tornado Cash (also stylized as TornadoCash) is an open source, non-custodial, fully decentralized cryptocurrency tumbler that runs on Ethereum Virtual Machine-compatible networks. It offers a service that mixes potentially identifiable or "tainted" cryptocurrency funds with others, so as to obscure the trail back to the fund's original source. This is a privacy tool used in EVM networks where all transactions are public by default.
 In August 2022, the U.S. Department of the Treasury blacklist...</p>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both;"></div>
</aside>

<p>Nocturne is a protocol enabling private accounts on Ethereum. Imagine a conventional Ethereum account but with built-in asset privacy. Nocturne allows users to deposit or receive funds to private, stealth addresses within the Nocturne contracts. Then, in the future, a user can prove ownership of assets in zero knowledge for use in arbitrary transactions or confidential transfers.It is currently abandoned.<br />
<a class="onebox" href="https://nocturne-xyz.gitbook.io/nocturne" rel="noopener nofollow ugc" target="_blank">https://nocturne-xyz.gitbook.io/nocturne</a></p>
<h2><a class="anchor" href="https://ethresear.ch#p-51030-the-privacy-audit-dilemma-facing-privacy-coins-1" name="p-51030-the-privacy-audit-dilemma-facing-privacy-coins-1"></a>The Privacy-Audit Dilemma Facing Privacy Coins</h2>
<h4><a class="anchor" href="https://ethresear.ch#p-51030-how-tornado-cash-achieves-privacy-protection-2" name="p-51030-how-tornado-cash-achieves-privacy-protection-2"></a>How Tornado Cash Achieves Privacy Protection</h4>
<p>At the core of Tornado Cash’s privacy capability is ZK technology, which enables proof of ownership without revealing user identities or transaction details. Tornado Cash’s main contracts, known as pools, are designed for deposit and withdrawal operations. Users deposit funds into a pool contract and receive an anonymous proof to use later for withdrawal, thereby obscuring the original source of funds.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-51030-how-privacy-protections-can-facilitate-illicit-activities-3" name="p-51030-how-privacy-protections-can-facilitate-illicit-activities-3"></a>How Privacy Protections Can Facilitate Illicit Activities</h4>
<p>This anonymity makes Tornado Cash a favored tool for money laundering. Several documented cases illustrate how malicious actors have leveraged Tornado Cash’s anonymity to launder stolen funds, often evading regulatory scrutiny. Criminals have effectively obscured the money trail, making it difficult for law enforcement to track illicit transactions.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-51030-the-us-treasury-sanctions-on-tornado-cash-4" name="p-51030-the-us-treasury-sanctions-on-tornado-cash-4"></a>The U.S. Treasury Sanctions on Tornado Cash</h4>
<p>In August 2022, the U.S. Treasury’s Office of Foreign Assets Control (OFAC) sanctioned Tornado Cash, adding its associated USDC and ETH addresses to the Specially Designated Nationals (SDN) list, barring U.S. residents from using the service. The Treasury cited Tornado Cash’s role in numerous decentralized finance (DeFi) hacks, where individuals and groups allegedly laundered over $7 billion worth of cryptocurrency through the platform since its inception in 2019.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51030-future-evolution-of-privacy-transactions-selective-auditing-as-a-path-forward-5" name="p-51030-future-evolution-of-privacy-transactions-selective-auditing-as-a-path-forward-5"></a>Future Evolution of Privacy Transactions: Selective Auditing as a Path Forward</h3>
<p>As privacy solutions evolve, selective auditing features may become standard, enabling both anonymity for users and transparency for regulators. For example, Japan’s recent crackdown on a Monero laundering operation involving over 100 million yen highlights the global regulatory push for compliance in privacy-preserving systems.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51030-balancing-anonymity-and-auditability-6" name="p-51030-balancing-anonymity-and-auditability-6"></a>Balancing Anonymity and Auditability</h3>
<h4><a class="anchor" href="https://ethresear.ch#p-51030-the-role-of-zero-knowledge-technology-7" name="p-51030-the-role-of-zero-knowledge-technology-7"></a>The Role of Zero-Knowledge Technology</h4>
<p>Zero-knowledge proofs (ZKPs) are central to maintaining anonymity in the cryptocurrency space. By proving information without revealing it, ZKPs provide a basis for private transactions. However, purely anonymous systems can pose regulatory challenges. Recent innovations in ZK technology, like “partially decryptable zero-knowledge proofs” or Selectively Auditable Zero-Knowledge Proofs (SA-ZKPs), offer a promising balance between privacy and auditability.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-51030-the-sa-zkp-algorithm-8" name="p-51030-the-sa-zkp-algorithm-8"></a>The SA-ZKP Algorithm</h4>
<p>The SA-ZKP algorithm comprises the following components:</p>
<ol>
<li><strong>Commitment Scheme</strong> <span class="math">C=(CKeygen,Commit,COpen)C = (CKeygen, Commit, COpen)C=(CKeygen,Commit,COpen)</span>: Establishes a commitment to private data, allowing it to be used in proofs without revealing it.</li>
<li><strong>Zero-Knowledge Proof</strong> <span class="math">Σ=(K,P,V)\Sigma = (K, P, V)Σ=(K,P,V)</span>: Allows verifiable proof of commitment without disclosing the committed data.</li>
<li><strong>Trapdoor Generation</strong>: Creates a cryptographic “trapdoor” to enable selective auditability.</li>
<li><strong>Selective Decryption Process</strong>: Allows authorized entities to selectively decrypt committed data for regulatory auditing.</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-51030-regulated-tornado-cash-workflow-with-sa-zkp-9" name="p-51030-regulated-tornado-cash-workflow-with-sa-zkp-9"></a>Regulated Tornado Cash Workflow with SA-ZKP</h3>
<p>Applying the SA-ZKP algorithm to a regulated version of Tornado Cash could create a privacy-compliant framework with selective auditability:</p>
<ol>
<li><strong>Regulator Registration (Trapdoor Generation)</strong>: Regulators register with the network to gain access to audit permissions through a cryptographic trapdoor.</li>
<li><strong>Transaction Flow</strong>: Users deposit funds anonymously, with cryptographic commitments created for auditing if necessary.</li>
<li><strong>Audit Process (Selective Decryption)</strong>: In cases of suspicious activity, regulators can selectively decrypt transaction data to investigate without compromising the privacy of all users.</li>
</ol>
<p>By integrating SA-ZKP with Tornado Cash’s core operations, we can achieve a dual objective: respecting user privacy while empowering regulatory authorities with necessary oversight capabilities.</p>
<p><strong>If I were to launch a new version of a mixer, where I would decrypt specific transaction proofs for law enforcement using a trapdoor key when requested, would you still use this mixer? Why or why not?</strong></p>
            <p><small>4 posts - 2 participants</small></p>
            <p><a href="https://ethresear.ch/t/censorable-tornado-cash/20920">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sun, 03 Nov 2024 13:52:56 +0000</pubDate>
</item>
<item>
<title>Full DAS Sampling Analysis</title>
<link>https://ethresear.ch/t/full-das-sampling-analysis/20912</link>
<guid>https://ethresear.ch/t/full-das-sampling-analysis/20912</guid>
<content:encoded><![CDATA[
<div> 关键词：数据可用性采样、去中心化、容错能力、恶意攻击、网络条件

总结:
本文分析了在全面实施数据可用性采样（DAS）的情景下，以太坊网络如何在相对较低的网络假设下实现高效且高度去中心化的快速采样。网络表现出对大规模相关故障的鲁棒性，即使超过三分之一的节点无法参与采样过程也能正常运行。然而，对于拥有不诚实大多数（如90%）的大型协同攻击，当前方案仍面临挑战，需要采用其他技术来应对。

文章介绍了在网络中，节点通过其节点ID和保管大小确定性地确定它们应负责的数据行和列。随后讨论了数据分散过程，其中节点利用GossipSub通道从区块构建者下载所需保管的部分数据，并假设只传输每个行的一半数据，另一半则通过Reed-Solomon编码现场重建。

对于采样过程，文章假设有轻量级通信协议可以支持短暂连接，使节点能够查询其同行是否持有特定块的一部分进行验证，而不必完整下载整个区块。研究发现，在给定的网络参数下（例如节点的直接连接数及其保管数据规模），节点可以在一跳或两跳范围内覆盖大量单元格，从而快速完成采样任务。

此外，研究还探讨了理想情况（大部分节点正常运行）、关联故障情况下（部分节点失效）以及恶意多数攻击下的网络性能。尽管在大规模协同攻击中，网络的表现显著恶化，但该工作为未来研究提供了基线和一些可调整的参数，以在安全性与去中心化之间寻找更好的平衡点。 <div>
<p>This work was done by the <a href="https://codex.storage/" rel="noopener nofollow ugc">Codex team</a>, and we would like to thank <a class="mention" href="https://ethresear.ch/u/dankrad">@dankrad</a>, <a class="mention" href="https://ethresear.ch/u/matt">@matt</a>, <a class="mention" href="https://ethresear.ch/u/nashatyrev">@Nashatyrev</a>, <a class="mention" href="https://ethresear.ch/u/oascigil">@oascigil</a>, <a class="mention" href="https://ethresear.ch/u/fradamt">@fradamt</a>, <a class="mention" href="https://ethresear.ch/u/pop">@pop</a> and <a class="mention" href="https://ethresear.ch/u/srene">@srene</a> for their feedback and contributions to this work.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51018-tldr-1" name="p-51018-tldr-1"></a>TL;DR</h1>
<p>This work attempts to answer most of the questions raised in the original PeerDAS post regarding sampling, data custody, network requirements, and honest byzantine ratios.</p>
<ul>
<li>Under normal conditions and relatively low networking assumptions, the network can quickly perform sampling while guaranteeing high decentralization.</li>
<li>The network is robust enough to absorb large correlated failures in which more than a third of nodes do not contribute to the sampling process.</li>
<li>Large coordinated attacks of a dishonest majority (90%) are still a challenge that need to be addressed with other techniques.</li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#p-51018-introduction-2" name="p-51018-introduction-2"></a>Introduction</h1>
<p>The Ethereum network has been running with EIP4844 since the DenCun (Deneb-Cancun) fork on March 13th, 2024. In the current conditions, nodes have to download a maximum of 6 blobs, each 128KB size. When full DAS comes to life, we expect much more data to be produced at every slot. The <a href="https://blog.codex.storage/data-availability-sampling/" rel="noopener nofollow ugc">current planned structure</a> is a 2D matrix of 512 rows by 512 columns (after erasure coding), in which each cell is 560 bytes in size (including KZG proofs). That makes each row/column have a size of 280 KB for a total matrix size of 140MB. Of course, these numbers can change, but we will use this estimate for this analysis.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51018-assumptions-3" name="p-51018-assumptions-3"></a>Assumptions</h1>
<p>Full DAS includes two main components: data-sharding and sampling. Data-sharding refers to the fact that the nodes in the network are not required to download the entire block/blobs but only a part of it. Sampling is the process by which nodes verify whether a block is available without downloading the whole block.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51018-custody-4" name="p-51018-custody-4"></a>Custody</h2>
<p>Before sharding the data into rows and columns, dispersing it over the P2P network, and sampling it, we must explain how nodes know what data they are responsible for. Nodes use their nodeID and custody_size to derive, deterministically, which rows and columns they need to have custody of. The custody_size is the number of rows and columns a node wants custody of, and it is transmitted inside the Ethereum node records (ENRs) during the discovery process. More on this later. For instance, a node that wants to have custody of 16 rows and columns will generate a SHA-256 hash of its own nodeID, then cut this 256-bit hash into 25 segments of 10-bit (discarding the last 6 bits), and those 10-bit numbers (from 0 to 1024) will be the list of rows and columns the node must have custody of. In this case, 25 is not enough because we need 32 (16 rows and 16 columns), then we simply increment the nodeID by one, SHA-256(nodeID+1), to get another 25 row/column IDs. This process can be repeated as many times as necessary. If any ID is repeated, we simply disregard it and take the next one until we reach our quota. This process needs to be done only once when the node is initialized. Please note that any node can do the same computation for all its peers because it has the nodeID and custody_size of all of them. Therefore, all nodes in the network know which data is under the custody of all their respective peers. This algorithm is just an example of implementing deterministic custody; many other options exist.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51018-data-dispersal-5" name="p-51018-data-dispersal-5"></a>Data Dispersal</h2>
<p>For data dispersal, we assume that nodes download the part of the data they need to have custody of, from the block builder. They download the data using GossipSub channels in the same way they are used today for attestations, blocks, aggregations, etc. We assume a total of 1024 GossipSub channels (512 rows and 512 columns), and the nodes in the network subscribe to the channels required to satisfy their custody needs. This strategy relies on the premise that GossipSub should scale efficiently to over a thousand channels, which has never been tested on a production network like Ethereum Mainnet. Nonetheless, it is <a href="https://ethresear.ch/t/fulldas-towards-massive-scalability-with-32mb-blocks-and-beyond/19529">not overly optimistic</a> to assume this can be the case in the near future. We also assume that nodes do not change their custody set frequently, only when they restart the node with a new nodeID, so they remain subscribed to these GossipSub channels for extended periods (e.g., months). Also, we assume that only half of the row/column is sent over the network, and the other half is reconstructed on the fly using Reed-Solomon encoding. This means that a node that wants to download the entire block (140 MB) only needs to download 256 rows, and only half of the row is received (32 MB). The node can then extend the received 256 half rows horizontally and then generate the missing 256 rows by extending the data vertically. In other words, if the node has custody of 256 or more rows/columns, it can regenerate everything from only 32 MB of data.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51018-sampling-6" name="p-51018-sampling-6"></a>Sampling</h2>
<p>For sampling, we assume that nodes can query each other through a lightweight communication channel (e.g., new discovery protocol, lightweight libp2p, etc) in which one can establish ephemeral connections that only last for a couple of seconds. Such a protocol does not exist today, but it is not unrealistic to believe we can implement this rather quickly. Most of the networking components already exist today. Given that the custody set is computed deterministically from the nodeID, any node in the network can know what rows and columns are in the custody set of its peers.</p>
<p>In addition, we also assume a P2P network similar to the current Ethereum network, which has about ten thousand nodes, and we also assume that all clients can handle a couple hundred peer connections in parallel without overhead issues, which seems realistic given that consensus layer (CL) clients today can easily maintain connections with about a hundred peers.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51018-objective-7" name="p-51018-objective-7"></a>Objective</h1>
<p>In this study, we analyze how difficult it would be to sample a block in full DAS using the network assumptions given above. Therefore, we model a P2P network in which every node has a certain number of peers P, and a custody set of C rows and columns of the 2D matrix. Then, we investigate how these two parameters impact the speed at which sampling is done.</p>
<p>In particular, we look at the number of hops necessary for sampling. For instance, when a node wants to sample a given cell, it looks around its peers to find who has custody of the row or column of that cell and then sends the query. If none of its peers has that cell in custody, it sends the query to a random peer. The peer receiving the query cannot send the cell, therefore, it will answer with a list of its peers that have it in their custody set. The original node can now send the query to any of those peers, resulting in a two-hop sampling process. The same procedure can be repeated as deep as needed (H hops) until finding a node with the cell in its custody set.</p>
<p>We give this concept the name of sampling horizon. We call the level 1 horizon, the set of cells that a node can “see” by querying his direct (one-hop) peers. In the figure above, the level 1 horizon of the blue node is represented by the cells of the 2D matrix that are in the custody set of the nodes in green. The level 2 horizon is the set of cells a node can “see/sample” from its direct peers and their direct peers (up to two hops), i.e., the cells in the custody of the green and orange nodes in the figure.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/a/aaa3d0865b14d25fdc32dd17407b709d89da28d6.png" title=""><img alt="" height="345" src="https://ethresear.ch/uploads/default/optimized/3X/a/a/aaa3d0865b14d25fdc32dd17407b709d89da28d6_2_660x345.png" width="660" /></a></div><p></p>
<p>The concept of sampling horizon, as defined in this work, is closely related to the degrees of separation from network theory. For instance, some studies predict that all humans are connected by <a href="https://en.wikipedia.org/wiki/Six_degrees_of_separation" rel="noopener nofollow ugc">six degrees of separation</a>. However, it is essential to distinguish between the two concepts: degrees of separation is the metric that defines the distance between two nodes in the network, while the sampling horizon, as described here, is the set of cells a node can see within x degrees of separation. This can also be described as the set union of custody of nodes within x degrees of separation.</p>
<p>Given all this, we try to answer the question: how much cell coverage do we get on the level 1 horizon, level 2 horizon, and so on for a specific network configuration? This would depend mainly on two parameters: the number of peers P of the nodes and their custody size C. Let’s assign some values to them.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51018-methodology-8" name="p-51018-methodology-8"></a>Methodology</h1>
<p>In this section, we explain how we assign values to the custody size and the number of peers.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51018-flat-custody-9" name="p-51018-flat-custody-9"></a>Flat Custody</h2>
<p>Custody refers to the responsibility of nodes for storing specific data partitions (rows and columns) within the decentralized network. Flat custody is an approach where each node is assigned an equal number of rows and columns to its custody set, regardless of how many validators the node has. For example, we could set the custody to be two rows and two columns (C=2) for all the nodes in the network and then evaluate how much cell coverage we get at horizon level 1, level 2, and so on.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/6/268dcf6804a2716ffc73a3f2b9c5a0728e32f3a2.png" title=""><img alt="" height="359" src="https://ethresear.ch/uploads/default/optimized/3X/2/6/268dcf6804a2716ffc73a3f2b9c5a0728e32f3a2_2_512x359.png" width="512" /></a></div><p></p>
<p>The figure above shows the percentage of cells a node can see at horizon level 1, assuming C=2 and for different values for P (50, 100, and 150). For every node in the entire P2P network, we compute the percentage of cells from the entire 2D matrix that it can see in its level 1 horizon. The figure shows the median, the 1st and 3rd quartiles, the whiskers, and the outliers. As we can see, the more peers a node has, the more extensive the level 1 horizon coverage becomes, up to 70% for P=150 peers. <a href="https://ethresear.ch/t/big-block-diffusion-and-organic-big-blocks-on-ethereum/17346">Block sizes impact data propagation</a>, so analyzing the amount of data nodes need to download is essential. In this scenario, all nodes need to store, and therefore download, 2<em>2</em>512*560 = 1MB of data at every slot, which is not far from the ~900KB a node has to download today under EIP-4844 for a block with six full blobs.</p>
<p>However, flat custody is not a fair data partitioning strategy because the network has many different types of nodes. Running a small node without validators at home is not the same than running a beefy node with hundreds of validators in the cloud. Those are two very different conditions because of the hardware they have, the network bandwidth they use, the amount of value they store, and the revenue they produce. Therefore, we propose another approach that considers the number of validators running in the node and the expected resources involved proportionally.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51018-validator-proportional-custody-10" name="p-51018-validator-proportional-custody-10"></a>Validator Proportional Custody</h2>
<p>Under the validator-proportional (VP) custody model, the custody allocation is based on the number of validators a node has. This ensures that nodes with more validators take on more responsibility, distributing the network load more efficiently according to node capacity.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-51018-custody-model-11" name="p-51018-custody-model-11"></a>Custody Model</h3>
<p>In VP custody, we define the total custody of a node with the following equation:</p>
<p><code>total_custody = min_custody + floor(val_per_node × val_custody)</code></p>
<p>where</p>
<ul>
<li>Minimum Custody (min_custody) is the minimum number of rows and columns every node should custody. This ensures that every node, regardless of its validators, takes on a base level of responsibility for data.</li>
<li>Validators Per Node (val_per_node) refers to the number of validators a node has. It is a property of the node. Different nodes have different values of val_per_node.</li>
<li>Custody Per Validator (val_custody) represents the number of rows and columns a validator in a node is assigned. It is a property of the network. All nodes in the network will have the same val_custody value. Note that this parameter can take a fractional value such as 0.5, and the nodes will add one row/column for every two validators they host.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-51018-model-parametrization-12" name="p-51018-model-parametrization-12"></a>Model Parametrization</h3>
<p>Now that we have a custody model, we need to give values to the parameters of that model.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-51018-validators-per-node-13" name="p-51018-validators-per-node-13"></a>Validators per node</h4>
<p>To model a network using validator-proportional custody, we need to know the distribution of validators across the network. The number of nodes in the Ethereum network today is <a href="https://monitoreth.io" rel="noopener nofollow ugc">well-known</a> thanks to several <a href="https://github.com/migalabs/armiarma" rel="noopener nofollow ugc">crawlers</a> that have been developed in the past. However, the number of validators per node is a more tricky issue. In our previous <a href="https://arxiv.org/abs/2404.02164" rel="noopener nofollow ugc">research work</a>, we used the past attestation topic subscription plus other extra data to infer the number of validators hosted in the nodes. According to that study, about 10% of the nodes had 64 or more validators, while 80% had none or just a few validators, and the heaviest node in the network hosts about 3000 validators. This work uses the same distribution of validators per node, as shown in the figure below. Please note that even if we have the list of the nodes in the network and estimate the distribution of the number of validators per node, we still cannot determine the exact number of validators each node has. Thus, in our simulations, we assign to each node, a random number of validators from the values observed in this given distribution. In this way, we can recreate the same distribution presented here.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/0/d0a71b133e6127cb40a4e86e6bbb86e099dda25f.png" title=""><img alt="" height="331" src="https://ethresear.ch/uploads/default/optimized/3X/d/0/d0a71b133e6127cb40a4e86e6bbb86e099dda25f_2_660x331.png" width="660" /></a></div><p></p>
<h4><a class="anchor" href="https://ethresear.ch#p-51018-custody-per-validator-14" name="p-51018-custody-per-validator-14"></a>Custody per validator</h4>
<p>To assign a value to the val_custody parameter, we can start by asking when a node should download the entire block. Let’s pick an arbitrary number, for example, 1 row/column per validator. From the moment the node downloads 256 rows, it can automatically regenerate the other half of the data using Reed-Solomon encoding, ending with the custody of the entire block, becoming a data availability (DA) provider. It is essential to note that in this scenario, the node will not need to download both rows and columns; either rows or columns is enough because it will have the entirety of the data after erasure coding. Then, let’s proceed with an economical analysis of a node with 256 validators. Such a node is securing ETH 8192, equivalent to over USD $20M+ as of today’s price. Therefore, it seems reasonable to assume that a node with that much capital has the economic resources and incentives to run that node in the very best hardware and network conditions. Such a node should be able to download and reconstruct every block quickly. Thus, we can set val_custody=1 so that a node with 256 validators will have custody of the entire block. This parameter can change if we want to increase/reduce the custody size of nodes and the number of DA providers in the network, as we will see in a later section.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-51018-minimum-custody-15" name="p-51018-minimum-custody-15"></a>Minimum custody</h4>
<p>We use a different approach to set a value to the min_custody parameter. In this case, the objective is to minimize the amount of data a small home node without validators needs to download and store. As we saw in the Flat Custody section, a node with a custody size of 2 rows and columns must download about 1MB of data per slot, which is reasonably close to the case of a block with six blobs today. Thus, for the remainder of this study, we set min_custody=2.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51018-number-of-peers-16" name="p-51018-number-of-peers-16"></a>Number of Peers</h2>
<p>In the preliminary results of the flat custody, we tested a different number of peers, from 50 to 150, which are realistic values today, as we have seen in <a href="https://migalabs.io/blog/post/ethereum-hardware-resource-analysis-update" rel="noopener nofollow ugc">previous studies</a>. However, one aspect of that simulation that could be more realistic is making the number of peers not the exact same value for all nodes. While most clients stay within the same range, different CL clients have different default values for the number of peers. Thus, we adapted the code of our simulator to give a range for the number of peers instead of a value. All nodes randomly choose a number of peers within the range given as a parameter. For example, we could study what happens when the number of peers ranges between 50 and 100, 100 to 150, and 150 to 200. We could further explore what happens when we increase these ranges, as we will see later.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/9/d983bc24ffa004d0e92a4d1d87bd7834f37c6974.png" title=""><img alt="" height="331" src="https://ethresear.ch/uploads/default/optimized/3X/d/9/d983bc24ffa004d0e92a4d1d87bd7834f37c6974_2_660x331.png" width="660" /></a></div><p></p>
<p>Thus, we study the sampling performance using these three ranges for the number of peers P and the same parameters for custody C as in the previous section (i.e., min_custody=2, val_custody=1). For example, we can ask one question: for one particular cell, how many direct peers (level 1) have custody of this cell? Then, we can repeat the same question for another cell, and another, and so on, until we have the answer for all the 262,144 cells in the block. Then, we can plot that distribution for the three different P ranges. The results are presented in the figure below.</p>
<p>We can see that on average, even for a connectivity range of 50 to 100 peers, most cells are under the custody of around ten directly connected peers (level 1). As we increase the number of peer connections, the level 1 horizon increases, thus increasing the reliability of the distributed network. For a range of 150 to 200 peers, the cells of the block can be found in about 33 direct peers on average, and all cells are under the custody set of at least 20 direct peers.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51018-evaluation-17" name="p-51018-evaluation-17"></a>Evaluation</h1>
<p>In this section, we perform several simulations to evaluate the sampling performance under multiple scenarios. We are interested in two key metrics:</p>
<ul>
<li>The level 1 horizon: This allows us to get an idea of the percentage of queries that can be answered quickly (1 hop). It also shows the network’s reliability level because it indirectly expresses the data redundancy in the network. For this metric, the higher, the better.</li>
<li>The amount of data downloaded by each node: This shows the network requirements that the different types of nodes should have to run a node. This is directly related to the level of decentralization of the network. For this metric, the lower, the better.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-51018-happy-case-18" name="p-51018-happy-case-18"></a>Happy Case</h2>
<p>The first scenario we look at is the “happy” case in which 99% of the nodes behave correctly and have no performance issues of any type, neither network data loss nor hardware failures. Using the proportional custody model and parameters defined above, the cell coverage of the level 1 horizon improves dramatically over the flat custody model presented previously, as shown in the figure below.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/5/355e49de5d6d3b6cacb5f4eefc6d659a48a93705.png" title=""><img alt="" height="331" src="https://ethresear.ch/uploads/default/optimized/3X/3/5/355e49de5d6d3b6cacb5f4eefc6d659a48a93705_2_660x331.png" width="660" /></a></div><p></p>
<p>All nodes can access all cells within one hop, and this is for all three ranges of the number of peers, except for a couple of outliers in the range of 50-100 peers. This is because, under the proportional custody model, nodes generally have larger custody sets and also due to the presence of DA providers —nodes with 256 or more validators— within their level 1 horizon.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/4/34f254e882e049369ef049442067d118f76793ed.png" title=""><img alt="" height="236" src="https://ethresear.ch/uploads/default/optimized/3X/3/4/34f254e882e049369ef049442067d118f76793ed_2_660x236.png" width="660" /></a></div><p></p>
<p>Regarding the amount of data that needs to be downloaded, we can see in the figure above that most nodes download less than 1 MB of data per slot. Only a few outliers, mostly DA providers, download over 2MBs of data up to the totality of the block (~32MBs). These results show that it is possible to perform fast sampling under ideal conditions while keeping the download needs for most nodes low. This is an ideal outcome because it guarantees high security and high decentralization. Now, let’s analyze what happens under more catastrophic scenarios.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51018-correlated-failures-19" name="p-51018-correlated-failures-19"></a>Correlated Failures</h2>
<p>In this section, we demonstrate what happens when there is a correlated failure in which a given feature is correlated within a large set of nodes, and this feature is the root cause of the failure. Thanks to our open-source network crawler at MigaLabs, we have detailed information about all the nodes in the consensus layer network, including their ISPs, geographical locations, and the CL clients they are using. This data enables us to selectively simulate the shutdown of specific groups of nodes based on those criteria.</p>
<p>We look at three scenarios regarding CL client diversity, geographical distribution, and type of hosting. For instance, if a majority CL client (i.e., Lighthouse) deploys a new release with a bug that renders the nodes incapable of syncing, we could see over 35% of the nodes go down in a very short time. The same could happen if the country with the highest node concentration (i.e., USA) suddenly bans crypto because of a new presidential candidate been elected, for example. A similar scenario could occur if the three larger cloud providers used in the Ethereum network (i.e., AWS, Hetzner, and OVH) decide to ban all crypto-related activity simultaneously. Thus, we simulate what happens when 35% of the nodes suffer from a correlated failure and stop providing samples and forwarding requests of any type.</p>
<p>Before analysing the reliability of the system, let us do a network bandwidth analysis. The amount of data that needs to be downloaded is not affected by the number of nodes failing. Hence, the bandwidth requirements stay the same. Downloading the data can take longer because many nodes will not forward anything on the Gossipsub channels. Therefore, more requests should be sent which has negligible bandwidth, but it has delay implications. This work focuses only on custody and sampling aspects; data dissemination is out of the scope of this article. For dispersal simulations, please refer to our <a href="https://arxiv.org/abs/2407.18085" rel="noopener nofollow ugc">previous paper</a>.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/7/c76192bab8cb6d5a0a29d447dbc8c412151d5fec.png" title=""><img alt="" height="331" src="https://ethresear.ch/uploads/default/optimized/3X/c/7/c76192bab8cb6d5a0a29d447dbc8c412151d5fec_2_660x331.png" width="660" /></a></div><p></p>
<p>Regarding the robustness of the sampling process, we can see in the results above that the network is highly reliable, even under such a catastrophic failure, and 99.9% of the nodes can perform fast sampling (1 hop) without any issues. The only outliers are some nodes in the 50 to 100 peers range that only see between 40% and 75% of the cells within their level 1 horizon. They will need to reach their level 2 horizon (2 hops) to satisfy their sampling needs, which means their sampling performance will be lower, but it does not mean they will necessarily fail the sampling process.</p>
<p>To understand how much those outliers can perturb the network, we compute how much time 75 sampling queries take for all the nodes in the network. To do this, we assign a sampling time for querying a level 1 node st1 and a level 2 node st2. We also set a timeout for when a node queries a malicious peer, and this peer does not answer. In this case, the node must launch a second query to another peer. Most nodes in the Ethereum network show an RTT of about 100ms. Therefore, we give our time parameters the following values: st1=100~200ms, st2=200~300ms, and to=500ms. Please note that all sampling queries are launched in parallel; the only queries executed sequentially are those launched after a timeout.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/a/9aacc7c4d25c28b5258d50657b9e2ad6242f3b46.png" title=""><img alt="" height="331" src="https://ethresear.ch/uploads/default/optimized/3X/9/a/9aacc7c4d25c28b5258d50657b9e2ad6242f3b46_2_660x331.png" width="660" /></a></div><p></p>
<p>In the results shown above, we can see that, even under this catastrophic failure, the large majority of nodes can perform their sampling duties within 4 seconds. Those taking more time are nodes that contacted multiple malicious nodes consecutively. These encouraging results indicate the possibility of reaching high security, even under highly correlated failures, without compromising decentralization.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51018-byzantine-attacks-20" name="p-51018-byzantine-attacks-20"></a>Byzantine Attacks</h2>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/0/8/08b97ba6af9984766fde06a13f11a3c335f94906.png" title=""><img alt="" height="331" src="https://ethresear.ch/uploads/default/optimized/3X/0/8/08b97ba6af9984766fde06a13f11a3c335f94906_2_660x331.png" width="660" /></a></div><p></p>
<p>Now, we want to take things to the extreme and simulate a coordinated attack in which a large majority of the nodes (90%) decide to behave maliciously by withholding the data and not answering/forwarding sampling requests. We simulated cases in which 60%, 70%, 80%, and 90% of the nodes in the network behave maliciously, but for brevity in this writeup, we only show results for the worst case: 90% of nodes are malicious, regardless of their number of validators. This means that even DA providers can be malicious and withhold data.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/1/a1e19fecbdeb24d8e7d362d8174cf4d30e4116dc.png" title=""><img alt="" height="331" src="https://ethresear.ch/uploads/default/optimized/3X/a/1/a1e19fecbdeb24d8e7d362d8174cf4d30e4116dc_2_660x331.png" width="660" /></a></div><p></p>
<p>In this scenario, we see a dramatic difference in the level 1 horizon. The distribution is catastrophic for all peer ranges. A massive proportion of the nodes have only a partial view of the block on level 1. Looking at level 2, things improve, primarily due to the presence of honest DA providers in their level 2. However, having an honest DA provider at reach does not guarantee good sampling performance because, with 90% of malicious nodes, it is difficult to reach an honest one by pure random trial. In this attack, we assume that even malicious nodes behave correctly for some time until they decide to launch the attack. Under those circumstances, finding an honest node becomes exceptionally challenging, even using reputation mechanisms.</p>
<p>Under these conditions, nodes will contact malicious nodes many times in a row, leading to long sampling times. We calculate the sampling time for all nodes for all three different peer ranges, and plot the distributions in the figure above. We see that the majority of the nodes take over 15 seconds to finish their sampling. However, in the Ethereum protocol, there are time constraints, and nodes cannot wait forever to decide whether a block is available or not. For this experiment, we set a hard final timeout fto, which represents the break time when, if the 75 sampling queries have not finished successfully, the nodes decide the block is not available. For this example, we set fto=12s, see the red dotted line in the figure.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/d/2dccc6b8b6dbada8bb8b796a544dea09d7bc7e4f.png" title=""><img alt="" height="331" src="https://ethresear.ch/uploads/default/optimized/3X/2/d/2dccc6b8b6dbada8bb8b796a544dea09d7bc7e4f_2_660x331.png" width="660" /></a></div><p></p>
<p>Under this scenario, only a few nodes manage to finish their sampling queries in time. An unexpected result of this experiment is that as we increase the number of peers, the distribution of the sampling duration widens up, and fewer nodes manage to finish sampling in time, as shown in the figure below. When we have many direct peers, we have a large pool of malicious nodes to whom we might inadvertently send a sampling query. This means that under such a large coordinated attack, having a large number of known peers does not help because the node is unaware of which ones are malicious and which ones are not. In these conditions, other techniques are necessary.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/7/c77dd55d92209a9804eb31b7fd77592b307a0de7.png" title=""><img alt="" height="257" src="https://ethresear.ch/uploads/default/optimized/3X/c/7/c77dd55d92209a9804eb31b7fd77592b307a0de7_2_660x257.png" width="660" /></a></div><p></p>
<h1><a class="anchor" href="https://ethresear.ch#p-51018-mitigating-malicious-majority-attacks-21" name="p-51018-mitigating-malicious-majority-attacks-21"></a>Mitigating Malicious Majority Attacks</h1>
<h2><a class="anchor" href="https://ethresear.ch#p-51018-increased-replication-22" name="p-51018-increased-replication-22"></a>Increased Replication</h2>
<p>One way to try to mitigate these types of attacks is by increasing the amount of data replication in the system. This can be done by increasing the validator custody so that nodes with validators have custody of more rows and columns, and this also lowers the bar to become a DA provider, which in turn increases the number of DA providers in the network. For instance, if we set val_custody=4, any node with 64 validators should become a DA provider, and there would be much more replication in the network. However, this is not enough to mitigate this attack, and our simulations show that only between 15% and 22% of the honest nodes see the block as available. In addition, this also increases the bandwidth requirements for all nodes, which is an undesirable effect of this strategy.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-51018-lossy-sampling-23" name="p-51018-lossy-sampling-23"></a>Lossy Sampling</h2>
<p><a href="https://ethresear.ch/t/lossydas-lossy-incremental-and-diagonal-sampling-for-data-availability/18963">Lossy sampling</a> is another strategy that allows nodes to tolerate one or several failed queries in the sampling process. This is particularly useful when one sample or a few samples are missing. However, lossy sampling is of little help for this massive coordinated attack. Under such conditions, most nodes do not even finish half of their requests; therefore, allowing a few failed queries will not absorb the Byzantine’s faults in any meaningful way.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51018-discussion-24" name="p-51018-discussion-24"></a>Discussion</h1>
<p>In this work, we have studied the sampling process under full DAS, considering a set of assumptions, mostly related to the network layer and a few others, such as the validator distribution. We have demonstrated that under fair networking conditions, DAS sampling is not only achievable but it can be performed rather fast. We have presented a number of parameters with which researchers can play to move the needle between decentralization and reliability. The study also shows how the network can absorb large correlated failures and still perform sampling reliably.</p>
<p>However, this work has several limitations:</p>
<ul>
<li>During the entire work, we assumed that nodes select peers randomly. In reality, nodes can tune their peers to improve their level 1 and level 2 horizons, which should lead to much more robust networking conditions. In some sense, the results presented in this work can be viewed as the baseline from which many improvements can be achieved by enhancing the peering algorithm.</li>
<li>We assume that when a node does not have a cell in its horizon level 1, then it contacts a node randomly to try to reach a potential level 2 node. There are other strategies that can be considered and that we have not considered in this work. For example, topicID routing could improve performance in this context.</li>
<li>The whole study was performed assuming that the nodes only know a small set of nodes and they sample only from that set. However, peer discovery algorithms can run in parallel, and nodes can store a rather large number of peers in their database. It is not impossible to contact thousands of peers while syncing and start the whole sampling process with a huge level 1 horizon.</li>
<li>This work assumes a rather static view in which the horizon of the nodes does not evolve. However, this is not realistic. As nodes discover other peers, their horizon is constantly evolving and with it the sampling performance.</li>
<li>We have shown results for only a network of 10K nodes and a specific validator distribution. However, both the size of the network and the validator distribution can change in the coming years. Thus, it is necessary to perform experiments under those conditions.</li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#p-51018-conclusions-25" name="p-51018-conclusions-25"></a>Conclusions</h1>
<p>This work demonstrates that under certain realistic networking assumptions, it is possible to perform fast sampling at scale. The study also shows that with the presented strategies, the network can even absorb large correlated failures. However, malicious majority attacks remain a challenge and should be further studied.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51018-acknowledgments-26" name="p-51018-acknowledgments-26"></a>Acknowledgments</h1>
<p>This research was done with the support of the Ethereum Foundation under grant FY24-1533.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/full-das-sampling-analysis/20912">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sat, 02 Nov 2024 17:02:23 +0000</pubDate>
</item>
<item>
<title>GossipSub Topic Observation (proposed GossipSub 1.3)</title>
<link>https://ethresear.ch/t/gossipsub-topic-observation-proposed-gossipsub-1-3/20907</link>
<guid>https://ethresear.ch/t/gossipsub-topic-observation-proposed-gossipsub-1-3/20907</guid>
<content:encoded><![CDATA[
<div> 关键词：主题观察、带宽消耗、GossipSub、稳定性、可扩展性

总结:
本文提出了主题观察的概念，旨在减少GossipSub协议中由于网状结构导致的带宽放大消费问题。主题观察允许节点仅被通知有新消息而无需实际接收完整消息。当一个节点想要观察某个主题时，它需要告知订阅该主题的其他节点，这些订阅节点会在接收到新消息时向观察节点发送通知。观察节点仅消费而不贡献网络稳定性，因此对网络稳定性的维持要求有足够的订阅节点存在。此设计可能带来的问题在于，若太多相邻节点请求观察，可能会增加发送通知的开销，但目前假设消息ID的通知开销可以忽略不计。为实现这一功能，文章提出两个新的控制消息：OBSERVE和UNOBSERVE，用于请求和取消主题观察。在接收到OBSERVE请求后，邻节点会在接收到新消息时发送修改后的IHAVE消息作为通知，此时的IHAVE消息不再仅在心跳时发送，而是可以在接收到消息后立即发送，观察节点收到IHAVE后无需再发送IWANT。 <div>
<p><em>Authors: <a href="https://github.com/ppopth" rel="noopener nofollow ugc">pop</a></em></p>
<p><em>tldr; topic observation enables the nodes to get notified when there is a new message in<br />
a topic without actually receiving the actual message.</em></p>
<p>This proposal enables you to tell your peers to notify you when there is a new message in the topic without consuming the bandwidth to download the actual message. When you do this, you are called an observing node in that topic.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51008-motivation-1" name="p-51008-motivation-1"></a>Motivation</h1>
<p>Topic observation is motivated by the amplification factor on bandwidth consumption due to the mesh degree of GossipSub. When you subscribe to a topic, you would need to download as many copies of messages as the mesh degree. For example, if the mesh degree is 8, you would roughly download 8 copies.</p>
<p>We have <code>IDONTWANT</code> implemented in GossipSub 1.2 which will reduce the number of copies you will download, but it doesn’t guarantee exactly how many.</p>
<p>When you observe a topic, you won’t receive any message. You will only get notified when there is a new message. If you want the actual message, you can request the message from the peer that notifies you first, so you will download only one copy. However, the message request part is out-of-scope of this proposal. This proposal only deals with notifications.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51008-high-level-design-2" name="p-51008-high-level-design-2"></a>High-level design</h1>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/7/b77e73993faca2c60d123d3700f2e20d3cbf3f24.png" title="output"><img alt="output" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/b/7/b77e73993faca2c60d123d3700f2e20d3cbf3f24_2_663x500.png" width="663" /></a></div><p></p>
<p>When you want to observe a topic, you would need to find subscribing nodes in the topic and tell them you want to observe the topic. Later, when there is a new message, those subscribing nodes will notify you.</p>
<p>Let’s see examples in the figure, node 11 is observing the topic. Node 1, 9, and 10 will notify node 11 when there are new messages. Similarly, node 4 and 5 will notify node 12.</p>
<p>Notice that the relationship is unidirectional rather then bidirectional like mesh connections.</p>
<p>You can also tell your subscribing peers when you don’t want to observe the topic anymore. That is when you want to unobserve it.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51008-stability-3" name="p-51008-stability-3"></a>Stability</h1>
<p>Notice that observing nodes only receive notifications. They neither send notifications nor forward messages. In other words, they only consume, not contribute. So, they are only on the border of the network (as shown in the figure) and don’t provide any stability to the network. It means that there must be enough subscribing nodes in the network to provide stability.</p>
<p>However, the good side of this is that the churn rate of observing nodes doesn’t matter at all. Nodes can observe and unobserve as often as they want.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51008-scalability-4" name="p-51008-scalability-4"></a>Scalability</h1>
<p>Currently, when your peers want to observe the topic and tell you to notify, you are obligated to notify them without the option to decline. This has a downside that if too many of your peers want to observe, you will have too much overhead to do the job.</p>
<p>However, the notifications consist only of messages ids which we now assume to be negligible. You may argue that if the number of observing peers is high enough, the total size of notifications will be significant. That’s true, but for the first iteration of the design, we should make this assumption to make the protocol simple.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-51008-protocol-messages-5" name="p-51008-protocol-messages-5"></a>Protocol messages</h1>
<p>There are two new control messages: <code>OBSERVE</code> and <code>UNOBSERVE</code>.</p>
<p>You send <code>OBSERVE</code> to your peer when you want to observe a topic and have that peer notify you.</p>
<p>You send <code>UNOBSERVE</code> to your peer to tell that you don’t observe the topic anymore.</p>
<p>After sending <code>OBSERVE</code> to your peer, the peer will send <code>IHAVE</code> to you as a notification, when there is a new message in the topic.</p>
<p>However, <code>IHAVE</code> in this proposal is different from the previous GossipSub versions. In the previous versions, <code>IHAVE</code> is sent only at the heartbeats, while in this version, it can also be sent right after peers receives messages. Previously, you can send <code>IWANT</code> after receiving <code>IHAVE</code>, but in topic observations, you aren’t expected to send <code>IWANT</code>, since <code>IHAVE</code> serves only as a notification.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/gossipsub-topic-observation-proposed-gossipsub-1-3/20907">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sat, 02 Nov 2024 06:34:49 +0000</pubDate>
</item>
<item>
<title>Curvy Protocol - Dual Key Stealth Address Protocol based on Elliptic Curve Pairing</title>
<link>https://ethresear.ch/t/curvy-protocol-dual-key-stealth-address-protocol-based-on-elliptic-curve-pairing/20897</link>
<guid>https://ethresear.ch/t/curvy-protocol-dual-key-stealth-address-protocol-based-on-elliptic-curve-pairing/20897</guid>
<content:encoded><![CDATA[
<div> 关键词：Curvy协议、隐形地址协议、椭圆曲线配对、DKSAP、优化

总结:
Curvy协议是一种基于椭圆曲线配对的隐形地址协议，旨在克服现有协议中的一些缺点。与使用共享密钥计算的DKSAP不同，Curvy协议不再使用共享密钥哈希值来计算地址，而是仅用于视图标签，从而提高了安全性。此外，通过使用Go语言和gnark-crypto库进行实现，针对交易接收方生成新隐形地址的时间敏感部分进行了优化。实验结果对比显示，Curvy协议（采用配对友好的BN254曲线及最优Ate配对）在处理不同数量的公告时，其查找时间性能优于DKSAP的实现。 <div>
<p>With this post, I would like to introduce you to the rationale behind the Curvy protocol, a pairing-based stealth address protocol. You can find a detailed description of the Curvy protocol in our paper <a href="https://arxiv.org/pdf/2312.12131" rel="noopener nofollow ugc">Elliptic Curve Pairing Stealth Address Protocols</a>. For those who prefer reading a blog instead of the paper, you can check out our <a href="https://3327.io/curvy-protocol-for-fast-anonymous-transactions-on-ethereum/" rel="noopener nofollow ugc">blog</a> on this topic. I will try to answer all your questions in the comments!</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50984-introduction-1" name="p-50984-introduction-1"></a>Introduction</h2>
<p>We can use different versions of the stealth address protocol. Some of them, such as DKSAP (Dual-Key Stealth Address Protocol), are based on a cryptographic mechanism similar to the Diffie-Hellman method, i.e. the private and public keys for the stealth address contain a shared secret that can be computed by both the sender and the recipient. DKSAP is explained in detail in <a href="https://vitalik.eth.limo/general/2023/01/20/stealth.html" rel="noopener nofollow ugc">Vitalik’s blog</a>, and Vitalik also suggested SAPs based on pairing as one of the directions of stealth address research. Stealth addresses can be generated using another cryptographic method: elliptic curve pairing. In the papers <a href="https://ieeexplore.ieee.org/document/9724375" rel="noopener nofollow ugc">EDKSAP: Efficient Double-Key Stealth Address Protocol in Blockchain</a> and <a href="https://dl.acm.org/doi/10.1145/3321408.3321573" rel="noopener nofollow ugc">A New Stealth Address Scheme for Blockchain</a>, SAPs based on pairing are presented. In the first paper, there is a gap that allows the sender and the person holding the viewing key to pair to obtain the private key of the stealth address. In the second paper, the private key of the stealth address is such that it can be computed by the sender without teamwork. We propose the Curvy protocol, which is based on elliptic curve pairing and overcomes these shortcomings.</p>
<p>Some of the optimizations we have made in the Curvy protocol (the reader can find more about them in our paper) can also be used to optimize DKSAP, while others cannot. The peculiarity is that the hash of the shared secret is not used to calculate the address in the Curvy protocol, but only for the view tag. Therefore, we can also use two bytes of the hash of the shared secret as the view tag, while using the same view tag would compromise the security of DKSAP. Note that in the Curvy protocol, we can also use the entire hash of the shared secret for the view tag without compromising security of the user.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50984-curvy-protocol-2" name="p-50984-curvy-protocol-2"></a>Curvy protocol</h2>
<p>Let <span class="math">e: \mathbb{G}_1 \times \mathbb{G}_2 \rightarrow \mathbb{G}_T </span> be elliptic curve pairing. Let <span class="math">g_e</span> be the generator point of the elliptic curve Secp256k1, and let <span class="math">g_1</span> and <span class="math">g_2</span> be the generator points of the subgroups <span class="math">\mathbb{G}_1</span> and <span class="math">\mathbb{G}_2</span> of the elliptic curve BN254.</p>
<p>Curvy protocol is shown in a detail in the following figure:<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/2/b28ce7c37a600294f313116d95fda9abcc921a6d.jpeg" title=""><img alt="" height="375" src="https://ethresear.ch/uploads/default/optimized/3X/b/2/b28ce7c37a600294f313116d95fda9abcc921a6d_2_471x375.jpeg" width="471" /></a></div><p></p>
<h2><a class="anchor" href="https://ethresear.ch#p-50984-implementation-results-3" name="p-50984-implementation-results-3"></a>Implementation results</h2>
<p>Since transaction speed is crucial from the users’ perspective, we aimed to create a protocol that meets their needs. For this reason, it was important to optimize the most sensitive part (which takes the longest) - the calculation of the new stealth address by the recipient of the transaction.</p>
<p>In our implementation of the Curvy protocol, we used the Go programming language together with the gnark-crypto library. The figure below compares the search times of the ephemeral public key registry for different numbers of announcements (5000, 10000, 20000, 40000 and 80000) of the Curvy protocol (with pairing-friendly curve BN254 and optimal Ate pairing) and the implementation of DKSAP from <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10426757" rel="noopener nofollow ugc">BaseSAP</a>. The values of the private keys and the size of the view tag (5.5 bytes) from BaseSAP were used.<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/1/41006e850082c189fb4f2ccb04dfc307d9566140.png" title=""><img alt="" height="300" src="https://ethresear.ch/uploads/default/optimized/3X/4/1/41006e850082c189fb4f2ccb04dfc307d9566140_2_517x300.png" width="517" /></a></div><p></p>
<p>Useful links for further exploration:</p>
<ul>
<li><a href="https://arxiv.org/pdf/2312.12131" rel="noopener nofollow ugc">Our paper</a></li>
<li><a href="https://3327.io/curvy-protocol-for-fast-anonymous-transactions-on-ethereum/" rel="noopener nofollow ugc">Our blog</a></li>
<li><a href="https://github.com/0x3327/ecpdksap?tab=readme-ov-file" rel="noopener nofollow ugc">GitHub repo</a></li>
</ul>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/curvy-protocol-dual-key-stealth-address-protocol-based-on-elliptic-curve-pairing/20897">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 30 Oct 2024 13:58:17 +0000</pubDate>
</item>
<item>
<title>Hybrid Rollups: Optimistic Performance meets ZK Security</title>
<link>https://ethresear.ch/t/hybrid-rollups-optimistic-performance-meets-zk-security/20896</link>
<guid>https://ethresear.ch/t/hybrid-rollups-optimistic-performance-meets-zk-security/20896</guid>
<content:encoded><![CDATA[
<div> 关键词: 混合rollup、乐观主义rollup、零知识证明、安全性、性能

总结:
本文探讨了混合rollup的概念，它是将乐观主义rollup和零知识证明（ZK）技术相结合的一种新型扩容解决方案。混合rollup在保持高吞吐量的同时，利用ZK有效性证明增强了安全性，简化并优化了挑战机制。与纯乐观主义rollup相比，它提供了更短的挑战期和更快的最终性，并在大多数情况下无需生成有效性证明，从而节省成本。此外，混合rollup作为向全ZK rollup过渡的桥梁，允许rollup逐渐适应ZK技术的进步。综上所述，混合rollup在保障安全性和提升性能方面取得了平衡，并具有一定的成本优势。 <div>
<p><em>Special thanks to <a href="https://x.com/sam_battenally" rel="noopener nofollow ugc">Sam Battenally</a>, <a href="https://ethresear.ch/u/hai-rise/summary">Hai Nguyen</a>, <a href="https://www.succinct.xyz/" rel="noopener nofollow ugc">Succinct Labs</a>, and <a href="https://norswap.com/" rel="noopener nofollow ugc">Norswap</a> for feedbacks and reviews.</em></p>
<h1><a class="anchor" href="https://ethresear.ch#p-50983-motivation-1" name="p-50983-motivation-1"></a>Motivation</h1>
<p>Many high-performance rollups adopted an optimistic design in the first place. This was primarily driven by the simplicity of the optimistic approach and the limitations of zero-knowledge (ZK) proving technology. At that time, simulating an EVM machine with ZK was not feasible, and ZK proving was unable to meet the desired throughput demands. Optimistic rollups, on the other hand, offered a simpler and more scalable solution.</p>
<p>However, the optimistic approach has its drawbacks. To maintain security, it must rely on fraud challenges. The current fraud challenge process can be complex and time-consuming, requiring significant interactions or unfriendly to challengers.</p>

<p>With recent advancements in the ZK ecosystem, we are now able to prove an EVM block in an order of minutes. This article explores the possibility of transitioning to a hybrid rollup model that combines the best aspects of both optimistic and ZK rollups to address the limitations of each.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50983-summary-2" name="p-50983-summary-2"></a>Summary</h2>
<p>Here’s a comprehensive overview of the key properties.</p>
<ul>
<li><strong>Optimistic Foundation with ZK Security.</strong> At its core, a hybrid rollup operates like an optimistic rollup, but incorporates ZK validity proofs for enhanced security. This unique combination allows for high throughput while maintaining robust security measures.</li>
</ul>

<ul>
<li><strong>Efficient Challenge Mechanism.</strong> In the event of a disputed transaction, the system employs ZK validity proofs. The hybrid design is more straightforward and has been shown to be robust in various ZK rollup implementations, offering a higher degree of reliability compared to complex fraud proof systems.</li>
<li><strong>Cost-Effective.</strong> The hybrid model maintains the cost-effectiveness of optimistic rollups for end-users. Under normal circumstances, users do not bear the cost of generating validity proofs, keeping transaction fees competitive. Furthermore, operational costs also stay low in the case of low-to-no traffic.</li>
<li><strong>Balancing Performance and Security.</strong> By leveraging the high throughput capabilities of optimistic rollups and the security assurances of ZK proofs, the hybrid design achieves a balance that caters to both performance needs and security requirements.</li>
<li><strong>Flexible Proving Strategy.</strong> The system only generates ZK proofs when challenges occur, reducing the computational overhead associated with constant proof generation in full ZK rollups.</li>
<li><strong>Transitional Technology.</strong> The hybrid design serves as a stepping stone towards a full ZK rollup implementation. It allows rollups to gradually adapt to advancements in ZK technology while maintaining current operational efficiency.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50983-optimistic-vs-zk-rollups-3" name="p-50983-optimistic-vs-zk-rollups-3"></a>Optimistic vs ZK Rollups</h2>
<p>Anyone can examine the data on a rollup and spot any mistakes. If even one person finds a problem, they can alert everyone else. This means the system is secure as long as at least one person is honest. In their simplest form, rollups work by</p>
<ol>
<li>Having <em>all input data published</em>, allowing anyone to read it.</li>
<li>Allowing anyone to <em>challenge against invalid behaviors</em>.</li>
</ol>
<p>The latter makes rollups secure and robust even if they are operated by centralized sequencers. Rollups are divided into two categories based on how they handle the second component.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50983-optimistic-rollup-4" name="p-50983-optimistic-rollup-4"></a>Optimistic Rollup</h3>
<p>An optimistic rollup (ORU) assumes off-chain transactions are valid when publishing a new state transition on chain. While an ORU significantly improves transaction throughput by using the same state machine as the L1, it relies on a mechanism known as <strong>fraud proofs</strong> to ensure the integrity of the system. A fraud proof is essentially a mechanism that allows users to challenge the validity of a state transition on the rollup chain.</p>
<p>After a new state is published, there is a long challenge window (usually 7 days) for anyone to attest its validity. If a user believes that a transaction has been processed incorrectly, they can submit a fraud challenge to the main Ethereum chain. Otherwise, after the challenge window, this state is considered to be finalized.</p>
<p>The design choice of fraud proofs can also divide rollups into two categories.</p>
<ul>
<li><strong>Non-interactive (or Re-executing) Fraud Proofs.</strong> In case of a dispute, the L1 contract would emulate the execution of all transactions in the relevant state, to see whether the outcome matches the given claim.
<ul>
<li>This incurs significant gas costs because there are potentially a lot of transactions. Furthermore, these transactions might be complicated and can exceed the gas limit given by the L1.</li>
<li>In addition, the might be some slightly differences between the rollup and the L1, making re-execution not possible, or hard to realize.
<blockquote>
<p>Optimism chose to use this approach in the first place but later abandoned this implementation.</p>
</blockquote>
</li>
</ul>
</li>
<li><strong>Interactive Fraud Proofs</strong>. Related parties (i.e, challenger vs sequencer) engage in a back-and-forth protocol to resolve dispute with minimal work required from any L1 contract.
<ul>
<li>The key principle behind interactive proving parties in a dispute should do as much off-chain work as possible needed to resolve their dispute, rather than putting that work onto the L1 contract.
<ul>
<li>Transaction execution can be broken down to multiple instructions, therefore, specifying which instruction was invalid is enough.</li>
<li>Parties communicate off-the-L1 to identity this erroneous instruction via multiple rounds of interaction.</li>
<li>Only the final step of the interactive dispute protocol involves L1 contract’s effort.</li>
</ul>
</li>
<li>This approach addresses the expensive gas costs incurred by the non-interactive approach, significantly reducing computational complexity.</li>
<li>However, this comes with several drawbacks.
<ul>
<li>Doubling the challenge period.
<ul>
<li>Apart from the regular challenge period, there is an additional challenge period for the interactive communication when a fraud challenge is invoked.</li>
<li>In the worst case, the total window time can be doubled as usual (i.e, 14 days).</li>
</ul>
</li>
<li>The rollup itself becomes more complex.
<ul>
<li>At the moment, Arbitrum is the only optimistic rollup having the interactive mechanism on mainnet.</li>
<li>Beside implementing the core protocol itself, a rollup must have an interface for parties to jump in for fraud challenges. This interactive mechanism introduces a new level of complexity in the protocol, and can be harder to design safely.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Next, we analyze a few properties of an ORU.</p>
<ul>
<li><strong>Cost</strong>. Transaction costs on an ORU include an L2 cost (execution and data) and an L1 cost (state transition) and a DA cost.
<ul>
<li>The L2 execution and data cost is extremely cheap.</li>
<li>The state transition cost is amortized over transactions and is fixed.</li>
<li>If an ORU uses L1 as the DA, then the dominating cost should be DA cost.
<ul>
<li>For an ORU, full transaction data must be published to DA. Therefore, the DA cost is considered higher than that of a zk rollup (see below).</li>
</ul>
</li>
</ul>
</li>
<li><strong>Finality</strong>. ORUs have slow finality because of the challenge window for fraud attestation.
<ul>
<li>Today, most ORUs have 7-day finality.</li>
<li>With interactive fraud proofs, an extended challenge window is created whenever a new challenge is invoked. As a result, users might have to wait for at most 14 days to withdraw their tokens from the rollups.</li>
</ul>
</li>
<li><strong>Throughput</strong>.  Throughput on an ORU is mainly limited by the L2 execution performance and DA bandwidth.
<ul>
<li>Regarding DA, fortunately, EigenDA is doing <a href="https://mobile.x.com/eigen_da/status/1834270511913693518" rel="noopener nofollow ugc">15+MB/s</a> while Celestia has a roadmap to <a href="https://cointelegraph.com/news/celestia-unveils-roadmap-to-1-gigabyte-blocks" rel="noopener nofollow ugc">1-gigabyte blocks</a> (i.e, 1 GB over 12 seconds =&gt; 80+ MB/s).</li>
</ul>
</li>
<li><strong>Simplicity</strong>. ORUs are much simpler than ZRUs.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50983-zk-rollups-5" name="p-50983-zk-rollups-5"></a>ZK Rollups</h3>
<p>A zkRollup (ZRU) relies on validity proofs to ensure the correctness of a state transition. It is able to verify a very complex operation (i.e, ~10k transactions) with very little information and a fixed cost. When submitting a rollup batch to L1, the sequencer must send along a validity proof proving that the off-chain computations are correct. ZRUs only need to provide validity proofs to finalize transactions on L1 instead of posting all transaction data on-chain like Optimistic Rollups.</p>
<p>Validity proofs are succinct and have fixed (<em>verifying</em>) cost. That is, no matter how many transactions there are, the final proof is fixed in length and verifying cost. This cost is amortized over all transactions included in the proofs. As the result, more transactions will result in less average cost per transaction.</p>
<p>We also analyze a few properties of a ZRU.</p>
<ul>
<li><strong>Cost</strong>. Users on a ZRU must pay an additional cost for validity proof beside L1, and L2 fees.
<ul>
<li>For a ZRU, the data published to DA is usually more compact than for an ORU. Therefore, users on a ZRU pay less DA fee than users on an ORU.
<ul>
<li>This is because a ZRU only needs to publish the differences (i.e, <em>state-diff</em>) between two continuous states while an ORU must publish whole transactional data for the sake of re-execution.</li>
</ul>
</li>
<li>Thanks to new advancements in ZK proving marketplace and new proving algorithms, proving cost now is cheaper and cheaper.</li>
<li>Today, proving cost per transaction is between half a cent to one cent. This additional cent balances out the reduced cost of DA mentioned above.</li>
</ul>
</li>
<li><strong>Finality</strong>. ZRUs allow fast finality. After a validity proof is submitted and verified on chain, the corresponding state is consider valid and finalized, no other challenge window is needed.
<ul>
<li>Today, most ZRUs have 24-hour (<em>or less</em>) finality.</li>
<li>Again, thanks to new advancements in ZK proving marketplace and new proving algorithms, 1-hour (or less) finality is possible.
<ul>
<li><a href="https://blog.succinct.xyz/op-succinct/" rel="noopener nofollow ugc">Succinct’s SP1</a> and <a href="https://x.com/RiscZero/status/1846556828559102114" rel="noopener nofollow ugc">RISC-0</a> show potentials to generate a validity proof in an order of minutes.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Throughput</strong>. ZK proving performance is around tens of thousands of gas per second at the moment. This is expected to increase with further optimization and by adding more proving machines/GPUs.</li>
<li><strong>Simplicity</strong>. While developing a new ZK proving infrastructure is considered complex, integrating an ORU with ZK proving is now much simpler.
<ul>
<li>For example, SP1 and RISC-0 only require deploying a few additional services while keeping the Optimism codebase the same.</li>
</ul>
</li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#p-50983-hybrid-rollups-6" name="p-50983-hybrid-rollups-6"></a>Hybrid Rollups</h1>
<p>The progress of ZK proving shows promising potentials. As a result, many rollups are considered to transition to a fully ZK mode. For high-performance rollups, this transition requires a stop. While doing a fully ZKU is expensive, we can definitely take a hybrid approach.</p>
<ul>
<li>Re-executing fraud proofs are not possible because it is impossible for L1 to re-execute these transactions.
<ul>
<li>Additionally, we might use external DA as a replacement for the L1 DA.</li>
</ul>
</li>
<li>Interactive fraud proofs are complicated and buggy, and they might double the (<em>already long</em>) finality time.</li>
<li>Validity proofs offer fast finality but the proving performance might not keep up with our execution client on real-time proving.</li>
</ul>
<p>We recognize that generating validity proofs is not always ideal. <a href="https://cointelegraph.com/news/arbitrums-fraud-proofs-havent-been-used-since-it-launched" rel="noopener nofollow ugc">Arbitrum has its fraud proof on mainnet for a few years but it has never been triggered</a>. This is to say that, <strong>as long as the sequencer behaves honestly</strong>, we will never need to use fraud proofs or validity proofs. We only need fraud proofs or validity proofs once a challenge is invoked. That is, if anyone spots an invalid state transition, he can initiate a challenge request and the remaining responsibility is at the sequencer side. The sequencer must (or ask external provers to) generate a validity proof for the required state transition and submit it to L1 attestation. Failure to generate this validity proof on time will get the sequencer slashed, and thus losing his stake.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/1/f139fca0c5794dafdcf7196770da727403692b4f.png" title="image"><img alt="image" height="347" src="https://ethresear.ch/uploads/default/optimized/3X/f/1/f139fca0c5794dafdcf7196770da727403692b4f_2_690x347.png" width="690" /></a></div><br />
<em><strong>Figure</strong>. A simplified version of a hybrid rollup.</em><p></p>
<p>In its simplest form, our approach is an optimistic rollup but with validity proofs. This offers several advantages.</p>
<ul>
<li><strong>Shorter challenge period (hence faster finality)</strong>. Validity proofs are only required once we have a challenge.
<ul>
<li>The extended challenge period can be reduced from 7 days to as little as just few hours, or 1-2 days if being conservative.</li>
<li>Most of the time (99.9999%), we do not need to generate validity proofs.</li>
<li>If a challenge is invoked, the sequencer than has an additional window to submit the required validity proof. The additional window time should be on an order of the maximum proving time for the sake of security.
<ul>
<li>For example, if proving time is one hour, we can have this extended window to be 24 hours.</li>
</ul>
</li>
<li>In the interactive fraud proof setting, the malicious sequencer will attempt to prevent 1) the state being challenged; and 2) the challenger finalizing the challenge. In the hybrid setting, (2) is not considered as if (1) is triggered, the sequencer has no other choice than submitting a valid proof.</li>
</ul>
</li>
<li><strong>Simple and robust fraud mechanism</strong>. ZK validity proofs appear to be more robust than fraud proofs.
<ul>
<li>Several ZK rollups have been running on the mainnet.</li>
<li>Arbitrum’s fraud proofs are the only permisisonless fraud proofs on mainnet that is running for years<sup class="footnote-ref"><a href="https://ethresear.ch#footnote-50983-1" id="footnote-ref-50983-1">[1]</a></sup> while Optimism’s fraud proofs have just been live for a few months.
<ul>
<li><a href="https://www.theblock.co/post/311702/optimism-foundation-disables-permissionless-fraud-proofs-plans-hard-fork-following-security-audits" rel="noopener nofollow ugc">Optimism also had to disable its permissionless fraud proofs following security audits</a>.</li>
</ul>
</li>
<li>With this approach, a challenger can just focus on keeping up with the chain progress and identifying the incorrect state transition (same as the re-executing fraud proofs), no other interaction is required.
<ul>
<li>Furthermore, no re-execution is required, therefore, challenging a state is cheaper than non-interactive fraud proofs.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Cost saving</strong>. The cost for users is the same as in an ORU and operational costs are lower than a ZRU.
<ul>
<li>This is because users do not have to bare the cost of validity proof generation.
<ul>
<li>Notice that the cost of generating proofs are compensated by either the sequencer or the challenger.</li>
</ul>
</li>
<li>ZRUs have to bear the cost of generating validity proofs for every state transition, even if there is no transaction. This is not required in a hybrid mode.</li>
</ul>
</li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#p-50983-conclusion-7" name="p-50983-conclusion-7"></a>Conclusion</h1>
<p>The hybrid rollup approach represents a significant advancement in blockchain scaling solutions. By combining the best aspects of optimistic and ZK rollups, it offers a more efficient, secure, and user-friendly experience. This innovative model not only addresses current scalability challenges but also pays the way for future improvements and adaptations as the ecosystem continues to develop.</p>
<p>We give a quick comparison between different approaches in the following table.</p>
<div class="md-table">
<table>
<thead>
<tr>
<th>Property</th>
<th>Optimistic Rollups</th>
<th>ZK Rollups</th>
<th>Hybrid Rollups</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Extended Finality</strong></td>
<td>7 days</td>
<td>N/A</td>
<td>1-24 hours, depending on the ZK solution</td>
</tr>
<tr>
<td><strong>Transaction Cost</strong></td>
<td>Dominated by DA cost</td>
<td>Dominated by ZK proving</td>
<td>Dominated by DA cost</td>
</tr>
<tr>
<td><strong>Operation Cost</strong></td>
<td>Lower</td>
<td>Higher</td>
<td>Lower</td>
</tr>
<tr>
<td><strong>Throughput</strong></td>
<td>Limited by DA (for full transaction data)</td>
<td>Limited by DA (for state-diff<sup class="footnote-ref"><a href="https://ethresear.ch#footnote-50983-2" id="footnote-ref-50983-2">[2]</a></sup>) and ZK proving</td>
<td>Limited by DA (for full transaction data<sup class="footnote-ref"><a href="https://ethresear.ch#footnote-50983-3" id="footnote-ref-50983-3">[3]</a></sup>) and ZK proving</td>
</tr>
<tr>
<td><strong>Simplicity</strong></td>
<td>Simple</td>
<td>Considered more complex, depending on the solution</td>
<td>Considered more complex, depending on the ZK solution</td>
</tr>
<tr>
<td><strong>Fraud Proofs</strong></td>
<td>- <strong>Non-interactive</strong>: less battle-tested, limited in performance, less customizable <br />- <strong>Interactive</strong>: complicated, time-consuming</td>
<td>Robust validity proofs</td>
<td>Robust validity proofs</td>
</tr>
</tbody>
</table>
</div><p><em><strong>Table</strong>. A quick comparison of different rollup designs.</em></p>
<hr class="footnotes-sep" />

<ol class="footnotes-list">
<li class="footnote-item" id="footnote-50983-1"><p>As Arbitrum’s fraud proofs never got invoked on mainnet, we do not know whether they are robust when they are actually challenged. <a class="footnote-backref" href="https://ethresear.ch#footnote-ref-50983-1">↩︎</a></p>
</li>
<li class="footnote-item" id="footnote-50983-2"><p>Some ZRUs (e.g, Polygon) still publish full transaction data rather than state diff. <a class="footnote-backref" href="https://ethresear.ch#footnote-ref-50983-2">↩︎</a></p>
</li>
<li class="footnote-item" id="footnote-50983-3"><p>One question naturally arises is whether we can apply state-diff to a hybrid rollup instead of publishing full transaction data without sacrificing security. <a class="footnote-backref" href="https://ethresear.ch#footnote-ref-50983-3">↩︎</a></p>
</li>
</ol>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/hybrid-rollups-optimistic-performance-meets-zk-security/20896">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 30 Oct 2024 11:15:41 +0000</pubDate>
</item>
<item>
<title>Hybrid Rollup: Optimistic Performance meets ZK Security</title>
<link>https://ethresear.ch/t/hybrid-rollup-optimistic-performance-meets-zk-security/20894</link>
<guid>https://ethresear.ch/t/hybrid-rollup-optimistic-performance-meets-zk-security/20894</guid>
<content:encoded><![CDATA[
<div> 很抱歉，由于您没有提供文章内容，我无法为您进行关键词提取和总结。请您提供需要总结的文章具体内容。 <div>
<p>(topic deleted by author)</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/hybrid-rollup-optimistic-performance-meets-zk-security/20894">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 30 Oct 2024 11:07:50 +0000</pubDate>
</item>
<item>
<title>Several implementations only randomnize δ when using Groth16. Does this means sampling only 1 of γ or δ is enough to prevent forgeries?</title>
<link>https://ethresear.ch/t/several-implementations-only-randomnize-when-using-groth16-does-this-means-sampling-only-1-of-or-is-enough-to-prevent-forgeries/20859</link>
<guid>https://ethresear.ch/t/several-implementations-only-randomnize-when-using-groth16-does-this-means-sampling-only-1-of-or-is-enough-to-prevent-forgeries/20859</guid>
<content:encoded><![CDATA[
<div> 关键词：trusted setup、α、β、γ、δ、攻击、随机化、δ、γ、Generator点、公开输入、离散对数关系、安全性

总结:
根据页面17所述，α、β、γ和δ应在可信设置过程中进行采样。然而，描述的攻击似乎仅在γ和δ相等时才有效。但目前有些实现方案只对δ进行随机化处理，从而将γ留给生成器点。这似乎意味着在现实中，为了防止可能的攻击，代表公开输入的两个G_2点应当具有未知的离散对数关系。但我未能明白为何这种做法可能不安全。 <div>
<p>According to page 17 <a href="https://eprint.iacr.org/2016/260.pdf" rel="noopener nofollow ugc">from groth16</a>,  α ; β ; γ ; δ should be sampled by the trusted setup. <a href="https://github.com/RareSkills/zk-book/blob/9289f45462a7fe73b7462cf6c45e5c76af97001d/content/groth16/en/groth16.md?plain=1#L276" rel="noopener nofollow ugc">This article describe an attack</a>. But the attack described seems to only work when <span class="math">\gamma, \delta</span> are equal. <a href="https://github.com/RareSkills/zk-book/blob/9289f45462a7fe73b7462cf6c45e5c76af97001d/content/groth16/en/groth16.md?plain=1#L303" rel="noopener nofollow ugc">Nerveless it also states both values should be sampled and not just 1</a>.</p>
<p><strong>But I’m seeing several implementation that randomize just δ thus leaving γ to the Generator point</strong>. Does this means in reality that the 2 <span class="math">G_2</span> points representing public inputs should just have unknown discrete logarithms relation in order to thwart any possible attack ? I fail to see how this could be unsafe.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/several-implementations-only-randomnize-when-using-groth16-does-this-means-sampling-only-1-of-or-is-enough-to-prevent-forgeries/20859">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 29 Oct 2024 12:08:04 +0000</pubDate>
</item>
<item>
<title>zkLogin Framework (Based on 4337 Contract Wallet)</title>
<link>https://ethresear.ch/t/zklogin-framework-based-on-4337-contract-wallet/20855</link>
<guid>https://ethresear.ch/t/zklogin-framework-based-on-4337-contract-wallet/20855</guid>
<content:encoded><![CDATA[
<div> 关键词：zkLogin、零知识证明、OAuth、前端应用、智能合约

总结:
zkLogin 是一种利用零知识证明（ZKP）技术实现用户通过支持 OpenID Connect 的平台（如谷歌、Facebook）进行区块链交易认证的身份验证方法，无需泄露敏感信息或依赖额外可信第三方。在这个流程中，前端应用引导用户完成 OAuth 登录并传递电路输入给 zkLogin 服务，生成零知识证明以验证 JWT 的有效性而不泄露其内容。证明随后被提交到智能合约中的入口点合同和验证合同进行验证与执行。整个流程包括前端、zkLogin 服务、中间件打包器、入口点合同、钱包合同以及验证合同等组件。

具体操作流程分为首次注册登录、再次登录及正常操作三种场景，其中涉及到新的公钥私钥对生成、JWT 解析、-proof 生成与验证、以及用户操作在钱包合同中的执行等步骤。ZEROBASE 是一个实时 ZK 证明网络，为速度、去中心化和法规遵从性设计，提供免费电路开发服务和商业许可证，用于构建像 zkLogin 这样的隐私保护应用。通过合作案例展示了 zkLogin 在实际应用场景中的优势，例如与 TOMO 钱包和世界自由金融的合作，提升了用户账户管理的安全性和隐私性。 <div>
<h1><a class="anchor" href="https://ethresear.ch#p-50895-zklogin-workflow-1" name="p-50895-zklogin-workflow-1"></a>zkLogin Workflow</h1>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/6/165bbdd72eacf89fd421c59b40e88d2b9a78a4e9.jpeg" title="|610.4251076758298x328"><img alt="|610.4251076758298x328" height="370" src="https://ethresear.ch/uploads/default/optimized/3X/1/6/165bbdd72eacf89fd421c59b40e88d2b9a78a4e9_2_690x370.jpeg" width="690" /></a></div><p></p>
<h2><a class="anchor" href="https://ethresear.ch#p-50895-components-2" name="p-50895-components-2"></a>Components</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50895-oauth-provider-3" name="p-50895-oauth-provider-3"></a>OAuth Provider</h3>
<p>Entities implementing OpenID Connect and OAuth 2.0 protocols, such as Google, Apple, and Meta, have a key feature: the ability to issue signed JWTs (JSON Web Tokens) containing a set of claims to verify the identity of end users.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50895-frontend-4" name="p-50895-frontend-4"></a>Frontend</h3>
<p>The frontend application supporting zkLogin guides users through the OAuth login process, passes circuit inputs to the zkLogin Service, and receives a proof. Since there is no need to securely store or export key pairs, no additional clients or plugins are required. The frontend can be seamlessly integrated into any Dapp, reducing the complexity for users when interacting with the Dapp.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50895-zklogin-service-5" name="p-50895-zklogin-service-5"></a>zkLogin Service</h3>
<p>A zero-knowledge proof is generated to verify the validity of the JWT without revealing its content. We currently use zk-SNARKs to generate the proof, as they are better suited for handling smaller proof sizes.</p>
<p>Bundler</p>
<p>Bundler is an intermediary responsible for bundling and submitting transactions. It collects operation requests from users’ wallets (such as UserOperations in EIP-4337), combines multiple user operations, and sends them as a transaction bundle to the blockchain for processing. When submitting these operations, the Bundler calculates and pays the necessary gas fees and ensures the validity of these operations.</p>
<p>Entrypoint Contract</p>
<p>Entrypoint contract is the core smart contract implementing the EIP-4337 standard, responsible for handling operations related to user accounts. It receives user operation requests submitted by the Bundler, verifies and executes these operations. The Entrypoint contract also manages account verification logic, gas payment logic, and calls functions from the user’s wallet.</p>
<p>Wallet Contract</p>
<p>Wallet contract is a smart contract on the blockchain that provides account functionalities for users, often replacing traditional EOAs (Externally Owned Accounts). It allows users to manage their digital assets and interaction logic through smart contracts in a flexible manner.</p>
<p>Verification Contract</p>
<p>Verification contract is responsible for verifying proofs related to user operations, such as zero-knowledge proofs generated by zk-SNARKs. In the zkLogin scenario, the Verification contract receives proofs generated by the zkLogin Service and verifies their validity to ensure the correctness of the user’s operations and identity information.</p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-50895-workflow-6" name="p-50895-workflow-6"></a>Workflow</h2>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/2/528170f306a2ee41b3fd148ad7e0a26992c66e73.png" title=""><img alt="" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/5/2/528170f306a2ee41b3fd148ad7e0a26992c66e73_2_575x500.png" width="575" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#p-50895-h-1-users-first-login-initial-registration-7" name="p-50895-h-1-users-first-login-initial-registration-7"></a>1. User’s First Login (Initial Registration)</h3>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/c/ac5361a9f41198533a3ab540b664a05341d8ad75.png" title=""><img alt="" height="197" src="https://ethresear.ch/uploads/default/optimized/3X/a/c/ac5361a9f41198533a3ab540b664a05341d8ad75_2_602x197.png" width="602" /></a></div><p></p>
<h4><a class="anchor" href="https://ethresear.ch#p-50895-frontend-operations-8" name="p-50895-frontend-operations-8"></a>Frontend Operations:</h4>
<ul>
<li>The frontend generates a secp256k1 public-private key pair (priv_k1，pub_k1) for the user and calculates the EOA address (addr1).</li>
<li>The frontend computes nonce1 = H(addr1, exp1)，and sends nonce1 to the OAuth server</li>
<li>After the user logs in via OAuth, the front end obtains the user’s JWT, which contains the fields iss, aud, sub, and nonce. The frontend parses these fields and calculates the salt required for creating the Wallet contract: salt = H(iss, aud, sub).</li>
<li>The frontend then sends a request to the zkLogin Service with the input data to generate the corresponding proof.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-50895-zklogin-service-operation-9" name="p-50895-zklogin-service-operation-9"></a>zkLogin Service  Operation：</h4>
<ul>
<li>The ZEROBASE backend receives the input sent by the frontend and passes it into the prove function.</li>
<li>It generates the corresponding proof and returns the proof (including publicSignals) to the frontend</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-50895-frontend-operations-10" name="p-50895-frontend-operations-10"></a>Frontend Operations：</h4>
<ul>
<li>The frontend takes the proof (with pubsignals containing addr2, iss, aud, sub) obtained from ZEROBASE, combines it with business parameters, and uses the private key (priv_k1) to create an encrypted signature. The signed data is then structured into a UserOperation and sent to the Bundler.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-50895-bundler-operations-11" name="p-50895-bundler-operations-11"></a>Bundler Operations：</h4>
<ul>
<li>The Bundler verifies the validity of the UserOperation and then sends the operation to the EntryPoint contract.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-50895-entrypoint-contract-12" name="p-50895-entrypoint-contract-12"></a>EntryPoint Contract：</h4>
<ul>
<li>The EntryPoint contract receives the UserOperation passed by the Bundler and calculates salt = H(iss, aud, sub) from UserOperation.proof.pubsignal. It then checks in the mapping table to see if a field with the key as salt exists. If it does not exist, the contract creates a Wallet contract for the user using the salt as a parameter and stores the corresponding salt value in the mapping table (key: salt, value: Wallet contract address).</li>
<li>The EntryPoint contract then forwards the UserOperation.proof and UserOperation.calldata to the Wallet contract.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-50895-wallet-contract-13" name="p-50895-wallet-contract-13"></a>Wallet Contract：</h4>
<ul>
<li>The EntryPoint contract calls the verification contract and passes in UserOperation.proof.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-50895-verification-contract-14" name="p-50895-verification-contract-14"></a>Verification Contract：</h4>
<ul>
<li>The verification contract verifies the zkProof to ensure that the information provided by the user matches the expected values.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-50895-wallet-contract-15" name="p-50895-wallet-contract-15"></a>Wallet Contract：</h4>
<ul>
<li>After successfully validating UserOperation.proof, the owner is updated to addr1, and the remaining business logic is executed.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50895-h-2-user-logs-out-and-logs-in-again-16" name="p-50895-h-2-user-logs-out-and-logs-in-again-16"></a>2. User Logs Out and Logs In Again</h3>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/e/7e0de1b4597bfe5406c786185cb1f9928159fc14.png" title=""><img alt="" height="167" src="https://ethresear.ch/uploads/default/optimized/3X/7/e/7e0de1b4597bfe5406c786185cb1f9928159fc14_2_602x167.png" width="602" /></a></div><p></p>
<h4><a class="anchor" href="https://ethresear.ch#p-50895-frontend-operations-17" name="p-50895-frontend-operations-17"></a>Frontend Operations：</h4>
<ul>
<li>When the user logs in again, the frontend generates a new secp256k1 public-private key pair (priv_k2, pub_k2) and logs in via OAuth to obtain a new JWT, which contains a new nonce2 = H(addr2, exp2) field.</li>
<li>The frontend parses the JWT, extracts the circuit input, and sends the input to the ZEROBASE Backend.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-50895-zklogin-service-zerobase-operations-18" name="p-50895-zklogin-service-zerobase-operations-18"></a>zkLogin Service (ZEROBASE) Operations：</h4>
<ul>
<li>The zkLogin Service backend receives the input sent by the frontend and passes it into the prove function.</li>
<li>It generates the corresponding proof and returns the proof (including publicSignals) to the frontend.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-50895-frontend-operations-19" name="p-50895-frontend-operations-19"></a>Frontend Operations：</h4>
<ul>
<li>The frontend takes the proof (with pubsignals containing addr2, iss, aud, sub) obtained from the zkLogin Service, combines it with business parameters, and signs it using priv_k2. This signed data is then structured into a UserOperation and sent to the Bundler.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-50895-bundler-operation-20" name="p-50895-bundler-operation-20"></a>Bundler Operation：</h4>
<ul>
<li>The Bundler sends the UserOperation to the EntryPoint contract.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-50895-entrypoint-contract-21" name="p-50895-entrypoint-contract-21"></a>EntryPoint Contract：</h4>
<ul>
<li>The EntryPoint contract verifies the signature, ensuring that the signer is addr2 from the publicSignals, but without needing to ensure that the Wallet contract owner is addr2. It then calculates salt = H(iss, aud, sub) from UserOperation.proof.pubsignal. The contract checks if an entry with the key as salt exists in the mapping table. If it exists, the EntryPoint forwards UserOperation.proof and UserOperation.calldata to the corresponding Wallet contract.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-50895-wallet-contract-22" name="p-50895-wallet-contract-22"></a>Wallet Contract：</h4>
<ul>
<li>The EntryPoint contract calls the verification contract, passing in the UserOperation.proof.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-50895-verification-contract-23" name="p-50895-verification-contract-23"></a>Verification Contract：</h4>
<ul>
<li>The verification contract verifies the zkProof to ensure that the information provided by the user matches the expected values.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-50895-wallet-contract-24" name="p-50895-wallet-contract-24"></a>Wallet Contract：</h4>
<ul>
<li>After successfully validating the UserOperation.proof, the owner is updated to addr2, and the remaining business logic is executed.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50895-h-3-normal-operation-user-has-not-logged-out-25" name="p-50895-h-3-normal-operation-user-has-not-logged-out-25"></a>3. Normal Operation (User Has Not Logged Out)</h3>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/0/10761fc5d1c3f2f334f0680f7846467e64a2e479.png" title=""><img alt="" height="163" src="https://ethresear.ch/uploads/default/optimized/3X/1/0/10761fc5d1c3f2f334f0680f7846467e64a2e479_2_602x163.png" width="602" /></a></div><p></p>
<h4><a class="anchor" href="https://ethresear.ch#p-50895-frontend-operations-26" name="p-50895-frontend-operations-26"></a>Frontend Operations：</h4>
<ul>
<li>After the user successfully logs in, they can proceed with regular transactions and operations.</li>
<li>The frontend computes the user’s salt = H(iss, sub, aud), combines it with business parameters, and signs the operation using the current private key (priv_k). It then constructs a UserOperation and sends it to the Bundler.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-50895-bundler-operation-27" name="p-50895-bundler-operation-27"></a>Bundler Operation：</h4>
<ul>
<li>The Bundler sends the user’s UserOperation to the EntryPoint contract.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-50895-entrypoint-contract-28" name="p-50895-entrypoint-contract-28"></a>EntryPoint Contract：</h4>
<ul>
<li>The EntryPoint contract retrieves the corresponding Wallet contract address based on UserOperation.salt, verifies the signature to ensure that the Wallet contract’s owner is equal to addr, and then executes the user’s UserOperation through the Wallet contract.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50895-h-29" name="p-50895-h-29"></a></h3>
<h3><a class="anchor" href="https://ethresear.ch#p-50895-the-standard-proof-of-zklogin-circuit-generation-takes-200ms-on-tee-devices-75tpsprover-node-supporting-large-scale-adoption-30" name="p-50895-the-standard-proof-of-zklogin-circuit-generation-takes-200ms-on-tee-devices-75tpsprover-node-supporting-large-scale-adoption-30"></a>The standard proof of zkLogin circuit generation takes 200ms on TEE devices. 75TPS/Prover Node, supporting large-scale adoption.</h3>
<p>1/4 196ms</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/1/910649aacc80b2ba242bbd3991a1ecceb018651d.png" title=""><img alt="" height="80" src="https://ethresear.ch/uploads/default/original/3X/9/1/910649aacc80b2ba242bbd3991a1ecceb018651d.png" width="602" /></a></div><p></p>
<p>2/4 199ms</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/4/d4e5b05d1121383a5909717b6c8545c1f447fa82.png" title=""><img alt="" height="84" src="https://ethresear.ch/uploads/default/original/3X/d/4/d4e5b05d1121383a5909717b6c8545c1f447fa82.png" width="602" /></a></div><p></p>
<p>3/4 209ms</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/a/caeb656e150d1bab05f19c44f02badfd091e2304.png" title=""><img alt="" height="87" src="https://ethresear.ch/uploads/default/original/3X/c/a/caeb656e150d1bab05f19c44f02badfd091e2304.png" width="602" /></a></div><p></p>
<p>4/4 210ms</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/a/4a6334d84ca0d5eae5074215172ad2f4c54c7f5d.png" title=""><img alt="" height="87" src="https://ethresear.ch/uploads/default/original/3X/4/a/4a6334d84ca0d5eae5074215172ad2f4c54c7f5d.png" width="602" /></a></div><p></p>
<h2><a class="anchor" href="https://ethresear.ch#p-50895-use-case-31" name="p-50895-use-case-31"></a>Use Case</h2>
<p><a href="https://www.zkey.org/" rel="noopener nofollow ugc">zkLogin - StarkNet</a><br />
zkLogin utilizes Zero-Knowledge Proofs (ZKPs) to enable users to authenticate blockchain transactions on StarkNet using existing credentials from OpenID Connect-enabled platforms (e.g., Google, Facebook) without disclosing sensitive information or relying on additional trusted parties.</p>
<p><a href="https://tomo.inc/" rel="noopener nofollow ugc">zkLogin - TOMO Wallet</a><br />
ZEROBASE is collaborating with TOMO Wallet and World Liberty Financial, a project associated with Trump, to deliver a more streamlined and privacy-focused login experience. By utilizing ZEROBASE 's ZKLogin technology, sensitive user details such as OpenID, avatar, and email are securely processed and converted into a zero-knowledge proof. This proof is then returned to TOMO for verification, ensuring that user data is authenticated without being exposed. Once validated, TOMO can securely complete the login process. This partnership, in collaboration with World Liberty Financial, enhances security and privacy, offering TOMO users a more efficient and trusted way to manage their accounts.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50895-about-zerobase-32" name="p-50895-about-zerobase-32"></a>About ZEROBASE</h2>
<p>ZEROBASE is a real-time ZK prover network designed for speed, decentralization, and regulatory compliance. It generates ZK proofs within hundreds of milliseconds and ensures decentralized, fast consensus through its HUB ring-wake mechanism, enabling large-scale commercial use. We offer free circuit development services and commercial licenses, including zkLogin, zkDarkPool, and zkVote systems, which require ZEROBASE provers for optimal performance. We also undertake general commercial circuit development.</p>
<p>ZEROBASE’s standard circuit integration takes one week, while custom circuit development takes only 2-3 weeks. Agile development and fast iteration are achieved through proof conversion using Circom and Gnark, with an optional decryptable circuit framework for compliance. With a core team of three years and partnerships with top institutions, ZEROBASE delivers high-quality circuit development and proof generation services.</p>
<aside class="onebox allowlistedgeneric">
  <header class="source">

      <a href="https://zerobase.pro/" rel="noopener nofollow ugc" target="_blank">zerobase.pro</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image"><img class="thumbnail" height="362" src="https://ethresear.ch/uploads/default/optimized/3X/1/5/155cd454679cd89b63694c0b0530a945ceacf0c7_2_690x362.jpeg" width="690" /></div>

<h3><a href="https://zerobase.pro/" rel="noopener nofollow ugc" target="_blank">Proving the World</a></h3>



  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both;"></div>
</aside>

<aside class="onebox allowlistedgeneric">
  <header class="source">
      <img class="site-icon" height="275" src="https://ethresear.ch/uploads/default/original/3X/4/9/49db747bb94ab9991285bed602056b1cf0451882.png" width="275" />

      <a href="https://docs.zerobase.pro/" rel="noopener nofollow ugc" target="_blank">docs.zerobase.pro</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image"><img class="thumbnail" height="362" src="https://ethresear.ch/uploads/default/optimized/3X/4/f/4f2ecb4bbd8f7a3588826e4644ffa3686065b88d_2_690x362.png" width="690" /></div>

<h3><a href="https://docs.zerobase.pro/" rel="noopener nofollow ugc" target="_blank">Introduction | ZEROBASE Docs</a></h3>



  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both;"></div>
</aside>

<p>Utilize your intelligence and my circuit development skills to establish cryptographic applications on Ethereum together.Share your insights.</p>
<p>We offer free circuit generation for public goods. Please feel free to email me directly at: mirror.tang@zerobase.pro</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/zklogin-framework-based-on-4337-contract-wallet/20855">Read full topic</a></p>
]]></content:encoded>
<pubDate>Mon, 28 Oct 2024 13:37:07 +0000</pubDate>
</item>
<item>
<title>Local Fee Markets in Ethereum</title>
<link>https://ethresear.ch/t/local-fee-markets-in-ethereum/20754</link>
<guid>https://ethresear.ch/t/local-fee-markets-in-ethereum/20754</guid>
<content:encoded><![CDATA[
<div> 关键词: 本地费用市场、以太坊、网络拥塞、动态费率倍增器、高需求地址

总结:
本文提出了一个针对以太坊的本地费用市场机制提案，旨在通过管理网络拥塞和确保公平的费用分配来优化交易费用。该提案在现有EIP-1559模型基础上进行增强，引入了过目标调整的概念，当区块 Gas 使用超过目标值时，识别那些消耗超过区块总 Gas 一定比例（如1/16）的高需求地址，并据此计算费率倍增器。在交易过程中，模拟交易并返回受影响的地址和存储位置，然后根据这些地址的高需求程度应用相应的费率倍增器。当区块 Gas 使用降低后，将逐步减少费率倍增器，直至其恢复到常规基础费率。

此外，还设计了一个累进式的费率倍增器，使得持续处于高需求状态的地址将面临更高的交易成本。该提案的优势在于提高了费用预测的准确性、保证了用户公平接入以及透明化的费用计算。然而，也存在一些潜在的问题，例如费用增加并非完全局部化、实现难度大、可能对某些主网热门应用不利等。

总之，该提案通过动态调整实际资源消耗相关的费用，旨在有效地管理和解决局部拥堵问题，同时不大幅改变现有的费用估计流程，为以太坊作为应用平台提供更好的支持。提案欢迎进一步反馈与细化讨论。 <div>
<h3><a class="anchor" href="https://ethresear.ch#p-50659-proposal-for-implementing-local-fee-markets-in-ethereum-1" name="p-50659-proposal-for-implementing-local-fee-markets-in-ethereum-1"></a>Proposal for Implementing Local Fee Markets in Ethereum</h3>
<p><strong>Objective</strong>: Introduce a local fee market mechanism to manage network congestion and ensure fair fee distribution, applying dynamic fee multipliers based on high-demand storage locations while maintaining accurate and predictable fee estimates.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50659-current-mechanism-2" name="p-50659-current-mechanism-2"></a>Current Mechanism</h3>
<p>Ethereum currently uses the EIP-1559 model for transaction fees, where the base fee is dynamically adjusted based on network demand to target an optimal gas usage per block. Wallets estimate transaction costs using the base fee and gas limit.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50659-proposed-enhancement-3" name="p-50659-proposed-enhancement-3"></a>Proposed Enhancement</h3>
<p><strong>Over-Target Adjustment</strong>:</p>
<ol>
<li><strong>Identify High-Demand Addresses</strong>: When a block exceeds the target gas usage, track accounts whose storage writes consume more than 1/16th of the total gas used in that block. It could be 1%, 50%, or any desired percentage instead of 1/16th but this impacts how many accounts end up needing to be tracked across blocks so adds to memory requirements.</li>
<li><strong>Calculate Fee Multiplier</strong>: Determine a fee multiplier for these high-demand addresses based on their “excess” gas usage.</li>
</ol>
<p><strong>Fee Multiplier Application</strong>:</p>
<ol>
<li><strong>Transaction Simulation with <code>eth_estimateGas</code></strong>: Simulate transactions using <code>eth_estimateGas</code> to determine which storage locations and addresses are accessed.</li>
<li><strong>Return Additional Data</strong>: Modify <code>eth_estimateGas</code> to return a list of touched addresses and storage locations during the simulation.</li>
<li><strong>Apply Multiplier</strong>: Apply any existing fee multiplier to the base fee for transactions that interact with currently identified high-demand addresses.</li>
<li><strong>Reducing Multiplier</strong>: If a given block’s gas usage falls below the target gas then the multiplier is removed and everything returns to normal. If the account no longer consumes more than the 1/16th threshold amount the multiplier reduces by an equal amount it would have increased by if it had surpassed the threshold until it is back at the regular base fee.</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-50659-multiplier-compounding-4" name="p-50659-multiplier-compounding-4"></a>Multiplier Compounding</h3>
<p>The fee multiplier can compound across blocks:</p>
<ul>
<li><strong>Incremental Multiplier</strong>: If an account remains over the target usage in subsequent blocks, the multiplier increases further.</li>
<li><strong>Cumulative Effect</strong>: The multiplier builds over time, applying an increasingly higher cost to transactions involving persistent high-demand addresses.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50659-example-process-5" name="p-50659-example-process-5"></a>Example Process</h3>
<ol>
<li>
<p><strong>Block N Analysis</strong>:</p>
<ul>
<li>Total gas used: 20 million units. (this is above target gas so a contention multiplier will be applied)</li>
<li>Account A’s storage writes: 3.2 million units</li>
<li>Threshold: 1.25 million units</li>
<li>Excess usage: 2 million units (10% over threshold)</li>
</ul>
</li>
<li>
<p><strong>Multiplier Calculation</strong>:</p>
<ul>
<li>Initial Multiplier: 1.10x (since we are 10% over threshold, but this multiplier could be modified depending on how much we want fees to increase faster for hot spots than other locations).</li>
<li>If Account A exceeds the threshold again in Block N+1, the new multiplier increases:
<ul>
<li>New excess percentage: 10%.</li>
<li>Compounded Multiplier: 1.10x (previous) * 1.10x (current excess) = 1.21x.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Fee Estimate for Block N+2</strong>:</p>
<ul>
<li>Base fee for Block N+2: 100 gwei.</li>
<li>Effective fee for transactions involving Account A: 100 gwei * 1.21 = 121 gwei.</li>
</ul>
</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-50659-benefits-6" name="p-50659-benefits-6"></a>Benefits</h3>
<ul>
<li><strong>Predictability</strong>: Users can rely on accurate fee estimates without requiring extensive transaction simulations.</li>
<li><strong>Fair Access</strong>: The fee increases are localized to high-demand storage locations, allowing other users to access the network at lower costs.</li>
<li><strong>Transparency</strong>: Users are informed about potential high-cost interactions, maintaining transparency in fee calculations.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50659-implementation-steps-7" name="p-50659-implementation-steps-7"></a>Implementation Steps</h3>
<ol>
<li><strong>Modify <code>eth_estimateGas</code></strong>: Enhance the function to return additional data indicating touched storage locations and addresses.</li>
<li><strong>Track High-Demand Addresses</strong>: Maintain a list of addresses that exceeded the 1/16th gas usage threshold from the previous block.</li>
<li><strong>Wallet Integration</strong>:
<ul>
<li>Update wallet software to use the additional data from <code>eth_estimateGas</code> for fee estimation.</li>
<li>Implement logic to apply the fee multiplier when high-demand addresses are detected.</li>
<li>Ensure compounding of multipliers if the same addresses continue to exceed thresholds in subsequent blocks.</li>
</ul>
</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-50659-weaknesses-8" name="p-50659-weaknesses-8"></a>Weaknesses</h3>
<ul>
<li>Fee increases are not entirely localized</li>
<li>Might be complex enough that we want to switch to providing touched addresses with the transaction up front to better support parallelization.</li>
<li>Unclear how impactful this would be to keeping fees lower on L1 for p2p payments or other use cases that some people feel L1 shouldn’t be used for.</li>
<li>From a historical perspective, I haven’t looked at how often a smart contract is getting touched that it would surpass the 1/16th (or w/e we decided on) gas trigger for the multiplier to take effect.</li>
<li>Popular apps on mainnet are probably opposed to getting charged more for being popular (another reason to spin-off as an alt-L1 or L2?)</li>
<li>Probably a pain to implement anyways <img alt=":smiley:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/smiley.png?v=12" title=":smiley:" width="20" /></li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50659-conclusion-9" name="p-50659-conclusion-9"></a>Conclusion</h3>
<p>This proposal seeks to enhance network fairness by dynamically adjusting fees based on actual resource consumption, maintaining predictable transaction costs for users. By applying these targeted fee adjustments, Ethereum can manage localized congestion effectively without requiring significant changes to the existing fee estimation processes. While there are some downsides to this approach I think it could be a reasonable step-wise approach to avoiding noisy neighbor problems on Ethereum improving its position as an app platform.</p>
<p>Feedback and further refinement are welcome.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/local-fee-markets-in-ethereum/20754">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 24 Oct 2024 21:00:23 +0000</pubDate>
</item>
<item>
<title>Practical endgame on issuance policy</title>
<link>https://ethresear.ch/t/practical-endgame-on-issuance-policy/20747</link>
<guid>https://ethresear.ch/t/practical-endgame-on-issuance-policy/20747</guid>
<content:encoded><![CDATA[
<div> 关键词: 发行政策、权益证明、奖励曲线、最高发行量、小型独立验证者

总结:
本文探讨了以太坊权益证明共识机制下的发行政策及其对权益质押的影响。文章提出了一种实用的发行终端策略，旨在停止权益增长的同时确保合理的共识激励并为勤勉的小型独立验证者提供正向收益。提出了两种可能的奖励曲线范围，一种硬性终端策略（红色），其收益率将降至负无穷大，虽能确保权益上限但具有较高复杂性；另一种则是更为实际可行的策略（绿色），它提供了概率性的权益上限保证，更容易实施。

当前约有三分之二的以太坊被质押，而随着质押成本降低和摩擦减少，质押总量将持续缓慢增长。因此，有必要降低发行量，从而降低硬件、风险、流动性差及其他成本，同时实现信任无损的稳健货币，这对于去中心化经济至关重要。文中建议了一个年度最高发行率的社会上限为0.5%，认为这是一个易于理解并有利于维持共识与合并激励的水平。

针对实用的发行终端策略，文章强调了奖励曲线应大致遵循边际效用递减规律，并提出了几种可能的曲线设计方案，如立方奖励曲线等。此外，还讨论了不同发行策略的优缺点，包括固定发行率（例如0.5%）的方案，以及采用平滑过渡或分段构造的方法来逐步接近一个正的最低发行量。

文章最后列举了一些值得社区成员和研究者进一步讨论的问题，比如：是否可以接受长期来看积极履行职责但不合并区块提案收益的小型独立验证者承受ETH损失？理想的发行量（或发行率）在不同的总质押量下应设定为多少？在当前MEV捕获及相关机制的不确定性下，社区是否愿意支持未来数年内将发行率固定在0.5%，并在之后重新审视该问题？ <div>
<p>By <a href="https://x.com/weboftrees">Anders Elowsson</a></p>
<p><em>Thanks to <a href="https://x.com/VitalikButerin">Vitalik Buterin</a>, <a href="https://x.com/casparschwa">Caspar Schwarz-Schilling</a> and <a href="https://x.com/adietrichs">Ansgar Dietrich</a> for feedback.</em></p>
<h2><a class="anchor" href="https://ethresear.ch#p-50642-h-1-introduction-1" name="p-50642-h-1-introduction-1"></a>1. Introduction</h2>
<p>This post presents a practical endgame on issuance policy that can stop the growth in stake while guaranteeing proper consensus incentives and providing positive regular rewards to diligent small solo stakers. Two possible ranges for an endgame reward curve are outlined in Figure 1. A <em>hard endgame</em> (red) with a reward curve that caps the quantity of stake by bringing the yield down to negative infinity comes at the cost of analytical, implementational, and political complexity (a <em>hard</em> cap can be <em>hard</em> to implement). Even setting the issuance yield to zero introduces additional complexities that would be advantageous to avoid if possible—particularly if there is no MEV burn mechanism in place, such that the 0% issuance yield merely halts regular rewards, while irregular rewards continue. Certainty can be the enemy of viability, because bringing down the staking yield to a low yet positive level will, in all likelihood, suffice. This post emphasizes viability: a <em>practical endgame</em> (green) with probabilistic guarantees on the quantity of stake that could be implemented at present.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/0/e0671a81f7944f4b5d0aeecaa0c0536fc6733af2.png" title="Figure 1"><img alt="Figure 1" height="474" src="https://ethresear.ch/uploads/default/optimized/3X/e/0/e0671a81f7944f4b5d0aeecaa0c0536fc6733af2_2_690x474.png" width="690" /></a></div><p></p>
<p><strong>Figure 1.</strong> Issuance ranges for two endgames: a <em>practical</em> endgame in green which can be viable for the near term and easier to come to an agreement on, and a <em>hard</em> endgame in red with higher analytical and political complexity that may push solo stakers into receiving a negative regular yield. Both endgames overlap at low stake deposit sizes (<span class="math">D</span>) and are therefore likely to lead to a similar equilibrium outcome, given reasonable assumptions about the willingness to supply stake at different staking yields.</p>
<p>The great news is that we can offer more stringent guarantees on the maximum proportion of the circulating supply issued each year, regardless of which exact endgame policy that is pursued. To Ethereum’s users, a stringent cap on issuance of native ETH tokens is desirable, because it caps the inflation rate. Since revenue of the protocol can be burned, as partially done today, the ETH inflation rate can be sustainably negative (deflation). <em>Ethereum can have trustless sound money with preserved economic security</em>—something that is very valuable for a decentralized economy. A social cap can be set at an issuance rate of <span class="math">i=0.5\%</span>, illustrated by a grey line in Figure 1. This is an easy-to-understand concept to commit to (memetic qualities), sufficiently high to ensure a viable staking set with ample room for consensus and consolidation incentives, and permits flexibility to temper the quantity of stake as the community sees fit.</p>
<p>What remains for a potential practical endgame is to come to an agreement on the exact specification of the reward curve and to outline how relevant micro incentives should be designed under it. The curve should approximately follow the shape of the <a href="https://ethresear.ch/t/reward-curve-with-tempered-issuance-eip-research-post/19171">reward curve with tempered issuance</a>. However, a slightly higher yield at lower quantities of stake and a slightly lower yield at higher quantities of stake seems preferable, if the goal is a practical endgame through one singular change in issuance policy. A few options along these lines are suggested, peaking at a 0.5% issuance rate. A cubed reward curve (red in Figure 3) can be highlighted as fitting within the preferable range, but others (e.g., purple and orange in Figure 3) should also be considered. There is an option to simply set the issuance rate to 0.5% for now (dashed in Figure 9) and revisit the issue again in a few years, but this seems harder to find community support for. The post concludes with a set of open questions that the community and researchers should weigh in on.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50642-h-2-recap-on-issuance-reduction-2" name="p-50642-h-2-recap-on-issuance-reduction-2"></a>2. Recap on issuance reduction</h2>
<p>First a short recap on issuance policy and the prospect of reducing issuance; for a deeper understanding, read the <a href="https://ethresear.ch/t/faq-ethereum-issuance-reduction/19675">FAQ</a>.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50642-h-21-motivation-3" name="p-50642-h-21-motivation-3"></a>2.1 Motivation</h3>
<p>Currently, around <a href="https://dune.com/hildobby/eth2-staking">35M ETH</a> is staked, <a href="https://ethresear.ch/t/faq-ethereum-issuance-reduction/19675#what-about-economic-security-more-stake-makes-ethereum-more-secure-right-8">arguably</a> already more than required, and the quantity of stake is slowly growing in line with diminishing costs of staking and on account of staking frictions being overcome. There are two fundamental <a href="https://ethresear.ch/t/faq-ethereum-issuance-reduction/19675#why-should-ethereum-reduce-its-issuance-4">reasons to reduce issuance</a>:</p>
<ol>
<li>The current reward curve compels users to incur higher costs than necessary for securing Ethereum (costs broadly defined to include hardware, risks, illiquidity, taxes, etc.). Reducing issuance <a href="https://notes.ethereum.org/@anderselowsson/Foundations-of-MVI">improves welfare</a> by lowering these costs in aggregate, as illustrated in the FAQ in <a href="https://ethresear.ch/t/faq-ethereum-issuance-reduction/19675#h-1-reduced-costs-raise-welfare-6">Figure 1</a> and <a href="https://ethresear.ch/t/faq-ethereum-issuance-reduction/19675#proportional-yield-40">Figure 26</a>.</li>
<li>It is valuable to have trustless sound money as the primary currency in a decentralized economy. High issuance can lead a liquid staking token (LST) to <a href="https://x.com/weboftrees/status/1710712326117097785">dominate as money</a>. Lower issuance ensures that app developers and users will not be subjected to monopolistic pressure from LST issuers, or needlessly risk the LST failing, potentially even threatening consensus if an LST becomes “<a href="https://x.com/weboftrees/status/1710713959362252884">too big to fail</a>”.</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-50642-h-22-impact-4" name="p-50642-h-22-impact-4"></a>2.2 Impact</h3>
<p>When discussing issuance and contemplating the effect on stakers, it is important to remember that the stake supply curve is upward sloping. Therefore, when the yield is reduced, the equilibrium yield will only fall by a fraction of the nominal reduction (see Figure 2 in <a href="https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448#h-22-influence-of-f-on-the-equilibrium-10">this post</a>). Furthermore, what matters to the staker is the proportion of all ETH they attain. Higher issuance dilutes also stakers, and if some stakers leave, remaining stakers may gain a higher proportion of the total ETH, in accordance with the equations in the <a href="https://notes.ethereum.org/@anderselowsson/MinimumViableIssuance#Benefits-of-MVI-to-user-utility">post on minimum viable issuance (MVI)</a>. This change in the attained proportion of the total ETH has also been referred to as “<a href="https://ethresear.ch/t/endgame-staking-economics-a-case-for-targeting/18751#real-yield-the-real-deal-10">real yield</a>”. The effect can be illustrated using an <a href="https://ethresear.ch/t/faq-ethereum-issuance-reduction/19675#the-isoproportion-map-42">isoproportion map</a>, with 1D examples also available (<a href="https://ethereum-magicians.org/t/electra-issuance-curve-adjustment-proposal/18825">1</a>, <a href="https://notes.ethereum.org/@mikeneuder/subsol#3-Scaled-Root-Curve-alternative-issuance">2</a>). Another way to illustrate this is <a href="https://x.com/weboftrees/status/1710706011185545671">presented in the thread on MVI</a>, which, in a single plot, tries to capture how reduced issuance can increase welfare (all groups are better off), increase the attained proportion of all ETH among stakers, and produce a moderated fall in nominal yield under equilibrium.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50642-endogenous-and-exogenous-yield-5" name="p-50642-endogenous-and-exogenous-yield-5"></a>Endogenous and exogenous yield</h4>
<p>It is important to <a href="https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448#h-21-supply-and-demand-9">understand</a> the difference between:</p>
<ul>
<li><em>endogenous yield</em>, constituting rewards derived from staked participation in the consensus process, such as issuance, MEV, the sale of preconfirmations—and even staking airdrops; and</li>
<li><em>exogenous yield</em>, constituting rewards derived outside of consensus participation, such as in DeFi in the form of for example restaking yield.</li>
</ul>
<p>Early in the debate, there was concern that, e.g., restaking would make solo staking impossible under an equilibrium enforced at a lower quantity of stake. The motivation was that delegating stakers are better equipped to derive exogenous yield. While there are some merits to the general concern, exogenous yield can also be derived directly from non-staked ETH. Thus, if the endogenous yield approaches zero, there will be no incentives to stake for anyone—not for solo stakers and <a href="https://ethresear.ch/t/faq-ethereum-issuance-reduction/19675#equilibrium-yield-and-the-proportion-of-solo-stakers-24">certainly not for delegating stakers</a>—other than to protect or attack Ethereum.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50642-h-23-downsides-6" name="p-50642-h-23-downsides-6"></a>2.3 Downsides</h3>
<p>However, an issuance reduction may not only bring benefits. A <a href="https://ethresear.ch/t/faq-ethereum-issuance-reduction/19675#how-will-a-reduction-in-issuance-affect-the-composition-of-the-staking-set-21">concern</a> is that solo stakers are more sensitive to a reduction in staking yield due to their higher fixed costs (e.g., hardware). It is therefore possible that a reduced issuance will decrease the proportion of solo stakers somewhat (review how differences in reservation yields in <a href="https://ethresear.ch/t/faq-ethereum-issuance-reduction/19675#illustrating-hypothetical-distributions-of-reservation-yields-26">Figure 11</a> could alter the proportion of solo stakers in Figure 13). This is a potential downside that must be balanced against upsides of reducing issuance, but there are also <a href="https://ethresear.ch/t/faq-ethereum-issuance-reduction/19675#equilibrium-yield-and-the-proportion-of-solo-stakers-24">counterarguments</a> pointing in the other direction. It should further be noted that when issuance falls with increased stake participation, the viability of discouragement attacks (<a href="https://github.com/ethereum/research/blob/09d9f34042262c8fb436171786ed6c62e1f57247/papers/discouragement/discouragement.pdf">1</a>, <a href="https://ethresear.ch/t/reward-curve-with-tempered-issuance-eip-research-post/19171#h-53-discouragement-attacks-32">2</a>) and <a href="https://ethresear.ch/t/reward-curve-with-tempered-issuance-eip-research-post/19171#h-54-cartelization-attacks-33">cartelization attacks</a> increases.</p>
<p>Furthermore, if issuance is reduced to below the level of the MEV, especially when close to zero or negative, <a href="https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448#h-3-consensus-incentives-11">consensus incentives</a> will be negatively affected, and <a href="https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448#h-4-variability-in-rewards-for-solo-stakers-12">variability in staking yield</a> among stakers who cannot effortlessly pool their MEV rewards (i.e., solo stakers) will increase. For these reasons, a mechanism for burning MEV (e.g., <a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590">1</a>, <a href="https://ethresear.ch/t/execution-tickets/17944">2</a>, <a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ">3</a>, <a href="https://ethresear.ch/t/sealed-execution-auction/20060">4</a>, <a href="https://ethresear.ch/t/mev-resistant-dynamic-pricing-auction-of-execution-proposal-rights/20024">5</a>) is important to implement—yet such a mechanism may be a long way from adoption. It might also not be possible to burn all MEV if, e.g., proposers and builders can collaborate to keep down the attested MEV (<a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590/4">1</a>, <a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590/23">2</a>, <a href="https://ethresear.ch/t/dr-changestuff-or-how-i-learned-to-stop-worrying-and-love-mev-burn/17384/3">3</a>). However, there are then <a href="https://ethresear.ch/t/burn-incentives-in-mev-pricing-auctions/19856">strong incentives</a> for competing stakers to integrate with builders to make bids that keep the burn at rather high levels.</p>
<p>The lack of consensus incentives at lower issuance can be compensated for by <a href="https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448/2">increasing the penalty</a> for missed attestations. Yet if the maximum issuance is zero (i.e., a zero <a href="https://eth2book.info/capella/part2/incentives/issuance/#the-base-reward-per-increment">base reward per increment</a>), such relative adjustments are ineffective. A regime of increased attestation penalties will also open up for <a href="https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448/11#minority-discouragement-attack-against-sync-committee-attestations-2">minority discouragement attacks</a>, wherein the proposer selectively drops attestations to harm competing stakers. If proposer penalties are adapted to compensate, a missed proposal will become rather costly to the offline solo staker, who will already take relatively high losses due to increased attestation penalties. Remedies such as reducing the penalty if the proposer was inactive during the previous 2-4 epochs can then be considered, but design complexity increases.</p>
<p><a href="https://ethresear.ch/t/orbit-ssf-solo-staking-friendly-validator-set-management-for-ssf/19928#incentivizing-consolidation-10">Consolidation incentives</a> under a transition to <a href="https://ethresear.ch/t/orbit-ssf-solo-staking-friendly-validator-set-management-for-ssf/19928">Orbit SSF</a> could also be negatively affected by a reduction in issuance. There are good reasons to distribute proposal rights according to stake in Orbit SSF, just as today, at least if there still is MEV to extract. Otherwise, if consolidated validators are premiered, it becomes difficult to retain fairness since the protocol is not aware of the expected MEV revenue. With proposal rights distributed according to stake and low or negative expected attestation revenue, stakers have strong incentives to deconsolidate and reduce their <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#p-50029-h-82-activity-rate-24">activity rate</a>, since this reduces slashing risks. Increased attestation penalties are then less relevant, since stakers simply can ensure that they are predominantly inactive by running many small validators.</p>
<p>To ensure consolidation, relatively substantial <a href="https://ethresear.ch/t/orbit-ssf-solo-staking-friendly-validator-set-management-for-ssf/19928#individual-consolidation-incentives-12">individual incentives</a> must be pursued, at least before a MEV burn mechanism is in place. This means that under very low or zero issuance, small solo stakers would have to lose ETH every epoch while waiting for the chance to propose. In line with the reasoning in Section 2.2, small validators’ expected endogenous yield would still remain positive, but solo stakers that cannot effortlessly pool rewards suffer from the high relative variance in rewards. The more complex option is to assign relatively more block proposals to consolidated stake when the consolidation level is low and no MEV burn is in place.</p>
<p><a href="https://ethresear.ch/t/orbit-ssf-solo-staking-friendly-validator-set-management-for-ssf/19928#collective-consolidation-incentives-11">Collective consolidation incentives</a> would also by definition further reduce issuance from an already low baseline if the validator set is not consolidated, once again potentially pushing the solo staker into negative territory.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50642-h-3-practical-endgame-7" name="p-50642-h-3-practical-endgame-7"></a>3. Practical endgame</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50642-h-31-the-role-of-the-reward-curve-8" name="p-50642-h-31-the-role-of-the-reward-curve-8"></a>3.1 The role of the reward curve</h3>
<p>Ethereum’s consensus mechanism relies on a reward curve that stipulates how much ETH that should be rewarded for performing each validator duty when a certain amount of ETH is staked, effectively determining the maximum total issuance under perfect participation. The reward curve should roughly reflect the diminishing marginal utility of adding additional stake once security has solidified. Specifically, it should be designed as the “<a href="https://ethresear.ch/t/faq-ethereum-issuance-reduction/19675#philosophical-underpinnings-of-the-optimization-problem-30">issuance policy expansion path</a>” that optimally balances relevant trade-offs. It can be understood as a utility maximizing locus of preferred equilibrium points along possible supply curves. In other words, the reward curve should not produce an equilibrium at less desirable points along the supply curve when more desirable equilibria can be achieved. A PID controller guaranteeing some specific quantity of stake is therefore <a href="https://ethresear.ch/t/faq-ethereum-issuance-reduction/19675#why-not-dynamically-adjust-the-yield-with-a-mechanism-like-eip-1559-to-guarantee-some-fixed-target-participation-level-16">not desirable</a>; it fails to accurately price the marginal utility of additional stake in the long run.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50642-h-32-endgame-categories-9" name="p-50642-h-32-endgame-categories-9"></a>3.2 Endgame categories</h3>
<p>Various approaches have been discussed for the issuance endgame; <a href="https://ethresear.ch/t/faq-ethereum-issuance-reduction/19675#overview-29">five loosely delineated categories</a> are highlighted in the issuance reduction FAQ.</p>
<p><a href="https://ethresear.ch/t/faq-ethereum-issuance-reduction/19675#h-4-economic-capping-34">Category 4</a> has been referred to as <a href="https://notes.ethereum.org/@vbuterin/single_slot_finality#Economic-capping-of-total-deposits">economic capping</a>, <a href="https://ethresear.ch/t/endgame-staking-economics-a-case-for-targeting/18751">targeting</a>, or <a href="https://www.reddit.com/r/ethereum/comments/14vpyb3/comment/jrnwpmk">stake capping</a>, constituting a yield that approaches negative infinity, which was discussed in a recent extensive <a href="https://ethresear.ch/t/endgame-staking-economics-a-case-for-targeting/18751">write-up</a>. The benefit is the absolute guarantee of capping the quantity of stake, even in the presence of MEV. A potential downside of this approach, certainly if the cap is set low, is that the equilibrium yield can become unattractive to solo stakers, particularly if there is no MEV burn to reduce reward variability (as discussed in Section 2.3). Another issue is the additional logic required to facilitate the negative yield (e.g., a <a href="https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448#h-3-consensus-incentives-11">staking fee</a> deducted each epoch). Figure 10 in <a href="https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448#h-42-effect-of-pooling-14">this post</a> shows a realistic scenario with negative regular income for solo stakers under a staking fee. A third downside is that a negative issuance yield can create a “consensus design debt” due to additional analytical and implementational complexity whenever micro incentives are adjusted. Figure 1 outlined the approximate range I see as suitable for a Category 4 design, with the cap reached at around 2/3 or 3/4 of all ETH staked. The figure refers to it as a “hard endgame” due to its complexity and hard cap.</p>
<p><a href="https://ethresear.ch/t/faq-ethereum-issuance-reduction/19675#h-2-temper-issuance-32">Category 2</a> instead involves a more modest reduction of issuance. It represents the type of reduction that would be <a href="https://ethresear.ch/t/faq-ethereum-issuance-reduction/19675#what-is-a-desirable-issuance-reduction-for-the-near-future-9">desirable for the near future</a> and does not require incorporating additional logic to maintain proper consensus incentives. It has been <a href="https://ethereum-magicians.org/t/electra-issuance-curve-adjustment-proposal/18825">proposed</a> as a first step followed by a Category 4 change. A longer write-up on a reward curve with tempered issuance is available <a href="https://ethresear.ch/t/reward-curve-with-tempered-issuance-eip-research-post/19171">here</a>. While it is likely that this type of reward curve could be viable for a long time, a potential downside is the lack of near-certainty.</p>
<p><a href="https://ethresear.ch/t/faq-ethereum-issuance-reduction/19675#h-3-cut-issuance-33">Category 3</a> occupies a middle ground between the two approaches, with curves retaining a positive issuance yield being most relevant. Issuance is then cut to a very low level when too much ETH is staked, yet remains positive. While consensus/consolidation incentives and solo staking can become strained, diligent solo stakers will not need to “pay to stake” while waiting to propose a block, and there is lower “consensus design debt”. This constitutes a practical endgame, if the goal is to make one near-term adjustment to issuance that has a high probability of lasting indefinitely. In reality, the difference between Categories 2 and 3 is relatively minor in the sense that they will likely lead to similar equilibrium outcomes. However, it might lend credibility to the notion of aiming for an endgame if the issuance yield at higher quantities of stake is set to a minimum (positive) level.</p>
<p>It has been <a href="https://x.com/ryanberckmans/status/1778124458303111658">argued</a> that Ethereum must aim for one single change to its issuance policy, credibly positioned as the last change ever needed. My personal contention is that there are many benefits to a <a href="https://ethresear.ch/t/reward-curve-with-tempered-issuance-eip-research-post/19171#h-23-graduated-approach-11">graduated approach</a>. If there was community support to set the reward curve at an issuance rate of 0.5% with a commitment to revisit the issue in a few years’ time, I would support it. It would make the process less complicated in the near term. However, the political nature of an issuance change and community reactions seem to support pursuing the endgame policy through a single change.</p>
<p>Due to the inherent costs of staking and the limited relevance of exogenous yield, a Category 3 change offers a credible case to be the last monetary policy change required. Responsibility then falls on developers to implement MEV burn moving forward. A MEV burn mechanism would: (1) alleviate the strain on consensus design parameters from low issuance at a high quantity of stake, and (2) make a lower equilibrium quantity of stake more likely. Note also that instituting a Category 4 stake cap does not preclude the necessity of future changes. Quantity of stake is not the only factor at play when Ethereum aims to balance issuance policy trade-offs. Indeed, key issues twenty years from now may differ greatly from those in focus today, just as many of the issues we currently face were not anticipated just a few years ago. A stake cap issuance policy could therefore need to be reversed, just like any other policy. In this case, the reversal would more likely be in the form of an increase in issuance, a direction that Ethereum has so far avoided.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50642-h-33-preferred-issuance-range-for-a-practical-endgame-10" name="p-50642-h-33-preferred-issuance-range-for-a-practical-endgame-10"></a>3.3 Preferred issuance range for a practical endgame</h3>
<p>Figure 2 roughly outlines a preferred range for a practical endgame reward curve (previously shown in green in Figure 1). This range represents my personal preferences, given early sketches of consolidation/consensus incentives. Others may have different preferences, which should be collated and further debated. Sketches of the incentives design may also evolve, leading to minor tweaks to the range. The dashed green curve is the previously proposed <a href="https://ethresear.ch/t/reward-curve-with-tempered-issuance-eip-research-post/19171">reward curve with tempered issuance</a>. If the requirement is an endgame, it seems reasonable to go lower than the green curve at higher deposit sizes. The preferred range suggests still issuing at least 60k ETH if everyone stakes (an issuance yield of around 0.05%) and at most 100k ETH more than that (160k ETH). My contention is that the only scenario where an equilibrium is approached at this level is if MEV is much higher than today. The idea is that the last fraction of would-be stakers have relatively very high reservation yields (require a rather high staking yield to stake), which should probabilistically be accounted for. A lower positive issuance might then not influence the equilibrium much anyway, while likely requiring us to push small solo stakers into negative regular rewards in the presence of MEV, something I find undesirable.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/5/9524b9c61add92bac15d043bdb501ffc0c3eb57e.png" title="Figure 2"><img alt="Figure 2" height="423" src="https://ethresear.ch/uploads/default/optimized/3X/9/5/9524b9c61add92bac15d043bdb501ffc0c3eb57e_2_690x423.png" width="690" /></a></div><p></p>
<p><strong>Figure 2.</strong> Preferred range for issuance across staking deposit size <span class="math">D</span> for a practical endgame issuance policy (personal rough view). The policy should ensure a sufficient issuance yield at low deposit sizes (but not more), and also such a low issuance yield at high deposit sizes that an equilibrium is extremely unlikely. For improved viability, issuance is still maintained at a positive level throughout.</p>
<p>Assume for example that developers are unable to institute MEV burn and that an equilibrium at a high deposit size is reached when MEV is 600k ETH per year (a doubling of the long-run average, which seems perfectly possible). Whether issuance is 0 ETH or 100k ETH will then not alter the equilibrium substantially. Issuance will at least certainly be of less importance to the delegating staker, who effortlessly derives pooled MEV rewards. Yet, if issuance is 0 ETH instead of 100k ETH, this presumably requires a larger redesign of the micro incentives and will force solo stakers to pay to stake, in the hope of getting the chance to propose a block. If MEV burn is instituted, an equilibrium will not be reached here anyway, and an issuance of 100k ETH thus still has little relevance. The upper limit to the preferred range was defined according to the following intuition:</p>
<ul>
<li>It can be desirable for Ethereum users to cap the issuance rate at 0.5% (see Section 3.4), producing a maximum 4% issuance yield at 15M ETH staked, 3% at 20M, and 2% at 30M.</li>
<li>The issuance rate should presumably not remain fixed at 0.5% for an endgame policy due to the diminishing utility of additional security past a certain level. A further reduction starting at the current deposit size (35M ETH) or lower then seems desirable.</li>
<li>Issuance should not be higher than absolutely necessary at the highest quantity of stake, due to how undesirable such an equilibrium is. Going above 160k ETH (around half of the yearly MEV measured in <a href="https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448">this post</a>; 0.133% in issuance yield) seems excessive—it should reasonably be possible to design viable consensus/consolidation incentives below that level.</li>
</ul>
<p>Presumably, the first fraction of stakers have relatively low reservation yields (require a rather low yield to stake), which should also probabilistically be accounted for. However, if the reward curve must remain fixed forever, Ethereum should issue slightly more tokens than what is needed to achieve an equilibrium at a desirable deposit size. The reason is that the MEV might eventually be burned, which must be factored in. With current thinking about desirable deposit sizes, it then seems beneficial to keep issuance at or ideally above the green dashed curve at lower quantities of stake.</p>
<p>A question is then how fast issuance should fall down to some minimum acceptable level as the deposit size increases. My contention here is that it seems reasonable to keep the issuance rate above 0.1% up to at least half the ETH is staked. The reason is that a 0.2% staking yield (0.1% issuance rate at 60M ETH staked, with MEV burn in place) could perhaps become more consequential to Ethereum, for example, in terms of effects on decentralization, than the fact that half the ETH is staked. This is in line with the philosophy outlined in Section 3.1—evaluating the utility of different equilibria along an upward-sloping supply curve. Of course, it is highly unlikely that an equilibrium would be established at such a low yield at 60M ETH staked, but the hypothetical options in that scenario must still be evaluated.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50642-h-34-tangible-framework-never-exceed-an-issuance-rate-of-05-11" name="p-50642-h-34-tangible-framework-never-exceed-an-issuance-rate-of-05-11"></a>3.4 Tangible framework: never exceed an issuance rate of 0.5%</h3>
<p>A practical endgame should ideally position the chosen reward curve within a tangible framework that is easy to understand. The upper grey line in Figures 1 and 2 represents an issuance of 0.5% of the circulating supply each year, i.e., an issuance rate of <span class="math">i=0.005</span>. From a communication perspective, committing to never issuing more than 0.5% of the circulating supply each year is an accessible policy with “memetic” qualities. Taking some liberties, it also fits within the “power-of-two” framework favored in Ethereum: a maximum of <span class="math">2^{-1}</span>% of the supply. Note that the framework is not only applicable to the practical endgame reward curve; it is intended to apply forever. Even if in the future there is a push for an issuance change, there could still be an existing social commitment, making an increase to the issuance rate above 0.5% particularly difficult to push through. Whereas a cap on the circulating supply is an untenable monetary policy, a cap on issuance rate is not. Yet it has the same simplicity.</p>
<p>The <a href="https://ethresear.ch/t/circulating-supply-equilibrium-for-ethereum-and-minimum-viable-issuance-during-the-proof-of-stake-era/10954">circulating supply</a> will <a href="https://www.youtube.com/watch?v=LtEMabS0Oas&amp;t=1187s">drift</a> to <a href="https://twitter.com/weboftrees/status/1710725744651825281">balance</a> supply, demand, and protocol income. Therefore, the pledge would ultimately be enforced by <a href="https://x.com/weboftrees/status/1710728179260731715">swapping</a> out <span class="math">D</span> for <span class="math">d</span> in the equation for the reward curve and normalizing by including the circulating supply at the time of the swap, once the circulating supply <a href="https://ethresear.ch/t/endgame-staking-economics-a-case-for-targeting/18751#how-to-set-the-target-in-relative-staking-ratio-instead-of-absolute-fixed-eth-amount-terms-20">begins to be tracked</a> at the consensus layer.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50642-h-4-practical-endgame-reward-curves-12" name="p-50642-h-4-practical-endgame-reward-curves-12"></a>4. Practical endgame reward curves</h2>
<p>This section proposes practical endgame reward curves, also including further analysis of the trade-offs Ethereum faces. Examples will be constructed to peak at <span class="math">i=0.5\%</span>, but this peak can be adjusted if desirable by altering the scale parameter (often denoted <span class="math">k</span>). In particular, the peak of any curve can always be reduced slightly while still remaining below <span class="math">i=0.5\%</span>.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50642-h-41-classical-tempering-13" name="p-50642-h-41-classical-tempering-13"></a>4.1 Classical tempering</h3>
<h4><a class="anchor" href="https://ethresear.ch#p-50642-issuance-14" name="p-50642-issuance-14"></a>Issuance</h4>
<p>Figure 3 provides examples using the classical tempering mechanism. The particular construction of these reward curves was first motivated by its <a href="https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448#h-51-a-neutral-reward-curve-19">minimal spec change</a>, as well as ensuring no issuance increase at any point. Note further that the generated smooth decay in issuance is desirable in light of discouragement attacks (<a href="https://github.com/ethereum/research/blob/09d9f34042262c8fb436171786ed6c62e1f57247/papers/discouragement/discouragement.pdf">1</a>, <a href="https://ethresear.ch/t/reward-curve-with-tempered-issuance-eip-research-post/19171#h-53-discouragement-attacks-32">2</a>) and <a href="https://ethresear.ch/t/reward-curve-with-tempered-issuance-eip-research-post/19171#h-54-cartelization-attacks-33">cartelization attacks</a>.</p>
<p>Green reward curves are constructed by dividing the equation of the current reward curve by <span class="math">1+D/k</span>, where <span class="math">k</span> also denotes peak stake participation. The shape of the curve can be altered by exponentiating <span class="math">D</span>, and the peak position (scale) can be altered by changing <span class="math">k</span>. The dashed green curve is the same as presented in Figure 2, whereas the full green curve increases <span class="math">k</span> so that the curve peaks at <span class="math">i=0.5\%</span>, the level marked by the grey line. The curves of other colors in the figure are constructed by increasing the exponentiation in steps of 0.5 up to 3.5 (for the yellow curve), adjusting <span class="math">k</span> to always produce a peak at <span class="math">i=0.5\%</span>. The purple curve is thus constructed through division by <span class="math">1+(D/k)^2</span>. The peak will then be located at <span class="math">D=k\sqrt{3}</span>, and the variable <span class="math">k</span> was in this case set to <span class="math">40\times10^6</span> to produce the peak at <span class="math">i=0.5\%</span>. In Figure 15 of the FAQ, a slightly lower setting for <span class="math">k</span> was instead relied upon for the purple curve.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/0/a0049de05847bf07cd1981d1cb9e4773a754c7b4.png" title="Figure 3"><img alt="Figure 3" height="423" src="https://ethresear.ch/uploads/default/optimized/3X/a/0/a0049de05847bf07cd1981d1cb9e4773a754c7b4_2_690x423.png" width="690" /></a></div><p></p>
<p><strong>Figure 3.</strong> Possible shapes of reward curves that temper issuance, constructed by altering the exponentiation of <span class="math">D</span> in the term added to the denominator of the current reward curve equation. The preferred range from Figure 2 is indicated in grey.</p>
<p>Which reward curve that is optimal will depend on how various trade-offs discussed in Sections 2-3 should be balanced, and this will naturally be subject to diverse opinions. The red reward curve is the only option that remains fully within the preferred range presented in Figure 2, but the orange and purple reward curves are also almost within that range. Speculatively, it might be easiest to reach an agreement on one of these three shapes, scaled as desired.</p>
<p>Note concerning the red reward curve that the exponentiation by 2.5 in the added term naturally combines with the exponentiation by 0.5 of the current reward curve. The resulting equation for issuance yield can therefore be rewritten simply as:</p>
<div class="math">
y_i = \frac{cF}{\sqrt{D} + (D/k)^3},
</div>
<p>which just requires an adjustment to <span class="math">k</span>. Specifically for the plotted curve, <span class="math">k</span> must be reduced from around <span class="math">35.4\times10^6</span> to <span class="math">1.95\times10^6</span>. We could thus refer to the red shape as a “cubed” reward curve and the orange shape as a “cubed+” reward curve, with the purple then denoted “squared+”, etc.  The yellow curve is created by increasing the exponent from 3 to 4  in the previous equation (thus denoted “quartic”). That reward curve brings issuance very close to zero at high quantities of stake.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50642-staking-yield-with-mev-15" name="p-50642-staking-yield-with-mev-15"></a>Staking yield with MEV</h4>
<p>The staking yield, inclusive of 300k ETH MEV per year (roughly the long-running average), is shown for the reward curves of Figure 3 in Figure 4. To make the discussion more tangible, equilibria under a hypothetical blue supply curve are marked by a circle, providing plausible scenarios a few years from now. Note that the supply curve will be nearly vertical at short time-scales due to frictions in the decision to stake (even when ignoring the deposit queue), but bends with time, and the focus here is the long run. The hypothetical supply curve would result in an equilibrium of 50M ETH staked and a yield of 2.9% under the current reward curve.</p>
<p>The other reward curves produce equilibria within 34M-40M ETH staked and a yield of 2-2.3%. It can be interesting to consider an extreme <a href="https://ethresear.ch/t/reward-curve-with-tempered-issuance-eip-research-post/19171#h-423-low-yield-scenario-26">low-yield scenario</a>, where the supply curve is much lower than the most likely outcome—perhaps a decade or two from now. Such a hypothetical supply curve is indicated by a dashed blue line. There should hopefully be rather broad agreement that the equilibrium under the current reward curve is not desirable in this scenario. At a staking yield of around 1.9%, issuance is 1.7M ETH—so high that it is located outside the boundaries of Figures 1-3. There is furthermore no longer a <a href="https://x.com/fradamt/status/1760808900792594593">trustless</a> sound primary currency in the Ethereum economy in the form of non-staked ETH. All holders of ETH or its derivatives are potentially worse off than under an equilibrium enforced at a lower quantity of stake.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/0/201cf37ccd91831cbf13a5f7172c07befeff6a61.jpeg" title="Figure 4"><img alt="Figure 4" height="481" src="https://ethresear.ch/uploads/default/optimized/3X/2/0/201cf37ccd91831cbf13a5f7172c07befeff6a61_2_690x481.jpeg" width="690" /></a></div><p></p>
<p><strong>Figure 4.</strong> Staking yield, inclusive of 300k ETH/year in MEV, for tempered reward curves of different shapes. Equilibria with a hypothetical supply curve a few years from now (blue) are indicated by circles. An unlikely very low supply curve is also illustrated by the dashed blue line, with hypothetical equilibria indicated by squares.</p>
<p>The equilibrium quantity of stake for the outlined reward curves varies between around 57M-80M ETH with the very low supply curve. That is above a desirable level. However, this is really the extreme scenario, where MEV burn has not materialized, and half of the token holders are ready to assume—directly or through delegation—the costs of staking under a total staking yield of 0.75%. Reducing issuance further to temper staking might not be desirable. The black dashed line indicates the outcome with no issuance at all. The equilibrium quantity of stake is then not reduced substantially relative to the equilibrium with the lower reward curves, in my view rendering such an approach an unmotivated sacrifice of solo staking viability and increase in consensus design complexity (see also Section 2.3 and Section 3).</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50642-issuance-yield-16" name="p-50642-issuance-yield-16"></a>Issuance yield</h4>
<p>Figure 5 instead shows issuance yield with no MEV, which could be the situation after incorporating a fully successful MEV burn mechanism. The equilibrium under the investigated reward curves then ends up at around 30M ETH staked for the outlined hypothetical supply curve. Even under the very low supply curve, the equilibrium is pushed down to between 39M-57M ETH: MEV burn will be a key component for achieving a low quantity of stake if the issuance yield remains positive.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/1/f181f5ec61c49483660c3cf61357e7218417b6be.jpeg" title="Figure 5"><img alt="Figure 5" height="481" src="https://ethresear.ch/uploads/default/optimized/3X/f/1/f181f5ec61c49483660c3cf61357e7218417b6be_2_690x481.jpeg" width="690" /></a></div><p></p>
<p><strong>Figure 5.</strong> Issuance yield for tempered reward curves of different shapes, which would also be the staking yield under full MEV burn. Equilibria with a hypothetical supply curve (blue) a few years from now are indicated by circles. An unlikely very low supply curve is also illustrated by the dashed blue line, with hypothetical equilibria indicated by squares.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50642-h-42-issuance-floor-17" name="p-50642-h-42-issuance-floor-17"></a>4.2 Issuance floor</h3>
<p>The reward curve can also be designed to smoothly transition from the current reward curve to an issuance floor <span class="math">I_f</span>, set at some desirable level. Three examples are shown in Figure 6. The light blue curve is constructed by blending with a sigmoid weight computed as</p>
<div class="math">
w = \frac{1}{1 + 2^{(D - D_c)/-k}}.
</div>
<p>The central point of the blend was set to <span class="math">D_c=32\times10^6</span> and the steepness of the transition set to <span class="math">k=7\times10^6</span>. This can be adjusted as desired. The curve then blends <span class="math">(1-w)I_r+wI_f</span>, where <span class="math">I_r</span> is issuance for the current reward curve [i.e., <span class="math">I_r(D)</span>] and <span class="math">I_f</span> is set to <span class="math">120\,000</span> (<span class="math">i=0.1\%</span>). Another solution is to simply blend between the maximum and minimum desired issuance level by replacing <span class="math">I_r</span> by, for example, a fixed <span class="math">600\,000</span> ETH. This is done with the pink curve, illustrating a piecewise option of transitioning from the current reward curve at the point where the sigmoidal weight intercepts it.</p>
<p>A third option is to employ a <a href="https://en.wikipedia.org/wiki/Hill_equation_(biochemistry)">Hill-type equation</a>:</p>
<div class="math">
Y(D) = \frac{D_{h}^{n}I_r + D^{n}I_f }{D^{n} + D_{h}^{n}}.
</div>
<p>It is a fairly clean construction, relying on a specified halfway point <span class="math">D_{h}</span> between <span class="math">I_r</span> and <span class="math">I_f</span>, as well as the exponent <span class="math">n</span> that further determines the shape. The brown curve was constructed from <span class="math">D_{h}=30\times10^6</span> and <span class="math">n=3</span>, setting <span class="math">I_f=90\,000</span>.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/2/529ba1036f4e8f5b30d9316329ab93ef6894c9c4.png" title="Figure 6"><img alt="Figure 6" height="423" src="https://ethresear.ch/uploads/default/optimized/3X/5/2/529ba1036f4e8f5b30d9316329ab93ef6894c9c4_2_690x423.png" width="690" /></a></div><p></p>
<p><strong>Figure 6.</strong> Three reward curves that asymptotically approach an issuance floor around <span class="math">i=0.1\%</span>. The preferred range outlined in Figure 2 is once again indicated in grey.</p>
<p>Figure 7 shows the staking yield inclusive of 300k ETH of MEV/year, investigating the same features as in Figure 4. The reward curves with an issuance floor provide an equilibrium staking of around 60M ETH for the low supply curve, approximately in line with the orange reward curve of the previous examples. Since issuance is kept fixed around the floor, the proportion of the MEV (“No issuance”) relative to issuance+MEV remains approximately constant at higher quantities of stake. Figure 8 instead shows the outcome with only issuance yield.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/2/1213843d6a485f14de4c57c553dce20c9fa6256c.jpeg" title="Figure 7"><img alt="Figure 7" height="481" src="https://ethresear.ch/uploads/default/optimized/3X/1/2/1213843d6a485f14de4c57c553dce20c9fa6256c_2_690x481.jpeg" width="690" /></a></div><p></p>
<p><strong>Figure 7.</strong> Staking yield, inclusive of 300k ETH/year in MEV, for reward curves that asymptotically approach an issuance floor. Equilibria with a hypothetical supply curve a few years from now (blue) are indicated by circles. An unlikely very low supply curve is also illustrated by the dashed blue line, with hypothetical equilibria indicated by squares.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/3/93bca5e5f62a9b4d058b575fca37e2b410187bc6.png" title="Figure 8"><img alt="Figure 8" height="481" src="https://ethresear.ch/uploads/default/optimized/3X/9/3/93bca5e5f62a9b4d058b575fca37e2b410187bc6_2_690x481.png" width="690" /></a></div><p></p>
<p><strong>Figure 8.</strong> Issuance yield for reward curves that asymptotically approach an issuance floor, which would also be the staking yield under full MEV burn. Equilibria with a hypothetical supply curve a few years from now (blue) are indicated by circles. An unlikely very low supply curve is also illustrated by the dashed blue line, with hypothetical equilibria indicated by squares.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50642-h-43-piecewise-constructions-18" name="p-50642-h-43-piecewise-constructions-18"></a>4.3 Piecewise constructions</h3>
<p>Smooth reward curves are not strictly about aesthetics. It seems reasonable to ensure that there is no discontinuity that can influence the decision to stake, for example making cartelization attacks more attractive at some specific range or point. The overarching issuance policy expansion path is arguably also smooth. However, linear piecewise constructions bring the benefit of simplicity, and the downsides may not be sufficient to forego that. Figure 9 shows three linear constructions. The dark blue and cyan reward curves reduce issuance by 1 ETH for every 100 ETH that is staked, in between an issuance rate of 0.5% and 0.1%. The beige reward curve is instead symmetrical around the mid point of 60M ETH, reducing issuance by 1 ETH for every 125 ETH that is staked while taking the issuance rate from 0.5% to 0.1%.</p>
<p>These constructions provide some tangible anchors that might simplify communication. Issuance is always easy to calculate, and at an issuance rate of 0.5%, the issuance yield becomes 4% at 15M ETH (1/8 of the supply) staked, 3% at 20M ETH (1/6), and 2% at 30M ETH (1/4). An issuance yield of 1% is reached at 40M ETH or 45M ETH, respectively, for the dark blue and cyan reward curve, and at 60M ETH (1/2), the issuance yield is 0.33% or 0.5% respectively.</p>
<p>The dashed dark red reward curve simply sets issuance to an issuance rate of 0.5%. This type of reward curve has been referred to as “<a href="https://notes.ethereum.org/@anderselowsson/Reward-curve-with-capped-issuance">capped issuance</a>” and was implied in a <a href="https://ethresear.ch/t/simplified-active-validator-cap-and-rotation-proposal/9022">proposal</a> by Vitalik in early 2021. It is not necessarily an endgame policy and would have to be adopted with the understanding that we might return to the conversation again in a few years. It would enable us to address the issue of a growing quantity of stake so that we can focus on other topics for a few years—until kinks related to for example MEV and Orbit SSF have been worked out. As previously mentioned, I find this type of solution appealing, but from the community’s point of view, there seems to be an apparent appetite for definite answers.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/4/d42dbd3d48521e49fa18487bcca4e0448bf2ef8d.png" title="Figure 9"><img alt="Figure 9" height="423" src="https://ethresear.ch/uploads/default/optimized/3X/d/4/d42dbd3d48521e49fa18487bcca4e0448bf2ef8d_2_690x423.png" width="690" /></a></div><p></p>
<p><strong>Figure 9.</strong> Illustrating piecewise constructions with issuance changing linearly or remaining fixed.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50642-h-5-conclusions-and-important-questions-19" name="p-50642-h-5-conclusions-and-important-questions-19"></a>5. Conclusions and important questions</h2>
<p>There are several downsides to an issuance policy that provides 100% certainty of tempering the quantity of stake to reasonable levels, as opposed to one that provides 99.9% certainty. This post has suggested practical endgame reward curves that will temper the growth in the quantity of stake without introducing unnecessary political, analytical, and implementational complexity. My personal preference, fitting within the outlined preferred range, would be to use the shape of either the orange, red, or purple tempered reward curves presented in Section 4.1, where the red curve fits fully within the preferred range. These curves make an equilibrium at a high quantity of stake very unlikely, yet can allow small solo stakers to always receive positive regular rewards when performing their duties adequately. Simply setting issuance to a 0.5% issuance rate for now, as illustrated by the red dashed curve in Figure 9, would in my view also be an appealing solution. The idea would be to then return to the conversation in a few years if necessary, once various issues such as MEV burn have been worked out. But this solution may not appeal to the Ethereum community.</p>
<p>A clear benefit of a practical endgame is that it has less dependencies and will not fail if MEV burn does not come to fruition under a low supply curve. This allows us to address issuance without unnecessary delay. On a related note, focusing on unrealistic scenarios—such as everyone staking at near-zero issuance yields—would be unfortunate, because we might then fail to move forward when a change would be very beneficial.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50642-questions-for-the-community-and-researchers-20" name="p-50642-questions-for-the-community-and-researchers-20"></a>Questions for the community and researchers</h3>
<p>Community feedback and debate would be welcome. For example:</p>
<ul>
<li>Is it ever okay to have solo stakers that do not pool block proposal rewards lose ETH while attesting diligently, as long as they attain a positive expected yield in the long run from infrequent block proposals? Should it be strictly avoided, or would it be acceptable at some higher deposit size to stem further growth, say at 60/90/120M ETH staked? At what percentage of being offline over a year would it be acceptble with negative yield?</li>
<li>What should the issuance (or equivalently, issuance yield) be set to at 15M, 30M, 45M, 60M, 75M, 90M, and 120M ETH staked? Community members and researchers are welcome to specify their own “preferred range”.</li>
<li>Given current uncertainty related to MEV and the benefits of reducing issuance, would you be supportive of simply fixing issuance at an issuance rate of 0.5% (grey line in the figures; red dashed line in Figure 9) for the next 3/4/5/6 years, with a commitment to return to the issue after that?</li>
</ul>
<p>From a research perspective, given the dependency on MEV burn and Orbit SSF, it seems that a lot would be gained by mapping out the likelihood of implementing these mechanisms within specific time frames. This is something that we as consensus researchers could do while we diligently keep working on the implementation details.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/practical-endgame-on-issuance-policy/20747">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 23 Oct 2024 14:44:39 +0000</pubDate>
</item>
<item>
<title>Proposal for a L2 Keystore based on global SSI standards and reference implementations</title>
<link>https://ethresear.ch/t/proposal-for-a-l2-keystore-based-on-global-ssi-standards-and-reference-implementations/20732</link>
<guid>https://ethresear.ch/t/proposal-for-a-l2-keystore-based-on-global-ssi-standards-and-reference-implementations/20732</guid>
<content:encoded><![CDATA[
<div> 关键词：Sidetree、DIDs、Ethereum、L2网络、key管理

总结：
我们提议构建一个基于Sidetree协议的二级（L2）密钥存储网络，由以太坊生态系统中值得信赖的实体，如L2运营商和大型企业部署并运营，以此作为公共基础设施。该网络将提供一个标准化的跨链和低成本非托管密钥管理解决方案，支持W3C去中心化身份标识符（DIDs）。文章指出，随着以太坊经历扩容（L2 rollups）、智能合约钱包安全性和隐私增强等三个重大转变，DIDs和可验证凭证（VCs）能解决用户在身份、密钥和地址管理方面面临的挑战。Sidetree作为一个可在现有区块链网络之上运行的L2协议，能够通过批量处理和数据锚定实现降低成本和提高扩展性，并具有区块链无关性。我们建议建立这样一个L2层的Sidetree网络，以实现跨链DID解析、降低密钥管理操作成本、利用零知识证明保护隐私、支持与智能合约钱包结合以及促进与其他DID系统和应用的互操作性和标准化，同时避免供应商锁定。 <div>
<p>TLDR: We propose a Sidetree-based key store L2 network as a public utility deployed and operated by trusted Ethereum ecosystem entities such as L2 operators and large enterprises supporting Ethereum as a standardized cross-chain and low-cost non-custodial key management stack using W3C Decentralized Identifiers (DIDs).</p>
<p><strong>Background</strong></p>
<p>The <a href="https://github.com/ethereum-oasis-op/L2" rel="noopener nofollow ugc">Ethereum Oasis Community Projects L2 Standards WG</a> recently published <a href="https://entethalliance.org/w3cs-did-and-vc-technology-can-help-with-ethereums-three-transitions/" rel="noopener nofollow ugc">a report</a> " How W3C DIDs and VCs can help with Ethereum’s Three Transitions". This report aligns with the recent posts (<a href="https://ethresear.ch/t/self-sovereign-identity-and-account-abstraction-for-privacy-preserving-cross-chain-user-operations-across-roll-ups/19599">1</a> and <a href="https://ethresear.ch/t/enabling-standardized-on-chain-executions-through-modular-accounts/20127">2</a>) by <a class="mention" href="https://ethresear.ch/u/eugere">@EugeRe</a>. In our report, we discuss how the integration of W3C Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs) can address challenges faced by Ethereum as it undergoes its three major transitions: scaling through L2 rollups, enhancing wallet security via smart contract wallets, and advancing privacy. Ethereum’s transitions necessitate changes in how users manage identities, keys, and addresses. DIDs and VCs, core components of the decentralized identity ecosystem, offer solutions for these challenges.</p>
<p>We argue that DIDs provide globally unique, resolvable identifiers, while VCs enable verifiable claims about identity, attributes, or qualifications. By leveraging DIDs and VCs, Ethereum can improve identity management, key rotation and recovery, and privacy. DID documents can store addresses across various networks and facilitate key management, including social recovery. Additionally, zero-knowledge proofs can enhance privacy when using keys from DID documents in Ethereum ecosystem transactions both on and offchain.</p>
<p>Several DID methods, based on the <a href="https://identity.foundation/sidetree/spec/" rel="noopener nofollow ugc">Sidetree Protocol</a>, a Layer 2 DID standard with <a href="https://github.com/decentralized-identity/sidetree" rel="noopener nofollow ugc">reference implementations</a>, are suitable for the Ethereum ecosystem, offering permissionless, blockchain-anchored, scalable, and cost-effective solutions. The Sidetree Protocol was developed by the Decentralized Identity Foundation (<a href="https://identity.foundation/" rel="noopener nofollow ugc">DIF</a>) and enables the creation and management of scalable DIDs on various blockchain networks, making it a blockchain-agnostic solution. This allows DIDs to be anchored to different distributed ledger technologies such as Ethereum Mainnet.</p>
<p><strong>Key Technical Aspects</strong></p>
<ul>
<li>
<p><strong>Layer 2 Protocol:</strong> Sidetree operates as a layer on top of existing blockchain networks, decoupling DID operations from the base layer’s transaction limitations. This allows for improved scalability and reduced costs compared to directly interacting with the blockchain for every DID operation.</p>
</li>
<li>
<p><strong>Blockchain Agnostic:</strong> The protocol is designed to work with any blockchain that supports anchoring data, providing flexibility in choosing the underlying network. Popular implementations exist on blockchains like Bitcoin (ION) and Ethereum (<a href="https://transmute.industries/technology/element/" rel="noopener nofollow ugc">Element</a>).</p>
</li>
<li>
<p><strong>Decentralized PKI (DPKI):</strong> Sidetree leverages a decentralized public key infrastructure, where DID controllers hold the private keys associated with their DIDs. This empowers users with full control over their digital identities.</p>
</li>
<li>
<p><strong>DID Operations:</strong> The core operations supported by Sidetree are Create, Update, Recover, and Deactivate. These actions allow for the lifecycle management of DIDs and their associated DID documents.</p>
</li>
<li>
<p><strong>Content-Addressable Storage (CAS):</strong> Sidetree utilizes CAS systems like IPFS or Filecoin to store DID documents and other related data. This ensures data immutability and availability through a decentralized network.</p>
</li>
<li>
<p><strong>Batching and Anchoring:</strong> To optimize efficiency, Sidetree nodes batch multiple DID operations together and anchor them to the underlying blockchain in a single transaction. This significantly reduces transaction costs and improves throughput.</p>
</li>
<li>
<p><strong>Conflict-Free Replicated Data Types (CRDTs):</strong> Sidetree employs CRDTs to manage DID document updates and resolve conflicts in a decentralized manner. This ensures data consistency across the network.</p>
</li>
</ul>
<p>In the Sidetree protocol, updating a DID document is a multi-step process as summarized below:</p>
<ol>
<li><strong>Generate Update Payload</strong>: The DID controller creates an update payload, which includes the following components:</li>
</ol>
<ul>
<li>
<p><code>didSuffix</code>: The unique suffix of the DID being updated.</p>
</li>
<li>
<p><code>revealValue</code>: The revealed value of the previous update commitment, used for verification.</p>
</li>
<li>
<p><code>patches</code>: An array of JSON Patch operations specifying the modifications to be made to the DID Document.</p>
</li>
<li>
<p><code>updateCommitment</code>: A new commitment value for the next update operation.</p>
</li>
</ul>
<ol start="2">
<li>
<p><strong>Submit to Sidetree Node</strong>: The DID controller submits the update payload to a Sidetree node.</p>
</li>
<li>
<p><strong>Batching and Anchoring</strong>: The Sidetree node collects multiple update operations and other DID operations, batches them together into an anchor file, and anchors this file to the underlying blockchain. The anchor file contains hashes of the operations and other metadata, but not the actual operation data.</p>
</li>
<li>
<p><strong>Store Operation Data</strong>: The Sidetree node stores the full update operation data, along with other operations, in a ‘chunk file’ and makes it available via a content-addressable storage (CAS) system like IPFS or Filecoin. The anchor file on the blockchain contains a reference to this chunk file.</p>
</li>
<li>
<p><strong>Resolution</strong>: When another entity wants to resolve the DID and obtain the latest DID Document, they query a Sidetree node.</p>
</li>
<li>
<p><strong>Retrieve and Apply Updates</strong>: The Sidetree node retrieves the relevant anchor files from the blockchain, follows the references to the chunk files on the CAS, and applies all the update operations in chronological order to the original DID Document to construct the latest version.</p>
</li>
</ol>
<p>The Sidetree protocol furthermore incorporates several security measures to ensure the integrity of DID operations and mitigate potential attack vectors:</p>
<p><strong>1. Decentralized Public Key Infrastructure (DPKI)</strong></p>
<ul>
<li><strong>DID Controller Holds Private Keys:</strong> Each DID controller possesses the private keys associated with their DIDs, giving them complete control over their identity data. This eliminates the risk of a single point of failure or compromise associated with centralized key management systems.</li>
</ul>
<p><strong>2. Commitment Scheme</strong></p>
<ul>
<li>
<p><strong>Update Commitments:</strong> Each update operation includes a commitment to the next update, cryptographically linking successive updates. This prevents unauthorized modifications or tampering with the DID Document’s history.</p>
</li>
<li>
<p><strong>Reveal Value:</strong> The reveal value associated with the previous update commitment ensures that only the DID controller with the corresponding private key can initiate a new update.</p>
</li>
</ul>
<p><strong>3. Batching and Anchoring</strong></p>
<ul>
<li>
<p><strong>Blockchain Anchoring:</strong> Batching multiple DID operations into anchor files and anchoring them to the blockchain provides a tamper-proof and auditable history of operations.</p>
</li>
<li>
<p><strong>Content-Addressable Storage (CAS):</strong> Storing operation data in a CAS like IPFS or Filecoin further enhances immutability and prevents unauthorized modification.</p>
</li>
</ul>
<p><strong>4. Cryptographic Operations</strong></p>
<ul>
<li>
<p><strong>Digital Signatures:</strong> Operations are signed using the DID controller’s private key, ensuring authenticity and non-repudiation.</p>
</li>
<li>
<p><strong>Hashing:</strong> Hash functions are used extensively for creating commitments, linking operations, and generating identifiers, adding another layer of security and integrity.</p>
</li>
</ul>
<p><strong>5. Conflict-Free Replicated Data Types (CRDTs)</strong></p>
<ul>
<li><strong>Deterministic Conflict Resolution:</strong> CRDTs enable the network to handle concurrent updates to the same DID Document and resolve conflicts in a consistent and predictable manner.</li>
</ul>
<p><strong>6. Network Redundancy and Decentralization</strong></p>
<ul>
<li>
<p><strong>Multiple Sidetree Nodes:</strong> The existence of multiple Sidetree nodes operated by different entities reduces the risk of a single point of failure or censorship.</p>
</li>
<li>
<p><strong>Peer-to-Peer Resolution:</strong> The ability to resolve DIDs by querying any Sidetree node further enhances decentralization and resilience.</p>
</li>
</ul>
<p><strong>Mitigation of Attack Vectors</strong></p>
<ul>
<li>
<p><strong>Unauthorized Updates:</strong> The commitment scheme and the requirement for the DID controller’s private key for updates prevent unauthorized modifications to the DID Document.</p>
</li>
<li>
<p><strong>Data Tampering:</strong> Blockchain anchoring and CAS storage ensure data immutability, making it difficult to tamper with past operations or DID documents.</p>
</li>
<li>
<p><strong>Censorship:</strong> The decentralized nature of the network with multiple nodes and peer-to-peer resolution mitigates the risk of censorship or denial of service attacks.</p>
</li>
<li>
<p><strong>Single Point of Failure:</strong> The distribution of private keys among DID controllers and the redundancy of Sidetree nodes reduce the risk of a single point of compromise.</p>
</li>
</ul>
<p><strong>Proposal</strong></p>
<p>Given the above, we propose that a Sidetree-based L2 key store network be deployed and operated as a public utility by trusted Ethereum ecosystem entities such as L2 operators, and large enterprises supporting Ethereum as a standardized cross-chain and low-cost non-custodial key management solution without vendor-lockin in contrast to any bespoke solution currently contemplated.</p>
<p>We envision the following beneficial characteristics of such an approach:</p>
<ol>
<li>
<p><strong>Cross-chain DID resolution:</strong> When interacting with different Ethereum networks (mainnet, testnets, or other L2s), the user’s DID can be resolved through the L2 Sidetree network. This allows applications and smart contracts on these networks to access the user’s DID document and verify their keys.</p>
</li>
<li>
<p><strong>Low-cost key management operations:</strong> Key rotation, recovery, and other key management operations can be performed on the L2 Sidetree network in batches significantly lowering individual transaction costs.</p>
</li>
<li>
<p><strong>ZK proofs for privacy:</strong> Zero-knowledge proofs can be used to selectively reveal information about the user’s keys and DID document while preserving privacy. This allows for secure and private interactions across different Ethereum networks without exposing sensitive data.</p>
</li>
<li>
<p><strong>Smart contract wallets and DIDs:</strong> Smart contract wallets can be associated with DIDs, enabling secure and decentralized key management for these wallets. The DID document can specify the controlling keys for the wallet, allowing for multi-signature and social recovery mechanisms.</p>
</li>
<li>
<p><strong>Interoperability and standardization:</strong> By leveraging the W3C DID standard and the Sidetree protocol, the L2 Sidetree network ensures interoperability with other DID systems and applications within the Ethereum ecosystem such as Polygon ID. This promotes a seamless and user-friendly experience for managing keys and identities across different chains.</p>
</li>
<li>
<p><strong>Avoiding Vendor Lock-In:</strong> By using global identity standards from the W3C and DIF together with open-source reference implementations and independent Sidetree node operators, the ecosystem can make cross-chain identity/key operations a public utility at very low to no costs.</p>
</li>
</ol>
<p>We are inviting and looking forward to comments on this proposal.</p>
            <p><small>2 posts - 2 participants</small></p>
            <p><a href="https://ethresear.ch/t/proposal-for-a-l2-keystore-based-on-global-ssi-standards-and-reference-implementations/20732">Read full topic</a></p>
]]></content:encoded>
<pubDate>Mon, 21 Oct 2024 18:58:56 +0000</pubDate>
</item>
<item>
<title>On Distributed FRI Computation</title>
<link>https://ethresear.ch/t/on-distributed-fri-computation/20697</link>
<guid>https://ethresear.ch/t/on-distributed-fri-computation/20697</guid>
<content:encoded><![CDATA[
<div> 分布式计算 FRI 协议 批量 FRI 分布式 FRI 可信证明<br /><br />总结:<br />
本文讨论了FRI协议的分布式计算方法。首先介绍了批量FRI协议，通过随机θ值计算线性组合来评估多个函数与RS码的接近程度。接着探讨了分布式FRI的情况，其中n个多项式由M个证明者分担处理。为提高效率，采用Merkle树承诺机制，各证明者将多项式承诺发送给主证明者。主证明者接收挑战θ并广播至各证明者，生成部分线性组合后发送回主证明者。主证明者执行标准FRI协议，对多项式求值和Merkle验证路径进行一致性检查，确保恶意行为可被检测。证明者的时间复杂度为O(d log d)，通信成本主要为传输部分线性组合，显著减少了验证最终证明所需的哈希调用次数。这种方法适用于分布式SNARK生成过程，有利于实施经济措施惩罚不合规行为。 <div>
<p>In this note we discuss the distributed computation of the FRI protocol. In practice, we often need to distribute the prover’s work across many servers. In the case of using a FRI-based proof system, this leads to the expensive recursive aggregation of the obtained proofs or exchanging data, the size of which is comparable to the size of the circuit. Below, we describe a technical trick that allows us to optimize obtaining a single final proof.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50535-batched-fri-1" name="p-50535-batched-fri-1"></a>Batched FRI</h3>
<p>The <strong>batched</strong> version of the <span class="math">\mathtt{FRI}</span> protocol allows one to estimate the closeness of each of the functions <span class="math">f_1, \dots, f_L</span> to the <span class="math">\mathsf{RS}</span> code. To do this, the <span class="math">\mathtt{Verifier}</span> samples and sends a random <span class="math">\theta \in \mathbb{F}_p</span> to the <span class="math">\mathtt{Prover}</span>. The latter calculates a linear combination</p>
<div class="math">
F = \theta^1 \cdot f_1 + \theta^2 \cdot f_2 + \dots + \theta^L \cdot f_L
</div>
<p>Then the <span class="math">\mathtt{Prover}</span> and <span class="math">\mathtt{Verifier}</span> execute the regular version of the <span class="math">\mathtt{FRI}</span> protocol for testing <span class="math">F</span>. The only difference is that each time <span class="math">F</span> is queried at point <span class="math">x</span>, the <span class="math">\mathtt{Verifier}</span> also performs a consistency check:</p>
<div class="math">
F(x) = \theta^1 \cdot f_1(x) + \theta^2 \cdot f_2(x) + \dots + \theta^L \cdot f_L(x).
</div>
<p>If the <span class="math">\mathtt{Verifier}</span> accepted in the end of the protocol, then all <span class="math">f_i</span> are close to <span class="math">\mathsf{RS}</span>.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50535-distributed-fri-2" name="p-50535-distributed-fri-2"></a>Distributed FRI</h3>
<p>Let us now consider a distributed setting in which <span class="math">n=L \cdot M</span> polynomials of degree at most <span class="math">d</span> are divided among <span class="math">M</span> <span class="math">\mathtt{Provers}</span>. The output of the protocol should be a proof that all the polynomials <span class="math">f_1, \dots, f_n</span> are close enough to the <span class="math">\mathsf{RS}</span> code. A naive approach would be to send all polynomials in plaintext to one of the provers, who in turn would execute the batched <span class="math">\mathtt{FRI}</span> protocol. Let us consider how this problem can be solved more efficiently.</p>
<p><span class="math">\mathtt{Provers}</span> generate <span class="math">\mathsf{Merkle~ Tree}</span> commitments to their polynomials and send them to the <span class="math">\mathtt{Master~Prover}</span> (this function can be performed by one of the provers, for simplicity we will assume that this is a separate entity). The <span class="math">\mathtt{Master~Prover}</span> gets a random challenge <span class="math">\theta</span> from the <span class="math">\mathtt{Verifier}</span> and broadcasts it among all <span class="math">\mathtt{Provers}</span>. Now each <span class="math">\mathtt{Prover}</span> <span class="math">P_i</span>, knowing its number <span class="math">i</span>, can generate its “part of the linear combination” and send it to the <span class="math">\mathtt{Master~Prover}</span>.</p>
<div class="math">
F_i = \sum_{j=1}^{L}\theta^{(i-1) \cdot L + j}f_{(i-1) \cdot L + j}.
</div>
<p><span class="math">\mathtt{Master~Prover}</span> runs a regular version of <span class="math">\mathtt{FRI}</span> for the polynomial <span class="math">\sum_{i=1}^{M}F_i</span>. However, it cannot provide polynomial evaluations and Merkle auth paths for  consistency checks in the query phase of the protocol for individual polynomials, so it asks the corresponding <span class="math">\mathtt{Prover}</span> for each of them.</p>
<p><span class="math">\mathtt{Master~prover}</span> can easily detect malicious behavior of individual <span class="math">\mathtt{Provers}</span>. This is achieved due to the fact that the partial linear combinations <span class="math">F_i</span> belong to <span class="math">\mathsf{RS}</span> code. This property is especially useful in a distributed SNARK generation process, as it allows for the implementation of economic measures to penalize participants for misbehaving.</p>
<p>It is easy to see that the time complexity of the <span class="math">\mathtt{Provers}</span> is <span class="math">O(d\log d)</span>. The communication cost (this is communication between provers and master prover) is dominated by sending a partial linear combination, whose size is <span class="math">O(d)</span> elements from <span class="math">\mathbb{F}_p</span>. Moreover, the number of hash invocations required to verify the final proof is significantly less than that needed to verify <span class="math">M</span> independent proofs.</p>
<p>You can find a more detailed description <a href="https://hackmd.io/@nil-research/rJ_NVyiRA" rel="noopener nofollow ugc">here</a>. Feel free to share your comments!</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/on-distributed-fri-computation/20697">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 14:48:50 +0000</pubDate>
</item>
<item>
<title>BitBadges: Cross-Chain Tokens (EVM <-> SOL <-> BTC <-> COSMOS)</title>
<link>https://ethresear.ch/t/bitbadges-cross-chain-tokens-evm-sol-btc-cosmos/20696</link>
<guid>https://ethresear.ch/t/bitbadges-cross-chain-tokens-evm-sol-btc-cosmos/20696</guid>
<content:encoded><![CDATA[
<div> BitBadges 多链 应用程序 代币 转移性<br /><br />总结:<br />BitBadges 是一种跨链代币标准，旨在为用户提供一个多链应用程序开发平台。它支持包括比特币、以太坊、Solana 和 Cosmos 在内的多个区块链。<br />用户可以通过单一界面和单一服务访问这些不同的链。<br />主要服务包括代币（徽章）的转移功能，使以太坊用户可以将代币发送到 Solana，反之亦然，并提供细粒度的可转让性、默认无智能合约、时间依赖余额等创新功能。<br />BitBadges 通过自身作为 Cosmos 区块链来支持多链操作，兼容任何生态系统的签名。<br />虽然有设计上的权衡，但 BitBadges 提供了一个一站式解决方案，适合需要整合多种区块链应用的用户。 <div>
<p>I wanted to introduce you all to BitBadges and what we are building: a cross-chain token standard. See <a href="https://bitbadges.io" rel="noopener nofollow ugc">https://bitbadges.io</a> for it in action or <a href="https://docs.bitbadges.io" rel="noopener nofollow ugc">https://docs.bitbadges.io</a>.</p>
<p>BitBadges offers a range of tools for building multi-chain applications (Bitcoin, Ethereum, Solana, and Cosmos). All tools are a single interface, a single service, but support users from all chains.</p>
<p>Our main service is tokens (badges) where an Ethereum user can transfer / send tokens to a Solana and vice versa, along with tons of new innovative features like fine-grained transferability, no smart contracts by default, time-dependent balances, and more.</p>
<p>How do we support multi-chain? We are our own L1 Cosmos blockchain but we support signatures from ANY user. Everything is scoped to our blockchain, so we are not “interoperable”, but we are signature compatible with any ecosystem.</p>
<p>Yes, there are tradeoffs with design, but if you need an all-in-one hub, BitBadges is the place to go.</p>
<p><img alt="image" height="366" src="https://ethresear.ch/uploads/default/original/3X/1/b/1b67f85c1f7a8c2da70e7d062287c2b64292a8b3.png" width="597" /></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/bitbadges-cross-chain-tokens-evm-sol-btc-cosmos/20696">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 11:36:38 +0000</pubDate>
</item>
<item>
<title>Dynamic Blob Targets for Better Blob Pricing</title>
<link>https://ethresear.ch/t/dynamic-blob-targets-for-better-blob-pricing/20687</link>
<guid>https://ethresear.ch/t/dynamic-blob-targets-for-better-blob-pricing/20687</guid>
<content:encoded><![CDATA[
<div> PID控制器 动态定价 以太坊 Blob 安全性 网络效率<br /><br />总结:<br />本文提出了一种基于以太坊的动态Blob定价机制，通过PID控制器调整Blob目标数量，以优化Blob使用和燃烧率，确保经济稳定性和可预测性。该机制在设定的目标范围内线性调整价格，超出范围则采用现有定价机制，实现指数级价格变化。关键在于PID控制器根据网络使用情况调整目标Blob数量，同时保持安全性和经济稳定性。此外，该方案还考虑了网络带宽限制和验证者数量等因素，旨在更高效利用网络带宽，同时避免超过验证者的最小带宽要求。通过模拟和建模来验证机制的稳定性和有效性，以期提供更稳定的长期行为。 <div>
<h1><a class="anchor" href="https://ethresear.ch#p-50495-dynamic-blob-targets-for-better-blob-pricing-1" name="p-50495-dynamic-blob-targets-for-better-blob-pricing-1"></a>Dynamic Blob Targets for Better Blob Pricing</h1>
<h2><a class="anchor" href="https://ethresear.ch#p-50495-abstract-2" name="p-50495-abstract-2"></a>Abstract</h2>
<p>This proposal introduces a dynamic pricing mechanism for blobs in Ethereum, using a PID (Proportional-Integral-Derivative) controller to adjust the target number of blobs. The goal is to maintain baseline security and assumes Data Availability Sampling (DAS) while optimizing blob usage and burn rates, ensuring economic stability and predictability for network participants.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50495-key-concepts-3" name="p-50495-key-concepts-3"></a>Key Concepts</h2>
<ol>
<li><strong>PID-Controlled Blob Target</strong>: Adjust the target number of blobs based on network usage over time.</li>
<li><strong>Bounded Pricing Mechanism</strong>: Implement different pricing behaviors within and outside target bounds.</li>
<li><strong>Existing Pricing Mechanism at Limits</strong>: Use existing blob pricing mechanisms at the bounds.</li>
<li><strong>Burn Rate Optimization</strong>: Balance per-blob pricing with overall burn amounts.</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#p-50495-detailed-mechanism-4" name="p-50495-detailed-mechanism-4"></a>Detailed Mechanism</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50495-blob-target-adjustment-5" name="p-50495-blob-target-adjustment-5"></a>Blob Target Adjustment</h3>
<ul>
<li>-A PID controller algorithm adjusts the target number of blobs based on consistent deviations from the current target.</li>
<li>-The target blob count floats between predetermined lower and upper bounds, the upper bound is set based on security considerations and the lower bound is assumed to be 1.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50495-pricing-mechanism-6" name="p-50495-pricing-mechanism-6"></a>Pricing Mechanism</h3>
<ol>
<li>
<p><strong>Within Target Bounds</strong>:</p>
<ul>
<li>-Price adjusts linearly as actual blob count varies from the target.</li>
<li>-Increases when above target, decreases when below.</li>
</ul>
</li>
<li>
<p><strong>Outside Target Bounds</strong>:</p>
<ul>
<li>-Existing blob pricing mechanisms take over, causing exponential price changes.</li>
<li>-This continues until actual blob count returns within the target bounds.</li>
</ul>
</li>
<li>
<p><strong>Blob Target Adjustment Effects</strong>:</p>
<ul>
<li>-When target increases: Price per blob decreases, but overall burn amount increases.</li>
<li>-When target decreases: Price per blob increases, but overall burn amount decreases.</li>
</ul>
</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-50495-security-and-economic-considerations-7" name="p-50495-security-and-economic-considerations-7"></a>Security and Economic Considerations</h3>
<ul>
<li>-Upper bound for blob target is determined by validator count and DAS security requirements assuming a given bandwidth per validator (i.e. we know in advance when validators are withdrawing and it is rate limited as well so we should be safe and able to account for those with enough time to start adjusting the upper blob target limit if needed). We can assume 33% or something similar of validators online in determine how many samples can be completed, etc. to determine a conservative upper limit.</li>
<li>-The system aims to maintain predictable pricing within the target range while ensuring security at the limits.</li>
<li>-Economic stability is prioritized by making gradual adjustments to the target, avoiding sudden price shocks.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50495-benefits-8" name="p-50495-benefits-8"></a>Benefits</h2>
<ol>
<li>More predictable and stable pricing within the target range.</li>
<li>Adaptive to long-term changes in network conditions.</li>
<li>Maintains crucial security guarantees through existing mechanisms at the bounds.</li>
<li>Optimizes network usage and burn rates over time. The current system always aims to underutilize available bandwidth while still requiring higher bandwidth be available to support blocks where max blobs is reached. This proposal aims to make more consistent use of available bandwidth without exceeding validator minimum bandwidth requirements (assuming we actually get those).</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#p-50495-challenges-and-considerations-9" name="p-50495-challenges-and-considerations-9"></a>Challenges and Considerations</h2>
<ol>
<li>Tuning the PID controller parameters for optimal responsiveness without introducing instability.</li>
<li>Ensuring the mechanism remains economically sound under various network conditions.</li>
<li>Balancing short-term price stability with long-term adaptability to network changes.</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#p-50495-implementation-considerations-10" name="p-50495-implementation-considerations-10"></a>Implementation Considerations</h2>
<ul>
<li>-Thorough economic modeling and simulation are necessary to validate the mechanism’s stability and effectiveness.</li>
<li>-At the lower and upper target blob bound you can define a target burn as a percentage of issuance rates as well. e.g. lower target burn of 1% of issuance and upper target burn of 33% of issuance before we are considered to have re-entered the target blob bounds and start making blob target adjustments. What these targets should be should have modeling and simulations and should be discussed thoroughly as well. As indicated above, a smooth curve where price per blob drops but overall burn increases would be reasonable.</li>
<li>-Currently another constraint–the bandwidth of the proposer/publisher–may exist on the upper target of blobs. Adopting multiple concurrent proposers where each proposer publishes a portion of the total blobs (doing sampling on the others could still occur) would enable the network to have blob targets beyond the limits of a single validator. One of the beautiful things about blobs <img alt=":slight_smile:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/slight_smile.png?v=12" title=":slight_smile:" width="20" /></li>
</ul>
<p>This proposal aims to create a more dynamic and responsive blob pricing system for Ethereum, enhancing network efficiency and economic stability while maintaining crucial security guarantees. The use of a PID controller for target adjustment provides a well-established control mechanism, potentially offering more predictable, sustainable, and stable long-term behavior compared to other algorithmic approaches. It is my belief that a pricing mechanism with similar behavior to what I’ve described would better match the inelastic demand common to L2s utilizing blobs for DA.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/dynamic-blob-targets-for-better-blob-pricing/20687">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 01:03:07 +0000</pubDate>
</item>
<item>
<title>Blob EIPs and Minimum Validator Requirements</title>
<link>https://ethresear.ch/t/blob-eips-and-minimum-validator-requirements/20679</link>
<guid>https://ethresear.ch/t/blob-eips-and-minimum-validator-requirements/20679</guid>
<content:encoded><![CDATA[
<div> 关键词：Max Resnick, Blob EIPs, 最小验证者要求, 独立质押, 带宽需求<br />

<blockquote>
总结: 文章由Max Resnick撰写，讨论了与Blob EIPs和以太坊最小验证者要求相关的提案。这些提案包括增加调用数据成本（EIP 7623），设置小的Blob价格下限（EIP 7762）以及解耦CL和EL中的Blob计数（EIP 7742）。尽管有独占质押者的反对意见，认为这会增加带宽需求，但作者指出这些提案实际上会降低区块的最大大小。此外，文章还讨论了最低质押要求的问题，建议设定50Mb/s的上传速度作为初始讨论点，以应对未来的扩展需求。作者反对降低最低质押要求的观点，认为现有的最低质押水平已经合适，并且有助于控制网络签名的数量。 
</blockquote> <div>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/7/477a44b70c262a922a22163da6ab2246f13fc6fa.png" title=""><img alt="" height="376" src="https://ethresear.ch/uploads/default/optimized/3X/4/7/477a44b70c262a922a22163da6ab2246f13fc6fa_2_690x376.png" width="690" /></a></div><p></p>
<p>Author: Max Resnick</p>
<p>Acknowledgements: <a class="mention" href="https://ethresear.ch/u/mikeneuder">@mikeneuder</a>, <a class="mention" href="https://ethresear.ch/u/timbeiko">@timbeiko</a>, <a class="mention" href="https://ethresear.ch/u/gakonst">@gakonst</a>, <a class="mention" href="https://ethresear.ch/u/mmp">@mmp</a>, and Tivas Gupta for helpful comments.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50474-blob-eips-and-minimum-validator-requirements-1" name="p-50474-blob-eips-and-minimum-validator-requirements-1"></a>Blob EIPs and Minimum Validator Requirements</h1>
<p>On the last execution ACD, a blob target increase as well as some other auxiliary proposals were moved to CFI status.</p>
<p><strong>Those proposals were:</strong></p>
<ul>
<li>EIP 7623 (Increase call data costs)
<ul>
<li>This EIP increases the gas cost of call data so that the maximum amount of call data possible in a block is lower, reducing the maximum size of a block. Before 4844 call data was how rollups posted their data to the blockchain so increasing the price of call data was not feasible but now that rollups are posting blobs instead we can raise the price of call data to control the max block size without causing problems for rollups.</li>
</ul>
</li>
<li>EIP 7762 (Increase min_base_fee_per_blob_gas)
<ul>
<li>This EIP sets a small reserve price for blobs (~1c) which is designed to increase the speed of price discovery. Each factor of 2 increase in the price of blobs takes almost 6 full blocks to achieve due to the controller implementation so setting this parameter to 2^25 wei rather than 1 wei saves a lot of time for the controller to ramp up.</li>
</ul>
</li>
<li>EIP 7742 (Uncouple blob count between CL and EL)
<ul>
<li>This is mostly a housekeeping change of putting blob count variables in the right place in keeping with the proper separation of concerns between the EL and CL.</li>
</ul>
</li>
<li>EIP 7691 (increase blob target to 4 from 3, blob limit remains at 6)
<ul>
<li>The EIP 4844 fee controller is an integral controller which would work just as well for 4 target, 6 limit as it does for 3 target, 6 limit.</li>
</ul>
</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50474-the-pushback-2" name="p-50474-the-pushback-2"></a>The Pushback</h2>
<p>There was some pushback on the call and from solo stakers against these proposals. In particular, solo stakers were worried about the additional bandwidth required to solo propose a block. But if you look at the above proposals those fears may be misplaced. In fact if all the proposals were included together, it would lower the maximum size of the block. Increasing the blob target doesn’t mean increasing the blob limit and the addition of 7623 would lower the maximum size of the non-blob portion of the block payload.</p>
<p>In addition some solo-stakers posted about their poor upload bandwidth speeds which sparked a discussion of minimum validator requirements for solo-staking, especially if they are solo-proposing . Only a small fraction of nodes solo-propose blocks and doing so has a high opportunity cost. Still, let’s take these concerns at face value.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50474-response-to-pushback-3" name="p-50474-response-to-pushback-3"></a>Response to Pushback</h2>
<p>First, how much bandwidth does it take to reliably propose a block of size x? The proposer needs their block to reach at least 40% of the network before 4 seconds into the slot. The block propagates through the P2P network but before this can happen, the proposer needs to seed it. It sends the full block to a subset of N of its peers. Sending to more and higher quality peers improves the probability that the block will reach a sufficient portion of the network before timeout.</p>
<p>But as I understand it, the default client implementations have very naive optimization for latency and reliability with peers. In other words, there may be ways that nodes can optimize their block propagation speed without using additional bandwidth.</p>
<p>Regardless, extremely poor connections from rural stakers are likely to present a bottleneck in the future so it is important to set internet connection requirements just as we set hardware requirements. I suggest 50Mb/s upload speed as a starting point for these discussions. While we don’t need nearly that much bandwidth today, the goal of the rollup roadmap is to get to 64 blobs per block so, even with optimizations coming with PeerDAS, we would have room for significant scaling in the future. Furthermore, 50MB upload speed on consumer internet is broadly available in North and South America, Asia and Europe. Africa also has substantial (albeit much less comprehensive) access at this speed range. This speed range will therefore give the network substantial headroom to grow, while still allowing for desirable levels of geographic decentralization.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50474-proposals-to-decrease-the-minimum-stake-4" name="p-50474-proposals-to-decrease-the-minimum-stake-4"></a>Proposals to Decrease The Minimum Stake</h2>
<p>On Twitter, Vitalik proposed lowering the stake required to run a node. I think this would be a bad idea. There are two reasons we have stake, the first is accountability (slashing for bad behavior), and the second is sybil proofness (you need stake to participate). Lowering the minimum staking requirement would be bad on the margin because we already have far too many signatures to aggregate for finality. Each additional signature imposes a cost on the network by introducing another signature that must be aggregated each epoch. The current minimum stake seems about right to me and hopefully we see a large reduction in the node count after MAXEB in the next hardfork. Further, out-of-protocol solutions already allow solo-stakers to stake with substantially lower collateral.</p>
            <p><small>7 posts - 6 participants</small></p>
            <p><a href="https://ethresear.ch/t/blob-eips-and-minimum-validator-requirements/20679">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 19:41:27 +0000</pubDate>
</item>
<item>
<title>MEV Capture Through Time-Advantaged Arbitrage</title>
<link>https://ethresear.ch/t/mev-capture-through-time-advantaged-arbitrage/20677</link>
<guid>https://ethresear.ch/t/mev-capture-through-time-advantaged-arbitrage/20677</guid>
<content:encoded><![CDATA[
<div> 时间优势 模型 套利 MEV 拍卖<br /><br />总结:<br />文章讨论了Ethereum上的最大可提取价值（MEV），包括价格套利、清算、背跑和夹心攻击等四种主要形式。提出了一种名为TimeBoost的时间拍卖机制，允许用户竞拍时间优势以优先执行交易。研究分析了拥有时间优势的套利者策略，并评估了不同交易排序政策对套利利润的影响。文章还探讨了如何通过让流动性池了解TimeBoost交易来捕获部分套利机会的价值，使流动性池和套利者在均衡状态下分别获得25%和50%的总价值。<br /> <div>
<p>Cross-posting from here:<br />
<a href="https://research.arbitrum.io/t/mev-capture-through-time-advantaged-arbitrage/9683" rel="noopener nofollow ugc">MEV Capture Through Time-Advantaged Arbitrage - Arbitrum Research</a> because of its potential general interest. Time advantage can come from many different sources, TimeBoost is just one example. We also discuss MEV capture by the liquidity pool itself, by letting the contract know about arbitrage transaction. This is completely independent of how this label is obtained.</p>
<p>"We discuss modeling assumptions and empirical results from the paper: (<a class="inline-onebox" href="http://arxiv.org/abs/2410.10797" rel="noopener nofollow ugc">[2410.10797] MEV Capture Through Time-Advantaged Arbitrage</a>), a joint work with Ed Felten, Robin Frisch, Maria Ines Silva and Ben Livshits.</p>
<p>We look into the Maximal Extractable Value (MEV). Currently, there are four most popular forms of MEV on Ethereum.</p>
<ol>
<li>price arbitrages: to capitalize on price discrepancies between different exchanges,</li>
<li>liquidations: collateral from debt repayment in lending protocols can be purchased at a discount,</li>
<li>backrunning: similar to price arbitrage, with a difference that the fundamental price on outside markets is not changed,</li>
<li>sandwiching: involves wrapping a victim’s swap transaction between two new transactions in such a way that provides a worse trade execution to the victim while tracking a small profit.</li>
</ol>
<p>Effective mechanisms for transaction ordering becomes a key aspect of blockchain design. With arbitrage activity increasing, rollups are considering which policies they should use to include and order transactions. The simplest and natural approach is the first-come-first-serve (FCFS), used in traditional exchanges, and Priority Fees, used in many popular blockchains. In FCFS system, transactions pay the same gas fees determined by the state transition function, and users cannot pay a premium for faster inclusion. Instead, users who value fast transaction inclusion are likely to invest in latency infrastructure in order to improve their latency.</p>
<p>A recent TimeBoost proposal auctions a time advantage for transaction inclusion. The winner of the auction gets its transactions scheduled in FCFS order. Other transactions are also scheduled in FCFS order, but with a fixed time delay. More concretely, users bid for the right to access an "express lane” where transactions are sequenced for execution immediately. This access is sold for a fixed time interval to a single user. The transactions submitted by the remaining users are artificially delayed by the sequencer for a predefined time, which results in the user who purchases the time advantage being able to guarantee that their transactions will be sequenced ahead of its potential competitors. Our paper investigate the impact of this time advantage, focusing on arbitrage opportunities exposed via automated market makers. We analyze the optimal strategy for a time-advantaged arbitrageur and compare it to the profits generated by other MEV extraction approaches.</p>
<p>As expected, the transaction ordering policy significantly influences how the profit from MEV extraction is distributed among participants. For instance, on Ethereum, most profits are paid to validators through MEV-Boost, while on Arbitrum currently, and on FCFS systems more generally, profits stay with MEV extractors and are likely invested in improving their access to sequencers (e.g., through latency improvements). One can argue that avoiding such a latency competition would be favorable and more efficient for the underlying chain. Letting the extractors bid for the opportunity to extract MEV in an auction, allows the rollup protocol redirect proceeds to it.</p>
<p>The emergence of these mechanisms raises the question of how arbitrageurs will react to the design and how much rollups are expected to capture. There is also the question of whether a particular design introduces new opportunities for MEV extraction or increases the magnitude of existing MEV opportunities. Both of these may lead to higher profits for extractors at the expense of other participants. In this paper, we aim to tackle these questions. First, note that time advantage does not create sadnwiching opportunities itself. As long as the sequencer is trusted not to leak transactions before they are sequenced, no party will have the information they would need to sandwich, with or without TimeBoost. Next, given zero (or very low) chain gas fees, backrunning opportunities cannot be efficiently exploited by the time-advantaged arbitrageur, as probabilistic exploitation by any player is still possible. The time-advantage can be efficiently used when exploiting price arbitrages and liquidations. We focus on the former, and leave the latter for future research. Our paper is the first that not only measures the impact on arbitrage profits of changing the block production policy, but also analyses how introducing a time advantage to the sequencing policy impacts arbitrageurs’ decisions. Concretely, this work provides five main contributions:</p>
<ol>
<li>A theoretical model to analyze the scenario where a single actor has a time advantage and uses it to perform arbitrage between a liquidity pool in a DEX and an infinitely liquid external market. Another assumption in the theoretical model is that nobody has any information advantage on the outside market, i.e., the race is decided at the DEX.</li>
<li>A theoretical derivation of the optimal strategy for when the advantaged arbitrageur submits the extraction transaction under the proposed timeboost model. For this derivation, we assume that prices evolve according to certain distributions (including a geometric Brownian motion) and that the liquidity pool charges no fees.</li>
<li>Using dynamic programming and the empirical price distributions of some key liquidity pools, we analyze the optimal strategy for when the advantaged arbitrageur submits the extracting trade. The state in the dynamic programming table is characterized by three parameters: how much time has passed in the 1 minute interval of holding the express lane license, how much time passed in the 200ms advantage since the price difference crossed the threshold where arbitraging is profitable, and the price difference.</li>
<li>We simulate the expected profits an arbitrageur could extract in some key liquidity pools under different sequencing regimes: FCFS, fixed time interval arbitraging and Timeboost, estimating the price change over time from the most traded marketplace.</li>
<li>We look into an option of letting pools capture some share of the value from the arbitrage opportunity by letting the contract know about Timeboost transaction at the time of execution. This results in a sequential game where the pool first sets up a fee structure for extracting the value, and then the arbitrageur best responds to it by choosing the price that maximizes its returns. In the equilibrium of this game, the pool obtains 25% and the time-advantaged arbitrageur obtains 50% of the total value.</li>
</ol>
<p>For details check out the paper. Any feedback is welcome."</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/mev-capture-through-time-advantaged-arbitrage/20677">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 16:42:41 +0000</pubDate>
</item>
<item>
<title>Comparing Two Hash Functions for Multi-Party Computation and Zero-Knowledge</title>
<link>https://ethresear.ch/t/comparing-two-hash-functions-for-multi-party-computation-and-zero-knowledge/20661</link>
<guid>https://ethresear.ch/t/comparing-two-hash-functions-for-multi-party-computation-and-zero-knowledge/20661</guid>
<content:encoded><![CDATA[
<div> Poseidon Hydra MPC ZKP 加密<br /><br />总结: 本文比较了针对零知识证明（ZKP）和多方计算（MPC）优化的哈希函数Poseidon和Hydra。研究发现，Hydra在大多数效率指标上优于Poseidon，尤其是在需要较长摘要或加密长明文的情况下。Hydra具有固定的输入长度限制，而Poseidon则能处理多种输入长度。尽管Hydra在某些场景下表现更好，但目前尚无基于Hydra的认证加密实现。Poseidon已应用于认证加密场景。文章还探讨了这两种哈希函数作为对称密钥加密构建模块的性能。<br /> <div>
<p><strong><a href="https://zenodo.org/records/13739511" rel="noopener nofollow ugc">Comparing Two Hash Functions for Multi-Party Computation and Zero-Knowledge</a></strong><br />
<em>by Burcu Yıldız (Anoma - <a href="https://anoma.network/" rel="noopener nofollow ugc">https://anoma.network/</a>) and Mary Maller (Ethereum Foundation - <a href="https://ethereum.foundation/" rel="noopener nofollow ugc">https://ethereum.foundation/</a>)</em></p>
<p>TL;DR; In our report <a href="https://zenodo.org/records/13739511" rel="noopener nofollow ugc">Comparing Two Hash Functions for Multi-Party Computation and Zero-Knowledge</a>, we compare hash functions Poseidon and Hydra for multi-party computation (MPC) and zero-knowledge proofs (ZKPs). Our observations suggest better efficiency using Hydra.</p>
<p><strong>MPC and ZKP</strong><br />
Zero-knowledge proofs (ZKPs) and MPCs are both examples of advanced cryptographic applications that are useful for privacy and integrity. In a zero-knowledge proof, a single party demonstrates that the output of a computation has been computed correctly without revealing any secret input values. In an MPC, multiple parties compute the output of a computation without revealing any secret input values to each other. Joint use of ZKP and MPC solutions are promising for different applications, including Anoma’s private solving protocol. Main efficiency measures are the number of R1CS constraints for ZKPs and the number of rounds and multiplication triples for MPCs, all of which fundamentally depends on the number of non-linear operations. However, the exact dependency on non-linear operations, and consequently the optimizations, are different for MPCs and ZKPs.</p>
<p><strong>Hash functions</strong><br />
Cryptographic hash functions are a paramount building block in cryptography and are used for numerous applications. The hash function Poseidon is widely favored for zero-knowledge applications (e.g. <a href="https://github.com/filecoin-project/neptune" rel="noopener nofollow ugc">FileCoin</a> , <a href="https://github.com/dusk-network/Poseidon252" rel="noopener nofollow ugc">Dusk Network</a>, <a href="https://tinyurl.com/y7tl537o" rel="noopener nofollow ugc">LoopRing</a>), and has been tailor designed for this purpose. The hash function Hydra is proposed and optimized to be computed in MPC. Hydra was presented in Eurocrypt 2023 and has less total number of rounds and transmitted data than its competitors. We answer following question in our report:<br />
<em>How do the hash functions Poseidon and Hydra, which are optimized for zero-knowledge and MPC applications, respectively, perform for the other application?</em></p>
<p><strong>Our techniques</strong><br />
Our comparison of Poseidon and Hydra consists of theoretical and experimental components, aiming 128 bits of security on Pallas curve (a 255-bit prime field). By computing multiplicative depth and multiplicative complexities of algorithms, we theoretically estimate the efficiency of hash functions by the number of R1CS constraints for ZKP; by the number of rounds and multiplication triples required for MPC. Our experimental results include benchmarks of MPC protocols to obtain an estimate of total running times, amount of exchanged data, and CPU time of each party. We visually present our results with respect to different lengths of output, number of parties, and parameters of Poseidon, when they are applicable.</p>
<p>Furthermore, we investigate Poseidon and Hydra as a building block for symmetric key encryption. We consider the Duplex Sponge authenticated encryption framework for Poseidon and stream cipher for Hydra, as suggested by the paper introducing Hydra. We report similar comparisons as described in the previous paragraph.</p>
<p><strong>Results</strong><br />
We observe that Hydra, in general, outperforms Poseidon for the efficiency measures of our interest. Hydra is especially efficient when the desired length for digest is long, e.g. if it is used as a PRNG. On the other hand, Hydra is limited to a certain input length while Poseidon accepts various input lengths. Poseidon’s various parameters enable optimizing the performance if the input/output lengths are fixed and known in advance; although Hydra still seems to outperform.</p>
<p>Hydra is more efficient to compute when a long plaintext needs to be encrypted. However, there does not exist any (theoretical or practical) implementation of authenticated encryption based on Hydra, although a way is mentioned by its paper. Our considerations for Poseidon are already for authenticated encryption.</p>
<p>Readers are invited to check <a href="https://zenodo.org/records/13739511" rel="noopener nofollow ugc">our report</a> for more details on the hash functions, our setup, results and discussion (including considerations for constructing a hash function for MPC and ZKP applications).</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/comparing-two-hash-functions-for-multi-party-computation-and-zero-knowledge/20661">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 14:00:10 +0000</pubDate>
</item>
<item>
<title>Manipulation-Resistant Prediction Market Derivatives with Language Models</title>
<link>https://ethresear.ch/t/manipulation-resistant-prediction-market-derivatives-with-language-models/20660</link>
<guid>https://ethresear.ch/t/manipulation-resistant-prediction-market-derivatives-with-language-models/20660</guid>
<content:encoded><![CDATA[
<div> 关键词：语言模型, 预测市场, 衍生品, 操纵风险, 信息聚合<br />

<br />总结:<br />
本文介绍了使用大型语言模型（LLM）来生成预测市场衍生品指数价格的方法，以应对传统预测市场中潜在的操纵风险。通过将衍生品与可操纵的现货市场脱钩，该方法利用LLM整合公开信息，增强了系统的抗操纵能力。研究证明了这种方法的有效性，提出了多个策略加强系统可靠性，包括信息验证、白名单机制和多LLM平均等。此外，这种方法能够支持更广泛的事件类型，促进更高效的信息聚合，为未来预测市场的创新发展提供了新思路。 <div>
<h1><a class="anchor" href="https://ethresear.ch#p-50423-manipulation-resistant-prediction-market-derivatives-with-language-models-1" name="p-50423-manipulation-resistant-prediction-market-derivatives-with-language-models-1"></a>Manipulation-Resistant Prediction Market Derivatives with Language Models</h1>
<p><em>Thanks to <a href="https://x.com/0xfuturistic" rel="noopener nofollow ugc">Diego</a> for profound discussions that led to this work.<br />
Thanks also to <a href="https://x.com/hxrts" rel="noopener nofollow ugc">Sam Hart</a>, <a href="https://x.com/DavideCrapis" rel="noopener nofollow ugc">David Crapis</a>, <a href="https://x.com/swp0x0" rel="noopener nofollow ugc">Swapnil</a>, <a href="https://x.com/mempoolsurfer" rel="noopener nofollow ugc">Jorik</a> and <a href="https://x.com/f_casey_fierro" rel="noopener nofollow ugc">Finn</a> for feedback and review.</em></p>
<p><img alt=":dvd:" class="emoji only-emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/dvd.png?v=12" title=":dvd:" width="20" /><strong>TL;DR:</strong> <em>LLM-based prediction derivatives offer a novel solution to manipulation risks of traditional prediction market derivatives, as well as a new primitive to support potentially richer event landscapes within prediction markets. By using language models to generate index prices, this approach decouples derivatives from manipulable spot markets. Mathematical proofs support the solution’s robustness, while additional strategies that fortify the system reliability are proposed.</em></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/0/30495fc69ce07b1ad3bef0346e87ec8c2b42e6b4.png" title="super_forecaster"><img alt="super_forecaster" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/3/0/30495fc69ce07b1ad3bef0346e87ec8c2b42e6b4_2_500x500.png" width="500" /></a></div><p></p>
<p>Prediction markets effectively aggregate information and forecast events. However, derivative prediction markets introduce a new risk: manipulation of the underlying “spot” market that determines the derivative’s index price.</p>
<p>A recent <a href="https://cmsholdings.substack.com/p/failed-polymarket-oracle-attack" rel="noopener nofollow ugc">failed attack</a> on Polymarket exemplifies this vector. On September 6, 2024, an attacker targeted a 2024 US Presidential election derivative market by:</p>
<ol>
<li>Acquiring a large position in the derivative market;</li>
<li>Attempting to push the price down in the “spot” market by spending about $7 million;</li>
<li>Receiving a $1.5 million payout if successful.</li>
</ol>
<p>Though this attempt failed, similar vulnerabilities have been successfully exploited on lending markets with analogous market structure, as with <a href="https://blockworks.co/news/mango-markets-mangled-by-oracle-manipulation-for-112m" rel="noopener nofollow ugc">Mango Markets</a>. These events highlight a key systemic risk inherent in derivative prediction markets.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50423-llm-based-derivatives-2" name="p-50423-llm-based-derivatives-2"></a><em>LLM-Based Derivatives</em></h1>
<p>As a reminder the key components of a perpetual contract are:</p>
<ul>
<li>the mark price, i.e the price at which the perp is traded</li>
<li>the index price, i.e the price of the underlying asset that is tracked by the perp</li>
<li>the funding rate, exchanged between the longs and the shorts as the mark price moves away from the index price</li>
<li>collateral needed to open a position</li>
</ul>
<p>The perpetual can track for example the mid-price of a ‘YES’ token traded on a prediction market, thereby becoming a prediction market derivative.</p>
<p>Instead of using information endogenous to the prediction market to generate the index price we propose using large language models (LLMs). By decoupling the derivative from spot markets and leveraging diverse, credible sources, this approach is potentially offering greater resistance to manipulation than traditional prediction markets, where any participant can influence the price.</p>
<p>The LLM can be interpreted as a mechanism to commoditize the changing qualitative public information. By aggregating the current information landscape into a probability, it can set a price on which one can build a tradeable instrument. Here is a more precise description of the structure of such a perp:</p>
<ul>
<li>the index price is a moving average of the probability calculated by the LLM.</li>
<li>the longs bet on the likelihood of the event increasing.</li>
<li>the shorts bet on the likelihood of the event decreasing.</li>
<li>the funding rate is paid depending on how the derivative market prices the odds compared to the LLM (how much information is hidden from the LLM).</li>
</ul>
<p>Recent research supports LLMs’ forecasting capabilities. The study <a href="https://arxiv.org/pdf/2402.18563v1" rel="noopener nofollow ugc">“Approaching Human-Level Forecasting with Language Models”</a> by Halawi et al. found that a fine-tuned LLM nearly matched and outperformed in some scenarios human forecasters on Polymarket events, having the potential to become a “superforecaster”. This suggests LLMs could effectively serve as oracles for index prices, allowing the creation of derivative instruments from events with thin or non existing spot markets.</p>

<h2><a class="anchor" href="https://ethresear.ch#p-50423-information-aggregation-3" name="p-50423-information-aggregation-3"></a>Information Aggregation</h2>
<p>The LLM functions as a computational agent, aggregating and automatically incorporating publicly available information. Traders, in contrast, contribute private information through their trading activity.</p>
<p>The system’s manipulation resistance is bolstered by the LLM’s source-weighting mechanism. This limits the impact of manipulating any single information source to the weight the LLM assigns to it. Consequently, the lower an individual source’s weight relative to others, the more resistant the system becomes to manipulation attempts targeting that source.</p>
<p>Let <span class="math">S</span> be the set of all sources used by the LLM. We consider a simple model where the LLM generates its probability <span class="math">P</span> as a weighted average of probabilities from all sources and where all sources are independent: <span class="math">P=\sum_{j \in S}(w_j*p_j)</span>. This corresponds effectively to an ensemble probability from multiple LLM inferences on independent data. However, the LLM could generate <span class="math">P</span> differently and sources could have dependencies.</p>
<p><img alt=":open_book:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/open_book.png?v=12" title=":open_book:" width="20" /> <strong>Theorem 1</strong> (<em>Manipulation Resistance of Weight-Adjusted Sources</em>)<strong>:</strong> Under the model above, let <span class="math">P</span> be the LLM-generated probability for an event and <span class="math">P_i</span> be the probability that would be generated if source <span class="math">i</span> were manipulated. Then:</p>
<div class="math">
|P - P_i| ≤ w_i
</div>
<p>where <span class="math">w_i</span> is the weight assigned to source <span class="math">i</span> by the LLM, with <span class="math">\sum w_i=1</span>.</p>
<ul>
<li>
<p><em>Proof</em><br />
Let <span class="math">S_{-i}</span> be the set of all sources except <span class="math">i</span>.<br />
If source <span class="math">i</span> is manipulated, the new probability <span class="math">P_i</span> would be:</p>
<div class="math">
P_i=w_i*p_i'+\sum_{j \in S_{-i}}(w_j*p_j)
</div>
<p>where <span class="math">p_i'</span> is the manipulated probability from source <span class="math">i</span>.</p>
<p>The difference between <span class="math">P</span> and <span class="math">P_i</span> is:</p>
<div class="math">
|P-P_i|=|w_i*(p_i-p_i')|
</div>
<p>The maximum possible difference between <span class="math">p_i</span> and <span class="math">p_i'</span> is <span class="math">1</span>. Therefore,</p>
<div class="math">
|P-P_i| \leq w_i
</div>
<p><span class="math">\blacksquare</span></p>
</li>
</ul>
<p>This theorem provides a quantifiable measure of manipulation resistance, directly linked to the source weights used by the LLM. Assuming indepence between the information sources, it demonstrates that <em>LLM-based prediction derivatives</em> become increasingly resistant to manipulation as the weight assigned to any individual source approaches zero.</p>
<p><img alt=":open_book:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/open_book.png?v=12" title=":open_book:" width="20" /> <strong>Corollary 1</strong> (<em>Manipulation Resistance of Equally Weighted Sources</em>)<strong>:</strong> If the LLM uses at least <span class="math">n</span> equally weighted independent sources, then:</p>
<div class="math">
|P-P_i|\leq\ \frac{1}{n}
</div>
<p>where <span class="math">P</span> is the LLM-generated probability for an event and <span class="math">P_i</span> is the probability that would be generated if source <span class="math">i</span> were manipulated.</p>
<ul>
<li>
<p><em>Proof</em></p>
<p>If sources are equally weighted, then <span class="math">w_i=\frac{1}{n}</span> for all <span class="math">i</span>.</p>
<p>Thus, from Theorem 1:</p>
<div class="math">
|P-P_i|\leq w_i=\frac{1}{n}
</div>
<p><span class="math">\blacksquare</span></p>
</li>
</ul>
<p>This corollary shows that increasing the number of equally-weighted sources reduces the impact of manipulating any single source. Intuitively, the number of available information sources tends to grow as an event nears its resolution, further enhancing the system’s resistance to manipulation.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50423-llm-based-prediction-perpetuals-4" name="p-50423-llm-based-prediction-perpetuals-4"></a><em>LLM-Based Prediction Perpetuals</em></h1>
<p>We formally introduce perpetuals on event probabilities where an LLM generates the index price instead of traditional markets.</p>
<p>Let <span class="math">P</span> be the LLM-generated probability. The perpetual is thus governed by:</p>
<div class="math">
\text{Collateral ratio} = \frac{\text{Equity}}{\text{Debt}} = \frac{\text{Collateral quantity} * \text{Collateral price}}{\text{Perpetual quantity} * P*\$1}
</div>
<p>We multiply the denominator by $1  since <span class="math">P</span> is a probability, ensuring unit consistency with the numerator.</p>
<p>A funding rate mechanism aligns the traded price (mark) with the LLM-generated probability (index), with the gap intuitively being proportional to how much information is hidden from the LLM.</p>
<div class="math">
\text{Funding} = \text{Mark} - \text{Index} = \text{Mark} - P
</div>
<h2><a class="anchor" href="https://ethresear.ch#p-50423-information-aggregation-informational-substitutes-5" name="p-50423-information-aggregation-informational-substitutes-5"></a>Information Aggregation: informational substitutes</h2>
<p>A different model of the LLM aggregation mechanism consists in the LLM performing Bayesian updates over a common prior on received individual signals. When presented with all signals simultaneously, it can aggregate them into a single probability estimate. The probability <span class="math">P_t</span> then becomes the result of the most recent Bayesian update over the last set of signals.</p>
<p>Formally, the LLM estimates a common prior <span class="math">P_{LLM}</span>. We denote the variable corresponding to the binary outcome underpinning the market as <span class="math">Y.</span>  As the LLM receives <span class="math">n</span> different signals <span class="math">\{x_1, \dots, x_n\}</span> it outputs a price <span class="math">P = P_{LLM}(Y=1|x_1, \dots, x_n)</span>.</p>
<p>Two signals are considered informational substitutes if they are conditionally independent given the ground truth. This is an important condition in the prediction market literature <a href="https://arxiv.org/pdf/1703.08636" rel="noopener nofollow ugc">ensuring incentive-compatibility</a>.</p>
<p>Such a model allows to study the case of a single LLM inference aggregating multiple substitutable signals or of successive LLM inferences on substituable signals. One signal may correspond to a set of information sources.</p>
<p><img alt=":open_book:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/open_book.png?v=12" title=":open_book:" width="20" /> <strong>Theorem 2</strong> (<em>Manipulation Resistance of the LLM Under Informational Substitutes Condition</em>): Under reasonable assumptions about the information structure, if the LLM has access to a sufficient number of signals that are informational substitutes, a malicious agent attempting to manipulate a signal can only expect to bias the final price by at most <span class="math">\epsilon</span>.</p>
<ul>
<li>
<p><em>Proof</em></p>
<p>This result stems from a reinterpretation of <em>Lemma 1</em> and <em>Theorem 1</em> from “<a href="https://arxiv.org/pdf/2306.04305" rel="noopener nofollow ugc">Self-Resolving Prediction Markets for Unverifiable Outcomes</a>” by Srinivasan et al.</p>
  
<p>We consider a particular signal <span class="math">t</span> that a malicious agent tampers with, resulting in a modified signal <span class="math">\tilde{x_t}</span>.  The LLM also has access to another set of information sources <span class="math">x_{-t}</span>, denoted simply as <span class="math">x_r</span>, where <span class="math">\Omega_r</span> corresponds to the underlying signal space’s structure. The LLM would produce a price <span class="math">P_{LLM}(Y=1 | x_t)</span> upon receiving the true signal <span class="math">x_t</span>.</p>
<p>We assume crucially that the agent cannot access this information set during manipulation. This assumption holds if the LLM updates instantly from numerous sources (leaving the agent no time to access them) or, alternatively, if we consider each signal as associated to a particular time <span class="math">t</span>. The LLM then first receives the manipulated signal along with the signal from other public sources bundled in <span class="math">\tilde{x_t}</span>, and some time later it receives updates from non manipulated sources as <span class="math">x_r</span>. The latter is perhaps especially interesting if the sources update rapidly over time.</p>
<p>According to the aforementioned lemma, we have:</p>
<div class="math">
E [P_{LLM}(Y=1 | x_r, \tilde{x_t}) | x_t] = P_{LLM}(Y=1 | x_t) + \Delta(\Omega_r, \tilde{x_t}, x_t)
</div>
<p>This equation indicates that the malicious agent’s expectation of the LLM’s price after aggregating other information sources under <span class="math">x_r</span> is affected by an error term dependent on the false report <span class="math">\tilde{x_t}</span>. A larger error term implies greater price manipulation potential.</p>
<p><em>Theorem 1</em> demonstrates that if <span class="math">\Omega_r</span> comprises signals that are information substitutes, the error term can be minimized as the number <span class="math">k</span> of substitutes increases.</p>
<p><span class="math">\blacksquare</span></p>
</li>
</ul>


<p>This theorem demonstrates that when the LLM has access to information sources that are informational substitutes, a malicious agent’s ability to manipulate the final price is significantly limited. Theorem <span class="math">1</span> can be of complementary value since one could ensemble several inferences on substitutable signals.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50423-market-efficiency-6" name="p-50423-market-efficiency-6"></a>Market Efficiency</h2>
<p>This derivative may achieve a novel form of efficiency, with the LLM rapidly incorporating public information and trading activity integrating private information.</p>
<p><img alt=":open_book:" class="emoji only-emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/open_book.png?v=12" title=":open_book:" width="20" /><strong>Theorem 3</strong> (<em>Efficiency of LLM-Based Prediction Perpetuals</em>)<strong>:</strong> Let <span class="math">P_t</span> be the LLM-generated probability at time <span class="math">t</span>, <span class="math">M_t</span> be the mark price of the perpetual at time <span class="math">t</span>, and <span class="math">I_t</span> be the public information set available at time <span class="math">t</span>. Denote by <span class="math">\delta_t</span> a delta-Dirac function corresponding to a price jump in the mark price related to the arrival of private information. Then:</p>
<ol>
<li><span class="math">|E[P_t|I_t]-P_t|\leq \epsilon_{\text{LLM}}</span> (<em>LLM Efficiency</em>)</li>
<li>Outside of the arrival of private information, <span class="math">P_t \approx M_t</span> (<em>mean reversal</em>)</li>
<li>When <span class="math">\delta_t</span> becomes positive and the mark price jumps, after a relaxation time <span class="math">P_t</span> jumps as well (<em>feedback loop between the LLM and the market</em>)</li>
</ol>

<ul>
<li><em>Proof</em>
<ol>
<li>
<p>LLM Efficiency: By design, the LLM processes all available public information <span class="math">I_t</span> to generate <span class="math">P_t</span>. The bound <span class="math">\epsilon_{\text{LLM}}</span> represents the maximum error in this process.</p>
</li>
<li>
<p>Let <span class="math">f(t)=k(M_t-P_t)</span> be the funding rate at time <span class="math">t</span>, where <span class="math">1&gt;k&gt;0</span> is a constant. If <span class="math">M_t&gt;P_t</span>, then <span class="math">f(t)&gt;0</span>, incentivizing traders to sell and driving <span class="math">M_t</span> down. If <span class="math">M_t&lt;P_t</span>, then <span class="math">f(t)&lt;0</span>, incentivizing traders to buy and driving <span class="math">M_t</span> up. This creates a generalised Ornstein-Uhlenbeck process:</p>
<div class="math">
dM_t=\alpha(P_t-M_t)dt+\sigma d W_t 
</div>
<p>where <span class="math">\alpha&gt;0</span> is the speed of adjustment, <span class="math">\sigma</span> is the volatility, and <span class="math">W_t</span> is a Wiener process.<br />
We assume <span class="math">P_t</span> is a slowly changing random variable. This is a realistic assumption since the news landscape does not change suddenly. Then for a time period <span class="math">[t_0,t_1]</span> we have <span class="math">P_t(\omega) \approx P(\omega)</span>. The SDE above then gives</p>
<div class="math">
|E[M_t]-P(\omega)|=|P(\omega)-M_{t_0}|e^{-k(t-t_0)}
</div>
<p>We hence see exponential mean reversal and <span class="math">P_t \approx M_t</span> under normal market conditions. One can also notice that the error bound on aggregating public information depends entirely on the LLM.<br />
We now introduce <span class="math">\delta_t</span> in the SDE above to account for private information:</p>
<div class="math">
dM_t=\alpha(P_t-M_t)dt+ \eta\delta_tdt + \sigma d W_t
</div>
<p>We suppose that <span class="math">\delta=0</span> outside of <span class="math">]t(\omega), t(\omega) + \epsilon(\omega)[</span> where <span class="math">t</span> and <span class="math">\epsilon</span> are random variables, <span class="math">\delta_t&gt;0</span> inside the interval and <span class="math">\int_{\mathbb{R}} \delta_t =1</span>. The term <span class="math">\eta(\omega)</span> is a random variable quantifying the magnitude of the jump, it can be equal to <span class="math">\pm \eta</span>. Since before <span class="math">t^*=t(\omega)</span> we have <span class="math">M_t\approx P_t</span> it is reasonable to assume that during the time <span class="math">\epsilon(\omega)</span> we have:</p>
<div class="math">
dM_t = \eta\delta_tdt + \sigma d W_t
</div>
<p>Thus the average price increase is <span class="math">\eta</span>. We can therefore have the simplified assumption that <span class="math">M_t= M_{t_0}</span> for <span class="math">t&lt;t^*</span> and <span class="math">M_t = M_{t_0} + \eta</span> for <span class="math">t &gt; t^* + \epsilon(\omega)</span> where <span class="math">t_0 &lt; t^*</span>.  We omit the rest of the proof for brievity but by solving the second SDE above and by asymptotic analysis for <span class="math">t&gt;&gt;t^*</span> one finds that:</p>
<div class="math">
M_{t_0} + \eta \approx P(\omega)[1-e^{-k(t-t^*)}]
</div>
<p>Hence indeed for large <span class="math">t</span> we must have <span class="math">P_t \approx M_{t_0} + \eta</span>, and the LLM is obliged to follow the initial jump.<br />
<span class="math">\blacksquare</span></p>
</li>
</ol>
</li>
</ul>
     
<p>This theorem formalizes the novel form of semi-strong efficiency of LLM-based prediction derivatives (all the public information is priced in), combining LLM information processing with market price discovery.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50423-ensuring-further-robustness-7" name="p-50423-ensuring-further-robustness-7"></a>Ensuring Further Robustness</h2>
<p>The LLM’s design must be resilient to errors such as omissions or inconsistencies. It should rely on credible, manipulation-resistant information sources and strive for maximum observability.</p>
<p>To enhance system reliability and resist manipulation, there exist more strategies:</p>
<ul>
<li>Verified information retrieval: Certify the LLM’s data retrieval from specific websites (e.g., using TLSNotary) to ensure information pipeline integrity, crucial for decentralized LLM operations.</li>
<li>Whitelisting: Restrict the LLM’s sources to ensure authority and relevance.</li>
<li>Adapted retrieval: Customize priority, API modules, and whitelists based on event types (e.g., sports vs. space flight).</li>
<li>Contract price tracking: Implement LLM feedback loops to promote convergence between index and mark prices, complementing funding rates and price convergence to <span class="math">0</span> or <span class="math">1</span>.</li>
<li>LLM prediction ensembling: Reduce variations and inconsistencies by combining multiple predictions.</li>
<li>Inclusion list mechanism: Require the LLM to review specific data regardless of its internal retrieval decisions.</li>
<li>Data-backed model identification: Implement a complex ontology to benchmark LLM forecasting, as described in <a href="https://arxiv.org/pdf/2407.01231" rel="noopener nofollow ugc">this paper</a>.</li>
<li>Multi-LLM averaging: Mitigate LLM-specific biases (e.g., poor calibration, overconfidence) through a “wisdom of the silicon crowd” approach with possibly multiple competing LLMs.</li>
<li>Permissionless mechanism with TEE: the LLM weights could be pre-committed and anyone could re-run the LLM to verify the output. Further, the final prompts could come from various participants (cf. this <a href="https://arxiv.org/abs/2407.07845" rel="noopener nofollow ugc">paper</a> for a general discussion of such mechanisms).</li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#p-50423-extending-the-event-landscape-8" name="p-50423-extending-the-event-landscape-8"></a>Extending the event landscape</h1>
<p>Prediction markets still support a very limited set of markets. A vast domain such as “science” only contains around 30 markets on Polymarket. By producing a dynamic price which updates as new information comes without the need for initial capital, the LLM could support virtually any market as long as it has access to reliable information sources.</p>
<p>Efficiently exploring the space of possible events through ‘super-questioners’ (the symmetrical role to ‘super-forecasters’) could be crucial to the understanding of the ‘existential’ questions that matter most, as suggested by this <a href="https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/66ba37a144f1d6095de467df/1723479995772/AIConditionalTrees.pdf" rel="noopener nofollow ugc">research report</a>.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50423-closing-thoughts-9" name="p-50423-closing-thoughts-9"></a>Closing thoughts</h1>
<p>We assumed that LLMs can have some inherent efficiency in aggregating public information into a probability. We also assumed that such forecasting LLMs could realistically be within the manipulation bounds we outlined above. We expect it to be the case if the information structure presented to the LLM is sufficiently rich.</p>
<p>Under these assumptions, the key claim that building a prediction market derivative using an LLM as an index price is both possible and efficient rests on the last result presented above which ensures that the funding rate can drive mean-reversion towards truthful resolution. The first results support the claim that <span class="math">\epsilon_{LLM}</span> can be chosen small enough.</p>
<p>The general intuition is that <span class="math">P_t</span> is a proxy for aggregating the public signals and <span class="math">M_t</span> is a proxy for aggregating public and private signals, while the funding rate drives the derivate price in the correct direction over time.</p>
<p>LLM-based prediction derivatives offer a novel solution to a key manipulation risk in derivative prediction markets. By decoupling from potentially manipulable spot markets, this approach aims to remove a major obstacle in developing derivative markets for event probabilities. We expect this solution to encourage experimentation with prediction market derivatives, particularly in scenarios where corresponding spot prediction markets are illiquid or non-existent.</p>
<p>Furthermore such a primitive might ensure efficiency in a much larger set of markets than what we currently see on e.g Polymarket, allowing us to access novel forms of information aggregation.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/manipulation-resistant-prediction-market-derivatives-with-language-models/20660">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 13:19:13 +0000</pubDate>
</item>
<item>
<title>What happened to our decentralized private new internet?</title>
<link>https://ethresear.ch/t/what-happened-to-our-decentralized-private-new-internet/20657</link>
<guid>https://ethresear.ch/t/what-happened-to-our-decentralized-private-new-internet/20657</guid>
<content:encoded><![CDATA[
<div> 关键词：Web3、隐私、合规性、选择性披露、去中心化

总结: 文章探讨了Web3中的隐私与合规性之间的平衡问题，特别是在去中心化金融(DeFi)领域。主要关注点在于如何在保护用户隐私的同时，确保监管机构能够执行必要的监督，避免过度依赖集中的控制机制。文章提出了通过去中心化的合规框架来实现这一目标的方案，该框架允许在隐私与合规之间达到平衡，即在日常交易中提供普遍的隐私保护，同时在特定情况下允许进行选择性披露以满足合规需求。通过引入不同层次的预交易合规和案例依赖后的审计透明度，可以实现既保持用户隐私又确保遵守法规的目标。此外，文章还讨论了阈值多点计算(Threshold Multi-Party Computation)等技术在增强隐私保护的同时，也能在某些场景下支持合规性的应用，如防止市场操纵、游戏公平性维护、医疗健康数据共享等。最后，文章强调了隐私在Web3中的重要性，并呼吁构建无缝集成隐私功能的Web3系统，使其如同Web2世界中的加密一样，成为用户的常态体验。 <div>
<p><strong>The Selective Disclosure/Compliance Challenge in Web3</strong></p>
<p>*By <a href="https://x.com/Pememoni" rel="noopener nofollow ugc">Peyman Momeni</a>, <a href="https://www.fairblock.network/" rel="noopener nofollow ugc">Fairblock</a>; <a href="https://x.com/amitchax" rel="noopener nofollow ugc">Amit Chaudhary</a>, <a href="https://x.com/Labyrinth_HQ" rel="noopener nofollow ugc">Labyrinth</a>; <a href="https://x.com/yusufxzy" rel="noopener nofollow ugc">Muhammad Yusuf</a>, <a href="https://t.co/rJirnhBM5S" rel="noopener nofollow ugc">Delphi Digital</a></p>
<p><strong>Why?</strong><br />
Today, after 16 years, blockchains’ utility is mostly limited to meme coins and circular infrastructure projects. Yes it’s fun, yes a lot of people made millions, but are we still building the new internet? What happened to the idea of our decentralized private new internet?</p>
<p>The most obvious application – transfers– hasn’t yet been fully realised even for basic scenarios. Most businesses can’t use it. They can’t share their payroll with everyone. Vitalik is getting bullied for his onchain donations to science. Businesses don’t want to share their confidential strategies and data publicly, users don’t like the choice between a centralized service or getting sandwiched maximally and begging for a change from middlemen mercenaries.</p>
<p>While the machine is running, we, as a collective, cannot continue this path for another 16 years.</p>
<p>With privacy should come confidentiality to secure your onchain actions, allowing for more useful and impactful applications to be built and for users to reap the benefits of a more expressive blockchain experience. Confidentiality is a standard across Web2, it’s imperative that it become a standard across Web3.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/7/5753c19290978d1767b2eb6b63aa71fc785ce47b.png" title=""><img alt="" height="158" src="https://ethresear.ch/uploads/default/optimized/3X/5/7/5753c19290978d1767b2eb6b63aa71fc785ce47b_2_439x158.png" width="439" /></a></div><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/8/68b00f87394db8a4f28ce7bb18d6c0a8c7797d2f.png" title=""><img alt="" height="200" src="https://ethresear.ch/uploads/default/optimized/3X/6/8/68b00f87394db8a4f28ce7bb18d6c0a8c7797d2f_2_438x200.png" width="438" /></a></div><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/7/97831183a8c9a297610ef8e0d78adbe14be28403.png" title=""><img alt="" height="250" src="https://ethresear.ch/uploads/default/optimized/3X/9/7/97831183a8c9a297610ef8e0d78adbe14be28403_2_440x250.png" width="440" /></a></div><p></p>
<p>The lack of onchain confidentiality has hindered the growth and adoption of even the most obvious applications. Confidentiality is one of the most misunderstood terms in crypto. On one end of the spectrum it is associated with money laundering and illegal financial activities and on the other, it unlocks high utility use cases such as normal private transfers, dark pools, frontrunning protection, confidential AI, zkTLS, gaming, healthcare and private governance.</p>
<p>Economies function efficiently when there is a balance between confidentiality and transparency. Take financial markets as an example - confidentiality makes information valuable and tradable, but selective disclosure of that information would be helpful in preventing [market abuse]. Many financial activities such as portfolio management, asset trading, payments, and banking require confidentiality with a need to balance data disclosure for compliance and regulation purposes. However, the phenomenon of selective disclosure is not limited to finance and compliance. A recent example is the confidential social media theme, which is currently battling misinformation and hate content, necessitating self-regulation by social media giants through <a href="https://www.brookings.edu/articles/transparency-is-essential-for-effective-social-media-regulation/" rel="noopener nofollow ugc">disclosures</a>).</p>
<p>Recent challenges in fake content generation through AI models have raised questions about the value of sharing secrets and the on-demand disclosure of secrets. During the COVID pandemic, vaccine research raised controversy because important stakeholders were kept in the dark about the detailed results. Balancing confidentiality and transparency takes on many shapes, and sometimes regulation and compliance make this the most important problem to tackle. Technology comes to the rescue in finding the balance - the key question being asked is whether we can remove the centralized party in selective disclosure or compliance use cases.</p>
<p>We have confidentiality inside our walls, 95% of the internet, iPhones, bank accounts, elections and even a friendly poker game. Just the other day US slapped TD - <a href="https://www.cbc.ca/news/business/td-bank-penalties-1.7348819" rel="noopener nofollow ugc">the largest bank in Canada- with $3B fine over cartel money laundering</a>. But no one is going to avoid banks, no one is scared of privacy in other banks or industries, and even TD itself is not going down. An impactful system shouldn’t be vaporized and banned because of a few bad actors. In web3, we shouldn’t overreact to a few bad examples and myths that we’ve seen. We shouldn’t shy away from building the new internet and turn it into short-term distractions. In most cases, we don’t even have a compliance problem. For private transfers, we can build systems that are as compliant as real-world banks, but still more transparent, private and decentralized. More impactful problems are harder to solve, that’s the way it is.</p>
<p>Even if we only care about money, we can’t extract value from memecoins for another 16 years. Now that we have the scalable infrastructure, opportunities are going to be orders of magnitude greater if we have onchain confidentiality and real applications.</p>
<p><strong>The Elephant in the room</strong><br />
One of the most pressing challenges of DeFi is the balance between confidentiality and compliance. Maintaining user privacy while ensuring regulatory oversight without centralization requires a delicate approach. This article explores the solution through selective disclosure, enabling privacy and accountability without compromising security or compliance.</p>
<p>So far in web3, we’ve figured out private transfers and know how to transfer and trade assets privately by proving the validity of transactions without leaking our private identities. The open-debated technical and philosophical challenge is how we can make sure that the technology is not used by minority of bad actors at the expense of the majority of active users, how can we have at least the same level of privacy as our current banks?</p>
<p>While the private transfers themselves are enabled by ZKPs, different centralized or decentralized techniques and cryptographic schemes such as MPC can be used for compliance. Some of the current efforts for making compliant private transfers are:</p>
<ul>
<li>
<p><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4563364" rel="noopener nofollow ugc">Pre-transfer proof of legitimate funds (0xbow/Privacy Pools/Railgun):</a> Users can prove non-association with lists of illicit activities or sanctioned addresses before execution of their transfers.</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2311.08167" rel="noopener nofollow ugc">Post-transfer selective de-anonymization:</a> Balancing blockchain privacy and regulatory compliance by providing accountability using zk and threshold cryptography</p>
</li>
<li>
<p><a href="https://www.zama.ai/post/programmable-privacy-and-onchain-compliance-using-homomorphic-encryption" rel="noopener nofollow ugc">DID and regulatory smart contracts:</a> Programming real-world rules such as the 10K limit, and other conditions by privately sharing information using decentralized identifiers and MPC/FHE.</p>
</li>
<li>
<p>ID verification: Users should engage with non-private and centralized long and haphazard processes of KYC for each of the services they are using.</p>
</li>
</ul>
<p>However, none of these approaches are complete by themselves as they fail to address the balance between privacy and regulations. Deposit limits aim to block illicit funds but often result in inconvenience to legitimate users. Sanction lists are slow to update, allowing bad actors to operate before detection, and there’s no recovery for wrongly flagged addresses. Blockchain analysis tools such as Chainalysis, miss illicit activities due to false negatives. “View-only” access relies on user cooperation, failing against malicious actors. The association sets in privacy pools delay the detection of illicit transactions and rely on untrusted set providers. KYC compromises privacy by forcing users to disclose sensitive information on the first step of using privacy applications, without solving the problem of users turning malicious later. Ultimately, these approaches rely on centralized controls, undermining the decentralized nature of Web3.</p>
<p><img alt="" height="368" src="https://ethresear.ch/uploads/default/original/3X/1/a/1a479b794ea00a59dce82111e6bc90077c3beaf0.png" width="602" /></p>
<p><strong>Co-existence of Privacy and Compliance through Decentralized Approaches</strong><br />
The answer to balancing privacy and compliance lies in a decentralized compliance framework. This approach allows compliance to coexist with privacy by creating systems where compliance measures can be enforced without compromising user anonymity.</p>
<p>There needs to be different levels of decentralized pre-transfer compliance and case-by-case post-execution audibility through selective disclosure. This way, we still achieve common sense privacy for DeFi while allowing authorities to request more information on a rare case-by-case basis. At the very least, this offers an equivalent level of web2 and tradfi privacy with more decentralization and transparency properties.</p>
<p>Dark pools in traditional finance enable trader anonymity while ensuring regulatory post-trade transparency. Recently dark pools and privacy-focused blockchain protocols such as Railgun, Penumbra, and Renegade are gaining attention. However, they’re either non-compliant or only partially compliant. Selective disclosures can address these issues by ensuring that users’ actions are legitimate while preserving anonymity where appropriate. While users can use a mix of methods to prove their legitimate source of funds and identities, threshold networks can ensure post-transaction accountability.</p>
<p><strong>Post-transaction accountability through MPC/Threshold decryption</strong><br />
In a threshold network, compliance and accountability are enforced without relying on central authorities. The system is based on independent entities such as Revokers and Guardians:</p>
<p>Accountable Privacy means that users must engage in legitimate activities. Malicious behavior can lead to selective de-anonymization, but only under lawful conditions, ensuring integrity without compromising user privacy unjustly.</p>
<p>Accountable De-Anonymization ensures that de-anonymization requests are public and traceable, requiring cooperation between Revokers and Guardians, thus preventing unauthorized disclosure.</p>
<p>Non-fabrication guarantees that honest users cannot be falsely accused, even if there is collusion. The cryptographic commitments ensure all participants are bound to act transparently, safeguarding user rights.</p>
<p>Here’s a detailed end-to-end flow explaining how transactions are managed, de-anonymization is requested, and the process is carried out in an accountable and publicly verifiable way by users, revokers, and guardians:</p>
<p><em>User Transaction (Onchain)</em><br />
Users are accountable for doing compliant transactions. Users face de-anonymization if they act maliciously. Misbehavior leads to loss of privacy, but only under lawful conditions. A user initiates an onchain private transaction on the protocol, and encrypted data is included in the transaction payload. This ensures that all transaction details remain private and secure on-chain, preventing unauthorized access to the user’s data.</p>
<p><em>Suspicious Activity Detection (Off-chain)</em><br />
A Revoker such as a DAO, trusted entity, or neutral gatekeeper, monitors the transaction off-chain, which uses monitoring tools to detect any potential illicit activity or suspicious behavior. The Revoker flags the user’s transaction if it appears to violate compliance rules or triggers suspicious activity alerts.</p>
<p><em>De-Anonymization Request Submission (Onchain)</em><br />
Once the Revoker identifies a suspicious activity, they submit an onchain de-anonymization request on the governance dashboard. This request initiates the de-anonymization process and makes the request publicly verifiable and transparent to all the network participants. The Revoker does not have de-anonymization rights at this stage but is merely flagging the transaction for further review.</p>
<p><em>Guardian Review and Voting (Off-chain)</em><br />
The request is picked up by a decentralized network of Guardians (trusted entities picked up through the governance process). These Guardians act as decision-makers and are responsible for validating the Revoker’s de-anonymization request. They assess the flagged transaction according to governance policies and determine whether de-anonymization should be allowed. This review process occurs off-chain to ensure the privacy of decision-making and governance.</p>
<p><em>Threshold Mechanism (Onchain)</em><br />
For the de-anonymization request to proceed, a certain threshold of Guardian approvals must be met (e.g., 6 out of 10 Guardians need to approve). Each Guardian that votes in favor of de-anonymization submits their cryptographic permission onchain, which is aggregated to reach the required threshold. This on-chain submission guarantees transparency and prevents any foul play or unauthorized actions.</p>
<p><em>De-Anonymization Execution (Off-chain)</em><br />
Once the necessary cryptographic permissions have been granted, the Revoker can decrypt the flagged transaction. This process happens off-chain, and only the specific transaction under investigation is revealed to the Revoker—no other data or transactions are affected or exposed. Importantly, even the Guardians who approved the request do not gain access to the transaction details; only the Revoker can view the decrypted transaction information.</p>
<p><em>Post-De-Anonymization (Onchain)</em><br />
If further suspicious activity is linked to the decrypted transaction, it will be flagged separately, requiring a new de-anonymization request to be submitted by the revoker and approved by the Guardians. The rest of the user’s transaction history and data remain encrypted and private. This ensures that privacy is maintained for non-flagged transactions while enabling compliant de-anonymization for suspicious activities.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/0/c06c6733354f18254792958d1ccf872057a2fbba.png" title=""><img alt="" height="293" src="https://ethresear.ch/uploads/default/optimized/3X/c/0/c06c6733354f18254792958d1ccf872057a2fbba_2_566x293.png" width="566" /></a></div><p></p>
<p><em>Security</em><br />
There’s a trust assumption in the threshold network. A dishonest majority of malicious validators can work together to decrypt transactions - with or without detection depending on the scheme.</p>
<p>It is worth mentioning that the consequence of such an attack is losing confidentiality, and neither the safety of the network, loss of funds nor private information regarding the identities of users. In this case, the system’s confidentiality will downgrade to the current state of public blockchains. The consequence is more limited in the cases where only ephemeral confidentiality is required or confidentiality is leveraged for better execution quality, not the privacy of users e.g. frontrunning protection, and sealed-bid auctions.</p>
<p>However, validators and operators should be incentivized to protect user privacy with respect to the stakes in the game. The solution lies in building robust networks where compliance can be enforced without compromising decentralization. The transition will involve integrating permissionless compliance mechanisms, where incentives are aligned to encourage honest validator behavior. Approaches like Proof of Stake (PoS) and AVS ensure network security, while <a href="https://www.youtube.com/watch?v=OnpCB5TWsGs" rel="noopener nofollow ugc">cryptographic traitor tracing </a>and slashing mechanisms deter malicious actors. There are many promising recent works in this area: <a class="inline-onebox" href="https://hackmd.io/@Fairblock/rkSiU78TR" rel="noopener nofollow ugc">Multimodal Cryptography - Accountable MPC + TEE - HackMD</a></p>
<p><strong>Threshold MPC and confidentiality beyond compliant transfers</strong></p>
<p>The use of Threshold MPC extends beyond compliance, finding applications across multiple confidentiality sectors:</p>
<ul>
<li>Frontrunning protection and MEV Protection: Preventing manipulative trading practices by hiding transaction data until completion. Replacing centralized relayers in Ethereum’s MEV supply chain. Leaderless and incentive-aligned MEV or preconfirmation auctions.</li>
<li>PvP GameFi or prediction markets: Ensuring fairness and excitement by concealing actions until necessary. Adding the element of onchain surprise by decrypting values. Decentralize decryption of oracle updates in prediction markets (instead of naive hashes as in UMA)</li>
<li>Private Governance/Voting: Prevention of manipulation during decision-making processes, coercion resistance, and privacy of voters.</li>
<li>Access Control and SocialFi: Enhancing privacy in decentralized applications while retaining usability and accountability as well monetization of contents in creator economies.</li>
<li>Confidential AI: Decentralized and privacy-preserving systems for training, inference and data sharing to unlock access to more players and data and not being limited because of privacy concerns.</li>
<li>Healthcare: Access to more and better personalized healthcare services by analysis of biological or health data without loss of privacy or trusting centralized parties.</li>
</ul>
<p><strong>Path Forward: Seamless Web3 Confidentiality</strong><br />
The ultimate goal for privacy in Web3 is to make it as seamless as it is in Web2. Encryption in traditional internet applications (e.g., HTTP transitioning to HTTPS) has become so common that users hardly notice it. A similar evolution is required for Web3—Confidentiality should be invisible to the user, seamlessly integrated into their experience. While most confidentiality schemes don’t have compliance challenges in the first place, private transfers can be compliant through multimodal cryptography techniques such as MPC and ZKPs.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/what-happened-to-our-decentralized-private-new-internet/20657">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 12:33:11 +0000</pubDate>
</item>
<item>
<title>Optimizing Your Investment Strategy: Always do the math first</title>
<link>https://ethresear.ch/t/optimizing-your-investment-strategy-always-do-the-math-first/20656</link>
<guid>https://ethresear.ch/t/optimizing-your-investment-strategy-always-do-the-math-first/20656</guid>
<content:encoded><![CDATA[
<div> 关键词：贷款重组、流动性位置、数学计算、风险调整、投资决策

总结:

在进行任何贷款重组或流动性位置调整之前，进行彻底的数学计算是非常关键的。本文通过一个具体例子，展示了在考虑减少对波动性资产对的暴露并增加更稳定资产的分配时，需要进行详细的财务分析。作者通过计算发现，尽管这样做可以降低潜在的不可永久损失风险，但对小额资产重新配置至稳定的池中带来的收益并不足以抵消成本。

文章强调了在做出任何投资决策前，进行详尽的数学分析的重要性。通过计算来评估策略的潜在回报和风险，可以帮助投资者做出更加明智的决定。在这个例子中，尽管风险调整后的预期收益低于预期，表明这种特定的资产配置变化可能不是经济上合理的决策。因此，谨慎的数学计算是避免不必要损失、确保投资决策有效性的关键步骤。 <div>
<p>It can be beneficial to reshuffle your loans or liquidity positions to earn a higher yield or lower your interest rate. However, the most crucial step before making any changes is to do the math.</p>
<p>Let’s break down an example.</p>
<p>I considered shifting some funds to reduce exposure to volatile asset pairs and increase allocations in more stable ones. I thought this could also bump up my yield since more assets would be allocated to the liquidity pools.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/2/627f517cd8aef22d0455835c82399c2c8430ee5f.png" title="b64b7fb79b9169e066a6a8c3d3cae231"><img alt="b64b7fb79b9169e066a6a8c3d3cae231" height="312" src="https://ethresear.ch/uploads/default/optimized/3X/6/2/627f517cd8aef22d0455835c82399c2c8430ee5f_2_690x312.png" width="690" /></a></div><p></p>
<p>Then I crunched the numbers:</p>
<ul>
<li>To withdraw $wrsETH from <span class="hashtag-raw">#Lore</span> while maintaining the same health factor, I’d need to repay 77.16% of its value in $ETH, which amounts to about 0.23 ETH.</li>
<li>To pair the entire amount of $wrsETH, I’d need an additional ~0.48 ETH.This means I’d need to withdraw ~0.71 ETH and 1,864 CHI (worth $3,750).</li>
<li>I could then deposit $2,060 worth into the ETH/wrsETH pool.</li>
<li>Additionally, I could deposit $1,864 into the CHI/USDC pool.</li>
</ul>
<p>The resulting yield changes would be:<br />
$3,750 * 0.7245 (withdrawn) + $2,060 * 0.2757 (new deposit) + $1,846 * 0.6762 (new deposit) = -$2,716 + $464 + $1,248 is even less then -$1000.</p>
<p>Although this shift would reduce my exposure to impermanent loss, the math showed it wasn’t worthwhile for a small allocation shift of just 0.48 ETH to a stable pool.</p>
<p>Always do the math first <img alt=":man_detective:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/man_detective.png?v=12" title=":man_detective:" width="20" /></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/optimizing-your-investment-strategy-always-do-the-math-first/20656">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 12:30:25 +0000</pubDate>
</item>
<item>
<title>The Multidimensional Scale of Overhead, Availability, and Validation Requirements</title>
<link>https://ethresear.ch/t/the-multidimensional-scale-of-overhead-availability-and-validation-requirements/20645</link>
<guid>https://ethresear.ch/t/the-multidimensional-scale-of-overhead-availability-and-validation-requirements/20645</guid>
<content:encoded><![CDATA[
<div> 关键词：Ethereum、验证成本、计算开销、最终确认时间（Permitted TTF）、验证权利与集中

总结：
文章探讨了以太坊协议中隐藏的权衡与相互影响，主要围绕着验证成本、计算开销、最终确认时间（Permitted TTF）、验证权利与集中的概念展开。首先，文章介绍了增加验证成本如何能够集中验证责任、减少恶意活动、降低计算开销，但同时也会对网络健康产生潜在负面影响。其次，讨论了最终确认时间的调整如何影响计算开销，提出通过延长或缩短最终确认时间来加快或减慢整个过程，从而可能降低或增加计算要求。再次，文章阐述了验证成本与安全之间的关系，指出提高验证成本可以降低恶意行为的威胁，但同时也可能导致验证权利集中和网络主权削弱。最后，文章强调了验证权利的提高会减少用户基数，而降低验证成本则会增加计算开销，这些机制之间存在着复杂的相互作用。整体而言，文章提醒开发者和用户，任何对核心以太坊协议的修改都将产生深远的影响，需要谨慎考虑。 <div>
<p>If you’ve ever wondered why the cost of Ethereum validation is so high; or why nodes can be so computationally demanding, this paper is for you. From SSF to Validation requirements, this paper aims to uncover the lost world of tradeoffs enshrined within the Ethereum protocol.</p>
<p>Special thanks to <a href="https://ethereum.org/en/" rel="noopener nofollow ugc">ethereum.org</a> and <a href="http://hacken.io" rel="noopener nofollow ugc">Hacken.io</a> for their insight into <a href="https://ethereum.org/en/roadmap/single-slot-finality/" rel="noopener nofollow ugc">SSF dynamics</a> and <a href="https://hacken.io/discover/eip-4844-explained/" rel="noopener nofollow ugc">Proto-Danksharding research</a>. Additional thanks to <a href="http://chainspace.co" rel="noopener nofollow ugc">Chainspace.co</a> for their invaluable mission of bringing semi-premises storage to the masses.</p>
<aside class="onebox allowlistedgeneric">
  <header class="source">
      <img class="site-icon" height="32" src="https://ethresear.ch/uploads/default/original/3X/e/4/e49d1fe9444f4691cbd97c962880d5b07059f973.png" width="32" />

      <a href="https://chainspace.co" rel="noopener nofollow ugc" target="_blank">Chainspace</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image"><img class="thumbnail" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/2/c/2cbf3304777231fb85177d3170f5cea3ba0ae54f_2_666x500.jpeg" width="666" /></div>

<h3><a href="https://chainspace.co" rel="noopener nofollow ugc" target="_blank">Ture Data Sovereignty</a></h3>

  <p>With Chainspace, maintain peace of mind by avoiding central data centers. Store data between peers, they never see your data but they verify its integrity at every step.</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both;"></div>
</aside>

<h1><a class="anchor" href="https://ethresear.ch#p-50393-overview-1" name="p-50393-overview-1"></a><strong>Overview:</strong></h1>
<p>Ethereum is a complicated system. Nearly every protocol choice has downstream cascading effects, from the scale of Validation Requirements (VR) and Security to TTF tradeoffs, every EIP or network alteration, no matter the size, will impact nearly every corner of the network. This paper features a few sections covering the hidden impacts of certain well-known network goals.</p>
<p>Every mechanic implemented in the Ethereum protocol has some downstream effect, for example, following the EIP-4844 Proto-Danksharding implementation, the Base rollup experienced a ~200% tx volume increase. Enjoy this simplicity while you can, as it’s the simplest example throughout this paper. Moving away from predictable effects, it’s clear that even core protocol mechanics have radiating effects on other protocol functionality; An example of this being validation cost.</p>
<p>This section merely introduces these concepts so you’ll have to read further to get more details, but in short, increasing validation cost concentrates validation responsibility, discourages malicious activity, decreases computational overhead, and affects countless other Ethereum components. Some of these effects are objectively good, some bad, and some in the grey. In addition to numerous other examples, this paper outlines these dynamics between a few critical Ethereum components. Interested? Good.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50393-computational-overhead-ttf-2" name="p-50393-computational-overhead-ttf-2"></a><strong>Computational Overhead &amp; TTF</strong></h1>
<p>SSF, or Single Slot Finality, has recently been a rather dismissed area of research. Overshadowed by ePBS, Verkle implementations, and countless other protocol research projects. SSF has missed the impact research phase and, nearly blindly has been accepted as a future protocol goal. While SSF should, in itself be a network improvement, many of the paths to get there could pose a detrimental impact to network health. Although an unlikely approach to solving the TTF problem, the permitted time to finality could be altered to achieve either a faster or slower finality time. You may be asking, “Why increase the Permitted TTF?” well, let’s start with the impact of decreasing it.</p>
<p>To clear things up, TTF is the time it takes for the Ethereum network to back a transaction with at least 33% of the total network ETH. This process requires validators to give a supermajority justification to a checkpoint that must be followed by another justified checkpoint. This practically gives an irreversible guarantee that transactions within that block are agreed upon and legitimate. This finality is required for many Dapps such as L2 - Mainnet bridges that need a guarantee that deposits were made. Permitted TTF simplifies the checkpoint bundling, block creation, and attestation process, and it represents the total time needed for this finality if the protocol enforced some time limit.</p>
<p>Assuming that all protocol, networking, computational inefficiencies, and bottlenecks are minimal, it can be roughly assumed that changing permitted TTF would have a linear impact on the average validator computational overhead. These are <em>incredibly</em> crude and idealistic circumstances but they do provide a baseline for expected network impacts when changing the permitted TTF. By increasing the pTTF, or by slowing down the entire finality process it can be expected that computational requirements will decrease, and under the previously stated circumstances, in a linear manner. On the other hand, by decreasing pTTF or forcibly speeding up the finality process, it can be expected that computational overhead will increase. Here are some basic and idealistic equations that represent this relationship:</p>
<p><strong>Change in computational overhead:</strong></p>
<p><span class="math">
CO_{new​}=CO_{old}​×\frac{TTF_{new}}{​TTF_{old}​​}
</span></p>
<p>(the percent change in TTF proportionally affects the change in Computational Overhead)</p>
<p><strong>Impact on computational overhead:</strong></p>
<p><span class="math">
\textbf{Percent Impact On CO}=\frac{CO_{old}}{CO_{new}​−CO_{old}​​}×100
</span></p>
<p>(Basic percent change formula)</p>
<p>&amp;&amp;</p>
<p><span class="math">
\textbf{Percent Impact on CO}=\frac{TTF_{new}}{​TTF_{old}​​−1}×100
</span></p>
<p>(the percent impact on Computational Overhead derived by the proportional change in TTF)</p>
<p>Once again, these equations do not represent the actual network impact if the pTTF was changed/enforced but it does provide a minimum expected impact on computational requirements.</p>
<p>Through these relationships, a scale can be derived that reveals how, although reducing pTTF and speeding up the finality process may provide countless benefits, its impact on computational requirements would reduce validator numbers and make it increasingly difficult for individuals to validate. By increasing validation requirements, smaller home validators would be deterred and provide more space for larger computational centers, this then reduces networking requirements and decreases CO, potentially attracting Validators again. Considering these interactions, Validator Count impact will still be noticeable but not crippling.</p>
<svg class="graphviz-svg-render" height="310pt" viewBox="0.00 0.00 456.43 310.40" width="456pt" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g class="graph" id="graph0" transform="scale(1 1) rotate(0) translate(4 306.4)">
<polygon fill="white" points="-4,4 -4,-306.4 452.43,-306.4 452.43,4 -4,4" stroke="transparent"></polygon>
<!-- pTTF -->
<g class="node" id="node1">
<title>pTTF</title>
<ellipse cx="185.35" cy="-284.4" fill="none" rx="33.06" ry="18" stroke="black"></ellipse>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="185.35" y="-280.2">pTTF</text>
</g>
<!-- Reduced TTF -->
<g class="node" id="node2">
<title>Reduced TTF</title>
<ellipse cx="185.35" cy="-195.6" fill="none" rx="64.31" ry="18" stroke="black"></ellipse>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="185.35" y="-191.4">Reduced TTF</text>
</g>
<!-- pTTF&#45;&gt;Reduced TTF -->
<g class="edge" id="edge1">
<title>pTTF-&gt;Reduced TTF</title>
<path d="M185.35,-266.25C185.35,-254.12 185.35,-237.62 185.35,-223.72" fill="none" stroke="red"></path>
<polygon fill="red" points="188.85,-223.71 185.35,-213.71 181.85,-223.71 188.85,-223.71" stroke="red"></polygon>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="234.52" y="-235.8">TTF enforcement</text>
</g>
<!-- CO -->
<g class="node" id="node3">
<title>CO</title>
<ellipse cx="185.35" cy="-106.8" fill="none" rx="27" ry="18" stroke="black"></ellipse>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="185.35" y="-102.6">CO</text>
</g>
<!-- Reduced TTF&#45;&gt;CO -->
<g class="edge" id="edge2">
<title>Reduced TTF-&gt;CO</title>
<path d="M185.35,-177.45C185.35,-165.32 185.35,-148.82 185.35,-134.92" fill="none" stroke="green"></path>
<polygon fill="green" points="188.85,-134.91 185.35,-124.91 181.85,-134.91 188.85,-134.91" stroke="green"></polygon>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="265.65" y="-147">Computational Compression</text>
</g>
<!-- Validator Count -->
<g class="node" id="node4">
<title>Validator Count</title>
<ellipse cx="185.35" cy="-18" fill="none" rx="73.56" ry="18" stroke="black"></ellipse>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="185.35" y="-13.8">Validator Count</text>
</g>
<!-- CO&#45;&gt;Validator Count -->
<g class="edge" id="edge3">
<title>CO-&gt;Validator Count</title>
<path d="M158.49,-104.67C112.77,-102.14 22.91,-94.22 3.57,-70.8 -1.19,-65.04 -1.19,-59.76 3.57,-54 16.73,-38.06 62.6,-29.3 104.96,-24.53" fill="none" stroke="red"></path>
<polygon fill="red" points="105.48,-27.99 115.06,-23.46 104.75,-21.03 105.48,-27.99" stroke="red"></polygon>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="84.24" y="-58.2">1: Increased Validation Cost</text>
</g>
<!-- CO&#45;&gt;Validator Count -->
<g class="edge" id="edge5">
<title>CO-&gt;Validator Count</title>
<path d="M183.51,-88.73C182.98,-83.04 182.47,-76.65 182.21,-70.8 181.88,-63.34 181.88,-61.46 182.21,-54 182.32,-51.44 182.49,-48.78 182.68,-46.12" fill="none" stroke="green"></path>
<polygon fill="green" points="186.17,-46.32 183.51,-36.07 179.19,-45.75 186.17,-46.32" stroke="green"></polygon>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="259.92" y="-58.2">3: Reduced Validation Cost</text>
</g>
<!-- Validator Count&#45;&gt;CO -->
<g class="edge" id="edge4">
<title>Validator Count-&gt;CO</title>
<path d="M257.99,-21.16C310.52,-25.46 367.97,-37.55 341.35,-70.8 326.68,-89.13 263.28,-98.52 221.96,-102.78" fill="none" stroke="red"></path>
<polygon fill="red" points="221.58,-99.3 211.97,-103.75 222.26,-106.26 221.58,-99.3" stroke="red"></polygon>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="397.89" y="-58.2">2: Network Relief</text>
</g>
</g>
</svg>
<p>Concluding this segment, it’s clear that as we demand a faster TTF, our computational overhead increases and we (potentially) begin to lose the major benefit of PoS. According to the Ethereum Foundation, PoS reduces energy requirements by over 99% compared to PoW, which is great, but we must notice that this is variable, and demanding some TTF convenience via “brute force” may negate this major benefit of PoS. There are a few patterns that will arise as I continue this paper but one I’ll reveal now is how PoS and PoW are on more of a spectrum than you might think. Both Sybil Resistance mechanisms provide benefits but both are distorted with each convenience. In this case, as TTF decreases (presumably through an enforced pTTF) and we move towards convenience we lose some of our consensus identity leading to either severe Validation concentration or a larger environmental impact.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50393-validation-cost-security-3" name="p-50393-validation-cost-security-3"></a><strong>Validation Cost &amp; Security</strong></h1>
<p>Looking into the world of validation costs, (either computationally or financially) you can find another tradeoff between the cost of validation and security. Validation costs, in the traditional sense, are large deposits required of users to participate in block assembly. This deposit incentivizes good behavior through the threat of penalties and reduces validator turbulence. The difference in validation requirements between BTC and ETH is found in the discouragement method, while the cost of entry for BTC is in computation only (leading to probability discouragement) ETH validation requirements are financial (leading to price discouragement). Both of these discouragement methods, make it too costly for a malicious actor to launch a chain-destabilizing attack. By increasing validation requirements, malicious actors can be better deterred and chain integrity can be further guaranteed. Increasing this validation requirement, however, doesn’t always benefit the chain. Take, for example, validation rights and concentration.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50393-validation-rights-and-concentration-4" name="p-50393-validation-rights-and-concentration-4"></a><strong>Validation Rights &amp; and Concentration</strong></h2>
<p>As validation cost increases, fewer and fewer people are able to participate in the validation process, and remaining validators are allowed more responsibility over network activities. As validation requirements increase, validation is concentrated and network manipulation is possible. The saying, ‘Too much of a good thing’ applies here, as without high validation requirements, bad actors have more opportunity to strike, but with VR being too high, validation responsibilities are concentrated and network sovereignty is compromised. The major punchline of these distributed ledgers is their ability to incorporate the community in network-wide decisions and block assembly. Currently, Bitcoin is able to provide a lower barrier to entry for community validation contributions, while Ethereum is able to provide more equitable validation selection. In short, validation cost influences network security on both ends, one end enables community validation but reduces the cost of malicious activity, on the other end validation is concentrated, and network sovereignty is compromised. One last thing to mention is that if VRs are lowered, more will participate in network validation leading to a higher computational overhead. This mechanism alongside the aforementioned ones plays into each other creating a web of interactions all based upon each other:</p>
<svg class="graphviz-svg-render" height="488pt" viewBox="0.00 0.00 722.30 488.00" width="722pt" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g class="graph" id="graph0" transform="scale(1 1) rotate(0) translate(4 484)">
<polygon fill="white" points="-4,4 -4,-484 718.3,-484 718.3,4 -4,4" stroke="transparent"></polygon>
<!-- pTTF -->
<g class="node" id="node1">
<title>pTTF</title>
<ellipse cx="560.22" cy="-462" fill="none" rx="33.06" ry="18" stroke="black"></ellipse>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="560.22" y="-457.8">pTTF</text>
</g>
<!-- Reduced TTF -->
<g class="node" id="node2">
<title>Reduced TTF</title>
<ellipse cx="560.22" cy="-373.2" fill="none" rx="64.31" ry="18" stroke="black"></ellipse>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="560.22" y="-369">Reduced TTF</text>
</g>
<!-- pTTF&#45;&gt;Reduced TTF -->
<g class="edge" id="edge1">
<title>pTTF-&gt;Reduced TTF</title>
<path d="M560.22,-443.85C560.22,-431.72 560.22,-415.22 560.22,-401.32" fill="none" stroke="red"></path>
<polygon fill="red" points="563.72,-401.31 560.22,-391.31 556.72,-401.31 563.72,-401.31" stroke="red"></polygon>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="609.39" y="-413.4">TTF enforcement</text>
</g>
<!-- CO -->
<g class="node" id="node3">
<title>CO</title>
<ellipse cx="468.22" cy="-284.4" fill="none" rx="27" ry="18" stroke="black"></ellipse>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="468.22" y="-280.2">CO</text>
</g>
<!-- Reduced TTF&#45;&gt;CO -->
<g class="edge" id="edge2">
<title>Reduced TTF-&gt;CO</title>
<path d="M506.85,-362.95C492.47,-357.78 478.46,-349.74 469.62,-337.2 464.6,-330.09 463.17,-320.97 463.34,-312.36" fill="none" stroke="green"></path>
<polygon fill="green" points="466.84,-312.55 464.2,-302.29 459.86,-311.95 466.84,-312.55" stroke="green"></polygon>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="550.52" y="-324.6">Computational Compression</text>
</g>
<!-- User Base -->
<g class="node" id="node4">
<title>User Base</title>
<ellipse cx="652.22" cy="-284.4" fill="none" rx="50.4" ry="18" stroke="black"></ellipse>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="652.22" y="-280.2">User Base</text>
</g>
<!-- Reduced TTF&#45;&gt;User Base -->
<g class="edge" id="edge3">
<title>Reduced TTF-&gt;User Base</title>
<path d="M599.05,-358.73C610.21,-353.39 621.66,-346.31 630.22,-337.2 636.83,-330.16 641.61,-320.84 645,-312.05" fill="none" stroke="green"></path>
<polygon fill="green" points="648.34,-313.09 648.25,-302.49 641.71,-310.83 648.34,-313.09" stroke="green"></polygon>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="677.76" y="-324.6">Convenience</text>
</g>
<!-- Validation Cost -->
<g class="node" id="node5">
<title>Validation Cost</title>
<ellipse cx="411.22" cy="-195.6" fill="none" rx="71.85" ry="18" stroke="black"></ellipse>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="411.22" y="-191.4">Validation Cost</text>
</g>
<!-- CO&#45;&gt;Validation Cost -->
<g class="edge" id="edge4">
<title>CO-&gt;Validation Cost</title>
<path d="M440.88,-283.42C414.36,-281.79 375.37,-274.7 356.03,-248.4 347.66,-237.02 356.04,-226.07 368.59,-217.19" fill="none" stroke="green"></path>
<polygon fill="green" points="370.72,-219.98 377.3,-211.67 366.98,-214.07 370.72,-219.98" stroke="green"></polygon>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="401.31" y="-235.8">1: Increased CR</text>
</g>
<!-- CO&#45;&gt;Validation Cost -->
<g class="edge" id="edge5">
<title>CO-&gt;Validation Cost</title>
<path d="M462.98,-266.71C459.22,-256.14 453.54,-242.45 446.22,-231.6 443.66,-227.81 440.66,-224.09 437.5,-220.56" fill="none" stroke="red"></path>
<polygon fill="red" points="439.91,-218.02 430.46,-213.22 434.85,-222.86 439.91,-218.02" stroke="red"></polygon>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="503.64" y="-235.8">4: Decreased CR</text>
</g>
<!-- Validator Count -->
<g class="node" id="node6">
<title>Validator Count</title>
<ellipse cx="529.22" cy="-106.8" fill="none" rx="73.56" ry="18" stroke="black"></ellipse>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="529.22" y="-102.6">Validator Count</text>
</g>
<!-- Validation Cost&#45;&gt;Validator Count -->
<g class="edge" id="edge6">
<title>Validation Cost-&gt;Validator Count</title>
<path d="M464.86,-183.57C479.99,-178.35 495.51,-170.71 507.22,-159.6 514.22,-152.95 519.09,-143.72 522.44,-134.89" fill="none" stroke="red"></path>
<polygon fill="red" points="525.81,-135.84 525.6,-125.24 519.16,-133.66 525.81,-135.84" stroke="red"></polygon>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="536.13" y="-147">2: VR</text>
</g>
<!-- Validation Cost&#45;&gt;Validator Count -->
<g class="edge" id="edge7">
<title>Validation Cost-&gt;Validator Count</title>
<path d="M429.62,-178.05C445.85,-163.49 467.24,-144.36 469.38,-142.8 476.09,-137.93 483.52,-133.14 490.8,-128.75" fill="none" stroke="green"></path>
<polygon fill="green" points="493.03,-131.5 499.88,-123.42 489.48,-125.46 493.03,-131.5" stroke="green"></polygon>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="487.13" y="-147">5: VR</text>
</g>
<!-- Security -->
<g class="node" id="node7">
<title>Security</title>
<ellipse cx="43.22" cy="-18" fill="none" rx="43.44" ry="18" stroke="black"></ellipse>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="43.22" y="-13.8">Security</text>
</g>
<!-- Validation Cost&#45;&gt;Security -->
<g class="edge" id="edge9">
<title>Validation Cost-&gt;Security</title>
<path d="M340.49,-191.86C242.7,-186.41 73.92,-170.48 35.95,-124.8 17.81,-102.97 24.15,-69.06 32.07,-45.53" fill="none" stroke="green"></path>
<polygon fill="green" points="35.47,-46.44 35.62,-35.85 28.89,-44.04 35.47,-46.44" stroke="green"></polygon>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="121.35" y="-102.6">Malicious Activity Deterrence</text>
</g>
<!-- Validator Concentration -->
<g class="node" id="node8">
<title>Validator Concentration</title>
<ellipse cx="320.22" cy="-106.8" fill="none" rx="104.75" ry="18" stroke="black"></ellipse>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="320.22" y="-102.6">Validator Concentration</text>
</g>
<!-- Validation Cost&#45;&gt;Validator Concentration -->
<g class="edge" id="edge10">
<title>Validation Cost-&gt;Validator Concentration</title>
<path d="M348.65,-186.52C332.99,-181.36 317.92,-173.02 308.31,-159.6 303.08,-152.3 303.55,-143.06 306.14,-134.41" fill="none" stroke="green"></path>
<polygon fill="green" points="309.42,-135.62 309.77,-125.03 302.9,-133.09 309.42,-135.62" stroke="green"></polygon>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="380.17" y="-147">Validation Concentration</text>
</g>
<!-- Validator Count&#45;&gt;CO -->
<g class="edge" id="edge8">
<title>Validator Count-&gt;CO</title>
<path d="M546.34,-124.53C550.75,-129.9 554.9,-136.19 557.22,-142.8 572.76,-187.1 581.93,-209.8 555.22,-248.4 543.73,-265 522.95,-273.73 504.58,-278.32" fill="none" stroke="red"></path>
<polygon fill="red" points="503.72,-274.93 494.7,-280.47 505.21,-281.77 503.72,-274.93" stroke="red"></polygon>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="621.76" y="-191.4">3: Network Relief</text>
</g>
<!-- Validator Concentration&#45;&gt;Security -->
<g class="edge" id="edge11">
<title>Validator Concentration-&gt;Security</title>
<path d="M272.28,-90.78C219.88,-74.36 136.67,-48.28 86.27,-32.49" fill="none" stroke="red"></path>
<polygon fill="red" points="87.24,-29.13 76.65,-29.48 85.15,-35.81 87.24,-29.13" stroke="red"></polygon>
<text font-family="Times,serif" font-size="14.00" text-anchor="middle" x="246.87" y="-58.2">Centralization</text>
</g>
</g>
</svg>
<p><em>CR=computational requirement, VR=validation requirement, CO=computational overhead</em></p>
<h1><a class="anchor" href="https://ethresear.ch#p-50393-wrapping-it-all-up-5" name="p-50393-wrapping-it-all-up-5"></a><strong>Wrapping it all up:</strong></h1>
<p>Overall this article aimed not just to list out singular interactions, but to remind developers and users alike that any development made to the core Ethereum protocol will have countless lasting impacts. From computational overhead to validation concentration, this web of interactions is a testimony to the complexity of the core Ethereum protocol, and a reminder that protocol decisions should not be taken lightly.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/the-multidimensional-scale-of-overhead-availability-and-validation-requirements/20645">Read full topic</a></p>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 16:53:07 +0000</pubDate>
</item>
<item>
<title>Hermes: a monitoring light node for Ethereum's Gossipsub network</title>
<link>https://ethresear.ch/t/hermes-a-monitoring-light-node-for-ethereums-gossipsub-network/20639</link>
<guid>https://ethresear.ch/t/hermes-a-monitoring-light-node-for-ethereums-gossipsub-network/20639</guid>
<content:encoded><![CDATA[
<div> 关键词：Hermes、轻节点、libp2p、网络监听、性能测量

总结:
Hermes是一个基于libp2p网络的监听和追踪工具，由轻节点组成。它能够连接到网络中的其他参与者，并依赖于一个可信的本地节点来确保响应所有传入请求并维护稳定的连接。Hermes设计用于与任何基于Consensus层的区块链网络协同工作，但未来将支持更多网络。它通过订阅所有可用的GossipSub主题来全面接收、揭示和追踪网络交互，这有助于开发者根据消息传播延迟和控制消息开销来调整网络协议。

Hermes的运行需要两个主要组件：一个可信的本地节点提供链状态信息以及一个事件消费者接收libp2p跟踪并存储它们。轻节点负责发现和连接到网络中的伙伴，维护与它们的连接，并监控libp2p主机内的内部事件。为了确保连接的稳定性，Hermes需要成功响应远程节点可能提出的链向RPC调用。

目前，Hermes支持将这些跟踪发送给不同的数据消费者或数据流，包括HTTP API和gRPC主机端点的EF使用。它还提供了三种不同的持久化选项以存储事件跟踪：EF使用的回调、日志输出（适用于本地开发测试）和存储在本地。

Hermes已被应用于研究Gossipsub在以太坊网络上的运行情况，并帮助发现了不同实现中Gossipsub操作的错误，以及支持了IDONTWANT消息的采纳。未来，团队欢迎社区对工具进行改进和扩展，支持更多网络或数据流，并提出了参与贡献的方式。

Hermes作为一个开源项目，旨在促进libp2p网络的研究和优化，通过收集关键数据来提高网络效率和性能。 <div>
<p>The <a href="https://probelab.io" rel="noopener nofollow ugc">ProbeLab team</a> is happy to announce <a href="https://github.com/probe-lab/hermes" rel="noopener nofollow ugc">Hermes</a>, built primarily by <a class="mention" href="https://ethresear.ch/u/dennis-tra">@dennis-tra</a> and <a class="mention" href="https://ethresear.ch/u/cortze">@cortze</a>. <em>Hermes is a <a href="https://docs.libp2p.io/concepts/pubsub/overview/" rel="noopener nofollow ugc">GossipSub</a> listener and tracer for <a href="https://libp2p.io/" rel="noopener nofollow ugc">libp2p</a>-based networks.</em> We hope that the tool will see wide use by the community and contributions to include more features and support for other networks.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50382-what-is-hermes-and-what-can-you-do-with-it-1" name="p-50382-what-is-hermes-and-what-can-you-do-with-it-1"></a>What is Hermes and what can you do with it</h2>
<p>Hermes behaves like a light-node and connects to other participants in the network. It relies on a trusted local node that ensures we can reply to any incoming requests and maintain stable connections.</p>
<p>Hermes-based experiments aim to measure the efficiency and performance of the GossipSub message broadcasting protocol in any libp2p-based network. Hermes can help developers tune their network’s protocols based on the message propagation latency and control message overhead.</p>
<p>The tool currently supports any <a href="https://ethereum.org/en/" rel="noopener nofollow ugc">Ethereum</a> network (at the Consensus Layer), although there will be more networks supported in the future.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50382-how-does-it-work-2" name="p-50382-how-does-it-work-2"></a>How does it work</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50382-components-3" name="p-50382-components-3"></a>Components</h3>
<p>Hermes operates as a single component, the <strong>light-node</strong>, for each network it is deployed in. However, it has two major requirements to work: 1) a <strong>trusted local node</strong> that will provide the right view of the chain’s state and 2) an <strong>event consumer</strong> that will receive all the libp2p traces and will store them. The light-node is responsible for discovering peers, maintain connections with them, and monitor the internal events at the libp2p host.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50382-light-node-4" name="p-50382-light-node-4"></a>Light-node</h3>
<p>Hermes operates like a standard libp2p node, discovering and connecting to the network while supporting all libp2p protocols to ensure reliable and secure communication. It connects to a trusted local, or remote node for certain RPCs that require chain state information, leveraging these trusted sources to enhance its capabilities.</p>
<p>Hermes subscribes to all available GossipSub topics, allowing it to comprehensively receive, unveil and trace all the interactions with the network. This enables the tool to keep track of network activity efficiently.</p>
<p>Additionally, Hermes can submit these traces to any defined consumer, with current support for <a href="https://aws.amazon.com/kinesis/" rel="noopener nofollow ugc">AWS Kinesis</a> and <a href="https://github.com/ethpandaops/xatu" rel="noopener nofollow ugc">Xatu</a> from the Ethereum Foundation (EF). This feature facilitates the integration of network data with analytics and monitoring tools, aiding in the overall security and functionality of the network.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50382-deployment-5" name="p-50382-deployment-5"></a>Deployment</h3>
<p>Hermes has been developed to run continuously. However, to increase the steadiness of the light-node’s connectivity, the node must successfully respond to chain-oriented RPC calls that remote peers might ask. Since Hermes has been designed to be chain-agnostic, we must rely on an external trusted node to serve that information.</p>
<p>Hermes must be paired with one chain client: in our case and for Ethereum’s Consensus Layer, we used Prysm. Giving Hermes the HTTP API and the gRPC host’s endpoints, Hermes can add itself as a “whitelisted” node, in order to later forward most of the chain- or state-related RPC calls. However, the same Prysm node could be connected to several Hermes instances as the following graph shows, which helps reduce the deployment requirements.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/9/f9928f77c5bd3d905868ad64c330545ef157b990.png" title="hermes-arch"><img alt="hermes-arch" height="238" src="https://ethresear.ch/uploads/default/optimized/3X/f/9/f9928f77c5bd3d905868ad64c330545ef157b990_2_690x238.png" width="690" /></a></div><p></p>
<p>Hermes has been extended to support a different set of data consumers or data streams. Since all the event traces need to be persisted somewhere, the tool offers three different options:</p>
<ul>
<li><a href="https://aws.amazon.com/kinesis/" rel="noopener nofollow ugc">AWS Kinesis</a> (our deployment)</li>
<li><a href="https://github.com/ethpandaops/xatu" rel="noopener nofollow ugc">Xatu</a> callbacks (used by the EF)</li>
<li>Trace logger to stdout (for local dev-testing)</li>
</ul>
<p>If you are interested in using or upgrading the tool to support other networks or data streams, <a href="https://probelab.network/contact" rel="noopener nofollow ugc">reach out to us</a>.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50382-what-data-does-hermes-gather-6" name="p-50382-what-data-does-hermes-gather-6"></a>What data does Hermes gather</h2>
<p>As Hermes maintains stable connections with the rest of the network nodes, it can emit debugging traces of how libp2p and GossipSub work in the wild.</p>
<p>Hermes gathers the following information out of the box from the libp2p host and the GossipSub protocol:</p>
<ul>
<li>Connections and Disconnections from the libp2p host</li>
<li>Control GossipSub messages:
<ul>
<li>Subscriptions from other nodes to topics</li>
<li><code>GRAFT</code> and <code>PRUNE</code> control messages</li>
<li>GossipSub peer scores</li>
<li><code>IHAVE</code> and <code>IWANT</code> control messages</li>
<li>the recent <a href="https://github.com/probe-lab/hermes/pull/37" rel="noopener nofollow ugc"><code>IDONTWANT</code></a> control messages</li>
</ul>
</li>
<li>Sent and received messages</li>
<li>Protocol RPCs</li>
</ul>
<p>This is a very valuable set of data to have, as it covers a large footprint of the behaviour of Gossipsub in a given network. Relatively simple data analysis can reveal very important metrics and inform engineers of the healthy (or not) operation of the protocol. For cases where Gossipsub serves as the block propagation protocol of a blockchain network, having deep insights into these metrics is critical!</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50382-how-have-we-used-it-so-far-7" name="p-50382-how-have-we-used-it-so-far-7"></a>How have we used it so far</h2>
<p>Hermes has helped the ProbeLab team carry out an extensive study on Gossipsub’s operation on the Ethereum network. Here are the links to the relevant <a href="http://ethresear.ch/">ethresear.ch</a> posts:</p>
<ul>
<li><a href="https://ethresear.ch/t/gossip-iwant-ihave-effectiveness-in-ethereums-gossipsusb-network/19686"><strong>Gossip IWANT/IHAVE Effectiveness in Ethereum’s Gossipsusb network</strong></a></li>
<li><a href="https://ethresear.ch/t/gossipsub-network-dynamicity-through-grafts-and-prunes/19750"><strong>Gossipsub Network Dynamicity through GRAFTs and PRUNEs</strong></a></li>
<li><a href="https://ethresear.ch/t/number-duplicate-messages-in-ethereums-gossipsub-network/19921"><strong>Number Duplicate Messages in Ethereum’s Gossipsub Network</strong></a></li>
<li><a href="https://ethresear.ch/t/ethereum-node-message-propagation-bandwidth-consumption/19952"><strong>Ethereum Node Message Propagation Bandwidth Consumption</strong></a></li>
<li><a href="https://ethresear.ch/t/gossipsub-message-propagation-latency/19982"><strong>Gossipsub Message Propagation Latency</strong></a></li>
</ul>
<p>Furthermore, it helped uncover bugs in the operation of Gossipsub across implementations and provided supporting evidence regarding the need for <code>IDONTWANT</code> message adoption.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50382-future-plans-how-to-contribute-8" name="p-50382-future-plans-how-to-contribute-8"></a>Future plans &amp; how to contribute</h2>
<p>Hermes is open-source - its Github repository is: <a class="inline-onebox" href="https://github.com/probe-lab/hermes" rel="noopener nofollow ugc">GitHub - probe-lab/hermes: A Gossipsub listener and tracer.</a>. We welcome Github Issues to discuss with the community, receive feature requests and hear feedback. We also welcome Pull Requests to improve the tool and add features or support for other networks. If you are interested in data from other networks and would like Hermes to support those, <a href="https://probelab.network/contact" rel="noopener nofollow ugc">reach out</a>.</p>
<p>We hope you’ll enjoy playing around with Hermes. We would love to hear how you’ve used it and any improvements you’ve made.</p>
<p>[Originally posted at: <a href="https://www.probelab.network/blog/hermes-a-monitoring-light-node-for-gossipsub-based-networks." rel="noopener nofollow ugc">https://www.probelab.network/blog/hermes-a-monitoring-light-node-for-gossipsub-based-networks.</a>]</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/hermes-a-monitoring-light-node-for-ethereums-gossipsub-network/20639">Read full topic</a></p>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 09:06:47 +0000</pubDate>
</item>
<item>
<title>The Impact of Quantum Computing on the Security of zk-Proofs: Approaches to Post-Quantum Cryptography</title>
<link>https://ethresear.ch/t/the-impact-of-quantum-computing-on-the-security-of-zk-proofs-approaches-to-post-quantum-cryptography/20623</link>
<guid>https://ethresear.ch/t/the-impact-of-quantum-computing-on-the-security-of-zk-proofs-approaches-to-post-quantum-cryptography/20623</guid>
<content:encoded><![CDATA[
<div> 关键词：量子计算、零知识证明、后量子密码学、挑战与机遇、未来发展

总结:
量子计算的兴起对基于经典算法的零知识证明（zk-Proofs）构成了重大威胁，尤其是针对依赖于离散对数问题和整数分解问题的安全性假设。然而，后量子密码学（Post-Quantum Cryptography, PQC）为抵御量子攻击提供了新的解决方案。本文深入探讨了量子计算对zk-Proofs的影响，分析了传统zk-Proofs在量子环境下的脆弱性，并介绍了几种基于后量子密码学的解决方案，如基于格的零知识证明和基于代码的零知识证明。

在面临量子计算带来的挑战时，研究人员提出了几种改进策略来集成PQC到zk-Proof中，包括优化算法以减少计算开销，提升实施复杂性，以及标准化和互操作性的问题。此外，文章还讨论了在实际应用中采用后量子zk-Proof可能遇到的挑战，如性能优化、安全性验证、标准制定和跨领域合作的重要性。

未来方向涉及进一步的算法创新、硬件加速技术、并行处理方法以及教育和培训计划，以促进量子安全的zk-Proof发展。同时，这些系统将扩展应用范围，提供增强的隐私保护、安全的电子投票系统以及具有保密性的智能合约等，从而在量子时代确保隐私和安全。总的来说，尽管存在挑战，但通过持续的研究和协作，后量子zk-Proof有望在未来实现高效、安全的应用，推动区块链、隐私计算和信息安全领域的革新。 <div>
<h2><a class="anchor" href="https://ethresear.ch#p-50344-introduction-1" name="p-50344-introduction-1"></a>Introduction</h2>
<p>Zero-Knowledge Proofs (zk-Proofs) have emerged as a cornerstone in modern cryptography, enabling secure and private verification of information without revealing the underlying data. These proofs are integral to various applications, including blockchain scalability solutions like zk-Rollups, secure authentication systems, and confidential computing. However, the rapid advancements in quantum computing pose significant threats to the foundational cryptographic assumptions underpinning many zk-Proofs. This article delves into the potential impact of quantum computing on the security of zk-Proofs and explores post-quantum cryptographic (PQC) approaches designed to mitigate these threats.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50344-understanding-zk-proofs-2" name="p-50344-understanding-zk-proofs-2"></a>Understanding zk-Proofs</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50344-what-are-zk-proofs-3" name="p-50344-what-are-zk-proofs-3"></a>What are zk-Proofs?</h3>
<p>Zero-Knowledge Proofs allow one party (the prover) to convince another party (the verifier) of the truth of a statement without revealing any additional information beyond the validity of the statement itself. The primary characteristics of zk-Proofs include:</p>
<ol>
<li><strong>Zero-Knowledge:</strong> No extra information is disclosed apart from the fact that the statement is true.</li>
<li><strong>Succinctness:</strong> Proofs are typically small in size and can be verified quickly.</li>
<li><strong>Soundness:</strong> If the statement is false, no cheating prover can convince the verifier that it is true, except with some small probability.</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-50344-types-of-zk-proofs-4" name="p-50344-types-of-zk-proofs-4"></a>Types of zk-Proofs</h3>
<ul>
<li><strong>zk-SNARKs (Zero-Knowledge Succinct Non-interactive Arguments of Knowledge):</strong> These require a trusted setup and are known for their compact proof sizes and fast verification times. Examples include Groth16 and PLONK.</li>
<li><strong>zk-STARKs (Zero-Knowledge Scalable Transparent Arguments of Knowledge):</strong> These do not require a trusted setup and are designed to be scalable and transparent, leveraging collision-resistant hash functions.</li>
<li><strong>zk-Rollups:</strong> A layer-2 scaling solution for blockchains that aggregates multiple transactions into a single proof, thereby enhancing scalability and reducing on-chain data.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50344-quantum-computing-and-its-threat-to-cryptography-5" name="p-50344-quantum-computing-and-its-threat-to-cryptography-5"></a>Quantum Computing and Its Threat to Cryptography</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50344-the-rise-of-quantum-computing-6" name="p-50344-the-rise-of-quantum-computing-6"></a>The Rise of Quantum Computing</h3>
<p>Quantum computers leverage principles of quantum mechanics to perform computations that are infeasible for classical computers. Algorithms like Shor’s and Grover’s can solve specific problems exponentially faster than their classical counterparts:</p>
<ul>
<li><strong>Shor’s Algorithm:</strong> Efficiently factors large integers and computes discrete logarithms, undermining the security of widely used cryptographic systems like RSA and ECC (Elliptic Curve Cryptography).</li>
<li><strong>Grover’s Algorithm:</strong> Provides a quadratic speedup for brute-force searching, affecting symmetric cryptographic schemes by effectively halving their key lengths.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50344-quantum-threats-to-zk-proofs-7" name="p-50344-quantum-threats-to-zk-proofs-7"></a>Quantum Threats to zk-Proofs</h3>
<p>The security of many zk-Proofs, especially zk-SNARKs, relies on cryptographic assumptions that are vulnerable to quantum attacks:</p>
<ul>
<li><strong>Discrete Logarithm Problem (DLP):</strong> zk-SNARKs often depend on the hardness of DLP on elliptic curves. Shor’s algorithm can solve DLP efficiently, compromising the security of these proofs.</li>
<li><strong>Integer Factorization:</strong> Similar to DLP, algorithms that depend on the difficulty of factoring large integers are at risk.</li>
<li><strong>Hash Functions:</strong> While zk-STARKs rely on hash functions, Grover’s algorithm can speed up finding collisions, necessitating the use of longer hash outputs to maintain security levels.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50344-post-quantum-cryptography-pqc-approaches-8" name="p-50344-post-quantum-cryptography-pqc-approaches-8"></a>Post-Quantum Cryptography (PQC) Approaches</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50344-overview-of-pqc-9" name="p-50344-overview-of-pqc-9"></a>Overview of PQC</h3>
<p>Post-Quantum Cryptography aims to develop cryptographic systems that are secure against both classical and quantum adversaries. The National Institute of Standards and Technology (NIST) has been leading efforts to standardize PQC algorithms, focusing on various mathematical problems believed to be resistant to quantum attacks.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50344-pqc-candidates-relevant-to-zk-proofs-10" name="p-50344-pqc-candidates-relevant-to-zk-proofs-10"></a>PQC Candidates Relevant to zk-Proofs</h3>
<ol>
<li><strong>Lattice-Based Cryptography:</strong></li>
</ol>
<ul>
<li><strong>Hard Problems:</strong> Learning With Errors (LWE), Ring-LWE, Shortest Vector Problem (SVP).</li>
<li><strong>Advantages:</strong> Strong resistance to quantum attacks, versatility in constructing various cryptographic primitives.</li>
<li><strong>Applications:</strong> Basis for post-quantum zk-SNARKs and other zero-knowledge systems.</li>
</ul>
<ol start="2">
<li><strong>Code-Based Cryptography:</strong></li>
</ol>
<ul>
<li><strong>Hard Problems:</strong> Syndrome Decoding, Generalized Decoding Problem.</li>
<li><strong>Advantages:</strong> High security margins, established mathematical foundations.</li>
<li><strong>Applications:</strong> Potential basis for new zk-Proof systems, though less flexible than lattice-based approaches.</li>
</ul>
<ol start="3">
<li><strong>Multivariate Quadratic (MQ) Cryptography:</strong></li>
</ol>
<ul>
<li><strong>Hard Problems:</strong> Solving systems of multivariate quadratic equations.</li>
<li><strong>Advantages:</strong> Efficient signature schemes and encryption methods.</li>
<li><strong>Applications:</strong> Limited in scope for zk-Proofs but useful for specific zero-knowledge protocols.</li>
</ul>
<ol start="4">
<li><strong>Hash-Based Cryptography:</strong></li>
</ol>
<ul>
<li><strong>Hard Problems:</strong> Security relies on the collision resistance of hash functions.</li>
<li><strong>Advantages:</strong> Simple security assumptions, robust against quantum attacks when using appropriate hash functions.</li>
<li><strong>Applications:</strong> Enhancing zk-STARKs with quantum-resistant hash functions.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50344-integrating-pqc-into-zk-proofs-11" name="p-50344-integrating-pqc-into-zk-proofs-11"></a>Integrating PQC into zk-Proofs</h3>
<h4><a class="anchor" href="https://ethresear.ch#p-50344-lattice-based-zk-snarks-12" name="p-50344-lattice-based-zk-snarks-12"></a>Lattice-Based zk-SNARKs</h4>
<p>Lattice-based zk-SNARKs aim to replace the elliptic curve assumptions with lattice problems like LWE. This integration involves:</p>
<ul>
<li><strong>Parameter Selection:</strong> Choosing lattice dimensions and error rates that ensure security against quantum attacks while maintaining efficiency.</li>
<li><strong>Protocol Design:</strong> Adapting zk-SNARK protocols to utilize lattice-based commitments and proofs.</li>
<li><strong>Optimization:</strong> Reducing proof sizes and verification times through algorithmic improvements and efficient implementations.</li>
</ul>
<p><strong>Example:</strong> Ligero is a lattice-based zk-SNARK that demonstrates the feasibility of constructing efficient zero-knowledge proofs resistant to quantum attacks.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50344-enhancing-zk-starks-with-pqc-13" name="p-50344-enhancing-zk-starks-with-pqc-13"></a>Enhancing zk-STARKs with PQC</h4>
<p>While zk-STARKs are inherently more resistant to quantum attacks due to their reliance on hash functions, further enhancements include:</p>
<ul>
<li><strong>Quantum-Resistant Hash Functions:</strong> Utilizing hash functions like SHA-3 or those specifically designed for post-quantum security to prevent collision attacks by quantum algorithms.</li>
<li><strong>Optimized Polynomial Commitments:</strong> Implementing commitment schemes that maintain zero-knowledge properties without relying on structures vulnerable to quantum attacks.</li>
</ul>
<p><strong>Example:</strong> Enhanced zk-STARKs incorporate SHA-3-based commitments and optimized FRI (Fast Reed-Solomon Interactive Oracle Proofs of Proximity) protocols to bolster security against quantum adversaries.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50344-hybrid-approaches-14" name="p-50344-hybrid-approaches-14"></a>Hybrid Approaches</h3>
<p>Combining multiple PQC techniques can provide layered security and mitigate the weaknesses of individual systems. For instance, integrating lattice-based commitments with hash-based schemes can enhance the overall security of zk-Proofs, ensuring robustness against a broader range of quantum attacks.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50344-challenges-in-adopting-pqc-for-zk-proofs-15" name="p-50344-challenges-in-adopting-pqc-for-zk-proofs-15"></a>Challenges in Adopting PQC for zk-Proofs</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50344-computational-overhead-16" name="p-50344-computational-overhead-16"></a>Computational Overhead</h3>
<p>Post-quantum zk-Proofs often require more computational resources than their classical counterparts due to the complexity of lattice-based or code-based operations. This increased overhead can impact:</p>
<ul>
<li><strong>Proof Generation Time:</strong> Longer computation times for generating proofs, which may affect real-time applications.</li>
<li><strong>Proof Size:</strong> Larger proof sizes that consume more bandwidth and storage, posing challenges for blockchain scalability.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50344-implementation-complexity-17" name="p-50344-implementation-complexity-17"></a>Implementation Complexity</h3>
<p>Developing zk-Proofs based on PQC involves intricate mathematical and algorithmic modifications. Ensuring correctness, efficiency, and security in these implementations demands:</p>
<ul>
<li><strong>Expertise:</strong> Specialized knowledge in both zero-knowledge systems and post-quantum cryptography.</li>
<li><strong>Rigorous Testing:</strong> Extensive testing and formal verification to prevent vulnerabilities arising from new algorithmic integrations.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50344-standardization-and-interoperability-18" name="p-50344-standardization-and-interoperability-18"></a>Standardization and Interoperability</h3>
<p>The ongoing standardization process for PQC algorithms poses challenges for zk-Proofs:</p>
<ul>
<li><strong>Algorithm Selection:</strong> Choosing the most appropriate and standardized PQC algorithms to ensure long-term security and compatibility.</li>
<li><strong>Interoperability:</strong> Ensuring that post-quantum zk-Proofs can seamlessly integrate with existing cryptographic protocols and blockchain infrastructures.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50344-security-assurance-19" name="p-50344-security-assurance-19"></a>Security Assurance</h3>
<p>While PQC offers robust theoretical security, practical implementations must undergo rigorous scrutiny:</p>
<ul>
<li><strong>Cryptanalysis:</strong> Continuous analysis to identify potential weaknesses in lattice-based or code-based zk-Proofs.</li>
<li><strong>Quantum-Safe Assumptions:</strong> Validating that the underlying mathematical problems remain hard for quantum adversaries.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50344-case-studies-and-research-developments-20" name="p-50344-case-studies-and-research-developments-20"></a>Case Studies and Research Developments</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50344-ligero-a-lattice-based-zk-snark-21" name="p-50344-ligero-a-lattice-based-zk-snark-21"></a>Ligero: A Lattice-Based zk-SNARK</h3>
<p>Ligero exemplifies the application of lattice-based cryptography in zk-Proofs:</p>
<ul>
<li><strong>Design:</strong> Utilizes Ring-LWE for secure and efficient proof generation.</li>
<li><strong>Performance:</strong> Demonstrates comparable proof sizes and verification times to traditional zk-SNARKs while ensuring quantum resistance.</li>
<li><strong>Implications:</strong> Highlights the feasibility of deploying post-quantum zk-Proofs in practical applications like blockchain and confidential transactions.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50344-enhanced-zk-starks-with-sha-3-22" name="p-50344-enhanced-zk-starks-with-sha-3-22"></a>Enhanced zk-STARKs with SHA-3</h3>
<p>Research has focused on integrating SHA-3 into zk-STARKs to enhance their quantum resistance:</p>
<ul>
<li><strong>Implementation:</strong> Replaces existing hash functions with SHA-3 in the FRI protocol.</li>
<li><strong>Security:</strong> Provides stronger guarantees against collision attacks by quantum adversaries.</li>
<li><strong>Performance:</strong> Maintains scalability and transparency advantages inherent to zk-STARKs.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50344-hybrid-zk-proofs-combining-lattices-and-hash-functions-23" name="p-50344-hybrid-zk-proofs-combining-lattices-and-hash-functions-23"></a>Hybrid zk-Proofs Combining Lattices and Hash Functions</h3>
<p>Hybrid zk-Proofs leverage both lattice-based and hash-based cryptographic primitives:</p>
<ul>
<li><strong>Security:</strong> Offers layered protection by addressing different aspects of quantum vulnerabilities.</li>
<li><strong>Efficiency:</strong> Balances the computational overhead by optimizing the integration of multiple cryptographic techniques.</li>
<li><strong>Use Cases:</strong> Suitable for high-security applications requiring robust resistance against diverse quantum attacks.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50344-future-directions-24" name="p-50344-future-directions-24"></a>Future Directions</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50344-advanced-optimization-techniques-25" name="p-50344-advanced-optimization-techniques-25"></a>Advanced Optimization Techniques</h3>
<p>To mitigate the computational and size overheads associated with post-quantum zk-Proofs, ongoing research focuses on:</p>
<ul>
<li><strong>Algorithmic Innovations:</strong> Developing more efficient lattice-based algorithms tailored for zero-knowledge applications.</li>
<li><strong>Hardware Acceleration:</strong> Utilizing specialized hardware like GPUs and FPGAs to speed up proof generation and verification.</li>
<li><strong>Parallelization:</strong> Implementing parallel processing techniques to distribute computational tasks and reduce latency.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50344-standardization-efforts-26" name="p-50344-standardization-efforts-26"></a>Standardization Efforts</h3>
<p>Collaborative efforts between cryptographic researchers and standardization bodies aim to:</p>
<ul>
<li><strong>Finalize PQC Standards:</strong> Ensure that zk-Proofs adopt widely accepted and vetted post-quantum algorithms.</li>
<li><strong>Develop Best Practices:</strong> Create guidelines for implementing and deploying post-quantum zk-Proofs securely and efficiently.</li>
<li><strong>Foster Interoperability:</strong> Promote compatibility between different zk-Proof systems and existing cryptographic infrastructures.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50344-expanding-applications-27" name="p-50344-expanding-applications-27"></a>Expanding Applications</h3>
<p>Post-quantum zk-Proofs have the potential to revolutionize various sectors by providing:</p>
<ul>
<li><strong>Enhanced Privacy:</strong> In healthcare, finance, and personal data management, ensuring that sensitive information remains secure against quantum threats.</li>
<li><strong>Secure Voting Systems:</strong> Building robust and verifiable electronic voting systems that are resistant to quantum tampering.</li>
<li><strong>Confidential Smart Contracts:</strong> Enabling smart contracts that can execute complex logic while maintaining the confidentiality of proprietary data against quantum adversaries.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50344-interdisciplinary-research-28" name="p-50344-interdisciplinary-research-28"></a>Interdisciplinary Research</h3>
<p>Bridging the gap between quantum computing and cryptographic research is crucial for advancing zk-Proofs:</p>
<ul>
<li><strong>Collaborative Projects:</strong> Initiatives that bring together experts from quantum physics, cryptography, and computer science to develop integrated solutions.</li>
<li><strong>Educational Programs:</strong> Training the next generation of cryptographers and quantum computing specialists to address emerging challenges in zk-Proofs.</li>
<li><strong>Open-Source Contributions:</strong> Encouraging community-driven projects to experiment with and refine post-quantum zk-Proofs.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50344-conclusion-29" name="p-50344-conclusion-29"></a>Conclusion</h2>
<p>The advent of quantum computing presents significant challenges to the security of zk-Proofs, particularly those relying on cryptographic assumptions vulnerable to quantum attacks. Post-Quantum Cryptography offers promising avenues to fortify zk-Proofs against these emerging threats through lattice-based, code-based, and hybrid approaches. However, the transition to post-quantum zk-Proofs involves overcoming substantial computational, implementation, and standardization hurdles. Continued research, collaboration, and innovation are essential to develop efficient, secure, and scalable post-quantum zk-Proofs that can safeguard privacy and security in the quantum era.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50344-references-30" name="p-50344-references-30"></a>References</h2>
<ol>
<li><strong>Groth, Jens</strong>, “A verifiable secret shuffle and its application to e-voting,” in EUROCRYPT 2010.</li>
<li><strong>Chiesa, Alessandro; Tromer, Eran; Virza, Madars</strong>, “Zerocash: Decentralized anonymous payments from bitcoin,” 2014.</li>
<li><strong>Tromer, Eran; Virza, Madars</strong>, “Polynomial commitments and applications,” 2017.</li>
<li><strong>Buterin, Vitalik</strong>, “Rollup-centric scaling for Ethereum,” 2021.</li>
<li><strong>Post-Quantum Cryptography Standardization</strong>, <a href="https://csrc.nist.gov/projects/post-quantum-cryptography" rel="noopener nofollow ugc">NIST Post-Quantum Cryptography</a>.</li>
<li><strong>Boneh, Dan; Mosca, Michele</strong>, “Quantum attacks on Bitcoin, and classical countermeasures,” 2017.</li>
<li><strong>Fisch, Ben; Langley, Adam; Regehr, John</strong>, “Quantum computer attacks on ECDSA and RSA: An experimental analysis,” 2018.</li>
<li><strong>Zebra Systems</strong>, “Lattice-based cryptography for post-quantum security,” 2020.</li>
<li><strong>Lauter, Kyle; Johnson, David S.; Peikert, Chris</strong>, “Practical Lattice-Based Cryptography: A Survey,” 2017.</li>
<li><strong>Aroca, Javier et al.</strong>, “Enhancing zk-STARKs with Lattice-based Commitments,” 2022.</li>
</ol>
            <p><small>2 posts - 2 participants</small></p>
            <p><a href="https://ethresear.ch/t/the-impact-of-quantum-computing-on-the-security-of-zk-proofs-approaches-to-post-quantum-cryptography/20623">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sat, 12 Oct 2024 11:50:02 +0000</pubDate>
</item>
<item>
<title>Potential impact of blob sharing for rollups</title>
<link>https://ethresear.ch/t/potential-impact-of-blob-sharing-for-rollups/20619</link>
<guid>https://ethresear.ch/t/potential-impact-of-blob-sharing-for-rollups/20619</guid>
<content:encoded><![CDATA[
<div> 关键词：EIP-4844、Blob共享、数据可用性（DA）、成本节约、服务质量提升

总结:
文章主要探讨了在以太坊网络中，通过EIP-4844引入的Blob机制后，小规模的Rollups所面临的挑战以及解决策略。主要包含以下几个关键点：

1. **挑战与困境**：小规模Rollups在使用Blob机制时面临的问题在于，由于其较低的数据吞吐量，无法充分利用Blob的固定大小空间，导致两种可能的选择：要么不充分使用空间而支付全额费用，要么积攒足够数据后再提交，从而导致数据提交时间较长，影响服务质量和成本效率。

2. **Blob共享方案**：文章提出了一种解决方案——Blob共享。该方案允许多个Rollups共同在一个Blob中存储数据，通过这种方式提高数据使用效率，减少成本并提升服务质量。

3. **模拟研究**：通过模拟实验，研究了将26个Rollups的Blob数据进行共享的效果。结果显示，无论是大Rollups还是小Rollups，都能从这种共享中获得至少85%的成本节省，同时提高了数据可用性服务的质量。

4. **成本与服务质量改善**：改进的关键在于通过Blob共享减少了Blob的数量，平滑了Blob基础费用的增长，避免了因Blob数量过多导致的基础费用急剧增加，从而有效降低了总成本，并通过更频繁的数据提交提升了服务质量。

5. **未来展望**：文章指出，实现Blob共享需要引入代理数据可用性（Proxy DA）合约，并升级每个Rollup的数据可用性合约以支持与代理合约的交互。尽管这涉及一定的技术调整和安全考量，但通过建立协作机制，可以显著优化整个以太坊网络的效率和成本效益。

通过上述分析，Blob共享作为一种创新的策略，为解决小规模Rollups在使用EIP-4844后遇到的特定问题提供了有效途径，不仅降低了成本，还提升了数据可用性的服务质量，对以太坊生态系统的优化具有重要意义。 <div>
<p>By <a href="https://suhyeonlee.xyz/" rel="noopener nofollow ugc">Suhyeon</a> - <a href="https://www.tokamak.network/" rel="noopener nofollow ugc">Tokamak Network</a></p>
<h2><a class="anchor" href="https://ethresear.ch#p-50336-tldr-1" name="p-50336-tldr-1"></a>TL;DR</h2>
<blockquote>
<ul>
<li>Small rollups encounter a dilemma because of their low L2 data throughput: blob use inefficiency or long blob delay.</li>
<li>One solution to this problem is blob sharing, which is to put multiple rollups’ data into one blob.</li>
<li>Our research simulated blob sharing of 26 rollups using nearly 6 months of data.</li>
<li>The simulation results show all rollups (not only small ones) get at least 85% cost savings in DA.</li>
<li>We found this is because active blob sharing reduces the number of blobs and smooths the blob base fee without spikes.</li>
</ul>
</blockquote>
<h2><a class="anchor" href="https://ethresear.ch#p-50336-introduction-2" name="p-50336-introduction-2"></a>Introduction</h2>
<p>Rollups are a key scalability solution for Ethereum, processing transactions off-chain and submitting compressed data back to the main network. Before EIP-4844, rollups faced high data availability (DA) costs because all transaction data (calldata) had to be included on-chain, which was expensive.</p>
<p>EIP-4844 introduced blobs, a new type of transaction data that is cheaper to include on-chain compared to calldata. Blobs are fixed in size at 128 KB and are stored for about 18 days. This upgrade was intended to reduce DA costs significantly for rollups.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50336-the-small-rollups-dilemma-3" name="p-50336-the-small-rollups-dilemma-3"></a>The Small Rollup’s Dilemma</h2>
<p>While blobs reduce costs, their fixed size poses a challenge for small rollups with low data throughput. These rollups often cannot fill the entire 128 KB blob, leading to inefficiencies. They are faced with two options:</p>
<ul>
<li><strong>Use blob space inefficiently:</strong> Pay for the full blob even if they don’t need all that space.</li>
<li><strong>DA submissions with long delay:</strong> Wait until they accumulate enough data to fill a blob, which can reduce their DA service quality.</li>
</ul>
<p>To illustrate this issue, we analyzed the blob utilization and performance of various rollups over a period from block number 19,426,589 to 20,611,514. The results are summarized in Table 1.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50336-defining-da-service-quality-4" name="p-50336-defining-da-service-quality-4"></a>Defining DA Service Quality</h2>
<p>Here, to quantitatively measure the data availability service quality, we use a new indicator defined by the following formula:</p>
<div class="math">
\text{DA_Quality} = \frac{1}{\ln(\text{Average_Block_Gap}) + 1}
</div>
<p>This formula assigns a higher DA quality score to rollups that submit blobs more frequently (i.e., smaller average block gaps between blob submissions). It’s a logarithmic function to handle large block intervals, ensuring meaningful comparisons across different rollups.</p>
<p><strong>Table 1: Rollup Blob Utilization and Performance (Blocks 19,426,589 to 20,611,514)</strong></p>
<div class="md-table">
<table>
<thead>
<tr>
<th>Rollup</th>
<th>Blob Count</th>
<th>Average Size (KB)</th>
<th>Total Size (GB)</th>
<th>DA Quality</th>
</tr>
</thead>
<tbody>
<tr>
<td>Base</td>
<td>690,356</td>
<td>126.51 (98.84%)</td>
<td>83.293</td>
<td>0.309</td>
</tr>
<tr>
<td>Arbitrum</td>
<td>375,014</td>
<td>127.52 (99.63%)</td>
<td>45.607</td>
<td>0.288</td>
</tr>
<tr>
<td>Taiko</td>
<td>328,805</td>
<td>30.70 (23.99%)</td>
<td>9.628</td>
<td>0.592</td>
</tr>
<tr>
<td>OP Mainnet</td>
<td>265,469</td>
<td>126.71 (98.99%)</td>
<td>32.080</td>
<td>0.239</td>
</tr>
<tr>
<td>Scroll</td>
<td>139,199</td>
<td>105.35 (82.30%)</td>
<td>13.985</td>
<td>0.332</td>
</tr>
<tr>
<td>Blast</td>
<td>106,033</td>
<td>115.03 (89.87%)</td>
<td>11.632</td>
<td>0.255</td>
</tr>
<tr>
<td>Linea</td>
<td>93,604</td>
<td>117.64 (91.90%)</td>
<td>10.501</td>
<td>0.237</td>
</tr>
<tr>
<td>StarkNet</td>
<td>62,361</td>
<td>127.98 (99.98%)</td>
<td>7.611</td>
<td>0.242</td>
</tr>
<tr>
<td>zkSync Era</td>
<td>62,149</td>
<td>128.00 (99.99%)</td>
<td>7.586</td>
<td>0.215</td>
</tr>
<tr>
<td>Paradex</td>
<td>44,300</td>
<td>127.99 (99.99%)</td>
<td>5.407</td>
<td>0.234</td>
</tr>
<tr>
<td>Metal</td>
<td>31,382</td>
<td>0.18 (0.14%)</td>
<td>0.005</td>
<td>0.220</td>
</tr>
<tr>
<td>Zircuit</td>
<td>27,821</td>
<td>6.04 (4.72%)</td>
<td>0.160</td>
<td>0.275</td>
</tr>
<tr>
<td>Kroma</td>
<td>24,193</td>
<td>59.65 (46.60%)</td>
<td>1.376</td>
<td>0.208</td>
</tr>
<tr>
<td>Zora</td>
<td>23,592</td>
<td>126.58 (98.89%)</td>
<td>2.848</td>
<td>0.161</td>
</tr>
<tr>
<td>Mode</td>
<td>22,585</td>
<td>126.56 (98.88%)</td>
<td>2.726</td>
<td>0.159</td>
</tr>
<tr>
<td>Rari</td>
<td>19,583</td>
<td>31.08 (24.28%)</td>
<td>0.581</td>
<td>0.214</td>
</tr>
<tr>
<td>Optopia</td>
<td>7,851</td>
<td>16.19 (12.65%)</td>
<td>0.121</td>
<td>0.179</td>
</tr>
<tr>
<td>Boba Network</td>
<td>6,236</td>
<td>3.29 (2.57%)</td>
<td>0.020</td>
<td>0.166</td>
</tr>
<tr>
<td>Debank Chain</td>
<td>3,358</td>
<td>113.84 (88.94%)</td>
<td>0.365</td>
<td>0.162</td>
</tr>
<tr>
<td>Camp Network</td>
<td>3,292</td>
<td>98.39 (76.87%)</td>
<td>0.309</td>
<td>0.136</td>
</tr>
<tr>
<td>Nal</td>
<td>2,998</td>
<td>0.21 (0.17%)</td>
<td>0.001</td>
<td>0.168</td>
</tr>
<tr>
<td>Mint</td>
<td>2,515</td>
<td>90.89 (71.01%)</td>
<td>0.218</td>
<td>0.140</td>
</tr>
<tr>
<td>Lambda</td>
<td>2,336</td>
<td>103.80 (81.09%)</td>
<td>0.231</td>
<td>0.143</td>
</tr>
<tr>
<td>Lumio</td>
<td>1,390</td>
<td>0.43 (0.33%)</td>
<td>0.001</td>
<td>0.137</td>
</tr>
<tr>
<td>Parallel</td>
<td>1,348</td>
<td>119.29 (93.20%)</td>
<td>0.153</td>
<td>0.113</td>
</tr>
<tr>
<td>XGA</td>
<td>1,031</td>
<td>98.20 (76.72%)</td>
<td>0.097</td>
<td>0.142</td>
</tr>
<tr>
<td>Lisk</td>
<td>811</td>
<td>8.04 (6.28%)</td>
<td>0.006</td>
<td>0.126</td>
</tr>
<tr>
<td>Kinto</td>
<td>204</td>
<td>13.20 (10.31%)</td>
<td>0.003</td>
<td>0.124</td>
</tr>
</tbody>
</table>
</div><p>As seen in Table 1, smaller rollups often have low blob utilization percentages, indicating they are not efficiently using the blob space. This leads to higher costs per unit of data or potential delays in DA submissions.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50336-blob-sharing-and-simulation-5" name="p-50336-blob-sharing-and-simulation-5"></a>Blob Sharing and Simulation</h2>
<p>Blob sharing is a strategy that allows multiple rollups to combine their data into a single blob. This approach maximizes the utilization of the blob space and reduces costs for all participating rollups. We wondered how much of the DA costs can be saved if rollups collaborate.</p>
<p>To evaluate the effectiveness of blob sharing, we conducted a simulation using real-world data collected over nearly six months (block number 19,426,589 to 24,848,485) after the implementation of EIP-4844. Here are the key assumptions and the process we followed:</p>
<ul>
<li><strong>Constant Data Rate:</strong> We assumed that each rollup produces data at a consistent rate between blob submissions.</li>
<li><strong>Blob Sharing Structure:</strong> In our simulation, blobs are shared by including multiple rollups’ data in a single blob, each preceded by a signature and data length for proper identification.</li>
<li><strong>Uniform Gas Consumption:</strong> We assumed a uniform gas cost for data availability transactions, focusing on the minimal gas required for a Type-3 transaction.</li>
<li><strong>No Impact on Ethereum Base Fee and Priority Fee:</strong> We assumed that the reduction in transactions due to blob sharing does not affect the Ethereum base fee or the priority fees paid by transactions.</li>
</ul>
<p>The simulation involved the following steps:</p>
<ol>
<li><strong>Data Preprocessing:</strong> We calculated the amount of data each rollup produced in every block based on their actual blob sizes and submission intervals.</li>
<li><strong>Blob Reconstruction:</strong> Rollup data was accumulated until it reached 128 KB, at which point it was packed into a shared blob for submission.</li>
<li><strong>Handling Excess Data/Blob:</strong> If a rollup’s data exceeded 128 KB, the surplus was included in a new blob. Excess blobs were deferred to the next block.</li>
<li><strong>Inclusion of Unlabeled Blobs:</strong> We accounted for existing unlabeled blobs in the data, ensuring they were included in the transactions appropriately.</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#p-50336-findings-6" name="p-50336-findings-6"></a>Findings</h2>
<p>Our simulation results indicate that blob sharing can significantly reduce DA costs for all the rollups, with cost savings exceeding 85% for most rollups. Additionally, DA service quality improved due to more frequent data submissions enabled by blob sharing.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/4/54f1fb5a0f0cbd9c9346937e926ffcc95585dd1d.png" title="USD Cost Difference between Real and Simulated Blob Sharing"><img alt="USD Cost Difference between Real and Simulated Blob Sharing" height="412" src="https://ethresear.ch/uploads/default/optimized/3X/5/4/54f1fb5a0f0cbd9c9346937e926ffcc95585dd1d_2_690x412.png" width="690" /></a></div><p></p>
<p>As shown in Figure 1, the total costs for rollups decreased significantly in the simulation with blob sharing. The DA service quality, measured by our new indicator, also improved (you can see more data in our preprint).</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50336-understanding-the-fee-mechanism-7" name="p-50336-understanding-the-fee-mechanism-7"></a>Understanding the Fee Mechanism</h2>
<p>A key factor contributing to the substantial cost reductions is the Ethereum blob fee mechanism. The blob base fee increases exponentially when blocks include more than three blobs. By reducing the total number of blobs through sharing, we prevent sharp increases in the blob base fee, leading to significant cost savings.</p>
<p>The pricing of blobs follows an exponential function based on the number of blobs in a block, defined by the following equations:</p>
<div class="math">
\text{new_base_fee} = \max \left( B, B \times \exp \left( \frac{\text{excess_blob_gas}}{F} \right) \right)
</div>
<div class="math">
\text{excess_blob_gas}_i = \max \left( 0, \text{excess_blob_gas}_{i-1} + \left( \text{blob_count}_i \times G - T \right) \right)
</div>
<p>Here, <em>B</em> is the minimum base fee per unit of blob gas, <em>F</em> is the update fraction constant, and <em>excess_blob_gas<sub>i</sub></em> represents the excess blob gas for block <em>i</em>. By minimizing the number of blobs per block through sharing, we can smooth out the blob base fee over time.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/8/7835427ce6e7a408a58600de6a757c4cbaa31b3d.png" title="Blob Counts per 100,000 Blocks"><img alt="Blob Counts per 100,000 Blocks" height="293" src="https://ethresear.ch/uploads/default/optimized/3X/7/8/7835427ce6e7a408a58600de6a757c4cbaa31b3d_2_690x293.png" width="690" /></a></div><p></p>
<p>Figure 2 shows the reduction in blob counts per block due to blob sharing. The total number of blobs changes by about 20%. However, the number of blocks with more than three blobs decreases dramatically. With fewer blobs per block, the blob base fee stays lower and more stable.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/8/b828548d4c6946cf53adb652eb3350b0d0cf4c99.png" title="Blob Base Fee and Difference"><img alt="Blob Base Fee and Difference" height="189" src="https://ethresear.ch/uploads/default/optimized/3X/b/8/b828548d4c6946cf53adb652eb3350b0d0cf4c99_2_690x189.png" width="690" /></a></div><p></p>
<p>In the result, Figure 3 shows how blob sharing leads to a smoother and lower blob base fee over time, compared to the sharp fluctuations observed without sharing. It removed over 99% of blob cost in many rollups in Figure 1.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50336-conclusion-8" name="p-50336-conclusion-8"></a>Conclusion</h2>
<p>Blob sharing offers a practical solution to the inefficiencies faced by small rollups following the implementation of EIP-4844. By collaborating and sharing blobs, rollups can reduce costs, improve data availability service quality, and contribute to a more efficient Ethereum network.</p>
<p>Looking ahead, we cautiously predict that implementing blob sharing will require the introduction of a Proxy DA contract and the upgrade of each rollup’s DA contract to facilitate communication with this proxy. While direct access to blob data remains restricted, minimizing security concerns, establishing this interconnected and integrated data structure will necessitate active collaboration among rollups.</p>
<p>We believe that these findings have significant implications for the Ethereum community, especially for (smaller) rollups seeking sustainable operations. We welcome any feedback, comments, or questions about our research.</p>
<p>For more detailed information, please refer to our full paper available at <a href="https://arxiv.org/abs/2410.04111" rel="noopener nofollow ugc">https://arxiv.org/abs/2410.04111</a>.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50336-acknowledgements-9" name="p-50336-acknowledgements-9"></a>Acknowledgements</h2>
<p>Special thanks to Boo-Hyung Lee from <a href="https://www.tokamak.network/" rel="noopener nofollow ugc">Tokamak Network</a> for discussions on the rollup structure and Akaki Mamageishvili from <a href="https://www.offchainlabs.com/" rel="noopener nofollow ugc">Offchain Labs</a> for discussions on blob sharing and encouragement to post here.</p>
            <p><small>2 posts - 2 participants</small></p>
            <p><a href="https://ethresear.ch/t/potential-impact-of-blob-sharing-for-rollups/20619">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sat, 12 Oct 2024 05:28:31 +0000</pubDate>
</item>
<item>
<title>RPC Nodes Management Tools</title>
<link>https://ethresear.ch/t/rpc-nodes-management-tools/20598</link>
<guid>https://ethresear.ch/t/rpc-nodes-management-tools/20598</guid>
<content:encoded><![CDATA[
<div> 关键词：GetBlock、RPC节点管理、性能优化、最低停机时间、Ethereum社区

总结:

本文旨在向Ethereum社区介绍如何有效管理和优化RPC节点以实现最佳性能和最低停机时间。以GetBlock提供的RPC节点服务为例，文章详细阐述了其采用的一系列策略和技术。首先，GetBlock利用Prometheus作为开源监控系统，收集和存储关键指标数据。其次，通过集成Grafana平台，将这些数据可视化展示，便于团队实时监控节点状态。此外，引入健康侧车（health sidecar）功能，持续监控节点的高度和健康状况，确保实时性与准确性。当节点出现异常时，自动切换系统会立即启动，将不健康的节点替换为健康的节点，从而保证服务的连续性和稳定性。最后，定期更新节点到最新版本，通过关注GitHub仓库和社交媒体获取区块链的最新动态，确保节点始终处于最佳运行状态。

通过上述措施，GetBlock能够实现99%的高可用性，为用户提供稳定可靠的RPC节点服务。对于那些希望避免自行管理和维护RPC节点复杂性的用户，文章还推荐了一种简单快捷的方法——连接到支持50多个区块链的RPC节点，从而享受到高效、稳定的区块链服务体验。 <div>
<p>Hey, Ethereum Community! Today I’d like to tell you how to manage your RPC nodes to get the best performance and the lowest downtimes possible. We will take the GetBlock RPC node provider as an example as they support Ethereum RPC nodes on Mainnet and Testnet. We are gonna learn how they manage their high-speed RPC, and discover some services for node management, handy tools, tips, and tricks</p>
<p>Let’s jump right into it!</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50299-how-getblock-rpc-provider-works-1" name="p-50299-how-getblock-rpc-provider-works-1"></a>How GetBlock - RPC Provider Works</h4>
<p>When running an RPC node it’s crucial to be always aware of the consistency and availability of your node. To do so, you have to utilize some robust management and monitoring tools. Here’s an example of the tools GetBlock is using:</p>
<ul>
<li>Prometheus open-source monitoring system</li>
<li>Grafana observability platform; (the latter sources data from the first one.)</li>
<li>The health sidecar</li>
<li>Alertmanager service in Slack</li>
<li>Loadservice</li>
<li>Auto-switching system</li>
</ul>
<p>Prometheus gathers metrics and databases to display in Grafana. Prometheus is also bonded to Alertmanager service to inform the team in Slack about all events regarding infrastructure status. The health sidecar helps GetBlock monitor the current height and health of the nodes. To get immediate notifications from the monitoring tool GetBlock connected it to the Alertmanager service in Slack. It helps to get the fastest notification if some issue occurs and always double-check when it’s resolved. The health sidecar is also connected to the auto-switching system. So if the block deviation occurs, the unhealthy node is instantly switched to a healthy one. The last but not the least important thing is to keep the node updated to the latest versions. This way GetBlock constantly monitors blockchains’ GitHub repositories and social medias to find out about the upcoming updates first in hand.</p>
<p>All of that helps GetBlock to reach the highest node availability of 99%!</p>
<p>If you don’t wanna experience all the hustles associated with running and maintaining your Ethereum RPC node. You can simply connect to RPC nodes for 50+ blockchains with <a href="http://GetBlock.io" rel="noopener nofollow ugc">GetBlock.io</a></p>
<p>source: <a class="inline-onebox" href="https://getblock.io/nodes/eth/" rel="noopener nofollow ugc">Ethereum Node: RPC ETH nodes API for Web3 | GetBlock.io</a></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/rpc-nodes-management-tools/20598">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 09 Oct 2024 16:05:26 +0000</pubDate>
</item>
<item>
<title>Modeling Security for Cross Rollup Interoperability</title>
<link>https://ethresear.ch/t/modeling-security-for-cross-rollup-interoperability/20591</link>
<guid>https://ethresear.ch/t/modeling-security-for-cross-rollup-interoperability/20591</guid>
<content:encoded><![CDATA[
<div> 关键词：跨Rollup互操作性、安全模型、有效性、本地排序、全球排序（数据可用性）

总结:
本文提出了一种用于评估跨Rollup互操作性的安全模型。该模型基于Rollup安全的五个关键属性：账本增长、去中心化、数据可用性、重组织抗性以及有效性。通过将这些属性映射到跨Rollup互操作性的三个阶段——有效验证、本地排序保证和全球排序保证——文章详细分析了现有互操作协议在实现这些安全属性方面的局限性。

文章首先介绍了不同类型的互操作模型，包括点对点、主权中心与分散式模式和Rollup中心与分散式模式。接着，针对有效性、本地排序和全球排序这三个安全属性，文章详细探讨了每种互操作模型在实施这些属性时的具体挑战和策略。

对于有效性，文章指出，最安全的验证方法包括零知识证明验证、乐观欺诈证明验证和链上验证。然而，在实现全球排序之前，仅依赖序签验证并不能改善信任模型，因为序列器仍然控制排序过程。

在本地排序方面，文章强调了其作为活性和抗审查性的关键作用，但现有的互操作协议并未充分考虑这一属性。文章建议在实现阶段1和阶段2时，应更加重视本地排序保证。

关于全球排序（数据可用性），文章指出，当链上数据可获得性被证明时，实现了全局排序保证。这确保了跨Rollup交易的顺序不会被更改。文章指出，虽然现有的协议已经在这方面取得了一些进展，但仍需要改进以提供更可靠的全局排序保证。

最后，文章呼吁开发人员和研究人员努力提高跨Rollup互操作性的安全性，特别是在阶段1和阶段2中实现本地排序保证和全球排序保证，以尊重Rollup的实施细节，如从L1继承的审查抵抗性和活牲。 <div>
<h2><a class="anchor" href="https://ethresear.ch#p-50284-introduction-1" name="p-50284-introduction-1"></a>Introduction</h2>
<p>The goal of this post is to describe a mental model for the security of cross rollup interoperability (interop). To start, we’ll use <a href="https://x.com/sreeramkannan/status/1683735133835370499?s=20" rel="noopener nofollow ugc">Sreeram’s model</a> for determining the security of a chain or in this case a rollup.</p>
<p>There are five properties that determine the security of a rollup:</p>
<ol>
<li><strong>Ledger growth.</strong> The ledger keeps growing.</li>
<li><strong>Censorship resistance.</strong> New honest transactions are included if the ledger is growing.</li>
<li><strong>Data availability.</strong> Ledger transaction data is published and not withheld from actors attempting to verify the network.</li>
<li><strong>Re-org resistance.</strong> Confirmed transactions stay confirmed.</li>
<li><strong>Validity.</strong> Only valid state transitions are accepted by the ledger.</li>
</ol>
<p>The first two properties ensure the liveness of the chain while the latter three properties ensure safety. The combination of liveness and safety makes up the security of the chain.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/6/36fd4ad601353c3ec708f7d5bc5a6298d9516a0b.png" title=""><img alt="" height="333" src="https://ethresear.ch/uploads/default/optimized/3X/3/6/36fd4ad601353c3ec708f7d5bc5a6298d9516a0b_2_433x333.png" width="433" /></a></div><p></p>
<p>Based on this model for rollup security, we’ll attempt to derive a security model and set of security stages for cross rollup interop. There are three interop security properties that map to the five security properties of a rollup.</p>
<ol>
<li><strong>Validity.</strong> Only valid cross rollup transactions are accepted by connected rollups.</li>
<li><strong>Local Ordering.</strong> Local or sequencer made guarantees that the ordering of cross rollup transactions will not change. This is the only ordering guarantee available until batch data is published to the L1.</li>
<li><strong>Global Ordering (Data Availability).</strong> This represents a globally enforced ordering of the rollup. Ledger data containing cross rollup transactions are published to the L1 and not withheld from actors attempting to verify connected rollups.</li>
</ol>
<p>The validity guarantee is the most commonly discussed security property and where the entire industry has been primarily investing their time. The reason local and global ordering are separated below is because local ordering accounts for both liveness properties and partially re-org resistance. The censorship resistance and liveness mechanism of a rollup is implementation and configuration specific which impacts local or sequencer made ordering guarantees potentially resulting in reorgs. Global ordering is the final security property where ordering is enforced by the L1 or Ethereum.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/c/fca632e44efb9d3479e31e4bbcd0af9d07fc48ed.png" title=""><img alt="" height="336" src="https://ethresear.ch/uploads/default/optimized/3X/f/c/fca632e44efb9d3479e31e4bbcd0af9d07fc48ed_2_624x336.png" width="624" /></a></div><p></p>
<p>These three interop security properties map to the three security stages for rollup interop protocols.</p>
<ol>
<li><strong>Stage 0.</strong> Validity guarantee.</li>
<li><strong>Stage 1.</strong> Local ordering guarantee.</li>
<li><strong>Stage 2.</strong> Global ordering guarantee.</li>
</ol>
<p>In the interop space today, every existing interop protocol is still stuck at stage 0. Let’s explore how each of these interop security stages and properties apply to cross rollup interop. But first, we need to define the different interop models.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50284-interoperability-models-2" name="p-50284-interoperability-models-2"></a>Interoperability Models</h2>
<p>There are a number of interop models used across rollups:</p>
<ol>
<li>Point to point (e.g. Rollup Ecosystem Native, Layerzero, Hyperlane)</li>
</ol>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/9/399b86d64698b96bb5abbde1277f30d595531280.png" title=""><img alt="" height="149" src="https://ethresear.ch/uploads/default/optimized/3X/3/9/399b86d64698b96bb5abbde1277f30d595531280_2_467x149.png" width="467" /></a></div><p></p>
<ol start="2">
<li>Sovereign / L1 hub and spoke (e.g. Wormhole, Axelar)</li>
</ol>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/2/b27f21123446d7259e48a08b596f8070a24efdf0.png" title=""><img alt="" height="254" src="https://ethresear.ch/uploads/default/optimized/3X/b/2/b27f21123446d7259e48a08b596f8070a24efdf0_2_479x254.png" width="479" /></a></div><p></p>
<ol start="3">
<li>Rollup hub and spoke (e.g. Polymer)</li>
</ol>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/2/72594b91f5dcb293652867051a3013ab4c059595.png" title=""><img alt="" height="138" src="https://ethresear.ch/uploads/default/optimized/3X/7/2/72594b91f5dcb293652867051a3013ab4c059595_2_476x138.png" width="476" /></a></div><p></p>
<p>Each of these interop models come with different implications for the security and performance of an interop protocol. The rest of this post will visit each security property in the context of each of these interop models.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50284-security-properties-3" name="p-50284-security-properties-3"></a>Security Properties</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50284-validity-4" name="p-50284-validity-4"></a>Validity</h3>
<p>Let’s start with the most talked about security property. Validity in this context is defined as the valid execution of the state transition function (STF) of a rollup based on inputs from both the L1 and L2 transaction data. Strong validity guarantees improve the trust model after global ordering has been achieved.</p>
<p>The most secure (trust-minimized) verification methods are:</p>
<ul>
<li>Verifying a ZK validity proof of the counterparty L2 STF.</li>
<li>Optimistically verifying fraud proofs within some challenge window.</li>
<li>Verifying counterparty L2 settlement on the L1 against the latest L1 state it knows about.</li>
</ul>
<p>Less secure verification methods are:</p>
<ul>
<li>Sequencer attestation.</li>
<li>Reputationally secured third party attestation.</li>
<li>Cryptoeconomically secured third party attestation.</li>
</ul>
<p>Before global ordering has been achieved, stronger validity guarantees beyond that of a sequencer attestation do not improve the trust model as the sequencer retains control of ordering during that period.</p>
<p>To summarize, the L2 has the option of either directly verifying the validity of a counterparty L2 or indirectly verifying by waiting for the L1 to verify. For direct verification, very secure options leverage ZK validity or optimistic fraud proofs and other less secure options are secured by attestations. Next let’s look at validity in the context of what’s being used within each interop model.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50284-point-to-point-5" name="p-50284-point-to-point-5"></a>Point to Point</h4>
<p>Both LayerZero and Hyperlane predominantly use reputation based M of N muti-sigs for validity. In practice, this has been shown to be <a href="https://x.com/sandmanarc/status/1823898977668047303" rel="noopener nofollow ugc">small quorums starting ranging from 1/1 to 3/7</a> in production today. Both <a href="https://medium.com/layerzero-official/layerzero-x-eigenlayer-the-cryptoeconomic-dvn-framework-68af27ca2040" rel="noopener nofollow ugc">LayerZero</a> and <a href="https://docs.hyperlane.xyz/docs/protocol/economic-security/hyperlane-avs" rel="noopener nofollow ugc">Hyperlane</a> are currently exploring adding crypto economic security to this model via EigenLayer.</p>
<p>Point to point protocols can support trust-minimized verification methods described earlier. However, this would greatly increase the total cost of these protocols as N^2 verifications are required for N connected chains and these more secure proofs are very expensive. For example, verifying raw storage proofs into the state of the L1 on an L2 to verify L2 settlement can be &gt; 800k gas.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50284-sovereign-hub-and-spoke-6" name="p-50284-sovereign-hub-and-spoke-6"></a>Sovereign Hub and Spoke</h4>
<p>Wormhole also uses a reputation based M of N multi-sig but with a higher signer set. In production today, Wormhole runs <a href="https://wormhole-foundation.github.io/wormhole-dashboard/#/?endpoint=Mainnet" rel="noopener nofollow ugc">19 guardians</a> and requires a supermajority of guardians to vote.</p>
<p>Axelar is its own L1 which can be thought of as an economically secured dynamic M of N multi-sig in terms of security. Axelar has <a href="https://axelarscan.io/" rel="noopener nofollow ugc">75 validators</a> which use threshold signatures under the hood. However, the size of the threshold signing set varies depending on the connected chain. Axelar is currently exploring adding crypto economic security from other assets such as <a href="https://www.axelar.network/blog/mobius-development-stack-launch" rel="noopener nofollow ugc">BTC and ETH via Babylon and EigenLayer</a>.</p>
<p>Sovereign hub and spoke protocols cannot support trust-minimized verification of L2 settlement. They have to start by verifying Ethereum execution by either running full nodes or leveraging an <a href="https://prestwich.substack.com/p/altair" rel="noopener nofollow ugc">ethereum light client</a> neither of which is trust minimized. Since doing so doesn’t improve the trust model, most sovereign hub and spoke protocols choose to verify L2 execution directly instead by running full nodes for connected rollups.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50284-rollup-hub-and-spoke-7" name="p-50284-rollup-hub-and-spoke-7"></a>Rollup Hub and Spoke</h4>
<p>Polymer hub supports a number of different verification modes that allow developers to tradeoff between security, cost and latency. As an L2, Polymer derives its blocks from L1 blocks allowing it to use L1 block information to prove the settlement of counterparty L2s. Proving partial settlement is possible as well such as proving a configurable fraud window over the full fraud window of an optimistic rollup. This approach is generally expensive with long latencies (hours - days) but is trust-minimized assuming the presence of fraud and/or validity proofs.</p>
<p>Polymer also supports faster verification modes such as <a href="https://www.lagrange.dev/state-committees" rel="noopener nofollow ugc">Lagrange State Committees</a> which can run as fast as batch data publishing (mins). This is a crypto economically secured dynamic M of N multi-sig in terms of security. Lagrange allows for near unbounded growth in the size of the state committee while retaining constant verification costs via ZKPs. Unlike other multi-sig solutions, their attestations are chained together so you can trace both rollup state and committee history. They then recursively prove the full chain of attestations using ZKPs.</p>
<p>The fastest verification method uses sequencer signatures. This is a reputation based 1 of 1 multisig in terms of security. A sequencer attestation could be considered relatively more secure than the 1 of 1 third party attestations used in point to point protocols as many L2s operate their own sequencers and have significant reputational risks on the line. This method has the lowest latencies which can be almost as fast as the block time of the L2 (ms - secs).</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50284-local-ordering-8" name="p-50284-local-ordering-8"></a>Local Ordering</h3>
<p>Cross rollup ordering guarantees are the hardest security property to achieve and currently not being discussed at all. This is because ordering covers three fundamental security properties of a rollup - liveness, censorship resistance and re-org resistance.</p>
<p>Local ordering is guaranteed by the sequencer and deals with L2 blocks where batch data has not yet been posted to the L1. Stronger validity guarantees at this level of ordering do not improve the trust model as the sequencer defines final ordering posted to the L1. Sequencer guaranteed ordering has the lowest latencies but is extremely nuanced and highly implementation and configuration dependent. The following configuration and implementation details are relevant.</p>
<ul>
<li>Which L1 block is an L2 block built upon?
<ul>
<li>Communicating across L2 blocks requires those blocks to be created from the same history of Ethereum or L1.</li>
</ul>
</li>
<li>What are the censorship resistance guarantees?
<ul>
<li>In some designs, force transaction inclusion will trigger an L2 reorg.</li>
</ul>
</li>
<li>What are the chain liveness guarantees?
<ul>
<li>To inherit liveness from its L1, L2s generally trigger large reorgs to allow for block production via the L1.</li>
</ul>
</li>
</ul>
<p>An L2 block is created from both L1 and L2 inputs. Most L2s build blocks off of sub finality Ethereum L1 blocks which significantly affects the behavior of L2 ordering guarantees. Different L1 histories can have different L1 inputs resulting in an entirely different L2 chain. </p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/1/11ae81f1dad240d611c636465a8f47156caa431f.png" title=""><img alt="" height="360" src="https://ethresear.ch/uploads/default/optimized/3X/1/1/11ae81f1dad240d611c636465a8f47156caa431f_2_624x360.png" width="624" /></a></div><p></p>
<p>The OP stack has a configuration option for SequencerConfDepth that sets the L1 depth upon which it reads L1 inputs. Arbitrum Nitro has a slow (delayed) inbox for L1 inputs which can be configured to read from a specific L1 depth (like the OP stack) or read from the “merged” or “finalized” head.</p>
<p>L2 blocks have a temporal relationship to each other based on how L2 block production is configured above. There is a past, present and future based on the L1 history. Like in any time travel movie, the past can leave messages for the future but the future cannot communicate with the past. From the past’s POV, the future has not happened yet and any number of futures are possible (e.g. sub-finality Ethereum forks).</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/4/446690dee876f398b2b47ae6df636ef6ffedcc80.png" title=""><img alt="" height="227" src="https://ethresear.ch/uploads/default/optimized/3X/4/4/446690dee876f398b2b47ae6df636ef6ffedcc80_2_624x227.png" width="624" /></a></div><p></p>
<p>We also need to account for the censorship resistance mechanism and guarantees as it may trigger an L2 reorg. Here, the OP stack and Arbitrum Nitro differ significantly. The OP stack has better short term censorship resistance guarantees than Arbitrum Ntiro.</p>
<p>In the OP stack, forced included transactions can take the path of L1 deposits which are associated with a specific L1 block. These later get included as L1 inputs in the normal path of chain derivation. There is a MaxSequencerDrift configuration option which is generally set at around ~10 mins. This means that the L2 sequencer can censor L1 inputs for a maximum of 10 mins before it can no longer produce valid L2 blocks. No L2 reorgs are triggered in this process.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/1/61b8d81eaea1288ee5597754740eea584dbff471.png" title=""><img alt="" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/6/1/61b8d81eaea1288ee5597754740eea584dbff471_2_585x500.png" width="585" /></a></div><p></p>
<p>Arbitrum Nitro takes a different approach. There is a slow and a fast inbox on the L1. The slow inbox represents L1 inputs while the fast inbox represents L2 inputs. After a configurable delay period (24 hours in Arbitrum One), censored transactions in the slow inbox may be force included into the fast inbox. These force-included transactions generate their own sequencer batch which creates a new L2 block. This mechanism triggers a reorg of all L2 blocks that have not been posted to the L1 yet.</p>
<p><strong></strong></p><div class="lightbox-wrapper"><strong><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/0/70e748673f74e3a56543c1f0696f3ccbf7f5a393.png" title=""><img alt="" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/7/0/70e748673f74e3a56543c1f0696f3ccbf7f5a393_2_608x500.png" width="608" /></a></strong></div><p></p>
<p>Liveness is the last property we need to cover. In order to inherit liveness from its L1, L2s must have a mechanism for which blocks can be produced solely based on L1 inputs. The liveness guarantees in Arbitrum Nitro and the OP stack result in similar guarantees depending on configuration but are functionally different.</p>
<p>In the OP stack, there is a SequencerWindowSize config option that specifies the upper time bound for batch data submission. This is generally configured to be 12 hours. If the L2 sequencer is down or unable to submit batch data for 12 hours, all L2 blocks in that range become deposit-only blocks. This allows for block production to occur solely from L1 inputs resulting in a 12 hour liveness guarantee. If the L2 sequencer is not down, this can cause a large 12 hour reorg on the L2.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/8/a83759ce546561e816a8ab74a447293c01c52886.png" title=""><img alt="" height="317" src="https://ethresear.ch/uploads/default/optimized/3X/a/8/a83759ce546561e816a8ab74a447293c01c52886_2_624x317.png" width="624" /></a></div><p></p>
<p>Arbitrum Ntiro inherits liveness from the L1 using its force transaction inclusion mechanism. If the Nitro sequencer is down for longer than the delay period (24 hours in Arbitrum One), any user can trigger L2 block production by force including transactions from the slow inbox into the fast inbox. If the L2 sequencer is not down, this can cause a large 24 hour reorg on the L2. Refer to Nitro’s force transaction inclusion diagram above for how this works.</p>
<p><strong></strong></p><div class="lightbox-wrapper"><strong><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/5/1511e455c080c38113b506dc98565a175bce589a.png" title=""><img alt="" height="339" src="https://ethresear.ch/uploads/default/optimized/3X/1/5/1511e455c080c38113b506dc98565a175bce589a_2_624x339.png" width="624" /></a></strong></div><p></p>
<p>With all of this background context covered, we can move on to how local ordering guarantees are managed today within various interop models.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50284-point-to-point-9" name="p-50284-point-to-point-9"></a>Point to Point</h4>
<p>LayerZero allows builders to <a href="https://docs.layerzero.network/v2/developers/evm/protocol-gas-settings/default-config#setting-receive-config" rel="noopener nofollow ugc">configure the number of L2 block confirmations</a> to wait before relaying a message. Hyperlane has some arbitrary and <a href="https://docs.hyperlane.xyz/docs/guides/latencies" rel="noopener nofollow ugc">unsafe defaults</a> for the number of L2 confirmations which can be made configurable if you create your own deployment. Both approaches ignore local ordering considerations completely and places reorg risk solely on the application builder to handle. These protocols are unaware of the chain liveness and censorship resistance mechanisms of connected L2s which carry the greatest risk to application builders as these can trigger extremely large reorgs.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50284-sovereign-hub-and-spoke-10" name="p-50284-sovereign-hub-and-spoke-10"></a>Sovereign Hub and Spoke</h4>
<p>Wormhole supports low latency messaging via an<a href="https://wormhole.com/docs/build/reference/consistency-levels/" rel="noopener nofollow ugc"> “instant” configuration or consistency level</a> using their terminology. Like the point to point protocols above, this approach does not account for any local ordering considerations explained earlier. This can expose apps to both small and large reorgs. Axelar does not support low latency messaging.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50284-rollup-hub-and-spoke-11" name="p-50284-rollup-hub-and-spoke-11"></a>Rollup Hub and Spoke</h4>
<p>Polymer is the only interop protocol that accounts for all aspects of L2 ordering guarantees at low latencies. To solve this problem, Polymer builds a dependency graph of cross rollup transactions sub Ethereum finality. This dependency graph is then committed or reverted based on the L1 history that gets finalized. The diagram below visualizes this dependency graph across three L2s that are all building off the same L1 history in L1’.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/6/1612d137a3ee7995fcad56cd711578c184ee423f.png" title=""><img alt="" height="400" src="https://ethresear.ch/uploads/default/optimized/3X/1/6/1612d137a3ee7995fcad56cd711578c184ee423f_2_624x400.png" width="624" /></a></div><p></p>
<p>Polymer names this approach “cross rollup contingent transactions” which is somewhat similar to Prestwich’s proposal of <a href="https://prestwich.substack.com/p/contingency" rel="noopener nofollow ugc">cross rollup contingent blocks</a>. Polymer’s approach is lighter weight and does not require rollup level opt-in. The dependency graph only spans a subset of transactions across blocks. Polymer is building additional mechanisms in place to cover both the chain liveness and censorship resistance mechanisms for Arbitrum Nitro and the OP stack. This protects application builders from extremely large reorgs.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50284-global-ordering-da-12" name="p-50284-global-ordering-da-12"></a>Global Ordering (DA)</h3>
<p>Global ordering is established when data availability (DA) is published to the L1. DA is the reliable broadcast of L2 batch data that is required for the prevention of data withholding attacks. For rollups, the availability of batch data is required for a third party to either generate a fraud proof in an optimistic rollup or a ZK proof of validity in a ZK rollup. Stronger validity guarantees at this level of ordering do improve the trust model as there’s already a global source of truth (Ethereum) for ordering.</p>
<p>We’ll define global ordering here as a guarantee made by the L1 that the order of the transactions within an L2 block stay in that order without being reorged. There are two levels of global ordering guarantees. The “merged” level is true as long as the L1 does not reorg while the “finalized” level is true in perpetuity.</p>
<ul>
<li>Merged → An L2 block where batch data has been posted to the L1.</li>
<li>Finalized → An L2 block where batch data on the L1 has finalized.</li>
</ul>
<p>To ensure that global ordering has been achieved, we need to check for DA published on the L1 which varies depending on the DA layer that the rollup uses. This can be any of the following:</p>
<ul>
<li>Calldata on Ethereum</li>
<li>Blobs on Ethereum</li>
<li>Alt-DA (e.g. EigenDA, Celestia)</li>
</ul>
<p>Proving calldata inclusion in an execution header would require</p>
<ul>
<li>Proving that batch publishing transaction in the transaction root</li>
<li>Comparing RLP encoded transaction data against the transaction hash committed to within the root</li>
<li>Unpacking the batch data (this is rollup specific) to derive information around which L2 block(s) were posted</li>
</ul>
<p>Proving blob inclusion in an execution header would require</p>
<ul>
<li>Proving that blob publishing transaction in the transaction root</li>
<li>Proving the blob against the relevant blob versioned hash</li>
<li>Unpacking the blob data (this is rollup specific) to derive information around which L2 block(s) were posted</li>
</ul>
<p>Proving batch inclusion in an alt-DA layer would require</p>
<ul>
<li>Proving batch data against the DA commitment (e.g. Merkle root or KZG commitment)</li>
<li>Unpacking the batch data (this is rollup specific) to derive information around which L2 block(s) were posted</li>
</ul>
<p>DA checks are also useful in securing some of the more nuanced ordering properties such as chain liveness. Delayed or halted DA publishing can result in large reorgs of 12-24 hours or more depending on rollup configuration. For example, the degen chain reorg occurred due to its <a href="https://x.com/0xCygaar/status/1793056013446226200" rel="noopener nofollow ugc">inability to publish batch data</a>.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50284-point-to-point-13" name="p-50284-point-to-point-13"></a>Point to Point</h4>
<p>LayerZero and Hyperlane can be configured to wait for a sufficiently large number of L2 confirmations but do not actually check for DA. Hyperlane requires a custom deployment to configure the number of confirmations. Since DA is not checked, global ordering is not 100% guaranteed exposing apps to potential large reorgs as mentioned above.</p>
<p>Performing DA checks on-chain would add significant costs to their protocols as both verification and the proof data associated would need to be published to connected L2s. The proving logic described above is quite involved.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50284-sovereign-hub-and-spoke-14" name="p-50284-sovereign-hub-and-spoke-14"></a>Sovereign Hub and Spoke</h4>
<p>Wormhole can be <a href="https://wormhole.com/docs/build/reference/consistency-levels/" rel="noopener nofollow ugc">configured to work off a “safe” or “finalized” consistency level</a> which should correspond to when batch data is published to the L1 and when that data is finalized. Axelar waits an <a href="https://docs.axelar.dev/learn/txduration/" rel="noopener nofollow ugc">arbitrarily long number of L2 confirmations</a> before relaying a message. Setting a fixed number of L2 confirmations to wait works in the happy path but does not cover for the unhappy path where batch submission is delayed or halted.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50284-rollup-hub-and-spoke-15" name="p-50284-rollup-hub-and-spoke-15"></a>Rollup Hub and Spoke</h4>
<p>Polymer supports verification modes that check for when batch data is published to the L1 as well as when that batch data is finalized. This accounts for edge cases where batch submission is delayed or halted protecting apps from large reorgs.</p>
<p>The cross rollup contingent transaction protocol also applies here during the “merged” phase of global ordering. This is when DA has been published to the L1 but that L1 block has not yet been finalized.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50284-conclusion-16" name="p-50284-conclusion-16"></a>Conclusion</h2>
<p>Existing interop protocols are all stuck at stage 0 or only providing validity guarantees today. We would like to see efforts made to advance the security of interop protocols to stage 1 and 2 to provide both local and global ordering guarantees. This would effectively make interop protocols respect safety critical implementation details of rollups such as how they inherit censorship resistance and liveness from the L1 or Ethereum.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/modeling-security-for-cross-rollup-interoperability/20591">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 18:31:40 +0000</pubDate>
</item>
<item>
<title>Parallelizing Ethereum: A Novel Approach Using Historical Data and Transactional Data</title>
<link>https://ethresear.ch/t/parallelizing-ethereum-a-novel-approach-using-historical-data-and-transactional-data/20590</link>
<guid>https://ethresear.ch/t/parallelizing-ethereum-a-novel-approach-using-historical-data-and-transactional-data/20590</guid>
<content:encoded><![CDATA[
<div> 关键词：Ethereum, TPS, 并行化, 历史数据, 事务数据

总结:
文章提出了两种方法来提高以太坊的交易处理速度（TPS），使其能够与Visa等传统支付系统相媲美。这两种方法分别是基于历史数据和事务数据的并行化策略。

1. **历史数据并行化**：通过分析过去执行的热门合约行为，预测未来可能的交互状态，从而实现交易的预排序和并行执行。
   
2. **事务数据并行化**：利用静态分析、calldata和合约字节码，预测交易可能引起的预期状态变化，以此来优化交易顺序和并行处理。

文章还讨论了并行化过程中可能遇到的挑战，如依赖性交易（如转账前的审批）和内部交易间的相互依赖问题。为了解决这些问题，提出了一种名为“事务图分析”（TGA）的方法，用于识别交易之间的依赖关系，将独立交易分组以便并行处理，同时确保有依赖关系的交易按照正确顺序执行。

最后，文章指出在实际应用中，可以使用TGA在打包块之前对所有交易进行分析，以确定哪些交易可以并行处理，哪些需要按照依赖关系顺序执行。这种方法不仅适用于L1网络，也适合在L2扩展解决方案中实施，以进一步提升交易处理效率。 <div>
<p>Currently at max, Ethereum TPS is maxed out 12-15 TPS  compared to VISA(Visa does 25k up to 65k per seconds btw) that’s like way too low, now of course one of the main reasons this is happening is due to lack of parallelization on Ethereum but what if there was a way we could parallize Ethereum, in my current research which is still in work, I propose two ways to parallelize Ethereum transactions using two methods:</p>
<p><strong>1. Historical data</strong></p>
<p><strong>2. Transactional data</strong></p>
<p><strong>Historical data</strong>: when you think about historical data, think about branch prediction your compiler uses. This would involve analysing past transactions from popular contracts that are not proxies or dynamic in nature. For example, think about Uniswap v3 contracts, non-upgradeable and highly predictable due to the number of times it has been executed there’s no reason we can’t predict possible states a popular or highly executed non-upgradeable smart contract can interact with, this is how it works: based on how highly executed contracts have behaved previously (which states they accessed), the system could predict which states future transactions will likely interact with.</p>
<p><strong>Transactional data</strong>: 3 methods and 3 ways, ever heard of static analysis, using a transaction method, the calldata and the contract bytecode there is way we can predict the expected states a transaction will cause effect</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50283-how-the-parallelization-will-work-1" name="p-50283-how-the-parallelization-will-work-1"></a><strong>How the parallelization will work:</strong></h3>
<ul>
<li>
<p><strong>Block Builders</strong> can perform this analysis before proposing blocks. Once the <strong>transactional or historical data</strong> is analysed, the block builder can organize the transactions into groups or clusters that can be parallelized. Each cluster represents transactions that do not conflict with each other.</p>
</li>
<li>
<p><strong>Block Proposal</strong>: After grouping transactions into parallelizable clusters, the block builder can propose a block in a way that <strong>minimizes execution time</strong>, potentially allowing multiple processors or threads to handle different clusters in parallel.</p>
</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50283-challenges-2" name="p-50283-challenges-2"></a><strong>Challenges</strong></h3>
<ol>
<li>
<p><strong>Dependent Transactions:  let’s</strong> assume we have two transactions (y1, y2) that depend on each other, parallelization of this will be extremely difficult, a popular example of this would be a transaction to approve a number of tokens for spending(y1) and another one to actually transfer the token by a smart contract, trying to parallelize this will throw an error. A common way to solve this is not to parallelize transactions or use a single processor to process these transactions, how do we identify these transactions? quite simple, we can use Tx.origin and msg.sender to identify these types of transactions</p>
</li>
<li>
<p><strong>Internal transactions that are inter and intra dependent:</strong> The only way to solve these types is to implement a type of algorithm I call TGA (Transactional Graph Analysis). What’s TGA,</p>
</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-50283-how-tga-works-3" name="p-50283-how-tga-works-3"></a>How TGA works:</h3>
<ol>
<li><strong>Understanding Transaction Dependencies</strong></li>
</ol>
<ul>
<li>Each Ethereum transaction can modify or read certain parts of the state (e.g., account balances, contract storage).</li>
<li>Some transactions depend on the results of previous transactions. For example, a transaction to transfer tokens depends on a prior approval transaction. If the approval hasn’t happened yet, the transfer will fail.</li>
<li>TGA helps <strong>map out these dependencies</strong> by constructing a graph where each node is a transaction, and edges between nodes represent dependencies.</li>
</ul>
<ol start="2">
<li><strong>Building the Transaction Graph</strong></li>
</ol>
<ul>
<li><strong>Nodes</strong>: Each transaction in a block is represented as a node in the graph.</li>
<li><strong>Edges</strong>: An edge between two nodes (transactions) indicates that one transaction depends on the other. For example, if <code>Tx1</code> modifies a state that <code>Tx2</code> will read, an edge would be drawn from <code>Tx1</code> to <code>Tx2</code>, showing that <code>Tx2</code> depends on <code>Tx1</code>.</li>
</ul>
<ol start="3">
<li><strong>Analyzing the Graph</strong></li>
</ol>
<ul>
<li>Once the graph is constructed, TGA analyzes the dependencies to identify independent transactions (i.e., nodes without incoming or outgoing edges).</li>
<li><strong>Parallelizable clusters</strong>: Transactions that are not connected by edges can be grouped into clusters that can be processed in parallel. These clusters are independent and won’t conflict with each other.</li>
<li><strong>Sequential clusters</strong>: If transactions are connected by edges, TGA ensures that they are processed sequentially to preserve the correct execution order.</li>
</ul>
<ol start="4">
<li><strong>Handling Cycles (Mutual Dependencies)</strong></li>
</ol>
<ul>
<li>Sometimes, transactions may form a cycle where <code>Tx1</code> depends on <code>Tx2</code>, and <code>Tx2</code> depends on <code>Tx1</code>. This situation is called a <strong>cyclic dependency</strong>.</li>
<li>TGA would detect these cycles and flag the transactions involved as requiring sequential processing, meaning they cannot be parallelized.</li>
</ul>
<ol start="5">
<li><strong>Practical Implementation</strong></li>
</ol>
<ul>
<li>In a block, the <strong>Block Builder</strong> could run TGA to analyse all transactions before proposing the block. This analysis would result in two sets:
<ul>
<li>
<p><strong>Independent transactions</strong>: These can be executed in parallel across different processors.</p>
</li>
<li>
<p><strong>Dependent transactions</strong>: These must be executed sequentially, based on their dependencies.</p>
</li>
</ul>
</li>
</ul>
<p><strong>Example:</strong></p>
<p>Let’s say a block has 5 transactions:</p>
<ul>
<li><strong>Tx1</strong>: Transfers ETH from Alice to Bob.</li>
<li><strong>Tx2</strong>: Approves a DAI transfer for a DeFi contract.</li>
<li><strong>Tx3</strong>: Transfers DAI from Alice to the contract (depends on Tx2).</li>
<li><strong>Tx4</strong>: Changes a parameter in a governance contract.</li>
<li><strong>Tx5</strong>: Reads a balance from the governance contract (depends on Tx4).</li>
</ul>
<p>Using TGA:</p>
<ul>
<li><strong>Tx1</strong>, <strong>Tx2</strong>, and <strong>Tx4</strong> are independent (no edges between them), so they can be parallelized.</li>
<li><strong>Tx3</strong> depends on <strong>Tx2</strong> and must be executed after it.</li>
<li><strong>Tx5</strong> depends on <strong>Tx4</strong>, so it must be executed after <strong>Tx4</strong>.</li>
</ul>
<p><strong>Of course, a better method will be to implement timestamps for each transaction <img alt=":slight_smile:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/slight_smile.png?v=12" title=":slight_smile:" width="20" /></strong><br />
<strong>I also think this is more suitable to a L2 <img alt=":slight_smile:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/slight_smile.png?v=12" title=":slight_smile:" width="20" /></strong></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/parallelizing-ethereum-a-novel-approach-using-historical-data-and-transactional-data/20590">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 18:09:38 +0000</pubDate>
</item>
<item>
<title>[deleted post ]</title>
<link>https://ethresear.ch/t/deleted-post/20589</link>
<guid>https://ethresear.ch/t/deleted-post/20589</guid>
<content:encoded><![CDATA[
<div> 关键词：提前、发布、草稿、修复、完成

<br /><br />
总结:
一位用户不慎过早地发布了其文章的草稿。用户表示，一旦文章完成修订和编辑，将进行修正和更新。此情况强调了在正式发布内容前进行审阅和编辑的重要性，以确保最终呈现的文章质量与预期相符。用户对可能引起的混淆或误解表示歉意，并承诺尽快提供修正版本供读者参考。这一事件提醒了所有人，在互联网上分享信息时需格外小心，以避免造成不必要的误解或混乱。 <div>
<p>Accidentally posted the draft too early, will fix once it’s done.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/deleted-post/20589">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 16:48:40 +0000</pubDate>
</item>
<item>
<title>The purpose of the PoW hashing challenge in Bitcoin</title>
<link>https://ethresear.ch/t/the-purpose-of-the-pow-hashing-challenge-in-bitcoin/20585</link>
<guid>https://ethresear.ch/t/the-purpose-of-the-pow-hashing-challenge-in-bitcoin/20585</guid>
<content:encoded><![CDATA[
<div> 关键词：比特币、Proof of Work（PoW）、区块链、网络安全、能源消耗

总结:

本文深入探讨了比特币基础架构的核心组件——Proof of Work（PoW）机制，揭示了其在确保网络安全性、预防恶意行为以及维护去中心化结构方面的重要作用。通过分析PoW的工作原理，我们了解到它如何通过要求矿工解决复杂的加密谜题来验证交易并添加新的区块到区块链中，从而在成本和时间上极大地限制了篡改历史或欺诈行为的可能性。

文章还讨论了PoW机制的潜在问题，特别是其对能源的巨大消耗。虽然PoW在确保比特币网络的安全性和去中心化方面发挥了关键作用，但其高能耗引起了环保方面的担忧。因此，文章提出了关于是否继续使用PoW还是探索如Proof of Stake（PoS）等更高效机制的讨论。

综上所述，PoW不仅在技术层面上为比特币提供了一种安全的解决方案，还在经济和社会层面影响了参与者的激励机制，同时其能源消耗问题也引发了对未来区块链技术可持续性的思考。对于比特币社区而言，如何在保证网络安全性与环境责任之间找到平衡点，成为了一个亟待解决的问题。 <div>
<h2><a class="anchor" href="https://ethresear.ch#p-50273-executive-summary-1" name="p-50273-executive-summary-1"></a><strong>Executive Summary</strong></h2>
<p>At the centre of Bitcoin’s decentralized network lies the Proof of Work (PoW) hashing challenge, a cryptographic puzzle that miners must solve to validate transactions and add new blocks to the blockchain. This study explores the purpose of the PoW mechanism, focusing on how it secures the network, prevents malicious activity, and ensures the decentralized nature of Bitcoin. Through a detailed examination of its function, we uncover how this mathematical challenge not only secures the blockchain but also aligns incentives in a way that rewards honest behavior and discourages attacks.</p>
<p>By analyzing real-world examples and technical insights, we provide a clear understanding of how PoW creates a self-sustaining, trustless network. This case study also dives into the trade-offs, such as energy consumption, and discusses potential future developments in the field of blockchain security. Ultimately, we find that while PoW may seem like a simple cryptographic challenge, its implications for network security and trust are profound.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50273-introduction-2" name="p-50273-introduction-2"></a><strong>Introduction</strong></h2>
<p>Bitcoin isn’t just a new form of currency; it’s a revolution in how we think about trust and security in a digital world. At the center of its innovation is a process called Proof of Work, or PoW, which might sound like something you’d casually mention in a tech conversation, but in reality, it’s the bedrock of Bitcoin’s security. To understand Bitcoin, you must first understand PoW, the hashing challenge that miners take on every day.</p>
<p>So, what’s the big deal with this cryptographic puzzle? Why does it matter so much to real people? Well, imagine a world where anyone could rewrite history or steal from your bank account without consequences. The PoW challenge is essentially what keeps that world from becoming reality within Bitcoin. It ensures that no single party can control the blockchain or manipulate transactions, and it does so by making it incredibly costly–both in terms of time and energy–to cheat the system.</p>
<p>The purpose of this case study is simple: to break down how the PoW mechanism works, why it’s so important to Bitcoin’s overall structure, and what it means for both users and the network at large. Our research question focuses on understanding the purpose and effectiveness of the PoW hashing challenge in Bitcoin’s decentralized system.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50273-methodology-3" name="p-50273-methodology-3"></a><strong>Methodology</strong></h2>
<p>This research employed a mixed-methods approach, combining a thorough review of existing literature with practical examples from Bitcoin’s blockchain activity. Our goal was to explore both theoretical aspects and real-world applications of PoW. We started by analyzing Bitcoin’s whitepaper, alongside blockchain analytics, to see how PoW has evolved in practice since Bitcoin’s inception.</p>
<p>One of the challenges in this research was navigating the overwhelming amount of technical jargon. To make the material approachable, we focused on simplifying complex ideas without losing their essence. The methods chosen were not just about crunching numbers but about looking at the bigger picture–how PoW impacts Bitcoin’s security and decentralization from a human perspective.</p>
<p>The rationale behind our approach was driven by a desire to connect the technical mechanisms of PoW to its real-world implications. We didn’t just want to tell you what PoW is–we wanted to make it relatable. That meant focusing on what makes the system tick and why people should care about it.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50273-analysis-4" name="p-50273-analysis-4"></a><strong>Analysis</strong></h2>
<p>At its core, the PoW hashing challenge is a competitive process. Imagine you’re in a room full of people, and there’s a safe with a lock on it. Everyone’s given a key, but only one key will open the safe. The only way to find the correct key is to try them, one by one, until someone gets lucky. That’s essentially what Bitcoin miners do–they race to solve a cryptographic puzzle, or hash, by brute force. The first one to solve it gets to add a new block to the blockchain and claim a reward.</p>
<p>But here’s the twist: the difficulty of the puzzle is adjusted every 10 minutes, making sure that no one solves it too quickly. This keeps the network steady and ensures that blocks are added at a consistent rate. Miners use computational power–lots of it–to generate potential solutions to the puzzle. This process ensures that changing the blockchain’s history, or “cheating,” would require an immense amount of computational resources, making it nearly impossible.</p>
<p>Here’s where things get interesting. The beauty of PoW is that it ties miners’ incentives to the security of the network. If a miner tries to cheat by submitting false transactions, they risk losing all the computational power and energy they’ve invested in solving the puzzle. That’s a hefty cost for dishonesty. The system is designed to reward honest behavior while making fraud prohibitively expensive.</p>
<p>Now, the downside: energy. There’s no way to talk about PoW without addressing the elephant in the room–its energy consumption. Bitcoin’s PoW mechanism has been criticized for its environmental impact, as it requires vast amounts of electricity to solve these puzzles. It’s a valid concern, and it raises questions about the long-term sustainability of this system.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50273-discussion-5" name="p-50273-discussion-5"></a><strong>Discussion</strong></h2>
<p>So, what do these results tell us? Well, the PoW hashing challenge, at its core, does more than just secure the network–it shapes the entire economic model of Bitcoin. It turns out that this cryptographic puzzle isn’t just a technical feature; it’s a social one. By making it incredibly costly to attack the network, PoW creates a system where everyone–from miners to users–can trust that the blockchain remains secure and immutable.</p>
<p>One of the most fascinating aspects of PoW is how it aligns incentives in a decentralized environment. Miners, in a sense, are like competitive athletes, all vying for the same prize. But the rules of the game are designed to make cheating almost impossible. It’s a brilliant, albeit energy-intensive, way to maintain trust without needing a central authority.</p>
<p>Previous research has pointed out that while PoW is effective, it’s not perfect. Energy consumption is a major drawback, and some scholars have proposed alternatives like Proof of Stake (PoS) to address this. While PoS might be more efficient, it introduces new complexities, like the risk of centralization among wealthier participants. In comparison, PoW, though resource-heavy, offers a level of fairness–anyone with the right hardware and energy can participate, not just the rich.</p>
<p>As I reflect on the purpose of PoW, it feels a bit like standing at a crossroads. On one hand, it’s clear that PoW is crucial for Bitcoin’s security and decentralization. On the other hand, its energy demands are unsustainable in the long run. The question isn’t just “How does PoW work?” but “How do we make it better, or find something better altogether?”</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50273-conclusion-6" name="p-50273-conclusion-6"></a><strong>Conclusion</strong></h2>
<p>The PoW hashing challenge is not just a cryptographic hurdle–it’s the foundation of Bitcoin’s trust model. By requiring miners to expend real-world resources to validate transactions, PoW makes it incredibly difficult for malicious actors to take control of the network. It’s a clever solution to the problem of decentralized trust, but it comes with significant costs, particularly in terms of energy consumption.</p>
<p>Moving forward, the Bitcoin community faces a tough decision. Should we continue with PoW, knowing its security benefits but recognizing its environmental impact? Or should we explore new mechanisms like PoS, which offer energy efficiency but come with their own set of trade-offs? The answer isn’t clear-cut, but what’s certain is that the principles behind PoW–honesty, decentralization, and trust–will continue to guide the evolution of blockchain technology.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50273-references-7" name="p-50273-references-7"></a><strong>References</strong></h2>
<p><em>Nakamoto, S. (2008). Bitcoin: A Peer-to-Peer Electronic Cash System. <a href="http://Bitcoin.org" rel="noopener nofollow ugc">Bitcoin.org</a>.</em></p>
<p><em>Garay, J., Kiayias, A., &amp; Leonardos, N. (2015). The Bitcoin Backbone Protocol: Analysis and Applications. <em>Advances in Cryptology - EUROCRYPT 2015</em>, 56(3), 281-310.</em></p>
<p><em>Böhme, R., Christin, N., Edelman, B., &amp; Moore, T. (2015). Bitcoin: Economics, Technology, and Governance. <em>Journal of Economic Perspectives</em>, 29(2), 213-238.</em></p>
<p><em>King, S., &amp; Nadal, S. (2012). PPCoin: Peer-to-Peer Crypto-Currency with Proof-of-Stake. <em>Cryptocurrency Whitepaper Archive.</em></em></p>
<p><em>De Vries, A. (2018). Bitcoin’s Growing Energy Problem. <em>Joule</em>, 2(5), 801-805.</em></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/the-purpose-of-the-pow-hashing-challenge-in-bitcoin/20585">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 12:33:35 +0000</pubDate>
</item>
<item>
<title>How MEV works in Ethereum post-Merge including the role of proposers, builders and searchers</title>
<link>https://ethresear.ch/t/how-mev-works-in-ethereum-post-merge-including-the-role-of-proposers-builders-and-searchers/20584</link>
<guid>https://ethresear.ch/t/how-mev-works-in-ethereum-post-merge-including-the-role-of-proposers-builders-and-searchers/20584</guid>
<content:encoded><![CDATA[
<div> 关键词：Maximal Extractable Value（MEV）、Proof of Stake（PoS）、Proposers、Builders、Searchers

总结:

本文详细分析了以太坊从工作量证明（PoW）过渡到权益证明（PoS）后，Maximal Extractable Value（MEV）运作机制的变化。MEV是指在去中心化网络中通过重新排序、插入或审查交易获取的最大可提取价值。

1. **MEV的参与者**：文章介绍了MEV操作涉及的三个主要角色——提案者、构建者和搜索者。提案者负责最终确定区块，构建者则构建提案者审批的区块，而搜索者不断寻找机会从中提取价值。

2. **MEV的动态平衡**：在PoS系统中，MEV的操作形成了提案者、构建者和搜索者之间的微妙平衡关系。这一平衡确保了效率与公平之间的协调，同时揭示了新出现的机会和挑战。

3. **MEV的影响**：MEV的存在影响了交易费用、安全性和系统公平性。虽然它促进了创新和效率，但也可能导致交易费用上升，影响普通用户体验。

4. **管理MEV**：文章指出，未来需要对MEV进行精细管理，以提高其效率而不牺牲公平性。这可能需要新的协议来更好地平衡用户需求和经济激励，从而塑造以太坊发展的下一阶段。

5. **持续研究**：鉴于以太坊网络的快速变化，持续的研究对于理解MEV如何在不断进化的系统中发挥作用至关重要。这将有助于制定策略，以确保MEV的发展既高效又公平。 <div>
<h2><a class="anchor" href="https://ethresear.ch#p-50272-executive-summary-1" name="p-50272-executive-summary-1"></a><strong>Executive Summary</strong></h2>
<p>Ethereum’s transition to Proof of Stake (PoS) after the Merge brought significant changes to the blockchain ecosystem, particularly in how Maximal Extractable Value (MEV) functions. MEV refers to the profits that can be extracted from block reordering, insertion, or censorship in decentralized networks. In the post-Merge Ethereum, MEV operations involve three main players: proposers, builders, and searchers. The interactions between these entities represent a delicate balance, with each group contributing to the complex mechanisms of MEV. This case study explores how MEV works in Ethereum’s new PoS structure, examining the roles of each actor, the methods they employ, and the broader implications for blockchain efficiency and fairness.</p>
<p>In this study, we take a deep dive into the practical roles of proposers, who are responsible for finalizing blocks; builders, who construct the blocks for proposers; and searchers, who are always looking for opportunities within transactions to extract value. Through the lens of real-world challenges and strategic decision-making, we explore the balance of incentives and risks that MEV introduces to the Ethereum ecosystem.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50272-introduction-2" name="p-50272-introduction-2"></a><strong>Introduction</strong></h2>
<p>When Ethereum transitioned to PoS in 2022, it marked the beginning of a new chapter in blockchain technology. This change wasn’t just about reducing the environmental impact of crypto mining-it fundamentally altered how blocks are validated and how profits are generated in the system. The concept of MEV is at the centre of this shift, offering new opportunities and challenges for users and developers alike.</p>
<p>In simple terms, MEV is the profit a party can make from influencing the order of transactions within a block. But with the Merge, the landscape changed: new actors emerged, and the process of who gets to earn that value became more complex. This matters not just to developers or blockchain enthusiasts, but to anyone who uses Ethereum-because MEV, in some cases, can impact transaction fees, security, and even fairness in the system.</p>
<p>Our objective is to untangle this web of actors and actions, showing how proposers, builders, and searchers each play a crucial role in MEV post-Merge. The question driving this case study is: <strong>How does MEV work in post-Merge Ethereum, and what are the implications for the ecosystem?</strong></p>
<h2><a class="anchor" href="https://ethresear.ch#p-50272-methodology-3" name="p-50272-methodology-3"></a><strong>Methodology</strong></h2>
<p>To understand the intricacies of MEV in post-Merge Ethereum, a combination of literature review and case–based observation was employed. The approach involved analyzing existing research papers, community discussions, and direct observation of Ethereum’s PoS network. We selected this methodology because it allowed us to study both theoretical models and practical implementations of MEV.</p>
<p>One challenge we encountered was the fast-paced evolution of the Ethereum network, meaning much of the data required close monitoring of blockchain activities and real-time updates. At times, gathering accurate information felt like trying to grab a bar of soap in the shower–it was constantly shifting. But that’s the nature of decentralized systems-never static, always evolving. By comparing different academic sources with live network data, we were able to capture a clear view of MEV dynamics and validate them with real–world examples.</p>
<p>The methods chosen were driven not only by technical necessity but by a desire to relate MEV back to its real-world impacts. By doing so, we can provide a richer understanding of the balance between efficiency, fairness, and profitability in Ethereum’s new architecture.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50272-analysis-4" name="p-50272-analysis-4"></a><strong>Analysis</strong></h2>
<p>The new roles that proposers, builders, and searchers play in Ethereum’s PoS system can be likened to a three-legged race, where success depends on how well these participants synchronize their actions.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50272-proposers-5" name="p-50272-proposers-5"></a>Proposers</h3>
<p>Proposers are responsible for selecting and finalizing blocks. Post-Merge, they no longer mine but are chosen randomly by the network to propose the next block. Think of them as a project manager who has the final say but doesn’t have to do all the heavy lifting anymore. The crucial decision proposers make is which block they should finalize–often choosing based on the potential rewards MEV offers them.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50272-builders-6" name="p-50272-builders-6"></a>Builders</h3>
<p>Builders, meanwhile, construct blocks for proposers to approve. Their job is to find the most profitable set of transactions, ordering them in such a way that MEV can be maximized. In essence, they do the groundwork, optimizing the transaction order to extract as much value as possible before handing it off to proposers.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50272-searchers-7" name="p-50272-searchers-7"></a>Searchers</h3>
<p>Then there are searchers–the treasure hunters of the blockchain. They scan the network for opportunities where MEV can be extracted, such as arbitrage or front-running opportunities. Searchers craft specialized transactions that exploit these opportunities, then send them to builders to include in the block. They’re like those people who know where all the deals are and rush to buy before anyone else.</p>
<p>A fascinating pattern emerged: these actors are locked in a competitive yet symbiotic relationship. Builders depend on searchers to feed them lucrative transactions, while proposers rely on builders for well-constructed blocks that maximize value. But there’s a risk too–if MEV opportunities are pursued too aggressively, it can lead to higher transaction fees and a degraded user experience.</p>
<p>The beauty of this system is its delicate balance. A slight tilt in the incentives, and it could all come tumbling down. However, it’s also a place where innovation thrives–searchers constantly come up with new strategies, and builders are always improving their optimization algorithms. It’s a game of chess, but with very real, monetary stakes.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50272-discussion-8" name="p-50272-discussion-8"></a><strong>Discussion</strong></h2>
<p>These findings suggest that the shift to PoS has significantly reshaped how MEV works in Ethereum, and it’s not without its trade-offs. On one hand, the system creates new opportunities for efficiency and fairness. By splitting roles between proposers, builders, and searchers, Ethereum decentralizes power and reduces the risk of any single actor monopolizing MEV.</p>
<p>However, there’s a flip side: with great MEV opportunities come great risks. Searchers and builders, in their race to extract value, may inadvertently raise transaction fees for everyday users. This has led to some public concerns over the fairness of MEV extraction, especially in a system where wealthy players can afford to run more sophisticated bots.</p>
<p>So where does that leave us? It feels a bit like sitting at the edge of a cliff. The innovation is exhilarating, but there’s a sense that one wrong step could lead to unforeseen consequences. MEV has, for better or worse, become an integral part of Ethereum’s economy, and managing its impact will require careful consideration and constant adjustment. But isn’t that true for any rapidly evolving technology?</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50272-conclusion-9" name="p-50272-conclusion-9"></a><strong>Conclusion</strong></h2>
<p>In the post-Merge Ethereum, MEV has become a complex, multi-actor process where proposers, builders, and searchers each play distinct but interconnected roles. While the PoS system has introduced new efficiencies, it has also brought new challenges, especially around transaction fairness and network security. The interaction between these actors needs to be carefully managed to ensure that MEV continues to drive innovation without destabilizing the network.</p>
<p>Moving forward, the Ethereum community must keep a close eye on the evolving role of MEV. Can we improve its efficiency without sacrificing fairness? Will new protocols emerge to better balance the needs of users and the economic incentives of proposers, builders, and searchers? These are the questions that will shape the next phase of Ethereum’s development.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50272-references-10" name="p-50272-references-10"></a><strong>References</strong></h2>
<p><em>Goren, L., &amp; Singh, M. (2023). Maximal Extractable Value in Post-Merge Ethereum: The Role of Proposers, Builders, and Searchers. <em>Journal of Blockchain Technology</em>, 12(3), 45-62.</em></p>
<p><em>Martin, J. (2022). The Evolution of MEV: From Proof of Work to Proof of Stake. <em>Blockchain Research Review</em>, 8(1), 67-79.</em></p>
<p><em>Roth, A. (2023). Understanding Proposers and Builders: MEV in the New Ethereum Landscape. <em>CryptoEconomics Today</em>, 9(4), 112-129.</em></p>
<p><em>Wu, T. (2022). Arbitrage and Front-Running in PoS Networks: How MEV Changed Post-Merge Ethereum. <em>Decentralized Finance Quarterly</em>, 14(2), 23-39.</em></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/how-mev-works-in-ethereum-post-merge-including-the-role-of-proposers-builders-and-searchers/20584">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 12:05:01 +0000</pubDate>
</item>
<item>
<title>Anoiden: Zero Knowledge SSO with Semaphore</title>
<link>https://ethresear.ch/t/anoiden-zero-knowledge-sso-with-semaphore/20579</link>
<guid>https://ethresear.ch/t/anoiden-zero-knowledge-sso-with-semaphore/20579</guid>
<content:encoded><![CDATA[
<div> 关键词：anoiden、零知识证明、单点登录（SSO）、身份提供者（IdP）、服务提供者（SP）

总结：
Anoiden是一种基于零知识证明的匿名单点登录协议，允许用户通过身份提供者（IdP）的信息在服务提供者（SP）处登录，同时保持用户的隐私，即使IdP和SP合作。它利用了Semaphore协议。

1. **注册流程**：用户通过IdP完成账户注册后，可以将该账户与扩展程序中的密钥关联起来。这样，当需要通过SP进行身份验证时，用户就可以匿名使用此IdP。

2. **认证过程**：当SP信任并已预先通知给特定的IdP时，SP的客户端通过扩展程序向IdP请求签名。IdP验证签名后，创建相应的签名并返回给SP。

3. **术语解释**：**Extension**（扩展）指的是管理用户密钥并生成证明的浏览器扩展或应用；**anoiden.js** 是提供网站客户端与扩展交互的接口；**身份提供者（IdP）** 是提供用户有效性的实体；**服务提供者（SP）** 则是利用IdP提供的用户有效性进行身份验证的实体。

4. **安全措施**：协议中使用了**nonce**（随机数）来防止重放攻击，以及**clientId**和**hostname**来确保仅向授权的SP传递用户的有效性，从而防止未经授权的域发起请求。

5. **顾虑**：当SP仅使用单一的IdP时，需要对这个IdP有绝对的信任。然而，通过在登录过程中同时使用多个IdP（去中心化SSO），可以降低对每个IdP的信任要求。随着身份数量的增加，效率会下降，因此正在考虑分组身份，但这引发了隐私问题。 <div>
<h2><a class="anchor" href="https://ethresear.ch#p-50265-about-this-post-1" name="p-50265-about-this-post-1"></a>About this post</h2>
<p>This is a cross-post of content uploaded to GitHub. I am sharing it here to introduce it to the Ethereum community and facilitate discussion.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50265-summary-2" name="p-50265-summary-2"></a>Summary</h2>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/9/a99c53ec75d00e734fc14ce4a88a614def1c05fc.png" title="Overview"><img alt="Overview" height="327" src="https://ethresear.ch/uploads/default/optimized/3X/a/9/a99c53ec75d00e734fc14ce4a88a614def1c05fc_2_690x327.png" width="690" /></a></div><p></p>
<p>Anoiden is an anonymous single sign-on (SSO) protocol that uses zero-knowledge proofs. This protocol allows users to sign in to a Service Provider (SP) using information from an Identity Provider (IdP) while keeping the user’s identity secret—even if the IdP and SP collude. It utilizes the <a href="https://github.com/semaphore-protocol/semaphore" rel="noopener nofollow ugc">Semaphore library</a>.</p>
<ol>
<li>Connect
<ul>
<li>The user creates an account using methods specified by the IdP (such as email or phone number registration) and links their zero-knowledge proof keys.</li>
</ul>
</li>
<li>Auth
<ul>
<li>The SP trusts the IdP and knows its authentication endpoint. The user requests to log in to the SP via the IdP and obtains a nonce.</li>
<li>The user then creates a proof of their identity and sends it to the IdP. The IdP verifies the proof and, if valid, creates a corresponding signature.</li>
</ul>
</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#p-50265-terminology-3" name="p-50265-terminology-3"></a>Terminology</h2>
<ul>
<li><strong>Extension</strong>: A browser extension (or app) that manages the user’s keys and creates proofs. It is necessary to keep the identity confidential even if the SP and IdP collude.</li>
<li><strong>anoiden.js</strong>: Provides an interface for website clients to communicate with the extension.</li>
<li><strong>Identity Provider (IdP)</strong>: An entity that provides information about the user’s validity to the SP.</li>
<li><strong>Service Provider (SP)</strong>: An entity that utilizes the user’s validity provided by the IdP.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50265-protocol-4" name="p-50265-protocol-4"></a>Protocol</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50265-registration-with-idp-connect-5" name="p-50265-registration-with-idp-connect-5"></a><strong>Registration with IdP (Connect)</strong></h3>
<p>After the user completes account registration with the IdP, they can link that account with the key in the extension. Through this linkage, the user can use this IdP when authentication is requested in the future. Additionally, it becomes possible to log in to the IdP anonymously using the key.</p>
<p>The IdP client obtains the signature and public key through anoiden.js:</p>
<pre><code class="lang-js">const {signature, publicKey} = await connect(serviceName, nonce);
</code></pre>
<ul>
<li><code>serviceName</code> refers to the Identity Provider’s name and is used for key management within the extension.</li>
<li><code>nonce</code> is an unpredictable string obtained from the server side of the IdP.</li>
</ul>
<p>The following diagram shows the flow until the IdP client obtains the user’s signature:<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/e/8e0f26036d0590458476d9aed15c0132bd64d615.png" title="Connect Flow"><img alt="Connect Flow" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/8/e/8e0f26036d0590458476d9aed15c0132bd64d615_2_591x500.png" width="591" /></a></div><p></p>
<p>The client sends the obtained signature, public key, and nonce to the server. The server checks the session and verifies whether it issued that nonce to the user. If valid, it verifies the signature, obtains the identifier (Poseidon hash of the public key), adds the identifier to a Merkle tree, and saves the Merkle root after the addition.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/2/523eb38b3a1fe30f597d2327ea178e4022597044.png" title="Connect Flow"><img alt="Connect Flow" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/5/2/523eb38b3a1fe30f597d2327ea178e4022597044_2_583x500.png" width="583" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#p-50265-auth-6" name="p-50265-auth-6"></a>Auth</h3>
<p>When the SP uses the IdP, it is assumed that it has informed the IdP in advance and has received a client ID from the IdP.</p>
<p>The SP’s client obtains a signature from the IdP through anoiden.js as follows:</p>
<pre><code class="lang-js">const idpSignature = await auth(endpoint, nonce, params);
</code></pre>
<ul>
<li><code>endpoint</code> is the endpoint of the IdP.</li>
<li><code>nonce</code> is obtained from the server side of the SP.</li>
<li><code>params</code> are arbitrary parameters defined between the SP and IdP, but clientId is required.</li>
</ul>
<p>After the extension receives the endpoint, nonce, and params, it obtains identifiers from the IdP’s endpoint and uses Semaphore to create a proof as follows:</p>
<pre><code class="lang-js">// https://js.semaphore.pse.dev/functions/_semaphore_protocol_proof.generateProof.html
await generateProof(identity, group, nonce, "signIn");
</code></pre>
<ul>
<li><code>identity</code> is the key created for each IdP.</li>
<li><code>group</code> is an object created from identifiers.</li>
<li><code>nonce</code> is the nonce passed from the SP.</li>
</ul>
<p>The extension adds the hostname of the SP to params. The IdP uses the clientId and hostname to identify the SP that requested the signature.</p>
<p>The IdP receives the proof and parameters, verifies the proof, and then creates a signature of the nonce and parameters using a key in a format shared in advance with the SP.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/2/f2da965eea0a5c5c1fe8dd36b1430094919c4b5d.png" title="Authorization Flow"><img alt="Authorization Flow" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/f/2/f2da965eea0a5c5c1fe8dd36b1430094919c4b5d_2_631x500.png" width="631" /></a></div><p></p>
<p>The client sends the received signature to the server, which verifies it on the server side.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50265-endpoint-interface-7" name="p-50265-endpoint-interface-7"></a>Endpoint interface</h3>
<p>The IdP’s endpoint has GET and POST methods. GET returns the identifiers of all accounts. POST verifies the proof and confirms the validity of the user.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50265-get-endpointidentifiers-8" name="p-50265-get-endpointidentifiers-8"></a>GET endpoint/identifiers</h4>
<p>There are no parameters. The response is in JSON and returns a list of identifiers as strings under the key identifiers.</p>
<p>Example response:</p>
<pre><code class="lang-json">{
  "identifiers": ["6423154662976160105169106896701549153516891642211172349909782921108153674476"]
}
</code></pre>
<h4><a class="anchor" href="https://ethresear.ch#p-50265-post-endpointauth-9" name="p-50265-post-endpointauth-9"></a>POST endpoint/auth</h4>
<p>The request body requires a JSON object containing the proof and parameters (params):</p>
<pre><code class="lang-json">{
  "proof": "Semaphore proof here",
  "params": {
    "clientId": "exampleClientId",
    "hostname": "example.com",
    "other_params": "additional parameters here"
  }
}
</code></pre>
<p>The response is returned in JSON format and includes the IdP’s signature as a string under the key signature:</p>
<pre><code class="lang-json">{
  "signature": "example_signature"
}
</code></pre>
<h2><a class="anchor" href="https://ethresear.ch#p-50265-security-10" name="p-50265-security-10"></a>Security</h2>
<p>The <code>nonce</code> is used to prevent replay attacks.</p>
<p>The <code>clientId</code> and <code>hostname</code> are mechanisms that ensure the IdP passes user validity only to SPs it has authorized. This also helps prevent requests from unauthorized domains.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50265-concerns-11" name="p-50265-concerns-11"></a>Concerns</h2>
<p>When an SP uses only a single IdP, absolute trust in that IdP is required. However, by utilizing multiple IdPs simultaneously during sign-in (Decentralized SSO), the necessary level of trust in each IdP is reduced.</p>
<p>When the number of identities increases, it becomes inefficient. Therefore, we are considering dividing identity groups, but this raises privacy concerns.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/anoiden-zero-knowledge-sso-with-semaphore/20579">Read full topic</a></p>
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 14:13:59 +0000</pubDate>
</item>
<item>
<title>EIP 7779: Discussion thread</title>
<link>https://ethresear.ch/t/eip-7779-discussion-thread/20574</link>
<guid>https://ethresear.ch/t/eip-7779-discussion-thread/20574</guid>
<content:encoded><![CDATA[
<div> 关键词：文章、内容删除、参与者、单一帖子、信息缺失

<br />
总结: 由于作者自行删除了文章的内容，我们无法获取具体的信息或讨论主题。仅从描述中，我们知道存在一个单一的帖子和一名参与者。由于关键信息的缺失，我们无法提供详细的分析或总结，只能推测可能涉及某个特定话题或事件的讨论。通常情况下，这类情况可能会导致沟通的中断或者对信息的误解，需要更多的上下文来理解讨论的背景和目的。 <div>
<p>(topic deleted by author)</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/eip-7779-discussion-thread/20574">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sun, 06 Oct 2024 19:53:48 +0000</pubDate>
</item>
<item>
<title>Number of peers you need for peer sampling in PeerDAS (EIP-7594)</title>
<link>https://ethresear.ch/t/number-of-peers-you-need-for-peer-sampling-in-peerdas-eip-7594/20562</link>
<guid>https://ethresear.ch/t/number-of-peers-you-need-for-peer-sampling-in-peerdas-eip-7594/20562</guid>
<content:encoded><![CDATA[
<div> 关键词：PeerDAS、CUSTODY_REQUIREMENT、DATA_COLUMN_SIDECAR_SUBNET_COUNT、peer_count、模拟

总结:

本文基于PeerDAS，旨在探讨节点需连接多少对等体以成功覆盖所有列进行采样。主要通过以下几点进行阐述：

1. **PeerDAS机制**：文章指出，节点连接的对等体数量由`CUSTODY_REQUIREMENT`和`DATA_COLUMN_SIDECAR_SUBNET_COUNT`决定。这是节点发现新对等体并了解它们所承担的子网覆盖情况的过程。

2. **模拟过程**：引入了一个名为`peer_count`的函数，用于模拟节点发现机制。该函数通过随机选择子网集来模拟节点如何继续发现新对等体直到所有子网被覆盖。

3. **结果分析**：
   - 当`C`（CUSTODY_REQUIREMENT）固定，`N`（`DATA_COLUMN_SIDECAR_SUBNET_COUNT`）增加时，所需的对等体数量增加。
   - 当`N`固定，`C`增加时，所需的对等体数量减少。
   - 有趣的是，即使`(N/C)`比值不变，`peer_count`值也可能变化，这表明随着子网数量的增加，所需的对等体数量会增加。

4. **结论**：文章强调，仅凭`N`和`C`值很难计算出预期的对等体数量，需要通过实际模拟来调整参数以获得准确值。

5. **实验方法**：通过运行大量模拟实验（例如1000次）来估算平均所需的对等体数量，展示了不同条件下的结果，并分析了这些结果以得出结论。 <div>
<p><em>Authors: <a href="https://github.com/ppopth" rel="noopener nofollow ugc">pop</a></em></p>
<p><em>This post is based on the <a href="https://github.com/ethereum/consensus-specs/tree/cb03c8/specs/_features/eip7594" rel="noopener nofollow ugc">current spec</a> of PeerDAS</em></p>
<p>This post will tell you the expected number of peers each node must connect to in order to cover all the columns to do peer sampling successfully.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50233-simulation-1" name="p-50233-simulation-1"></a>Simulation</h1>
<p>In PeerDAS, at the time of writing, the number of peers you are expected to connect to is solely calculated by <code>CUSTODY_REQUIREMENT</code> and <code>DATA_COLUMN_SIDECAR_SUBNET_COUNT</code>. We write the following simulation to count how many peers you need to connect.</p>
<pre><code class="lang-auto">import math
import random

def peer_count(N, C):
    num_trials = 1000
    counts = []
    for trial in range(num_trials):
        covered = set()
        peers = set()
        count = 0
        while len(covered) != N:
            selected = frozenset(random.sample(range(N), C))
            if not selected in peers:
                peers.add(selected)
                covered = covered.union(selected)
                count += 1
        counts.append(count)
    return sum(counts)/len(counts)
</code></pre>
<p><em>where N and C are <code>DATA_COLUMN_SIDECAR_SUBNET_COUNT</code> and <code>CUSTODY_REQUIREMENT</code>, respectively.</em></p>
<p>What this function does is to simulate the node discovery mechanism. What it does in the real world is that the node will discover new peers and find out what subnets the peer is taking custody of. It will continue discovering new peers until all the subnets are covered by those peers. What’s important is that the node will not keep a peer with a set of custody it has seen, because it doesn’t add any more coverage to the subnets. So when N=32 and C=1, <code>peer_count</code> will be exactly 32.</p>
<p>The set of subnets each peer is supposed to custody is determined by its Peer ID. However, in our simulation, such set is just randomized, which shouldn’t be different from the real world.</p>
<p>In order to get the expected number, we run 1,000 simulations and get only the mean.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50233-result-2" name="p-50233-result-2"></a>Result</h1>
<p>Trivially, if we fix C and make only N variable, <code>peer_count</code> will increase as N increases since, when there are more subnets to cover, the more peers you should have. And, if we fix N and make only C variable, <code>peer_count</code> will decrease as C increases since, when each peer custodies more, the number of peers you should have should be lower. Even if it’s trivial, we did the simulation for completeness.</p>
<div class="d-image-grid">
<p><img alt="" height="360" src="https://ethresear.ch/uploads/default/original/3X/e/6/e683b38be6eadbcd02ae9e8167a275f9ba87d18d.png" width="480" /><br />
<img alt="" height="360" src="https://ethresear.ch/uploads/default/original/3X/5/1/5187900ba4a547aea139a2534e59b1e896ad50bf.png" width="480" /></p>
</div>
<p>You can see that with N=128 and C=16, you need 41.6 peers on average (so it’s far different from N/C=8).</p>
<p>From the second figure, you can see that when C=1, <code>peer_count</code> is outstandingly low. It’s because there is no overlap of the custody sets.</p>
<p>Now, let’s consider an interesting non-trivial scenario, if the ratio (N/C) remains unchanged, do you think <code>peer_count</code> will remain unchanged? That is, do you think <code>peer_count(64, 8)</code> and <code>peer_count(128, 16)</code> will be the same? The answer is no. As shown below, the higher N is, the higher <code>peer_count</code> is.</p>
<p><img alt="Figure_3" height="360" src="https://ethresear.ch/uploads/default/original/3X/a/8/a85174f1e76d15d90336da7ffd6bc8ce7a0c2c5f.png" width="480" /></p>
<p>So the expected number of peers you will have cannot be easily calculated with N and C. You need to do the simulation to tune the parameters.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/number-of-peers-you-need-for-peer-sampling-in-peerdas-eip-7594/20562">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 17:41:34 +0000</pubDate>
</item>
<item>
<title>Can Ethereum Distribution System reduce disk space usage &amp; enable object passing?</title>
<link>https://ethresear.ch/t/can-ethereum-distribution-system-reduce-disk-space-usage-enable-object-passing/20560</link>
<guid>https://ethresear.ch/t/can-ethereum-distribution-system-reduce-disk-space-usage-enable-object-passing/20560</guid>
<content:encoded><![CDATA[
<div> 关键词：Ethereum Distribution System、智能合约分布、区块链大小、状态无状态、执行层迁移

文章总结：

Ethereum Distribution System（EDS）是一个旨在解决当前以太坊网络中智能合约分布碎片化和效率低下的问题的项目。主要关注点包括：

1. **当前状况**：目前的智能合约部署方式导致了大量无法优化存储在数据库中的元数据影响代码哈希的情况，这增加了节点操作者的区块链大小，远超必要水平。

2. **问题与挑战**：由于执行客户端可能并未将合同字节码分割到单独的表中，且开发者缺乏复用代码的激励，这可能导致了不必要的部署冗余，增加了字节码占用的存储空间，长期来看可能会成为一个实际问题。

3. **新兴优势**：新生态系统如Sui等声称拥有竞争优势，例如通过在合约之间传递对象的能力。EDS通过提出状态无状态的分布概念，允许在EVM上构建类似抽象模型，从而实现类似于Sui的功能，如两个通过同一分发系统分发的应用程序无需用户明确批准即可协同工作。

4. **潜在应用**：状态无状态的分布块有可能被用于提供执行层迁移脚本，比如EIP 7702讨论中的例子，这是一种对许多安全假设构成破坏性改变的硬分叉。这可以设想为“以太坊3.0”，一个网络内的网络，在分叉日，信标链承诺迁移社区添加的所有基础设施，并在用户请求时通过指定迁移合约进行用户状态（资产）迁移。

5. **未来展望与开放问题**：文章提出了几个关键问题和可能性，包括是否低估了提案的价值，字节码大小在未来是否成为问题，以及是否应该将使用字节码哈希的提议整合到EIP中。此外，考虑到广泛的代理使用，可能需要一个专门的EIP来支持原生代理功能，尽管代理在当前已经很受欢迎。

总结：
Ethereum Distribution System旨在通过改进智能合约的部署和管理方式，解决当前以太坊网络中的效率问题。它通过减少冗余部署、利用状态无状态的分布特性，以及可能的执行层迁移功能，为以太坊生态系统引入新的潜力。同时，文章提出了若干关键问题和未来可能的扩展方向，包括字节码大小管理、EIP整合、以及原生代理支持的可能性，这些都指向了EDS作为以太坊技术演进的重要组成部分所具有的长远价值。 <div>
<p>Im looking for a developer community feedback for Ethereum Distribution System project: a generalized, semver enabled fully-on chain distribution system (generalized factory)</p>
<p>Please reach back in thread or leave comments in discussion tab on github! <img alt=":handshake:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/handshake.png?v=12" title=":handshake:" width="20" /></p>
<aside class="onebox githubrepo">
  <header class="source">

      <a href="https://github.com/peeramid-labs/eds" rel="noopener nofollow ugc" target="_blank">github.com</a>
  </header>

  <article class="onebox-body">
    <div class="github-row">
  <img class="thumbnail" height="344" src="https://ethresear.ch/uploads/default/optimized/3X/0/f/0f4d7eee7f9125d3e2d66d5eb61f0979eea61f6d_2_690x344.png" width="690" />

  <h3><a href="https://github.com/peeramid-labs/eds" rel="noopener nofollow ugc" target="_blank">GitHub - peeramid-labs/eds: Ethereum Distribution System </a></h3>

    <p><span class="github-repo-description">Ethereum Distribution System </span></p>
</div>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both;"></div>
</aside>

<p><strong>TLDR, hypotesis I want to validate:</strong></p>
<h3><a class="anchor" href="https://ethresear.ch#p-50231-hypotesis-1-1" name="p-50231-hypotesis-1-1"></a>Hypotesis 1</h3>
<p>The current landscape of smart contract distribution on the Ethereum network is fragmented and inefficient.</p>
<p>Mostly, projects generate numerous deployment artifacts that <strong>cannot be optimized in database</strong>(?) because of built metadata affects code hashes.</p>
<p>This practice results in a increase in blockchain size for node operators, far beyond what is necessary.<br />
This likely is much below what storage / tx takes, however, contrary to tx data, bytecode is not something nodes can prune, meaning in the long run it may become more actual?</p>
<p>I am not familiar with how execution clients store contract bytecode, but I doubt they split metadata objects in separate table, and even if, the smart contract devs are not incentivised to re-use heavily and likely anyway produce overhead number of artefacts</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50231-hypotesis-2-2" name="p-50231-hypotesis-2-2"></a>Hypotesis 2</h3>
<p>Newer ecosystems such as Sui claim competitive advantage over Ethereum, for example, for being able to pass objects between contracts.</p>
<p>From my understanding, the EVM is eventually right concept because it allows to build similar abstraction model on top, as it is just a computer architecture.</p>
<p>With stateless Distributions proposed in EDS, <strong>it is possible to write software that will enable functionality similar as Sui proposes</strong> on inside evm. ( e.g,: Two applications distributed trough same distribution / distributor do not need for user explicit <code>approval</code>s as they are by design distributed together. )</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50231-hypotesis-3-3" name="p-50231-hypotesis-3-3"></a>Hypotesis 3</h3>
<p>Distributions that are stateless chunks of code, in principle could be used to provision execution layer migration scripts for the whole protocol.</p>
<p>EIP 7702 discussion is exemplary for this, it’s a breaking change for many security assertions.</p>
<p>It could be presented as “Ethereum 3.0”, a network within a network, where upon fork day, Beacon chain promises to migrate all of infrastructure that community added and then do user states (assets) migration ad-hoc whenever users reach out to designated migration contract.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50231-open-questions-4" name="p-50231-open-questions-4"></a>Open questions</h3>
<p>How dumb am I <img alt=":slight_smile:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/slight_smile.png?v=12" title=":slight_smile:" width="20" /> any of these hypothesises can hold their truth?</p>
<p>Can bytecode size on Ethereum become a problem in foreseeable future?</p>
<p>I’ve encapsulated proposal to enshrine use of bytecode hashes in <a href="https://eips.ethereum.org/EIPS/eip-7744" rel="noopener nofollow ugc">ERC7744</a> however If this proves to be very helpful, perhaps it’s worth to move in EIPs?</p>
<p>If EDS proves to have positive perception, this implies that there will be extensive use of proxies instead of new deployments, perhaps an EIP needed to bake native proxy support for this (even if not, proxies are very popular) ?</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/can-ethereum-distribution-system-reduce-disk-space-usage-enable-object-passing/20560">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 13:27:32 +0000</pubDate>
</item>
<item>
<title>Honest MEV: a Theoretical Perspective</title>
<link>https://ethresear.ch/t/honest-mev-a-theoretical-perspective/20550</link>
<guid>https://ethresear.ch/t/honest-mev-a-theoretical-perspective/20550</guid>
<content:encoded><![CDATA[
<div> 关键词：MEV、区块链研究、算法优化、图参数、诚实MEV问题

总结:

文章提出了一种新的研究方向，利用图参数解决诚实MEV（最大费用验证者）问题。该方法在现实世界数据上成功测试，显著提高了矿工收入，尤其是在比特币和卡尔达诺区块链中，分别增加了约13.4%和55.7%。然而，以太坊面临独特的挑战，包括复杂的交易依赖关系、独特的奖励机制和非UTXO模型导致的不可决定性。

文章回顾了其团队在其他区块链上的研究成果，并强调了在以太坊上独立工作的必要性，由于以太坊的执行模型复杂，不遵循UTXO模型，交易之间的最终费用和消耗可能相互依赖，这与其它网络形成了巨大的可操作性差距。团队计划通过识别交易干扰类型（依赖、冲突、气体依赖）来检测交易间的依赖关系，并将其表示为干扰图，随后将问题表示为具有图输入的优化实例。

文章进一步解释了如何将问题转化为动态规划算法来解决，以实现快速、高效和最优的解决方案。然而，面对复杂的干扰图结构，团队正在寻找最适合的图类，并设计相应的高效算法。最后，文章呼吁社区提供意见和建议，共同探讨如何克服以太坊面临的挑战并提高矿工收入。 <div>
<h1><a class="anchor" href="https://ethresear.ch#p-50212-honest-mev-a-theoretical-perspective-1" name="p-50212-honest-mev-a-theoretical-perspective-1"></a>Honest MEV: a Theoretical Perspective</h1>
<h1><a class="anchor" href="https://ethresear.ch#p-50212-greetings-2" name="p-50212-greetings-2"></a>Greetings</h1>
<p>Hi there! We are a research team <a href="https://sites.google.com/view/btogzhan/about" rel="noopener nofollow ugc">Togzhan</a>, <a href="http://soroush.farokhnia.me/" rel="noopener nofollow ugc">Soroush</a>, <a href="https://amir.goharshady.com/" rel="noopener nofollow ugc">Amir</a>, <a href="https://polka125.github.io/" rel="noopener nofollow ugc">Sergei</a> who cares about blockchain research, algorithms, and optimization.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50212-tldr-aka-abstract-3" name="p-50212-tldr-aka-abstract-3"></a>TL;DR; (aka abstract)</h1>
<p>We propose a new research direction: using graph parameters to solve the honest MEV problem (the problem of forming a block with maximum gas fees by an honest miner). The method was successfully tested on real-world data from other blockchains, increasing miners’ revenue by <strong>13.4%</strong> on Bitcoin <strong>(~100 million USD/year)</strong> and by <strong>55.7%</strong> on Cardano <strong>(~0.5 million USD/year)</strong>. However, Ethereum presents unique challenges that are yet to be overcome: non-trivial transaction dependencies, a unique reward model, and a non-UTXO model which causes undecidability. We look forward to the community’s opinions and suggestions regarding the challenges we face.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50212-related-works-4" name="p-50212-related-works-4"></a>Related Works</h1>
<p>Our previous for other blockchains:</p>
<ul>
<li>Bitcoin: <a href="https://hal.science/hal-03232783v2/file/OptimalMining.pdf" rel="noopener nofollow ugc">https://hal.science/hal-03232783v2/file/OptimalMining.pdf</a></li>
<li>Cardano: <a href="https://hal.science/hal-04616639/file/main.pdf" rel="noopener nofollow ugc">https://hal.science/hal-04616639/file/main.pdf</a></li>
</ul>
<p>Similar work on ethresear.ch:</p>
<ul>
<li><a class="inline-onebox" href="https://ethresear.ch/t/block-building-is-not-just-knapsack/19871">Block Building is not just knapsack!</a></li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#p-50212-the-project-5" name="p-50212-the-project-5"></a>The Project</h1>
<p><strong>What is Honest MEV?</strong> Let’s consider the perspective of an honest validator – one who refrains from frontrunning, sandwiching, and other dishonest behaviors. The reward we receive for validating a block includes the base reward, and the gas fees paid by transactions. The gas fees are the only variable we can influence, so naturally, we’re interested in maximizing them.</p>
<p><strong>What do we want to do?</strong> We have only one degree of freedom: how to take an ordered subset of the transactions from the pool to a new block to maximize the gas fees. Our main ambition is to make this decision procedure <strong>optimal</strong>, <strong>fast</strong>, and <strong>accessible</strong> for everyone.</p>
<p><strong>I’m not a block proposer, why care?</strong> Besides the obvious benefits to an honest block proposer, the indirect impact of our project is far greater:</p>
<ul>
<li>It helps validators achieve maximum block <strong>utilization</strong></li>
<li>It helps with <strong>throughput</strong>, allowing more transactions to be added to the consensus chain faster</li>
<li>The increase in revenues also <strong>incentivizes</strong> more people to become validators, helping <strong>more decentralized</strong> consensus.</li>
<li>In addition to protocol constraints (e.g. dynamic pricing policy for gas), external constraints also commonly influence miners’ decisions. For example, miners may choose to produce smaller blocks to speed up block propagation or adjust the algorithm to manage different sets of transactions within the pool. The primary goal of the divine algorithm is to allow miners to maximize their profitability, within the constraints. This means that even if a miner has to create a small block, she can still maximize her revenue by picking a better set of transactions.</li>
</ul>
<p><strong>What has been done already?</strong> If the problem is so fundamental, someone must have done something about it already, right? Right. Our team has a line of research devoted to developing optimal mining algorithms, i.e. algorithms that produce blocks with maximum rewards, for different blockchains, we successfully tested, implemented and published optimal approaches for the <a href="https://hal.science/hal-03232783v2/file/OptimalMining.pdf" rel="noopener nofollow ugc">Bitcoin</a> and <a href="https://hal.science/hal-04616639/file/main.pdf" rel="noopener nofollow ugc">Cardano</a> blockchains. We tested our approach on the real world data and achieved a <strong>13.4%</strong> increase in transaction fees on Bitcoin <strong>(~100 million USD/year)</strong> and a <strong>55.7%</strong> growth on Cardano <strong>(~0.5 million USD/year)</strong>. Moreover, the problem has independently attracted the attention of <a href="https://ethresear.ch/t/block-building-is-not-just-knapsack/19871">other Ethereum researchers</a></p>
<p><strong>Why not just reuse the previous algorithms?</strong> The Ethereum execution model is inherently complex as it does not follow the UTXO model; the final gas fees as well as gas consumption of one transaction might depend on others, it makes a huge tractability gap compared to other networks.</p>
<p>While for two transactions it is simple to say if they are not conflicting (by checking a set of predefined rules, like nonce, balance, etc), it is really hard to say if one transaction interferes with others gas consumptions; if not a block gas limits, the gas dependency problem is <strong>theoretically undecidable</strong>, and if having a gas bound it is <a href="https://en.wikipedia.org/wiki/EXPTIME#EXPTIME-complete" rel="noopener nofollow ugc">EXPTIME-complete</a>.</p>
<p><strong>So what is your plan?</strong> Our plan has several subgoals:</p>
<ul>
<li>Find a way to detect all <strong>dependencies</strong> between the transactions in a pool</li>
<li>Formulate the problem as an <strong>optimization</strong> instance with a <strong>graph</strong> input</li>
<li><strong>Implement</strong> the algorithm and test it on real pools and blocks</li>
</ul>
<p><strong>Where can I see more technical details?</strong> Sure, in the next few paragraphs we want to give a deeper introduction for our framework, and what challenges we are facing.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50212-transaction-interference-6" name="p-50212-transaction-interference-6"></a>Transaction Interference</h1>
<p>There are three ways how transactions might affect each other: they might</p>
<ul>
<li>depend on each other, i.e. one transaction cannot be included to a block unless another is included</li>
<li>conflict, i.e. cannot both appear in the same block</li>
<li>gas dependent, i.e. depending on co-occurrence of two transactions, their gas fees might be different</li>
</ul>
<p>We will illustrate all interference types with a simple example.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50212-insdinsependency-interference-7" name="p-50212-insdinsependency-interference-7"></a><ins>D</ins>ependency Interference</h3>
<p>In a correct block, transactions originating from the same account must have sequential nonce, lower nonce must appear before transactions with higher nonce to preserve the ordering. When a miner observes the transaction pool, she should choose transactions in a way that preserves an increasing nonce order for a particular account.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/a/faf35736906a2c9a3948871cc08e092dc0c6ed50.png" title=""><img alt="" height="219" src="https://ethresear.ch/uploads/default/optimized/3X/f/a/faf35736906a2c9a3948871cc08e092dc0c6ed50_2_450x219.png" width="450" /></a></div><p></p>
<p>This requirement creates the dependency relations, captured as oriented <strong>dependency edges</strong> in the interference graph.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50212-inscinsonflict-interference-8" name="p-50212-inscinsonflict-interference-8"></a><ins>C</ins>onflict Interference</h3>
<p>Transactions originating from the same account must have sequential nonce, it is not allowed to have two transactions with the same nonce. When the miner encounters two transactions with the same nonce, he has to choose only one of them to be included in the block.</p>
<p><strong></strong></p><div class="lightbox-wrapper"><strong><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/9/59c90103b23f5e9379b0a6ec8bb378bbd63ee680.png" title=""><img alt="" height="234" src="https://ethresear.ch/uploads/default/optimized/3X/5/9/59c90103b23f5e9379b0a6ec8bb378bbd63ee680_2_478x234.png" width="478" /></a></strong></div><p></p>
<p>We introduce undirected <strong>conflict edges</strong> to the graph to represent this restriction.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50212-insginsas-dependency-interference-9" name="p-50212-insginsas-dependency-interference-9"></a><ins>G</ins>as Dependency Interference</h3>
<p>The last one is the most complicated relation, we will demonstrate it using a simple example. Consider a smart contract with the following two functions <code>f</code> and <code>g</code>:</p>
<pre><code class="lang-auto">function f(): x = 1000
function g(): for i=1 to x { do something }
</code></pre>
<p>Assume that <code>x</code> is originally <code>0</code> and there are two transactions <code>Tx1</code> calling <code>f</code> and <code>Tx2</code> calling <code>g</code>. If  the validator puts <code>Tx2</code> before <code>Tx1</code>, the for loop in <code>g</code> terminates immediately, using very little gas. However, if <code>Tx1</code> is put before <code>Tx2</code>, then much more gas is used in <code>g</code>, leading to higher revenues for the validator.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50212-insdcgins-graph-10" name="p-50212-insdcgins-graph-10"></a><ins>DCG</ins> Graph</h3>
<p>We call a DCG graph a graph with each vertex representing a transaction in the mempool and edges of three sorts, with one for each type of interference.<br />
A graph abstraction is an important step in our framework as it splits the problem into two subtasks: constructing the graph from the mempool and solving the optimization problem over the graph.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50212-graphs-and-optimizations-11" name="p-50212-graphs-and-optimizations-11"></a>Graphs and Optimizations</h1>
<p>Many graph problems are known for being very difficult. Our case is no exception: having only conflict-type edges makes the problem of choosing an optimal subset of vertices for the answer set <strong>strongly NP-hard</strong>. This essentially means we cannot hope for an algorithm that is <strong>fast</strong>, <strong>optimal</strong>, and <strong>works on all inputs</strong>. When we cannot have all three, we can still choose two, and that is what happens in practice:</p>
<ul>
<li>{works on all inputs, fast, <s>optimal</s>}: if we discard optimality, we find ourselves in the realm of approximate and heuristic algorithms.</li>
<li>{works on all inputs, <s>fast</s>, optimal}: discarding the requirement to be fast leads to exponential time algorithms. While they might work on tiny inputs, it is unfeasible to run them on real data.</li>
<li>{<s>works on all inputs</s>, fast, optimal}: generally hard problems become <strong>very easy</strong> in the special cases. For instance, a three coloring problem is trivial for tree graphs. We claim that real-world graphs are <strong>well-structured,</strong> and this structure <strong>can be exploited</strong> to build fast and optimal algorithms.</li>
</ul>
<p>However, we don’t provide an exact definition of a <strong>well-structured</strong> graph, there is a [zoo of graph classes](<a href="https://www.graphclasses.org/" rel="noopener nofollow ugc">https://www.graphclasses.org/</a>). For some graph classes, there often exists a specially tailored algorithm which works well within the given class (see our previous publications). One step of our research program is to <strong>identify the most suitable class</strong> the real graphs belong to and design an algorithm which works within that class.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50212-an-example-12" name="p-50212-an-example-12"></a>An Example</h1>
<p>We will demonstrate our ideas with an example. The oversimplifications here are intentional, allowing us to showcase the main techniques without getting bogged down in technical details.</p>
<p>The classical knapsack problem is [weakly NP-hard](<a href="https://en.wikipedia.org/wiki/Weak%5C_NP-completeness" rel="noopener nofollow ugc">https://en.wikipedia.org/wiki/Weak\_NP-completeness</a>), and there are polynomial algorithms when all item volumes are discrete and each volume belongs to the set [1,…,N]. In this case, there is a dynamic programming algorithm that is polynomial in:</p>
<ul>
<li>The size of the knapsack S</li>
<li>The maximum possible volume N</li>
</ul>
<p>Consider an extremely simple version of the knapsack problem: all item volumes are equal to 1. How can we find an optimal value packing when the capacity of the knapsack is S? The algorithm is trivial: sort the items in decreasing order of price and select the top S most valuable items.</p>
<p>Translating this to the optimal block mining language, all transactions pay an equal gas fee of 1, and have no conflicts or dependencies, the optimal block would consist of the transactions offering the highest tips. This holds when there are no conflicts or dependencies.</p>
<p>What happens when we add interference edges? Again, let’s keep it simple and assume that we have only conflicts. Going back to the knapsack formulation, for some pairs of items, we cannot pack them together.</p>
<p>The problem suddenly became <strong>very hard.</strong> It’s not just NP-hard, it’s <strong>strongly NP-hard</strong>. The reason for that, that the problem of finding a [maximal independent set](<a href="https://en.wikipedia.org/wiki/Maximal%5C_independent%5C_set" rel="noopener nofollow ugc">https://en.wikipedia.org/wiki/Maximal\_independent\_set</a>) is a special case of our problem: by setting the knapsack capacity to infinity and all weights to 1, finding the optimal knapsack becomes equivalent to finding a maximal independent set.</p>
<p>Although it is clear that not all graphs can be achieved as a conflict graph of a set of transactions (and in fact, the dependency graph is always a union of cliques), this aligns with our final goal: exploit specific graph patterns to design a faster algorithm.</p>
<p>Do demonstrate how, assume that the conflict edges form a binary tree. The full list assumptions we are making till this point are:</p>
<ul>
<li>Each transaction uses 1 unit of gas</li>
<li>We don’t make a restriction on the tips the transactions can pay</li>
<li>The only type of interference edges we have are conflict edges, and they form a binary tree (the least realistic assumption)</li>
</ul>
<p>This problem from strongly NP hard becomes again polynomial time solvable! The main algorithmic technique for this is dynamic programming, we will demonstrate it on a concrete example.</p>
<p>Consider a pool of 6 transactions {a, b, c, d, e, f} with unit gas usage and with the miner tips given in the table</p>
<div class="md-table">
<table>
<thead>
<tr>
<th style="text-align: left;">Tx:</th>
<th style="text-align: left;">a</th>
<th style="text-align: left;">b</th>
<th style="text-align: left;">c</th>
<th style="text-align: left;">d</th>
<th style="text-align: left;">e</th>
<th style="text-align: left;">f</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Total tip:</td>
<td style="text-align: left;">6</td>
<td style="text-align: left;">7</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">7</td>
<td style="text-align: left;">5</td>
</tr>
<tr>
<td style="text-align: left;">Gas usage:</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
</tr>
</tbody>
</table>
</div><p>Let us fix the size of the block to be 3. Further suppose that the miner observed a set of conflicts which form a binary tree with the edges {(a, b), (a, c), (b,d), (c,e), (c,f)}:​​</p>
<p><strong></strong></p><div class="lightbox-wrapper"><strong><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/7/47e875969aa2f1f1f722a8d61000f2d28b96b284.png" title=""><img alt="" height="190" src="https://ethresear.ch/uploads/default/optimized/3X/4/7/47e875969aa2f1f1f722a8d61000f2d28b96b284_2_149x190.png" width="149" /></a></strong></div><p></p>
<p>Introduce a tensor table opt[tx, IsIncl, Cap] with tx ∊ {a, b, c, d, e, f}; IsIncl ∊ {“no”, “yes”} and Cap ∊ {0, 1, 2, 3, 4}.<br />
The value of the tensor is defined as:</p>
<p><code>opt[tx, “no”, c] := “the best revenue a miner can get, only using transactions from the subtree of tx, when tx is not included, if the block capacity is c”</code></p>
<p>And the similar rule for “yes”:</p>
<p><code>opt[tx, “yes”, c] := “the best revenue a miner can get, using only transactions from the subtree of tx, when tx is included, if the block capacity is c”.</code></p>
<p>Now our goal is to fill this table with values, then we can find the maximum possible benefit as<br />
<code>max(opt[a, “no”, 3], opt[a, “yes”, 3]),</code><br />
I.e. as maximum profit which can be achieved using the subtree of a (which is the whole set {a, b, c, d, e, f}) and a block of capacity 3.</p>
<p>The filling out strategy is called bottom-up dynamic approach, we are going to fill the tables starting from the leaves of the tree, advancing to the root.</p>
<p>The values in the leaves are determined as:</p>
<p><code>opt[tx, “no”, *] := 0</code><br />
As if we don’t take the leaf transaction, the bag is forced to be empty<br />
<code>opt[tx, “yes”, 0] := - inf</code><br />
It is merely a convention to put the negative infinity profit, as such a configuration is not feasible: the “yes” field forces us to take the item tx to the set, but the value 0 constrains the block capacity to be 0. And the last case for the leaf table:<br />
<code>opt[tx, “yes”, ≥ 1] := tip(tx),</code><br />
As we are forced to include transaction tx to the block, and including it does not exceed the block capacity.</p>
<p>For the running example after the initialization step we will have:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/d/ad77eeec2e5f3a6fb6776d40b52d38161adf6759.png" title=""><img alt="" height="312" src="https://ethresear.ch/uploads/default/optimized/3X/a/d/ad77eeec2e5f3a6fb6776d40b52d38161adf6759_2_624x312.png" width="624" /></a></div><p></p>
<p>To fill out the tables for the internal nodes, we again will consider two cases, if the node is forced to be included or not “yes”/”no”. Let’s start from the “yes” case, when a note tx is forced to be included to the block. The node tx has up to two children, suppose that it has exactly two children tx_l and tx_r (the case of one child can be handled by introducing a fake node with unit gas usage and zero tips).</p>
<p>Then to find the value of opt[tx, “yes”, c] first we need to allocate one unit of our capacity to the transaction tx, and the remaining capacity is split between two subtrees. Observe that if we took tx, then we cannot take neither tx_l or tx_r to the block because of the conflict edges from tx. Therefore<br />
<code>opt[tx, “yes”, c] := tip(tx) +</code><br />
<code>max (opt[tx_l, “no”, i] + opt[tx_r, “no”, c - i - 1])</code><br />
<code>for i in [0, c)</code></p>
<p>The i indicates the capacity allocated to the left child, and all remaining capacity is allocated to the right child.</p>
<p>Similar reasoning leads us to the formula for the “no” case:<br />
<code>opt[tx, “no”, c] := max(</code><br />
<code>max{opt[tx_l, “no”, i], opt[tx_l, “yes”, i}</code><br />
<code>+ max{opt[tx_r, “no”, c - i - 1], opt[tx_r, “no”, c - i - 1]})</code></p>
<p>where having to have max{opt[tx_l, “no”/”yes”, *]} is caused by necessity to consider two cases whether to include or exclude the child from the set. Filling the tables using these formulas, we end up with:</p>
<p><strong></strong></p><div class="lightbox-wrapper"><strong><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/4/2461dd155e6cb079d8ae3afe3642efd3840b0742.png" title=""><img alt="" height="312" src="https://ethresear.ch/uploads/default/optimized/3X/2/4/2461dd155e6cb079d8ae3afe3642efd3840b0742_2_624x312.png" width="624" /></a></strong></div><p></p>
<p>We can see that the optimal block of size 3 will produce revenue 19 from the table at the root, which is the case for the block {b, e, f}. Finding not only the answer but also the block itself is achieved by a standard technique of keeping the optimal capacity split between the children when evaluating the formula for the internal nodes, as well as keeping the optimal decision whether to take a child to the set or not.</p>
<p>Despite the oversimplifications we made, our general algorithms for other blockchains have the same flavor: we find a way to build the graph starting from elementary parts (leafs for this case) using simple combining operations (combining two children subtrees into the node’s tree in this case) and providing the way to recompute the objective function during the combination operation.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50212-challenges-13" name="p-50212-challenges-13"></a>Challenges</h1>
<p>We identified the challenges we need to address during the project. The challenges vary in nature: some are related to engineering and computation, while other are purely theoretical.</p>
<p>We anticipate to face the following challenges:</p>
<ul>
<li>to have a well-connected node to access the transaction pool.</li>
<li>to be able to execute transaction bundles and obtain their gas usage.</li>
<li>to be able to Identify if two transactions are exactly gas dependent is possible if we execute every possible subset of transactions and in every possible order. One way is to use various tools which include symbolic execution tools and static analysis tools.</li>
<li>to examine the structure of the transactions interference graph, choose the closest class from the zoo of graph classes and create an efficient algorithm solving the optimization problem</li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#p-50212-opinion-request-14" name="p-50212-opinion-request-14"></a>Opinion Request</h1>
<p>We truly believe it is possible to obtain an increase in the transaction revenue for Ethereum and beat real-world miner results. If we do this, it would greatly benefit both miners and the Ethereum ecosystem. What do you think about this project? Please feel free to share your opinion and potential concerns. Let’s have a discussion below! <img alt=":slight_smile:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/slight_smile.png?v=12" title=":slight_smile:" width="20" /></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/honest-mev-a-theoretical-perspective/20550">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 06:26:01 +0000</pubDate>
</item>
<item>
<title>On solo staking, local block building and blobs</title>
<link>https://ethresear.ch/t/on-solo-staking-local-block-building-and-blobs/20540</link>
<guid>https://ethresear.ch/t/on-solo-staking-local-block-building-and-blobs/20540</guid>
<content:encoded><![CDATA[
<div> 关键词：独占质押、本地区块构建、块重组、专业节点运营商、MEV-Boost

总结:

文章主要探讨了在Pectra网络中，关于增加blob吞吐量的讨论与争议。其中，独占质押者被视为以太坊社区的核心力量，他们反对牺牲独占质押者来实现线性扩展。然而，社区需要权衡的是，当弱小、低带宽的质押者对去中心化做出贡献时，其限制是否足以阻碍以太坊的扩展能力。

文章提供了关键数据和见解：

1. **独占质押者的角色**：独占质押者在区块链中扮演着重要角色，但他们也面临着更高的块重组风险，这表明他们在网络中的稳定性不如专业节点运营商。

2. **块重组率**：数据显示，自Dencun硬叉以来，块重组率总体呈下降趋势，但不同类型的节点（如独占质押者、未识别类别等）的重组率变化不一。独占质押者和未识别类别的重组率有所下降，而使用本地构建的节点（如非MEV-Boost用户）的重组率则相对稳定或略有上升。

3. **本地区块构建与MEV-Boost**：本地构建的节点倾向于更频繁地重组，可能是因为它们无法从MEV-Boost提供的快速传播优势中获益。此外，MEV-Boost用户的重组率明显低于本地构建节点。

4. **角色分析**：独占质押者是本地构建节点的最大组成部分，而一些专业节点运营商如Lido，即使使用本地构建，也可能选择不使用MEV-Boost或仅使用最低竞标标志。

5. **多因素导致重组**：文章强调了多种因素可能导致块重组，包括独占质押者的高槽位错过率、本地构建的策略以及与MEV-Boost相关的时机策略等。

综上所述，文章通过数据分析和案例研究，深入探讨了以太坊网络中独占质押者、本地区块构建、块重组及其与专业节点运营商和MEV-Boost之间的关系。这些发现对于理解以太坊网络的去中心化与扩展挑战至关重要。 <div>
<h1><a class="anchor" href="https://ethresear.ch#p-50190-on-solo-staking-local-block-building-and-blobs-1" name="p-50190-on-solo-staking-local-block-building-and-blobs-1"></a>On solo staking, local block building and blobs</h1>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/7/679782c02f64eba56e5061a305e16a7815b77bc5.jpeg" title=""><img alt="" height="375" src="https://ethresear.ch/uploads/default/optimized/3X/6/7/679782c02f64eba56e5061a305e16a7815b77bc5_2_377x375.jpeg" width="377" /></a></div><br />
In recent weeks, discussions about a potential increase in blob throughput in Pectra have intensified, with two distinct groups emerging. One advocates for the increase, while the other is hesitant, preferring to wait for clear data supporting such a change.<p></p>
<p>From my perspective, one sentiment is overwhelmingly clear within the community:</p>
<div class="math">
\textbf{Solo stakers are at the heart of Ethereum.}
</div>
<p>While there hasn’t been a consensus on the minimum requirements for validators (see <a href="https://x.com/sassal0x/status/1839684947864322320">sassal.eth’s tweet</a> on that), the Ethereum community has made one thing clear:</p>
<div class="math">
\textbf{We do not want to sacrifice solo/home stakers for additional linear scaling.}
</div>
<p>In my view, this reflects a healthy direction for Ethereum and underscores the community’s view on the importance of viable solo staking.</p>
<p>However, it raises an important question: “<em><strong>Where is the line?</strong></em>”</p>
<p><em><strong>Specifically, at what point does the contribution of a weaker, lower-bandwidth staker to decentralization no longer justify the limitations it imposes on Ethereum’s ability to scale?</strong></em></p>
<p>In this piece, I aim to provide additional data points to help the community make an informed decision on whether we want to pursue a blob throughput increase in Pectra.</p>
<blockquote>
<p>As <a href="https://x.com/potuz_eth">Potuz</a>, a core developer from Prysm, aptly stated, the real question is not “Do we want to scale, and how?” but rather, “Are we ready to do so now?”</p>
</blockquote>
<h2><a class="anchor" href="https://ethresear.ch#p-50190-who-is-being-reorged-today-oct-23-oct-24-2" name="p-50190-who-is-being-reorged-today-oct-23-oct-24-2"></a>Who is being reorged today?  (Oct 23 - Oct 24)</h2>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/7/f76b082ee7f082c61d5c9d69136f1151f702f5d3.png" title=""><img alt="" height="345" src="https://ethresear.ch/uploads/default/optimized/3X/f/7/f76b082ee7f082c61d5c9d69136f1151f702f5d3_2_690x345.png" width="690" /></a></div><p></p>
<ul>
<li>On average ~0.2% of blocks are reorged (=reorged ⊆ missed).</li>
<li>Professional node operators (NOs) such as Lido, Kiln, Figment, and EtherFi are reorged less often than the average.</li>
<li>Less professional NOs such as solo stakers, Rocketpool operators, or the unidentified category which likely includes many solo stakers that couldn’t be identified, are more frequently reorged.</li>
</ul>
<br />
<p>As shown in <a href="https://ethresear.ch/t/steelmanning-a-blob-throughput-increase-for-pectra/20499">an earlier analysis</a>, the reorg rate has been trending down since the Dencun hardfork.<br />
In the following chart, we can see that this effect was different for different entities:<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/0/60009e3ddd9404a75f92b335b6acfa5fc7dfc4d2.png" title="reorgs_entites_over_time (3)"><img alt="reorgs_entites_over_time (3)" height="345" src="https://ethresear.ch/uploads/default/optimized/3X/6/0/60009e3ddd9404a75f92b335b6acfa5fc7dfc4d2_2_690x345.png" width="690" /></a></div><p></p>
<ul>
<li>The reorg rate decreased for solo stakers and <code>unidentified</code> since Dencun.</li>
<li>The same applies to Rocketpool operators, as well as larger operators such as Lido, Coinbase, Figment, and OKX.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50190-what-about-local-block-building-oct-23-oct-24-3" name="p-50190-what-about-local-block-building-oct-23-oct-24-3"></a>What about local block building? (Oct 23 - Oct 24)</h2>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/3/8395fc7a154b30a8dc6cfabd6e3f5ca242a37c1d.png" title=""><img alt="" height="345" src="https://ethresear.ch/uploads/default/optimized/3X/8/3/8395fc7a154b30a8dc6cfabd6e3f5ca242a37c1d_2_690x345.png" width="690" /></a></div><p></p>
<ul>
<li>Local builders have a reorg rate of approximately 1.02%.</li>
<li>MEV-Boost builders have a reorg rate of approximately 0.20%.</li>
<li>Local builders are approximately 5 times more likely to be reorged than MEV-Boost builders.<br />
<br /></li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/c/9cb8adb2f9f76b5c455c8d2857d15ae29686ef03.png" title=""><img alt="" height="345" src="https://ethresear.ch/uploads/default/optimized/3X/9/c/9cb8adb2f9f76b5c455c8d2857d15ae29686ef03_2_690x345.png" width="690" /></a></div><p></p>
<ul>
<li>The reorg share for local block builders seems to have remained constant or even increased after the Dencun hardfork.</li>
<li>For MEV-Boost users, reorgs have been trending down since Dencun.</li>
<li>Notably, <a href="https://ethresear.ch/t/blobs-reorgs-and-the-role-of-mev-boost/19783">previous analysis</a> showed that local builders included on average more blobs into their blocks. Furthermore <a href="https://ethresear.ch/t/big-blocks-blobs-and-reorgs/19674">we have seen</a> that right after the Dencun hardfork blocks with 6 blobs struggled a bit, but this eventually stabilized again. This might explain why the reorg rate didn’t decrease for local builders.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50190-who-are-the-local-builders-oct-23-oct-24-4" name="p-50190-who-are-the-local-builders-oct-23-oct-24-4"></a>Who are the local builders? (Oct 23 - Oct 24)</h2>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/6/c69dfe7af38af8f4d0f4da5172d1b415c82c1ddd.png" title=""><img alt="" height="172" src="https://ethresear.ch/uploads/default/optimized/3X/c/6/c69dfe7af38af8f4d0f4da5172d1b415c82c1ddd_2_690x172.png" width="690" /></a></div><p></p>
<ul>
<li>Solo stakers (here labeled as “solo staker” but with many solo stakers in the <code>unidentified</code> category) are the largest entity within the “local builder” category.</li>
<li>Furthermore there are Lido NOs that are not using MEV-Boost at all or use the min-bid flag.</li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#p-50190-key-insights-5" name="p-50190-key-insights-5"></a>Key Insights</h1>
<ul>
<li>Solo stakers tend to miss more slots compared to professional validators.</li>
<li>Solo stakers often build their blocks locally rather than using MEV-Boost.</li>
<li>Local block builders don’t benefit from the fast propagation offered by MEV-Boost relays.</li>
<li>Relays engage in timing strategies (e.g., relay delays, allowing time to wait for even more profitable blocks).</li>
<li>Epoch boundaries contribute to an increase in reorgs.</li>
</ul>
<p>Multiple factors can lead to reorgs, making it challenging to pinpoint exactly why certain validators, like solo stakers, experience them more frequently than others.</p>
            <p><small>5 posts - 5 participants</small></p>
            <p><a href="https://ethresear.ch/t/on-solo-staking-local-block-building-and-blobs/20540">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 13:16:48 +0000</pubDate>
</item>
<item>
<title>FOCIL CL &amp; EL workflow</title>
<link>https://ethresear.ch/t/focil-cl-el-workflow/20526</link>
<guid>https://ethresear.ch/t/focil-cl-el-workflow/20526</guid>
<content:encoded><![CDATA[
<div> 关键词：Fork-Choice enforced Inclusion Lists (FOCIL)、Ethereum、链中性、交易排除、边案例对

总结:

本文详细介绍了Fork-Choice enforced Inclusion Lists (FOCIL)机制，旨在提升以太坊的去中心化和抗审查能力。FOCIL通过确保交易及时被纳入区块链来实现这一目标。文章阐述了FOCIL的工作流程，包括了IL委员会成员、节点、提议者和见证者的角色与职责。重点强调了构建本地Inclusion List（IL）的规则、节点验证和转发IL的过程、提议者整合IL数据以及见证者验证提议者的执行负载。

在FOCIL机制中，IL委员会成员负责构建并广播本地IL，节点接收并验证这些IL，提议者在特定时间点冻结IL视图并更新执行负载，而见证者则监控提议者块并验证其有效性。为确保FOCIL的稳定运行，文章讨论了可能遇到的边缘情况和应对策略，如处理无效交易、防止委员会成员的投票欺诈以及防止IL填充攻击等。

在边案例处理方面，文章指出通过使用验证函数检查缺失交易的有效性，FOCIL能够兼容账户抽象。同时，通过限制节点转发本地IL的数量，有效防止了委员会成员的投票欺诈行为。此外，文中还提到了一种防御IL填充攻击的方法，即通过在交易计划被提交到内存池前确保能够成功获得构建块的权利，使得这种攻击变得风险极高且不具成本效益。

总的来说，FOCIL通过精心设计的流程和策略，不仅增强了以太坊的链中性和去中心化特性，还有效应对了潜在的安全挑战，从而确保了交易的及时和公正处理。 <div>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/d/3d093019b9cabaa6a15341bc12abbbb000ed0b62.jpeg" title="Screenshot 2024-09-30 at 14.48.26"><img alt="Screenshot 2024-09-30 at 14.48.26" height="450" src="https://ethresear.ch/uploads/default/optimized/3X/3/d/3d093019b9cabaa6a15341bc12abbbb000ed0b62_2_450x450.jpeg" width="450" /></a></div><p></p>
<p>by <a href="https://ethresear.ch/u/soispoke/summary">Thomas Thiery</a>, on <code>September 30th, 2024</code></p>
<p><em>Thanks to <a href="https://x.com/_julianma" rel="noopener nofollow ugc">Julian</a>, <a href="https://ethresear.ch/u/terence/summary">Terence</a>, <a href="https://x.com/jih2nn" rel="noopener nofollow ugc">Jihoon</a>, <a href="https://ethresear.ch/u/jacobkaufmann/summary">Jacob</a> and <a href="https://ethresear.ch/u/aelowsson/summary">Anders</a> for their input and feedback on this post.</em></p>
<h2><a class="anchor" href="https://ethresear.ch#p-50164-introduction-1" name="p-50164-introduction-1"></a>Introduction</h2>
<p>Fork-Choice enforced Inclusion Lists (FOCIL) is a mechanism designed to enhance Ethereum’s censorship resistance and chain neutrality properties by enforcing timely transaction inclusion. Since its <a href="https://ethresear.ch/t/fork-choice-enforced-inclusion-lists-focil-a-simple-committee-based-inclusion-list-proposal/19870">inception</a>, our focus has shifted towards working on an EIP and specifying its implementation on both the <a href="https://github.com/terencechain/consensus-specs/pull/2" rel="noopener nofollow ugc">consensus layer</a> (CL) and the execution layer (EL). This document outlines the workflow of FOCIL, detailing the roles and responsibilities of various participants, including IL committee members, nodes, proposers, and attesters. It also addresses potential edge cases and mitigation strategies to ensure the robustness of the mechanism.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50164-roles-participants-2" name="p-50164-roles-participants-2"></a>Roles &amp; Participants</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50164-il-committee-members-3" name="p-50164-il-committee-members-3"></a>IL committee members</h3>
<ul>
<li>
<p>Slot <span class="math">n</span>, <code>t = 0 to 9s</code>: IL committee members construct their local ILs and broadcast them over the P2P network after processing the block for slot <span class="math">n</span> and confirming it as the head. If no block is received by <code>t = 8s</code>, they should run <code>get_head</code> and build and release their local ILs based on their node’s canonical head.</p>
<blockquote>
<p><em>By default, local ILs are built by selecting raw  transactions from the public mempool, ordered by priority fees, up to the local IL’s maximum <code>gas</code> limit (we could also set a limit in <code>bits</code> if we consider that local ILs mostly consume bandwidth). Additional rules can be optionally applied to maximize CR, such as prioritizing valid transactions that have been pending in the mempool the longest.</em></p>
</blockquote>
</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50164-nodes-4" name="p-50164-nodes-4"></a>Nodes</h3>
<ul>
<li>
<p>Slot <span class="math">n</span>, <code>t = 0 to 9s</code>: Nodes receive local ILs from the P2P network and only forward and cache those that pass the CL P2P validation rules.</p>
</li>
<li>
<p>Slot <span class="math">n</span>, <code>t = 9s</code>, <em>IL freeze deadline</em>: Nodes freeze their local ILs view, stop forwarding and caching new local ILs.</p>
<blockquote>
<p><strong>CL P2P validation rules:</strong></p>
<ul>
<li><em>The number of transactions in the local IL does not exceed the maximum <code>gas</code> limit allowed.</em></li>
<li><em>The slot of the local IL matches the current slot. Locals ILs not matching the current slot should be ignored.</em></li>
<li><em>The parent hash of the IL is recognized.</em></li>
<li><em>The IL is received before the local IL freeze deadline (e.g., <code>9s</code>) into the slot.</em></li>
<li><em>Received two or fewer local ILs from this IL committee member (see Local IL equivocation section below).</em></li>
<li><em>The local IL is correctly signed by the validator.</em></li>
<li><em>The validator is part of the IL committee.</em></li>
</ul>
</blockquote>
</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50164-proposer-5" name="p-50164-proposer-5"></a>Proposer</h3>
<ul>
<li>
<p>Slot <span class="math">n</span>, <code>t = 11s</code>: The proposer freezes its view of local ILs, and asks the EL to update its execution payload by adding transactions from its view (the exact timings will be defined after running some tests/benchmarks). Optionally, an RPC endpoint can be added to allow the proposer to request the missing local ILs from its peers (e.g., by committee index).</p>
<blockquote>
<p><em>By <strong>(1)</strong> allowing sufficient time between the local IL freeze deadline and the moment the proposer must broadcast its block with the updated execution payload, and <strong>(2)</strong> potentially adding a mechanism for the proposer to request missing local ILs from peers via an RPC endpoint, we ensure that the proposer’s IL aggregate contains <em>all</em> transactions observed local ILs, thus eliminating the need for the Δ parameter described in the  <a href="https://ethresear.ch/t/fork-choice-enforced-inclusion-lists-focil-a-simple-committee-based-inclusion-list-proposal/19870">FOCIL research post</a>.</em></p>
</blockquote>
</li>
<li>
<p>Slot <span class="math">n+1</span>, <code>t = 0s</code>: The proposer broadcasts its block with the up-to-date execution payload satisfying IL transactions over the P2P network.</p>
</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50164-attesters-6" name="p-50164-attesters-6"></a>Attesters</h3>
<ul>
<li>
<p>Slot <span class="math">n+1</span>, <code>t = 0 to 4s</code>: Attesters monitor the P2P network for the proposer’s block. Upon detecting the block, they check whether all transactions from their cached local ILs are included in the proposer’s execution payload. The <span class="math">Valid</span> function verifies if the execution payload satisfies IL validity conditions either when all transactions are present or when any missing transactions are found to be invalid when appended to the end of the payload. In these cases, attesters use the EL to verify the validity of missing transactions.</p>
<blockquote>
<p>Since we set <span class="math">Δ = 0</span> (or rather, got rid of the parameter altogether), the proposer’s execution payload must contain at least all transactions from the attester’s local ILs. Therefore, the attester does not need an IL aggregate from the proposer to perform this check.</p>
</blockquote>
</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50164-mitigations-and-edge-cases-7" name="p-50164-mitigations-and-edge-cases-7"></a>Mitigations and Edge Cases</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50164-invalidation-8" name="p-50164-invalidation-8"></a>Invalidation</h3>
<p>By having attesters use the <span class="math">Valid</span> function to check if each missing transaction would be valid when added to the end of the execution payload, we ensure that FOCIL is compatible with account abstraction.</p>
<p>To handle invalidation cases—including those introduced by full Account Abstraction (AA)—attesters use the <span class="math">Valid</span> function to verify each missing transaction from the execution payload. They check whether each missing transaction would be valid if appended to the end of the execution payload using the EL.</p>
<p>By evaluating missing transactions in the context of the execution payload’s post-state, attesters can accurately determine whether the proposer has wrongly omitted any valid transactions from the ILs. If a missing transaction is found to be invalid when appended—due to state changes caused by earlier transactions—the proposer isn’t penalized for leaving it out, and the IL validity condition is satisfied.</p>
<p>This approach effectively handles invalidation scenarios and provides a robust mechanism that accommodates the complexities of transaction validity, ensuring that all valid transactions from local ILs are included whenever possible.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50164-equivocation-9" name="p-50164-equivocation-9"></a>Equivocation</h3>
<p>To mitigate local IL equivocation, attesters should stop caching local ILs after the freeze deadline but continue to monitor the P2P network and forward multiple local ILs from the same IL committee member. If the proposer or attesters detect that a committee member has broadcast multiple local ILs (i.e., has equivocated), they should ignore all local ILs from that member.</p>
<p>We also introduce a P2P network rule that limits nodes to forwarding no more than two local ILs per validator index. This approach ensures that information about equivocation spreads while preventing spam, keeping the maximum bandwidth increase to at most <code>2×</code>.</p>
<p>It is also worth noting that the IL aggregate doesn’t exist as an explicit object in FOCIL; instead, the proposer includes transactions from the local ILs directly into the execution payload. Therefore, there is no IL aggregate that can cause equivocations.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50164-il-stuffing-10" name="p-50164-il-stuffing-10"></a>IL stuffing</h3>
<p>We also considered IL stuffing, where a builder floods the mempool with transactions they plan to invalidate later—such as by draining the account’s ETH to prevent base fee payment or causing the transactions to revert. However, this attack is risky because the builder must submit these transactions to the mempool before the target block, without knowing if they will secure the right to build it. This means another builder could win the block, include and execute the transactions intended to be invalidated, making the attack costly and impractical.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/focil-cl-el-workflow/20526">Read full topic</a></p>
]]></content:encoded>
<pubDate>Mon, 30 Sep 2024 13:51:43 +0000</pubDate>
</item>
<item>
<title>Attestation Verification Optimization</title>
<link>https://ethresear.ch/t/attestation-verification-optimization/20516</link>
<guid>https://ethresear.ch/t/attestation-verification-optimization/20516</guid>
<content:encoded><![CDATA[
<div> 关键词：BLS签名、聚合验证、随机化、防篡改攻击、证明所有权

总结:
本文主要探讨了基于BLS（Boneh-Lynn-Shacham）签名的聚合验证方法及其优化策略，特别是Tuyen优化。文章首先介绍了BLS签名的基本概念和性质，包括其线性组合验证的有效性以及验证过程中的非退化特性。接着，阐述了如何通过生成64位随机数来对每个签名进行盲化处理，以实现高效安全的签名聚合。

Tuyen优化则进一步提出了一种方法，即在聚合前将随机数与公钥而非消息进行乘法运算，以此来保护验证过程免受特定构造的公钥攻击（“rogue”攻击）。这种方法简化了聚合过程，使得验证阶段仅需一次配对计算，从而显著提高了效率。文章还提供了理论证明，确认了Tuyen优化方法在聚合验证过程中的正确性和安全性。

此外，文章还讨论了与随机化相关的其他安全考虑，如防止签名间的抵消攻击，以及如何通过证明所有权机制来增强系统的鲁棒性。最后，文章强调了在聚合签名时采取的安全措施能够有效抵御篡改攻击，并符合最佳安全实践。 <div>
<h1><a class="anchor" href="https://ethresear.ch#p-50147-attestation-verification-optimization-1" name="p-50147-attestation-verification-optimization-1"></a>Attestation Verification Optimization</h1>
<p>In the ethresearch post <a href="https://ethresear.ch/t/fast-verification-of-multiple-bls-signatures/5407">Fast Verification of Multiple BLS Signatures</a>, Vitalik describes a method for efficiently and securely aggregating multiple <strong>already aggregated</strong> BLS signatures where each aggregated signature can have a different message through randomization.</p>
<p>Randomization, or “blinding,” can be used to protect against <a href="https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-bls-signature#definitions" rel="noopener nofollow ugc">rogue public key attacks</a>, in which a specially crafted public key (the “rogue” key) is used to forge an aggregated signature. Vitalik suggests 64 bit random numbers be locally generated on the client side for scalar multiplication with each signature during aggregation.  This creates a linear combination style computation of points that upholds the validity of the verification, due to the underlying bi-linear map properties of BLS signatures. The same locally generated random values are scalar multiplied with each of the messages that get aggregated prior to final verification, resulting in equality of pairings.</p>
<p>We note that there is also an expired IETF draft <a href="https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-bls-signature" rel="noopener nofollow ugc">standard</a> that is referenced by the BLST <a href="https://github.com/supranational/blst" rel="noopener nofollow ugc">library</a>, which <a href="https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-bls-signature#name-proof-of-possession" rel="noopener nofollow ugc">suggests</a> to use a modified hash-to-curve function to implement proof of ownership as a protection against rogue public key attacks.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50147-the-tuyen-optimization-2" name="p-50147-the-tuyen-optimization-2"></a>The Tuyen Optimization</h2>
<p>The <a href="https://lodestar.chainsafe.io/" rel="noopener nofollow ugc">Lodestar</a> team, at <a href="https://chainsafe.io" rel="noopener nofollow ugc">ChainSafe</a>, wants to use a similar local randomization technique to securely aggregate multiple attestations <strong>over the same message</strong>, and have proposed <strong>The Tuyen Optimization</strong>. Similar to Vitalik’s implementation, 64-bit random numbers are generated client side and scalar multiplied with each signature during signature aggregation, creating a linear combination of points. To uphold equality in the final aggregated verification, each 64-bit random value is scalar multiplied with its corresponding public key (prior to aggregation) instead of the messages as in Vitalik’s post.  This is done to maintain a common message and avoid multiple aggregate verifications which is the heart of the optimization. It was found that aggregation and then single verification can be upwards of 4x faster due to the single pairing computation.</p>
<p>In this report we present a formal proof that the Tuyen Optimization is correct. We also explore the security aspects of applying randomness to the public keys instead of the messages as suggested by Vitalik.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50147-aggregation-over-common-message-3" name="p-50147-aggregation-over-common-message-3"></a>Aggregation over Common Message</h1>
<p>Here we discuss why a direct aggregation of BLS signatures over a common message works, and more importantly the potential vulnerability, for mitigation.  First, we briefly cover some of the properties of bilinear pairings that are useful for proving validity of aggregated BLS signature verification adapted from <a href="https://www.math.uwaterloo.ca/~ajmeneze/publications/pairings.pdf" rel="noopener nofollow ugc">Menezes</a>.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50147-pairings-4" name="p-50147-pairings-4"></a>Pairings</h2>
<p>A pairing, or bilinear map, is a function</p>
<div class="math">
e: G_1 \times G_2  \to G_T
</div>
<p>between three groups <span class="math">G_1,G_2</span>, and <span class="math">G_T</span> of prime order <span class="math">p</span>, with generators <span class="math">g_1 = \langle G_1 \rangle</span>, <span class="math">g_2 = \langle G_2 \rangle</span> and <span class="math">g_T = \langle G_T \rangle</span>, respectively.</p>
<p>When <span class="math">G_1 = G_2</span>, the pairing is called <strong>symmetric</strong>, otherwise it is called <strong>asymmetric</strong>. In our case the BLS pairing is asymmetric.</p>
<p>A <strong>bilinear pairing</strong> on <span class="math">(G_1, G_2, G_T)</span> is a map:</p>
<div class="math">
e : G_1 \times G_2 \to G_T
</div>
<p>that satisfies the following conditions:</p>
<ul>
<li><strong>Bilinearity:</strong> For all <span class="math">u \in G_1</span>, <span class="math">v \in G_2</span>, and <span class="math">a,b \in \mathbb{Z}_p</span>,</li>
</ul>
<div class="math">
e(u^a, v^b) = e(u, v)^{ab}\tag{1}
</div>
<ul>
<li><strong>Non-degeneracy:</strong> <span class="math">e(g_1, g_2) \neq 1_{G_t}</span>. (recall <span class="math">g_1</span>, <span class="math">g_2</span> are generators of the <span class="math">G_1</span> and <span class="math">G_2</span>)</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50147-bls-signatures-5" name="p-50147-bls-signatures-5"></a>BLS Signatures</h2>
<p>For a message <span class="math">m \in \{0,1\}^*</span>, a private key <span class="math">sk \in \mathbb{Z}_p</span>, and a corresponding public key</p>
<div class="math">
pk = sk \cdot g \in G_1,\tag{2}
</div>
<p>the signature on <span class="math">m</span> is</p>
<div class="math">
\sigma = sk \cdot H(m),\tag{3} \in G_2
</div>
<p>where <span class="math">H: \{0,1\}^* \rightarrow G_2</span> is a hash function mapping the message to <span class="math">G_2</span> and <span class="math">H</span> is generally a <a href="https://www.ietf.org/archive/id/draft-irtf-cfrg-hash-to-curve-10.html" rel="noopener nofollow ugc">hash to curve function</a>.</p>
<p><em>Note: The <span class="math">\cdot</span> operation denotes scalar multiplication.</em></p>
<h3><a class="anchor" href="https://ethresear.ch#p-50147-signature-verification-6" name="p-50147-signature-verification-6"></a>Signature Verification</h3>
<p>Given <span class="math">m</span>, <span class="math">pk</span>, and <span class="math">\sigma</span>, the signature is valid if</p>
<div class="math">
e(g, \sigma) =  e(pk, H(m))
</div>
<p>Where,</p>
<div class="math">
e(g, \sigma) \stackrel{3}{=} e(g, sk \cdot H(m)) \stackrel{1}{=} e(g, H(m))^{sk} \stackrel{1}{=} e(sk \cdot g, H(m)) \stackrel{2}{=} e(pk, H(m))
</div>
<h2><a class="anchor" href="https://ethresear.ch#p-50147-naive-aggregation-over-common-message-7" name="p-50147-naive-aggregation-over-common-message-7"></a>Naive Aggregation over Common Message</h2>
<p>Consider <span class="math">n</span> signatures  <span class="math">\sigma_1, \sigma_2, \dots, \sigma_n \in G_2</span> corresponding to a common message <span class="math">H(m) \in G_2</span>, private keys <span class="math">sk_1, sk_2, \dots, sk_n \in \mathbb{Z}_p</span> and public keys <span class="math">pk_1, pk_2, \dots, pk_n \in G_1</span>.</p>
<p>Multiple signatures can be combined to only require one verification as follows:</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50147-step-1-aggregate-signature-8" name="p-50147-step-1-aggregate-signature-8"></a>Step 1: Aggregate Signature</h4>
<p>The aggregated signature <span class="math">\sigma_{agg}</span> is computed as:</p>
<div class="math">
\sigma_{agg} = \sum_{i=1}^{n} \sigma_i = \sum_{i=1}^{n} (sk_i \cdot H(m)) = \left(\sum_{i=1}^{n} sk_i\right) \cdot H(m) \tag{4}
</div>
<h4><a class="anchor" href="https://ethresear.ch#p-50147-step-2-aggregate-public-key-9" name="p-50147-step-2-aggregate-public-key-9"></a>Step 2: Aggregate Public Key</h4>
<p>The corresponding aggregate public key is:</p>
<div class="math">
pk_{agg} = \sum_{i=1}^{n} pk_i = \sum_{i=1}^{n} (sk_i \cdot g)= \left( \sum_{i=1}^{n} sk_i\right) \cdot g\tag{5}
</div>
<h4><a class="anchor" href="https://ethresear.ch#p-50147-step-3-verification-10" name="p-50147-step-3-verification-10"></a>Step 3: Verification</h4>
<p>The verification of the aggregate signature is done by checking:</p>
<div class="math">
e(g, \sigma_{agg}) \stackrel{4}{=} e\left(g, \left(\sum_{i=1}^{n} sk_i\right) \cdot H(m) \right)
                                \stackrel{1}{=} e\left(\left( \sum_{i=1}^{n} sk_i\right) \cdot g, H(m) \right)
                   \stackrel{5}{=} e\left(pk_{agg}, H(m)\right)
</div>
<p>The above proves that if the equality holds the aggregated signature is valid, implying all signatures that were aggregated are also valid.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50147-blinding-with-tuyen-optimization-11" name="p-50147-blinding-with-tuyen-optimization-11"></a>Blinding With Tuyen Optimization</h1>
<p>The Lodestar team has proposed the <strong>Tuyen Optimization</strong> for signature aggregation where random values <span class="math">r_i \in \mathbb{Z}</span> are scalar multiplied with the public keys <span class="math">pk_i</span>. This is in contrast to scalar multiplying the random values with the messages <span class="math">H(m_i)</span> as suggested in the <a href="https://ethresear.ch/t/fast-verification-of-multiple-bls-signatures/5407">Ethereum Research blog post</a>. From our research we believe, <strong>The Tuyen Optimization</strong> technique is novel within the Ethereum landscape and among client implementations.  We pose this proof for public review/audit to formalize it validity before expanding its use within the ecosystem. If a more formal audit is deemed necessary that will also be accommodated.</p>
<p>In the following we show that applying blinding to the public key / signature pairs provides valid aggregated verification and adheres to security best-practices.</p>
<p>To verify the blinded, aggregated signature using <strong>The Tuyen Optimization</strong>, the verifier needs to check if</p>
<div class="math">
e\left(g, \sum_{i=1}^{n} (r_i \cdot \sigma_i)\right) = e\left(\sum_{i=1}^{n} (r_i \cdot pk_i), H(m) \right)
</div>
<p>after applying scalar multiplications with a unique 64-bit random value <span class="math">r_i</span> to each signature <span class="math">\sigma_i</span> and public key <span class="math">pk_i</span> pair (randomness is common to the pair) during aggregation. The proof that equality holds and therefor verification of the aggregate signatures is valid goes as follows:</p>
<p><span class="math">\begin{aligned} 
e\left(g, \sum_{i=1}^{n} (r_i \cdot \sigma_i)\right) &amp;\stackrel{3}{=}  e\left(g, \sum_{i=1}^{n} (r_i sk_i \cdot H(m)) \right) &amp; \\
                                &amp;\stackrel{}{=} e\left(g, \left(\sum_{i=1}^{n} r_i sk_i\right) \cdot H(m) \right) &amp;\\
                                &amp;\stackrel{1}{=} e\left(\left( \sum_{i=1}^{n} r_i sk_i\right) \cdot g, H(m) \right)  &amp;\\
                                &amp;\stackrel{}{=}  e\left(\sum_{i=1}^{n} (r_i sk_i \cdot g), H(m) \right) &amp;
                                &amp;\stackrel{2}{=} e\left(\sum_{i=1}^{n} (r_i \cdot pk_i), H(m)\right) 
\end{aligned}</span></p>
<p>The above proves that if the equality holds, the blinded, aggregated signature is valid.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50147-further-considerations-12" name="p-50147-further-considerations-12"></a>Further Considerations</h1>
<h2><a class="anchor" href="https://ethresear.ch#p-50147-eth-research-post-13" name="p-50147-eth-research-post-13"></a>Eth Research Post</h2>
<p>We note that Vitalik’s post has been “community audited” but not formally proven/audited/verified that we can find aside from <a href="https://ethresear.ch/t/security-of-bls-batch-verification">here</a>.  In the comments section of the original comment, there is consideration of aggregating over <a href="https://ethresear.ch/t/fast-verification-of-multiple-bls-signatures/5407/11">public keys</a> when the message is common. Vitalik mentions that it is well known to aggregate over public keys for common messages. We think they say this assuming over aggregated attestations because the verification of the individual signature pairs has already been done, so there is no chance to introduce a rogue key attack.  In our case that aggregation is happening prior to verification which opens up the attack surface.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50147-security-14" name="p-50147-security-14"></a>Security</h2>
<p>When aggregating signatures in the naive way attackers can craft signature/message pairs that are not legitimate and cancel each other out (add to zero) within the linear combination that is computed during aggregation.  Consequently, invalid signatures can be hidden and pass verification. Applying randomization to “<strong>blind</strong>” every signature in the aggregation is a technique used to help prevent these types of attacks.</p>
<p>There is a published security <a href="https://crypto.stanford.edu/~dabo/pubs/papers/BLSmultisig.html" rel="noopener nofollow ugc">proof</a> by Dan Boneh for aggregating securely with public keys. The above report outlines that what is done in the Tuyen Optimzation is commensurate with what is mentioned in the “Batch Verification” section. The paper addresses signing and verification, and could be used to support the security of Tuyen Optimization.  It is in our opinion that the approaches put forth by Dan Boneh are more intricate than is necessary, or required, due to the imbued security considerations of the Ethereum protocol.</p>
<p>Rogue key attacks are also referenced in the BLS IETF <a href="https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-bls-signature-04#section-3" rel="noopener nofollow ugc">draft</a> and in the Ethereum Specification where proof-of-possession is considered necessary for mitigation when aggregating signatures. Specifically, the Beacon Node Specifications require that validator signatures be verified prior to being allowed to attest to chain state.  That check is done in <a href="https://github.com/ethereum/consensus-specs/blob/0c5ad81145fea58504b6db9e8c73214a9fcb331a/specs/phase0/beacon-chain.md?plain=1#L1882" rel="noopener nofollow ugc">apply_deposits</a></p>
<p>It is our opinion, that between proof-of-possession and the blinding techniques involved in <strong>The Tuyen Optimization</strong>, rogue key attacks are mitigated and the solution meets security best-practices.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50147-acknowledgements-15" name="p-50147-acknowledgements-15"></a>Acknowledgements</h2>
<p>Special thanks to some amazing people who helped put this together:</p>
<ul>
<li>Tuyen Nguyen <a href="https://github.com/twoeths" rel="noopener nofollow ugc">@twoeths</a></li>
<li><a href="https://www.linkedin.com/in/sebastian-lindner-594363110/" rel="noopener nofollow ugc">Sebastian Lindner</a></li>
<li>Cayman <a class="mention" href="https://ethresear.ch/u/wemeetagain">@wemeetagain</a></li>
</ul>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/attestation-verification-optimization/20516">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sat, 28 Sep 2024 12:43:43 +0000</pubDate>
</item>
<item>
<title>Where's the home staking bandwidth research?</title>
<link>https://ethresear.ch/t/wheres-the-home-staking-bandwidth-research/20507</link>
<guid>https://ethresear.ch/t/wheres-the-home-staking-bandwidth-research/20507</guid>
<content:encoded><![CDATA[
<div> 关键词：研究、家庭质押、带宽要求、基础设施、全球互联网

总结:

本文讨论了关于家庭质押中带宽需求的研究空白。首先，文章提出了一些关键问题，包括无法运行家庭验证器的地方、低带宽对奖励和参与度的影响、上下载速度的重要性差异、消费者互联网套餐上传性能的现状以及不同地区和国家的带宽目标设定。

在带宽分析方面，文章强调了需要考虑运行多个验证器（如5或10个）的影响，以及是否应将星链（Starlink）排除在分析之外，因为其在市场上的独特地位可能导致依赖性增加。此外，文章还讨论了制定带宽要求以促进中立性的科学方法，并提出目标应基于全球互联网性能的特定百分位数。

文章旨在推动社区对家庭质押基础设施的带宽需求进行更深入、更科学的研究，以确保网络的可靠性和公平性，同时考虑到全球各地的不同条件和挑战。通过设定合理的带宽目标，可以为未来可能出现的迁移需求提供指导，增强系统的适应性和韧性。 <div>
<p>As a non-researcher but committed long-term observer, I’ve been surprised to see a lack of research focused on home staking bandwidth requirements. Please let me know if these studies exist and I haven’t seen them.</p>
<p>Here are some starter questions for home staking bandwidth research that seem interesting to me:</p>
<ul>
<li>Where can’t you run a home validator today?</li>
<li>What happens if your bandwidth is slightly below where it needs to be? How sensitive are rewards/participation to bandwidth requirements? Is this symmetric in upload/download or is one more important?</li>
<li>Many consumer internet packages have significantly worse upload. But how much worse? imo we need a rigorous target for bandwidth requirements. Do we target, say, 10th decile of global upload speeds in towns with population over 10,000, excluding countries in bottom 5% of GDP? or maybe excluding, say, Africa? We use the phrase “home staking” but for which homes do we intend support?</li>
<li>How sensitive are these analyses to running N validators at home instead of 1? Do we want our target to aim for a particular number of home validators on the same internet connection, like 5 or 10?</li>
<li>Can we agree to omit Starlink from our analyses and target bandwidth requirements due to its extraordinary position as the monopolistic apex outlier? Starlink is so good, and will likely remain so far ahead of other options for so many years, that if we allow Starlink to satisfy our chosen target, then Ethereum could come to rely on Starlink.</li>
</ul>
<p>I would like to see us scientifically develop a rigorous definition of staking minimum bandwidth requirements to help maximize credible neutrality, especially so we have an solid idea of <strong>where the entire validator set can and can’t migrate should a disaster warrant such a migration.</strong></p>
            <p><small>3 posts - 3 participants</small></p>
            <p><a href="https://ethresear.ch/t/wheres-the-home-staking-bandwidth-research/20507">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 13:45:24 +0000</pubDate>
</item>
<item>
<title>Steelmanning a blob throughput increase for Pectra</title>
<link>https://ethresear.ch/t/steelmanning-a-blob-throughput-increase-for-pectra/20499</link>
<guid>https://ethresear.ch/t/steelmanning-a-blob-throughput-increase-for-pectra/20499</guid>
<content:encoded><![CDATA[
<div> 关键词：Pectra、硬分叉、块重组、数据分片、费用预测

总结:
本文围绕Pectra网络的当前状态和对即将进行的硬分叉中关于数据包（blob）目标或限制增加的讨论进行了深入分析。以下是关键点的总结：

1. **硬分叉背景**：讨论的核心在于如何继续扩展去中心化应用（DA），并为现有的第二层（L2）解决方案及其应用提供足够的空间以实现进一步的扩展。

2. **块重组趋势**：过去一年内，共发生了约5900次块重组事件，占总区块的0.225%，同时有14426次槽位未被提议者接收到信号，占比0.549%。平均每月发生492次重组和1202次槽位错过事件。

3. **硬分叉后重组减少**：尽管最近的硬分叉（Dencun升级）可能导致重组事件增加的预期，实际情况却是重组事件数量减半。这可能归因于核心开发团队持续改进客户端软件。

4. **数据包对重组的影响**：早期的研究表明，带有6个数据包的区块比无数据包区块更频繁地发生重组。然而，更新后的分析显示，这种差异已显著减少，单个数据包与多个数据包的区块在重组率上几乎无明显区别。

5. **数据包分布**：数据显示，大多数区块要么包含0个数据包，要么包含6个数据包，而包含1到5个数据包的区块相对较少。但自某项改进以来，这种情况有所改善，极端情况下的区块数量减少了。

通过这些分析，文章强调了在考虑增加数据包限制时，需要权衡网络扩展需求、现有L2解决方案的成本效益以及避免对未来采用产生不利影响的可能性。 <div>
<h1><a class="anchor" href="https://ethresear.ch#p-50111-steelmanning-a-blob-throughput-increase-for-pectra-1" name="p-50111-steelmanning-a-blob-throughput-increase-for-pectra-1"></a>Steelmanning a blob throughput increase for Pectra</h1>
<p>With the discussions about the Pectra hardfork scope continuing, I want to provide some empirical input on the current state of the network.<br />
I’ll try to do so by answering some commonly raised questions that arise in discussions on the proposed blob target/limit increase for Pectra.</p>
<p><strong>The arguments for shipping <a href="https://eips.ethereum.org/EIPS/eip-7691">EIP-7691</a> in Pectra are:</strong></p>
<ul>
<li><strong>Continue scaling DA</strong> - with <a href="https://github.com/ethereum/EIPs/blob/b4da6a963f0afc5e78b6071ec4c6e5ae7cada145/EIPS/eip-4844.md">EIP-4844</a>, we have only set the foundation.
<ul>
<li>Provide existing L2s and their apps enough blob space for further scaling.</li>
<li>Avoid creating a precedent of “<em>blob fees can explode and are unpredictable</em>” (h/t Ansgar); this harms future adoption if rollups have to account for rare fee spikes over extended periods.</li>
</ul>
</li>
<li><strong>The number of reorgs has been trending down since Dencun.</strong></li>
<li><strong>The impact of blobs on reorgs has decreased as well.</strong></li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50111-how-did-the-number-of-reorgs-evolve-over-time-2" name="p-50111-how-did-the-number-of-reorgs-evolve-over-time-2"></a>How did the number of reorgs evolve over time?</h2>
<blockquote>
<p>reorged = “nodes saw a block by the proposer of the respective slot”<br />
missed = “no sign that the proposer was online”</p>
</blockquote>
<ul>
<li>Within the last 365 days, 5,900 blocks were reorged.</li>
<li>This equates to 0.225% of the blocks in that time interval.</li>
<li>At the same time, 14,426 slots were missed, representing 0.549%.</li>
<li>On average, we observe 492 reorgs and 1,202 missed slots per month.</li>
</ul>
<p>The number of reorgs has been decreasing, which is a positive development, though not surprising, as core devs continuously improve client software. Interestingly, contrary to expectations that the most recent hard fork (= Dencun) would lead to a significant rise in reorgs, we actually observed the opposite trend.</p>
<p><strong>Since the Dencun upgrade, the number of reorgs halved.</strong></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/b/4b1dde8030fe4c0167480cd65d800b986c292802.png" title="combined_reorgs_missed"><img alt="combined_reorgs_missed" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/4/b/4b1dde8030fe4c0167480cd65d800b986c292802_2_571x500.png" width="571" /></a></div><p></p>
<p>It’s challenging to identify the exact reason for the change in trend, but it may be attributed to the ongoing improvements made by core devs to their client software.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50111-whats-the-impact-of-blobs-on-reorgs-3" name="p-50111-whats-the-impact-of-blobs-on-reorgs-3"></a>What’s the impact of blobs on reorgs?</h2>
<p><a href="https://ethresear.ch/t/big-blocks-blobs-and-reorgs/19674">Initial analysis</a> conducted a few months after the Dencun hardfork showed that blocks with 6 blobs were reorged 3 times more frequently than 0-blob blocks. In general, we observed that the reorg rate has increased steadily with a growing number of blobs.</p>
<p>Updating this analysis presents a different picture today. Even though we still see that 6-blob blocks are reorged more frequently than 0-blob or 1-blob blocks, the numbers have decreased significantly, showing no substantial difference between blocks with one blob and those with six blobs.<br />
We still observe a small difference in the reorg rate for 0-blob blocks and x-blob blocks (where x &gt; 0).</p>
<p><img alt="reorgrate_animation" class="animated" height="301" src="https://ethresear.ch/uploads/default/original/3X/0/6/0609208e43c3fb8f94ebcc46b3b99a4fc79eda27.gif" width="690" /></p>
<h2><a class="anchor" href="https://ethresear.ch#p-50111-how-well-are-blobs-distributed-over-blocks-4" name="p-50111-how-well-are-blobs-distributed-over-blocks-4"></a>How well are blobs distributed over blocks?</h2>
<p>Plotting the distribution, we can see that most blobs contain either 0 or 6 blobs, with blocks containing 1 to 5 blobs representing the minority. However, the situation has improved since the <a href="https://ethresear.ch/t/big-blocks-blobs-and-reorgs/19674">last study</a>, with fewer slots at the extremes of 0 blobs and 6 blobs.</p>
<p><img alt="all_blobs_day" class="animated" height="301" src="https://ethresear.ch/uploads/default/original/3X/c/c/ccf6c5d3f8fad8bbd1330b98c1bb8e804fc19014.gif" width="690" /></p>
<h2><a class="anchor" href="https://ethresear.ch#p-50111-related-work-5" name="p-50111-related-work-5"></a>Related work</h2>
<div class="md-table">
<table>
<thead>
<tr>
<th>Title</th>
<th>Url</th>
</tr>
</thead>
<tbody>
<tr>
<td>On Attestations, Block Propagation, and Timing Games</td>
<td><a href="https://ethresear.ch/t/on-attestations-block-propagation-and-timing-games/20272">ethresearch</a></td>
</tr>
<tr>
<td>Blobs, Reorgs, and the Role of MEV-Boost</td>
<td><a href="https://ethresear.ch/t/blobs-reorgs-and-the-role-of-mev-boost/19783">ethresearch</a></td>
</tr>
<tr>
<td>Big blocks, blobs, and reorgs</td>
<td><a href="https://ethresear.ch/t/big-blocks-blobs-and-reorgs/19674">ethresearch</a></td>
</tr>
<tr>
<td>On Block Sizes, Gas Limits and Scalability</td>
<td><a href="https://ethresear.ch/t/on-block-sizes-gas-limits-and-scalability/18444">ethresearch</a></td>
</tr>
<tr>
<td>The Second-Slot Itch - Statistical Analysis of Reorgs</td>
<td><a href="https://ethresear.ch/t/the-second-slot-itch-statistical-analysis-of-reorgs/16333/12">ethresearch</a></td>
</tr>
</tbody>
</table>
</div>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/steelmanning-a-blob-throughput-increase-for-pectra/20499">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 26 Sep 2024 13:12:23 +0000</pubDate>
</item>
<item>
<title>Proposal: Delay stateRoot Reference to Increase Throughput and Reduce Latency</title>
<link>https://ethresear.ch/t/proposal-delay-stateroot-reference-to-increase-throughput-and-reduce-latency/20490</link>
<guid>https://ethresear.ch/t/proposal-delay-stateroot-reference-to-increase-throughput-and-reduce-latency/20490</guid>
<content:encoded><![CDATA[
<div> 关键词：Ethereum、stateRoot、block production、throughput、latency

总结：

文章提出了一项关于以太坊区块链技术的提案，旨在通过延迟对区块状态根（stateRoot）的引用，以期增加网络吞吐量并减少延迟。以下是该提案的主要要点：

1. **当前问题**：当前每个区块头包含一个表示执行了所有交易后的状态的状态根，这要求区块构建者和中介（如MEV-Boost中继）计算状态根，这一过程耗时且增加了验证时间。

2. **提案建议**：修改区块结构，使得区块n中的状态根引用的是上一区块（n-1）执行完交易后开始的状态，而非结束时的状态。这样可以将状态根计算从关键路径中移除，减少链端验证的延迟，释放资源以提高网络吞吐量。

3. **技术细节**：验证区块n时，节点需要确保状态根与执行块n-1后产生的状态匹配。交易顺序不变，仍然基于n-1块的状态进行。

4. **动机与优势**：移除不必要的状态根验证工作可以提高网络效率，节省时间用于执行操作，进而可能降低区块时间，改善用户体验。同时，减少了对构建区块和验证区块的硬件要求，有助于提高网络稳定性和健康状况。

5. **潜在影响与挑战**：一些依赖实时状态信息的应用（如轻客户端、SPV客户端等）可能会受到影响，需要调整策略或实施。此外，需要对现有应用进行适应性调整，包括可能的网络硬分叉需求。对于状态无状态客户端协议，可能需要容忍最多一区块的延迟。

通过这一提案，以太坊社区期望在不影响大多数应用的前提下，实现网络性能的显著提升，特别是通过减少验证时间，提高整体网络效率和用户体验。 <div>
<h1><a class="anchor" href="https://ethresear.ch#p-50080-proposal-delay-stateroot-reference-to-increase-throughput-and-reduce-latency-1" name="p-50080-proposal-delay-stateroot-reference-to-increase-throughput-and-reduce-latency-1"></a>Proposal: Delay stateRoot Reference to Increase Throughput and Reduce Latency</h1>
<p>By: <a href="https://x.com/_charlienoyes" rel="noopener nofollow ugc">Charlie Noyes</a>, <a href="https://x.com/MaxResnick1" rel="noopener nofollow ugc">Max Resnick</a></p>
<h2><a class="anchor" href="https://ethresear.ch#p-50080-introduction-2" name="p-50080-introduction-2"></a>Introduction</h2>
<p>Right now, each block header includes a <code>stateRoot</code> that represents the state after executing all transactions within that block. This design requires block builders and intermediaries (like MEV-Boost relays) to compute the <code>stateRoot</code>, which is computationally intensive and adds significant latency during block production.</p>
<p>This proposal suggests modifying the Ethereum block structure so that the <code>stateRoot</code> in block <code>n</code> references the state at the beginning of the block (i.e., after executing the transactions in block <code>n - 1</code>, rather than the state at the end of the block).</p>
<p>By delaying the <code>stateRoot</code> reference by one block, we aim to remove the <code>stateRoot</code> calculation from the critical path of block verification at the chain tip, thereby <strong>reducing L1 latency and freeing up capacity to increase L1 throughput.</strong></p>
<h2><a class="anchor" href="https://ethresear.ch#p-50080-technical-specification-high-level-3" name="p-50080-technical-specification-high-level-3"></a>Technical Specification (High-Level)</h2>
<p>When validating block <code>n</code>, nodes ensure that the <code>stateRoot</code> matches the state resulting from executing block <code>n-1</code> (i.e., the pre-state root of block <code>n</code>).</p>
<p>To be clear, there is no change to exeuction ordering. Transactions in block <code>n</code> are still applied to the state resulting from block <code>n-1</code>.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50080-motivation-4" name="p-50080-motivation-4"></a>Motivation</h2>
<p><code>stateRoot</code> calculation and verification is unnecessary work on the critical path of block production. A builder cannot propose a block on MEV boost without first calculating the <code>stateRoot</code> and the attestation committee cannot verify a block without computing the <code>stateRoot</code> to compare with the proposed <code>stateRoot</code>. <code>stateRoot</code> calculation itself accounts for approximately <strong>half</strong> of time spent by all consensus participants working at the tip. Moreover, whatever latency implications the <code>stateRoot</code> calculation imposes are paid twice on the critical path: once at the block building stage and then again during verification.</p>
<ul>
<li>
<ul>
<li>When block builders submit blocks to relays, they are required to provide the calculated <code>stateRoot</code>. From surveying three of the four largest builders, each spends on average only 40%-50% of their time actually building each block, and the rest on <code>stateRoot</code> calculation.</li>
</ul>
</li>
<li>
<ul>
<li>When MEV-Boost relays recieve blocks from builders, they are supposed to verify their correctness. In Flashbots’ relay, also approximately half of the ~100ms (p90) verification time is spent on the <code>stateRoot</code> calculation.</li>
</ul>
</li>
<li>
<ul>
<li>When validators receive a new block, or when non-MEV-Boost validators (“home builders”) produce a block, they are also required to re-verify its execution and its <code>stateRoot</code>. Commodity hardware Reth nodes spend approximately 70% of its time in live-sync on the <code>stateRoot</code> (remainder on execution).</li>
</ul>
</li>
</ul>

  <div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/9/a94d0412f9a5ba4edd2674fa0c9e7227711a7d65.png" title="RETH benchmarks"><img alt="RETH benchmarks" height="362" src="https://ethresear.ch/uploads/default/optimized/3X/a/9/a94d0412f9a5ba4edd2674fa0c9e7227711a7d65_2_690x362.png" width="690" /></a></div>
  ~70% of RETH's block processing time is spent on `stateRoot` calculation.

<p>These participants - builders, relays, and validators - are highly latency sensitive. They operate under tight timing constraints around slot boundaries (particularly with the <a href="https://ethresear.ch/t/on-attestations-block-propagation-and-timing-games/20272">incresaing prevalence of timing games</a>).</p>
<p>The latency introduced by <code>stateRoot</code> verification at the tip is unnecessary and removing it could allow us to improve the health of the block production pipeline, and network stability.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50080-benefits-of-delaying-the-stateroot-5" name="p-50080-benefits-of-delaying-the-stateroot-5"></a>Benefits of Delaying the <code>stateRoot</code></h3>
<ul>
<li>Higher L1 throughput, because the time currently spent verifying the <code>stateRoot</code> can be re-allocated to execution. <code>stateRoot</code> verification would be pipelined to occur in parallel with the next slot (i.e. during time that nodes are currently idle). Bandwidth requirement increases and state growth would also need to be acceptable before activating a throughput increase.</li>
<li>Time saved by pipelining the <code>stateRoot</code> could also be allocated towards lowering slot times - improving L1 Ethereum UX, and likely resulting in tighter spreads for users of decentralized exchanges.</li>
<li>Builders and relays avoid an unnecessary latency speedbump. Both are highly latency-sensitive actors. We want to minimize the sophistication it takes to be a relay or validator. Removing <code>stateRoot</code> latency from the critical path of block verification means they will no longer have to worry about optimizing it, improving the health and efficiency of the block production pipeline.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50080-potential-downsides-and-concerns-6" name="p-50080-potential-downsides-and-concerns-6"></a>Potential Downsides and Concerns</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50080-impacted-applications-7" name="p-50080-impacted-applications-7"></a>Impacted Applications</h3>
<ol>
<li><strong>Light Clients and SPV Clients</strong></li>
</ol>
<ul>
<li><strong>Impact</strong>: These clients rely on the latest <code>stateRoot</code> to verify transactions and account balances without downloading the entire blockchain. A one-block delay introduces a latency in accessing up-to-date state information. Cross-chain communication protocols (like bridges that utilize light clients) would also experience this delay.</li>
<li>
<ul>
<li><strong>Consideration</strong>: We do not see an obvious issue with light clients being delayed by a single block.</li>
</ul>
</li>
</ul>
<ol start="2">
<li><strong>Stateless Client Protocols</strong></li>
</ol>
<ul>
<li><strong>Impact</strong>: Stateless clients rely on the latest <code>stateRoot</code> to verify transaction witnesses. A one-block delay could affect immediate transaction validation.</li>
<li>
<ul>
<li><strong>Consideration</strong>: If these clients can tolerate a one-block delay, the impact may be minimal. This aligns with ongoing discussions in the statelessness roadmap.</li>
</ul>
</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50080-rationale-8" name="p-50080-rationale-8"></a>Rationale</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50080-why-this-approach-9" name="p-50080-why-this-approach-9"></a>Why This Approach?</h3>
<ul>
<li><strong>Efficiency</strong>: Removing <code>stateRoot</code> computation from the critical path significantly reduces block verification time.</li>
<li><strong>Simplicity</strong>: The change is straightforward in terms of protocol modification, affecting only the placement of the <code>stateRoot</code> reference. This is backwards-compatible with the existing block production pipeline (i.e., native building and MEV-Boost). Other proposals which include execution pipelining, like <a href="https://eips.ethereum.org/EIPS/eip-7732" rel="noopener nofollow ugc">ePBS</a>, are significantly broader in scope and complexity. Delaying the <code>stateRoot</code> is a simpler change we can make with immediate benefit and little risk.</li>
<li><strong>Minimal Disruption</strong>: While some applications may be affected, we think most (all?) can tolerate a one-block delay without significant issues. We should collect feedback from application developers to validate this.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50080-backwards-compatibility-and-transition-10" name="p-50080-backwards-compatibility-and-transition-10"></a>Backwards Compatibility and Transition</h3>
<ul>
<li><strong>Hard Fork Requirement</strong>: This change is not backwards compatible and would require a network hard fork.</li>
<li><strong>Application Adaptations</strong>: Affected applications (light clients, Layer 2 solutions, stateless clients) may need to adjust their protocols or implementations.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50080-request-for-feedback-11" name="p-50080-request-for-feedback-11"></a>Request for Feedback</h2>
<p>We invite the community to provide feedback on this proposal, particularly:</p>
<ul>
<li><strong>Feasibility</strong>: Are there technical challenges that might impede the implementation of this change?</li>
<li><strong>Upside</strong>: How much throughput will we be able to eke out from pipelining <code>stateRoot</code> calculation, and reallocating the time to exeuction?</li>
<li><strong>Affected Applications</strong>: We don’t obviously see a class of widely used applications which would be affected. We hope any developers whose applications do depend on same-block <code>stateRoot</code> will let us know.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50080-next-steps-12" name="p-50080-next-steps-12"></a>Next Steps</h2>
<p>We plan to formalize this proposal into an EIP for potential inclusion in Pectra B.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50080-acknowledgements-13" name="p-50080-acknowledgements-13"></a>Acknowledgements</h3>
<p>Thanks to Dan Robinson, Frankie, Robert Miller, and Roman Krasiuk for feedback and input on this proposal.</p>
            <p><small>14 posts - 10 participants</small></p>
            <p><a href="https://ethresear.ch/t/proposal-delay-stateroot-reference-to-increase-throughput-and-reduce-latency/20490">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 25 Sep 2024 17:12:42 +0000</pubDate>
</item>
<item>
<title>Understanding Minimum Blob Base Fees</title>
<link>https://ethresear.ch/t/understanding-minimum-blob-base-fees/20489</link>
<guid>https://ethresear.ch/t/understanding-minimum-blob-base-fees/20489</guid>
<content:encoded><![CDATA[
<div> 关键词：blob费用、最低基费用、网络拥堵、交易费用、市场响应时间

总结:

文章探讨了以太坊网络中关于数据包（blobs）的费用问题，特别是最低基费用的设置及其影响。主要观点如下：

1. **交易费用结构**：交易包含数据包时，不仅需要支付执行费用，还需要支付网络上的数据包费用。尽管执行费用相对较低，数据包费用则根据网络拥堵程度而变化。当网络拥堵时，数据包费用可能显著增加。

2. **市场响应时间**：数据包费用市场的调整速度较慢，这导致在网络拥堵期间，数据包费用未能有效反映需求，从而引发了所谓的“冷启动”问题。为解决这一问题，提议设置最低数据包基费用。

3. **最低基费用的影响**：最低基费用的设定旨在加速数据包费用市场对需求变化的响应，减少网络拥堵时的不确定性。根据分析，这种调整对大多数数据包提交者的影响较小，对那些成本效益较低的链影响更大。

4. **市场效率与优化**：文章讨论了如何通过改进数据包费用市场和优化算法来提高网络效率，特别是在处理大量数据包的场景下。同时，提出了关于是否应该从优先级气体拍卖中获取收入的讨论。

5. **未来展望**：文章提出了几个开放性问题，包括数据包费用市场是否能在未来达到平衡，以及是否会出现更多的“冷启动”问题等，这些都是未来研究和讨论的重点。

文章的核心在于探讨如何优化以太坊网络中数据包的费用机制，以提高整体网络效率和用户体验，同时考虑不同数据包提交者可能面临的成本差异。 <div>
<h1><a class="anchor" href="https://ethresear.ch#p-50078-understanding-minimum-blob-base-fees-1" name="p-50078-understanding-minimum-blob-base-fees-1"></a>Understanding Minimum Blob Base Fees</h1>
<hr />
<p>by <a href="https://x.com/data_always" rel="noopener nofollow ugc">Data Always</a> - <a href="https://www.flashbots.net/" rel="noopener nofollow ugc">Flashbots Research</a></p>
<p>Special thanks to <a href="https://x.com/0xQuintus" rel="noopener nofollow ugc">Quintus</a>, <a href="https://x.com/sarahalle_" rel="noopener nofollow ugc">Sarah</a>, <a href="https://ethresear.ch/u/jcschlegel/summary">Christoph</a>, and <a href="https://x.com/potuz_eth" rel="noopener nofollow ugc">Potuz</a> for review and discussions.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50078-tldr-2" name="p-50078-tldr-2"></a>tl;dr</h3>
<blockquote>
<p>The myth that blobs pay zero transaction fees is false. Depending on type of data being posted and the state of gas prices, it costs submitters between $0.10 and $3.00 per blob in mainnet execution fees. <a href="https://ethereum-magicians.org/t/eip-7762-increase-min-base-fee-per-blob-gas/20949" rel="noopener nofollow ugc">EIP-7762</a>, the implementation of a ~$0.01 minimum blob base fee, should have a minimal impact on the market, yet vastly reduce the time that the blob market spends in PGAs during surges of demand while blob usage remains below the blob target.</p>
</blockquote>
<hr />
<p>Proposals to set a blobspace reserve price are <a href="https://x.com/EffortCapital/status/1829663405218693624" rel="noopener nofollow ugc">controversial</a> <a href="https://x.com/ryanberckmans/status/1829878445553402179" rel="noopener nofollow ugc">in the</a> <a href="https://x.com/0xfoobar/status/1830313459076182183" rel="noopener nofollow ugc">community</a>, but this may stem from a <a href="https://x.com/ryanberckmans/status/1830643363126587454" rel="noopener nofollow ugc">misunderstanding</a> of how blobs find their way on chain. A common impression is that blobs are currently contributing zero fees to the protocol, but this is misguided and only true when we restrict our analysis to blobspace fees.</p>
<p>Although the blobspace fee market has been slow to reach the targeted level of demand, thus suffering from the <a href="https://ethresear.ch/t/eip-4844-fee-market-analysis/15078">cold-start problem</a> initially predicted by <a href="https://x.com/DavideCrapis" rel="noopener nofollow ugc">Davide Crapis</a> a year before Deneb, blob carrying transactions still pay <a href="https://www.blocknative.com/blog/blobsplaining" rel="noopener nofollow ugc">mainnet gas fees</a>, both for execution and priority. The current concern, raised by <a href="https://x.com/MaxResnick1" rel="noopener nofollow ugc">Max Resnick</a>, is that the hard limit of six blobs per block and the slow response time of the blobspace fee market creates the potential for long-lasting priority gas auctions (PGAs) when the network sees periods of high demand. During these PGAs it becomes much harder for L2s to price their transactions, and when coupled with the current strict <a href="https://x.com/titanbuilderxyz/status/1809231370243211601" rel="noopener nofollow ugc">blob mempool rules</a>, blob inclusion becomes less predictable.</p>
<p><a href="https://ethereum-magicians.org/t/eip-7762-increase-min-base-fee-per-blob-gas/20949" rel="noopener nofollow ugc">EIP-7762</a> aims to minimize future dislocations between the price of blobspace and blob demand until the adoption of L2s pushes us past the cold-start problem. The current configuration, with the minimum blobspace base fee set to 1 wei, requires at least 30 minutes of fully saturated blocks for blobspace fees to reach $0.01 per blob and to begin to influence blob pricing dynamics. Under the current system, when surges of demand arise the network sees a reversion to unpredictable PGAs as L2s fight for timely inclusion.</p>
<p>As an example, on June 20th the network saw its <a href="https://www.blocknative.com/blog/june-20th-blob-contention-event-retrospective" rel="noopener nofollow ugc">second blob inversion event</a>, stemming from the <a href="https://www.theblock.co/post/302945/arbitrum-cashed-in-on-layerzero-airdrop-but-the-boost-was-short-lived" rel="noopener nofollow ugc">LayerZero airdrop</a>. It took six hours of excess demand for blobs until the network reached equilibrium.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/e/5ed1c53aac66015377915a554d424c352fbeab0e.png" title="Blobspace Fees During the LayerZero Airdrop"><img alt="Blobspace Fees During the LayerZero Airdrop" height="337" src="https://ethresear.ch/uploads/default/optimized/3X/5/e/5ed1c53aac66015377915a554d424c352fbeab0e_2_689x337.png" width="689" /></a></div><p></p>
<blockquote>
<p>Source: <a href="https://dune.com/queries/4050212/6819676?category=abstraction&amp;namespace=blobs&amp;id=ethereum.blobs" rel="noopener nofollow ugc">https://dune.com/queries/4050212/6819676</a></p>
</blockquote>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-50078-the-state-of-blob-transaction-fees-3" name="p-50078-the-state-of-blob-transaction-fees-3"></a>The State of Blob Transaction Fees</h3>
<p>Six months post-Deneb blobspace usage <a href="https://dune.com/queries/3757544/6319515" rel="noopener nofollow ugc">remains below the target</a>. As a result, the blobspace base fee has remained low and the majority of blobs have incurred negligible blobspace gas fees. To date, there have only been three weeks where the average cost of blobspace rose above $0.01 per blob: the weeks of March 25 and April 1 during the <a href="https://www.coindesk.com/tech/2024/03/28/ethereum-hit-by-blobscriptions-in-first-stress-test-of-blockchains-new-data-system/" rel="noopener nofollow ugc">blobscription craze</a> and the week of June 17th during the LayerZero airdrop.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/5/559e1ee7311c976cd4e130f54e920c7e07c9ce8e.png" title="Average Blobspace Fees per Blob"><img alt="Average Blobspace Fees per Blob" height="325" src="https://ethresear.ch/uploads/default/optimized/3X/5/5/559e1ee7311c976cd4e130f54e920c7e07c9ce8e_2_690x325.png" width="690" /></a></div><p></p>
<blockquote>
<p>Source: <a href="https://dune.com/queries/4050128/6819454?category=abstraction&amp;namespace=blobs&amp;id=ethereum.blobs" rel="noopener nofollow ugc">https://dune.com/queries/4050128/6819454</a></p>
</blockquote>
<p>In contrast to fees in blobspace, blob carrying transactions (also known as Type-3) are still required to pay gas fees for execution on mainnet. Despite gas prices falling to a multi-year low, the average blob pays between $0.50 to $3.00 in execution fees. When compared to the <a href="https://0xpantarhei.substack.com/i/145648175/is-using-blobs-always-more-cost-effective-than-calldata" rel="noopener nofollow ugc">price of call data</a> historically posted by L2s these costs are insignificant and blobs are essentially fully subsidized by the network, yet this small cost is important when framing a minimum base fee for blobs.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/0/e04ba54d1e4aedf3447c63dd1569912eaf3b1fec.png" title="Average Execution Fees per Blob"><img alt="Average Execution Fees per Blob" height="314" src="https://ethresear.ch/uploads/default/optimized/3X/e/0/e04ba54d1e4aedf3447c63dd1569912eaf3b1fec_2_690x314.png" width="690" /></a></div><p></p>
<blockquote>
<p>Source: <a href="https://dune.com/queries/4050088/6819431" rel="noopener nofollow ugc">https://dune.com/queries/4050088/6819431</a></p>
</blockquote>
<p>If we go a step further and segment the execution cost of blob carrying transactions by their blob contents we see that market is highly heterogeneous. Transactions that carry only one blob pay the highest fees per blob, while transactions that carry five or six blobs pay little-to-no fees per blob. In fact these five or six blob carrying transactions pay <a href="https://dune.com/queries/4053870/6841415" rel="noopener nofollow ugc">significantly lower total fees</a>.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/d/2d2ed2bc476512d73167911f3d279200c9dc7c49.png" title="Execution Cost per Blob for Blob Carrying Transactions"><img alt="Execution Cost per Blob for Blob Carrying Transactions" height="296" src="https://ethresear.ch/uploads/default/optimized/3X/2/d/2d2ed2bc476512d73167911f3d279200c9dc7c49_2_690x296.png" width="690" /></a></div><p></p>
<blockquote>
<p>Source: <a href="https://dune.com/queries/4053870/6825759" rel="noopener nofollow ugc">https://dune.com/queries/4053870/6825747</a></p>
</blockquote>
<p>A large factor in this discrepancy is the variance in blob <a href="https://dune.com/queries/4062897/6841224" rel="noopener nofollow ugc">submission strategies of different entities</a>: Base, OP Mainnet, and Blast, as well as many smaller L2s, are extremely financially efficient because they post their data to an EOA which requires only 21,000 mainnet gas for execution regardless of blob count, but these transactions are not well suited for fraud proofs. These chains account for the vast majority of transactions that carry five or more blobs, pushing down the perceived price of submitting many blobs in one transaction. By contrast, L2s that post more complex data to better enable fraud proofs, for instance: Arbitrum, StarkNet, Scroll, ZkSync Era, Taiko, and Linea, use <a href="https://dune.com/queries/4097909/6900990" rel="noopener nofollow ugc">significantly more mainnet gas</a> and tend to <a href="https://dune.com/queries/4097685/6900477" rel="noopener nofollow ugc">submit fewer blobs</a> (often only a single blob) per transaction.</p>
<hr />
<p>Following from the statistics above, if we combine the blobspace and execution fees on a per transaction basis, we see that outside of the brief surges in demand for blobs, which would not have been affected by adding a minimum base fee, the current distribution of fees paid is almost entirely concentrated in execution fees. This demonstrates that the blobspace fee market is currently non-functional and that there is room to raise the minimum cost of blob gas without meaningfully raising the total cost paid by blobs.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/4/54a1b2fb5c880dfa45e1d1263db446697dc45501.png" title="Blobspace Share of Transaction Fees Paid by Blobs"><img alt="Blobspace Share of Transaction Fees Paid by Blobs" height="325" src="https://ethresear.ch/uploads/default/optimized/3X/5/4/54a1b2fb5c880dfa45e1d1263db446697dc45501_2_690x325.png" width="690" /></a></div><p></p>
<blockquote>
<p>Source: <a href="https://dune.com/queries/4034097/6792385" rel="noopener nofollow ugc">https://dune.com/queries/4034097/6792385</a></p>
</blockquote>
<p>By contrast, if we focus on the periods when the blobspace fee market entered price discovery we see that the majority of fee density rapidly transitions into blobspace fees. When the market works, it appears to work well. As such, the most valuable issue to address is the repeated cold-start problem—where the market currently finds itself.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/c/7c26bb97930ae40a5902839d4d2f933b87c02fcc.png" title="Blobspace Share of Transaction Fees Paid by Blobs When Active"><img alt="Blobspace Share of Transaction Fees Paid by Blobs When Active" height="325" src="https://ethresear.ch/uploads/default/optimized/3X/7/c/7c26bb97930ae40a5902839d4d2f933b87c02fcc_2_690x325.png" width="690" /></a></div><p></p>
<blockquote>
<p>Source: <a href="https://dune.com/queries/4060561/6837143" rel="noopener nofollow ugc">https://dune.com/queries/4060561/6837143</a></p>
</blockquote>
<p>When the blobspace fee market is in an execution fee-dominant environment it benefits blob submitters who post less execution data—mostly OP Stack chains. It also complicates the block building process: historically many algorithms were deciding blob inclusion by priority fee per gas, but since the mainnet gas usage of these transactions varied greatly it forced the L2s that submit higher quality proofs to pay higher rates for the entirety of much larger transactions, further amplifying the advantage of submitting less execution data. By moving closer to a blobspace fee-dominant environment we decrease this advantage.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-50078-the-impact-of-a-minimum-fee-4" name="p-50078-the-impact-of-a-minimum-fee-4"></a>The Impact of a Minimum Fee</h3>
<p>At the current value of ether, Max’s <a href="https://x.com/MaxResnick1/status/1829736908655624374" rel="noopener nofollow ugc">original proposal</a> opted to price the minimum fee per blob at $0.05 per blob. Supplementing the cost of execution with this new minimum fee, the proposal would have increased the average cost per blob by 2%.</p>
<p>The revised proposal has decreased the minimum blob base fee to 2^25, about 1/5th the originally proposed value or $0.01 per blob under the same assumptions. Since the beginning of July, this implies an average increase in cost of 0.7% for blobs, but due to the dispersion of financial efficiencies amongst blob submitters the percentage changes are not uniform across entities.</p>
<div class="md-table">
<table>
<thead>
<tr>
<th>Blob Submitter</th>
<th style="text-align: right;">Dataset Size</th>
<th style="text-align: right;">Current Cost per Blob</th>
<th style="text-align: right;">Proposed Cost</th>
<th style="text-align: right;">Historic Impact</th>
</tr>
</thead>
<tbody>
<tr>
<td>Base</td>
<td style="text-align: right;">385,077</td>
<td style="text-align: right;">$0.0687</td>
<td style="text-align: right;">$0.0797</td>
<td style="text-align: right;">16.0%</td>
</tr>
<tr>
<td>Taiko</td>
<td style="text-align: right;">271,786</td>
<td style="text-align: right;">$3.0152</td>
<td style="text-align: right;">$3.0262</td>
<td style="text-align: right;">0.4%</td>
</tr>
<tr>
<td>Arbitrum</td>
<td style="text-align: right;">178,127</td>
<td style="text-align: right;">$1.0099</td>
<td style="text-align: right;">$1.0209</td>
<td style="text-align: right;">1.1%</td>
</tr>
<tr>
<td>OP Mainnet</td>
<td style="text-align: right;">106,979</td>
<td style="text-align: right;">$0.0830</td>
<td style="text-align: right;">$0.0940</td>
<td style="text-align: right;">13.3%</td>
</tr>
<tr>
<td>Blast</td>
<td style="text-align: right;">78,430</td>
<td style="text-align: right;">$0.1655</td>
<td style="text-align: right;">$0.1765</td>
<td style="text-align: right;">6.6%</td>
</tr>
<tr>
<td>Scroll</td>
<td style="text-align: right;">49,632</td>
<td style="text-align: right;">$2.1304</td>
<td style="text-align: right;">$2.1414</td>
<td style="text-align: right;">0.5%</td>
</tr>
<tr>
<td>Linea</td>
<td style="text-align: right;">37,856</td>
<td style="text-align: right;">$0.5817</td>
<td style="text-align: right;">$0.5927</td>
<td style="text-align: right;">1.9%</td>
</tr>
<tr>
<td>zkSync Era</td>
<td style="text-align: right;">11,837</td>
<td style="text-align: right;">$2.6971</td>
<td style="text-align: right;">$2.7081</td>
<td style="text-align: right;">0.4%</td>
</tr>
<tr>
<td>Others</td>
<td style="text-align: right;">233,494</td>
<td style="text-align: right;">$0.6273</td>
<td style="text-align: right;">$0.6384</td>
<td style="text-align: right;">1.8%</td>
</tr>
<tr>
<td><strong>Total</strong></td>
<td style="text-align: right;">1,354,218</td>
<td style="text-align: right;"><strong>$1.5734</strong></td>
<td style="text-align: right;"><strong>$1.5844</strong></td>
<td style="text-align: right;">0.7%</td>
</tr>
</tbody>
</table>
</div><blockquote>
<p>Table: Blob submission statistics by entity from July 1, 2024 to September 17, 2024, assuming a ETH/USD rate of $2,500. Source: <a href="https://dune.com/queries/4089576" rel="noopener nofollow ugc">https://dune.com/queries/4089576</a></p>
</blockquote>
<p>Modifying the earlier per-transaction breakdown to account for a 2^25 wei minimum blobspace base fee, and only considering transactions where the original blobspace base fee was less than the proposed new minimum, we see that although the profile begins to meaningfully shift, the blob base fee remains a minority component for all affected blob carrying transactions. The highly efficient transactions submitted by Base and OP Mainnet that carry five blobs would see an increase between 10 to 30% depending on the state of L1 gas prices, which should be easily absorbed. Less efficient transactions, particularly those carrying one to three blobs would see total fee increases of less than 10%.</p>
<p>There have been no blob carrying transactions to date where a minimum blob base fee of 2^25 would have accounted for the majority of the cost paid by the transaction.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/4/347cdc27de8a573c6be2c32f5ffde9279f6dc917.png" title="Blobspace Share of Transaction Fees Paid by Blobs with 2^25 Base Fee"><img alt="Blobspace Share of Transaction Fees Paid by Blobs with 2^25 Base Fee" height="325" src="https://ethresear.ch/uploads/default/optimized/3X/3/4/347cdc27de8a573c6be2c32f5ffde9279f6dc917_2_690x325.png" width="690" /></a></div><p></p>
<blockquote>
<p>Source: <a href="https://dune.com/queries/4034254/6792625" rel="noopener nofollow ugc">https://dune.com/queries/4034254/6792625</a></p>
</blockquote>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-50078-blobspace-response-time-5" name="p-50078-blobspace-response-time-5"></a>Blobspace Response Time</h3>
<p>Under <a href="https://github.com/ethereum/EIPs/blob/7ced2f3a283ae9c2af6a4c2e33bba7fffab3e4c3/EIPS/eip-4844.md#helpers" rel="noopener nofollow ugc">EIP-4844</a>, the maximum interblock update to the blobspace base fee is 12.5%. Starting from a price of 1 wei, it takes 148 blocks at max capacity, over 29 minutes with 12 second block times, for the base fee to rise above 2^25 wei. This updating period has been framed as the response time of the protocol, but it still only represents a minimum amount of time. Due to market inefficiencies blocks do not end up full of blobs, vastly increasing the duration of price discovery.</p>
<p>Leading into the LayerZero airdrop on June 20th, the blob base fee was sitting at its minimum value of 1 wei. At its peak, the blob base fee reached <a href="https://dune.com/queries/4050697/6820492" rel="noopener nofollow ugc">7471 gwei</a> ($3,450 per blob). Although this level could have theoretically been reached in under 51 minutes, the climb took nearly six hours. Under Max’s proposal this maximum could have theoretically been reached in 21 minutes, but it’s clear that these theoretical values are not accurate approximations.</p>
<p>Rather than focusing on time, the goal of the proposal is to price the minimum blob base fee below, but close to, the inflection point where blobspace fees begin to form a measurable share of total fees paid by blobs. On June 20th, despite the surge in blobs beginning just after 11:00 UTC, it wasn’t until 15:17 UTC that blobspace fees began to contribute 0.1% of total fees paid by blobs, and it wasn’t until 15:41 UTC that a base fee of 2^25 wei (0.0335 gwei) was exceeded.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/2/62d30681b4d36ed46fda6e27a080d10ba6283ca2.png" title="Breakdown of Blob Fees During the LayerZero Airdrop"><img alt="Breakdown of Blob Fees During the LayerZero Airdrop" height="296" src="https://ethresear.ch/uploads/default/optimized/3X/6/2/62d30681b4d36ed46fda6e27a080d10ba6283ca2_2_690x296.png" width="690" /></a></div><p></p>
<blockquote>
<p>Source: <a href="https://dune.com/queries/4050166/6819510" rel="noopener nofollow ugc">https://dune.com/queries/4050166/6819510</a></p>
</blockquote>
<p>By contrast, had the minimum base fee been 2^25 wei during the LayerZero airdrop, the network may have leapfrogged the cold-start problem and minimized the dislocation between price and demand. We might expect the distribution of blob fees to have behaved as follows, with the blob market still taking an hour or longer to normalize.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/c/bc512855f739910e4b76132ceefbde4cc0ede375.png" title="Blob Fee Breakdown During the LayerZero Airdrop (above 2^25 wei base)"><img alt="Blob Fee Breakdown During the LayerZero Airdrop (above 2^25 wei base)" height="325" src="https://ethresear.ch/uploads/default/optimized/3X/b/c/bc512855f739910e4b76132ceefbde4cc0ede375_2_690x325.png" width="690" /></a></div><p></p>
<blockquote>
<p>Source: <a href="https://dune.com/queries/4050746/6820583" rel="noopener nofollow ugc">https://dune.com/queries/4050746/6820583</a></p>
</blockquote>
<p>In summary, raising the minimum blobspace base fee is not a magic bullet, but it should be viewed as a welcome change to the protocol. The market impact from the leading proposal should be minimal, with only the cheapest and lowest quality blobs seeing a price increase larger than 1%, while still remaining significantly cheaper than their competitors.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-50078-open-questions-6" name="p-50078-open-questions-6"></a>Open Questions</h3>
<ul>
<li>Will the blobspace fee market reach an equilibrium before the Pectra hardfork(s)?</li>
<li>Will we see additional cold-start problems each time the blob limit is increased with future hard forks?</li>
<li>Will the blob market move towards <a href="https://x.com/bertcmiller/status/1836139173159264579" rel="noopener nofollow ugc">private mempools</a>?</li>
<li>How have block building algorithms changed to better handle blobs since the LayerZero airdrop?</li>
<li>Should revenue from these PGAs be captured by proposers or by the protocol?</li>
</ul>
            <p><small>3 posts - 3 participants</small></p>
            <p><a href="https://ethresear.ch/t/understanding-minimum-blob-base-fees/20489">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 25 Sep 2024 15:56:18 +0000</pubDate>
</item>
<item>
<title>The Portable Web: Hackable, No Data Lock-in, and Crypto-native Web World</title>
<link>https://ethresear.ch/t/the-portable-web-hackable-no-data-lock-in-and-crypto-native-web-world/20488</link>
<guid>https://ethresear.ch/t/the-portable-web-hackable-no-data-lock-in-and-crypto-native-web-world/20488</guid>
<content:encoded><![CDATA[
<div> 关键词：可定制性、数据自由、加密原生、Web2问题、Web3局限

总结：

文章提出了“可携带网络”这一概念，旨在解决当前网络环境中的用户数据管理难题。其核心在于提供给用户对自身数据的完全控制权和选择权，通过构建平行于现有网络的新型架构，实现应用程序的可定制化、数据的去中心化以及加密经济作为激励机制。

1. **可定制性**：每个用户都可以根据自己的需求创建和定制应用单元，即“集群”，并在其中开发或引入不同的客户端、服务器和API规范，以适应个性化需求。
   
2. **数据自由**：用户数据被客户端缓存，通过与遵守API规范的服务器交互，实现了数据在不同服务器间的共享与迁移，确保数据不会被任何单一实体锁定。

3. **加密原生**：系统采用加密技术作为经济激励的基础，集群创建者可通过发行特定于集群的加密货币来吸引用户，从而推动集群的经济活动和增长。

4. **解决Web2问题**：通过限制浏览器直接访问目标URL的方式，避免用户内容和身份被单一服务垄断，减少数据和账户锁定现象。

5. **Web3局限性**：尽管Web3试图挑战现有权力结构，但其实际应用仍建立在传统Web架构之上，未能充分利用区块链的去中心化优势。而“可携带网络”旨在构建一个真正去中心化的网络环境，最大化用户权利。 <div>
<h1><a class="anchor" href="https://ethresear.ch#p-50076-about-this-post-1" name="p-50076-about-this-post-1"></a>About this post</h1>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/7/7736a1481d1296569277c270b8dc598d5fb8e52d.png" title="Comparison of Web Paradigms: Privacy, User Rights, and Monopoly"><img alt="Comparison of Web Paradigms: Privacy, User Rights, and Monopoly" height="353" src="https://ethresear.ch/uploads/default/optimized/3X/7/7/7736a1481d1296569277c270b8dc598d5fb8e52d_2_690x353.png" width="690" /></a></div><p></p>
<p>In the current web environment, users find it difficult to manage their own data and are often locked into specific services. The <strong>Portable Web</strong> operates as a parallel web alongside the existing one, aiming to provide users with greater control over their data and the ability to make choices. Applications on the Portable Web are primarily envisioned to serve as public infrastructure.</p>
<p>In this post, I will introduce the core ideas of the Portable Web. Detailed specifications unrelated to its feasibility are not included. This is still a rough draft, but I’m submitting this because if I waited for it to be perfect, I’d never finish.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50076-summary-2" name="p-50076-summary-2"></a>Summary</h1>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/4/b45eddabc00a7701fa71566c6cb4ae00604da350.png" title="Cluster Architecture Overview"><img alt="Cluster Architecture Overview" height="389" src="https://ethresear.ch/uploads/default/optimized/3X/b/4/b45eddabc00a7701fa71566c6cb4ae00604da350_2_690x389.png" width="690" /></a></div><p></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/7/67bcae0547fa80eec667beeca99468609b846e79.png" title="An Example of Incentive Mechanisms in the Portable Web"><img alt="An Example of Incentive Mechanisms in the Portable Web" height="359" src="https://ethresear.ch/uploads/default/optimized/3X/6/7/67bcae0547fa80eec667beeca99468609b846e79_2_690x359.png" width="690" /></a></div><p></p>
<ul>
<li>
<p><strong>Hackable: Users Can Customize Web Applications</strong></p>
<ul>
<li>A cluster represents a single application unit.</li>
<li>Anyone can create a cluster, and within the cluster, entities other than the creator can create their own clients, provide servers, define API schemas, and write migration scripts.</li>
<li>Clients and servers are loosely coupled and connected through an API schema, allowing different developers to create them independently.</li>
<li>For example, users can create customized UIs to tailor applications to their specific needs, making them easier to use. Additionally, they can develop their own API schemas and host servers to extend particular features. In this way, the Portable Web allows not only developers but also regular users to actively contribute to the evolution of applications.</li>
</ul>
</li>
<li>
<p><strong>No Data Lock-In: Users Have Control Over Their Data</strong></p>
<ul>
<li>A client caches the user’s data.</li>
<li>By using a server that conforms to the API schema, clients can share cached data across different servers.</li>
<li>Cached data on a client can also be migrated using migration scripts.</li>
<li>A client caches the data that a user sends and receives, but by transmitting this data to a server chosen by the user, it is managed in a decentralized manner. If needed, users can migrate their data to other servers, ensuring that their data is not locked into any particular entity.</li>
</ul>
</li>
<li>
<p><strong>Crypto-Native: Crypto-Economics as an Incentive Mechanism</strong></p>
<ul>
<li>In the Portable Web, cluster providers issue tokens and are incentivized by offering clusters that create real demand for those tokens.</li>
<li>All payments within a cluster are made using the issued tokens.</li>
<li>The presence of new participants contributes to the growth of the cluster, so the original cluster providers do not exclude them.</li>
<li>While Web2 operates as a monopoly and winner-takes-all game, the Portable Web promotes a collaborative and inclusive approach.</li>
</ul>
</li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#p-50076-background-3" name="p-50076-background-3"></a>Background</h1>
<h2><a class="anchor" href="https://ethresear.ch#p-50076-web2-4" name="p-50076-web2-4"></a>Web2</h2>
<p>The Web3 community has extensively discussed the problems of Web2, so I won’t delve deeply into that here. However, it is important to emphasize that <strong>the root of Web2’s problems lies in its architecture</strong>—specifically, the way browsers directly access target URLs.</p>
<p>In the Web2 architecture, users submit the content they generate directly to the service, without retaining ownership or local copies. User accounts and content exist within the service, and the service accumulates this data. This accumulation accelerates the creation of new data. It is extremely difficult for users to switch to another service and achieve the same level of utility. To do so, users would need to transfer their content, and other users would also need to migrate en masse.</p>
<p>The existing Web architecture leads to content lock-in and account lock-in, which in turn fosters the concentration of power and a winner-takes-all dynamic.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50076-web3-5" name="p-50076-web3-5"></a>Web3</h2>
<p>While Web3 often claims to challenge existing power structures and maximize user rights, in reality, it is currently just adding a blockchain layer on top of Web2.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/c/1cbf70828e9cfde6ab305afa732da5b326b710fa.png" title="Web3 as a marketing word"><img alt="Web3 as a marketing word" height="322" src="https://ethresear.ch/uploads/default/optimized/3X/1/c/1cbf70828e9cfde6ab305afa732da5b326b710fa_2_690x322.png" width="690" /></a></div><p></p>
<p>Although blockchain is decentralized, the fact that existing Web3 applications are built on top of the current Web architecture undermines its potential</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50076-portable-web-architecture-6" name="p-50076-portable-web-architecture-6"></a>Portable Web Architecture</h1>
<p>To solve the above issues and achieve a decentralized web while maximizing user rights, it is necessary to build a new architecture. The proposed solution is the <strong>Portable Web</strong>. This new web architecture provides an environment where users have complete control over their data and identity and enables developers and service providers to collaboratively evolve a single application.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/9/b9ffee4e26b1460e627f2401db4b0003f784a8cc.png" title="Portable Web Architecture"><img alt="Portable Web Architecture" height="437" src="https://ethresear.ch/uploads/default/optimized/3X/b/9/b9ffee4e26b1460e627f2401db4b0003f784a8cc_2_690x437.png" width="690" /></a></div><p></p>
<h2><a class="anchor" href="https://ethresear.ch#p-50076-components-of-the-portable-web-7" name="p-50076-components-of-the-portable-web-7"></a>Components of the Portable Web</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50076-portable-web-browser-8" name="p-50076-portable-web-browser-8"></a>Portable Web Browser</h3>
<p>The browser plays several key roles in enabling the Portable Web.</p>
<ol>
<li><strong>Controlled Server Communication</strong>: It limits the servers with which the client can communicate. Clients cannot interact with servers unless explicitly intended by the user.</li>
<li><strong>Currency Restriction</strong>: It restricts the currency used for payments in applications. The browser contains a wallet, ensuring that payments can only be made using the currency initially set by the cluster. By default, the browser interacts with an internal exchange (DEX or CEX), so the user is unaware of the currency being used.</li>
<li><strong>Identity Management</strong>: It manages the user’s identity as a Self-Sovereign Identity (SSI), preventing servers or clients from locking in the user’s identity.</li>
<li><strong>Built-In Support for Bootstrapping</strong>: It comes with built-in client and server information for the index cluster to support bootstrapping. Users can later connect to other clients or servers.</li>
<li><strong>Data Migration and Updates</strong>: It executes migration scripts specified by the client to transfer data and manages client updates.</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-50076-cluster-9" name="p-50076-cluster-9"></a>Cluster</h3>
<p>A cluster represents a single application, identified by its purpose document.</p>
<p>The components that make up a cluster are:</p>
<ul>
<li><strong>Purpose Document</strong></li>
<li><strong>API Schema</strong></li>
<li><strong>Migration Script</strong></li>
<li><strong>Client</strong></li>
<li><strong>Server</strong></li>
</ul>
<p>Anyone can contribute components other than the purpose document to help develop and evolve the cluster.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50076-index-cluster-10" name="p-50076-index-cluster-10"></a>Index cluster</h3>
<p>The index cluster functions like an App Store within the Portable Web (although anyone can provide it).</p>
<p>Providers of cluster components register their data with the index cluster. The index cluster hosts this registered data, offering users information and software. Additionally, the information includes details such as version and compatibility.</p>
<p>The index cluster knows which components belong to which clusters and understands the relationships between servers and API schemas, clients and API schemas, as well as clients and migration scripts.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50076-components-of-a-cluster-11" name="p-50076-components-of-a-cluster-11"></a>Components of a Cluster</h2>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/4/e49d0d3e46a311053e744c69bc9f63c792601554.png" title="The relationships between components"><img alt="The relationships between components" height="378" src="https://ethresear.ch/uploads/default/optimized/3X/e/4/e49d0d3e46a311053e744c69bc9f63c792601554_2_690x378.png" width="690" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#p-50076-purpose-document-12" name="p-50076-purpose-document-12"></a>Purpose Document</h3>
<p>The purpose document serves to enable and promote community-driven development. It defines:</p>
<ol>
<li><strong>The Ultimate Goal</strong>: The overarching objective that the cluster aims to achieve.</li>
<li><strong>Tokens Used</strong>: The specific tokens to be utilized within the cluster.</li>
</ol>
<p>This document is made public upon the cluster’s creation and remains immutable thereafter. While the ultimate goal stated in the purpose document does not have any systemic function, the community uses this document as a basis for improving and adding features.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50076-api-schema-13" name="p-50076-api-schema-13"></a>API Schema</h3>
<p>The API schema is a protocol that defines the communication methods between clients and servers. It needs to be in a developer-readable format. By adhering to this schema, clients and servers created by different developers can communicate with each other.</p>
<p>If there is compatibility between API schemas, servers and clients can support multiple Web API schemas.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50076-migration-script-14" name="p-50076-migration-script-14"></a>Migration Script</h3>
<p>A migration script assumes that the client has a specific data model. It allows data transfer and synchronization between clients that refer to the same migration script.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50076-client-15" name="p-50076-client-15"></a>Client</h3>
<p>A client consists of static content like HTML or JavaScript and can operate independently without relying on constant internet connectivity or specific servers. The client can only communicate with destinations specified by the user. It should not be implemented to depend on a specific server.</p>
<p>The client can cache data that the user sends to or receives from the server. It must specify a particular migration script and cache data in a data structure that allows data migration by executing that script.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50076-server-16" name="p-50076-server-16"></a>Server</h3>
<p>A server provides APIs that conform to the API schema. Any functionality that can be defined in the API schema can be provided.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50076-versioning-17" name="p-50076-versioning-17"></a>Versioning</h2>
<p>In the Portable Web, a cluster is a single application unit, but it can behave differently depending on which components are used. Since anyone can create components such as migration scripts, API schemas, clients, and servers, various versions coexist within a cluster.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50076-migration-script-18" name="p-50076-migration-script-18"></a>Migration Script</h3>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/1/816f44d7982b94d6c1e4e466919907be66f633b5.png" title="Version control of Migration Script"><img alt="Version control of Migration Script" height="251" src="https://ethresear.ch/uploads/default/optimized/3X/8/1/816f44d7982b94d6c1e4e466919907be66f633b5_2_690x251.png" width="690" /></a></div><p></p>
<p>Version management of migration scripts is represented using a Directed Acyclic Graph (DAG) and can be updated by anyone. When creating a new migration script, you must specify a backward-compatible migration script. The new migration script must be able to migrate data by transforming the data structure, even when executed from clients that supported the specified backward-compatible migration script. Since anyone can create migration scripts, they may branch but can also merge.</p>
<p>By executing the appropriate number of migration scripts, data can be migrated from older clients to clients that support the latest migration script. For example, a client that supports migration script ‘a’ can migrate data to a client that supports migration script ‘e’ by executing migration scripts three times（b→c→e or b→d→e）.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50076-api-schema-19" name="p-50076-api-schema-19"></a>API Schema</h3>
<p>A new API schema does not carry information about relationships with other API schemas, such as backward compatibility. Clients and servers can support multiple API schemas, so compatibility management is handled individually by clients and servers. They can support additional API schemas as long as compatibility is not broken.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50076-client-20" name="p-50076-client-20"></a>Client</h3>
<p>Client updates are mainly divided into three types. For all types of updates, the user can choose whether to accept the update.</p>
<ol>
<li><strong>Type 1</strong>: Updates that do not change either the migration script or the API schema.</li>
<li><strong>Type 2</strong>: Updates that change the API schema.</li>
<li><strong>Type 3</strong>: Updates that change the migration script.</li>
</ol>
<p>Type 1 does not affect other components.</p>
<p>In the case of Type 2, compatibility with the servers that the user usually uses may be lost unless the server also updates to the corresponding API schema.</p>
<p>In the case of Type 3, the client can update to a new migration script that specifies the current migration script as backward-compatible. Data can be migrated from a client supporting the previous migration script, but since it’s only backward-compatible and not fully compatible, data cached in the client that has updated the migration script cannot be migrated back to other clients still using the older migration script. In such cases, as shown in the diagram below, other clients need to either support the updated migration script or create a new one to ensure compatibility.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/f/3fe644e9f22f77184275a467cfb51593e41bafad.png" title="Resolve Migration Script Compatibility"><img alt="Resolve Migration Script Compatibility" height="215" src="https://ethresear.ch/uploads/default/optimized/3X/3/f/3fe644e9f22f77184275a467cfb51593e41bafad_2_690x215.png" width="690" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#p-50076-server-21" name="p-50076-server-21"></a>Server</h3>
<p>A server update means changing or adding the corresponding API schema. You can update by registering the updated API schema information in the index cluster.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50076-the-economics-of-the-portable-web-22" name="p-50076-the-economics-of-the-portable-web-22"></a>The Economics of the Portable Web</h1>
<p>For the Portable Web to function sustainably and for developers and service providers to actively participate, economic incentives are essential. This section explains the economic system that supports this architecture.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50076-incentives-for-participants-23" name="p-50076-incentives-for-participants-23"></a>Incentives for Participants</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50076-cluster-creators-24" name="p-50076-cluster-creators-24"></a>Cluster Creators</h3>
<p>Cluster creators launch new applications within the Portable Web ecosystem. They can issue tokens specific to their clusters, which become the foundation of the cluster’s economy. By designing tokens that encourage widespread adoption of their applications, cluster creators can earn revenue through seigniorage (profit from token issuance).</p>
<p>As the cluster gains popularity and more users join, the demand for these tokens increases. This heightened demand raises the value of the tokens, providing economic incentives for cluster creators to continue developing and improving their applications. The success of the cluster is directly linked to the value of the tokens, aligning the interests of cluster creators with those of users and other participants.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50076-server-providers-25" name="p-50076-server-providers-25"></a>Server Providers</h3>
<p>Servers within the Portable Web host data and provide APIs that conform to the cluster’s API schema. Server providers can monetize their services through various billing models, such as subscription fees, pay-per-use charges, or offering premium features. Since users manage their own data and can choose which servers to interact with, service providers are encouraged to offer high-quality, reliable services to attract and retain users.</p>
<p>By accepting payments in the cluster’s tokens, service providers also participate in the cluster’s economy. If the token’s value increases, the potential revenue for service providers grows as well. In this way, a symbiotic relationship is formed where service providers contribute to the cluster’s growth while profiting from its success.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50076-client-developers-26" name="p-50076-client-developers-26"></a>Client Developers</h3>
<p>Client developers create software that provides the cluster’s user interface and caches data sent to and received from servers. They can monetize their efforts by selling premium clients or offering additional features for a fee—all transacted in the cluster’s tokens.</p>
<p>Anyone can provide clients, and because of the interoperability within the cluster’s ecosystem, developers are encouraged to innovate and offer more valuable user experiences. They are motivated to continuously improve the products they provide.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50076-users-27" name="p-50076-users-27"></a>Users</h3>
<p>By using the Portable Web, users enjoy greater control over their data and the ability to customize their application experience. They participate in the cluster’s economy by using tokens to access premium features and more. Additionally, users who hold tokens may see their value increase as the cluster grows, providing an incentive to support and promote the cluster.</p>
<p>By engaging in the cluster’s economy, users have more opportunities to actively participate, provide feedback, and contribute to the community. Their involvement is expected to help develop the ecosystem further.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50076-discussions-28" name="p-50076-discussions-28"></a>Discussions</h1>
<p>Here, I briefly outline concerns and future challenges.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50076-versioning-29" name="p-50076-versioning-29"></a>Versioning</h2>
<p>With the current version management method, there’s a risk that migration scripts and API schemas could proliferate uncontrollably, negatively impacting user experience and data portability.</p>
<p>At present, it might be desirable for the initial cluster creator to have initiative over specifications while still allowing anyone to customize.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50076-incentives-for-data-lock-in-30" name="p-50076-incentives-for-data-lock-in-30"></a>Incentives for Data Lock-in</h2>
<p>The initial cluster creators have a disincentive against implementing data lock-in. This is because their goal is to profit from token seigniorage rather than from data lock-in (if they aimed to profit from data lock-in, they would not choose the Portable Web architecture). To profit from token seigniorage, they need to increase the real demand for the token, thereby boosting its price. To increase this demand, cluster creators must offer more attractive applications to users. Applications that appeal to users typically offer:</p>
<ul>
<li>No data lock-in</li>
<li>Customizability by anyone, fostering diversity and rapid development.</li>
</ul>
<p>Given this, cluster creators are likely to see remaining open as more beneficial than implementing data lock-in.</p>
<p>In other words, within the cluster, at least one component set (server, API schema, client, and migration script) must support data portability.</p>
<p>Service providers who join later and are not token stakeholders have a positive incentive for data lock-in, similar to conventional web environments. However, since users can choose components from the cluster, the most user-preferred components will be utilized. In an environment without data lock-in, if users still choose a locked-in component, it is a result of their own decision. This is also part of the value the Portable Web offers, and it cannot deny this choice.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50076-economics-31" name="p-50076-economics-31"></a>Economics</h2>
<p>If payments can be made through methods other than those provided by the browser’s standard, the system’s economy could collapse, rendering this architecture unviable.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50076-when-the-purpose-of-a-cluster-changes-32" name="p-50076-when-the-purpose-of-a-cluster-changes-32"></a>When the Purpose of a Cluster Changes</h2>
<p>The components of a cluster must align with its purpose. If functionalities that do not follow the cluster’s purpose are implemented, the cluster will lose its distinct identity—the symbol that differentiates it from other clusters. This would be similar to Facebook and LinkedIn—which have different purposes—losing their boundaries and becoming inconvenient applications. Moreover, if a feature does not align with users’ objectives, it is unlikely to gain their support.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50076-qa-33" name="p-50076-qa-33"></a>Q&amp;A</h1>

Is the Portable Web feasible? <a href="https://ethresear.ch/t/the-portable-web-hackable-no-data-lock-in-and-crypto-native-web-world/20488/1">(click for more details)</a>

What is the difference between Fediverse? <a href="https://ethresear.ch/t/the-portable-web-hackable-no-data-lock-in-and-crypto-native-web-world/20488/1">(click for more details)</a>

Why am I posting this here? <a href="https://ethresear.ch/t/the-portable-web-hackable-no-data-lock-in-and-crypto-native-web-world/20488/1">(click for more details)</a>

Is this post the final version? <a href="https://ethresear.ch/t/the-portable-web-hackable-no-data-lock-in-and-crypto-native-web-world/20488/1">(click for more details)</a>
<p>I welcome your feedback and collaboration to further develop and refine the Portable Web concept.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/the-portable-web-hackable-no-data-lock-in-and-crypto-native-web-world/20488">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 25 Sep 2024 12:57:12 +0000</pubDate>
</item>
<item>
<title>Using FRI for DA with Optimistic Correctable Commitments in Rollups</title>
<link>https://ethresear.ch/t/using-fri-for-da-with-optimistic-correctable-commitments-in-rollups/20467</link>
<guid>https://ethresear.ch/t/using-fri-for-da-with-optimistic-correctable-commitments-in-rollups/20467</guid>
<content:encoded><![CDATA[
<div> 关键词：FRI、数据可用性、乐观可纠正承诺、递归汇总、SPoRA挖矿

总结：

文章提出了一种利用FRI（快速重叠距离检验）与乐观可纠正承诺来实现数据可用性的新方法，该方法显著减少了存储在区块链上的数据量，相比分布式网络服务的数据量减少数百到数千倍。在汇总上下文中，引入了新的承诺构建技术，确保了在汇总中可靠存储和数据可用性，而不是使用标准承诺C。

该解决方案支持通过递归汇总实现无限区块链扩展，通过将大量数据处理移至链下并仅存储少量承诺在链上，提供无限制的扩展能力，同时不牺牲安全性和分散性。通过账户抽象简化了用户体验，用户无需关注特定汇总存储其资金，同样，开发者可以创建服务而无需担心数据由哪个汇总处理。

该技术允许以数百或数千倍的效率在链上存储数据，对于每个兆字节在网络中分布的数据，仅需存储32字节的哈希值。这大大降低了存储和交易成本，使技术更易于大规模采用。同时，它提供了灵活性，即块可以在不需要丢失数据完整性和可用性的情况下从链上移动到链下。

此解决方案为Web2应用向Web3的转变提供了基础设施，为公司提供了利用区块链的安全性、透明度和分散性而不必完全重新设计其商业模式的机会。它在行业中的竞争优势在于结合了高效性、可扩展性和与高级技术如递归汇总的兼容性。 <div>
<h2><a class="anchor" href="https://ethresear.ch#p-50034-abstract-1" name="p-50034-abstract-1"></a>Abstract</h2>
<p>Scaling blockchains and transitioning from Web2 to Web3 require efficient solutions for storing and accessing large volumes of data. We present a new technique for using FRI commitments combined with optimistic correctable commitments to implement Data Availability (DA). This allows for a significant reduction in the volume of data stored on-chain, by hundreds and thousands of times compared to the volume of data served by the distributed network. For each cluster of megabytes, it is sufficient to store only a short 32-byte hash on-chain. In the context of rollups, we introduce a new commitment construction that ensures reliable storage and data availability when used in rollups instead of the standard commitment <span class="math">C</span>. Combined with recursive rollups, our solution paves the way for unlimited blockchain scaling and the transfer of Web2 to Web3, ensuring reliability, security, and data availability.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50034-introduction-2" name="p-50034-introduction-2"></a>Introduction</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50034-the-problem-of-scaling-and-data-storage-in-blockchain-3" name="p-50034-the-problem-of-scaling-and-data-storage-in-blockchain-3"></a>The Problem of Scaling and Data Storage in Blockchain</h3>
<p>Blockchains produce enormous amounts of data, and efficient management of this data is critical for their scaling and widespread application in Web3. Traditional solutions, such as Filecoin, do not provide reliable data storage at the consensus level. Other solutions, like Arweave, while offering storage reliability, are not suitable for dynamic Web3 applications due to the impossibility of modifying or deleting data.</p>
<p>Modern solutions, such as EthStorage and 0g, aim to solve these problems but face limitations:</p>
<ul>
<li><strong>EthStorage</strong> uses data replication, requiring the storage of multiple copies of the same volume of data to ensure reliability.</li>
<li><strong>0g</strong> applies Reed-Solomon codes for data sharding but uses KZG10 commitments, which are not optimal for building recursion — a key mechanism for scaling through recursive rollups.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50034-the-potential-of-our-solution-4" name="p-50034-the-potential-of-our-solution-4"></a>The Potential of Our Solution</h3>
<ul>
<li>
<p><strong>Scaling through recursive rollups</strong>: Support for recursive rollups allows for a significant increase in network throughput. This is achieved by processing large volumes of data off-chain and storing only minimal commitments on-chain. This approach provides unlimited scaling without compromising security and decentralization.</p>
</li>
<li>
<p><strong>Ease of Use through Account Abstraction</strong>: Despite the technical complexity of recursive rollups, they remain transparent for end users and developers. With the implementation of account abstraction, users don’t need to concern themselves with which specific rollup stores their funds — they see a total balance and can perform operations without additional complications. Similarly, developers of decentralized applications can create services without worrying about which rollup processes their data. This is akin to how Bitcoin wallets use UTXO abstraction: users see their total balance without delving into technical details. Thus, our solution provides scalability without compromising convenience and accessibility for users and developers.</p>
</li>
<li>
<p><strong>Economical data storage</strong>: Our solution allows storing hundreds or thousands of times less information on-chain compared to the volume of data processed off-chain. For each megabyte of data distributed in the network, only a short 32-byte hash is stored on the blockchain. This significantly reduces storage and transaction costs, making the technology more accessible for mass adoption.</p>
</li>
<li>
<p><strong>Flexibility through off-chain block transfer</strong>: Using recursive rollup technology, blocks can subsequently be moved off-chain without losing data integrity and availability. This provides flexibility in network architecture and optimizes resource usage.</p>
</li>
<li>
<p><strong>Transformation of Web2 to Web3</strong>: Our technology provides infrastructure for transferring existing Web2 applications and services to the decentralized Web3 environment. This opens up new opportunities for companies, allowing them to leverage the advantages of blockchain — security, transparency, and decentralization — without the need to completely rethink their business models.</p>
</li>
<li>
<p><strong>Competitive advantage</strong>: Unlike existing solutions, our proposal combines efficiency, scalability, and compatibility with advanced technologies such as recursive rollups. This creates a significant competitive advantage and sets new standards in the industry.</p>
</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50034-our-goal-5" name="p-50034-our-goal-5"></a>Our Goal</h3>
<p>We propose a solution that allows the use of FRI (Fast Reed-Solomon Interactive Oracle Proofs of Proximity) for DA with optimistic correctable commitments. This provides:</p>
<ul>
<li><strong>Efficient data storage</strong>: Storing only a small commitment (e.g., 32 bytes) on-chain for data clusters of several megabytes.</li>
<li><strong>Compatibility with recursive rollups</strong>: Reducing the volume of data required for on-chain storage, contributing to unlimited blockchain scaling.</li>
<li><strong>Data reliability and availability</strong>: Ensuring data correctness and availability even in the presence of errors or malicious actions.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50034-preliminary-information-6" name="p-50034-preliminary-information-6"></a>Preliminary Information</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50034-fri-and-its-application-7" name="p-50034-fri-and-its-application-7"></a>FRI and Its Application</h3>
<p><strong>FRI</strong> is a method used in zkSNARK protocols to verify the proximity of data to Reed-Solomon code. It allows creating compact commitments to large volumes of data and provides efficient verification of their correctness.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50034-problems-when-using-fri-for-da-8" name="p-50034-problems-when-using-fri-for-da-8"></a>Problems When Using FRI for DA</h3>
<p>Direct implementation of FRI for DA faces problems:</p>
<ul>
<li><strong>Incorrect commitments</strong>: A malicious participant can present a commitment with errors in correction codes. If these errors are few enough, such a commitment will be accepted by the network.</li>
<li><strong>Lack of connection between commitment and data</strong>: Even if the data is recovered, establishing a connection with the original commitment is difficult due to possible errors in the original commitment.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50034-our-solution-optimistic-correctable-commitments-9" name="p-50034-our-solution-optimistic-correctable-commitments-9"></a>Our Solution: Optimistic Correctable Commitments</h2>
<p>We propose an extension over FRI that solves these problems by introducing a new commitment construction <span class="math">\mathcal{C} = (C, \chi, \{a_i\})</span>, where</p>
<p><span class="math">\mathbf{D}</span> - data we want to store in the network,</p>
<p><span class="math">C=\mathrm{Commit}(\mathbf{D})</span> - commitment to the data,</p>
<p><span class="math">\mathrm{Shard}_j</span> - shards into which the data is divided using Reed-Solomon codes,</p>
<p><span class="math">H_j = \mathrm{Hash}(\mathrm{Shard}_j)</span> - hash of the shard,</p>
<p><span class="math">\chi = \mathrm{Challenge}(C, \{H_j\})</span> - pseudorandom challenge,</p>
<p><span class="math">\{a_i\}</span> - result of opening the commitment <span class="math">C</span> at <span class="math">\chi</span>.</p>
<p>This construction provides:</p>
<ul>
<li><strong>Connection between data and commitment</strong>: Ensures that the data corresponds to the commitment and can be verified.</li>
<li><strong>Possibility of error correction</strong>: The system is capable of detecting and correcting errors without trusting validators or clients.</li>
<li><strong>Use in rollups</strong>: Provides mechanisms for integration with rollups, allowing them to use <span class="math">\mathcal{C}</span> instead of the standard commitment <span class="math">C</span>.</li>
</ul>
<p><img alt="fig0" height="500" src="https://ethresear.ch/uploads/default/original/3X/6/9/69b5279febeda88a190490a8338a91e9431cf046.svg" width="206" /></p>
<h2><a class="anchor" href="https://ethresear.ch#p-50034-implementation-details-10" name="p-50034-implementation-details-10"></a>Implementation Details</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50034-data-structure-and-sharding-11" name="p-50034-data-structure-and-sharding-11"></a>Data Structure and Sharding</h3>
<h4><a class="anchor" href="https://ethresear.ch#p-50034-data-representation-12" name="p-50034-data-representation-12"></a>Data Representation</h4>
<p>Let the data <span class="math">D</span> be represented as a matrix of size <span class="math">T \times K</span>, where <span class="math">|D| = T \cdot K</span>. We consider the matrix as a function of columns.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50034-domain-extension-13" name="p-50034-domain-extension-13"></a>Domain Extension</h4>
<p>We apply domain extension to the matrix, increasing the number of columns from <span class="math">K</span> to <span class="math">N</span> using Reed-Solomon codes and the Discrete Fourier Transform (DFT). This allows for redundancy for error correction.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50034-data-sharding-14" name="p-50034-data-sharding-14"></a>Data Sharding</h4>
<p>The resulting matrix has a size of <span class="math">T \times N</span>. Each column <span class="math">\text{Shard}_j</span> (for <span class="math">j = 1, \dots, N</span>) is considered as a separate data shard. These shards are distributed among network nodes for storage.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50034-shard-hashing-15" name="p-50034-shard-hashing-15"></a>Shard Hashing</h4>
<p>Each shard is hashed using a cryptographic hash function:</p>
<p><span class="math">
H_j = \mathrm{Hash}(\text{Shard}_j).
</span></p>
<p><img alt="fig1" height="362" src="https://ethresear.ch/uploads/default/original/3X/a/b/aba4dab24d942f940b86fe1914cac86be03f1d1b.svg" width="690" /></p>
<h3><a class="anchor" href="https://ethresear.ch#p-50034-commitment-generation-and-opening-16" name="p-50034-commitment-generation-and-opening-16"></a>Commitment Generation and Opening</h3>
<h4><a class="anchor" href="https://ethresear.ch#p-50034-commitment-to-data-17" name="p-50034-commitment-to-data-17"></a>Commitment to Data</h4>
<p>We apply FRI to the matrix, considering it as a function of rows. This allows obtaining a commitment <span class="math">C = \mathrm{Commit}(D)</span>, compactly representing all the data.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50034-random-point-generation-18" name="p-50034-random-point-generation-18"></a>Random Point Generation</h4>
<p>We use the <strong>Fiat-Shamir heuristic</strong> to calculate a pseudorandom point <span class="math">\chi</span>, dependent on the commitment and shard hashes:</p>
<p><span class="math">
\chi = \mathrm{Challenge}(C, H_1, H_2, \dots, H_N).
</span></p>
<h4><a class="anchor" href="https://ethresear.ch#p-50034-commitment-opening-19" name="p-50034-commitment-opening-19"></a>Commitment Opening</h4>
<p>We perform a polynomial opening of the commitment <span class="math">C</span> at point <span class="math">\chi</span>, obtaining proof <span class="math">\{a_i\}</span>:</p>
<p><span class="math">
\{a_i\} = \mathrm{Open}(C, \chi).
</span></p>
<h3><a class="anchor" href="https://ethresear.ch#p-50034-new-commitment-construction-for-rollups-20" name="p-50034-new-commitment-construction-for-rollups-20"></a>New Commitment Construction for Rollups</h3>
<h4><a class="anchor" href="https://ethresear.ch#p-50034-use-in-rollups-21" name="p-50034-use-in-rollups-21"></a>Use in Rollups</h4>
<p>When applied in rollups, <span class="math">\mathcal{C}</span> is used instead of the standard commitment <span class="math">C</span>.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50034-shard-correctness-verification-22" name="p-50034-shard-correctness-verification-22"></a>Shard Correctness Verification</h3>
<p>For each shard <span class="math">\text{Shard}_j</span>, a node can verify it without the need to store all the data:</p>
<ol>
<li>
<p><strong>Calculate the shard value at point <span class="math">\chi</span></strong>:</p>
<p><span class="math">
s_j = \mathrm{Eval}(\text{Shard}_j, \chi).
</span></p>
</li>
<li>
<p><strong>Calculate the opening value at the point corresponding to the shard</strong>:</p>
<p><span class="math">
s'_j = \mathrm{Eval}(\{a_i\}, P_j),
</span></p>
<p>where <span class="math">P_j</span> is the point associated with sharding.</p>
</li>
<li>
<p><strong>Compare values</strong>:</p>
<p><span class="math">
s_j \stackrel{?}{=} s'_j.
</span></p>
</li>
</ol>
<p>If the equality holds, shard <span class="math">\text{Shard}_j</span> is considered correct.</p>
<p><img alt="fig2" height="414" src="https://ethresear.ch/uploads/default/original/3X/0/9/09b2e020e39c1b237b66c4e6c51d7b41153b6825.svg" width="690" /></p>
<h4><a class="anchor" href="https://ethresear.ch#p-50034-correctness-lemma-23" name="p-50034-correctness-lemma-23"></a>Correctness Lemma</h4>
<p><strong>Lemma</strong>: If for shard <span class="math">\text{Shard}_j</span> the equality <span class="math">s_j = a_j</span> holds, then with high probability <span class="math">\text{Shard}_j</span> is a correct shard of data <span class="math">D</span>.</p>
<p><strong>Proof</strong>:</p>
<p>Using the <strong>Schwartz-Zippel lemma</strong>, we know that two different polynomials of degree <span class="math">d</span> can coincide in no more than <span class="math">d</span> points out of <span class="math">|F|</span>, where <span class="math">F</span> is the field. Since <span class="math">\chi</span> is chosen randomly, the probability that an incorrect shard will pass the check is <span class="math">\frac{d}{|F|}</span>, which is negligibly small for large fields.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50034-system-architecture-24" name="p-50034-system-architecture-24"></a>System Architecture</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50034-data-writing-process-25" name="p-50034-data-writing-process-25"></a>Data Writing Process</h3>
<ol>
<li>
<p><strong>Transaction Initiation</strong>:</p>
<ul>
<li>The client forms a transaction with data <span class="math">\mathbf{D}</span> and metadata <span class="math">\mathcal{M}</span>:
<ul>
<li>Commitment <span class="math">C</span>.</li>
<li>Shard hashes <span class="math">\{H_j\}</span>.</li>
<li>Opening <span class="math">\pi = \{a_i\}</span> at point <span class="math">\chi</span>.</li>
<li>Construction <span class="math">\mathcal{C} = (C, \chi, \{a_i\})</span>.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Transaction Validation</strong>:</p>
<ul>
<li>Validators check the client’s signature.</li>
<li>Verify the correctness of metadata and opening <span class="math">\pi</span>.</li>
<li>Verify the correctness of <span class="math">\mathcal{C}</span>.</li>
</ul>
</li>
<li>
<p><strong>Transaction Signing</strong>:</p>
<ul>
<li>If all checks are successful, validators sign the transaction and transmit metadata and shards to network nodes.</li>
</ul>
</li>
<li>
<p><strong>Shard Distribution</strong>:</p>
<ul>
<li>Data <span class="math">\mathbf{D}</span> is sharded, and shards <span class="math">\{\text{Shard}_j\}</span> are distributed among network nodes.</li>
</ul>
</li>
<li>
<p><strong>Node Verification</strong>:</p>
<ul>
<li>Nodes receive their shards and check:
<ul>
<li>Correspondence of shard hash <span class="math">H_j</span> and received shard <span class="math">\text{Shard}_j</span>.</li>
<li>Correctness of opening <span class="math">\pi</span> and construction <span class="math">\mathcal{C}</span>.</li>
<li>If all checks are successful, nodes sign the transaction, after which the transaction can be included in a block.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Metadata Storage</strong>:</p>
<ul>
<li>Nodes parse the verified metadata into fragments, sign them using threshold signature, and distribute these fragments for storage among themselves.</li>
</ul>
</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-50034-optimistic-error-correction-26" name="p-50034-optimistic-error-correction-26"></a>Optimistic Error Correction</h3>
<p>If errors or inconsistencies are detected:</p>
<ul>
<li><strong>Decentralized Recovery</strong>: Nodes jointly recover correct data using the redundancy of Reed-Solomon codes.</li>
<li><strong>Fraud Proofs</strong>: Nodes can form fraud proofs if they detect incorrect actions.</li>
<li><strong>Use of SPoRA Mining</strong>: In the process of SPoRA mining, nodes find in their data structure the data for which they can receive a reward. We have improved SPoRA mining to incentivize the node to recover all original data and commitments for the sectors involved in mining. Thus, to check the correctness of shard hashes and commitment, the miner only needs to compare several hashes, which minimally uses their resources but stimulates the miner to actively check and recover data.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50034-classification-and-elimination-of-errors-27" name="p-50034-classification-and-elimination-of-errors-27"></a>Classification and Elimination of Errors</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50034-types-of-defects-28" name="p-50034-types-of-defects-28"></a>Types of Defects</h3>
<h4><a class="anchor" href="https://ethresear.ch#p-50034-uncorrectable-defects-29" name="p-50034-uncorrectable-defects-29"></a>Uncorrectable Defects</h4>
<ul>
<li><strong>Incorrect metadata structure</strong>: Inability to interpret <span class="math">C</span>, <span class="math">\pi</span>, <span class="math">\{H_j\}</span>, <span class="math">\mathcal{C}</span>.</li>
<li><strong>Mismatch between hash and shard</strong>: <span class="math">H_j \neq \mathrm{Hash}(\text{Shard}_j)</span>.</li>
</ul>
<p><strong>Reaction</strong>: The transaction is rejected by validators and nodes.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50034-metadata-inconsistency-30" name="p-50034-metadata-inconsistency-30"></a>Metadata Inconsistency</h4>
<ul>
<li><strong>Incorrect opening</strong>: Checking <span class="math">\pi</span> returns false.</li>
<li><strong>Mismatch in shard verification</strong>: <span class="math">s_j \neq a_j</span>.</li>
<li><strong>Incorrect calculation of <span class="math">\chi</span></strong>: <span class="math">\chi</span> does not correspond to the calculated value from <span class="math">C</span> and <span class="math">\{H_j\}</span>.</li>
</ul>
<p><strong>Reaction</strong>:</p>
<ul>
<li>Nodes form <strong>fraud proofs</strong>, proving incorrectness.</li>
<li>Validators who proposed such a transaction are subject to penalties.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-50034-correctable-defects-31" name="p-50034-correctable-defects-31"></a>Correctable Defects</h4>
<ul>
<li>
<p><strong>Errors in correction codes</strong>: Small errors in shards that can be corrected.</p>
<p>Circuit for this fraud proof for 1 MiB cluster requires <span class="math">\sim 3 \cdot 2^{15}</span> <span class="math">(16 \to 8)</span> poseidon2 hashes.</p>
</li>
<li>
<p><strong>Partial incorrectness of shards</strong>: Some shards are damaged but can be recovered from others.</p>
<p>Circuit for this fraud proof for 1 MiB cluster requires <span class="math">\sim 2^{15}</span> <span class="math">(16 \to 8)</span> poseidon2 hashes.</p>
</li>
</ul>
<p><strong>Reaction</strong>:</p>
<ul>
<li>Nodes use data redundancy to recover correct shards in the mining process.</li>
<li>Miners will form <strong>fraud proofs</strong> consisting of a zkSNARK that recalculates this data correctly. Since the data of one block is just hundreds of thousands of field elements, there is no difficulty for the miner to generate such a zkSNARK. The proof and verification of the zkSNARK will be paid from the penalty of validators who proposed such a transaction.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50034-security-guarantees-32" name="p-50034-security-guarantees-32"></a>Security Guarantees</h3>
<p>It’s important to note that the optimistic elements of the protocol do not reduce the security of the system, as the assumption of the presence of a sufficient number of honest nodes is as reliable as these elements themselves. If an honest node misses defects or refuses to store data, it still won’t be able to mine and receive rewards.</p>
<ul>
<li>
<p>Data preservation with an honest minority: If there is a sufficient number of honest nodes in the network, the data will be correctly stored and correspond to the corrected code close to the original code used to generate the commitment. Even if the commitment or shards were generated with errors, the network will be able to correct them and restore the data without errors.</p>
</li>
<li>
<p>Protection against incorrect changes: In the process of error correction, a dishonest majority will not be able to substitute shard hashes with incorrect ones or replace the commitment with one that does not correspond to the corrected code. This guarantees the immutability of data and their correspondence to the stated commitment. If there is already an accepted commitment in the network, it is impossible to introduce errors into it during the error correction process, even if the client, all validators, and all nodes are malicious.</p>
</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50034-examples-and-scenarios-33" name="p-50034-examples-and-scenarios-33"></a>Examples and Scenarios</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50034-example-1-data-recovery-with-errors-34" name="p-50034-example-1-data-recovery-with-errors-34"></a>Example 1: Data Recovery with Errors</h3>
<p><strong>Situation</strong>: Several shards are damaged due to failures.</p>
<p><strong>System Actions</strong>:</p>
<ol>
<li><strong>Error Detection</strong>: Nodes detect incorrect shards during verification.</li>
<li><strong>Data Recovery</strong>: Using Reed-Solomon code redundancy, nodes recover correct shards.</li>
<li><strong>Metadata Update</strong>: Update corresponding shard hashes <span class="math">H_j</span>.</li>
<li><strong>Continued Operation</strong>: The system functions without interruptions, data remains available.</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-50034-example-2-use-in-rollups-35" name="p-50034-example-2-use-in-rollups-35"></a>Example 2: Use in Rollups</h3>
<p><strong>Situation</strong>: A developer implements our system in a recursive rollup.</p>
<p><strong>Actions</strong>:</p>
<ol>
<li><strong>Integration of <span class="math">\mathcal{C}</span></strong>: The rollup uses the construction <span class="math">\mathcal{C} = (C, \chi, \{a_i\})</span> instead of the standard commitment <span class="math">C</span>.</li>
<li><strong>Additional Opening</strong>: The rollup performs commitment opening at point <span class="math">\chi</span> and includes this in the state proof.</li>
<li><strong>State Verification</strong>: Using <span class="math">\mathcal{C}</span>, the rollup proves the correctness of its state in zkSNARK or zkSTARK.</li>
<li><strong>Scaling</strong>: Data corresponding to <span class="math">\mathcal{C}</span> is not stored on-chain, which allows for a significant reduction in the volume of data stored on-chain.</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#p-50034-explanation-of-key-concepts-36" name="p-50034-explanation-of-key-concepts-36"></a>Explanation of Key Concepts</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50034-fiat-shamir-heuristic-37" name="p-50034-fiat-shamir-heuristic-37"></a>Fiat-Shamir Heuristic</h3>
<p>A method of transforming interactive protocols into non-interactive ones using hash functions. In our case, it is used to generate a pseudorandom point <span class="math">\chi</span> dependent on the commitment and shard hashes.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50034-schwartz-zippel-lemma-38" name="p-50034-schwartz-zippel-lemma-38"></a>Schwartz-Zippel Lemma</h3>
<p>A theorem stating that two different polynomials of degree <span class="math">d</span> can coincide in no more than <span class="math">d</span> points from field <span class="math">F</span>. It ensures that the probability of successful forgery of verification is negligibly small.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50034-reed-solomon-codes-39" name="p-50034-reed-solomon-codes-39"></a>Reed-Solomon Codes</h3>
<p>Error correction codes that allow data recovery in the presence of errors or losses. Used to create redundancy and ensure reliability of data storage.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50034-mathcalc-construction-in-rollups-40" name="p-50034-mathcalc-construction-in-rollups-40"></a><span class="math">\mathcal{C}</span> Construction in Rollups</h3>
<ul>
<li><strong>Why it’s needed</strong>: Provides a link between data and commitment, allowing rollups to prove the correctness of their state.</li>
<li><strong>How it’s used</strong>: The rollup includes <span class="math">\mathcal{C}</span> in its proofs, which guarantees data availability and its correspondence to the commitment.</li>
<li><strong>Advantages</strong>:
<ul>
<li>Reduction of on-chain data volume.</li>
<li>Increasing proof efficiency.</li>
<li>Ensuring data reliability and availability.</li>
</ul>
</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50034-conclusion-41" name="p-50034-conclusion-41"></a>Conclusion</h2>
<p>We have presented a technique for using FRI for DA with optimistic correctable commitments, introducing a new construction <span class="math">\mathcal{C} = (C, \chi, \{a_i\})</span>, which is especially useful in the context of rollups. Our system allows for a significant reduction in the volume of data stored on-chain and ensures data reliability and availability. It provides developers with new tools for creating scalable decentralized applications, integrating with recursive rollups, and incentivizing nodes to behave honestly through SPoRA-mining mechanisms and fraud proofs.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50034-appendices-42" name="p-50034-appendices-42"></a>Appendices</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50034-detailed-explanation-of-commitment-and-opening-43" name="p-50034-detailed-explanation-of-commitment-and-opening-43"></a>Detailed Explanation of Commitment and Opening</h3>
<h4><a class="anchor" href="https://ethresear.ch#p-50034-commitment-generation-44" name="p-50034-commitment-generation-44"></a>Commitment Generation</h4>
<ol>
<li><strong>Data</strong>: Matrix <span class="math">D</span> of size <span class="math">T \times K</span>.</li>
<li><strong>Function</strong>: consider rows of <span class="math">D</span> as values of T-1 degree polynomial function <span class="math">f(x)</span> over field <span class="math">F</span>.</li>
<li><strong>FRI Application</strong>: Apply FRI to <span class="math">f(x)</span> to obtain commitment <span class="math">C</span>.</li>
</ol>
<h4><a class="anchor" href="https://ethresear.ch#p-50034-generation-of-random-point-chi-45" name="p-50034-generation-of-random-point-chi-45"></a>Generation of Random Point <span class="math">\chi</span></h4>
<ul>
<li>Use a hash function to generate <span class="math">\chi</span>:<br />
<span class="math">
\chi = \mathrm{Hash}(C, H_1, H_2, \dots, H_N).
</span></li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-50034-opening-at-point-chi-46" name="p-50034-opening-at-point-chi-46"></a>Opening at Point <span class="math">\chi</span></h4>
<ul>
<li>Calculate values <span class="math">\{a_i\}</span> necessary to prove that <span class="math">f(\chi)</span> corresponds to the data.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50034-use-in-rollup-47" name="p-50034-use-in-rollup-47"></a>Use in Rollup</h3>
<ul>
<li><strong>Additional Opening</strong>: The rollup includes in the proof the opening of the commitment at point <span class="math">\chi</span>, i.e., <span class="math">\{a_i\}</span>.</li>
<li><strong><span class="math">\mathcal{C}</span> Construction</strong>: The rollup publishes <span class="math">\mathcal{C} = (C, \chi, \{a_i\})</span>.</li>
<li><strong>Advantages</strong>:
<ul>
<li>Provides data verifiability without the need for access to all data.</li>
<li>Guarantees that data is available and stored in the network.</li>
<li>Allows the rollup to reduce the volume of data needed for on-chain storage.</li>
</ul>
</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50034-proof-of-the-correctness-lemma-48" name="p-50034-proof-of-the-correctness-lemma-48"></a>Proof of the Correctness Lemma</h3>
<p><strong>Assumption</strong>: Let <span class="math">\text{Shard}_j</span> be incorrect but pass the check <span class="math">s_j = a_j</span>.</p>
<p><strong>Probability</strong>:</p>
<ul>
<li>The probability that an incorrect shard will coincide at point <span class="math">\chi</span> with a correct one is <span class="math">\frac{d}{|F|}</span>, where <span class="math">d</span> is the degree of the polynomial.</li>
<li>For a large field <span class="math">F</span>, the probability is negligibly small.</li>
</ul>
<p><strong>Conclusion</strong>: With high probability, if a shard has passed the check, it is correct.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/using-fri-for-da-with-optimistic-correctable-commitments-in-rollups/20467">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sat, 21 Sep 2024 20:40:04 +0000</pubDate>
</item>
<item>
<title>Vorbit SSF with circular and spiral finality: validator selection and distribution</title>
<link>https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464</link>
<guid>https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464</guid>
<content:encoded><![CDATA[
<p><em>Thanks to <a href="https://x.com/fradamt">Francesco D’Amato</a> and <a href="https://x.com/barnabemonnot">Barnabé Monnot</a> for feedback, and to participants of the <a href="https://efdn.notion.site/Robust-Incentives-Group-RIG-Homepage-802339956f2745a5964d8461c5ccef02">RIG</a>+<a href="https://x.com/nero_eth">frie</a><a href="https://x.com/adietrichs">nds</a> meetup where parts of <a href="https://ethresear.ch/t/orbit-ssf-solo-staking-friendly-validator-set-management-for-ssf/19928">Orbit</a> came together—including Barnabé’s <a href="https://ethresear.ch/t/orbit-ssf-solo-staking-friendly-validator-set-management-for-ssf/19928#fast-rotation-5">finality stairwell</a>.</em></p>
<p>By <a href="https://x.com/weboftrees">Anders Elowsson</a></p>
<h2><a class="anchor" href="https://ethresear.ch#p-50029-h-1-introduction-1" name="p-50029-h-1-introduction-1"></a>1. Introduction</h2>
<p>Ethereum will transition to <a href="https://notes.ethereum.org/@vbuterin/single_slot_finality">single-slot finality</a> (SSF) to provide fast and strong economic guarantees that transactions included in a block will not revert. This requires upgrades to the consensus algorithm (<a href="https://ethresear.ch/t/a-simple-single-slot-finality-protocol/14920">1</a>, <a href="https://arxiv.org/abs/2310.11331">2</a>, <a href="https://notes.ethereum.org/@fradamt/chained-3sf">3</a>, <a href="https://github.com/fradamt/ssf/tree/main/high_level">4</a>) and the signature aggregation scheme (<a href="https://ethresear.ch/t/horn-collecting-signatures-for-faster-finality/14219">1</a>, <a href="https://ethresear.ch/t/flooding-protocol-for-collecting-attestations-in-a-single-slot/17553">2</a>, <a href="https://ethresear.ch/t/signature-merging-for-large-scale-consensus/17386">3</a>), as <a href="https://ethresear.ch/t/orbit-ssf-solo-staking-friendly-validator-set-management-for-ssf/19928#where-we-are-1">previously outlined</a>. A third requirement is to upgrade Ethereum’s validator economics and management, with current progress on <a href="https://ethresear.ch/t/sticking-to-8192-signatures-per-slot-post-ssf-how-and-why/17989#approach-3-rotating-participation-ie-committees-but-accountable-5">rotating participation</a> presented in the <a href="https://ethresear.ch/t/orbit-ssf-solo-staking-friendly-validator-set-management-for-ssf/19928">Orbit SSF</a> proposal. A few considerations in this area are how to incentivize validator consolidation (<a href="https://notes.ethereum.org/@vbuterin/single_slot_finality#Economic-capping-of-total-validator-count">1</a>, <a href="https://ethresear.ch/t/orbit-ssf-solo-staking-friendly-validator-set-management-for-ssf/19928#incentivizing-consolidation-10">2</a>), how to temper the quantity of stake (<a href="https://ethresear.ch/t/faq-ethereum-issuance-reduction/19675">1</a>, <a href="https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448">2</a>, <a href="https://ethresear.ch/t/endgame-staking-economics-a-case-for-targeting/18751">3</a>, <a href="https://ethresear.ch/t/reward-curve-with-tempered-issuance-eip-research-post/19171">4</a>), and how to select validators for the active set.</p>
<p>With SSF, the consensus mechanism will still consist of an available chain (e.g., <a href="https://arxiv.org/abs/2302.11326">RLMD GHOST</a>) and a finality gadget (e.g., resembling <a href="https://arxiv.org/abs/1710.09437">Casper FFG</a> or <a href="https://tendermint.com/static/docs/tendermint.pdf">Tendermint</a>). It remains unlikely that all validators will be able to participate in every slot, eventhough validator consolidation from <a href="https://eips.ethereum.org/EIPS/eip-7251">EIP-7251</a> can offer a tangible improvement. This means that validators must be partitioned into committees, with each committee voting on the head of the available chain and/or finalizing successive checkpoints. Committees voting on the available chain must <a href="https://ethresear.ch/t/orbit-ssf-solo-staking-friendly-validator-set-management-for-ssf/19928#slow-rotation-6">rotate slowly</a>, but such strict requirements do not apply to the finality gadget (after validators have finalized their checkpoint).</p>
<p>This post will take a closer look at cumulative finality when finality committees rotate quickly or moderately, proposing strategies for validator selection and distribution. A forthcoming post is intended to review the dynamics of slower validator rotations, with a focus on the available chain. To properly model the impact of consolidation, equations for generating a “pure Zipfian” distribution for a specific quantity of stake are first presented in <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#p-50029-h-2-zipfian-staking-sets-used-for-modeling-2">Section 2</a>, with modeled staking sets generated at varying levels of purity. A method for generating committees in a new type of “epoch” is then presented in <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#p-50029-h-3-committees-cumulative-finality-and-aggregate-finality-gap-5">Section 3</a> and cumulative finality analyzed under different levels of validator consolidation and stake quantities—applying various committee selection criteria. A good evaluation measure is the “aggregate finality gap”, tallying missing finality for a block during its progression to full finality. It turns out that the activity rate of a validator should not strictly be determined by its size. Ideally, it varies with the quantity of stake and the composition of the validator set, hence “Vorbit", as in variable Orbit.</p>
<p>Cumulative finality is impeded at epoch boundaries when committees are shuffled. Circular finality is therefore suggested in <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#p-50029-h-4-circular-and-spiral-finality-10">Section 4</a>, wherein successive epochs are repeated across a longer era, such that finality accrues in a circular fashion. A mechanism for shuffling the validator set in a spiral fashion is also introduced, to improve finality at shuffling boundaries. The impact of various selection and distribution methods is analyzed in <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#p-50029-h-5-optimized-selection-and-distribution-of-auxiliary-validators-13">Section 5</a>, and the effect on finality across deposited stake is presented in <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#p-50029-h-6-analysis-across-d-16">Section 6</a>. <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#p-50029-h-7-predicting-the-optimal-number-of-auxiliary-committees-17">Section 7</a> reviews methods for predicting the optimal number of validating committees, and <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#p-50029-h-8-properties-related-to-consensus-formation-22">Section 8</a> reviews features related to consensus formation and staking risks.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50029-h-2-zipfian-staking-sets-used-for-modeling-2" name="p-50029-h-2-zipfian-staking-sets-used-for-modeling-2"></a>2. Zipfian staking sets used for modeling</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50029-h-21-pure-zipfian-distribution-3" name="p-50029-h-21-pure-zipfian-distribution-3"></a>2.1 Pure Zipfian distribution</h3>
<p>To model committee-based SSF, it is necessary to define the expected level of consolidation in the validator set, including a realistic range. The idea is to generate validator sets across this range and then explore how to optimally partition each set into committees. Optimization criteria relate to for example cumulative finality. An achievable consolidation level will also serve as a healthy bound when exploring consolidation incentives in a forthcoming study.</p>
<p>Vitalik reviewed the distribution of stakers in the <a href="https://x.com/VitalikButerin/status/1335729572633923584">early days</a> of Ethereum and <a href="https://notes.ethereum.org/@vbuterin/single_slot_finality#The-good-news-gains-from-enabling-voluntary-validator-balance-consolidation">established</a> that it was roughly <a href="https://en.wikipedia.org/wiki/Zipf%27s_law">Zipfian</a>. The relationship between the staking deposit size <span class="math">D</span> and the quantity of stakers <span class="math">N</span> was then stipulated to <span class="math">D=32N\log_2{N}</span> under a “pure” Zipfian distribution. A straightforward procedure for generating a “pure” Zipfian staking set is to distribute stakers’ balances as</p>
<div class="math">
\frac{32N}{1}, \frac{32N}{2}, ..., \frac{32N}{N}.
</div>
<p>When <span class="math">N</span> is large (as in this case), the associated harmonic series</p>
<div class="math">
1 + \frac{1}{2} + ... + \frac{1}{N}
</div>
<p>approaches <span class="math">\ln(N)+\gamma</span>, where <span class="math">\gamma</span> is the Euler–Mascheroni constant, approximately 0.577. The total quantity of stake is then</p>
<div class="math">
D = 32N(\ln(N)+\gamma),
</div>
<p>which is close to Vitalik’s approximation. <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#a1-quantity-of-stakers-under-a-pure-zipfian-distribution-28">Appendix A.1</a> shows that <span class="math">N</span> therefore can be determined as</p>
<div class="math">
N = e^{ W \left( \frac{D}{32} e^\gamma \right) - \gamma},
</div>
<p>where <span class="math">W</span> denotes the <a href="https://en.wikipedia.org/wiki/Lambert_W_function">Lambert <span class="math">W</span> function</a>. These equations provide the blueprint for generating a pure Zipfian staking set given any specific <span class="math">D</span>. The equation is first applied to <span class="math">D</span> to determine <span class="math">N</span>, and the harmonic series involving <span class="math">N</span> is used to create the distribution. The corresponding two lines of Python code are provided in <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#a1-quantity-of-stakers-under-a-pure-zipfian-distribution-28">Appendix A.1</a>.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50029-h-22-modeled-validator-sets-4" name="p-50029-h-22-modeled-validator-sets-4"></a>2.2 Modeled validator sets</h3>
<p>Figure 1 shows the resulting distribution of staker balances in cyan. In purple is a second distribution (“1/2 Zipfian”) created by removing half the stakers (every other staker in the sorted set, starting with the second largest), and reallocating the removed ETH across 32-ETH validators. This aims to capture a scenario where many larger stakers maintain 32-ETH validators. Even if they eventually consolidate, it could still represent an intermediate distribution of “nominal” staker set sizes over the next few years as consolidation slowly progresses. This post uses several such distributions, including also a 9/10 Zipfian distribution (removing every tenth staker), a 4/5 Zipfian distribution, and a 2/3 Zipfian distribution.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/e/3e3f0e2f66e56acff2e5f3ccc2eec5ff19dbb590.png" title="Figure 1"><img alt="Figure 1" height="494" src="https://ethresear.ch/uploads/default/optimized/3X/3/e/3e3f0e2f66e56acff2e5f3ccc2eec5ff19dbb590_2_690x494.png" width="690" /></a></div><p></p>
<p><strong>Figure 1.</strong> Log-log plot of distributions of staker set sizes used for modeling in this post, at <span class="math">D=</span> 30M ETH. The set sizes in cyan follow a “pure” Zipfian distribution, and the set sizes in purple remove every other staker and reallocates the stake to 32-ETH validators.</p>
<p>The pure Zipfian distribution has <span class="math">N\approx79\,000</span> at <span class="math">D=</span> 30M ETH staked. Ethereum’s node count is hard to estimate; crawlers can only provide lower bounds. But it would appear that the node count is a bit below the staker set size for this hypothetical distribution. The 1/2 Zipfian distribution in purple has <span class="math">N\approx481\,000</span>. This is a point that hopefully will be passed through on the way to a consolidated validator set; yet it is uncertain how quickly progress will be made.</p>
<p>The staking sets are converted to validator sets <span class="math">\mathcal{V}</span> by having stakers with more than 2048 ETH (excluding those already reallocated to 32-ETH validators) divide their stake into validators of the maximum allowed size (<span class="math">s_{\text{max}}=2048</span>), thus capturing the ideal outcome. The last two validators in this procedure are set to an equal size below 2048. For example, a staker with 5048 ETH will have validators of size {2048, 1500, 1500}.</p>
<p>Most of the stakers hold less than 2048 ETH under the Zipfian distributions, so this only adds around 9000 validators for the pure Zipfian distribution and around 5000 validators for the 1/2 Zipfian distribution. For the Zipfian staking set, <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#a2-quantity-of-validators-under-a-pure-zipfian-distribution-29">Appendix A.2</a> shows that the corresponding Zipfian validator set size <span class="math">V=|\mathcal{V}|</span> can be estimated quite precisely as</p>
<div class="math">
V = \frac{N}{64} \left(63+\ln(N/64) + 2\gamma \right).
</div>
<p>The distribution of validator counts and sums across consolidation levels is shown in Figure 2.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/a/ba77c943f869cad7ef52b96cb713ab86123443d7.png" title="Figure 2"><img alt="Figure 2" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/b/a/ba77c943f869cad7ef52b96cb713ab86123443d7_2_609x500.png" width="609" /></a></div><p></p>
<p><strong>Figure 2.</strong> Distribution of validator count and sum at 30M ETH staked in the five modeled validator sets. Axes are log-scaled.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50029-h-3-committees-cumulative-finality-and-aggregate-finality-gap-5" name="p-50029-h-3-committees-cumulative-finality-and-aggregate-finality-gap-5"></a>3. Committees, cumulative finality, and aggregate finality gap</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50029-h-31-generation-of-committees-6" name="p-50029-h-31-generation-of-committees-6"></a>3.1 Generation of committees</h3>
<p>Define <span class="math">\hat{V_a}</span> as a desirable upper limit for the active validator set size <span class="math">V_a</span>. The protocol can allow (and wants) <span class="math">V_a</span> to increase up to <span class="math">\hat{V_a}</span>, but not beyond this limit. This post sets <span class="math">\hat{V_a}=31250</span>, which corresponds to the committee size when 1 million validators are split up into 32 committees (reflecting approximately the current committee size). There has been some progress in enabling clients to handle larger committees, yet the finality gadget may have a slightly different profile than today (e.g., subject the network to twice the signature load). <a href="https://ethresear.ch/t/sticking-to-8192-signatures-per-slot-post-ssf-how-and-why/17989#approach-3-rotating-participation-ie-committees-but-accountable-5">Smaller committees</a> such as <span class="math">\hat{V_a}=4096</span> could therefore also be modeled using the same framework if required.</p>
<p>Let <span class="math">C</span> denote the number of committees in a new form of “epoch” constituting a full rotation of the validator set. The validator set is first split up into <span class="math">C</span> disjoint <em>regular committees</em>, ensuring <span class="math">V/C&lt;\hat{V_a}</span>. As an example, the 4/5 Zipfian staking set at <span class="math">D=</span> 30M ETH consists of around 233 thousand (k) validators. An epoch must therefore be split up into at least <span class="math">C=8</span> committees, with each regular committee in that case consisting of around 29100 validators. Setting <span class="math">C=8</span> leaves room to include around <span class="math">V_{\mathrm{aux}}=\hat{V_a}-29\,100=2150</span> auxiliary validators in each committee—validators that also have been assigned to participate in some other regular committee. Once these 2150 validators have been added, the final <em>full committees</em> consist of <span class="math">\hat{V_a}</span> validators.</p>
<p>To select auxiliary validators for the committees, each validator of size <span class="math">s</span> ETH is assigned a weight <span class="math">w</span>. The baseline weighting is</p>
<div class="math">
w(s)=\frac{s}{s_{\mathrm{max}}},
</div>
<p>where <span class="math">s_{\mathrm{max}}=2048</span> as previously discussed. This is similar to the thresholding operation of <a href="https://ethresear.ch/t/orbit-ssf-solo-staking-friendly-validator-set-management-for-ssf/19928#active-validator-set-management-8">Orbit SSF</a>, but it uses <span class="math">s_{\text{max}}</span> rather than 1024. This change differentiates validators in the range 1024-2048. Vorbit performs optimally under full differentiation, and the change also makes individual consolidation incentives (discussed in <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#p-50029-h-83-the-activity-ratio-and-its-implications-on-staking-economics-25">Section 8.3</a>) reasonable above 1024. <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#p-50029-h-51-adjusted-weighting-14">Section 5.1</a> discusses how Orbit can adopt full differentiation. The probability  <span class="math">P(s)</span> for a validator of size <span class="math">s</span> ETH to be drawn as the next auxiliary validator to be included in a committee is given by:</p>
<div class="math">
P(s) = \frac{w(s)}{\sum_{v \in \mathcal{V}_{¢}} w(s_v)},
</div>
<p>where <span class="math">v</span> represents each validator in the complementary set <span class="math">\mathcal{V}_{¢}</span> not already part of the committee, and <span class="math">s_v</span> is the size of validator <span class="math">v</span>. The smallest validators will then tend to participate in roughly <span class="math">1/C</span> of the slots and larger validators more frequently, with outcomes depending on the quantity of stake and consolidation level (see also Figure 22).</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50029-h-32-cumulative-finality-7" name="p-50029-h-32-cumulative-finality-7"></a>3.2 Cumulative finality</h3>
<p>Figure 3 shows the <a href="https://ethresear.ch/t/orbit-ssf-solo-staking-friendly-validator-set-management-for-ssf/19928#fast-rotation-5">stepwise</a> <a href="https://ethresear.ch/t/a-model-for-cumulative-committee-based-finality/10259">committee-based</a> cumulative finality for the 4/5 Zipfian staking set, with committees finalizing consecutive slots. For transactions included in block <span class="math">n</span>, aggregate finality is visually accounted for at the conclusion of the slot that the committee voted in. Finality when only using the regular committee is illustrated using a dashed blue line. Since each regular committee is completely disjoint and proportionally reflects the overall distribution, each committee adds an equal marginal cumulative finality to non-finalized transactions/blocks.  The solid blue line in Figure 3 shows cumulative finalization when each regular committee in the example has been supplemented by auxiliary validators up to <span class="math">\hat{V_a}</span> (“Full”).</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/a/eab7b069864af3da6ff936dff826036bdf43c703.png" title="Figure 3"><img alt="Figure 3" height="471" src="https://ethresear.ch/uploads/default/optimized/3X/e/a/eab7b069864af3da6ff936dff826036bdf43c703_2_690x471.png" width="690" /></a></div><p></p>
<p><strong>Figure 3.</strong> Cumulative finalization of block <span class="math">n</span> for the 4/5 Zipfian staking set. The finality gap (blue arrow) gradually falls. The aggregate finality gap is the sum of all finality gaps until full finality (cyan area).</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50029-h-33-aggregate-finality-gap-8" name="p-50029-h-33-aggregate-finality-gap-8"></a>3.3 Aggregate finality gap</h3>
<p>Let <span class="math">D_f</span> be the quantity of stake that has finalized a block, and <span class="math">D</span> the total quantity of stake deposited for staking. The finality gap <span class="math">F_g</span> is the proportion of the stake that has not yet finalized a block:</p>
<div class="math">
F_g = \frac{D-D_f}{D}.
</div>
<p>While <span class="math">D_f</span> is a relevant measure of economic security in isolation, <span class="math">D-D_f</span> is less useful if <span class="math">D</span> is unknown. A block’s finality gap will fall with each new slot as long as new validators participate in the finalizing committees. The example with full committees has a lower finality gap due to the additional finality afforded by the auxiliary validators. Since they are selected in a weighted fashion, the effect is rather pronounced eventhough only around 2150 additional validators were added in this example. The difference in the finality gap diminishes as full finality approaches. At this point, most validators will have been present as part of their regular allocation anyway, and repeating a validator does not, in this comparison, improve upon finality (an argument could potentially be made for higher economic security when repeating a validator, but this is beyond the scope of this post).</p>
<p>A useful utility measure when dealing with cumulative finality is the aggregate finality gap <span class="math">\widetilde{F}_{\!g}</span> that a block is subjected to during consensus formation, until full finalization. It is represented by the cyan area in Figure 3 and is calculated as</p>
<div class="math">
\widetilde{F}_{\!g} = C - \sum_{i=1}^{C} F_{g}(i).
</div>
<h3><a class="anchor" href="https://ethresear.ch#p-50029-h-34-auxiliary-committees-9" name="p-50029-h-34-auxiliary-committees-9"></a>3.4 Auxiliary committees</h3>
<p>What happens if the 4/5 Zipfian set is divided into 9 committees instead of 8? The added auxiliary committee (<span class="math">C_{\mathrm{aux}}=1</span>) results in <span class="math">\hat{V_a}/9\approx3470</span> auxiliary validators in each committee, facilitating a further reduction in <span class="math">\widetilde{F}_{\!g}</span>. A comparison between epochs of 8 committees (blue) and 9 committees (purple) is shown in Figure 4. The difference in the finality gap <span class="math">\Delta F_{\!g}</span> for block <span class="math">n</span> is indicated in green for the slots when there is a reduction in the gap, and in red when there is an increase. Cumulative finality first improves due to the additional auxiliary validators, and this is the most pronounced effect. As the number of duplicated validators increases, the reduction diminishes. At the beginning of slot <span class="math">n+7</span>, the validator set divided into 8 committees instead has a lower <span class="math">F_g</span>, and it reaches full finality one slot earlier, at the start of slot <span class="math">n+8</span>.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/8/4837d194d111146a383429331be3d4ec77b89a4f.png" title="Figure 4"><img alt="Figure 4" height="471" src="https://ethresear.ch/uploads/default/optimized/3X/4/8/4837d194d111146a383429331be3d4ec77b89a4f_2_690x471.png" width="690" /></a></div><p></p>
<p><strong>Figure 4.</strong> Cumulative finalization of block <span class="math">n</span> for the 4/5 Zipfian staking set. When adding an auxiliary committee, there is more room for auxiliary validators with high balances in each committee (purple line), and finality therefore accrues faster during the initial phase.</p>
<p>The aggregate finality gap continues to fall when more auxiliary committees are added, as indicated in Figure 5. In the comparison between <span class="math">C_{\mathrm{aux}}=3</span> and <span class="math">C_{\mathrm{aux}}=4</span>, <span class="math">\Delta F_{g}</span> is negative starting at the beginning of slot <span class="math">n+5</span>, and continues to fall all the way up to slot <span class="math">n+12</span>. As a result, the aggregate finality gap <span class="math">\widetilde{F}_{\!g}</span> is about equal for these two configurations.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/b/4b1bf568acf6824092c1f1d8b8d124e8755e32a2.png" title="Figure 5"><img alt="Figure 5" height="471" src="https://ethresear.ch/uploads/default/optimized/3X/4/b/4b1bf568acf6824092c1f1d8b8d124e8755e32a2_2_690x471.png" width="690" /></a></div><p></p>
<p><strong>Figure 5.</strong> Cumulative finalization of block <span class="math">n</span> for the 4/5 Zipfian staking set, comparing the outcome between different numbers of auxiliary committees.</p>
<p>Figure 6 shows the same example for a purely Zipfian staking set. At <span class="math">C_{\mathrm{aux}}=4</span>, almost 20M ETH (2/3 of the stake) will finalize the block in the first slot. As in the previous example, the benefit of adding auxiliary committees diminishes as more are added (<span class="math">\widetilde{F}_{\!g}</span> stops decreasing and eventually reverses).</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/4/d42336e923e31a5109fb72fdd065b9830b360f0d.png" title="Figure 6"><img alt="Figure 6" height="471" src="https://ethresear.ch/uploads/default/optimized/3X/d/4/d42336e923e31a5109fb72fdd065b9830b360f0d_2_690x471.png" width="690" /></a></div><p></p>
<p><strong>Figure 6.</strong> Cumulative finalization of block <span class="math">n</span> for a pure Zipfian staking set, comparing the outcome between different numbers of auxiliary committees.</p>
<p>Figure 7 instead shows the outcome with a 1/2 Zipfian staking set. The approximately 486k validators need to be split into at least</p>
<div class="math">
\left\lceil \frac{486000}{\hat{V}_{\!a}} \right\rceil = 16
</div>
<p>committees. With no full committees, only 1.875 million ETH will finalize each round. This might seem problematic since a committee that fails to finalize will <a href="https://ethresear.ch/t/a-model-for-cumulative-committee-based-finality/10259#mechanism-2">hold up finality</a> until a sufficient amount of stake in the committee has been replaced through an inactivity leak or a similar mechanism. An accelerated inactivity leak could be considered under such circumstances. From an accountability perspective, this level of stake has however been argued to be <a href="https://ethresear.ch/t/sticking-to-8192-signatures-per-slot-post-ssf-how-and-why/17989#why-not-just-do-committees-1">totally sufficient</a>.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/8/483f1199756edb6315f1f56de0a5788694f5775b.png" title="Figure 7"><img alt="Figure 7" height="471" src="https://ethresear.ch/uploads/default/optimized/3X/4/8/483f1199756edb6315f1f56de0a5788694f5775b_2_690x471.png" width="690" /></a></div><p></p>
<p><strong>Figure 7.</strong> Cumulative finalization of block <span class="math">n</span> for the 1/2 Zipfian staking set, comparing the outcome between different numbers of auxiliary committees.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50029-h-4-circular-and-spiral-finality-10" name="p-50029-h-4-circular-and-spiral-finality-10"></a>4. Circular and spiral finality</h2>
<p>The figures in the previous subsection have all captured cumulative finality over one epoch and are representative for the first block in an epoch. During each epoch, the complete validator set is iterated over, so full finality is reached at the end of the epoch. However, if the validator set is shuffled between epochs, then only the first block of the epoch will achieve full finality by the end of the epoch. For blocks in later slots of the epoch, full finality will not be reached until the end of the <em>next</em> epoch. Marginal cumulative finality decreases markedly at epoch boundaries, because committees on each side of the boundary will have more validator overlaps (even when using only regular committees). Thus, when stating that full finality can be reached within eight slots for the 4/5 Zipfian staking set in Figures 3-5, this is a qualified statement. For the second block of the epoch, full finality is not reached until after 15 slots (7 slots in the first epoch and 8 slots in the subsequent epoch). Recall that <span class="math">C</span> denotes the number of committees in an epoch, which is also then the number of slots in an epoch during regular operation. The average number of slots to full finality <span class="math">\bar{S}_{\!f}</span> then becomes</p>
<div class="math">
\bar{S}_{\!f}=C + \frac{C-1}{2}.
</div>
<p>While full finality might be more of an ideational concern, degradation from shuffling begins already at the second slot/committee if the block was proposed in the last slot of the epoch. There are two ways to improve on this: <em>circular finality</em> and <em>spiral finality</em>. Both provide benefits starting from a block’s second slot of accruing finality.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50029-h-41-circular-finality-11" name="p-50029-h-41-circular-finality-11"></a>4.1 Circular finality</h3>
<p>The most straightforward solution is to avoid shuffling the validator set each epoch. Instead, the validator set is shuffled in eras, where each era can consist of multiple epochs. The number of epochs per era <span class="math">E_{\text{era}}</span> is determined from the desired number of slots per era <span class="math">\hat{S}_{\text{era}}</span> and <span class="math">C</span>, rounded to the nearest integer:</p>
<div class="math">
E_{\text{era}}=\lfloor\hat{S}_{\text{era}}/C\rceil. 
</div>
<p>With this change, the first <span class="math">(E_{\text{era}}-1)C</span> blocks of the era will be finalized in <span class="math">C</span> slots, whereas the last <span class="math">C</span> blocks will finalize in accordance with the previous equation <span class="math">C + (C-1)/2</span>. Furthermore, and perhaps more importantly, cumulative finality will not degrade when crossing epoch boundaries within the era. The average number of slots to full finality among the <span class="math">E_{\text{era}}\times C</span> blocks of an era becomes:</p>
<div class="math">
\bar{S}_{\!f}=\frac{C(E_{\text{era}}-1)C + C(C + (C-1)/2)}{E_{\text{era}}\times C}, 
</div>
<p>which simplifies to</p>
<div class="math">
\bar{S}_{\!f}=C + \frac{C-1}{2E_{\text{era}}}.
</div>
<p>As an example, set <span class="math">\hat{S}_{\text{era}}=64</span>. The 4/5 Zipfian staking set with no auxiliary slots will then finalize 57 out of 64 blocks in 8 slots, with one block each among the remaining finalizing in 9, 10, 11 slots, etc. Furthermore, only the last 7 out of 64 slots in the era will suffer degraded cumulative finalization, whereas 56 out of 64 will do so without circular finality. The average number of slots to full finality becomes <span class="math">\bar{S}_{\!f} \approx 8.4</span>. In contrast, without circular finality, the result is <span class="math">\bar{S}_{\!f}=C+(C-1)/2 = 11.5.</span></p>
<h3><a class="anchor" href="https://ethresear.ch#p-50029-h-42-spiral-finality-12" name="p-50029-h-42-spiral-finality-12"></a>4.2 Spiral finality</h3>
<p>While circular finality is effective in reducing <span class="math">\bar{S}_{\!f}</span> and the proportion of blocks with degraded cumulative finality, it does not reduce the maximum time to full finality, which remains <span class="math">S_{f} = 2C-1</span>. This maximum applies to the block proposed in the second slot of the last epoch of the era. A method to reduce this maximum is spiral finality, where limits are placed on how many slots validators may shift forward within the epoch when they are shuffled. This is controlled by the variable <span class="math">C_{\text{shift}}</span>. Setting <span class="math">C_{\text{shift}}=2</span> means that validators may only shift two slots forward, but they can always shift back to the start of the epoch. The regular validators located in the first committee of the epoch <span class="math">\mathcal{C}_n</span> can then be reassigned between committees <span class="math">\mathcal{C}_n</span> and <span class="math">\mathcal{C}_{n+2}</span>, the regular validators in committee <span class="math">\mathcal{C}_{n+1}</span> can be reassigned between committees <span class="math">\mathcal{C}_n</span> and <span class="math">\mathcal{C}_{n+3}</span>, etc. If <span class="math">\hat{V}_a</span> is set relatively low, it might be reasonable to make further stipulations on the random selection, to ensure an even distribution of large and small validators.</p>
<p>Circular and spiral finality can be combined to achieve a low average time to full finality, as well as a lower upper bound on it. In this setup, spiral finality is applied to the last epoch of an era.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50029-h-5-optimized-selection-and-distribution-of-auxiliary-validators-13" name="p-50029-h-5-optimized-selection-and-distribution-of-auxiliary-validators-13"></a>5. Optimized selection and distribution of auxiliary validators</h2>
<p>This section reviews two different methods for optimizing the distribution of auxiliary validators. The plots will as previously disregard epoch boundaries (presume circular finality). In fact, to provide a more stable results in light of the randomness inherent in the validatior selection process, finality is evaluated in a circular fashion in all plots of cumulative finality in this post. This involves computing results for <span class="math">C</span> consecutive slots across all <span class="math">C</span> different starting positions. Additionally, the approach ensures that spacing and distribution of validators are not attuned to epoch boundaries, and is particularly useful in <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#p-50029-h-52-equal-spacing-15">Section 5.2</a> that introduces equally spaced validators.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50029-h-51-adjusted-weighting-14" name="p-50029-h-51-adjusted-weighting-14"></a>5.1 Adjusted weighting</h3>
<p>The most straightforward modification is to adjust the weighting scheme by adding the power <span class="math">p</span> to the original equation:</p>
<div class="math">
w(s)=\Big(\frac{s}{s_{\mathrm{max}}}\Big)^p.
</div>
<p>If <span class="math">p&gt;1</span>, larger auxiliary validators are further prioritized over smaller validators. This can be useful since the smaller validators are still guaranteed to be included in one committee, and <span class="math">C</span> can be relatively small (short epochs). A potential change to the Orbit slow-rotation paradigm, when validators are selected directly from the weighting and there is no regular committee, is that <span class="math">p</span> instead can be set below 1. This reduces the “slope” of the thresholding mechanism, allowing smaller validators  to be selected with a higher probability than for example 1/32 or 1/64. This can be beneficial for reasons discussed in <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#p-50029-h-83-the-activity-ratio-and-its-implications-on-staking-economics-25">Section 8.3</a>, and will be further explored in a post covering the slowly rotating validator set.</p>
<p>Figure 8 shows the difference in the finality gap in terms of finalized stake <span class="math">\Delta D_{f}</span> when changing <span class="math">p</span> from 1 to 2. The average outcome across the five validator sets (from 1/2 Zipfian to fully Zipfian) was used. The reader may also wish to review Figure 22 in <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#p-50029-h-82-activity-rate-24">Section 8.2</a>, which shows how the change in weighting alters the probability for a validator of size <span class="math">s</span> to be active.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/1/9140afbcdd9422b8a85f7a916db431d418609dcb.png" title="Figure 8"><img alt="Figure 8" height="484" src="https://ethresear.ch/uploads/default/optimized/3X/9/1/9140afbcdd9422b8a85f7a916db431d418609dcb_2_690x484.png" width="690" /></a></div><p></p>
<p><strong>Figure 8.</strong> Change in <span class="math">D_{f}</span> at 30M staked when <span class="math">p</span> is changed from 1 to 2, during a block’s progression to full finality.</p>
<p>As evident, <span class="math">D_{f}</span> is on average almost 5M ETH higher for the first slot, when <span class="math">C_{\text{aux}}</span> is between 3-4 (those lines are somewhat overlapping in the graph). This is a significant improvement, reducing the finality gap at the first slot by almost 1/6. The examples with 2-4 auxiliary committees then experience a slight reduction starting at <span class="math">n+5</span>. This is because validators with the most stake become included in almost every committee: repeated validators do not increase the cumulative finality, and they occupy space in the committees, preventing new validators from finalizing the block.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50029-h-52-equal-spacing-15" name="p-50029-h-52-equal-spacing-15"></a>5.2 Equal spacing</h3>
<p>Since repeated validators do not increase cumulative finality, it is advantageous to equally space repeated auxiliary validators across the epoch so that repetitions occur as far apart as possible. The distribution of auxiliary validators can then be done slightly differently. The number of auxiliary inclusions <span class="math">\lambda</span> can be set for a validator with stake <span class="math">s</span> as</p>
<div class="math">
\lambda(s) = \frac{V_{\text{aux}}(C-1)w(s)-\widetilde{V}_{\!\text{aux}}}{\sum_{v \in \mathcal{V}} w(s_v)}.
</div>
<p>In this equation, <span class="math">\widetilde{V}_{\!\text{aux}}</span> sums the auxiliary validator instances added across the full epoch among validators that are present in every slot. It is initially set to zero. For any validator <span class="math">v</span> with <span class="math">\lambda_v &gt; C-1</span>, an iterative procedure sets <span class="math">\lambda_v = C-1</span>, removes the validator from <span class="math">\mathcal{V}</span>, adds <span class="math">C-1</span> to <span class="math">\widetilde{V}_{\!\text{aux}}</span>, and recomputes <span class="math">\lambda</span> for the remaining validators. The iterative procedure relying on <span class="math">\widetilde{V}_{\!\text{aux}}</span> is necessary because a validator can never be included more than once per slot (<span class="math">\lambda \not &gt; C</span>).</p>
<p>Given <span class="math">\lambda</span>, each validator is guaranteed inclusion in <span class="math">\lfloor \lambda \rfloor</span> committees, with any remaining fraction used when drawing validators that will be included in one additional auxiliary committee. The final outcome is denoted <span class="math">\lambda_f</span>. Auxiliary inclusions for validators are equally spaced at intervals of <span class="math">C/(\lambda_f+1)</span> slots. The spacing procedure starts from the regular committee position, rounding the computed distance to the nearest integer, and wrapping around epoch boundaries using the modulo operation.</p>
<p>Due to randomness in the distribution of validators, slots will with this procedure generally end up slightly below or above <span class="math">\hat{V}_a</span>. In general, this should not be an issue because <span class="math">\hat{V}_a</span> would typically allow for some flexibility. However, to maintain consistency with the random draw in the evaluation, an iterative procedure reallocated validators from committees with more than <span class="math">\hat{V}_a</span> validators to committees with fewer, still ensuring no duplications of validators within a committee.</p>
<p>Let <span class="math">\equiv</span> represent equal spacing and <span class="math">\not\equiv</span> the spacing achieved due to random draw. The change <span class="math">\Delta D_f</span> at 30M ETH staked, computed as  <span class="math">D_f(\equiv) - D_f(\not\equiv)</span>, is shown in Figure 9. The variable <span class="math">p</span> was set to 2 both for random and equally spaced validators.</p>
<p>The most significant improvement from equal spacing occurs in the second slot after the block has been proposed. By definition, the first slot will not contain any repetitions anyway. The improvements are most pronounced when there are fewer auxiliary committees, as these are the circumstances where 2048-ETH validators are not included in nearly every committee.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/c/acc51601fde62d0126aae0a675c43936385592aa.png" title="Figure 9"><img alt="Figure 9" height="475" src="https://ethresear.ch/uploads/default/optimized/3X/a/c/acc51601fde62d0126aae0a675c43936385592aa_2_690x475.png" width="690" /></a></div><p></p>
<p><strong>Figure 9.</strong> Change in <span class="math">D_{f}</span> at 30M staked when validators are equally spaced across the epoch, during a block’s progression to full finality.</p>
<p>The outcome for the 4/5 Zipfian staking set using <span class="math">p=2</span> and equal spacing is shown in Figure 10. It can be compared with the previous plot in Figure 5, that shows the outcome with <span class="math">p=1</span> and random spacing. The changes increase <span class="math">D_f</span> from 15M ETH to 20M ETH in the first slot when <span class="math">C_{\text{aux}}</span> is 3-4 and in the second slot when <span class="math">C_{\text{aux}}=1</span>.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/6/f6aff7e708f3cf850bec13a5f24edecadfbb27c3.png" title="Figure 10"><img alt="Figure 10" height="471" src="https://ethresear.ch/uploads/default/optimized/3X/f/6/f6aff7e708f3cf850bec13a5f24edecadfbb27c3_2_690x471.png" width="690" /></a></div><p></p>
<p><strong>Figure 10.</strong> Cumulative finalization of block <span class="math">n</span> for the 4/5 Zipfian staking set, with <span class="math">p=2</span> and equal spacing <span class="math">\equiv</span>.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50029-h-6-analysis-across-d-16" name="p-50029-h-6-analysis-across-d-16"></a>6. Analysis across <span class="math">D</span></h2>
<p>The analysis in this and the next section relies on <span class="math">p=2</span> and the randomized distribution of auxiliary validators <span class="math">\not\equiv</span>. Figure 11 shows how the aggregate finality gap varies with <span class="math">C_{\text{aux}}</span> across deposit size for the 4/5 Zipfian set. At lower quantities of stake, <span class="math">C_{\text{aux}}=0</span> gives the lowest <span class="math">\widetilde{F}_{\!g}</span>. At higher quantities of stake, <span class="math">C_{\text{aux}}=5</span> gives the lowest among those plotted. But increasing <span class="math">C_{\text{aux}}</span> all the way up to 7 will spuriously give the lowest results above 70M ETH staked. However, outcomes are very tightly overlapping at higher settings (hence they were not plotted), implying that in terms of <span class="math">\widetilde{F}_ {\! g}</span>, venturing above <span class="math">C_{\text{aux}}=4</span> will not offer significant improvements.</p>
<p>The characteristic shark fin-pattern emerges when validators are redistributed due to changes in <span class="math">C</span>. As <span class="math">D</span> increases while the distribution is kept fixed, <span class="math">V</span> also increases. Each “fin” represents the addition of one committee. This addition gives room for more auxiliary validators, which reduces <span class="math">\widetilde{F}_ {\! g}</span> when <span class="math">C_{\text{aux}}</span> is relatively low. However, if <span class="math">C_{\text{aux}}</span> is too high for the given validator set, the outcome is reversed, and the addition of one committee instead increases <span class="math">\widetilde{F}_ {\! g}</span>. This is evident in Figure 12, which zooms in on the outcome below <span class="math">D=</span> 35M ETH.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/9/39ecbbbad73e7b8ce1c6eeadf0da8d724fb1078a.png" title="Figure 11"><img alt="Figure 11" height="489" src="https://ethresear.ch/uploads/default/optimized/3X/3/9/39ecbbbad73e7b8ce1c6eeadf0da8d724fb1078a_2_690x489.png" width="690" /></a></div><p></p>
<p><strong>Figure 11.</strong> Aggregate finality gap for the 4/5 Zipfian set across <span class="math">D</span> for various numbers of auxiliary committees.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/9/79bedfdb55133ee3488fbd4872f8cb4d332a9c4c.png" title="Figure 12"><img alt="Figure 12" height="480" src="https://ethresear.ch/uploads/default/optimized/3X/7/9/79bedfdb55133ee3488fbd4872f8cb4d332a9c4c_2_690x480.png" width="690" /></a></div><p></p>
<p><strong>Figure 12.</strong> Aggregate finality gap for the 4/5 Zipfian set with <span class="math">D\leq</span> 35M ETH for various numbers of auxiliary committees.</p>
<p>Define <span class="math">\widetilde{F}^*_{\!g}</span> as the minimum aggregate finality gap, achieved at the associated minimum auxiliary committees <span class="math">C^*_{\text{aux}}.</span> This corresponds to the lowest line at any specific <span class="math">D</span> in Figures 11-12. Figure 13 plots <span class="math">\widetilde{F}^ * _ {\!g}</span> for all five staking sets. As evident, there are two fundamental factors that degrade committe-based cumulative finality: a higher quantity of stake and a lower level of consolidation.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/3/43112cac286cc97b03b0559f11dd9f0122178bff.png" title="Figure 13"><img alt="Figure 13" height="482" src="https://ethresear.ch/uploads/default/optimized/3X/4/3/43112cac286cc97b03b0559f11dd9f0122178bff_2_690x482.png" width="690" /></a></div><p></p>
<p><strong>Figure 13.</strong> The minimum aggregate finality gap across stake. Fast finality is degraded both by a higher quantity of stake and a lower level of consolidation.</p>
<p>Figure 14 instead focuses on how the optimal number of committees at <span class="math">\widetilde{F}^*_{\!g}</span> varies. However, the optimal number of committees will fluctuate greatly due to the fin-like pattern evident in Figure 12, and it is also a discrete measure. Therefore, parabolic interpolation (see <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#b3-interpolated-ground-truth-33">Appendix B.3</a>) was applied to three points around the minimum, resulting in a smoother representation of total committees, here denoted <span class="math">C^y</span>. Both the aggregate finality gap and the total number of committees rise linearly with an increase in the quantity of stake, keeping the distribution fixed.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/f/df23ba4bf89121fb4c3096ca2f52a45fee749450.png" title="Figure 14"><img alt="Figure 14" height="486" src="https://ethresear.ch/uploads/default/optimized/3X/d/f/df23ba4bf89121fb4c3096ca2f52a45fee749450_2_690x486.png" width="690" /></a></div><p></p>
<p><strong>Figure 14.</strong> Interpolated total number of committees that minimizes the aggregate finality gap.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50029-h-7-predicting-the-optimal-number-of-auxiliary-committees-17" name="p-50029-h-7-predicting-the-optimal-number-of-auxiliary-committees-17"></a>7. Predicting the optimal number of auxiliary committees</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50029-h-71-overview-18" name="p-50029-h-71-overview-18"></a>7.1 Overview</h3>
<p>How should the number of auxiliary committees (or any other setting such as <span class="math">p</span>) be determined during operation if the suggested variant of committee-based finality is pursued? Five options can be highlighted:</p>
<ol>
<li>Generate committees and compute <span class="math">\widetilde{F}_ {\! g}</span> (or some more appropriate measure, as further discussed in <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#b2-weighted-aggregate-finality-gap-32">Appendix B.2</a>) for various <span class="math">C_{\text{aux}}</span>-settings (e.g., 0-6), selecting the one that minimizes <span class="math">\widetilde{F}_ {\! g}</span>. This solution might have a high computational load if there are several hundred thousand validators.</li>
<li>Run the process for only the current and adjacent <span class="math">C_{\text{aux}}</span>-settings. If the analysis is performed frequently, store the results and rely on hysteresis, switching only if a clear majority of recent evaluations are in favor. For example, a threshold of 80% over the last week could be used.</li>
<li>If the computational load in (2) is still too high, a reduced validator set <span class="math">\mathcal{V}_ r</span> can be relied upon, with validators in <span class="math">\mathcal{V}_ r</span> drawn evenly spaced from the ordered full set <span class="math">\mathcal{V}</span>. The setting for <span class="math">\hat{V_a}</span> should then be reduced proportional to the reduction in the validator set before generating committees.</li>
<li>Compute some simple features of the validator set related to for example variability, and determine an appropriate number of auxiliary slots based on these features.</li>
<li>Specify a fixed number of auxiliary committees so that the mechanism performs reasonably well under a wide range of validator sets, e.g., <span class="math">C_{\text{aux}}=2</span>.</li>
</ol>
<p>When it comes to implementation complexity, all options are rather straightforward. A benefit of Options 1-3 is that client will need to implement the function for assigning validators to committees anyway. The remaining process is then the evaluation function described in <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#b1-evaluation-procedure-31">Appendix B.1</a>. This process does not have a high time complexity, but it might still be too computationally intensive if there are many hundred thousand validators.</p>
<p>Option 2 is then rather appealing, with some parallels to how hysteresis is leveraged when <a href="https://eth2book.info/capella/part2/incentives/balances/#hysteresis">updating validators’ effective balance</a>. Option 3 can further reduce the computational requirements by at least an order of magnitude (10x), perhaps up to two (100x). A question then is what accuracy that can be achieved if the validator set is reduced to for example <span class="math">|\mathcal{V}_ r| = 1000</span> or <span class="math">|\mathcal{V}_ r| = 5000</span>. This is studied in <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#p-50029-h-72-prediction-accuracy-with-a-reduced-validator-set-19">Section 7.2</a>. Another question is of course the viability of Option 4, which could further reduce the computational requirements. This is studied in <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#p-50029-h-73-prediction-accuracy-using-general-features-20">Section 7.3</a>, and Option 5 is reviewed in <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#p-50029-h-74-prediction-accuracy-using-a-fixed-number-of-auxiliary-committees-21">Section 7.4</a>. The conclusion of the experiment, expanded on in <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#p-50029-h-9-conclusion-and-discussion-26">Section 9</a>, is that Option 2, 3 or 5 seems like the most viable, with Option 5 as a natural starting point.</p>
<p>The ground truth for modeling was not based on the optimal number of auxiliary committees <span class="math">C_{\text{aux}}</span> but instead on the optimal number of auxiliary validators <span class="math">V_{\text{aux}}</span>, mainly to circumvent the fin-like pattern in Figures 11-12. To achieve a smoother target, a more refined point <span class="math">V^{y}_{\text{aux}}</span> between neighboring <span class="math">V_{\text{aux}}</span> values was derived via parabolic interpolation—as previously illustrated also for <span class="math">C^y</span> in Figure 14. A further small adjustment was made before interpolation, slightly weighting up <span class="math">\widetilde{F}_ {\! g}</span> when a large number of auxiliary slots relative to the number of regular slots was applied. <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#b2-weighted-aggregate-finality-gap-32">Appendix B.2</a>-<a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#b3-interpolated-ground-truth-33">3</a> explains the full procedure for generating the ground truth. <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#b4-generation-of-a-log-normal-distributed-validator-set-34">Appendix B.4</a> then describes the generation of an additional log-normal validator set to provide a greater spread in the evaluated examples. It is shown in yellow in Figures 15-20. One thousand validator sets were generated for each of the six different distributions for the analysis with <span class="math">D</span> in the range 10M-80M ETH, giving a total of 6000 examples.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50029-h-72-prediction-accuracy-with-a-reduced-validator-set-19" name="p-50029-h-72-prediction-accuracy-with-a-reduced-validator-set-19"></a>7.2 Prediction accuracy with a reduced validator set</h3>
<p>How accurate can the predictions be with a reduced validator set <span class="math">\mathcal{V}_r</span> from Option 3, if <span class="math">\mathcal{V}_r</span> consists of only 1000 or 5000 validators? To test this, the predicted optimal, <span class="math">V^{x}_{\text{aux}}</span>, was computed on the reduced set, using the same evaluation procedure as when setting the ground truth <span class="math">V^{y}_{\text{aux}}</span> on the full set (<a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#b1-evaluation-procedure-31">Appendix B.1</a>). The outcome is shown in Figure 15 for 1000 validators (<span class="math">R^2=0.893</span>) and in Figure 16 for 5000 validators (<span class="math">R^2=0.960</span>). The broader black diagonal line represents perfect predictions, while the thinner black lines indicate the range where predictions fall within <span class="math">\hat{V}_{\!a}</span>.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/6/c64f24c7ae178b92f231406d54797377a84fbef2.png" title="Figure 15"><img alt="Figure 15" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/c/6/c64f24c7ae178b92f231406d54797377a84fbef2_2_563x500.png" width="563" /></a></div><p></p>
<p><strong>Figure 15.</strong> Predictions of the optimal number of auxiliary validators compared to ground truth, based on a reduced set of 1000 validators.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/8/c81b0158efa89ae7166369f10c33be5d8d67858c.png" title="Figure 16"><img alt="Figure 16" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/c/8/c81b0158efa89ae7166369f10c33be5d8d67858c_2_563x500.png" width="563" /></a></div><p></p>
<p><strong>Figure 16.</strong> Predictions of the optimal number of auxiliary validators compared to ground truth, based on a reduced set of 5000 validators.</p>
<p>As evident, at higher values for <span class="math">V_\text{aux}</span>, predictions become increasingly less accurate. This is related to the phenomenon shown in Figure 11, wherein the relative difference in <span class="math">\widetilde{F}_{\! g}</span> will not be that large between the the best settings for <span class="math">C_{\text{aux}}</span> (and thus <span class="math">V_{\text{aux}}</span>). The broader implication is that getting <span class="math">C_\text{aux}</span> slightly wrong will then not matter much. At the other end, getting it wrong at lower <span class="math">C_\text{aux}</span> towards the bottom left corner is more of a concern. Note also that only examples where <span class="math">D&gt;</span> 40M ETH are problematic (review Figure 20 for the ground truth range 25M-35M).</p>
<p>The errors in the predictions stem from how the random selection influences the composition of the committees. Repeating the experiment several times and averaging the outcomes will therefore serve to improve accuracy (as would equally spaced validators described in <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#p-50029-h-52-equal-spacing-15">Section 5.2</a>). An example of the average outcome for four validator sets with <span class="math">V_{r} =</span> {1000, 2000, 3000, 5000} respectively is shown in Figure 17. The predictions are now all within the <span class="math">\hat{V}_a</span> boundary, and <span class="math">R^2=0.981</span>. It must also be remembered that while <span class="math">V^{y}_{\text{aux}}</span> by definition is the ground truth ideal outcome, it will also itself reflect a random division of validators during generation.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/4/e427ff3eddc10528a1ae16c782f8ef56b0a3a066.png" title="Figure 17"><img alt="Figure 17" height="499" src="https://ethresear.ch/uploads/default/optimized/3X/e/4/e427ff3eddc10528a1ae16c782f8ef56b0a3a066_2_564x499.png" width="564" /></a></div><p></p>
<p><strong>Figure 17.</strong> Predictions of the optimal number of auxiliary validators compared to ground truth, based on the average of four reduced sets.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50029-h-73-prediction-accuracy-using-general-features-20" name="p-50029-h-73-prediction-accuracy-using-general-features-20"></a>7.3 Prediction accuracy using general features</h3>
<p>Can general features be used to determine the optimal number of auxiliary validators? To explore this, features were generated for the simulated validator sets, capturing basic properties such as validator count, deposit size, and various measures of variability. Polynomial feature expansion of degree 2 was used to generate all monomials of the original features, capturing interactions and non-linear relationships. Predictions were then made using multiple linear regression. The final features were selected through a semi-automatic forward feature selection process, manually choosing among top predictors to premier those that are easier to interpret (a key requirement is a simple model). This process resulted in a linear regression model consisting of three features: {<span class="math">V\sigma</span>, <span class="math">V_w\delta</span>, <span class="math">D^2</span>}.</p>
<p>The first feature is the number of validators <span class="math">V</span> multiplied by the standard deviation of the validator set <span class="math">\sigma</span>. If the standard deviation is high, auxiliary validators become particularly useful for reducing the finality gap. The second feature multiplies the average absolute deviation, denoted <span class="math">\delta</span>, with a weighted count of validators <span class="math">V_w</span>. The weighting assigned validators of size 32 and 2048 a weight of 1, with the weight then log-linearly falling to 0 at the mean validator size <span class="math">\bar{\mathcal{V}}</span>. Specifically, each validator holding <span class="math">s</span> ETH, received a weighting of:</p>
<div class="math">
\text{Weighted count}(s) = 
\begin{cases}
1 - \frac{\log(s) - \log(32)}{\log(\bar{\mathcal{V}}) - \log(32)} &amp; \text{if } s \leq s_1 \\
1 - \frac{\log(2048) - \log(s)}{\log(2048) - \log(\bar{\mathcal{V}})}
&amp; \text{if } s &gt; s_1
\end{cases}.
</div>
<p>Predictions with <span class="math">V^{x}_{\text{aux}}&lt;0</span> were set to 0 (the number of auxiliary validators cannot be negative). The predictions had <span class="math">R^2=0.975</span> and are shown in Figure 18. The wider dispersion in the lower left corner is somewhat problematic, as previously discussed. Option 4 therefore gives slightly worse outcomes than Option 3. Also note that since there was no training/test split, quite a bit of overfitting can be assumed. If Option 4 is to be pursued seriously, there would need to be a test set and a wider variety in training examples.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/8/c8872c73082ff82fefb3fb7ce62dab2bd031b5fe.png" title="Figure 18"><img alt="Figure 18" height="499" src="https://ethresear.ch/uploads/default/optimized/3X/c/8/c8872c73082ff82fefb3fb7ce62dab2bd031b5fe_2_564x499.png" width="564" /></a></div><p></p>
<p><strong>Figure 18.</strong> Predictions of the optimal number of auxiliary validators compared to ground truth, based on general features capturing, e.g., variability in the validator set.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50029-h-74-prediction-accuracy-using-a-fixed-number-of-auxiliary-committees-21" name="p-50029-h-74-prediction-accuracy-using-a-fixed-number-of-auxiliary-committees-21"></a>7.4 Prediction accuracy using a fixed number of auxiliary committees</h3>
<p>It may also be interesting to review the outcome with a fixed number of auxiliary committees. The outcome when setting all examples to a fixed <span class="math">C_{\text{aux}}=2</span> is shown in Figure 19. It generates predictions in a vertical band that is <span class="math">\hat{V}_a</span> wide, with deviations from the ground truth extending well beyond <span class="math">\hat{V}_a</span>. Figure 20 instead focuses on the range 25-35M ETH. In this case predictions tend to fall within <span class="math">\hat{V}_a</span>, except for the log-normal distribution, which has several examples with little to no variability in validator balances (in that case, auxiliary slots bring no benefit). This illustrates that if <span class="math">D</span> is kept in a narrow range, the variation in the optimal number of auxiliary committees/validators is reduced considerably. Starting with a fixed number of auxiliary committees is therefore a viable baseline strategy.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/6/3621612fbea7f760e82b240c1375c1e7a8b2d7c1.png" title="Figure 19"><img alt="Figure 19" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/3/6/3621612fbea7f760e82b240c1375c1e7a8b2d7c1_2_563x500.png" width="563" /></a></div><p></p>
<p><strong>Figure 19.</strong> Predictions of the optimal number of auxiliary validators compared to ground truth, with a fixed <span class="math">C_{\text{aux}}=2</span>.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/2/d2d1b3f17a10ce1ce87327404385f1eba9dd8068.png" title="Figure 20"><img alt="Figure 20" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/d/2/d2d1b3f17a10ce1ce87327404385f1eba9dd8068_2_563x500.png" width="563" /></a></div><p></p>
<p><strong>Figure 20.</strong> Predictions of the optimal number of auxiliary validators compared to ground truth, with a fixed <span class="math">C_{\text{aux}}=2</span>, for validator sets in the range <span class="math">D=</span> 25M-35M ETH.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50029-h-8-properties-related-to-consensus-formation-22" name="p-50029-h-8-properties-related-to-consensus-formation-22"></a>8. Properties related to consensus formation</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50029-h-81-committee-rotation-ratio-23" name="p-50029-h-81-committee-rotation-ratio-23"></a>8.1 Committee rotation ratio</h3>
<p>Define the committee rotation ratio <span class="math">R</span> as the proportion of the stake that is replaced in successive committees following finalization. If all validators are replaced, <span class="math">R=1</span>, and if all remain, <span class="math">R=0</span>. Figure 21 shows how <span class="math">R</span> changes across <span class="math">V_{\text{aux}}</span> at 30M ETH staked. Aside from the relevance to the slow-rotation regime of the available chain, a ceiling has previously been <a href="https://ethresear.ch/t/a-model-for-cumulative-committee-based-finality/10259#alternative-single-slot-epoch-casper-ffg-3">discussed</a> for a finality gadget leveraging Casper FFG. That suggestion, <span class="math">R=0.25</span>, is indicated by a black horizontal line. The circles indicate the point where the aggregate finality gap is minimized (<span class="math">V^*_{\text{aux}}</span>). This happens at rather modest rotation ratios and can of course readily be adjusted. Rotation becomes comparatively slow after adding around 150k auxiliary validator instances (<span class="math">C_{\text{aux}}\approx5</span>), where 90% of the stake remains whenever a committee finalizes and rotates.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/9/b9b1d8106767f5edb53d135da3da373450034380.png" title="Figure 21"><img alt="Figure 21" height="474" src="https://ethresear.ch/uploads/default/optimized/3X/b/9/b9b1d8106767f5edb53d135da3da373450034380_2_690x474.png" width="690" /></a></div><p></p>
<p><strong>Figure 21.</strong> Committee rotation ratio <span class="math">R</span> across auxiliary validators. Circles indicate the point where the aggregate finality gap is minimized.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50029-h-82-activity-rate-24" name="p-50029-h-82-activity-rate-24"></a>8.2 Activity rate</h3>
<p>The activity rate <span class="math">a</span> captures the proportion of the committees that a validator is active in (defined as <span class="math">p</span> in the <a href="https://ethresear.ch/t/orbit-ssf-solo-staking-friendly-validator-set-management-for-ssf/19928#active-validator-set-management-8">Orbit post</a>). The reciprocal <span class="math">a^{-1}</span> captures the average number of slots until the validator has participated in one of them and will be referred to as a validator’s “<a href="https://en.wikipedia.org/wiki/Apsis">apsis</a>”—its orbital distance.</p>
<p>Figure 22 shows how <span class="math">a</span> varies with validator size <span class="math">s</span> when the aggregate finality gap is minimized (at <span class="math">V^*_{\text{aux}}</span> marked by circles in Figure 21). As evident, <span class="math">a(s)</span> is not a fixed property across validator sets, and will vary with, e.g., consolidation level and deposit size. Leveraging a variable orbit (“Vorbit”) seems natural, because a multitude of features that Ethereum wishes to optimize for vary with the composition of the validator set (a <a href="https://ethresear.ch/t/orbit-ssf-solo-staking-friendly-validator-set-management-for-ssf/19928#active-validator-set-management-8">dynamic threshold</a> has also previously been suggested).</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/a/ca23deafe8d0c0ba47e93cec6d46fa38284b7f8b.png" title="Figure 22"><img alt="Figure 22" height="474" src="https://ethresear.ch/uploads/default/optimized/3X/c/a/ca23deafe8d0c0ba47e93cec6d46fa38284b7f8b_2_689x474.png" width="689" /></a></div><p></p>
<p><strong>Figure 22.</strong> The activity rate <span class="math">a</span> across validator balances <span class="math">s</span> at the minimized aggregate finality gap and 30M staked for the five sets. Note that the x-axis is log-scaled.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50029-h-83-the-activity-ratio-and-its-implications-on-staking-economics-25" name="p-50029-h-83-the-activity-ratio-and-its-implications-on-staking-economics-25"></a>8.3 The activity ratio and its implications on staking economics</h3>
<p>The activity ratio <span class="math">a_r = a(s_{\mathrm{min}})/a(s_{\mathrm{max}})</span> captures how often validators with a small balance are active, relative to validators with larger balances. The apsis ratio again denotes the reciprocal <span class="math">a^{-1}_r</span> and can sometimes be easier to interpret: <span class="math">a^{-1}_r=32</span> means that small validators are present 32 times less frequenly than big validators. When <span class="math">a_r</span> is small (and <span class="math">a^{-1}_r</span> thus big), stakers running validators with a lower balance close to <span class="math">s_{\mathrm{min}}</span> will <a href="https://ethresear.ch/t/orbit-ssf-solo-staking-friendly-validator-set-management-for-ssf/19928#individual-consolidation-incentives-12">bear a lower slashing risk</a> than stakers running validators with higher balances close to <span class="math">s_{\mathrm{max}}</span>. Inactive validators can hardly (by mistake or otherwise) make slashable actions. Someone running many small validators can therefore catch a faulty setup early so that a lower proportion of their validators are affected. Likewise, a small validator is less likely to get caught up in a <a href="https://dankradfeist.de/ethereum/2022/03/24/run-the-majority-client-at-your-own-peril.html">catastrophic</a> slashing event. Even if such an event only takes place every 100 years, it still meaningfully impacts the expected value of staking, particularly if the total staking yield decreases in the future.</p>
<p>In a slow-rotating mechanism, <span class="math">a_r</span> is particularly relevant, given that stakers with a high average apsis on their validators can have even more time to, e.g., adjust faulty setups for inactive validators before they return as active. Yet <span class="math">a_r</span> is relevant also in a fast-rotating mechanism. To encourage consolidation, <a href="https://ethresear.ch/t/orbit-ssf-solo-staking-friendly-validator-set-management-for-ssf/19928#individual-consolidation-incentives-12">individual incentives</a> should compensate for the additional risks that large validators take on, relative to small validators. Individual incentives can potentially also be combined with collective consolidation incentives. The individual incentives will generally need to be higher when <span class="math">a_r</span> is smaller, because the benefit of running smaller validators increases. It is therefore desirable to keep <span class="math">a_r</span> closer to 1, whenever possible. This minimizes the yield differential between small and large validators, reducing “tensions” among stakers. Such tensions emerge under high yield differentials, where Ethereum will favor (or at least appear to favor) stakers with more capital.</p>
<p>Even if the additional yield is intended to compensate for increased risks, only stakers with high capital will have the option to choose between higher and lower risk. Stakers with more capital will also disproportionately benefit from the ability to adjust faulty setups among one of their many validators, should they decide to split up their stake. Tensions may therefore also emerge if a large staking service provider (SSP) relies on small validators to reduce its risk profile, and <a href="https://ethresear.ch/t/orbit-ssf-solo-staking-friendly-validator-set-management-for-ssf/19928#collective-incentives-15">collective incentives</a> as a result bring down everyone’s yield. There may for example be calls to discouragement attack the specific SSP’s validators, introducing an unhealthy dynamic to consensus formation. There are similarities to the type of issues that may emerge in <a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590">MEV burn</a> when using an English auction, where SSPs will need to <a href="https://ethresear.ch/t/burn-incentives-in-mev-pricing-auctions/19856#d-metagame-staker-initiated-griefing-9">specifically target each other</a> through early builder bids to remain competitive.</p>
<p>In Figure 22, <span class="math">a_r</span> is approximately 1/6 for the Zipfian staking set at <span class="math">p=2</span>. Validators with 2048 ETH are always present, and validators with 32 ETH are only present as regular validators in one of the 6 committees of the epoch. This situation is generally better than if the smallest validators only are present once every 32 slots, as with the Orbit thresholding mechanism. The Orbit thresholding mechanism can however be adjusted by setting <span class="math">p&lt;1</span>. In a subsequent post, addressing slow rotation for the available chain, that avenue is intended to be explored further, together with other related consensus issues beyond the scope of this post.</p>
<p>Allowing 1-ETH validators would further reduce the activity ratio, requiring an increased yield differential. Smaller validator balances such as 1 ETH will thus require a communication effort to explain why these validators receive a markedly lower yield, and why large staking service providers cannot be prevented from relying on 1-ETH validators to lower their risk profile (but certainly nudged in the opposite direction via public discourse).</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50029-h-9-conclusion-and-discussion-26" name="p-50029-h-9-conclusion-and-discussion-26"></a>9. Conclusion and discussion</h2>
<p>Cumulative committee-based finality has been reviewed under fast-rotating validator committees. A good measure for evaluating cumulative finality is the aggregate finality gap <span class="math">\widetilde{F}_{\!g}</span>, which aggregates the finality gap for a block during its progression to full finality. The four main avenues for reducing (improving) <span class="math">\widetilde{F}_{\!g}</span> are:</p>
<ul>
<li>adding a few auxiliary committees (around 2-4) beyond those required for all validators to cast one vote in an epoch,</li>
<li>including the largest validators in almost every committee,</li>
<li>equally spacing auxiliary validators to minimize successive repetitions,</li>
<li>pursuing “circular finality” (repeating epochs over a longer era) and “spiral finality” (constrained shuffling) to mitigate degradation in cumulative finality during shuffling.</li>
</ul>
<p>Five validator sets were used in the analysis, capturing various levels of Zipfianness. <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#p-50029-h-6-analysis-across-d-16">Section 6</a> made it clear that both insufficient validator consolidation and a higher quantity of stake (keeping the distributions fixed) impede finality, thereby increasing <span class="math">\widetilde{F}_{\!g}</span>. When considering tempering the quantity of stake, one argument has been that it will generate a higher quantity of small validators. Regardless of the merits of this theory, it should be noted that a higher quantity of stake and a large proportion of small validators combines to produce the worst conditions for accruing finality, as shown in Figure 13.</p>
<p>Methods for dynamically adjusting the number of auxiliary commitees were reviewed. The best method is to simply simulate and evaluate the outcome with the same or one more/less auxiliary committees. This can be done on a reduced validator set to improve performance, if necessary. However, it is not a strict requirements that the number of auxiliary slots should change dynamically. The optimal setting for a given point in time is likely to remain viable for quite a while.</p>
<p>Properties related to consensus formation are important to keep in mind. As shown in <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#p-50029-h-8-properties-related-to-consensus-formation-22">Section 8</a>, the committee rotation ratio <span class="math">R</span> falls rather quickly with added auxiliary validators. It would be beneficial to map out more specific requirements on <span class="math">R</span> from a consensus perspective going forward, both for the available chain and the finality gadget. Requirements regarding the activity ratio <span class="math">a_r</span> are easier to understand in some respects; a higher ratio is better when considered in isolation, as it reduces tensions and yield differentiation.</p>
<p>The assumption of a pure Zipfian staking distribution becomes rather dubious if the range is extended much further. If the minimum staking balance is reduced from 32 ETH to 1 ETH, there will not necessarily be an exponential increase in stakers. One reason is that fixed costs for running a validator eventually surpass yield revenue as the staking balance decreases. For example, when focusing exclusively on fixed costs, if running a 32-ETH validator requires a 1% yield to remain profitable, then running a 1-ETH validator would require a 32% yield. Another point to keep in mind when considering a move to allow for 1-ETH validators is the decreased activity ratio <span class="math">a_r</span> that it would bring. At the same time, allowing users with less capital to become active participants in the consensus process is of course fundamentally valuable and something to strive for.</p>
<hr />
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-50029-appendix-a-zipfian-distribution-27" name="p-50029-appendix-a-zipfian-distribution-27"></a>Appendix A: Zipfian distribution</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50029-a1-quantity-of-stakers-under-a-pure-zipfian-distribution-28" name="p-50029-a1-quantity-of-stakers-under-a-pure-zipfian-distribution-28"></a>A.1 Quantity of stakers under a pure Zipfian distribution</h3>
<p>For large <span class="math">N</span>, the harmonic series</p>
<div class="math">
1 + \frac{1}{2} + ... + \frac{1}{N}
</div>
<p>approaches <span class="math">\ln(N)+\gamma</span>, where <span class="math">\gamma</span> is the Euler–Mascheroni constant, approximately 0.577. The total quantity of stake is</p>
<div class="math">
D = 32N(\ln(N)+\gamma)
</div>
<p>The task now is to deduce <span class="math">N</span>, given a specific <span class="math">D</span>. Let <span class="math">u = \ln(N) + \gamma</span>. Then <span class="math">N = e^{u - \gamma}</span>, and the equation can be rearranged as follows:</p>
<div class="math">
\frac{D}{32}e^\gamma = u e^{u}
</div>
<p>Use the definition of the Lambert W function, which gives <span class="math">u = W(z)</span>, where<br />
<span class="math">z = \frac{D}{32} e^\gamma</span>:</p>
<div class="math">
u = W \left( \frac{D}{32} e^\gamma \right)
</div>
<p>Recall that <span class="math">u = \log(N) + \gamma</span> and <span class="math">N = e^u</span>. Substituting this in gives</p>
<div class="math">
\log(n) + \gamma = W \left( \frac{D}{32} e^\gamma \right),
</div>
<p>and thus</p>
<div class="math">
\log(N) = W \left( \frac{D}{32} e^\gamma \right) - \gamma.
</div>
<p>Both sides are finally exponentiated to solve for <span class="math">N</span>:</p>
<div class="math">
N = e^{ W \left( \frac{D}{32} e^\gamma \right) - \gamma}
</div>
<p>The equation provides a simple way to deduce <span class="math">N</span> given <span class="math">D</span>, such that the baseline Zipfian distribution in the form of the harmonic series can be used in accordance with the previous specification. The following two lines of Python code generate the staking set <code>S</code> for a specific deposit size, where <code>eg=np.euler_gamma</code>:</p>
<pre><code class="lang-python">N = round(np.exp(scipy.special.lambertw(D*np.exp(eg)/(32))-eg))
S = 32*N/np.arange(1, N + 1)
</code></pre>
<h3><a class="anchor" href="https://ethresear.ch#p-50029-a2-quantity-of-validators-under-a-pure-zipfian-distribution-29" name="p-50029-a2-quantity-of-validators-under-a-pure-zipfian-distribution-29"></a>A.2 Quantity of validators under a pure Zipfian distribution</h3>
<p>Recall that the number of stakers is as previously derived</p>
<div class="math">
N = e^{ W \left( \frac{D}{32} e^\gamma \right) - \gamma}.
</div>
<p>Among these <span class="math">N</span> stakers, 1/64 will have a stake of 2048 or higher:</p>
<div class="math">
2048 = \frac{32N}{N_{h}},
</div>
<div class="math">
N_{h} = \frac{N}{64}.
</div>
<p>Under ideal circumstances, that stake will be divided up into</p>
<div class="math">
V_{h} = \frac{1}{2048}\sum_{n=1}^{N_h} \frac{32N}{n} = \frac{32N}{2048} \cdot (\ln(N/64) + \gamma) = \frac{N}{64} \cdot (\ln(N/64) + \gamma)
</div>
<p>validators. However, the <span class="math">\frac{N}{64}</span> stakers will roughly add an expected <span class="math">\gamma</span> validators each to that figure, due to waste when splitting up stake into its last validators (i.e., the cumulative effect of the fractional parts). There are <span class="math">63N/64</span> stakers with less than 2048 ETH. The total number of validators under a pure Zipfian distribution, where stakers maximize consolidation, is thus</p>
<div class="math">
V=\frac{N}{64} \left(63+\ln(N/64) + 2\gamma \right).
</div>
<p>The equation is a fairly exact approximation. For example, at 30M ETH, the outlined procedure gives 88065 validators, and the equation (after rounding <span class="math">N</span> in the first step) gives 88065.385 validators.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50029-appendix-b-prediction-and-evaluation-30" name="p-50029-appendix-b-prediction-and-evaluation-30"></a>Appendix B: Prediction and evaluation</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50029-b1-evaluation-procedure-31" name="p-50029-b1-evaluation-procedure-31"></a>B.1 Evaluation procedure</h3>
<p>Cumulative finality of a block is simulated by a boolean mask that iterates through the committees, entering every validator seen up to and including a particular slot/committee. Thus, the starting point is a binary mask of all validators in the committee of the first processed slot. The operation then progresses through the slots of the epoch, entering previously unseen validators from a committee to the binary mask applicable to a specific slot. Finalized validators are then summed at each slot, from which <span class="math">D_f</span> and thus <span class="math">\widetilde{F}_ {\! g}</span> are computed. For best accuracy, the evaluation is performed in a circular fashion, as previously discussed. If <span class="math">S_{\text{ep}}</span> is high, there can be an upper limit on how many starting points that are evaluated, e.g., starting at every other or every third slot. The evaluation for <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#p-50029-h-6-analysis-across-d-16">Section 6</a> used a limit of ten different starting points.</p>
<p>A potential optimization is to also compute a separate mask (or list of indices) that specifies only the newly unseen validators for each slot. This mask/list specifies if the validator is present in the current committee <code>AND</code> <code>NOT</code> present in the cumulative finality mask computed up to the previous committee. The benefit is that summation can be done only of the newly added validators, subsequently adding the cumulative sum derived at the previous slot/committee.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50029-b2-weighted-aggregate-finality-gap-32" name="p-50029-b2-weighted-aggregate-finality-gap-32"></a>B.2 Weighted aggregate finality gap</h3>
<p>The aggregate finality gap <span class="math">\widetilde{F}_ {\! g}</span> generally seems like a well-balanced optimization criterion. Yet in some scenarios, it may be desirable to prioritize high finality in the initial slots (thus also slowing down rotation), while in others, a short time to full finality overall may be preferred (thus also increasing the activity ratio discussed in <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#p-50029-h-83-the-activity-ratio-and-its-implications-on-staking-economics-25">Section 8.3</a>). A weighted aggregate finality gap can therefore be useful. This post used a weighting that provides a slightly shorter time to full finality on average. This helped resolve edge cases in the log-normal set (Appendix B.4) where the mechanism was brought very close to full finality, but the last fraction of finality took several slots. However, the opposite direction can also be explored, factoring in, for example, requirements concerning rotation speed.</p>
<p>Define a scenario where full finality can be achieved in 2 regular slots, but two auxiliary validators are added, as {2|4}. The weighting was designed to affect the following outcomes equally: {2|4}, {4|7}, {10|15} and {21|28}. This weighting for {<span class="math">a</span>|<span class="math">b</span>} was:</p>
<div class="math">
w = \frac{b\sqrt[3]{b-a}}{ka}.
</div>
<p>The constant <span class="math">k</span> was set to <span class="math">2^6</span> and the weight applied to each evaluated number of auxiliary slots. This can be done in two ways: <span class="math">\widetilde{F}_ {\! gw} = \widetilde{F}_ {\! g}(1+w)</span> or <span class="math">\widetilde{F}_ {\! gw} = \widetilde{F}_ {\! g} \ +w</span>, with the first being used in this work.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50029-b3-interpolated-ground-truth-33" name="p-50029-b3-interpolated-ground-truth-33"></a>B.3 Interpolated ground truth</h3>
<p>The ground truth for modeling was not auxiliary committees <span class="math">C_{\text{aux}}</span> but instead the number of auxiliary validators <span class="math">V_{\text{aux}}</span> to be added. The main reason  why <span class="math">C_{\text{aux}}</span> is generally undesirable as a ground truth is related to the fin-like pattern in Figures 11-12. The optimal <span class="math">C_{\text{aux}}</span> will shift at the boundaries where regular committees require one more regular slot, causing the ground truth to oscillate as <span class="math">D</span> rises. Targeting auxiliary validators avoids this issue. The total number of committees <span class="math">C</span> shown in Figure 14 could have been used as well, but this precludes parabolic interpolation with <span class="math">\widetilde{F}_ {\! gw}</span> for the regular committee  as one of the interpolation points.</p>
<p>A remaining issue is that the <span class="math">V_{\text{aux}}</span> that minimizes <span class="math">\widetilde{F}_ {\! gw}</span> is discretized into steps differing by <span class="math">\hat{V}_ {\!a}</span>, since the minimum can only be defined at integers in <span class="math">C_ {\text{aux}}</span>. Define the number of auxiliary committees that minimizes <span class="math">\widetilde{F}_ {\! gw}</span> as <span class="math">C^*_{\text{aux}}</span>. Parabolic interpolation was performed across the three neighboring points, <span class="math">\widetilde{F}_ {\! gw}(C^*_{\text{aux}}-1)</span>, <span class="math">\widetilde{F}_ {\! gw}(C^*_{\text{aux}})</span> and <span class="math">\widetilde{F}_ {\! gw}(C^*_{\text{aux}}+1)</span> to derive a relative position:</p>
<div class="math">
w_{V} = \frac{\widetilde{F}_ {\! gw}(C^*_{\text{aux}}+1)-\widetilde{F}_ {\! gw}(C^*_{\text{aux}}-1)}{2(2\widetilde{F}_ {\! gw}(C^*_{\text{aux}}) - \widetilde{F}_ {\! gw}(C^*_{\text{aux}}-1) - \widetilde{F}_ {\! gw}(C^*_{\text{aux}}+1))}.
</div>
<p>Define <span class="math">V_{\text{aux}}(C^*_{\text{aux}})</span> as the number of auxiliary validators at the optimal number of auxiliary committees. The ground truth <span class="math">V^{y}_{\text{aux}}</span> is given by the <span class="math">w_{V}</span>-weighted average of neighboring values. Thus, if <span class="math">w_{V}&lt;0</span>, it becomes</p>
<div class="math">
V^{y}_{\text{aux}}=V_{\text{aux}}(C^*_{\text{aux}}-1)(1-w_{V}) + V_{\text{aux}}(C^*_{\text{aux}})w_{V},
</div>
<p>with a corresponding weighting applied if <span class="math">w_{V}&gt;0</span>. Predictions for the number of auxiliary validators in <a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464#p-50029-h-7-predicting-the-optimal-number-of-auxiliary-committees-17">Section 7</a> are correspondigly defined as <span class="math">V^{x}_{\text{aux}}</span>.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50029-b4-generation-of-a-log-normal-distributed-validator-set-34" name="p-50029-b4-generation-of-a-log-normal-distributed-validator-set-34"></a>B.4 Generation of a log-normal distributed validator set</h3>
<p>To provide a wider variety of examples of validator sets, an additional set with a log-normal distribution was generated. The mean <span class="math">\mu_{\mathcal{V}}</span> was first drawn from a normal distribution centered at 400 ETH with a standard deviation of 128 ETH, restricted to lie within the range <span class="math">s_{\text{min}} = 32</span> ETH to <span class="math">s_{\text{max}} = 2048</span> ETH. Next, the standard deviation <span class="math">\sigma</span> of the log-normal distribution was drawn uniformly from the interval [0, 3]. To provide edge cases with validator sets that have no variability at all, any <span class="math">\sigma</span> below 0.2 was set to 0.</p>
<p>Given the selected mean <span class="math">\mu_{\mathcal{V}}</span> and standard deviation <span class="math">\sigma</span>, the mean of the log-normal distribution in the logarithmic space <span class="math">\mu</span> was computed as</p>
<div class="math">
\mu = \ln(\mu_{\mathcal{V}}) - \frac{\sigma^2}{2},
</div>
<p>with the goal of keeping the mean in the original space close to <span class="math">\mu_{\mathcal{V}}</span>. Validators were then generated up to the sought quantity of stake <span class="math">D</span> by sampling from a log-normal distribution defined by the parameters <span class="math">\mu</span> and <span class="math">\sigma</span>, ensuring that each generated validator remained within the bounds <span class="math">s_{\text{min}}</span> to <span class="math">s_{\text{max}}</span>.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 20 Sep 2024 18:44:41 +0000</pubDate>
</item>
<item>
<title>Privacy Problems in the P2P Network and What They Tell Us</title>
<link>https://ethresear.ch/t/privacy-problems-in-the-p2p-network-and-what-they-tell-us/20463</link>
<guid>https://ethresear.ch/t/privacy-problems-in-the-p2p-network-and-what-they-tell-us/20463</guid>
<content:encoded><![CDATA[
<div> 关键词：Ethereum、P2P网络、验证者、匿名性、安全风险

总结:

文章探讨了在以太坊的点对点(P2P)网络中验证者身份被匿名观察者识别的风险。以太坊的P2P网络允许验证者交换重要消息，如区块和验证，以保持区块链运行。通过分析节点行为，研究发现，仅需三天的数据，即可识别超过15%的以太坊验证者。这主要归因于验证者的投票（即验证）被分为时间分割和网络分割两个方面。在时间分割中，验证者每32个周期只需投票一次；在网络分割中，验证者被划分为64个委员会，每个委员会的验证者负责收集并组合验证信息。

研究者提出了一种简单的匿名识别方法，利用理想节点的行为模式来识别验证者与节点之间的关联。实际情况下，由于网络消息数据不完美，研究者还开发了一系列策略（称为启发式方法）来更准确地将验证者与节点关联起来。

对比其他已知的识别方法，本文的方法需要的数据量更少，且对噪声的敏感度较低，能更精确地识别更多网络中的验证者。实验证明，在四台节点上部署测量工具的三个月内，可以识别出超过15%的以太坊验证者。此外，研究还揭示了验证者的地理位置分布、云托管情况以及参与验证的大型staking池的规模。

然而，这些匿名性丧失带来的安全风险不容忽视。攻击者可能利用已知的验证者身份发动DoS攻击或BGP劫持，甚至阻止网络同步，从而破坏网络的最终性。为缓解这些风险，文章建议采取提高网络匿名性（如增加子网参与度、跨多节点部署验证者等）和防御分布式拒绝服务攻击（如使用秘密领导者选举机制）等措施。

通过上述分析，文章强调了在提升以太坊网络安全性的同时保护验证者隐私的重要性。 <div>
<p>Authors: <a href="https://liobaheimba.ch/" rel="noopener nofollow ugc">Lioba Heimbach</a>, <a href="https://yannvonlanthen.com/" rel="noopener nofollow ugc">Yann Vonlanthen</a>, Juan Villacis, <a href="https://luciannakiffer.com/" rel="noopener nofollow ugc">Lucianna Kiffer</a>, <a href="https://disco.ethz.ch/" rel="noopener nofollow ugc">Roger Wattenhofer</a></p>
<p>Preprint: <a class="inline-onebox" href="https://arxiv.org/abs/2409.04366" rel="noopener nofollow ugc">[2409.04366] Deanonymizing Ethereum Validators: The P2P Network Has a Privacy Issue</a></p>
<h2><a class="anchor" href="https://ethresear.ch#p-50027-tldr-1" name="p-50027-tldr-1"></a>TL;DR</h2>
<p>The messages exchanged in the Ethereum P2P network allow a silent observer to deanonymize validators in the network, i.e., map a validator to an IP address of a node in the network. Our deanonymization technique is simple, cost-effective, and capable of identifying over 15% of Ethereum’s validators with only three days of data. This post discusses the technique, its implications, and potential mitigations to protect validators’ privacy in the P2P network.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50027-background-2" name="p-50027-background-2"></a>Background</h2>
<p>Ethereum’s P2P network is what allows validators to exchange important messages like blocks and attestations, which keeps the blockchain running. With over a million validators, it’s not practical for each one to send a vote (attestation) to every node for every block, especially to keep Ethereum accessible to solo stakers. To make things manageable, voting (i.e., attestations by validators) is divided in two main ways:</p>
<p><strong>Time Division Across Slots</strong>: Validators only need to vote once per epoch (i.e., once every 32 slots), in a random slot. Thus, only a fraction are voting in a given slot.</p>
<p><strong>Network Division Across Committees</strong>: Validators are split into 64 <em>committees</em>. Within each committee, a set of validators is assigned as <em>aggregators</em> that collect and combine attestations into a single aggregate. This division of attestations into committees is further mirrored in the network layer, which is also divided into 64 attestation <em>subnets (overlay sub-networks)</em>. Each committee is assigned to one of these 64 subnets, and the corresponding attestations are broadcast only within the respective subnet. These subnets are also referred to as <em>topics</em> in the context of GossipSub,  the underlying P2P implementation used by Ethereum.</p>
<p><strong>Attestation Propagation in GossipSub:</strong> When a validator signs an attestation, it gets published to its specified subnet by sending it to a subset of peers that are part of the subnet. The node hosting a validator does not need to be subscribed to this specific subnet, since their committee changes every epoch. Instead, each node in the network subscribes to two subnets by default, and participates in propagating attestations only in these two subnets - these are known as <em>backbone duties</em>. Additionally, each node maintains a connection with at least one peer in each subnet so that their own attestations can be sent to the correct subnet in one hop.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50027-deanonymization-approach-3" name="p-50027-deanonymization-approach-3"></a>Deanonymization Approach</h2>
<p>Given the background on Ethereum node behavior, we describe how an ideal peer (a peer who gives us perfect information) would behave. Let us assume we are connected to a peer running <span class="math">V</span> validators who is a backbone in two subnets. The peer’s validators will attest <span class="math">V</span> times per epoch. Let us assume we receive perfect information from this peer, meaning they forward all attestations they hear about in their two backbones to us. In each epoch, we will receive <span class="math">V</span> attestations from our peer for their validators, and <span class="math">N\cdot \frac{2}{64}</span> for all other <span class="math">N</span>  validators.</p>
<p><strong>Observation:</strong> An ideal peer will only send us an attestation in a subnet they are not a backbone of if they are the signer of the attestation.</p>
<p>Thus, in this scenario, we receive all attestations for the <span class="math">V</span> validators of the peer and can distinguish them as the only attestations we do not receive from the two backbones of the peer. Thus, linking validators to peers in this scenario is trivial.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50027-case-study-4" name="p-50027-case-study-4"></a>Case Study</h3>
<p>In practice, however, network message data is not perfect. To showcase this, we plot the attestations received from an example peer across time and subnets. On this peer, we will identify four validators associated with the peer; their respective attestations are highlighted in red, blue, yellow, and green, while the remaining attestations are shown in pink. Notice that the attestations from these four validators, who happen to have consecutive identifiers, appear equally distributed across subnets. In contrast, the vast majority of attestations come from the two subnets where the peer acts as a backbone (subnets 12 and 13 for the sample peer). Thus, we can locate validators on our peers by observing how the attestations belonging to a validator, which we receive from the peer, are distributed across subnets.</p>
<p>Additionally, and this is where the imperfect information comes into play, the validators hosted on the peer are occasionally tasked with being aggregators in a subnet approximately every 30 epochs per validator. During these times, they temporarily become a backbone (smaller pink horizontal strips) for these subnets and receive attestations from multiple validators belonging to the subnet.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/c/8c8f040874963de5a0c732cf369fb3fde4802d4a.png" title="casestudy"><img alt="casestudy" height="388" src="https://ethresear.ch/uploads/default/optimized/3X/8/c/8c8f040874963de5a0c732cf369fb3fde4802d4a_2_690x388.png" width="690" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#p-50027-heuristics-5" name="p-50027-heuristics-5"></a>Heuristics</h3>
<p>Based on the above observations and other network behaviors which lead to imperfect information such as temporary disconnects, we develop a set of heuristics to link a validator with a node. We verify our results  (see <a href="https://arxiv.org/abs/2409.04366" rel="noopener nofollow ugc">pre-print</a> for more details).</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50027-comparison-to-other-approaches-6" name="p-50027-comparison-to-other-approaches-6"></a>Comparison to Other Approaches</h3>
<p>We are aware of three existing approaches to deanonymize peers in the P2P network that, similarly to us, only rely on observing messages.</p>
<p>A <a href="https://ethresear.ch/t/packetology-validator-privacy/7547">research post</a> explores mapping validators to peers by observing which peer consistently first broadcasts a block. There also exists a <a href="https://medium.com/hoprnet/proof-of-stake-validator-sniping-research-8670c4a88a1c" rel="noopener nofollow ugc">medium post</a> that discusses using attestation arrival times in a similar fashion. The presented analysis is based on data collected on the Gnosis Beacon Chain. Finally, in parallel to our work, a further <a href="https://ethresear.ch/t/estimating-validator-decentralization-using-p2p-data/19920">research post</a> discussed using dynamic subscriptions to deanonymize validators in the P2P network.</p>
<p>We believe that compared to these approaches our method requires significantly less data or concurrent network connections (in the case of timing analyses). Further, it is less prone to noise  in comparison to those approaches based on arrival times and also works if a node hosts more than 62 validators (this is the limit of the approach based on dynamic subscriptions). Thus, we suspect it to be able to more precisely deanonymize a larger proportion of the network in less time.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50027-measurement-results-7" name="p-50027-measurement-results-7"></a>Measurement Results</h2>
<p>By deploying our logging client across four nodes over a period of three days, we were able to deanonymize more than <strong>15% of Ethereum’s validators</strong> in the P2P network. Our nodes were located in Frankfurt (FR), Seoul (SO), Virginia (VA), and Zurich (ZH). By deploying a greater number of nodes and running the measurement for longer, we presume this figure would increase.</p>
<p>With the data we collected, we can also make additional observations about the geographic decentralization and hosting of validators, as well as the behavior of staking pools.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50027-geographic-decentralization-8" name="p-50027-geographic-decentralization-8"></a>Geographic Decentralization</h3>
<p>We show the distribution of validators across countries in the following figure both overall and  separately for the four nodes we ran. We locate the largest proportion (around 14%) in the Netherlands. Further, 71.53% of the validators we locate are in Europe, 11.95% are in North America, 11.52% are in Asia, 4.90% are in Oceania, 0.06% are in Africa and 0.03% are in South America.</p>
<p>Additionally, we notice geographical biases, e.g., the SO node’s high relative proportion of deanonymizations in Australia and South Korea. Thus, we presume that the skew towards Europe could be a result of us running two out of the four nodes in Europe.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/4/f45fb3a3f65719354e1dc95150461dccb139b567.png" title="country_validators"><img alt="country_validators" height="218" src="https://ethresear.ch/uploads/default/original/3X/f/4/f45fb3a3f65719354e1dc95150461dccb139b567.png" width="690" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#p-50027-cloud-hosting-9" name="p-50027-cloud-hosting-9"></a>Cloud Hosting</h3>
<p>We perform a similar analysis to understand how peers are run - if they run hosted on cloud providers or through residential ISP (likely home stakers). Overall, around 90% of the validators we locate are run through cloud providers, with the other 10% belonging to residential ISPs. We plot the distribution across organizations and find that  eight out of the ten largest organizations are cloud providers. Further, we locate the largest number of validators in Amazon data centers, i.e., 19.80% of the validators we locate.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/d/bdf51bb01378fcc59ecf23b602ee18f3e0dd65fd.png" title="org_validators"><img alt="org_validators" height="218" src="https://ethresear.ch/uploads/default/original/3X/b/d/bdf51bb01378fcc59ecf23b602ee18f3e0dd65fd.png" width="690" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#p-50027-staking-pools-10" name="p-50027-staking-pools-10"></a>Staking Pools</h3>
<p>We also take a deeper look at the practices of the  five largest staking pools (Lido, Coinbase, <a href="http://Ether.Fi" rel="noopener nofollow ugc">Ether.Fi</a>, Binance, and Kraken). On average, we observe 678 validators on a given peer for staking pools, with the largest node running 19,263 validators (!).</p>
<blockquote>
<p>Additionally, many staking pools utilize node operators and many of the node operators run validators for various staking pools. This creates a dependency between the staking pools. In particular, we find five instances of validators from two different staking pools that utilize the same node operators being located on the same machine.</p>
</blockquote>
<h2><a class="anchor" href="https://ethresear.ch#p-50027-security-implications-11" name="p-50027-security-implications-11"></a>Security Implications</h2>
<p><strong>Taking Out Previous Block Proposers</strong>: One security issue that’s been discussed is the incentive for the proposer of slot <span class="math">n+1</span> to prevent the proposer of slot <span class="math">n</span> from publishing a block. If successful, the slot <span class="math">n+1</span> proposer can include both the missed transactions and new ones in their block, earning more in fees. Since proposers are known in advance (about six minutes), an attacker could deanonymize the proposer for slot <span class="math">n</span> and launch a temporary DoS or BGP hijack attack, preventing them from submitting their block. Importantly, this attack only needs to last for four seconds - the window for making a block proposal.</p>
<p><strong>Breaking Liveness and Safety</strong>: Extending this attack, an attacker could continuously target the upcoming proposers to stop the network’s progress. If more than one-third of block proposals are missed, Ethereum’s finality gadget won’t be able to finalize blocks, halting the network. Even worse, safety could be compromised as many Ethereum light clients assume the chain head is finalized. By breaking network synchrony through DoS or network partitioning, attackers could cause serious issues.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50027-mitigations-12" name="p-50027-mitigations-12"></a>Mitigations</h2>
<p>To mitigate these security risks one can either improve privacy in the P2P network or protect against potential attacks. We discuss both avenues.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50027-providing-anonymity-13" name="p-50027-providing-anonymity-13"></a>Providing anonymity</h3>
<p><strong>Increase Subnet Participation</strong>: Validators could subscribe to more subnets than the default, making it harder for adversaries to link specific attestations to validators. This increases the communication overhead on the network, potentially undermining Ethereum’s goal of enabling solo stakers to run validators with minimal resources. However, given the increase of the <a href="https://ethresear.ch/t/increase-the-max-effective-balance-a-modest-proposal/15801">MAX_EFFECTIVE_BALANCE</a> in the upcoming hardfork there might be room for a slight increase in the number of P2P messages.</p>
<p><strong>Run Validators Across Multiple Nodes</strong>: Validators could distribute their attestation broadcasts across multiple nodes, making it harder to deanonymize them. While this increases operational costs, it can enhance privacy by spreading validator responsibilities across different IP addresses.</p>
<p><strong>Private Peering Agreements</strong>: Both Lighthouse and Prysm clients allow validators to set up private peering agreements, where a group of trusted peers helps relay gossip messages. While this improves performance and reliability, it also provides some privacy, making it harder to trace validators to a single IP. Instead, an attacker would have to target multiple peers in the agreement. However, finding trusted peers can be costly and difficult, especially for smaller stakers.</p>
<p><strong>Anonymous Gossiping</strong>: Protocols like Dandelion and Tor have been proposed to enhance anonymity. Dandelion, for example, sends messages through a single node first (the “stem” phase) before broadcasting to the network (the “fluff” phase), which helps conceal the message origin. However, these methods introduce delays and might not be fast enough for the Ethereum P2P network.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50027-defending-against-dos-14" name="p-50027-defending-against-dos-14"></a>Defending Against DoS</h3>
<p><strong>Network Layer Defenses</strong>: The libp2p framework used for the Ethereum P2P layer already includes some defenses like limiting the number of connections, rate-limiting incoming traffic, and auto-adjusting firewalls. However, these aren’t foolproof, and manual intervention might still be needed during attacks.</p>
<p><strong>Secret Leader Election</strong>: Another potential defense against DoS attacks is keeping the identity of block producers secret until they propose blocks. This idea, called <strong>secret leader election</strong>, avoids other issues and looks promising. Some proposals have been made for Ethereum, but they’re still in the early design phase as far as we are aware.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/privacy-problems-in-the-p2p-network-and-what-they-tell-us/20463">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 20 Sep 2024 16:04:42 +0000</pubDate>
</item>
<item>
<title>FOCIL Resource Design Considerations</title>
<link>https://ethresear.ch/t/focil-resource-design-considerations/20457</link>
<guid>https://ethresear.ch/t/focil-resource-design-considerations/20457</guid>
<content:encoded><![CDATA[
<div> 关键词：资源约束、Electra硬分叉、Inclusion List委员会、提案者、验证节点

总结:

本文探讨了在Electra硬分叉背景下，以太坊网络中Inclusion List（IL）机制的实施和资源管理。文章首先强调了在设计过程中需要对资源限制进行更深入考虑的原因，主要是因为某些细节在原始协议中并未明确说明。

文章进一步阐述了在实施Inclusion List机制时涉及的关键参与者及其职责。这些参与者包括Inclusion List委员会成员、提案者、见证者以及节点。各参与者的任务范围从发布本地Inclusion List、下载和验证这些列表，到构建满足Inclusion List要求的区块。

文章还详细描述了不同时间点的流程和参与者的行为。例如，在slot n-1的t=6时刻，Inclusion List委员会会发布其本地列表；slot n-1的t=9时刻，见证者和诚实验证节点锁定他们的本地视图；在slot n的t=0时刻，提案者发布包含满足IL要求的区块B；最后，在slot n的t=4时刻，见证者投票确认区块B的有效性。

在分析阶段，文章识别出了潜在的资源密集型区域，特别是IL委员会的CPU负载、节点的带宽使用、提案者的CPU负载以及验证节点在区块验证过程中的CPU和内存消耗。文章指出，虽然在实施过程中存在一些挑战，如带宽压力、资源消耗增加和潜在的安全风险，但总体上，这些挑战可以通过优化设计和策略来缓解。文章最后提出了几个开放问题，涉及如何处理不满足评估功能的区块、处理Inclusion List的可能矛盾情况、提案者与不同头部网络交互的情况、交易无效化对区块构建的影响以及提案者观察到的IL委员会子网的策略选择。 <div>
<p>Special thanks to Thomas, Julian, Barnabe, and Jihoonsong for reviewing it</p>
<p>This document was motivated by our work on the <a href="https://github.com/terencechain/consensus-specs/pull/2" rel="noopener nofollow ugc">FOCIL consensus spec</a>, where we realized that the protocol required more thoughtful consideration around resource constraints since certain details were not explicitly specified in the <a href="https://ethresear.ch/t/fork-choice-enforced-inclusion-lists-focil-a-simple-committee-based-inclusion-list-proposal/19870">FOCIL Ethereum research post</a>.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50013-prerequisite-1" name="p-50013-prerequisite-1"></a>Prerequisite</h1>
<p>Before we begin, we assume the following setup to establish a clean baseline for our considerations:</p>
<ul>
<li>The setup is based on the Electra hard fork. It also makes sense to revisit this on top of EIP-7732 (ePBS) for comparison</li>
<li>We are assuming solo block building and releasing, where the proposer is not running MEV-Boost. This is the first key component to get right, while the Builder API is a secondary consideration</li>
<li>We are assuming a solo staker setup with typical compute, memory requirements, and bandwidth that you can easily follow on the Ethereum chain today</li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#p-50013-actors-2" name="p-50013-actors-2"></a>Actors</h1>
<p>Before we proceed, we assume the following actors are part of the protocol and analyze their responsibilities:</p>
<ul>
<li>Inclusion List (IL) committee members, who are responsible for constraining the next slot proposer by its set of inclusion list transactions</li>
<li>The proposer, who is responsible for proposing the next slot</li>
<li>Attesters, who are attesting to the next slot for the head of the chain</li>
<li>Nodes, which are verifying and following the chain. Proposers and attesters are part of nodes that have staked Ether</li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#p-50013-timeline-3" name="p-50013-timeline-3"></a>Timeline</h1>
<p>We assume the following timeline in which the IL committee, proposer, and attesters perform some honest actions:</p>
<ul>
<li>Slot <code>n-1</code>, <code>t=6</code>: The IL committee publishes their local Inclusion Lists (ILs) over a global topic after learning the contents of block <code>n-1</code></li>
<li>Slot <code>n-1</code>, <code>t=9</code>: Attesters and honest verifying nodes lock in their view of the local ILs</li>
<li>Slot <code>n</code>, <code>t=0</code>: The block proposer for slot <code>n</code> releases block <code>B</code>, which includes the payload which should satisfy the IL requirement</li>
<li>Slot <code>n</code>, <code>t=4</code>: Attesters for slot <code>n</code> vote on block <code>B</code>, verifying the IL aggregation by comparing it to their local IL views and confirming whether block <code>B</code> is “valid”
<ul>
<li>We overload the word “valid” when referring to a block, but it could mean “importable,” “canonical”, or something else. See the open question for further clarification</li>
</ul>
</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50013-interval-1-il-committee-releases-local-il-4" name="p-50013-interval-1-il-committee-releases-local-il-4"></a>Interval 1: IL Committee Releases Local IL</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50013-actor-inclusion-list-committee-5" name="p-50013-actor-inclusion-list-committee-5"></a>Actor: Inclusion List Committee</h3>
<p>IL committee members retrieve a list of IL transactions from EL client given the head (CL → EL call), then they sign the local IL (transactions + summaries) and release it to the gossip network.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50013-resource-considerations-6" name="p-50013-resource-considerations-6"></a>Resource Considerations</h4>
<ul>
<li>Retrieving IL transactions from the EL mempool → CPU/MEM</li>
<li>Signing the inclusion list → CPU</li>
<li>Uploading the inclusion list to the gossip network → Bandwidth (Upload)</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50013-actor-nodes-including-attesters-7" name="p-50013-actor-nodes-including-attesters-7"></a>Actor: Nodes (including Attesters)</h3>
<p>Nodes following the chain will download the IL, verify it for anti-DOS (not importing it to EL yet), and forward it to other peers. Nodes also import the IL into fork choice and track which ILs have been seen using an aggregate cache. Attesters and nodes following the chain should have the same view of the chain.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50013-resource-considerations-8" name="p-50013-resource-considerations-8"></a>Resource Considerations</h4>
<ul>
<li>Downloading the IL → Bandwidth (Download)</li>
<li>Forwarding the IL → Bandwidth (Upload)</li>
<li>Verifying the IL for anti-DOS → CPU/MEM</li>
<li>Caching seen and aggregate ILs → MEM</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50013-actor-proposer-9" name="p-50013-actor-proposer-9"></a>Actor: Proposer</h3>
<p>The proposer for the next slot actively monitors the IL gossip network and, collects and aggregates the local ILs, then at IL aggregation cutoff (interval <span class="hashtag-raw">#2</span>) proposer updates the block-building process with a list of IL transactions to be included for its block. This requires a CL to EL call.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50013-resource-considerations-10" name="p-50013-resource-considerations-10"></a>Resource Considerations</h4>
<ul>
<li>Inherits the same costs as nodes following the chain</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-50013-proposer-edge-case-11" name="p-50013-proposer-edge-case-11"></a>Proposer Edge Case</h4>
<p>If the next slot’s proposer observes a sufficient number of inclusion lists based on a parent hash it hasn’t seen, the proposer will need to manually request the missing beacon block, import the block, and build on top of that block.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50013-conclusion-12" name="p-50013-conclusion-12"></a>Conclusion</h3>
<p>Based on the above, we can identify potential resource-intensive areas and narrow down on them:</p>
<ul>
<li><strong>IL Committee’s CPU impact</strong>: IL transaction retrieval from EL &amp; signing: while there are resource demands here, this is presumed to be relatively inexpensive and not a major concern.</li>
<li><strong>Nodes bandwidth impact</strong>: Nodes downloading and uploading ILs may use tons of bandwidth, especially research post currently states that the inclusion list size is flexible/unbounded. This introduces a potential DOS risk, as a malicious IL committee member could flood the network with a large number of transactions, even if they are invalid. Nodes would still gossip about the IL before they import the ILs. Anti-DoS measures need to be considered carefully.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50013-interval-2-nodes-lock-their-view-proposer-import-il-transactions-13" name="p-50013-interval-2-nodes-lock-their-view-proposer-import-il-transactions-13"></a>Interval 2: Nodes lock their view, proposer import IL transactions</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50013-actor-proposer-14" name="p-50013-actor-proposer-14"></a>Actor: Proposer</h3>
<p>The proposer updates block building process with a list of inclusion list transactions. This is a CL → EL call.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50013-resource-considerations-15" name="p-50013-resource-considerations-15"></a>Resource Considerations</h4>
<ul>
<li>Updates block building process with a list of inclusion list transactions → CPU/MEM</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50013-actor-nodes-including-attesters-16" name="p-50013-actor-nodes-including-attesters-16"></a>Actor: Nodes (including Attesters)</h3>
<p>Lock inclusion list view. Stop accepting local inclusion list from this point.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50013-resource-considerations-17" name="p-50013-resource-considerations-17"></a>Resource Considerations</h4>
<ul>
<li>Lock local inclusion list view → None</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50013-conclusion-18" name="p-50013-conclusion-18"></a>Conclusion</h3>
<ul>
<li><strong>Proposer’s CPU impact</strong>: Importing the IL transactions into the block-building process could disrupt the block building process, potentially straining the execution layer client’s CPU during transaction simulation. This may become complicated under account abstraction as transactions may invalidate each other. This should be further analyzed.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-50013-interval-3-proposer-releases-block-19" name="p-50013-interval-3-proposer-releases-block-19"></a>Interval 3: Proposer Releases Block</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50013-actor-proposer-20" name="p-50013-actor-proposer-20"></a>Actor: Proposer</h3>
<p>The proposer retrieves the execution payload from the EL client (CL → EL call), and releases it to the beacon block gossip network. Everyone else then verifies the block.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50013-resource-considerations-21" name="p-50013-resource-considerations-21"></a>Resource Considerations</h4>
<ul>
<li>Retrieving the payload from the EL client → CPU/MEM</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50013-actor-nodes-22" name="p-50013-actor-nodes-22"></a>Actor: Nodes</h3>
<p>Nodes receive the beacon block and verify it. The new verification steps include checking the inclusion list aggregate construction and confirming whether the inclusion list satisfies the evaluation function, which is be completed on the CL. The checking of IL conditions (whether they can be skipped due to conflicts or not) will be performed on the EL.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-50013-resource-considerations-23" name="p-50013-resource-considerations-23"></a>Resource Considerations</h4>
<ul>
<li>Verifying that the inclusion list is satisfied on CL → CPU</li>
<li>Verifying inclusion list conditions on EL → CPU</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50013-conclusion-24" name="p-50013-conclusion-24"></a>Conclusion</h3>
<p>The additional duties for the proposer do not seem to be a significant concern. The new verification steps for nodes—checking verifying that the inclusion list meets the satisfactory conditions—may introduce some additional CPU load, but it doesn’t appear to be a major issue.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-50013-interval-4-attester-committee-25" name="p-50013-interval-4-attester-committee-25"></a>Interval 4: Attester Committee</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-50013-actor-attester-26" name="p-50013-actor-attester-26"></a>Actor: Attester</h3>
<p>The attester votes for the beacon block using LMD GHOST fork choice rule. Attesters will only vote for a beacon block that satisfies the inclusion list evaluation function, based on observations from Interval 1.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-50013-resource-considerations-27" name="p-50013-resource-considerations-27"></a>Resource Considerations</h3>
<ul>
<li>Attesters voting for a block that satisfies the inclusion list evaluation function → No additional cost</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-50013-conclusion-28" name="p-50013-conclusion-28"></a>Conclusion</h3>
<p>There is no difference than today.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50013-resource-consideration-summary-29" name="p-50013-resource-consideration-summary-29"></a>Resource Consideration Summary</h1>
<p>As seen above, the most significant resource concerns revolve around inclusion list upload, download, and the potential for spamming from a node’s perspective. Another key concern is the overhead on nodes for verifying and importing the inclusion list, as well as the proposer’s need to update its block-building process to satisfy the inclusion list. These aspects require careful consideration and design to ensure efficiency and security.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-50013-open-questions-30" name="p-50013-open-questions-30"></a>Open Questions</h1>
<p>Based on the above, we outline several open questions that will influence how the specification is written:</p>
<ol>
<li>
<p><strong>Block Not Satisfying the Evaluation Function</strong>: How should a block that fails the inclusion list evaluation function be handled, and what design considerations come into play for such conditions?</p>
<ul>
<li>Should it be treated similarly to blobs and <strong>cannot be imported</strong>?</li>
<li>Should it <strong>not be filtered</strong> by fork choice?</li>
<li>Should it <strong>not be valid</strong> in the state transition function?</li>
</ul>
</li>
<li>
<p><strong>Inclusion List Equivocations</strong>: If an inclusion list committee member sends different versions of the inclusion list to different nodes, and they are all propagated across the network, what are the consequences of this action? How could such behavior negatively impact the proposer building the next block?</p>
</li>
<li>
<p><strong>Proposer Already Building on a Different Head</strong>: If the proposer builds on a different head than the one sent by the inclusion list committee, and thus needs to change its head view, what are the consequences of this action for block validity and proposer behavior?</p>
</li>
<li>
<p><strong>Inclusion List Transactions Invalidations</strong>: Local inclusion list transactions can be invalidated in a few ways. Even if these transactions are invalidated, the block should still be able to satisfy the evaluation function. Transactions may be invalidated as multiple inclusion lists merge with each other or with transactions in the block. Besides typical nonce checking, account abstraction introduces new ways for transactions to be invalidated, as balance can be drained with a static nonce. How much additional simulation a block builder needs to perform due to transaction invalidation and how much this affects its CPU compute remains to be seen for both MEV-Boost actors and local builders.</p>
</li>
<li>
<p><strong>Proposer’s Observation of the IL Committee Subnet</strong>: The proposer monitors the inclusion list committee subnet to know when it is ready to construct the aggregate. There are two design approaches here, and it’s worth considering them further. The first approach is a greedy proposer, where the proposer waits until <code>t=9</code>, gathers as many ILs as possible, sends them to the EL, and the EL updates its block. The second approach is a selective proposer, where the proposer waits until it has a sufficient inclusion list to satisfy the eval function, sends them to the EL, and can do this in less than <code>t=9s</code> or even earlier. The question is whether the second approach justifies the optimization to allow the proposer to release the inclusion list aggregate earlier. The second approach may only be well suited for an IL with its own dedicated gas limit.</p>
</li>
</ol>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/focil-resource-design-considerations/20457">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 19 Sep 2024 14:30:43 +0000</pubDate>
</item>
<item>
<title>Trusted Advantage in Slot Auction ePBS</title>
<link>https://ethresear.ch/t/trusted-advantage-in-slot-auction-epbs/20456</link>
<guid>https://ethresear.ch/t/trusted-advantage-in-slot-auction-epbs/20456</guid>
<content:encoded><![CDATA[
<div> 关键词：ePBS、No Trusted Advantage、slot auction、payload-timeliness committee、ascending-bid first-price auction

总结:
本文探讨了ePBS（执行Payload公平交换协议）在slot auction（时段拍卖）设计中是否满足“无信任优势”（No Trusted Advantage）这一关键特性。"无信任优势"意味着，出块提议者应激励使用内部协议承诺，将构建执行Payload的权利出售给最大化自身收益的出块生产者。

在slot auction Payload-timeliness committee（PTC）设计中，出块提议者在时段开始时就与出块生产者进行承诺。然而，文章提出，如果提议者在时段开始时就决定是通过早拍卖（t=0）还是晚拍卖（t=6）来出售执行Payload的构建权利，这种设计可能违反“无信任优势”。早拍卖中，出块生产者的出价基于潜在价值的分布；而晚拍卖中，出价则基于实际可提取的价值。文章指出，从统计学角度看，基于实际价值的预期收益往往高于基于潜在价值的预期收益。因此，理性提议者可能会选择晚拍卖来最大化收益。

文章进一步分析了两个关键假设：
1. 提议者能够通过二级市场以更高的价格出售执行Payload构建权利。
2. 出块生产者对二级市场的依赖程度低于提议者。

最后，文章指出，满足“无信任优势”的关键是确保提议者在时段开始时就出售执行Payload构建权利，例如通过MEV-Burn等方法。实现这一点的挑战性表明，关于slot auction ePBS部署的决策将取决于市场结构和参与者的风险偏好，这可能需要进一步的实证验证。 <div>
<p><em>Thanks to Thomas Thiery, Anders Elowsson and Barnabé Monnot for reviewing this post. Also, thanks to everyone in the <a href="https://rig.ethereum.org/" rel="noopener nofollow ugc">Robust Incentives Group</a> and the ePBS Breakout Call <span class="hashtag-raw">#9</span> attendees for discussions. This post’s argument was presented during the ePBS Breakout Call <span class="hashtag-raw">#9</span> (<a href="https://youtu.be/2BUsiUnUZYc?si=6pbGlXJ-nkRK2dh9" rel="noopener nofollow ugc">recording</a>; <a href="https://docs.google.com/presentation/d/1-MnAqDzR7JapIPpUEbScALEtY5axyRcPgJDpgaJ_qVI/edit?usp=sharing" rel="noopener nofollow ugc">slides</a>).</em></p>
<p>ePBS facilitates the fair exchange between a beacon proposer and a block producer. A block producer is the party that constructs the execution payload, which could be a sophisticated builder or a proposer. The beacon proposer sells the rights to build an execution payload to a block producer in exchange for a bid. The <strong>Unconditional Payment</strong> and <strong>Honest Builder Safety</strong> <a href="https://hackmd.io/@potuz/rJ9GCnT1C#Optional-changes" rel="noopener nofollow ugc">are two desiderata</a> of this fair exchange. The former means that a proposer must receive the bid’s value regardless of the block producer’s actions. The latter roughly implies the block producer must receive the execution payload rights if it paid for them. The final desideratum is that there is <strong>No Trusted Advantage.</strong> In this context, we define this final desideratum as follows.</p>
<blockquote>
<p><strong>(Definition) No Trusted Advantage</strong>: The beacon proposer is incentivized to use the in-protocol commitment to commit to the block producer whose in-protocol bid value maximizes the beacon proposer’s utility.</p>
</blockquote>
<p>Since <a href="https://mevboost.pics/" rel="noopener nofollow ugc">most beacon proposers run</a> MEV-Boost, it has become apparent that they want to outsource block building to sophisticated block producers. The No Trusted Advantage desideratum ensures that the commitment that beacon proposers use to outsource to block producers benefits from the Unconditional Payment and Honest Builder Safety guarantees that ePBS provides. Suppose an ePBS design does not satisfy No Trusted Advantage, then a rational beacon proposer may be incentivized to sell the execution payload construction rights or to receive payment for the sale via other commitments that do not have the trustless fair exchange properties of ePBS. The trust a rational beacon proposer must then pose in a third party hurts the credible neutrality of the network and defeats the purpose of ePBS.</p>
<p>This post argues that the slot auction <a href="https://ethresear.ch/t/payload-timeliness-committee-ptc-an-epbs-design/16054#proposer-initiated-splitting-18">Payload-timeliness committee (PTC)</a> ePBS design does not satisfy No Trusted Advantage. We will present an informal model in which a rational proposer outsources execution payload construction via another commitment than the commitment facilitated via ePBS. In practice, this attack means that a proposer will not use ePBS as intended. Instead, the beacon proposer waits until the execution payload must be revealed and sells the execution payload construction rights via an out-of-protocol block auction, like how MEV-Boost currently works.</p>
<p>In the slot auction Payload-timeliness committee (PTC) ePBS design, a beacon proposer commits to a block producer at the beginning of the slot (t = 0). The block producer must reveal its execution payload halfway into the slot (t = 6). This time difference exists because a block producer must be able to observe attestations for the beacon block so that it knows that the beacon block will likely become canonical, ensuring Honest Builder Safety.</p>
<p>Consider a proposer that decides whether to sell the execution payload construction rights Early (t = 0) or Late (t = 6). The proposer must decide which auction it will use at time t = 0, and it can only choose one auction format. If the proposer is incentivized to sell Early, slot auction ePBS satisfies No Trusted Advantage. If the proposer is incentivized to sell Late, No Trusted Advantage is violated since the proposer would commit to itself in the beacon block and run a MEV-Boost-like auction just before t = 6. Assume the proposer maximizes its payoff.</p>
<p>Builders also bid to maximize payoffs. At the beginning of the block, builders only know the distribution of values they could extract by building the execution payload. Just before t = 6, builders learn the precise value they could extract by building an execution payload called the realized value.</p>
<p>If the proposer were to host an auction Early, builders would bid based on their distribution of values; specifically, risk-neutral builders would bid according to the expected value. If the proposer hosts an auction Late, builders bid based on their realized value. This post assumes bids are monotonically increasing in value. The critical insight is that the expected auction revenue is likely higher based on realized value than expected values under many circumstances. This is because only the highest-order statistics of realized values are relevant.</p>
<p>Considering an ascending-bid first-price auction, the winning builder should pay the builder’s value with the second-highest valuation. Hence, if the second-highest expected value in the Early auction is lower than the expectation of the second-highest realized value, then the proposer will choose to sell via the Late auction.</p>
<blockquote>
<p><strong>(Main Result) Slot Auction ePBS does not satisfy No Trusted Advantage:</strong> In an ascending-bid first-price auction, a rational beacon proposer will auction the execution payload construction rights via the Late auction if the second-highest expected value (Early auction revenue) is lower than the expectation of the second-highest realized value (Late auction expected revenue). <strong>In this case, slot auction ePBS does not satisfy No Trusted Advantage.</strong></p>
</blockquote>
<p>This result relies on two key assumptions:</p>
<ol>
<li>The beacon proposer has access to a secondary market to sell the execution payload construction rights to block producers in the Late auction.</li>
<li>Block producers have less access to a secondary market for selling the execution payload construction rights to other block producers in the Late auction than the beacon proposer.</li>
</ol>
<p>If the first assumption does not hold, then the beacon proposer effectively has no option to sell in the Late auction. A secondary market will likely be available to the beacon proposer since this market already exists via MEV-Boost. On the other hand, <a href="https://discord.com/channels/595666850260713488/874767108809031740/1284179164672426118" rel="noopener nofollow ugc">Terence has argued</a> that ePBS makes same-slot unbundling attacks easier if a beacon proposer were to sell Late because a beacon proposer could release equivocation execution payloads without losing fork-choice weight or being slashed.</p>
<p>The second assumption could be motivated by the complexity of facilitating trust between two sophisticated parties. Perhaps relays are less willing to facilitate fair exchange between two sophisticated parties. Perhaps block producers will continuously request bids from bidders in the secondary market, whereas today, <a href="https://ethresear.ch/t/bid-cancellations-considered-harmful/15500">proposers do not</a>. Perhaps <a href="https://www.youtube.com/watch?v=-PXGPFFneMI" rel="noopener nofollow ugc">adverse selection</a> is worse if the auctioneer is sophisticated.</p>
<p>Suppose the second assumption does not hold, and the beacon proposer and builders have equal access to the secondary market. In that case, the beacon proposer’s decision to sell in the Early or the Late auction is a risk-reward trade-off. This is because a block producer would then incorporate the value it could get by reselling the item in the secondary market in its bid, which would be equivalent to the value a beacon proposer could get. If the beacon proposer is less risk-averse than builders, it will sell in the Late auction; otherwise, it will sell in the Early auction.</p>
<p>The key point is that whether ePBS is used—whether No Trusted Advantage is satisfied—depends on the risk appetite of beacon proposers and the state of the secondary market if ePBS were deployed with slot auctions. Yet it is entirely unclear what these proposers’ risk appetite or the secondary market’s state will be. In contrast, MEV-Boost has given the ecosystem <a href="https://ieeexplore.ieee.org/abstract/document/10634354" rel="noopener nofollow ugc">a</a> <a href="https://openreview.net/forum?id=XMBR5UEo3f" rel="noopener nofollow ugc">lot</a> <a href="https://arxiv.org/abs/2405.01329" rel="noopener nofollow ugc">of</a> <a href="https://arxiv.org/abs/2407.13931" rel="noopener nofollow ugc">information</a> on how block auctions work, providing confidence in it satisfying No Trusted Advantage.</p>
<p>Satisfying No Trusted Advantage in slot auction ePBS is challenging. This desideratum could be achieved by forcing a sale in the early auction, for example, using MEV-Burn. <a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ" rel="noopener nofollow ugc">Execution Auctions</a> use <a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590">MEV-Burn</a>. Whether MEV-Burn is sufficient to satisfy the No Trusted Advantage is still understudied.</p>
<blockquote>
<p>This argument could be pivotal in whether ePBS is deployed with block or slot auctions. The difference between the two is huge in terms of market structure. Therefore, it would be very valuable to have numerical validation of the theoretical argument presented here. If you want to create a simulation that tests this theory, please get in touch with Julian Ma (<a href="mailto:julian.ma@ethereum.org">julian.ma@ethereum.org</a>)</p>
</blockquote>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/trusted-advantage-in-slot-auction-epbs/20456">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 19 Sep 2024 11:37:12 +0000</pubDate>
</item>
<item>
<title>Decentralized Anti-MEV sequencer based on Order-Fairness Byzantine Fault-Tolerant (BFT) consensus</title>
<link>https://ethresear.ch/t/decentralized-anti-mev-sequencer-based-on-order-fairness-byzantine-fault-tolerant-bft-consensus/20427</link>
<guid>https://ethresear.ch/t/decentralized-anti-mev-sequencer-based-on-order-fairness-byzantine-fault-tolerant-bft-consensus/20427</guid>
<content:encoded><![CDATA[
<div> 关键词：Decentralized Anti-MEV Sequencer、Order-Fairness BFT Consensus、Transaction Fairness、Decentralization、Byzantine Fault Tolerance

总结:
本文提出了一种基于秩序公平拜占庭容错(BFT)共识的去中心化反矿工提取价值(MEV)序列器。该机制旨在对抗MEV并确保交易公平性。其关键特性如下：

1. **去中心化**：通过构建由多个节点共同参与交易排序和打包的网络，实现交易处理的分散化。

2. **秩序公平**：遵循规则，如果节点接收的交易tx早于tx′，那么tx的排序不应晚于tx′，以此保障交易的公平性。

3. **拜占庭容错**：即使部分参与者行为恶意，共识协议也能确保系统正常运行，维护其可靠性与稳定性。

4. **工作流程**：用户提交交易至序列器网络后，通过秩序公平BFT共识确定交易顺序。达成共识后，序列器将交易打包并提交给以太坊上的Rollup智能合约执行。

5. **系统实施细节**：对于秩序公平BFT共识的具体实现，请参考文章末尾引用的文献。

此解决方案旨在通过引入去中心化和秩序公平的机制，以及提供拜占庭容错保障，有效对抗MEV，确保区块链交易的公平性和安全性。 <div>
<p><em>by <a href="https://x.com/0x_1cc" rel="noopener nofollow ugc">KD.Conway</a></em></p>
<h2><a class="anchor" href="https://ethresear.ch#p-49955-tldr-1" name="p-49955-tldr-1"></a>TL;DR</h2>
<ul>
<li>This post introduces a <strong>Decentralized Anti-MEV Sequencer</strong> based on <strong>Order-Fairness Byzantine Fault-Tolerant (BFT) Consensus,</strong> a mechanism designed to counteract MEV and ensure transaction fairness.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-49955-order-fairness-2" name="p-49955-order-fairness-2"></a>Order Fairness</h2>
<p><strong>Received-Order-Fairness</strong>[1]: with parameter 1/2 &lt; 𝛾 ≤ 1 dictates that if 𝛾 fraction of honest nodes receive a transaction tx before tx′, then tx should be ordered no later than tx’.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49955-introducing-the-anti-mev-sequencer-3" name="p-49955-introducing-the-anti-mev-sequencer-3"></a>Introducing the Anti-MEV Sequencer</h2>
<p>Our proposed solution is a <strong>Decentralized Anti-MEV Sequencer</strong> that leverages an <strong>Order-Fairness Byzantine Fault-Tolerant (BFT) consensus</strong> mechanism. This system provides:</p>
<ol>
<li>
<p><strong>Decentralization</strong>: Instead of a centralized sequencer, we will build a sequencer network with multiple nodes contributing to transaction ordering and batching.</p>
</li>
<li>
<p><strong>Order-Fairness</strong>: Transactions are processed based on the time they were received by the nodes in the sequencer network, ensuring no one participant can manipulate transaction ordering.</p>
</li>
<li>
<p><strong>Byzantine Fault Tolerance</strong>: The consensus protocol ensures the system remains operational even if some of the participants behave maliciously.</p>
</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#p-49955-workflow-4" name="p-49955-workflow-4"></a>Workflow</h2>
<ol>
<li>
<p>When a user wants to send a transaction on a layer 2 blockchain, they submit the transaction to the sequencer network.</p>
</li>
<li>
<p>The Order-Fairness BFT consensus is employed to determine the correct order of transactions. This guarantees that, even if a minority of nodes act maliciously, the system can still reach consensus on a fair transaction order.</p>
</li>
<li>
<p>After reaching consensus, the sequencer batches the transactions and submits them to the Rollup smart contract on Ethereum, where they are executed in the agreed-upon order.</p>
</li>
</ol>
<p>For details on the system implementation of the Order-Fairness BFT consensus, please refer to the corresponding references at the end of this post.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49955-references-5" name="p-49955-references-5"></a>References</h2>
<p>[1] Kelkar, Mahimna, et al. “Order-fairness for byzantine consensus.” <em>Advances in Cryptology–CRYPTO 2020: 40th Annual International Cryptology Conference, CRYPTO 2020, Santa Barbara, CA, USA, August 17–21, 2020, Proceedings, Part III 40</em>. Springer International Publishing, 2020.</p>
<p>[2] Kelkar, Mahimna, et al. “Themis: Fast, strong order-fairness in byzantine consensus.” <em>Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security</em>. 2023.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/decentralized-anti-mev-sequencer-based-on-order-fairness-byzantine-fault-tolerant-bft-consensus/20427">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 13 Sep 2024 15:27:10 +0000</pubDate>
</item>
<item>
<title>PANDAS: A Practical Approach for Next-Generation Data Availability Sampling</title>
<link>https://ethresear.ch/t/pandas-a-practical-approach-for-next-generation-data-availability-sampling/20426</link>
<guid>https://ethresear.ch/t/pandas-a-practical-approach-for-next-generation-data-availability-sampling/20426</guid>
<content:encoded><![CDATA[
<div> 关键词：PANDAS、网络层协议、数据可用性采样、资源丰富的构建者、点对点通信

总结:

文章介绍了一种名为PANDAS的网络层协议，旨在支持高达32MB的大型数据块进行随机采样。PANDAS的核心目标是在4秒内完成采样过程，通过一种假设机制实现这一目标，即资源丰富的构建者负责初始数据分布。该协议分为两个阶段：播种阶段和行/列合并与采样阶段。

播种阶段中，构建者直接将数据块的行/列子集分发给验证节点（VN），而合并与采样阶段则允许节点同时进行随机采样和数据恢复，以提高数据的可用性。

PANDAS采用了一种直接通信策略，这与基于分发的多跳通信或分布式哈希表（DHT）方法形成对比。在设计PANDAS时，考虑了几个关键假设，包括资源丰富的构建者、激励机制、验证节点的独立操作、恶意多数的系统假设以及节点间不完全一致的视图。

为了确保验证节点在投票前完成随机采样，PANDAS引入了连续的节点发现机制，以便在协议执行期间保持节点视图的更新。此外，PANDAS协议包含两个不协调的阶段，即播种和行/列合并与采样，每个阶段都重复执行，以完成整个过程。

在播种阶段，构建者使用哈希空间将节点动态分配给特定的行/列区域，以实现本地和确定性的映射。构建者随后尝试通过直接通信方式将数据块的子集分发给相应的节点。在合并与采样阶段，节点需要在四秒内完成采样，同时获取其被分配的完整行和列以提升数据可用性。

最后，PANDAS还在持续实验中，探索不同的数据分布策略，特别是在存在恶意验证节点的情况下，以优化成本和数据可用性之间的平衡。 <div>
<p><strong>PANDAS: A Practical Approach for Next-Generation Data Availability Sampling</strong></p>
<p>Authors: Onur Ascigil<sup>1</sup>, Michał Król<sup>2</sup>, Matthieu Pigaglio<sup>3</sup>, Sergi Reñé<sup>4</sup>, Etienne Rivière<sup>3</sup>, Ramin Sadre<sup>3</sup></p>
<p><strong>TL;DR</strong></p>
<ul>
<li>PANDAS is a network layer protocol that supports <a href="https://www.google.com/url?q=https://a16zcrypto.com/posts/article/an-overview-of-danksharding-and-a-proposal-for-improvement-of-das/&amp;sa=D&amp;source=editors&amp;ust=1726244230013129&amp;usg=AOvVaw1uA0yFlQiomkk9RvjmwyCi" rel="noopener nofollow ugc">Danksharding</a> with 32 MB blobs and beyond.</li>
<li>PANDAS aims to achieve a 4-second deadline for random sampling (under the <a href="https://www.google.com/url?q=https://ethresear.ch/t/das-fork-choice/19578&amp;sa=D&amp;source=editors&amp;ust=1726244230013705&amp;usg=AOvVaw3doAIsKjvcjcIC7S-PusJW" rel="noopener nofollow ugc">tight fork choice model</a>).</li>
<li>Following the <a href="https://www.google.com/url?q=https://ethereum.org/en/roadmap/pbs/&amp;sa=D&amp;source=editors&amp;ust=1726244230013999&amp;usg=AOvVaw2dMSikl29D4bH315qZQzBC" rel="noopener nofollow ugc">Proposer-Builder Separation (PBS)</a>, resourceful builders perform the initial distribution (i.e., seeding) of samples to the nodes.</li>
<li>PANDAS proceeds in two phases during each slot: 1) <em>Seeding Phase,</em> where the chosen builder of a slot distributes <em>subsets of rows and columns</em> of a 2-D encoded blob to the validator nodes, and 2) Row/Column <em>Consolidation and</em> <em>Sampling phase,</em> where nodes sample random cells and at the same time retrieve and reconstruct assigned rows/columns to boost the data availability of cells.</li>
<li>PANDAS uses a <em>direct communication</em> approach, which means 1-hop, i.e., point-to-point communications, for both seeding and sampling phases rather than a gossip-based, multi-hop approach or a DHT.</li>
</ul>
<p>We make the following assumptions when designing PANDAS:</p>
<p><strong>Assumption 1) Resourceful Builders:</strong> Following the <a href="https://www.google.com/url?q=https://ethereum.org/en/roadmap/pbs/&amp;sa=D&amp;source=editors&amp;ust=1726244230016209&amp;usg=AOvVaw2u6aLVhX_45QF4JizRPRrL" rel="noopener nofollow ugc">Proposer-Builder Separation (PBS)</a> scheme, in PANDAS, a set of resourceful builders — e.g., cloud instances with sufficiently high upload bandwidth such as 500 Mbps or more — undertake the distribution of seed samples to the network.</p>
<p><strong>Assumption 2) Builder Incentives:</strong> The builders have an incentive for the blob data to be available since the block will be accepted only if DAS succeeds. However, different builders can have different amounts of resources. The interest of rational builders is to guarantee that data will be considered available while spending a minimal amount of resources.</p>
<p><strong>Assumption 3) Validator Nodes (VNs) are the primary entities of DAS protocol:</strong> A single Validator Node (VN) performs only a single sampling operation (as one entity), independent of the number of validators it hosts.</p>
<p><strong>Assumption 4) Dishonest Majority:</strong> A majority (or even supermajority) of VNs and builders can be malicious and, therefore, may not follow the protocol correctly.</p>
<p><strong>Assumption 5) Sybil-resistance</strong> <strong>VNs</strong>: An honest VN can use a <a href="https://www.google.com/url?q=https://ethresear.ch/t/proof-of-validator-a-simple-anonymous-credential-scheme-for-ethereums-dht/16454&amp;sa=D&amp;source=editors&amp;ust=1726244230018106&amp;usg=AOvVaw2S4QQ8d0c8zadKQKbVBQ6W" rel="noopener nofollow ugc">Proof-of-Validator</a> scheme to prove that it hosts at least one validator. If multiple nodes attempt to re-use the same proof, they can be blocklisted by other honest nodes and builders.</p>
<p>Below are the objectives of PANDAS:</p>
<p><strong>Objective 1)</strong> <strong>Tight fork choice:</strong> <em>Honest validator nodes (VNs) complete</em> <em>random sampling</em> <em>before voting for a block, even when the</em> <em>majority of VNs are malicious</em>*.* Therefore, we target the <a href="https://www.google.com/url?q=https://ethresear.ch/t/das-fork-choice/19578&amp;sa=D&amp;source=editors&amp;ust=1726244230019035&amp;usg=AOvVaw0jtQl6ICcsg9iqBiQh4Mst" rel="noopener nofollow ugc">tight fork choice model</a>, which means that honest VNs in a slot’s committee must complete random sampling before voting <em>within four seconds</em> into that slot.</p>
<p><strong>Objective 2)</strong> <strong>Flexible builder seeding strategies:</strong> Given that different builders can have different resources, our design allows the block builder the flexibility to implement different blob distribution strategies, each with a different trade-off between security and resource usage. For higher security, the builder can send more copies of the blob’s cells to validator nodes, ensuring higher availability. Conversely, to minimise resource usage, the builder can distribute a single copy of each cell at most, reducing bandwidth usage at the expense of lower security. This flexible approach allows the builder to navigate the trade-off between ensuring data availability and optimising bandwidth while under the incentive for the block to be deemed available by validator nodes to be accepted.</p>
<p><strong>Objective 3) Allowing Inconsistent Node Views:</strong> Our objective is to ensure that the VNs and the builders are not required to reach a consensus on the list of VNs. While we aim for the VNs and builders to generally agree on the set of VNs in the system, it is not necessary for the VNs to maintain strictly consistent views or for the builders’ and VNs’ views to be fully synchronised.</p>
<p><strong>PANDAS Design</strong></p>
<p><strong>Continuous Peer Discovery:</strong> To achieve Objective 3, the nodes in the system perform continuous peer discovery in parallel to the protocol phases below to maintain an up-to-date “view” containing other peers. <em>The builder</em> and the VNs aim to discover all the VNs with a valid <a href="https://www.google.com/url?q=https://ethresear.ch/t/proof-of-validator-a-simple-anonymous-credential-scheme-for-ethereums-dht/16454&amp;sa=D&amp;source=editors&amp;ust=1726244230020521&amp;usg=AOvVaw1-IdEEIXKyEqrBkmFcpdG1" rel="noopener nofollow ugc">Proof-of-Validator</a>. We expect both the builder and VNs to have a close but not perfect view of all the VNs in the system.</p>
<p>A membership service running the peer discovery protocol inserts new (verified) VNs to the view and eventually converges to a complete set of VNs. Peer discovery messages are piggybacked to sample request messages to reduce discovery overhead.</p>
<p>PANDAS protocol has two (uncoordinated) phases, which repeat during each slot:</p>
<p><strong>Phase 1</strong>) Seeding,</p>
<p><strong>Phase 2)</strong> Row/Column Consolidation and Sampling</p>
<p>In the seeding phase, the builder pushes subsets of row/columns directly to the VNs where row/column assignment is based on a deterministic function. Once a VN receives its samples from the builder, it consolidates the entire row/column it is assigned to (by requesting missing cells from other VNs assigned to the corresponding row/column) and simultaneously performs random sampling.</p>
<p>VNs do not coordinate to start consolidating and sampling. Therefore, a node finishing phase 1 can begin phase 2 immediately without coordinating with other nodes. The VNs who are the committee members of a slot must complete seeding and random sampling within 4 seconds into the slot.</p>
<p>Below, we explain the two phases of our protocol in detail.</p>
<p><strong>Phase 1- Seeding</strong>: The builder assigns VNs to individual rows/columns using a deterministic function that uses a hashspace as we explain below. This mapping of VNs to individual rows/columns is dynamic and changes in each slot. The mapping allows nodes to locally and deterministically map nodes to rows/columns without requiring the number or full list of nodes to be known.</p>
<p>The Builder prepares and distributes seed samples to the VNs as follows:</p>
<p><strong>1.a) <em>Mapping Rows/Columns to static regions in the hashspace:</em></strong> The individual rows and columns are assigned static regions in the hashspace as shown in the upper portion of Figure 1.</p>
<p><strong>1.b)</strong> <em><strong>Mapping VNs to a hashspace</strong></em> <em>:</em> The builder uses a sortition function F<sub>NODE</sub>(NodeID, epoch, slot, R) to assign each VN to a key in the hashspace. The function takes parameters such as NodeID, which is the identifier of the node (i.e., peer ID), epoch and slot numbers, and a random value R derived from the header of the block header from the previous slot.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/8/48bcaeee1eef51ab1128caf78e0d29d6d54dbe7e.png" title="image3"><img alt="image3" height="352" src="https://ethresear.ch/uploads/default/optimized/3X/4/8/48bcaeee1eef51ab1128caf78e0d29d6d54dbe7e_2_690x352.png" width="690" /></a></div><br />
<strong>Figure 1:</strong> Assignment of Row samples to VNs. The Column samples are mapped similarly.<p></p>
<p>A VN assigned to a row’s region will receive a subset of the cells belonging to that row from the builder. As the VNs are re-mapped to the hashspace during each slot using F<sub>NODE</sub>, their row/column assignments can also change.</p>
<p><strong>NOTE:</strong> A dynamic, per-slot assignment of rows and columns to VNs is impossible in a gossip-based seeding approach where per-row and per-column gossip channels must remain relatively stable over time.</p>
<p><strong>1.c)</strong> <strong>Row/Column <em>Sample Distribution:</em></strong> For each row and column, the builder applies a best-effort distribution strategy to push subsets of each row/column to the VNs mapped to the corresponding row/column’s region. The builder uses a direct communication approach, particularly a UDP-based protocol, to distribute the cells for each row/column directly to the VNs.</p>
<p><em>Rationale for direct communication</em>*:* We aim to complete the seeding phase as quickly as possible to give time for committee members to complete random sampling before voting (Objective 1).</p>
<p>Row/Column Distribution Strategies: We allow the builders to choose distribution strategies based on resource availability in line with Objective 2. A trade-off between resource usage and data availability exists for different distribution strategies. Consider the example in Figure 2 for distributing two rows. In one extreme case (on the left), the builder distributes the entire row 1 to each VN in the row’s region for improved data availability at the expense of higher resource usage. In another extreme case, the builder sends non-overlapping row pieces of row 6 to each VN in that row’s region, which requires fewer resources but results in less availability of individual cells.</p>
<p>We are currently evaluating different distribution strategies, including ones that can deterministically map individual cells of rows/columns to individual VNs in the row/column’s region.</p>
<p><strong>NOTE</strong>: The builder is only involved in the Seeding phase.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/d/2d447780eca05e1064f2f780b7a1dcaad92a386e.png" title="image1"><img alt="image1" height="374" src="https://ethresear.ch/uploads/default/optimized/3X/2/d/2d447780eca05e1064f2f780b7a1dcaad92a386e_2_690x374.png" width="690" /></a></div><br />
<strong>Figure 2:</strong> Two (extreme) strategies to distribute row samples to the VNs in the corresponding row’s region.<p></p>
<p><strong>Phase 2- Row/Column Consolidation and Sampling</strong>: VNs that are part of the current slot’s committee aim to complete random sampling within the slot’s first four seconds (i.e., voting deadline). To boost the availability of cells, particularly for the committee members of the slot who must perform (random) sampling within four seconds, the VNs also consolidate, i.e., retrieve the full row and column they are assigned to based on the F<sub>NODE</sub> mapping as part of row/column sampling.</p>
<p><strong>2.a) VN Random Sampling:</strong> The VNs in the current slot’s committee attempt to retrieve <a href="https://www.google.com/url?q=https://ethresear.ch/t/peerdas-a-simpler-das-approach-using-battle-tested-p2p-components/16541/5?u%3Doascigil&amp;sa=D&amp;source=editors&amp;ust=1726244230027394&amp;usg=AOvVaw3dcjIB9khL3JRIj_je8GsM" rel="noopener nofollow ugc">73 randomly chosen cells</a> as soon as they receive their seed samples from the builder.</p>
<p>Using the deterministic assignment F<sub>NODE</sub>, VNs can locally determine the nodes expected to eventually custody a given row or column.</p>
<p><em>Sampling Algorithm:</em> Some of these nodes may be offline or otherwise unresponsive. Sequentially sending requests for cells risks missing the 4-second deadline for the committee members.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/3/b315051e7742d17e0aabd822c6d282c69402eead.png" title="image2"><img alt="image2" height="276" src="https://ethresear.ch/uploads/default/optimized/3X/b/3/b315051e7742d17e0aabd822c6d282c69402eead_2_690x276.png" width="690" /></a></div><br />
<strong>Figure 3:</strong> Sample Fetching Example: The rows and columns assigned to each VN are shown on the top of the corresponding VN. VN14 knows to send a request to VN78 to retrieve cell one based on the knowledge of the mapping F<sub>NODE</sub>.<p></p>
<p>At the same time, sending requests to all peers holding copies will lead to an explosion of messages in the network and bear the risk of congestion. Fetching must, therefore, seek a tradeoff between the use of parallel and redundant requests on the one hand and latency constraints on the other hand. Our approach employs an adaptive cell-fetching strategy using direct communication between nodes through a UDP-based (connectionless) protocol. The fetching algorithm can tolerate losses and offline nodes.</p>
<p><strong>2.b) VN Row/Column Consolidation:</strong> If a VN receives less than half of the cells of its assigned row or column from the builder (as a consequence of the builder’s chosen distribution strategy), it requests the missing cells from other VNs. A VN requests cells from only the VNs assigned to the same row/column’s region during row/column consolidation. When a VN has half of the cells of a row or column, it can locally reconstruct the entire row or column.</p>
<p><em>The Rationale for Consolidating Row/Column:</em></p>
<ul>
<li><em>Reconstructing missing cells</em>: while performing row/column sampling, VNs reconstruct missing cells.</li>
<li><em>To boost the availability of cells</em>: Given the deterministic mapping (F<sub>NODE</sub>), the builder can choose any distribution strategy to send subsets of rows and columns to the VNs. Row/Column consolidation aims to improve the availability of samples so that random sampling can be completed on time.</li>
</ul>
<p>Ideally, the builder should select a seed sample distribution strategy that enables VNs to consolidate rows and columns efficiently. To facilitate this, the builder can push each VN a map (together with the seed samples) that details how individual cells of a row/column are assigned to VNs within that row/column’s region as part of the builder’s distribution strategy. With this map, VNs can quickly identify and retrieve missing cells to reconstruct a complete row, thereby improving the availability of the data.</p>
<p><strong>NOTE:</strong> In some DAS approaches, the term ‘row/column sampling’ refers to nodes retrieving multiple rows and columns before voting on the availability of the blob. In our approach, nodes retrieve rows and columns to enhance data availability, supporting validators who must perform random sampling before they vote.</p>
<p>We refer to this as ‘row/column consolidation’ instead of ‘row/column sampling’ because in PANDAS, committee members vote based on random sampling, and they do not directly sample entire rows or columns.</p>
<p><strong>What about Regular Nodes (RNs)?</strong></p>
<p>Unlike VNs, RNs do not obtain seed row/column samples from the builder. The builder sends initial seed samples to a Sybil-resistant group of VNs that use the <a href="https://www.google.com/url?q=https://ethresear.ch/t/proof-of-validator-a-simple-anonymous-credential-scheme-for-ethereums-dht/16454&amp;sa=D&amp;source=editors&amp;ust=1726244230030492&amp;usg=AOvVaw3FE6AN3DjFuibklaNwyDPO" rel="noopener nofollow ugc">Proof-of-Validator</a> scheme. There is currently no mechanism for RNs to prove that they are not Sybils; therefore, the initial distribution of samples from the builder only uses VNs.</p>
<p>Using the public deterministic function F<sub>NODE</sub>, RNs can be similarly mapped to individual row/column regions. Once mapped to a region, RNs can (optionally) perform row/column consolidation to retrieve entire rows and columns and respond to queries for cells within their assigned region.</p>
<p>Like other nodes, RNs must perform peer discovery. In general, RNs aim to discover all the VNs and can also seek to discover other RNs. Given the knowledge of other peers through peer discovery, RNs can perform random sampling through direct communication. Unlike VNs, RNs are not under strict time constraints to complete sampling — they can start sampling after the VNs, for instance, after receiving the block header for the current slot.</p>
<p><strong>Discussion &amp; On-going Work</strong></p>
<p>We assume rational builders to have an incentive to cut costs (and under provision) but, at the same time, aim to make blocks available (to be rewarded). This implies that the builders will aim for the row/column consolidation to be as efficient as possible, i.e., with efficient consolidation, which boosts the availability of cells, the builder can send less copies of each cell during the seeding phase to cut costs.</p>
<p>We are currently experimenting with different distribution strategies with malicious VNs withholding samples and attempting to disrupt peer discovery. Our DAS simulation code is available on <a href="https://www.google.com/url?q=https://github.com/datahop/kademlia-simulator/&amp;sa=D&amp;source=editors&amp;ust=1726244230032037&amp;usg=AOvVaw1UC1uGBj8kO1dTX4Q01QRY" rel="noopener nofollow ugc">DataHop GitHub repository</a>.</p>
<p><sup>1</sup> Lancaster University, UK</p>
<p><sup>2</sup> City, University London, UK</p>
<p><sup>3</sup> Université Catholique de Louvain (UCLouvain)</p>
<p><sup>4</sup> DataHop Labs</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/pandas-a-practical-approach-for-next-generation-data-availability-sampling/20426">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 13 Sep 2024 15:23:19 +0000</pubDate>
</item>
<item>
<title>AUCIL: An Auction-Based Inclusion List Design for Enhanced Censorship Resistance on Ethereum</title>
<link>https://ethresear.ch/t/aucil-an-auction-based-inclusion-list-design-for-enhanced-censorship-resistance-on-ethereum/20422</link>
<guid>https://ethresear.ch/t/aucil-an-auction-based-inclusion-list-design-for-enhanced-censorship-resistance-on-ethereum/20422</guid>
<content:encoded><![CDATA[
<div> 关键词：AUCIL、输入列表创建机制、聚合策略、拍卖机制、抗审查能力

总结:
本文介绍了一种基于竞拍的纳入列表设计（AUCIL），旨在通过理性参与者的竞争来解决以太坊网络中中央化构建生态系统导致的审查问题。AUCIL设计了两个关键组件：

1. **输入列表创建机制**：允许委员会成员在不重复选择交易的同时最大化费用。这确保了许多被屏蔽的交易有机会被考虑纳入。

2. **拍卖机制**：鼓励参与者尽可能多地纳入输入列表，从而形成最终的输出纳入列表。这一机制旨在通过竞争激励，确保尽可能多的交易被纳入。

在AUCIL中，首先通过算法生成输入列表，每个参与者根据特定的概率分布选择交易，以避免贪婪策略带来的非均衡状态。接着，通过拍卖机制确定哪个参与者将获得构建下一个区块的权利，该参与者需要提交最大的交易集合，以此作为赢得拍卖的代价。

此外，文章还讨论了抗审查能力，分析了外部对手可能采取的贿赂策略，包括直接从输入列表中移除交易或减少聚合者的投标，以阻止特定交易的执行。为了应对这些攻击，文章提出使用随机偏置和证明机制来确保至少有p个参与者能够输出无审查的纳入列表。

总的来说，AUCIL通过创新的输入列表生成和聚合策略，结合拍卖机制，为以太坊网络提供了一种抵抗审查的方法，同时保持了系统的公平性和效率。 <div>
<p>By <a class="mention" href="https://ethresear.ch/u/sarisht">@sarisht</a> <a class="mention" href="https://ethresear.ch/u/kartik1507">@kartik1507</a> <a class="mention" href="https://ethresear.ch/u/voidp">@voidp</a> <a class="mention" href="https://ethresear.ch/u/soispoke">@soispoke</a> <a class="mention" href="https://ethresear.ch/u/julian">@Julian</a><br />
In collaboration with <a class="mention" href="https://ethresear.ch/u/barnabe">@barnabe</a> <a class="mention" href="https://ethresear.ch/u/luca_zanolini">@luca_zanolini</a> <a class="mention" href="https://ethresear.ch/u/fradamt">@fradamt</a> - <span class="discourse-local-date">2024-09-12T04:00:00Z UTC</span></p>
<h2><a class="anchor" href="https://ethresear.ch#p-49946-tldr-1" name="p-49946-tldr-1"></a>TLDR;</h2>
<p>In this post, we introduce an <ins>AUC</ins>tion-based-<ins>I</ins>nclusion <ins>L</ins>ist design, AUCIL, that leverages competition within an inclusion list committee consisting of rational parties. The protocol design leverages two key components: (i) an input list creation mechanism allowing committee members to pick non-overlapping transactions while maximizing their fees, and (ii) an auction mechanism allowing parties to ensure most of these input lists are included in the final output inclusion list. The former ensures many censored transactions are considered for inclusion, and the latter employs competition where including as many of the input lists as possible is incentivized to produce the output inclusion list.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49946-introduction-2" name="p-49946-introduction-2"></a>Introduction</h1>
<p>The centralized builder ecosystem of Ethereum today has led to ~2 builders with the power to decide <em>which</em> transactions are posted on Ethereum. This centralization leads to censorship concerns since the builders have complete authority over which transactions are included. The current solution proposed (and rejected) by Ethereum (<a href="https://eips.ethereum.org/EIPS/eip-7547" rel="noopener nofollow ugc">EIP 7547</a>) requires the current proposer to determine the <em>inclusion list</em> (or the set of censored transactions) to be included by the next proposer. Such a proposer also acts as a single point of failure, which can easily be bribed to exclude transactions. This has led to proposals such as <a href="https://ethresear.ch/t/the-more-the-less-censored-introducing-committee-enforced-inclusion-sets-comis-on-ethereum/18835">COMIS</a> and <a href="https://ethresear.ch/t/fork-choice-enforced-inclusion-lists-focil-a-simple-committee-based-inclusion-list-proposal/19870">FOCIL</a> that require inputs from multiple proposers to be aggregated to form the inclusion list.</p>
<p>Intuitively, using multiple proposers implies the need to bribe multiple parties for a transaction to be excluded. However, do all parties include the transaction in the first place? Since the resulting inclusion list is finite (limited to block size), <em>how do each of these parties decide which transactions to include in their local list such that maximizing the utility also increases the system’s throughput?</em> Moreover, when aggregating the transactions to produce the inclusion list, how many points of failure can be bribed to exclude transactions? This post introduces a multi-proposer design called AUCIL to address these questions.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49946-motivation-3" name="p-49946-motivation-3"></a>Motivation</h1>
<p>Let’s first motivate the first part as to how the inclusion lists should be created. For existing inclusion list designs, the intricate assumption is that an IL Proposer can include as many transactions as it sees. While <a href="https://ethresear.ch/t/fork-choice-enforced-inclusion-lists-focil-a-simple-committee-based-inclusion-list-proposal/19870">FOCIL</a> or <a href="https://ethresear.ch/t/the-more-the-less-censored-introducing-committee-enforced-inclusion-sets-comis-on-ethereum/18835">COMIS</a>, leave the proposal of transactions in Local Inclusion List underspecified, <a href="https://arxiv.org/abs/2301.13321" rel="noopener nofollow ugc">Fox et al.</a> assumes that there is no network congestion. However, including all the transactions could lead to a scenario where the size of the inclusion list is larger than the block size. In such a scenario, the builder (constrained by transactions in the Inclusion List) would add as many transactions as possible, dropping any leftover transactions in the inclusion list.</p>
<p>The first thing to note above is that for an IL Proposer, it never makes sense to add more transactions than the block size, and thus, there could be an implicit block space size constraint (<span class="math">\mathcal{L}</span>) on the Local Inclusion List (We would refer to these as Input Lists).</p>
<p>Now, consider that the proposer is passive (i.e., rational but does not accept a bribe). Since each input could be size <span class="math">\mathcal{L}</span>, the resulting union of lists could be of size <span class="math">\geq \mathcal{L}</span>. Now, the builder (or proposer without the PBS) is constrained to pick transactions from the Inclusion List; it would pick the top <span class="math">\mathcal{L}</span> paying transactions, and the rest would not execute. Thus, the inclusion list proposers would only want to include the top <span class="math">\mathcal{L}</span> transactions. Thus, all the previous analysis made for inclusion lists with a scale factor of the number of inclusion list proposers holds in this case (<a href="https://arxiv.org/abs/2301.13321" rel="noopener nofollow ugc">Fox et al.</a>, <a href="https://ethresear.ch/t/fork-choice-enforced-inclusion-lists-focil-a-simple-committee-based-inclusion-list-proposal/19870">FOCIL</a>, <a href="https://ethresear.ch/t/the-more-the-less-censored-introducing-committee-enforced-inclusion-sets-comis-on-ethereum/18835">COMIS</a>).</p>
<p>However, things look very different in the presence of a bribing adversary. Consider that one party is bribed enough (we will quantify this at the end of paragraph) to exclude a top <span class="math">\mathcal{L}</span> paying transaction and instead replace it with <span class="math">(\mathcal{L}+1)^{th}</span> transaction. The builder now receives an inclusion list with <span class="math">\mathcal{L}+1</span> transactions and can choose any transaction to exclude. The adversary can further bribe the builder to exclude the target transaction. Since there is one extra transaction in the list, the block can be formed without violating the properties of an inclusion list (All transactions are executed, or the block space is full). Coming back to the incentives for the party, if it is the only party that deviates from picking top <span class="math">\mathcal{L}</span> transactions, then it would be the only recipient of the fee from <span class="math">(\mathcal{L}+1)^{th}</span> transaction. This may be larger than the utility received (if <span class="math">f_t</span> for the target transaction is not <span class="math">n</span> times larger than <span class="math">f_{\mathcal{L}+1}</span> for the inserted transaction). Even in the worst case, the bribe required would be slightly larger than <span class="math">f_t/n</span>.</p>
<p>All in all, the property of inclusion list that allows the transaction to be excluded if the block is full is a property the design in this post wishes to avoid. Thus, we would restrict the size of input lists to less than <span class="math">\mathcal{L}/n</span> such that even if all parties propose unique transactions, the size of the inclusion list is less than the available block size.<sup class="footnote-ref"><a href="https://ethresear.ch#footnote-49946-1" id="footnote-ref-49946-1">[1]</a></sup></p>
<p>There could exist other solutions to this problem like cumulative non-expiring inclusion list and unconditional inclusion lists, however, these require additional state support, where parties would have to keep track of previous inclusion lists.<sup class="footnote-ref"><a href="https://ethresear.ch#footnote-49946-2" id="footnote-ref-49946-2">[2]</a></sup></p>
<p>As for the other question of how many points of failure exist while using multi-proposer designs, aggregation of lists from all parties is the most critical point of failure, which hasn’t yet been adequately studied. Fox et al. sidestep this by never truly aggregating and assuming that the proposer’s inputs would be included without truly analyzing the problem. In COMIS, the aggregator role is formalized, and they assume that this role is trusted for their analysis. FOCIL removes this assumption by using the proposer of the next block and keeping the point of failure in check with the committee of attesters. However, relying on attesters comes with its share of problems. Attesters are not incentivized to verify; as long as they vote with other attesters, they receive rewards without the risk of a penalty. Using attesters to compute is thus more unreliable than relying on the attesters to confirm the existence of the block or verify a proof as used in this post.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49946-model-4" name="p-49946-model-4"></a>Model</h1>
<p>In this post, we consider all parties involved in consensus as rational, i.e., trying to maximize the value they receive through transaction fees, consensus, or bribery. We will call each party collectively proposing the inclusion list as an IL Proposer and their input as an input list. We will refer to the aggregator as the party that computes a union of these input lists to create an inclusion list. Differing from previous proposals, we assume that the input list size of each party is constrained. The size of an input list can be at most <span class="math">k \leq \mathcal{L}/n</span>, as mentioned in the previous section. The total number of IL proposers is considered to be <span class="math">n</span>. Each transaction <span class="math">tx_i</span> pays a fee of <span class="math">f_i</span> for inclusion in the inclusion list, which is paid to the IL Proposer(s) that include it (chosen by the user independently from the base fee and Ethereum transaction fee). If the transaction repeats across multiple input lists, the fee is equally divided amongst all the IL Proposers that included it tracably on-chain.</p>
<p>We assume an external adversary with a budget such that it can bribe parties to take adversarial actions.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49946-problem-statement-5" name="p-49946-problem-statement-5"></a>Problem Statement</h1>
<p>The problem setting consists of <span class="math">n</span> rational parties who locally have access to a set of censored transactions (<span class="math">M_i</span>) that are continually updated (their mempool). Let <span class="math">M = \cap_i M_i</span>. The problem is to create a list of <em>valid</em> transactions with each party contributing a share of transactions it observes. </p>
<p><strong>Adversarial model.</strong> We assume each of the <span class="math">n</span> parties is rational, i.e., they maximize their utility. We assume a bribing adversary will bribe these parties to censor one or more transactions.</p>
<p><strong>Definition (<span class="math">(b,p,T)</span>-Censorship Resistance.)</strong> We say that a protocol is <em><span class="math">(b,p,T)</span>-censorship resistant</em> if given a budget <span class="math">b</span> to an external adversary for bribing parties, for all transactions <span class="math">t \in T(M)</span> at least <span class="math">p</span> parties output a list which contains all the transactions in <span class="math">T(M)</span>.</p>
<p>The protocol design aims to maximize <span class="math">b</span> for a fixed <span class="math">p</span> and <span class="math">|T(M)|</span>. More concretely, in non-multi-proposer inclusion list design schemes, <span class="math">b</span> is typically <span class="math">O(f)</span>, but our protocol aims to obtain <span class="math">b = O(n\cdot f)</span>.</p>
<p>To facilitate understanding of the goal, <span class="math">T(M)</span> can be considered the “feasible” subset of transactions in <span class="math">M</span>, e.g., those paying sufficiently high fees subject to a space limit. The definition of <span class="math">T</span> depends on the protocol we implement, and it is justified why such a <span class="math">T</span> is used.</p>
<p>In our protocol, we assume that <span class="math">M_i = M</span>. When <span class="math">M_i \neq M</span>, our protocol does not satisfy the definition since it may output a higher paying transaction that appears in some <span class="math">M_i</span> at the expense of some lower paying transaction in the intersection</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49946-input-list-creation-mechanism-6" name="p-49946-input-list-creation-mechanism-6"></a>Input List Creation Mechanism</h1>
<p>The first question we address is how IL Proposers select transactions for their input lists. A simple approach is for IL Proposers to naively choose the transactions that pay the highest fees, regardless of the actions of others. However, this greedy approach is not a Nash equilibrium. If all other IL Proposers are greedily selecting transactions, the rational choice for any IL Proposer might not be to do the same. <strong>Table 1</strong> illustrates this point.</p>
<div class="md-table">
<table>
<thead>
<tr>
<th>Strategy</th>
<th>Objects Picked</th>
<th>Utility</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pick Top Paying</td>
<td><span class="math">(o_1,o_2)</span></td>
<td>7</td>
</tr>
<tr>
<td>Alternate</td>
<td><span class="math">(o_3,o_4)</span></td>
<td>15</td>
</tr>
</tbody>
</table>
</div><p><strong>Table 1</strong>: Picking top-paying objects is not a Nash equilibrium. Consider transactions <span class="math">(\{o_1,o_2,o_3,o_4,o_5,o_6\})</span> with utilities <span class="math">(\{11, 10, 9, 6, 4, 3\})</span> respectively and three players with max size input list of 2. Other players are assumed to follow the strategy of picking the top-paying transaction.</p>
<p>A more viable approach is to use mixed strategies, where each party selects transactions based on a predefined probability distribution. Deviating from this distribution would result in lower expected revenue. However, a mixed Nash equilibrium may not be sufficient, especially in games where players can wait to observe others’ actions before deciding. Thus, this post explores a correlated equilibrium instead.</p>
<p>A correlated equilibrium is a situation where each player is suggested specific actions, and deviating from these suggestions leads to lower utility, assuming others follow the suggestions. To prevent centralization (by asking a single known party to send recommendations), we propose a well-known algorithm that each party can run locally to simulate these suggested actions. Deviating from the algorithm would result in lower utility for the deviating party.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49946-algorithm-1-a-greedy-algorithm-for-transaction-inclusion-7" name="p-49946-algorithm-1-a-greedy-algorithm-for-transaction-inclusion-7"></a>Algorithm 1: A Greedy Algorithm for Transaction Inclusion</h3>
<p><strong>Input</strong>: <span class="math">( n \geq 0 )</span>, <span class="math">( m \geq 0 )</span>, <span class="math">( k \geq 0 )</span>  (number of players, transactions, input list size)</p>
<p><strong>Output</strong>: <span class="math">( L_i )</span> arrays for all <span class="math">( i \in P )</span> (final inclusion lists for each player)</p>
<ol>
<li><span class="math">P \gets [1,\dots,n]</span></li>
<li><span class="math">U \gets [u_1,\dots, u_m]</span></li>
<li><span class="math">N \gets [1,\dots,1]</span></li>
<li><span class="math">\forall i \in P: L_i \gets [1,\dots,1]</span></li>
<li><span class="math">l \gets 0</span></li>
<li><strong>while</strong> <span class="math">l &lt; k</span>  <strong>do</strong>
<ol>
<li><span class="math">i \gets 0</span></li>
<li><strong>while</strong> <span class="math">i &lt; n</span> <strong>do</strong>
<ol>
<li><span class="math">U_{curr} \gets (U \otimes L_i) \oslash N</span></li>
<li><span class="math">s \gets argmax(U_{curr})</span></li>
<li><span class="math">L_{i}[s] \gets 0</span></li>
<li><span class="math">N[s] \gets N[s] + 1</span></li>
<li><span class="math">i \gets i + 1</span></li>
</ol>
</li>
<li><strong>end while</strong></li>
<li><span class="math">l \gets l + 1</span></li>
</ol>
</li>
<li><strong>end while</strong></li>
<li><strong>return</strong> <span class="math">\forall i \in P: L_i</span></li>
</ol>
<hr />
<p>This algorithm iteratively updates each player’s transaction inclusion status. Each player’s input list <span class="math">(L_i)</span> indicates whether a transaction has been included (0) or not (1). The algorithm aims to maximize utility values greedily, including transactions based on their current utility and the number of times each transaction has been included.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49946-description-of-the-algorithm-8" name="p-49946-description-of-the-algorithm-8"></a>Description of the algorithm</h3>
<p>Consider the following simulation protocol. All parties are first numbered randomly. Since the randomness needs to be the same across all parties, a random seed is agreed upon before the start of the protocol. All parties are assigned items greedily, one at a time. Each party picks the item that gives the maximum utility at that instant. To do so, it computes the current utility of all objects yet to be chosen <span class="math">\left((U \otimes L_i) \oslash N\right)</span>. The first <span class="math">(U \otimes L_i)</span> makes the utility of all objects already chosen by <span class="math">i</span> as 0, and then <span class="math">\oslash N</span> divides by the number of parties sharing the object if party <span class="math">i</span> decides to pick that object. The list of objects the party picks is updated (0 implies the object is chosen), and the number of parties picking the object is also updated. The procedure is repeated <span class="math">k</span> times such that each party picks <span class="math">k</span> objects. This protocol achieves a correlated equilibrium. Note that while the protocol assigns objects to parties one at a time, in practice, the output recommends all transactions to the parties at once.</p>
<p>This protocol provably achieves a correlated equilibrium while also achieving a notion of game-theoretic-fairness properties (almost equal distribution of fee) (Paper to follow soon). The set of all transactions chosen by the input list creation algorithm is <span class="math">T(M)</span>, for which we achieve <span class="math">(b,p, T)</span>-censorship resistance through AUCIL, which follows.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49946-aggregation-of-input-lists-9" name="p-49946-aggregation-of-input-lists-9"></a>Aggregation of input lists</h1>
<p>After creating input lists, the next step is to aggregate the lists to create an inclusion list for the next block. If a transaction appears in the inclusion list, it is constrained to appear in the next block. Since the space occupied by the input list is fixed, it cannot suffer from spam transactions since each transaction is confirmed valid (with an adequate base fee) right before the block that includes it.</p>
<p>A standard way to approach this problem is to assign a party the role of an <em>aggregator</em>. This aggregator would compute the union of all the input lists and add it to the inclusion list. However, this aggregator is now a single point of failure. For instance, it may be the case that the aggregator may not receive input lists from all IL proposers and thus cannot be expected to add all input lists. However, if we consider this and only require it to include some threshold number of input lists, then the aggregator can strategically omit specific input lists and significantly reduce the required budget to censor transactions.</p>
<p>So, what can be done in this case? <a href="https://ethresear.ch/t/fork-choice-enforced-inclusion-lists-focil-a-simple-committee-based-inclusion-list-proposal/19870">FOCIL</a> requires the proposer of the following block to include an inclusion list, a superset of local input lists. However, it still allows for some transactions to not be on the inclusion list (due to the threshold). Instead, we look at a different way to deal with this problem. We auction off the role of the aggregator; however, instead of paying a bid to win the role of the aggregator, the bids are the size of the inclusion list. Thus, if a party <span class="math">P</span> proposes a larger inclusion list than all other parties, then <span class="math">P</span> would be rewarded with the aggregator role and reward.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49946-algorithm-aucil-outline-10" name="p-49946-algorithm-aucil-outline-10"></a>Algorithm: AUCIL Outline</h3>
<p><strong>Participants:</strong> All IL proposers <span class="math">P_1, P_2, \ldots, P_n</span></p>
<h4><a class="anchor" href="https://ethresear.ch#p-49946-step-1-il-proposers-broadcast-input-lists-11" name="p-49946-step-1-il-proposers-broadcast-input-lists-11"></a>Step 1: IL Proposers Broadcast Input Lists</h4>
<ul>
<li>For each proposer <span class="math">P_i</span>:
<ul>
<li><span class="math">P_i \rightarrow_B</span> (broadcasts to all parties): <span class="math">\text{inpL}_i</span></li>
</ul>
</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-49946-step-2-parties-aggregate-input-lists-into-an-inclusion-list-and-broadcast-it-12" name="p-49946-step-2-parties-aggregate-input-lists-into-an-inclusion-list-and-broadcast-it-12"></a>Step 2: Parties Aggregate Input Lists into an Inclusion List and Broadcast It</h4>
<ul>
<li>For each party <span class="math">P_j</span>:
<ul>
<li><span class="math">\text{incL}_j = \bigcup_{i=1}^{n} \text{inpL}_i</span></li>
<li><span class="math">P_j \rightarrow_B</span> (broadcasts to all parties):<span class="math">\left(\text{incL}_j, \ell_j = \text{size}(\text{incL}_j)\right)</span></li>
</ul>
</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-49946-step-3-proposer-selects-the-highest-bid-inclusion-list-13" name="p-49946-step-3-proposer-selects-the-highest-bid-inclusion-list-13"></a>Step 3: Proposer Selects the Highest Bid Inclusion List</h4>
<ul>
<li>Proposer receives: <span class="math">\{(\text{incL}_1, \ell_1), (\text{incL}_2,\ell_2), \ldots, (\text{incL}_n,\ell_n)\}</span></li>
<li>Proposer selects the highest bid.</li>
</ul>
<p>While <strong>Step 2</strong> has its incentives clear by introducing aggregation rewards (<span class="math">u_a</span>), <strong>Step 1</strong> and <strong>Step 3</strong> are not incentive compatible. If all other parties broadcast their input lists, then it is dominant not to broadcast its input list for a party. This way, it can create the largest inclusion list and thus win the auction. Thus, <strong>Step 1</strong> is not incentive-compatible. Similarly, the proposer is not incentivized to pick the largest bid. Censorship in auctions (<a href="https://arxiv.org/abs/2301.13321" rel="noopener nofollow ugc">Fox et al.</a>) has been studied and is easily applicable here. Thus, <strong>Step 3</strong> is also not incentive-compatible.</p>
<p>Recall the definition of censorship resistance. If some protocol satisfies the definition of <span class="math">(b,p, T)</span>-censorship resistance, then at least <span class="math">p</span> parties output a non-censored inclusion list. Thus, we require the proposer to include proof of the included bid being greater than <span class="math">n-p</span> other bids (e.g., including <span class="math">n-p</span> bids). If the proposer fails to add such proof, the block would be considered invalid, thus making <strong>Step 3</strong> incentive compatible.</p>
<p>We make the auction biased to deal with the problem of not broadcasting. First, observe that if no party is broadcasting its input list, then the probability of winning the auction for any party is very low; thus, broadcasting its input list at least yields the rewards from including the input list in making the inclusion list. Thus, if more people believe that keeping its input list private does not lead to a significant increase in the probability of winning, then parties would be incentivized to broadcast its input list.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/5/d5699e95fe4dfa9c4562533d720b3400e0a1805b.png" title="AUCIL-Outline"><img alt="AUCIL-Outline" height="420" src="https://ethresear.ch/uploads/default/optimized/3X/d/5/d5699e95fe4dfa9c4562533d720b3400e0a1805b_2_690x420.png" width="690" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#p-49946-algorithm-aucil-14" name="p-49946-algorithm-aucil-14"></a>Algorithm: AUCIL</h3>
<p><strong>Participants:</strong> All IL proposers <span class="math">P_1, P_2, \ldots, P_n</span></p>
<h4><a class="anchor" href="https://ethresear.ch#p-49946-step-0-il-proposers-generate-their-auction-bias-15" name="p-49946-step-0-il-proposers-generate-their-auction-bias-15"></a>Step 0: IL Proposers Generate Their Auction Bias</h4>
<ul>
<li>For each proposer <span class="math">P_i</span>:
<ul>
<li><span class="math">P_i</span> generates a random bias: <span class="math">\text{bias} \gets \text{VRF}(P_i, \text{biasmax})</span></li>
<li><em>(The bias is uniformly distributed between 0 and <span class="math">\text{biasmax}</span> and is added to the bid.)</em></li>
</ul>
</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-49946-step-1-il-proposers-broadcast-input-lists-16" name="p-49946-step-1-il-proposers-broadcast-input-lists-16"></a>Step 1: IL Proposers Broadcast Input Lists</h4>
<ul>
<li>For each proposer <span class="math">P_i</span>:
<ul>
<li><span class="math">P_i \rightarrow_B</span> (broadcasts to all parties): <span class="math">\text{inpL}_i</span></li>
<li><em>(Proposers broadcast their input lists to all parties.)</em></li>
</ul>
</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-49946-step-2-parties-aggregate-input-lists-into-an-inclusion-list-and-broadcast-it-17" name="p-49946-step-2-parties-aggregate-input-lists-into-an-inclusion-list-and-broadcast-it-17"></a>Step 2: Parties Aggregate Input Lists into an Inclusion List and Broadcast It</h4>
<ul>
<li>For each party <span class="math">P_j</span>:
<ul>
<li><span class="math">\text{incL}_j = \bigcup_{i=1}^{y_j} \text{inpL}_i</span>
<ul>
<li><em>(where <span class="math">y_j</span> is the number of input lists party <span class="math">P_j</span> receives.)</em></li>
</ul>
</li>
<li><span class="math">P_j \rightarrow_B</span> (broadcasts to all parties): <span class="math">\left(\text{incL}_j, \ell_j = y_j + \text{bias}\right)</span></li>
<li><em>(Parties declare their bid with the added bias.)</em></li>
</ul>
</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-49946-step-3-proposer-selects-the-highest-bid-inclusion-list-18" name="p-49946-step-3-proposer-selects-the-highest-bid-inclusion-list-18"></a>Step 3: Proposer Selects the Highest Bid Inclusion List</h4>
<ul>
<li>Proposer receives: <span class="math">\{(\text{incL}_1, \ell_1), (\text{incL}_2,\ell_2), \ldots, (\text{incL}_n,\ell_n)\}</span></li>
<li>Proposer selects the highest bid and adds it to the block <span class="math">(\text{incL},\ell)</span>.</li>
<li>Proposer adds proof that the highest bid is greater than <span class="math">n-p</span> other bids.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#p-49946-step-4-attesters-vote-on-the-validity-of-the-block-19" name="p-49946-step-4-attesters-vote-on-the-validity-of-the-block-19"></a>Step 4: Attesters Vote on the Validity of the Block</h4>
<ul>
<li>For each attester:
<ul>
<li>Attester receives: <span class="math">\{(\text{incL}_1, \ell_1), (\text{incL}_2,\ell_2), \ldots, (\text{incL}_n,\ell_n)\}</span> and <span class="math">(\text{incL},\ell)</span></li>
<li>Attester verifies the attached proof and votes only if the proof is correct.</li>
</ul>
</li>
<li>Block is considered valid if it receives more than a threshold of votes.</li>
</ul>
<p>With the above algorithm, we claim that the party is incentivized to broadcast the input list unless the bias drawn is greater than <span class="math">\text{biasmax} -1</span>. Even when the bias is greater than <span class="math">\text{biasmax} -1</span>, a mixed Nash equilibrium still exists, and parties could still choose to broadcast.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49946-censorship-resistance-20" name="p-49946-censorship-resistance-20"></a>Censorship Resistance</h1>
<h3><a class="anchor" href="https://ethresear.ch#p-49946-censorship-by-bribery-to-il-proposers-21" name="p-49946-censorship-by-bribery-to-il-proposers-21"></a>Censorship by bribery to IL Proposers</h3>
<p>The first attack step an adversary can take is removing a transaction from the input lists. For this, assume that a bribe is given to those IL Proposers who are assigned to include the target transaction. This bribe should be enough to ensure that the target transaction is excluded from each input list with probability 1. It is assumed (for now) that each of these IL Proposers would compute the union of all observed input lists in <strong>Step 3</strong>.</p>
<p>Fox et al. analyze the bribe required for a multi-proposer scenario. In their case, it is assumed that the transaction repeats across all proposers. If a transaction pays a fee (higher fee for them) of <span class="math">f_i</span>, then the adversary would have to pay <span class="math">n</span> times the fee to censor the transaction.</p>
<p>In our case, the analysis is similar. If the transaction repeats across <span class="math">\kappa_i</span> input lists, then the expected bribe required is <span class="math">\kappa_i f_i</span>. The parameter <span class="math">\kappa_i</span> is directly proportional to <span class="math">\frac{n\cdot f_i\cdot k}{\sum f_i}</span>, where <span class="math">\sum f_i</span> is the sum of fees paid by all transactions chosen by the protocol. As an intuition for this number, one of our results ensures that the revenue distribution from each transaction is <em>fair</em>, and thus, assumes that each transaction gives the same utility. (Let’s say there exist two transactions paying a fee of 15 and 5, respectively, then the former transaction would be included in thrice as many input lists as the latter transaction. Thus, revenue is the same). <span class="math">n\cdot k</span> represents the total available slots out of which a transaction with fee <span class="math">f_i</span> would occupy <span class="math">\frac{f_i}{\sum f_i}</span> off the total space to maintain the same revenue assumption. Thus, if bribing the IL Proposers to exclude the transaction from the input list is the dominant action (as compared to bribery by aggregator we will mention next), then the protocol would be <span class="math">(b=O(\frac{nkf_i^2}{\sum f_i}),n, T</span>)-censorship resistant.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49946-censorship-by-bribery-to-aggregator-22" name="p-49946-censorship-by-bribery-to-aggregator-22"></a>Censorship by bribery to aggregator</h3>
<p>In an alternate bribery attack, the adversary could bribe a party to reduce its bid by excluding all input lists that contain the target transaction. Thus, the bid for each party decreases by <span class="math">\kappa_i</span>. This would be the same as drawing a bias <span class="math">\kappa_i</span> less than what is drawn. A bias of <span class="math">\text{biasmax}-1</span> is supposed to have almost <span class="math">0</span> probability of winning, and thus, reduction of a party bias to <span class="math">\text{biasmax}-\kappa_i</span>, essentially means the adversary is bribing the party to not participate in the auction. From our analysis, the adversary would have to pay in expectation <span class="math">\frac{\kappa_i n}{biasmax}</span> parties (Each with a bias greater than <span class="math">n-\kappa_i</span>) a bribe of <span class="math">u_a</span> each in order for them not to include the input lists containing the target transaction. Setting <span class="math">\text{biasmax}</span> and <span class="math">u_a</span> to be <span class="math">\sqrt n</span> and <span class="math">\sqrt n \cdot u_{il} \geq \sqrt n \cdot f_i</span>, we achieve <span class="math">(b = O(\frac{n^2kf_i^2}{\sum f_i}),n-\kappa_i\sqrt n+1,T)</span>-censorship resistant.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49946-conclusion-23" name="p-49946-conclusion-23"></a>Conclusion</h1>
<p>We outline an input list building scheme that all parties are incentivized to follow. Working within the confines of limited-size inclusion lists, we achieve significant censorship resistance guarantees (proportional to the number of parties, including the transaction). Then, we looked at an aggregation scheme, AUCIL, that utilizes auctions to incentivize parties to include the largest inclusion list. AUCIL ensures that the aggregator is incentivized to add all input lists to the transaction. We are also analyzing how coalition affects the censorship resistance guarantees and will publish the results soon. Meanwhile, it would be amazing to hear thoughts on AUCIL and the inclusion list building mechanism.</p>
<hr class="footnotes-sep" />

<ol class="footnotes-list">
<li class="footnote-item" id="footnote-49946-1"><p>Note that with <a href="https://eips.ethereum.org/EIPS/eip-1559" rel="noopener nofollow ugc">EIP-1559</a>, the cost to fill the block scales when the block space is full. And so, if the network is not congested, and the adversary is inserting artificial transactions to raise the congestion, then the cost of bribery would be high across multiple blocks. <a class="footnote-backref" href="https://ethresear.ch#footnote-ref-49946-1">↩︎</a></p>
</li>
<li class="footnote-item" id="footnote-49946-2"><p>We achieve the same “unconditional” property as Unconditional ILs without assigning exclusive Inclusion List space. <a class="footnote-backref" href="https://ethresear.ch#footnote-ref-49946-2">↩︎</a></p>
</li>
</ol>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/aucil-an-auction-based-inclusion-list-design-for-enhanced-censorship-resistance-on-ethereum/20422">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 12 Sep 2024 17:45:11 +0000</pubDate>
</item>
<item>
<title>Pricing Ethereum Blocks with Vol Markets with Implications for Preconfirmations</title>
<link>https://ethresear.ch/t/pricing-ethereum-blocks-with-vol-markets-with-implications-for-preconfirmations/20419</link>
<guid>https://ethresear.ch/t/pricing-ethereum-blocks-with-vol-markets-with-implications-for-preconfirmations/20419</guid>
<content:encoded><![CDATA[
<div> 关键词：Ethereum、Block Pricing、Volatility、Arbitrage、Preconfs

总结：

文章探讨了以太坊区块定价的问题，将区块链网络视为金融工具，提出了一个基于期权定价理论的模型来确定购买连续区块的最低价格。该模型结合了传统金融市场波动率（Vol）和以太坊网络中的交易费用、流动性等因素。通过分析历史数据和市场行为，文章发现以太坊短期波动率远高于长期均值，最高可达273%，并提出了一种策略，即在中心化交易所（CEX）卖出波动率期权（Strangles），同时在以太坊网络上购买预确认区块（Preconfs），以此获得无风险利润。

文章进一步指出，随着未来以太坊区块空间承诺合同的推出，这种策略将更加可行，可以覆盖多个区块，从而更精确地定价预确认区块。此外，文章还讨论了与不同交易场所（如中心化交易所、去中心化交易所）之间的套利机会，以及如何利用这些机会构建对冲策略或进行相对价值交易。

总的来说，文章提供了一种创新的方法，通过结合金融市场波动性和区块链技术特性，为以太坊预确认区块定价提供了新的视角和策略。这种方法不仅能够帮助交易者进行有效套利，还可能成为未来区块链金融领域的一个重要研究方向。 <div>
<h1><a class="anchor" href="https://ethresear.ch#p-49938-ethereum-block-pricing-in-the-context-of-vol-markets-1" name="p-49938-ethereum-block-pricing-in-the-context-of-vol-markets-1"></a>Ethereum Block Pricing in the Context of Vol Markets</h1>
<p><em>by <a href="https://x.com/lepsoe">Lepsoe </a> (<a href="https://www.ethgas.com/">@ETHGas</a>)</em></p>
<p><em>With thanks to the <a href="https://x.com/Commit_Boost">Commit Boost</a>, and <a href="https://x.com/titanbuilderxyz">Titan </a> teams for making Preconfs a near-term open and scalable possibility, and <a href="https://x.com/DrewVdW">Drew</a> for prompting the market sizing exploration</em></p>
<h2><a class="anchor" href="https://ethresear.ch#p-49938-tldr-2" name="p-49938-tldr-2"></a><em>TL;DR</em></h2>
<ul>
<li>With the forthcoming gas markets and the ability to buy Entire Blocks, we look at how to price these taking into account prevailing market Volatility, Token prices, Transaction Fees, and Liquidity</li>
<li>Treating the Blockchain/Network as a financial instrument, Block purchases are effectively Options on this network. If one can buy 5 blocks of Ethereum (e.g. 1 minute), one can observe prices in CEXs over this time with an option to monetize the difference between CEX and DEX prices (e.g. latency arb trade)</li>
<li>Buying a block is analogous to buying a Straddle on the Network, and all its DEXs. Taking into account transaction fees, liquidity and slippage, however, this is more analogous to a Strangle.</li>
<li>We then employ an arbitrage trade that involves Shorting European Strangles in CEX (e.g. Deribit, Binance, OKX), and Buying Blocks or Preconfs of Ethereum. This implies a minimum or floor price for one or many consecutive blocks</li>
<li>We can then draw a direct, real-time connection between the current implied Vol for ETH, BTC, SOL, etc… and Preconfs prices</li>
<li>We conclude that if ETH Vol is 75%, and transaction fees are 0.10%, then buying 5 consecutive blocks of Ethereum should be no lower than 6.9 Gwei</li>
<li>Historically, very short-end vol appears to rise dramatically higher than 75% with a Mean of 273%, although the median remains at 75% over the last 2 years</li>
<li>With the current PBS flow and prior to blockspace commitment contracts, this strategy is possible but limited to only the current/next block. With the ability to buy two or more blocks, it becomes easier to execute on and thus price Preconfs with confidence</li>
<li>Connecting the two markets, Vol and Macro traders may therefore trade the Preconf markets, in some cases, with little care as to how these instruments are used or valued with respect to the underlying physical gas markets themselves (e.g. typical orderflow, MEV)</li>
<li>The terms Preconfs and Blocks are used interchangeable for readability</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-49938-background-3" name="p-49938-background-3"></a>Background</h2>
<p>How much are Ethereum’s blocks worth?</p>
<p>Arbitrage, often referred to as ‘arb’ trading, typically involves quantitative strategies that exploit pricing discrepancies or minor imbalances between closely related financial instruments. These instruments may be similar in nature or expected to exhibit similar behaviors over time - they can be priced with models or priced using dynamic replication (such as options replicated through dynamic hedging).</p>
<p>One such arb is statistical arbitrage (‘stat arb’) that frequently employs mean reversion models to capitalize on short-term pricing inefficiencies. Another one is latency arbitrage that takes advantage of minute price variations across different trading venues. In the cryptocurrency, a common form of arbitrage is known as CEX/DEX arb, a type of latency arbitrage where decentralized exchanges (DEXs) respond more slowly to market changes than centralized exchanges (CEXs), largely due to differing block or settlement times. In such scenarios, traders engage in relative-value or pairs trading between centralized exchanges (such as Binance and OKX) and decentralized exchanges (such as Uniswap and Curve).</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49938-the-network-as-a-financial-instrument-4" name="p-49938-the-network-as-a-financial-instrument-4"></a>The Network As a Financial Instrument</h3>
<p>In this article, we look to delineate, and quantify such an arbitrage trade between two seemingly different instruments: the Vol markets on CEX vs the Ethereum Blockchain itself (i.e. the Network, not DEXs).</p>
<p>The purpose of this article is to introduce a closed-form solution to price a floor price for Ethereum Blocks drawing a direct relationship between Vol markets and the minimum price one should pay for Ethereum Blocks. More specifically, we will look at the effect of selling Strangles on ETH (and other tokens) in CEX, while buying Blockspace Commitments (or Preconfirmations) on Ethereum.</p>
<p>While this type of relationship may exist with limited effect today for 12 seconds, the burgeoning space of preconfirmations and validator commitments will enable this to exist for much longer periods turning what may be a theoretical exercise today into a practical exercise tomorrow.</p>
<p>Through this exercise, we position the Blockchain or Network itself as a financial instrument that can be used for macro hedging or relative value trading purposes.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49938-what-is-a-strangle-5" name="p-49938-what-is-a-strangle-5"></a>What is a Strangle?</h3>
<p>The building blocks of options markets or ‘Vol’ markets are ‘vanilla’ options known as calls and puts. Combining such vanilla options together at the same strike produces a ‘V-shaped’ payoff known as a ‘Straddle’. A Straddle will always have a positive intrinsic value or payoff enabling the buyer to monetize any movement of the underlying instrument.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/1/71515733c74792ef64bb5afa1456f44c8a078678.png" title="image"><img alt="image" height="399" src="https://ethresear.ch/uploads/default/optimized/3X/7/1/71515733c74792ef64bb5afa1456f44c8a078678_2_690x399.png" width="690" /></a></div><br />
<em>Figure 1: Straddles vs Strangles</em><p></p>
<p>When the strikes are apart from one another, in the above example by a distance of ‘z’, they are called a ‘Strangle’. For example:</p>
<ul>
<li>A Put and Call both with strikes of 100 (i.e. X) would collectively be called a Straddle</li>
<li>A Put and Call with strikes of 90 (i.e. X - z) and 110 (i.e. X + z) respectively, would collectively be a Strangle</li>
</ul>
<p>Strangles payoff or have an intrinsic value only when the underlying spot price has moved by a sufficient distance, in this case ‘z’.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49938-what-are-preconfirmations-6" name="p-49938-what-are-preconfirmations-6"></a>What Are Preconfirmations?</h3>
<p>Preconfirmations and Blockspace Commitments are part of a new field of Ethereum Research and Development focused on giving Validators (called Proposers, i.e. those that Propose the next epoch of blocks) expanded abilities to sell blockspace in a way that gives them more flexibility than they are currently afforded within the current PBS (Proposer-Builder- Separation) flow.</p>
<p>Such an initiative is intended broadly to bring more control in-protocol (as opposed to externally with Block Builders), and streamline scaling technology for the new field of Based Rollups.</p>
<p>While there are different forms of Blockspace Commitments, the general form has Proposers providing commitments to buyers - typically Searchers, Market Makers, Block Builders, and others looking to use the blockspace for transactions, among other purposes. For example, there are:</p>
<ul>
<li>Inclusion Preconfirmations: Where Proposers issue guarantees to include transactions within a specified block, anywhere in the block</li>
<li>Execution Preconfirmations: Where Proposers issue guarantees to include transactions within a specific block, with a specific state or result</li>
<li>Whole Block Sales which may be called Entire Blocks or Execution Tickets: Where Proposers sell their block en masse to an intermediary who then engages in some form of pseudo block building consisting perhaps of a mix of their own trades, Inclusion Preconfirmations, Execution Preconfirmations, private order flow, and public order flow.</li>
</ul>
<p>For the purposes of this paper, we will be referring to Whole Block Sales by Proposers, but may refer to them generically as Preconfirmations or Preconfs for ease of reading and consistency with some current nomenclature.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49938-current-preconf-and-blockspace-pricing-7" name="p-49938-current-preconf-and-blockspace-pricing-7"></a>Current Preconf and Blockspace Pricing</h3>
<p>The value of Ethereum blocks are often associated with the Maximum Extractable Value (MEV), that is, the largest amount of value that one could extract or monetize within a 12 second period. This may include a mix of the public’s willingness to pay for transactions (financial and non-financial), private order flow, as well as other MEV trades including sandwich attacks, atomic arbitrage, CEX/DEX arb, or other.</p>
<p>Extending into the Multi-block MEV (MMEV) or Consecutive Block valuation, MMEV valuation is often performed in the context of TWAP oracle manipulation attacks producing forced liquidations by price manipulation. While there is an intersection between longer-term CEX/DEX arb captured in single-block pricing discussions vs the relative value vol markets, we prefer the simplicity and forward-looking nature of the vol markets for the purpose of our pricing exercise.</p>
<p>Putting this together, there are multiple ways to value a single or multiple set of Ethereum blocks. From our analysis, we present a floor price for Ethereum blocks driven by non-arbitrage pricing and the Vol markets in CeFi. From this floor price one may additionally then consider encompassing other forms of value capture to arrive at a true mid-market price of an Ethereum Block.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49938-the-trade-8" name="p-49938-the-trade-8"></a>The Trade</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-49938-historical-background-9" name="p-49938-historical-background-9"></a>Historical Background</h3>
<p>Buying a block, or multiple blocks of Ethereum enables one more control over order execution and states. Simply, if it were possible to buy 12.8 minutes of Ethereum (i.e. 64 blocks or two epoch) one could watch prices as they move in CEX during this time, and at any time during this 12 minute period, one could put on a relative value trade capturing the difference in prices between the CEXs and DEXs. If, for example, prices rose 5% in CEX during this time, one could sell assets in CEX, and buy those same assets in DEX (where the prices haven’t moved) earning 5% in the process. While this may not be currently feasible, it is the starting point for discussion.</p>
<p>Historically, we can look at these dynamics measuring the maximum price movements over 12 secs, 1 min, or more. We can then take into account the liquidity on DEXs and calculate a historical breakeven between the profitability of such transactions with the number of blocks for a given period. For more on this see this article: <a class="inline-onebox" href="https://greenfield.xyz/2024/09/10/statistical-arbitrage-on-amms-and-block-building-on-ethereum-part-1/">| Greenfield</a></p>
<p>While possible to calculate, we’re more interested in looking forward, not backward. Enter the Vol markets.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49938-vol-markets-strangles-10" name="p-49938-vol-markets-strangles-10"></a>Vol Markets &amp; Strangles</h3>
<p>To execute the trade above, one must cross bid-offer, paying transaction fees on both the CEX and DEX side as well as ‘time’ the market accordingly to maximize the arbitrage. One furthermore has to factor in the liquidity or depth of the market. That is, for the strategy to pay off, prices need to move beyond a certain minimum threshold or in our case, a Strike price different from the current Spot price.</p>
<p>Let us assume that the “sum of transaction fees and slippage between CEX/DEX” - our ‘threshold’ or Strike is 0.10%. If we have the Vol of the asset, and a time horizon, we can now price this using Black-Scholes as a simple Strangle.</p>
<p>Assume the following:</p>
<ul>
<li>Trade Size: $10mm</li>
<li>Token: ETH</li>
<li>Spot Price: 100 || to keep things simple</li>
<li>Interest Rates: 4.00%</li>
<li>Dividend Yield: 0.00%</li>
<li>Vol: 75%</li>
<li>Expiry: 32 Blocks (12.8mins)</li>
<li>Fees: 0.10 as accounted for in the following Strikes:
<ul>
<li>Strike 1: 100 + 0.10 = 100.10 - for the Call Option</li>
<li>Strike 2: 100 - 0.10 = 99.90 - for the Put Option</li>
</ul>
</li>
</ul>
<p>Result:</p>
<ul>
<li>Call Price: 0.0620%</li>
<li>Put Price: 0.0619%</li>
<li>Strangle Price: 0.0620% + 0.0619% = 0.1239%</li>
<li>Price in USD Terms: $12,388</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/a/ca508d8bddc9793de7b6326ed95552f760925a2e.jpeg" title="image"><img alt="image" height="376" src="https://ethresear.ch/uploads/default/optimized/3X/c/a/ca508d8bddc9793de7b6326ed95552f760925a2e_2_690x376.jpeg" width="690" /></a></div><br />
<em>Figure 2: A Strangle on Ethereum and all its DEXs combined</em><p></p>
<p>Per the diagram above, if one could trade this Strangle in CEX for $12,388 (see <a href="https://docs.google.com/spreadsheets/d/1wwhe-O8L0eG72Mb0PJGLhlDi1Fxe1E0CSzpX0sAaz2U/edit?usp=sharing">spreadsheet</a> for calculations), one should equivalently be able to trade Preconfs on Ethereum for the same price. If the underlying spot market in CEX moves up or down more than 0.10, whilst DEX prices stay the same, then these options become in-the-money…</p>
<p>Putting CEX and DEX together below, one would sell the Strangle on ETH in CEX but buy Preconfs on Ethereum giving them an almost identical payoff where z represents both the expected transaction fees and the distance to the Strike price for pricing purposes:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/b/9bf362500aa612b87a9254ea96544495005030a3.jpeg" title="image"><img alt="image" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/9/b/9bf362500aa612b87a9254ea96544495005030a3_2_422x500.jpeg" width="422" /></a></div><br />
<em>Figure 3: Short CEX Strangle + Long Ethereum Preconf</em><p></p>
<p>If the Vol markets imply a price of $12,399 for 12.8mins (i.e. 32 blocks) then this is the amount (less one dollar) that one would be willing to pay to buy up 32 consecutive blocks (i.e. 12.8mins) of Ethereum. Given the assumptions above, the expected value is always positive and we thus have a closed-form solution to Floor pricing for Preconfs.</p>
<p>The arbitrage carries two scenarios:</p>
<ul>
<li>Prices are between 99.90 and 100.10: Both the Strangle and Preconf Expire ‘out-of-the-money’ without any cash settlement</li>
<li>Prices are beyond 99.90 and 100.10 with options expiring ‘in-the-money’. The Trader incurs a loss on the CEX Strangle, but then monetizes the gain in DeFi by entering into an off-market spot trade (with respect to CEX) crystallizing the in-the-money value of the option</li>
</ul>
<p>Vol Traders do this 1000s of times a day, with automated systems and razor-sharp precision. Trading Vol vs Preconfs opens up an entirely new relative-value asset class for them to potentially buy vol or gamma much more cheaply.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49938-scenario-analyses-and-sensitivities-11" name="p-49938-scenario-analyses-and-sensitivities-11"></a>Scenario Analyses and Sensitivities</h3>
<p>Turning to Gas Market terminology, the price of $12,399 translates into a Gwei price of 165 Gwei ($12,399 / 2,500 * 1e9 / 30e6) assuming the ETH price is 2,500 in this example. Using the Strangle pricing method, we can then infer from the ETH Vol markets (75% vol in this case) the price of 1 block, all the way up to 32 consecutive blocks or slots as follows:<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/7/57defb97d1f6360692afa2c53920481469468872.png" title="image"><img alt="image" height="285" src="https://ethresear.ch/uploads/default/optimized/3X/5/7/57defb97d1f6360692afa2c53920481469468872_2_690x285.png" width="690" /></a></div><br />
<em>Figure 4: Price for N-Consecutive Blocks of Ethereum</em><p></p>
<p>Comparing the difference in Strangle prices between a period of N(0,1), to a Strangle with a period of length N(0,2), we can then price the Strangle for Slot 2 N(1,2), as follows for the entire curve. We can furthermore take the ‘average preconf price’ for N slots.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/d/cd5b4a440fe65ad5649f789572044f132c3a152f.jpeg" title="image"><img alt="image" height="379" src="https://ethresear.ch/uploads/default/optimized/3X/c/d/cd5b4a440fe65ad5649f789572044f132c3a152f_2_690x379.jpeg" width="690" /></a></div><p></p>
<p><em>Figure 5: Slot N Price vs Avg Price for N-Slots</em></p>
<p>The following table highlights the fees in Gwei that validators would get paid for specific blocks/slots with 5.16 Gwei as the average. This may be compared, for example, to historical Priority Fees that one receives via MEV-Boost where 4.04 Gwei is the average:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/3/c3625c9aef9793341f69e2c496ba4151b8ba8a06.jpeg" title="image"><img alt="image" height="369" src="https://ethresear.ch/uploads/default/optimized/3X/c/3/c3625c9aef9793341f69e2c496ba4151b8ba8a06_2_690x369.jpeg" width="690" /></a></div><br />
<em>Figure 6: Historical Priority Fees from MEV-Boost. Priority Fees from 24 Jan 2024 to 9 Sep 2024.</em><p></p>
<h4><a class="anchor" href="https://ethresear.ch#p-49938-transaction-costs-impact-on-pricing-12" name="p-49938-transaction-costs-impact-on-pricing-12"></a>Transaction Costs Impact on Pricing</h4>
<p>The difference between the Strike Prices and Spot Price or transaction costs above are taken to be uniform at 0.10%. In practice however, transaction costs encompass i) actual transaction fees, and ii) liquidity/slippage in execution. Below, we see that Transaction Costs have a significant impact on Preconf pricing especially where there is a shorter time-to-maturity.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/b/bb067308cb50d9cac678fffc4c4df056a7b697c3.jpeg" title="image"><img alt="image" height="395" src="https://ethresear.ch/uploads/default/optimized/3X/b/b/bb067308cb50d9cac678fffc4c4df056a7b697c3_2_690x395.jpeg" width="690" /></a></div><br />
<em>Figure 7: Preconf Pricing for varying levels of Transaction Costs</em><p></p>
<h4><a class="anchor" href="https://ethresear.ch#p-49938-volatility-impact-on-pricing-13" name="p-49938-volatility-impact-on-pricing-13"></a>Volatility Impact on Pricing</h4>
<p>Finally, as the CEX leg of the trade uses Volatility as the primary market input, we now consider the impact that volatility has on Preconf pricing with Vega close to 0.1 Gwei at the 4th slot, and ~0.06 Gwei at the 32nd slot. That is, <strong>at Slot 4, a 10% change in Vol is impacts Block prices by 1 Gwei.</strong></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/c/6c7b2d7c8762d69423e0fc605cd9d64cd9f85dbc.jpeg" title="image"><img alt="image" height="406" src="https://ethresear.ch/uploads/default/optimized/3X/6/c/6c7b2d7c8762d69423e0fc605cd9d64cd9f85dbc_2_690x406.jpeg" width="690" /></a></div><br />
<em>Figure 8: Preconf Prices for Different levels of Volatility</em><p></p>
<h2><a class="anchor" href="https://ethresear.ch#p-49938-refinements-market-sizing-14" name="p-49938-refinements-market-sizing-14"></a>Refinements &amp; Market Sizing</h2>
<p>For market sizing, we look exclusively at the CEX Strangle vs Preconf on Ethereum L1.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49938-consecutive-blocks-15" name="p-49938-consecutive-blocks-15"></a>Consecutive Blocks</h3>
<p>The exercise considers buying multiple blocks, potentially up to 32 or 64 blocks depending on the lookahead window. In reality however, this is extremely difficult due to the diversity of Validators.</p>
<p>There is a subset of Validators that, for ideological reasons or other, do not adopt MEV-Boost, and would be unlikely to adopt a framework that captures more MEV. In economic terms, they are not rational. It could be that they do not ‘believe’ in MEV, or they simply could be an at-home staker that hasn’t upgraded to MEV-Boost. Either way, these Vanilla or self-built blocks account for slightly less than 10% (and decreasing) of blocks (see realtime with ETHGas’ <a href="http://www.ethgas.com">GasExplorer</a>, and research with <a href="https://www.blocknative.com/blog/how-self-built-blocks-unintentionally-introduce-base-fee-volatility">Blocknative</a>).</p>
<p>Let’s assume the other 90% are rational (i.e. they are economically motivated) and that they are somehow able to coordinate among one another through some unifying medium for the sale of consecutive blocks. In this case, we can then model the frequency of single vs consecutive blocks where about half of the time there are less than 7 consecutive blocks, and the other half have somewhere between 8 and 32 consecutive blocks.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/9/a96d126f31fbaa6afded0e3cdd01172eceba7b03.png" title="image"><img alt="image" height="401" src="https://ethresear.ch/uploads/default/optimized/3X/a/9/a96d126f31fbaa6afded0e3cdd01172eceba7b03_2_690x401.png" width="690" /></a></div><br />
<em>Figure 9: Frequency of Consecutive Blocks</em><p></p>
<h3><a class="anchor" href="https://ethresear.ch#p-49938-historical-volatility-analysis-16" name="p-49938-historical-volatility-analysis-16"></a>Historical Volatility Analysis</h3>
<p>Looking at almost 2 years of trades from 10 Sep 2022 to 10 Sep 2024 on Deribit, we uncover some fascinating dynamics for short-dated transactions.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-49938-h-1-hour-to-expiry-17" name="p-49938-h-1-hour-to-expiry-17"></a>1 Hour to Expiry</h4>
<p>For those transactions with less than 1 hour to expiry, we find approx 13,500 trades over this period, a mean Vol of 107.52%, a Median of 63%, and 75th Percentile as 102%. Note that Deribit’s Vols are capped at 999 suggesting that the mean may be higher than that which is indicated.<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/f/af0a574ea2876081224b6e7b6ecd012ff163221a.jpeg" title="image"><img alt="image" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/a/f/af0a574ea2876081224b6e7b6ecd012ff163221a_2_655x500.jpeg" width="655" /></a></div><br />
<em>Figure 10: Distribution of Implied Vol on ETH Options with less than 1 Hour to Expiry</em><p></p>
<h4><a class="anchor" href="https://ethresear.ch#p-49938-h-12-mins-to-expiry-18" name="p-49938-h-12-mins-to-expiry-18"></a>12 Mins to Expiry</h4>
<p>For transactions with less than 12 mins to expiry (or approx 64 blocks), we find almost 1,400 trades over this period with a mean of 273% Vol, median of 75% Vol, and 75th Percentile as 395% Vol.<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/e/deede16e86a88239a6ecc6e13c99cd93778fa614.jpeg" title="image"><img alt="image" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/d/e/deede16e86a88239a6ecc6e13c99cd93778fa614_2_662x500.jpeg" width="662" /></a></div><br />
<em>Figure: 11: Distribution of Implied Vol on ETH Options 12 Mins to Expiry</em><p></p>
<h4><a class="anchor" href="https://ethresear.ch#p-49938-h-12-minutes-to-expiry-19" name="p-49938-h-12-minutes-to-expiry-19"></a>&lt;12 Minutes to Expiry</h4>
<p>Across these 1,400 trades, we then split them into their 1-minute buckets to view distributions across times more closely associated with Preconf Block timeframes.<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/4/14686bf48d5c989a80356631a03434fa70bca82f.jpeg" title="image"><img alt="image" height="493" src="https://ethresear.ch/uploads/default/optimized/3X/1/4/14686bf48d5c989a80356631a03434fa70bca82f_2_690x493.jpeg" width="690" /></a></div><br />
<em>Figure 13: Distribution of ETH Implied Vol for the last 12 mins to Expiry</em><p></p>
<p>The Vol numbers are far larger than we expected warranting further research into this area. While liquidity will need to be analyzed, we have provided some Preconf-implied Pricing given Vols of a much higher magnitude for convenience:<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/f/df84a11dc4575ba83d2836af936004045965d8a4.jpeg" title="image"><img alt="image" height="411" src="https://ethresear.ch/uploads/default/optimized/3X/d/f/df84a11dc4575ba83d2836af936004045965d8a4_2_690x411.jpeg" width="690" /></a></div><br />
<em>Figure 14: Preconf Implied Prices for very high levels of Volatility</em><p></p>
<h4><a class="anchor" href="https://ethresear.ch#p-49938-vol-smile-20" name="p-49938-vol-smile-20"></a>Vol Smile</h4>
<p>As you may recall, we’re not looking for at-the-money Vol (used for a Straddle) but rather for Vol as it may relate to Strangles. The Vol for out-of-the-money options is almost always higher than at-the-money options. To this effect, we have provided a heat map below providing some color on the smile accordingly.</p>
<p><em>Figure 15: Vol Smile for 0 to 12 minutes</em></p>
<h3><a class="anchor" href="https://ethresear.ch#p-49938-market-sizing-21" name="p-49938-market-sizing-21"></a>Market Sizing</h3>
<p>Bringing the above information together, we decide to take the combined Vol set and use that as a proxy for Strangle pricing. To account for illiquidity, we then provide different scenarios at lower volatilities assuming that as we sell more Strangles, the Vol would decrease accordingly.</p>
<p>We can now size the market considering:</p>
<ul>
<li>The historical mean Vol: 275%</li>
<li>The frequency of Consecutive Blocks: Per the above</li>
<li>The implied preconf Floor pricing as a function of Vol: Black-Scholes</li>
<li>And, making some adjustment for Liquidity: Reducing Vol by up to 200%</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/7/b7472612ce4624f0df42973ca84c9800c70d6792.jpeg" title="image"><img alt="image" height="369" src="https://ethresear.ch/uploads/default/optimized/3X/b/7/b7472612ce4624f0df42973ca84c9800c70d6792_2_690x369.jpeg" width="690" /></a></div><br />
<em>Figure 16: Preconf Pricing Based on Frequency of Consecutive Blocks, Historical Volatility and adjusted for Liquidity</em><p></p>
<p>The annual market size for Blockspace could equal approximately 419,938 ETH per year historically (~$1bln equiv) and with approx 33 million Staked ETH, this amounts to 5.33 Gwei per block or an extra 1.25% in Validator Yields as a floor above Base Fees.</p>
<div class="md-table">
<table>
<thead>
<tr>
<th>Vol</th>
<th>275% Vol</th>
<th>225% Vol</th>
<th>175% Vol</th>
<th>125% Vol</th>
<th>75% Vol</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gwei Total</td>
<td>282,615</td>
<td>218,322</td>
<td>155,081</td>
<td>93,997</td>
<td>38,350</td>
</tr>
<tr>
<td>Gwei per Block</td>
<td>39.25</td>
<td>30.32</td>
<td>21.54</td>
<td>13.06</td>
<td>5.33</td>
</tr>
<tr>
<td>ETH Total Fees</td>
<td>3,094,638</td>
<td>2,390,631</td>
<td>1,698,137</td>
<td>1,029,270</td>
<td>419,938</td>
</tr>
<tr>
<td>Increase to APYs</td>
<td>9.10%</td>
<td>7.03%</td>
<td>4.99%</td>
<td>3.03%</td>
<td>1.24%</td>
</tr>
<tr>
<td>$ Total Fees</td>
<td>7,736,594,273</td>
<td>5,976,577,160</td>
<td>4,245,342,208</td>
<td>2,573,176,209</td>
<td>1,049,844,310</td>
</tr>
</tbody>
</table>
</div><h2><a class="anchor" href="https://ethresear.ch#p-49938-other-considerations-22" name="p-49938-other-considerations-22"></a>Other Considerations</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-49938-liquidity-23" name="p-49938-liquidity-23"></a>Liquidity</h3>
<p>On the CEX side, we would like to assume there is infinite liquidity but this is not realistic. In the example immediately above, we bump the Vol downward to adjust for this but in reality, we would need more order book information. Looking forward, this market could also be illiquid because there was never another market to trade it against, e.g. Preconfs. We furthermore would need to run the analysis considering tokens other than ETH.</p>
<p>Everyday there is a 12-minute direct overlap where a set of option expiries for BTC, ETH, SOL, XRP on Deribit (and other exchanges) roughly match the time-frame for preconfs enabling one to recalibrate and reconcile any intraday Vol positions vs the actual Preconf markets with more accuracy. For the rest of the day, traders would need to run basis-risk between the Vol positions on their books, with their Preconf positions accordingly. As such, execution in the Vol markets and direct one-for-one pairs trading may be limited on a regular basis and only possible sporadically.</p>
<p>As an alternative to directly offsetting the Short Strangle positions with Long Preconfs, a trader may approach this on a portfolio basis and trade the greeks. In this instance, a preconf buyer may consider selling longer-dated, more liquid straddles, and buying them back up to 12 mins later or whenever the preconf is exercised. The gamma profile there is much less sharp meaning any moves in Spot will have a lesser impact on option price. There is additional Vol/Vega to consider (although less impactful for a short-dated option) and the time decay (which is in the arbitrageur’s favor here as they would be Short the options and theta decays faster closer to expiry). If one could seemingly buy Vol 5-10% cheaper via Preconfs over time, then this would indeed be attractive to options traders.</p>
<p>On the DEX side, liquidity across ETH, and other tokens is limited to about $4-5mm at the time of this article. Taking into account the total volume on major DEXs, we’d additionally expect about $200k of additional demand every block from general order flow. Although most of this typically may not be seen in the public mempool, over 32 blocks this would be $6.4mm which one could either use to estimate option expiration liquidity and/or capture via other conventional MEV approaches (i.e. front/back-runs).</p>
<p>More research on liquidity, and execution is warranted.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49938-inventory-24" name="p-49938-inventory-24"></a>Inventory</h3>
<p>To execute trades on two different venues, traders will need to hold sufficient inventory on both locations. For this reason, an additional cost of capital is not considered in this exercise.</p>
<p>For example, if the Call part of the Strangle ends up in-the-money (ITM), when the Preconf is exercised, the user will:</p>
<ul>
<li>Buy, let’s say, ETH in the DEX and sell it in the CEX. That is, the user needs USDT/C inventory onchain, and ETH inventory in the CEX, to avoid any transfer lag.</li>
</ul>
<p>Larger market makers should have sufficient liquidity on both sides making this lesser of an issue.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49938-european-vs-american-options-25" name="p-49938-european-vs-american-options-25"></a>European vs American Options</h3>
<p>The CEX Strangle (i.e. where the Arbitrageur is ‘Short’) is a European Option unlike the Preconfirmation (i.e. where the Arbitrageur is ‘Long’) which is more an American Option. This gives the Arbitrageur positive basis such that the instrument they are ‘Long’ has more optionality or upside built into it. If the Preconf is early exercised, the trader receives the intrinsic value while the Strangle still has some time value (although minimal), therefore, the PNL is equal to the Net Premium minus the time value difference.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49938-what-about-other-mev-and-mmev-26" name="p-49938-what-about-other-mev-and-mmev-26"></a>What About Other MEV and MMEV?</h3>
<p>While there is some intersection between conventional MEV and the Strangle strategy as highlighted above, there is still the value to the everyday deal-flow, alongside significant other forms of MEV that are not captured. Monetization of such flows would be separate to, and in addition to, that of the Floor price.</p>
<p>The Strangle exercise above suggests that some types of single-block MEV may currently be constrained by transaction costs which would indicate a non-linear MMEV for when multi-block purchases are possible (at least within the first few blocks).</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49938-conclusions-27" name="p-49938-conclusions-27"></a>Conclusions</h2>
<p>The purpose of this paper is to open up a discussion and illustrate a novel approach for the pricing of preconfs - one that importantly responds in real-time to prevailing market conditions. While the execution of such a strategy is difficult, it is not insurmountable for sophisticated players to automate.</p>
<p>Perhaps the most important consideration is that the Price of the Preconfs is a function of the Size of the Markets. If both the Options markets on Deribit and DEX liquidity are 10x larger than they are today, the Preconf Price Floors would be 10x those indicated above. Financial markets often look for inflection points where trades that were almost-possible suddenly become mainstream. With Gas Markets opening up, Macro traders now able to hedge Vol with Preconfs, Based Rollups increasing liquidity, and a trend towards lower transaction fees, this is indeed an interesting area of research.</p>
<p>We believe that highlighting a seemingly odd relationship between token Vol and the Ethereum Blockchain itself will help to further the study of risk-neutral block pricing and are excited to discuss and explore this, and other approaches, with any other parties who may be interested.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49938-references-28" name="p-49938-references-28"></a>References</h1>
<p>[ 1 ] Pascal Stichler, <a href="https://ethresear.ch/t/does-multi-block-mev-exist-analysis-of-2-years-of-mev-data/20345">Does multi-block MEV exist? Analysis of 2 years of MEV Data</a></p>
<p>[ 2 ] Öz B, Sui D, Thiery T, Matthes F. Who Wins Ethereum Block Building Auctions and Why?. arXiv preprint arXiv:2407.13931. 2024 Jul 18.</p>
<p>[ 3 ] Jensen JR, von Wachter V, Ross O. Multi-block MEV. arXiv preprint arXiv:2303.04430. 2023 Mar 8.</p>
<p>[ 4 ] Christoph Rosenmayr, Mateusz Dominiak - Statistical Arbitrage on AMMs and Block Building On Ethereum - <a href="https://greenfield.xyz/2024/09/10/statistical-arbitrage-on-amms-and-block-building-on-ethereum-part-1/">Part 1</a></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/pricing-ethereum-blocks-with-vol-markets-with-implications-for-preconfirmations/20419">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 12 Sep 2024 11:55:19 +0000</pubDate>
</item>
<item>
<title>Resolving the Dichotomy: DeFi Compliance under Zero Knowledge</title>
<link>https://ethresear.ch/t/resolving-the-dichotomy-defi-compliance-under-zero-knowledge/20413</link>
<guid>https://ethresear.ch/t/resolving-the-dichotomy-defi-compliance-under-zero-knowledge/20413</guid>
<content:encoded><![CDATA[
<div> 关键词：DeFi、合规挑战、区块链、零知识证明、智能合约

总结:
这篇文章探讨了去中心化金融(DeFi)协议面临的合规挑战，尤其是由于交易资产的特性及其分散的治理方式。为了解决这一问题，文章提出利用区块链原生的合规机制，特别是智能合约和基于链上的可验证零知识证明，以确保合规性、风险管理和必要的交易报告，同时保护用户隐私。该框架通过将与合规相关的辅助信息(CRAI)附加到链上交易，实现了实时的合规监控/验证，并使用零知识证明以隐私保护的方式进行。框架还规定了合规安全的DeFi交互模式，包括使用智能合约钱包、合规智能合约、合规智能合约系统以及零知识证明来执行由合规智能合约系统定义的合规规则。

文章提出了几个关键的研究问题：
1. 实施此框架在现有DeFi协议中可能遇到的挑战和限制。
2. 如何增强框架的隐私功能，以适应复杂合规场景中的多个合规断言，例如使用证明聚合和证明递归。
3. 如何扩展框架以支持更广泛的合规要求，超越KYC/AML，例如集成DAO和代理权。
4. 管理和更新框架内的合规政策时可能出现的治理挑战。
5. 如何利用框架的透明度和问责制特性进一步增强DeFi，如自定义钩子。
6. 如何根据不同的监管环境和司法管辖区调整框架。
7. 实施此框架对DeFi用户和协议的经济影响。

文章旨在激发关于该框架及其建议的研究问题的进一步讨论，期待来自以太坊研究社区的反馈。 <div>
<p>This is a summary and enumeration of relevant research questions based on the recent <a href="https://entethalliance.org/2024-08-20-resolving-the-dichotomy-defi-compliance-under-zero-knowledge/" rel="noopener nofollow ugc">EEA Article by the same title as this post</a>.</p>
<p><strong>Bulleted Summary</strong></p>
<ul>
<li>DeFi protocols face a compliance challenge due to the type of assets traded and their often decentralized governance.</li>
<li>A solution is leveraging blockchain-native compliance mechanisms, specifically smart contracts, and onchain verifiable zero-knowledge proofs.</li>
<li>This approach ensures regulatory compliance, weighted risk management, and required transaction reporting while preserving user privacy.</li>
<li>The framework attaches Compliance-Relevant Auxiliary Information (CRAI) to onchain transactions, enabling real-time compliance monitoring/verification, in a privacy-preserving way using zero-knowledge proofs.</li>
<li>The framework also specifies compliance-safe DeFi interaction patterns involving using smart contract wallets, DeFi compliance contracts, a compliance smart contract system, and zero-knowledge proofs to enforce compliance rules specified in the compliance smart contract system that defines compliance policies, attestation providers, and compliant assets.</li>
<li>The framework offers benefits like regulatory compliance, risk management, privacy protection, security, versatility, transparency, and accountability.</li>
<li>By adopting such a framework, DeFi protocols could navigate the regulatory landscape while maintaining their core principles.</li>
<li>Some of this solution already exists (compliance smart contract system, compliant assets, etc.) and need to be further expanded (smart contract wallets, compliance wrapper contracts, DeFi-specific custom hooks, etc.)</li>
</ul>
<p>Below is a list of open research questions in no particular order:</p>
<ul>
<li>What are the potential challenges and limitations of implementing this framework in existing DeFi protocols?</li>
<li>How can the framework’s privacy features be further enhanced to accommodate complex compliance scenarios with many compliance assertions as zkps e.g. using proof aggregation and proof recursion?</li>
<li>How can the framework be extended to support a broader range of compliance requirements beyond KYC/AML e.g. incorporated DAOs, Power-of-Attorney?</li>
<li>What are the potential governance challenges associated with managing and updating compliance policies within the framework?</li>
<li>How can the framework’s transparency and accountability features be leveraged to further enhance DeFi e.g. custom hooks?</li>
<li>How can the framework be adapted to different regulatory environments and jurisdictions?</li>
<li>What are the economic implications of implementing this framework for DeFi users and protocols?</li>
</ul>
<p>Given that part of the framework already exists, this post is to stimulate further discussion on the framework itself, and its suggested open research questions.</p>
<p>Looking forward to the feedback from the Ethereum research community.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/resolving-the-dichotomy-defi-compliance-under-zero-knowledge/20413">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 11 Sep 2024 21:51:53 +0000</pubDate>
</item>
<item>
<title>Lookup argument and its tweaks</title>
<link>https://ethresear.ch/t/lookup-argument-and-its-tweaks/20409</link>
<guid>https://ethresear.ch/t/lookup-argument-and-its-tweaks/20409</guid>
<content:encoded><![CDATA[
<div> 关键词：Placeholder Proof系统、Aztec研究、Plookup技术、Join-and-Split算法、Selector列

总结:
本文讨论了构建名为Placeholder的证明系统的过程，该系统用于验证等式是否为nil。系统的核心在于一种基于查找的论证方法，其灵感来源于Aztec研究人员的工作。该方法通过将原始表格与输入列和查找表列进行连接和分割来实现，允许验证者证明特定约束是否满足。连接和分割算法是Plookup技术的基础，它通过重新排列原始列与输入列和查找表列来创建一个大矢量，然后将此矢量分割回原始大小的部分。

为了适应更复杂的电路设计需求，作者对Plookup技术进行了改进，包括支持多个查找表和任意行/列约束。他们修改了连接和分割算法以支持多个输入列，这使得能够高效地处理大量查找表，即使这些表的大小超过原始行的数量。此外，引入了选择器列的概念，允许设计者精确控制哪些行受约束以及哪些行用于存储查找表。

文章还指出，Plookup论文中描述的方法限制了查找表的总数不能超过原始行的数量总和。作者通过结合查找表标识符的使用与他们的选择器列构造方法，克服了这一限制，使得查找表可以更灵活地存储和使用，而无需受到论证方法的约束。这一改进使得查找论证成为一种通用且灵活的工具。

为了详细阐述这些修改的具体内容，读者可以访问相关页面获取更多信息。 <div>
<p>In building the Placeholder proof system for =nil; Foundation, we use a lookup argument based on the <a href="https://eprint.iacr.org/2020/315.pdf" rel="noopener nofollow ugc">Plookup paper</a> by Aztec researchers. We took Plookup technique as a starting point and then made some practical improvements for writing large PLONK circuits with a complex logic.</p>
<p>Lookup argument allows prover to prove that some table over prime field (hereafter assignment table) satisfies specific constraints: some cells computed from assignment table (lookup input) belong to list of values that is also computed from the assignment table (lookup table).</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49919-join-and-split-algorithm-1" name="p-49919-join-and-split-algorithm-1"></a>Join-and-split algorithm</h2>
<p>The core of Plookup techinque is a sorting algorithm. We call it <strong>join-and-split</strong> because it includes two steps:</p>
<ul>
<li><strong>join</strong> — lookup table columns are joined together with input columns into single large vector using special reordering algorithm.</li>
<li><strong>split</strong> — constructed vector is split again into original size parts.</li>
</ul>
<p>The case with the single lookup table and single input column is described in the Plookup paper in detail. But it wasn’t enough for our use-cases. We needed lots of efficiently packed lookup tables and lookup constraints applied to arbitrary rows and columns, and we didn’t want to repeat lookup argument for each <code>(input, table)</code> pair.</p>
<p>So, we modified join-and-split algorithm to be able to join more than two columns. It allows us to use multiple lookup constraints even if they are applied on same rows and use a large lookup table, even if its size is greater than the whole assignment table rows amount by appending columns to assignment table instead of rows. Balance between assignment table rows and columns amounts helps to find a perfect balance for the best prover performance and verification cost.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49919-selector-columns-2" name="p-49919-selector-columns-2"></a>Selector columns</h2>
<p>Original article contains technique to lookup tuples of values that are placed in the same or neighboring rows. It constructs linear combinations of columns with a random factor. Combining this approach with polynomial expressions usage for lookup tables and input columns both we achieved selector columns full support. Circuit designer now can manage which rows exactly are constrained and what rows are reserved for lookup tables storing.</p>
<p>Plookup paper also describes technique for multiple lookup tables support. They propose to associate each lookup table with its unique identifier and fill tag column to mark what rows contains lookup tables with which identifier. Tag column for input helps to mark what constraints are applied to marked row. Tag columns should be a part of the random linear combination constructed for the lookup table and input columns respectively. This approach is obviously limited. Sum of lookup tables sizes should be less than the whole table rows amount.</p>
<p>We combined lookup table identifier usage with our selector columns construction and algorithms for large lookup tables. These modifications allow lookup tables to be stored and used without regard to lookup argument restrictions, but according to the best circuit design. It made our lookup argument into a universal and flexible tool.</p>
<p>Detailed description of our modifications can be found on our <a href="https://hackmd.io/@nil-research/rkjJFAtiC" rel="noopener nofollow ugc">HackMD</a> page. Feel free to share your comments!</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/lookup-argument-and-its-tweaks/20409">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 11 Sep 2024 15:14:26 +0000</pubDate>
</item>
<item>
<title>The Shape of Issuance Curves to Come</title>
<link>https://ethresear.ch/t/the-shape-of-issuance-curves-to-come/20405</link>
<guid>https://ethresear.ch/t/the-shape-of-issuance-curves-to-come/20405</guid>
<content:encoded><![CDATA[
<div> 关键词：有效收益率、真实收益率、发行曲线、中心化风险、发行削减

总结：

本文探讨了以太坊网络中发行曲线形状对验证者集分散化的影响。首先引入了有效收益率的概念，即考虑了新发行导致的稀释后的实际收益。接着定义了真实收益率，即在所有成本（如电费、网络费、税费等）后的实际收益。

文章指出，随着质押率接近100%，新发行会导致非质押持有者的有效收益率降低，而质押者的有效收益率则趋向于零。这可能导致中心化趋势，因为高质押率下，小规模、不相关质押者可能会因低真实收益率而退出。

为了减少中心化风险，文章提出了发行曲线应具备的几个属性：鼓励最低限度的质押量以确保网络安全；增加对非相关节点的激励，以保持验证者集的分散性；设定一个质押率阈值，使持有和质押的收益相等，以防止过度质押；设定一个质押率阈值，使得不同质押类型（如个人质押者、大型运营商）达到负真实收益率的时间相近，避免单一类型的验证者主导网络。

文章还展示了通过引入负发行削减机制（即发行减量），可以实现上述目标，同时保护网络免受过度质押的风险。此外，通过加入非相关激励机制，可以进一步引导质押者分散其质押资源，增强网络的安全性和抗审查性。

最后，文章强调了关注发行曲线或收益曲线的特性而非其具体数学形式的重要性，关键在于提供适当的经济激励，以维持理想的质押率水平和分散的验证者集。 <div>
<p>In this post we will analyze the consequences that the shape of the issuance curve has for the decentralization of the validator set.</p>
<p>The course of action is the following:</p>
<p>First, we will introduce the concept of effective yield as the yield observed after taking into account the dilution generated by issuance.</p>
<p>Second, we will introduce the concept of real yield as the effective yield that a validator obtains post expenses (OpEx, CapEx, taxes…).</p>
<p>Armed with these definitions we will be able to make some observations about how the shape of the issuance curve can result in centralization forces, as the real yield observed can push out small uncorrelated stakers at high stake rates.</p>
<p>Then, we will propose a number of properties we would expect the issuance curve to satisfy to minimize these centralization forces. And explore some alternative issuance curves that could deal with the aforementioned issues.</p>
<p>Finally, some heuristic arguments on how to fix a specific choice of issuance and yield curves.</p>
<p>Source Code for all plots can be found here: <a class="inline-onebox" href="https://github.com/pa7x1/ethereum-issuance" rel="noopener nofollow ugc">GitHub - pa7x1/ethereum-issuance</a></p>
<h2><a class="anchor" href="https://ethresear.ch#p-49913-effective-yield-1" name="p-49913-effective-yield-1"></a>Effective Yield</h2>
<p>By effective yield we mean the yield observed by an Ethereum holder after taking into account circulating supply changes. For instance, if everyone were to be a staker, the yield observed would be <em>effectively</em> 0%. As the new issuance is split evenly among all participants, the ownership of the circulating supply experienced by each staker would not change. Pre-taxes and other associated costs this situation resembles more a token re-denomination or a fractional stock split. So we would expect the effective yield to progressively reach 0% as stake rates grow to 100%.</p>
<p>On the other hand, non-staking holders are being diluted by the newly minted issuance. This causes holders to experience a negative effective yield due to issuance. We would expect this effect to be more and more acute as stake rates grow closer and closer to 100%.</p>
<p>These ideas can be put very simply in math terms.</p>
<p>Let’s call <span class="math">s</span> the amount of ETH held by stakers, <span class="math">h</span> the amount of ETH held by non-stakers (holders), and <span class="math">t</span> the total circulating supply. Then:</p>
<p><span class="math">s + h = t</span></p>
<p>After staking for certain period of time, we will reach a new situation <span class="math">s' + h' = t'</span>. Where <span class="math">s'</span> and <span class="math">t'</span> have been inflated by the new issuance <span class="math">i</span> are obviously related to the nominal staking yield <span class="math">y_s</span>:</p>
<p><span class="math">s' = s + i = s \cdot y_s</span></p>
<p><span class="math">h' = h</span></p>
<p><span class="math">t' = t + i = t + s \cdot (y_s - 1)</span></p>
<p>Now, let’s introduce the normalized quantities <span class="math">s_n</span> and <span class="math">h_n</span>. They simply represent the proportion of total circulating supply that each subset represents:</p>
<p><span class="math">s_n \equiv \frac{s}{t}</span></p>
<p><span class="math">h_n \equiv \frac{h}{t}</span></p>
<p>We can do the same for <span class="math">s'_n</span> and <span class="math">h'_n</span>:</p>
<p><span class="math">s'_n \equiv \frac{s'}{t'} = \frac{sy_s}{s(y_s - 1) +t}</span></p>
<p><span class="math">h'_n \equiv \frac{h'}{t'} = \frac{t-s}{s(y_s - 1) + t}</span></p>
<p>With these definitions we can now introduce the effective yield as the change in the proportion of the total circulating supply observed by each subset.</p>
<p><span class="math">y_s^{eff} \equiv \frac{s'_n}{s_n} = \frac{y_s}{\frac{s}{t}(y_s-1) + 1}</span></p>
<p><span class="math">y_h^{eff} \equiv \frac{h'_n}{h_n} = \frac{1}{\frac{s}{t}(y_s-1) + 1} </span></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/f/6f2aac4b14043e89f4a7b2722d58ad5d288fc5c4.png" title="effective_yield"><img alt="effective_yield" height="414" src="https://ethresear.ch/uploads/default/optimized/3X/6/f/6f2aac4b14043e89f4a7b2722d58ad5d288fc5c4_2_690x414.png" width="690" /></a></div><p></p>
<h2><a class="anchor" href="https://ethresear.ch#p-49913-net-yield-2" name="p-49913-net-yield-2"></a>Net Yield</h2>
<p>Staking has associated costs. A staker must acquire a consumer-grade PC, it must pay some amount (albeit small) for electricity, it must have a high-speed internet connection. And they also must put their own labor and time to maintain the system operational and secure, or must pay someone to do that job for them. Stakers also observe other forms of costs that eat away from the nominal yield they observe, e.g. taxes. We would like to model this net yield observed after all forms of costs, because it can give us valuable information on how different stakers are impacted by changes in the nominal stake yield.</p>
<p>To model this we will introduce two types of costs; costs that scale with the nominal yield (e.g. taxes or fees charged by an LST would fit under this umbrella), and costs that do not (i.e. HW, electricity, internet, labor…).</p>
<p>With our definitions, after staking for a reference period stakers would have earned <span class="math">s' = y_s s = s + s(y_s - 1)</span></p>
<p>But if we introduce costs that eat away from the nominal yield (let’s call them <span class="math">k</span>), and costs that eat away from the principal (let’s call them <span class="math">c</span>). We arrive to the following formula for the net stake:</p>
<p><span class="math">s' = s(1-c) + s(y_s - 1) - \max(0, sk(y_s - 1))</span></p>
<p>NOTE: The max simply prevents that a cost that scales with yield becomes a profit if yield goes negative. For instance, if yield goes negative it’s unlikely that an LST will pay the LST holders 10%. Or if yield goes negative you may not be able to recoup in taxes as if it were negative income. In those cases we set it to 0. This will become useful later on when we explore issuance curves with negative issuance regimes.</p>
<p>This represents the net stake our stakers observe after all forms of costs have been taken into account. To be noted that this formula can be easily modified to take into account other types of effects like validator effectiveness (acts as multiplicative factor on the terms <span class="math">(y_s - 1)</span>) or correlation/anti-correlation incentives (which alter <span class="math">y_s</span>).</p>
<p>To fix ideas, let’s estimate the net yield observed by 3 different types of stakers. A home staker, an LST holder, and an institutional large-scale operator. The values proposed are only orientative and should be tuned to best reflect the realities of each stakeholder.</p>
<p>A home staker will have to pay for a PC that they amortize over 5 years and costs around 1000 USD, so 200 USD/year. Pay for Internet, 50 USD per month, for around 600 USD/year. Something extra for electricity, less than 100 USD/year for a typical NUC. Let’s assume they are a hobbyist and decide to do this with their spare time, valuing their time at 0 USD/year. This would mean that his staking operation has a cost of around 1000 USD/year for them. If they have 32 ETH, with current ETH prices we can round that at ~100K USD. This would mean that for this staker, <span class="math">c = \frac{1}{1000}</span>. As their costs represent around 1 over 1000 their stake value.</p>
<p>Now for the costs that scale with the yield. They will have to pay taxes, these are highly dependent on their tax jurisdiction, but may vary between 20% and 50% in most developed countries. Let’s pick 35% as an intermediate value. In that case, their stake after costs looks like:</p>
<p><span class="math">s' = s\left(1-\frac{1}{1000}\right) + s(1-0.35)(y_s - 1)</span></p>
<p>We can do the same exercise for a staker using an LST. In this case, <span class="math">c=0</span> and <span class="math">k</span> is composed of staking fees (10-15%) and taxes (20-50%) which depend on the tax treatment. Rebasing tokens have the advantage of postponing the realization of capital gains. If we assume a 5 year holding period, equivalent to the amortization time we assumed for solo staking, it could look something like this:</p>
<ul>
<li>Fixed costs: 0</li>
<li>Staking fees: 10%</li>
<li>Capital gains tax: 20%</li>
<li>Holding period: 5 years</li>
</ul>
<p><span class="math">s' = s(1-0) + s(1-0.14)(y_s - 1)</span></p>
<p>Finally, for a large scale operator. They have higher fixed costs, they will have to pay for labor, etc… But also will run much higher amount of validators. In that case, c can get much smaller as it’s a proportion of s. Perhaps 1 or 2 orders of magnitude smaller. And taxes will be typical of corporate tax rates (20-30%).</p>
<p><span class="math">s' = s\left(1-\frac{1}{10000}\right) + s(1-0.25)(y_s - 1)</span></p>
<h2><a class="anchor" href="https://ethresear.ch#p-49913-net-effective-yield-aka-real-yield-3" name="p-49913-net-effective-yield-aka-real-yield-3"></a>Net Effective Yield (a.k.a Real Yield)</h2>
<p>Finally, we can blend the two concepts together to understand what’s the real yield a staker or holder obtains net of all forms of costs and after supply changes dilution. I would suggest calling this <em>net effective yield</em> as the <em>real yield</em>, because well, that’s the yield you are <em>really</em> getting.</p>
<p><span class="math">y_s^{real} = \frac{(1-c) + (y_s - 1) - \max(0,k(y_s - 1))}{\frac{s}{t}(y_s-1)+1}</span></p>
<p><span class="math">y_h^{real} = y_h^{eff} = \frac{1}{\frac{s}{t}(y_s-1) + 1}</span></p>
<p>In the second equation we are simply stating the fact that there is no cost to holding, so the real yield (after costs) of holding is the same as the effective yield of holding.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49913-the-issuance-curve-and-centralization-4" name="p-49913-the-issuance-curve-and-centralization-4"></a>The Issuance Curve and Centralization</h2>
<p>Up to here all the equations presented are agnostic of Ethereum’s specificities and in fact are equally applicable to any other scenario where stakeholders observe a yield but that yield is coming from new issuance.</p>
<p>To bring this analysis back to Ethereum-land it suffices to substitute <span class="math">y_s</span> by Ethereum’s issuance yield as a function of the total amount staked <span class="math">s</span>. And substitute <span class="math">t</span> by the total circulating supply of ETH.</p>
<p><span class="math">t \approx 120\cdot 10^6 \quad \text{ETH}</span></p>
<p><span class="math">i(s) = 2.6 \cdot 64 \cdot \sqrt{s} \quad \text{ETH}\cdot\text{year}^{-1}</span></p>
<p><span class="math">y_{s}(s) = 1 + \frac{2.6 \cdot 64}{\sqrt{s}} \quad \text{year}^{-1}</span></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/9/d91eba03c4aa2412aea2ad66fade8b7328651aa0.png" title="ethereum_issuance_plot"><img alt="ethereum_issuance_plot" height="414" src="https://ethresear.ch/uploads/default/optimized/3X/d/9/d91eba03c4aa2412aea2ad66fade8b7328651aa0_2_690x414.png" width="690" /></a></div><p></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/7/c7cd4df4c27030c8209dfb921fb4deae6cd9218a.png" title="ethereum_nominal_yield_plot"><img alt="ethereum_nominal_yield_plot" height="414" src="https://ethresear.ch/uploads/default/optimized/3X/c/7/c7cd4df4c27030c8209dfb921fb4deae6cd9218a_2_690x414.png" width="690" /></a></div><p></p>
<p>We can plot the real yield for the 4 different types of ETH stakeholders we introduced above, as a way to visualize the possible centralization forces that arise due to economies scale, and exogenous factors like taxes.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/0/7/0767a5fc8d2f7667fc9050a1c7d3a9a173b72ce6.png" title="ethereum_real_yield_plot"><img alt="ethereum_real_yield_plot" height="414" src="https://ethresear.ch/uploads/default/optimized/3X/0/7/0767a5fc8d2f7667fc9050a1c7d3a9a173b72ce6_2_690x414.png" width="690" /></a></div><p></p>
<p>We can make the following observations, from which we will derive some consequences.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49913-observations-5" name="p-49913-observations-5"></a>Observations</h3>
<ul>
<li>
<p><strong>Observation 0</strong>: The economic choice to participate as a solo staker, LST holder, ETH holder or any other option is made on the gap between the real yields observed and the risks (liquidity, slashing, operational, regulatory, smart contract…) of each option. Typically higher risks demand a higher premium.</p>
</li>
<li>
<p><strong>Observation 1</strong>: Holding always has a lower real yield than staking, at least for the assumptions taken above for costs. But the gap shrinks with high stake rates.</p>
</li>
<li>
<p><strong>Observation 2</strong>: Different stakers cross the 0% real yield at different stake rate. Around 70M ETH staked solo validators start to earn negative real yield. At around 90M ETH institutional stakers start to earn negative real yield. At around 100M LST holders start to earn negative real yield.</p>
</li>
<li>
<p><strong>Observation 3</strong>: When every staker and every ETH holder is becoming diluted (negative real yield), staking is a net cost for everyone.</p>
</li>
<li>
<p><strong>Observation 4</strong>: There is quite a large gap between the stake levels where different stakers cross the 0% real yield.</p>
</li>
<li>
<p><strong>Observation 5</strong>: Low nominal yields affect disproportionately home stakers over large operators. From the cost structure formula above we can see that as long as nominal yields are positive, the only term that can make the real yield negative is <span class="math">c</span>. This term is affected by economies of scale, and small operators will suffer larger <span class="math">c</span>.</p>
</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-49913-implications-6" name="p-49913-implications-6"></a>Implications</h3>
<p><strong>Observation 0</strong> and <strong>Observation 1</strong> imply that as the gap between real yields becomes sufficiently small, participating in the network as some of those subsets may become economically irrational. For example, solo staking may be economically irrational given the operational risks, liquidity risks, slashing risks if the yield premium becomes sufficiently small vs holding. In that case solo stakers may become holders or switch to other forms of staking (e.g. LSTs) where the premium still satisfies the risk.</p>
<p>Together with <strong>Observation 2</strong> and <strong>Observation 4</strong> implies that as stake rates become higher and higher the chain is at risk of becoming more centralized, as solo stakers (which are the most uncorrelated staking set) must continue staking when it may be economically irrational to do so. Given the above assumptions LSTs will always observe at least 1% real yield higher than holding even at extreme stake rates (~100%), this may mean that there is always an incentive to hold an LST instead of ETH. Furthermore, when solo stakers cross to negative real yield but other stakers do not, other stakers are slowly but steadily gaining greater weight.</p>
<p>From <strong>Observation 3</strong> we know that the very high stake rate regime, where everyone is observing a negative real yield, is costly for everyone. Everyone observes dilution. The money is going to the costs that were included in the real yield calculation (tax, ISPs, HW, electricity, labor…).</p>
<p><strong>Observation 5</strong> implies that nominal yield reductions need to be applied with care and certainly not in isolation without introducing uncorrelation incentives at the same time. As they risk penalizing home solo stakers disproportionately.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49913-recommendations-7" name="p-49913-recommendations-7"></a>Recommendations</h3>
<p>Given the above analysis, we can put forward a few recommended properties the yield curve (respectively the issuance curve) should have. The idea of establishing these properties is that we should be able to discuss them individually and agree or disagree on their desirability. Once agreed they constrain the set of functions we should consider. At a minimum, it will make the discussion about issuance changes more structured.</p>
<ul>
<li>
<p><strong>Property 0</strong>: Although this property is already satisfied with the current issuance curve, it is worth stating explicitly. The protocol should incentivize some minimum amount of stake, to ensure the network is secure and the cost to attack much larger than the potential economic reward of doing so. This is achieved by defining a yield curve that ramps up the nominal yield as the total proportion of stake (<span class="math">s_n</span>) is reduced.</p>
</li>
<li>
<p><strong>Property 1</strong>: The yield curve should contain uncorrelation incentives such that stakers are incentivized to spin-up uncorrelated nodes and to stake independently, instead of joining large-scale operators. From a protocol perspective the marginal value gained from another ETH staked through a large staking operation is much smaller than if that same ETH does so through an uncorrelated node. The protocol should reward uncorrelation as that’s what allows the network to achieve the extreme levels of censorship resistance, liveness/availability and credible neutrality the protocol expects to obtain from its validator set. The economic incentives must be aligned with the expected outcomes, therefore the yield curve must contain uncorrelation incentives.</p>
</li>
<li>
<p><strong>Property 2</strong>: The issuance curve (resp. yield curve) should have a regime where holding is strictly more economically beneficial than staking, at sufficiently high stake rates. This means that the real yield of holding is greater than the real yield of staking if the stake rate is sufficiently high. As explained above it’s the real yield gap the defining characteristic that establishes the economically rational choice to join one subset or another. If the holding real yield can be greater than the staking real yield at sufficiently high stake rates there is an economic incentive to hold instead of continue staking. To be noted, up to here we are not making an argument at what stake rate this should be set. It suffices to agree that 99.9% stake rate is unhealthy for the protocol (it’s a cost for everyone, LSTs will displace ETH as pristine collateral, etc…). If that’s the case, then we can prevent this outcome by setting the holding real yield to be higher than staking at that level. Unhealthy levels are likely found at much lower values of stake rate.</p>
</li>
<li>
<p><strong>Property 3</strong>: To prevent centralization forces, the stake rates at which uncorrelated validators vs correlated validators cross to negative real yield should be small, as small as possible. A large gap between the thresholds to negative real yields of uncorrelated (e.g. home stakers) and correlated sets (e.g. large operators) creates a regime where the validator set can become more and more centralized. To make the case more clear, if uncorrelated validators reach 0 real yield at 30M ETH staked, while holding an LST composed of large operators (e.g. cbETH, wstETH) does so at 100M ETH. The regime where the stake ranges between 30M and 100M is such that solo stakers will tend to disappear, either quickly (they stop staking) or slowly (they become more and more diluted), the outcome in either case is a more centralized validator set.</p>
</li>
<li>
<p><strong>Property 4</strong>: The yield curve should taper down relatively quick to enter the regime of negative real yields. From <strong>Property 2</strong> and <strong>Property 3</strong> we know we should build-in a regime where the real yield from issuance goes negative, but we want this regime to occur approximately at the same stake rate for the different types of stakers, to prevent centralization forces. <strong>Observation 5</strong> implies that if the slope of this nominal yield reduction is slow, stakers with different cost structures will be pushed out at much different stake rates. Hence, we need to make this yield reduction quick.</p>
</li>
<li>
<p><strong>Property 5</strong>: The issuance yield curve should be continuous. It’s tempting to play with discontinuous yield curves, but yield is the main incentive to regulate the total stake of the network. We would like that the changes induced to the total stake <span class="math">s</span> are continuous, therefore the economic incentive should be a continuous function.</p>
</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-49913-exploring-other-issuance-curves-8" name="p-49913-exploring-other-issuance-curves-8"></a>Exploring Other Issuance Curves</h2>
<p>The desired properties can be summarized very succinctly:</p>
<ul>
<li>The yield curve should be continuous.</li>
<li>The yield curve should go up as stake rate goes to 0.</li>
<li>The yield curve should go to 0 as the stake rate goes up, crossing 0 at some point that bounds from above the desired stake rate equilibrium.</li>
<li>The yield curve should have uncorrelation incentives such that spinning up uncorrelated validators is rewarded and incentivized over correlated validators.</li>
<li>The real yield curves of correlated and uncorrelated stakers should become negative relatively close to each other.</li>
</ul>
<p>A very simple solution to meet the above is to introduce a negative term to Ethereum’s issuance yield and uncorrelation incentives.</p>
<p>The negative term should grow faster than the issuance yield as the stake grows, so that it eventually overcompensates issuance and makes the yield go negative quickly at sufficiently high stake rates. This negative term can be thought of as a stake burn, and should be applied on a slot or epoch basis such that it’s unavoidable (thanks to A. Elowsson for this observation).</p>
<p>Uncorrelation incentives are being explored in other posts. We will simply leave here the recommendation of adopting them as part of any issuance tweaks. Read further: <a href="https://ethresear.ch/t/diseconomies-of-scale-anti-correlation-penalties-eip-7716/20114/4">Anti-Correlation Penalties by Wahrstatter et al.</a></p>
<h3><a class="anchor" href="https://ethresear.ch#p-49913-ethereums-issuance-with-stake-burn-9" name="p-49913-ethereums-issuance-with-stake-burn-9"></a>Ethereum’s Issuance with Stake Burn</h3>
<p>The following is an example of how such a negative term can be introduced.</p>
<p><span class="math">i(s) = 2.6 \cdot 64 \cdot \sqrt{s} - 2.6 \cdot \frac{s \ln s}{2048} \quad \text{ETH} \cdot \text{year}^{-1}</span></p>
<p><span class="math">y_{s}(s) = 1 + \frac{2.6 \cdot 64}{\sqrt{s}} - \frac{2.6 \ln s}{2048} \quad \text{year}^{-1}</span></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/7/5734f552fcad604fd70816d71a7b1af07c067f7c.png" title="ethereum_issuance_with_burn_plot"><img alt="ethereum_issuance_with_burn_plot" height="414" src="https://ethresear.ch/uploads/default/optimized/3X/5/7/5734f552fcad604fd70816d71a7b1af07c067f7c_2_690x414.png" width="690" /></a></div><p></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/3/e345685acc94592ba49432571ae51c9c72286588.png" title="ethereum_nominal_yield_with_burn_plot"><img alt="ethereum_nominal_yield_with_burn_plot" height="414" src="https://ethresear.ch/uploads/default/optimized/3X/e/3/e345685acc94592ba49432571ae51c9c72286588_2_690x414.png" width="690" /></a></div><p></p>
<p>The negative stake burn term eventually dominates issuance and can make it go negative. There is complete freedom deciding where this threshold occurs, by simply tweaking the constant pre-factors. In this particular case, the parameters have been chosen so they are rounded in powers of 2 and so that the negative issuance regime roughly happens around 50% stake rate.</p>
<p>This negative issuance regime induces a positive effective yield on holders, which provides the protocol with an economic incentive to limit the stake rate. As the real yield will eventually be greater holding ETH than staking. It also serves to protect the network from overloading its consensus layer, as it provides the protocol with a mechanism to charge exogenous sources of yield that occur on top of it. If priority fees, MEV, or restaking provide additional yield that would push stake rates above the desired limit, the protocol would start charging those extra sources of yield by making issuance go negative. Hence redistributing exogenous yield onto ETH holders.</p>
<p>To understand better the impact that this stake burn has on the different stakeholders we can plot the real yield curves.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/6/b664771684f7f8abc3d591b6b6acdaae46a63cbe.png" title="ethereum_real_yield_with_burn_plot"><img alt="ethereum_real_yield_with_burn_plot" height="414" src="https://ethresear.ch/uploads/default/optimized/3X/b/6/b664771684f7f8abc3d591b6b6acdaae46a63cbe_2_690x414.png" width="690" /></a></div><p></p>
<p>We can see how the introduction of a negative issuance yield regime has helped achieve most of the properties we desired to obtain. Particularly, we can notice the stake rates at which different stakeholders reach 0 real yield have compressed and are much closer to each other. And we can appreciate how when stake rates get close to 50% (given the choice of parameters) holders start to observe a positive real yield which disincentivizes additional staking. Holding real yields can become quite large so even large exogenous sources of yield can be overcome.</p>
<p>Given that we haven’t touched the positive issuance term, this results in a large reduction of the staking yield. We can increase the yield trivially while respecting the same yield curve shape. Here the same curve with larger yield:</p>
<p><span class="math">i(s) = 2.6 \cdot 128 \cdot \sqrt{s} - 2.6 \cdot \frac{s \ln s}{1024} \quad \text{ETH} \cdot \text{year}^{-1}</span></p>
<p><span class="math">y_{s}(s) = 1 + \frac{2.6 \cdot 128}{\sqrt{s}} - \frac{2.6 \ln s}{1024} \quad \text{year}^{-1}</span></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/b/3b1b509db61e4ede02779c912e6abaabd9f83019.png" title="ethereum_issuance_with_burn_plot_2"><img alt="ethereum_issuance_with_burn_plot_2" height="414" src="https://ethresear.ch/uploads/default/optimized/3X/3/b/3b1b509db61e4ede02779c912e6abaabd9f83019_2_690x414.png" width="690" /></a></div><p></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/2/b22b3240440e4855aff51d30c28097545bf16400.png" title="ethereum_real_yield_with_burn_plot_2"><img alt="ethereum_real_yield_with_burn_plot_2" height="414" src="https://ethresear.ch/uploads/default/optimized/3X/b/2/b22b3240440e4855aff51d30c28097545bf16400_2_690x414.png" width="690" /></a></div><p></p>
<p>This shows the target yield that is observed at a specific stake rate is a separate consideration to the curve shape discussion. So if you dislike this particular example because of the resulting yield at current stake rates. Fear not, that has an easy fix.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49913-adding-uncorrelation-incentives-to-the-mix-10" name="p-49913-adding-uncorrelation-incentives-to-the-mix-10"></a>Adding Uncorrelation Incentives to the Mix</h3>
<p>We will not cover the specifics of how uncorrelation incentives should be introduced nor how they should be sized, but we will illustrate how the introduction of a correlation penalty can help align the economic incentives with the network interest of maintaining an uncorrelated validator set.</p>
<p>To do so we will simulate what would happen to the real yields observed by the following stakeholders:</p>
<ul>
<li>Home Validator (Very Uncorrelated): -0.0% subtracted to the nominal yield through correlation penalties</li>
<li>LST Holder through a decentralized protocol (Quite Uncorrelated): -0.2% subtracted to the nominal yield through correlation penalties</li>
<li>LST Holder through staking through large operators (Quite Correlated): -0.4% subtracted to the nominal yield through correlation penalties</li>
<li>Large Institutional Operator (Very Correlated): -0.6% subtracted to the nominal yield through correlation penalties</li>
</ul>
<p>The following figure zooms in to the area where negative real yields are achieved:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/5/c5272375fa151b1e666bb07feb5f5a9112fbc457.png" title="ethereum_real_yield_with_burn_uncorrelation_plot"><img alt="ethereum_real_yield_with_burn_uncorrelation_plot" height="377" src="https://ethresear.ch/uploads/default/optimized/3X/c/5/c5272375fa151b1e666bb07feb5f5a9112fbc457_2_690x377.png" width="690" /></a></div><p></p>
<p>Important Note: The above values for correlation penalties are not based on any estimation or study. They have been chosen arbitrarily to showcase that the inclusion of uncorrelation incentives in the issuance curve can be used to disincentivize staking through large correlated operators. We refer the analysis of the right incentives to other papers.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49913-fixing-the-issuance-yield-curve-11" name="p-49913-fixing-the-issuance-yield-curve-11"></a>Fixing the Issuance Yield Curve</h2>
<p>Up until now the focus has been on the shape of the yield curve (respectively the issuance curve) but very little has been said about the specific yield we should target at different stake rates. As illustrated above, by simply applying a multiplicative factor we can keep the same curve shape but make yields be higher or lower as wished.</p>
<p>In this section we will provide some heuristic properties to address this problem and be able to specify the prefactors that allow us to define a concrete yield curve.</p>
<p>These heuristic properties are orientative. There is no hard science behind them, just some soft arguments that provide reasonable justification for these choices.</p>
<p><strong>Heuristic 0</strong>: The nominal issuance yield should become negative at 50% stake rate or lower. Higher stake rates start to become problematic, above those levels the majority of circulating supply is staking. In case of supermajority bug the majority of ETH holders could be incentivized to break the consensus rules. The negative yield regime can be seen as a protection mechanism from the protocol to prevent this type of situations from happening, it sets an economic incentive to align the social layer with the protocol interests.</p>
<p><strong>Heuristic 1</strong>: Target 3% yield at 25% stake rate. When PoS was released there was no idea what would be the expected staking yield the market would consider appetizing. Would 5% be enough? Or 3%?</p>
<p>Now we have data points, current staking yield is 3% as measured by <a href="https://beaconcha.in" rel="noopener nofollow ugc">https://beaconcha.in</a> (issuance, MEV, and priority fees included). So we know the market certainly has appetite for ETH yield at 3%. There is also some soft-arguments by V. Buterin, J. Drake et al. that 25% stake rate should provide enough security.</p>
<p>And finally, the current issuance curve happens to provide 3% yield at 25% stake rate. So by fixing the new curve to meet that same yield at 25% we anchor the same yield (and issuance) at the target rate. But any extra amount of stake will be met with a reduction in yield and issuance that makes it go to 0 before hitting 50%.</p>
<p>As current stake rate is a tad over 25% the proposed change to the issuance curve would imply a bit of issuance reduction, nothing very significant. But most importantly it avoids the ever growing issuance increase as stake rates become higher.</p>
<p>In conjunction with well designed uncorrelation incentives it could help the protocol ensure it does not overpay for security, stake rates are self-limiting, and the validator set very uncorrelated.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49913-final-words-12" name="p-49913-final-words-12"></a>Final Words</h2>
<p>The analytic form of the yield curve or the issuance curve matter much less than we may think. It might be tempting to spend time tinkering with its concrete analytic form but for all it matters it could be equally defined with a piece-wise continuous function.</p>
<p>Its purpose is to provide an economic incentive to get stake rates where the protocol needs them to be (not too high, not too low) and maintaining a large uncorrelated validator set.</p>
<p>This post is an invitation to steer the discussion towards said properties instead of getting lost with the fine details. If we nail down the properties we will constrain the solution space enough so that almost any function we choose will do the job.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/the-shape-of-issuance-curves-to-come/20405">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 10 Sep 2024 21:11:48 +0000</pubDate>
</item>
<item>
<title>Introducing CCTP Express: a faster and cheaper way to use CCTP</title>
<link>https://ethresear.ch/t/introducing-cctp-express-a-faster-and-cheaper-way-to-use-cctp/20396</link>
<guid>https://ethresear.ch/t/introducing-cctp-express-a-faster-and-cheaper-way-to-use-cctp/20396</guid>
<content:encoded><![CDATA[
<div> 关键词：CCTP Express、USDC、Cross-Chain Transfer Protocol（CCTP）、off-chain attestation、trustless design

总结：
CCTP Express 是一种旨在增强 CCTP（Cross-Chain Transfer Protocol）并提供更快、更低成本体验的跨链桥接系统。它基于意图进行桥接，通过采用“填空支付优先”机制加速交易过程。CCTP Express 设计为信任无碍的系统，允许任何人都可以作为填空者或数据守护人参与其中，无需许可。为了降低填空者面临的风险，CCTP Express 引入了保险费，该费用根据用户定义的发起截止时间变化。通过优化交易成本和数据传输，CCTP Express 降低了用户和填空者的交易费用，同时通过将偿还和重新平衡交易打包来节约成本，并利用哈希形式传输跨链消息以减少数据大小。

CCTP Express 采用中心化与分散化的架构，由请求报价机制、填空网络以及结算层组成，确保用户、填空者和 CCTP 的共赢。其设计遵循 ERC-7683 标准，强调行业标准的一致性，促进不同跨链意图系统之间的互操作性和资源共享。通过这种设计，CCTP Express 增强了用户体验，提高了服务效率，并促进了市场的竞争性。 <div>
<p>By <a href="https://twitter.com/wels_eth" rel="noopener nofollow ugc">Wel</a> and <a href="https://twitter.com/alau1218" rel="noopener nofollow ugc">Alan</a> on behalf of CCTP Express<br />
<em>For most recent information about CCTP Express, please visit <a href="https://twitter.com/cctpexpress" rel="noopener nofollow ugc">our X</a>.</em></p>
<h1><a class="anchor" href="https://ethresear.ch#p-49895-motivation-1" name="p-49895-motivation-1"></a>Motivation</h1>
<p>We recognize the vital role stablecoins play in the Web3 ecosystem, especially within DeFi. Among them, USDC stands out for its high transparency and regulatory compliance. Circle, the issuer of USDC, introduced the Cross-Chain Transfer Protocol (CCTP) to securely transfer USDC across chains using a native burn-and-mint mechanism.</p>
<p>CCTP is a game-changing tool that drives USDC adoption in the multichain world, allowing developers to create applications that offer secure, 1:1 USDC transfers across blockchains. This eliminates the added risks of using bridges.</p>
<p>However, CCTP has a key limitation: wait time. Its off-chain attestation service requires block confirmations on the source chain to ensure finality before minting USDC on the destination chain. This process can take anywhere from 20 seconds to 13 minutes, which is not ideal for users needing instant transfers. To address this, CCTP Express was designed to provide instant USDC bridging while leveraging CCTP. We position CCTP Express as a booster tool of CCTP, enabling users to benefit from faster and cheaper transactions.</p>
<p>We believe CCTP Express is an essential tool to achieve chain abstraction by providing an instant USDC bridging experience.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49895-tldr-2" name="p-49895-tldr-2"></a>TL;DR</h1>
<ul>
<li>CCTP Express is positioned as a booster tool to use CCTP, where users enjoys a faster and cheaper experience;</li>
<li>It is an intent-base bridging system built upon CCTP, instant USDC bridging is enabled by the “Filler-Pay-First” mechanism;</li>
<li>CCTP Express is a trustless design, allowing anyone to participate as a filler or datadaemon without permission;</li>
<li>To mitigate the reorg risk exposed to the fillers, CCTP Express introduces an insurance fee that varies based on the user-defined initiateDeadline.;</li>
<li>In order to lower the transaction costs, repayment and rebalancing transactions are bundled, cross-chain messages are transmitted as hashes to reduce data size.</li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#p-49895-primary-principles-3" name="p-49895-primary-principles-3"></a>Primary principles</h1>
<p><strong>1. CCTP Dependency</strong><br />
CCTP Express is specifically designed to enhance CCTP. All fund rebalancing must be done exclusively through CCTP to avoid exposure to potential risks associated with other bridges.</p>
<p><strong>2. Decentralization</strong><br />
The system must be trustless to ensure maximum protection for everyone’s assets. Players in the system, including Fillers and Datamaemon, are permissionless.</p>
<p><strong>3. Win-Win-Win</strong><br />
The design should benefit all stakeholders — users, fillers, and CCTP. Users gain a faster and more cost-effective experience, fillers receive satisfactory rewards while their funds are safeguarded, and CCTP grows stronger through the support of CCTP Express.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49895-key-concepts-4" name="p-49895-key-concepts-4"></a>Key concepts</h1>
<p>CCTP Express is an intent based cross-chain bridging system built upon CCTP. The key to speed up the transaction is the adoption of the “Filler-pay-first” mechanism.</p>
<p>When a user submits a bridging intent, fillers initiate an order on the origin chain, then immediately call a fillOrder on the destination chain and transfer funds to the user accordingly.</p>
<p>The system periodically validates the payments and repays to fillers in batches. Rebalancing across domains is done across CCTP if needed. This settlement process is out of the scene of the users, the repayments and rebalancing are bundled to save costs.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49895-dive-deeper-5" name="p-49895-dive-deeper-5"></a>Dive Deeper</h1>
<p>CCTP Express adopts a Hub-and-Spoke architecture, it can be broken down into a 3-layered system: a request for quote mechanism to obtain users’ bridging intent, enabling a filler network to claim and fill those orders, and lastly a settlement layer periodically repay fillers through CCTP and utilizing attestation service from Iris (Circle’s off-chain attestation service).</p>
<p>Our design adheres to ERC-7683, emphasizing the importance of aligning with industry standards. This ensures that cross-chain intent systems can interoperate and share infrastructure like order dissemination services and filler networks. By fostering this interoperability, we enhance the end-user experience by increasing competition for fulfilling user intents. Below is a diagram of the architecture of CCTP Express:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/0/3/037c645827e04b44e5ed2f79fedaddff4f92eab3.jpeg" title="Architecture of CCTP Express"><img alt="Architecture of CCTP Express" height="451" src="https://ethresear.ch/uploads/default/optimized/3X/0/3/037c645827e04b44e5ed2f79fedaddff4f92eab3_2_690x451.jpeg" width="690" /></a></div><p></p>
<p><strong>Order initiation</strong></p>
<ol>
<li>User signs an off-chain message defining the parameters of an order:</li>
</ol>
<pre><code class="lang-auto"> function deposit(
        bytes32 recipient,
        bytes32 inputToken,
        bytes32 outputToken,
        uint256 inputAmount,
        uint256 outputAmount,
        uint32 destinationDomainId,        
        bytes32 exclusiveFiller,
        uint32 exclusivityDeadline,
        uint32 initiateDeadline,
        uint32 fillDeadline,
        bytes calldata message
    ) external;
</code></pre>
<ol start="2">
<li>The order is disseminated to Fillers. The Filler calls <code>initiate</code> on the origin chain SpokePool. A <code>CrossChainOrder</code> will be created and the user’s funds are transferred to the SpokePool for escrow.</li>
<li>The SpokePool on origin chain submits a <code>Deposit</code> message to Circle’s off-chain attestation service, Iris, for attestation and subsequently a <code>DepositAttestation</code> will be generated.</li>
</ol>
<p><strong>Filler Network Fills Order</strong></p>
<ol start="4">
<li>
<p>Fillers call <code>fillOrder</code> on the destination SpokePool with their own assets which are then transferred to the user from the SpokePool.</p>
</li>
<li>
<p>The SpokePool on destination chain submits a <code>Fill</code> message to Iris and a <code>FillAttestation</code> will be generated.</p>
</li>
</ol>
<p><strong>Settlement</strong></p>
<ol start="6">
<li>
<p>A permissionless Datadaemon retrieves the <code>DepositAttestation</code> and <code>FillAttestation</code> and relays to the Hub Pool on the Settlement Chain.</p>
</li>
<li>
<p>Periodically, the Datadaemon calls <code>repayFunds</code> and <code>rebalanceFunds</code> at the Hub Pool, which would collect all the attestations and perform the following steps:</p>
</li>
</ol>
<ul>
<li>
<p>Iterate through a list of attestations, a valid filled order is supported by both <code>Deposit</code> and <code>Fill</code> attestation.</p>
</li>
<li>
<p>Determine the aggregate settlement sum from all valid fills for each filler.</p>
</li>
<li>
<p>If there is sufficient funds on SpokePool to repay filler, a <code>repayFunds</code> message in the form of merkle root hash is sent to Iris.</p>
</li>
<li>
<p>For the remaining outstanding payment, the Hub Pool will send a <code>rebalanceFunds</code> message in the form of merkle root hash to Iris, which indicates how much a SpokePool with surplus funds would send to another pool in deficit to fulfill the need for repayment.</p>
</li>
</ul>
<ol start="8">
<li>
<p>Once the <code>repayFunds</code> and <code>rebalanceFunds</code> messages get attested by Iris, they are sent to respective SpokePools. Datamaemon will call <code>repayFunds</code> and <code>rebalanceFunds</code> on SpokePools with merkle root hash and their respective transaction details. Accordingly, funds would be repaid to fillers and sent to other SpokePools to ensure sufficient funds for handling repayments.</p>
</li>
<li>
<p>Repay funds to fillers from the SpokePool on destination chain, and rebalance funds across SpokePools on different chains via CCTP.</p>
</li>
</ol>
<p><strong>Cctp Fill Settlement</strong></p>
<ol start="10">
<li>
<p>In case of an order initiated by Fillers not being filled, anyone can call <code>cctpFill</code> and mark the order status on destination chain SpokePool to <code>RequestCctpFill</code> and block any filler from filling it. At the same time, the SpokePool will emit a <code>CctpFill</code> message to Iris for attestation.</p>
</li>
<li>
<p>The <code>CctpFillAttestation</code> will be used to replace the <code>FillAttestation</code> mentioned in 5. and allow the user fund to be transferred via the CCTP route.</p>
</li>
</ol>
<h1><a class="anchor" href="https://ethresear.ch#p-49895-risk-and-solutions-6" name="p-49895-risk-and-solutions-6"></a>Risk and solutions</h1>
<p><strong>Reorg risk</strong><br />
The reorg risk is uniquely borne by fillers. If the filler fills the intent too fast without waiting for the finality on the source chain, the source chain may reorg and cause a loss to the filler since the intent has been filled on the destination chain and the filler would end up in empty hand.</p>
<p>The reorg risk is effectively mitigated by the <strong>Insurance Fee</strong>, which varies based on the <code>initiateDeadline</code> specified by the user. If the <code>initiateDeadline</code> is sufficiently long, the filler can reinitiate the <code>CrossChainOrder</code> on the origin chain in the event of a reorg, ensuring the user’s funds are transferred again. The insurance fee is calculated using below formula:</p>
<p><img alt="Formula of Insurance Fee" height="96" src="https://ethresear.ch/uploads/default/original/3X/8/f/8f2626d45b13f0ec864c696fa581f0af9f7491b6.png" width="244" /></p>
<p>Where:<br />
<em>f(t)</em> is the insurance fee which is a function varies with <em>t</em><br />
<em>V</em> is the trading volume, representing the maximum insurance fee<br />
<em>e</em> is the base of the natural logarithm<br />
<em>k</em> is a constant that control the descending rate of the fee<br />
<em>t</em> is the time between order creation time and the initiateDeadline<br />
<em>T</em> is the time required for finality on the origin chain</p>
<p>The insurance fee varies with the <code>initiateDeadline</code>- it decreases with the increment of time between the order creation time and the <code>initiateDeadline</code>:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/6/1626648dc74ed8b71e5d3e7f66c0b1d9731baba1.png" title=""><img alt="" height="250" src="https://ethresear.ch/uploads/default/optimized/3X/1/6/1626648dc74ed8b71e5d3e7f66c0b1d9731baba1_2_321x250.png" width="321" /></a></div><p></p>
<p>Since the insurance fee decreases significantly when the <code>initiateDeadline</code> is long (it drops to nearly zero if it is 2x of the time needed for finality on the origin chain), a normal user is likely to set a long initiateDeadline to avoid paying the fee, minimizing the reorg risk for the filler.</p>
<p><strong>High system costs</strong><br />
The complexity of the design apparently implies higher costs compared to bridging directly using CCTP. To align with our goal of providing a faster and cheaper way to use CCTP, we mitigate costs through two key strategies: <em><strong>transaction bundling</strong></em> and <em><strong>data compression</strong></em>.</p>
<p>Transactions bundling-</p>
<p>Datadaemon works periodically to call repayment and rebalancing on the hub pool. This interval is adjustable to make sure a sufficient number of transactions are processed in each batch.</p>
<p>In this architecture design, gas costs are primarily incurred in rebalancing via CCTP and fund transfers. By processing rebalancing in batches and handling repayments in aggregate sums to the fillers, these costs are distributed across multiple transactions, reducing the costs on any single transaction.</p>
<p>Data Compression-</p>
<p>Cross-chain messages are transmitted between spoke pools and the hub pool via Iris, Circle’s off-chain attestation service. To minimize data size and reduce gas costs, these messages are sent in the form of a hash.</p>
<p>For a detailed comparison of gas consumption between CCTP and CCTP Express, check out <a href="https://medium.com/@cctpexpress/cctp-express-is-cheaper-than-cctp-2c527e0afa62" rel="noopener nofollow ugc">this article</a>.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49895-faq-7" name="p-49895-faq-7"></a>FAQ</h1>
<p><strong>1. What does it mean to the end user?</strong><br />
When using CCTP Express’s front end or applications integrated with CCTP Express, users benefit from a significantly faster and cheaper way to bridge USDC across chains. By leveraging CCTP as the underlying asset bridge, the system enhances user experience while maintaining robust security.</p>
<p><strong>2. What are the possible use cases?</strong><br />
We believe CCTP Express is essential to achieve chain abstraction by providing an instant USDC bridging experience. Possible use cases included-</p>
<p><em>USDC-denominated dApps</em><br />
USDC is widely adopted in various dApps, e.g. dYdX and Polymarket. dApps can integrate CCTP Express SDK to offer their users instant transfer in and out from all CCTP supported chains without the usual waiting time.</p>
<p><em>Payment Network</em><br />
CCTP Express can offer instant settled transaction experience for users across chains, enabling them to pay their USDC for a coffee from any CCTP supported chain.</p>
<p><em>Money Lego</em><br />
Arbitragers and Solvers can utilize CCTP Express to be the backbone of their cross chain actions. It’s highly undesirable for arbitragers or solvers to wait for long in the high speed crypto world, CCTP Express can offer them superior speed without worrying about security as CCTP Express is using CCTP as the underlying bridge.</p>
<p><strong>3. With a similar idea of providing cross chain bridging powered by off chain agents, how is CCTP Express different from other intent-based bridges, say Across?</strong></p>
<p>The primary distinction between CCTP Express and Across are: positioning and settlement mechanism.</p>
<p><em>Positioning -</em></p>
<p>While both protocols are intent-based bridges powered by fillers/relayers, CCTP Express is positioned to be a booster tool to use CCTP.</p>
<p>Given this focus, CCTP Express is closely integrated with CCTP and evolves in tandem with it. For instance, if CCTP supports EURC, CCTP Express will promptly support it as well.</p>
<p>And this alignment also applies to the choice of picking which chain CCTP Express supports. CCTP Express aims to cover all EVM and non-EVM chains CCTP operates. And like<a href="https://developers.circle.com/stablecoins/docs/message-format#message-header" rel="noopener nofollow ugc"> CCTP</a>, CCTP Express adopts the bytes32 address format, instead of the 20 byte address used in EVM, to handle 32 byte addresses in many non-EVM chains.</p>
<p>In contrast, Across is limited to EVM chains only, as it has a<a href="https://docs.across.to/resources/new-chain-requests" rel="noopener nofollow ugc"> hard requirement</a> to support EVM- chains only.</p>
<p><em>Settlement mechanism -</em></p>
<p>In CCTP Express, the Hub Pool smart contract utilizes the Iris attestation service used in CCTP to relay and verify messages. Deposit and Filled messages from various Spoke Pools are sent to Iris for attestation and then collected in the Hub Pool, which processes repayments on-chain.</p>
<p>In contrast, Across uses canonical bridges to relay messages and utilizes<a href="https://docs.across.to/use-cases/settle-cross-chain-intents" rel="noopener nofollow ugc"> UMA</a> to optimistically verify fill events off-chain. Since UMA works off-chain, an interval is needed as a dispute window.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49895-discuss-with-us-8" name="p-49895-discuss-with-us-8"></a>Discuss with Us</h1>
<p>To shape a better product, we are keen to discuss with fillers and teams who need instant USDC bridging. If anyone is interested in CCTP Express, we have a public telegram group here to discuss about it: <a href="https://t.me/cctpexpress" rel="noopener nofollow ugc">Join Group Chat</a></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/introducing-cctp-express-a-faster-and-cheaper-way-to-use-cctp/20396">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 10 Sep 2024 00:14:12 +0000</pubDate>
</item>
<item>
<title>Fake GLV: You don't need an efficient endomorphism to implement GLV-like scalar multiplication in SNARK circuits</title>
<link>https://ethresear.ch/t/fake-glv-you-dont-need-an-efficient-endomorphism-to-implement-glv-like-scalar-multiplication-in-snark-circuits/20394</link>
<guid>https://ethresear.ch/t/fake-glv-you-dont-need-an-efficient-endomorphism-to-implement-glv-like-scalar-multiplication-in-snark-circuits/20394</guid>
<content:encoded><![CDATA[
<div> 关键词：GLV算法、非标准椭圆曲线、SNARK电路、P-256曲线、ECDSA验证优化

总结:

本文探讨了在没有有效共轭的情况下，如何在SNARK电路中实现类似GLV的标量乘法技术。GLV算法通常用于具有有效共轭的椭圆曲线，以加速标量乘法操作。然而，对于像P-256这样的曲线，由于缺乏有效共轭，传统的GLV方法不可用。文章提出了一种“假GLV”技巧，通过引入辅助参数u和v来实现优化。

1. **传统标量乘法**: 文章首先介绍了标准的标量乘法算法，如双倍加算法，以及在SNARK电路中的实现挑战，特别是对于条件分支的处理。

2. **GLV算法介绍**: 接着，文章解释了GLV算法的工作原理，特别强调了当椭圆曲线具有有效共轭时，如何利用该特性加速标量乘法计算。

3. **假GLV技巧**: 为了克服P-256等曲线缺乏有效共轭的问题，文章提出了“假GLV”技巧，通过引入辅助参数u和v，使得即使在没有有效共轭的情况下，也能实现类似的加速效果。

4. **实施与性能比较**: 通过在gnark库中的实现，文章展示了该技术在P-256和其他曲线上的性能提升，特别是在ECDSA验证过程中的应用，相比于传统方法，性能显著提高。

5. **案例研究与应用**: 最后，文章引用了实际应用案例，如基于zk-SNARKs的WebAuthn和P-256签名验证，展示了该技术的实际效益，特别是在减少证明大小和验证时间方面的优势。

通过上述技巧和实施，文章提供了一种在缺乏有效共轭的椭圆曲线上进行高效标量乘法的新途径，这对于依赖于P-256或其他类似曲线的系统具有重要意义，特别是在需要在SNARK电路中执行ECDSA验证的场景中。 <div>
<pre><code class="lang-auto"> _____     _           ____ _ __     __
|  ___|_ _| | _____   / ___| |\ \   / /
| |_ / _` | |/ / _ \ | |  _| | \ \ / /  
|  _| (_| |   &lt;  __/ | |_| | |__\ V /   
|_|  \__,_|_|\_\___|  \____|_____\_/   
</code></pre>
<h1><a class="anchor" href="https://ethresear.ch#p-49892-you-dont-need-an-efficient-endomorphism-to-implement-glv-like-scalar-multiplication-in-snark-circuits-1" name="p-49892-you-dont-need-an-efficient-endomorphism-to-implement-glv-like-scalar-multiplication-in-snark-circuits-1"></a>You don’t need an efficient endomorphism to implement GLV-like scalar multiplication in SNARK circuits</h1>
<ul>
<li><a href="https://ethresear.ch#Introduction">Introduction</a>
<ul>
<li><a href="https://ethresear.ch#Other-applications">Other applications</a></li>
</ul>
</li>
<li><a href="https://ethresear.ch#Background">Background</a></li>
<li><a href="https://ethresear.ch#The-fake-GLV-trick6">The fake GLV trick</a>
<ul>
<li><a href="https://ethresear.ch#Implementation7">Implementation</a>
<ul>
<li><a href="https://ethresear.ch#Benchmark">Benchmark</a></li>
<li><a href="https://ethresear.ch#Comparison">Comparison</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-49892-introduction-2" name="p-49892-introduction-2"></a>Introduction</h2>
<p>P-256, also known as secp256r1 and prime256v1, is a 256-bit prime field Weierstrass curve standardized by the NIST. It is widely adopted in internet systems, which explains its myriad use cases in platforms such as TLS, DNSSEC, Apple’s Secure Enclave, Passkeys, Android Keystore, and Yubikey. The key operation in elliptic curves based cryptography is the scalar multiplication. When the curve is equipped with an efficient endomorphism it is possible to speed up this operation through the well-known <a href="https://www.iacr.org/archive/crypto2001/21390189.pdf" rel="noopener nofollow ugc">GLV</a> algorithm. P-256 does unfortunately not have an efficient endomorphism (see <a href="https://neuromancer.sk/std/nist/P-256#" rel="noopener nofollow ugc">parameters</a>) to enjoy this speedup.</p>
<p>Verifying ECDSA signatures on Ethereum through precompiled contracts, i.e. smart contracts built into the Ethereum protocol (there are only 9) is only possible with the <em>secp256k1</em> curve and not the P-256.<br />
Verifying ECDSA signatures on P-256 requires computing scalar multiplications in Solidity and is especially useful for smart-contract wallets, enabling hardware-based signing keys and safer, easier self-custody. Different solutions can bring P-256 signatures on-chain. There are primarily three interesting approaches: (zk)-SNARK based verifiers, smart contract verifiers (e.g. <a href="https://eprint.iacr.org/2023/939.pdf" rel="noopener nofollow ugc">[Dubois23]</a>, <a>Ledger/FCL</a> (deprecated), <a href="https://github.com/get-smooth/crypto-lib" rel="noopener nofollow ugc">smoo.th/SCL</a> and <a href="https://daimo.com/blog/p256verifier" rel="noopener nofollow ugc">daimo/p256verifier</a>), and native protocol precompiles (<a href="https://github.com/ethereum/RIPs/blob/196f28d2164f30333b503481e7da954d4bf32ea3/RIPS/rip-7212.md" rel="noopener nofollow ugc">EIP/RIP 7212</a>).</p>
<p>Using SNARK (succinctness) properties, provides a great way to reduce gas cost for computation on Ethereum (e.g. ~232k gas for <a href="https://eprint.iacr.org/2016/260.pdf" rel="noopener nofollow ugc">Groth16</a>, ~285k gas for <a href="https://eprint.iacr.org/2019/953.pdf" rel="noopener nofollow ugc">PLONK</a> and ~185k gas for <a href="https://eprint.iacr.org/2021/1167" rel="noopener nofollow ugc">FFLONK</a>). This is very competitive with (and sometimes better that) the currently gas-optimal smart contract verifier. Moreover one can batch many ECDSA verifications in a single proof, amortizing thus the gas cost. However verifying P-256 signatures in a SNARK circuit can be very expensive i.e. long proving time. This is because the field where the points on the P-256 curve lie is different than the field where the SNARK computation is usually expressed. To be able to verify the proof onchain through the procompile the SNARK field needs to be the <a href="https://hackmd.io/@jpw/bn254" rel="noopener nofollow ugc">BN254</a> scalar field. Different teams tried to implement the ECDSA verification on P-256 in a BN254 SNARK circuit efficiently. Among these: <a href="https://github.com/zkwebauthn/webauthn-halo2" rel="noopener nofollow ugc">zkwebauthn/webauthn-halo2</a>, <a href="https://github.com/zkwebauthn/webauthn-circom" rel="noopener nofollow ugc">https://github.com/zkwebauthn/webauthn-circom</a> and <a href="https://github.com/privacy-scaling-explorations/circom-ecdsa-p256" rel="noopener nofollow ugc">PSE/circom-ecdsa-p256</a>.</p>
<p><em>If P-256 had an efficient endomorphism we could have optimized the proving time a great deal!</em></p>
<p>In this note we show a way to implement a GLV-like scalar multiplications in-circuit without having an efficient endomorphism.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49892-other-applications-3" name="p-49892-other-applications-3"></a>Other applications</h3>
<ul>
<li>This technique can be applied to any elliptic curve without an efficient endomorphism (e.g. <a href="https://en.wikipedia.org/wiki/Curve25519" rel="noopener nofollow ugc">Curve25519</a>, <a href="https://en.wikipedia.org/wiki/P-384" rel="noopener nofollow ugc">P-384</a>, MNT-753 (<a href="https://coinlist.co/build/coda/pages/MNT4753" rel="noopener nofollow ugc">k=4</a>, <a href="https://coinlist.co/build/coda/pages/MNT6753" rel="noopener nofollow ugc">k=6</a>), <a href="https://docs.starknet.io/architecture-and-concepts/cryptography/stark-curve/" rel="noopener nofollow ugc">STARK curve</a>, <a href="https://eprint.iacr.org/2024/869" rel="noopener nofollow ugc"><span class="math">\mathcal{B}</span> of “cycle5”</a>, …). See this <a href="https://neuromancer.sk/std/" rel="noopener nofollow ugc">database</a> for other curves.</li>
<li>This would question the choice of <a href="https://eprint.iacr.org/2021/1152" rel="noopener nofollow ugc">Bandersnatch</a> (<em>an embedded endomorphism-equipped curve over BLS12-381</em>) over <a href="https://github.com/zkcrypto/jubjub" rel="noopener nofollow ugc">Jubjub</a> (<em>an embedded curve over BLS12-381 without endomorphism</em>) for <a href="https://verkle.info/" rel="noopener nofollow ugc">Ethereum Verkle trees</a>.</li>
<li>This can speedup ECDSA verification in <a href="https://docs.cairo-lang.org/hello_starknet/signature_verification.html" rel="noopener nofollow ugc">Starknet</a> and <a href="https://www.cairo-lang.org/" rel="noopener nofollow ugc">Cairo</a> (through the <a href="https://docs.starknet.io/architecture-and-concepts/cryptography/stark-curve/" rel="noopener nofollow ugc">STARK curve</a>).</li>
<li>This can speedup natively the folding step (à la Nova) of Ed25519 signatures through the 2-cycles proposed <a href="https://moderncrypto.org/mail-archive/curves/2024/001050.html" rel="noopener nofollow ugc">here</a> by Aurore Guillevic.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-49892-background-4" name="p-49892-background-4"></a>Background</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-49892-standard-scalar-multiplication-5" name="p-49892-standard-scalar-multiplication-5"></a>Standard scalar multiplication</h3>
<p>Let <span class="math">E</span> be an elliptic curve defined over the prime field <span class="math">\mathbb{F}_p</span> and let <span class="math">r</span> be a prime divisor of the curve order <span class="math">\#E(\mathbb{F}_p)</span> (i.e. the number of points).<br />
Let <span class="math">s \in \mathbb{F}_r</span> and <span class="math">P(x,y) \in E(\mathbb{F}_p)</span>, we are interested in proving scalar multiplication <span class="math">s\cdot P</span> over the <span class="math">r</span>-torsion subgroup of <span class="math">E</span>, denoted <span class="math">E[r]</span> (i.e. the subset of points of order <span class="math">r</span>).</p>
<p>The simplest algorithm is the standard left-to-right <em>double-and-add</em>:</p>
<pre><code class="lang-auto">INPUT: s = (s_{t−1},..., s_1, s_0), P ∈ E(Fp).
OUTPUT: sP.
1. Q ← ∞.
2. For i from t−1 downto 0 do
    2.1 Q ← 2Q.
    2.2 If s_i = 1 then Q ← Q + P.
3. Return(Q).
</code></pre>
<p>If/else branching is not possible in SNARK circuits so this is replaced by constant window table lookups inside the circuit. This can be achieved using polynomials which vanish at the constants that aren’t being selected, i.e. a 1-bit table lookup <code>Q ← s_i * Q + (1 - s_i) * (Q+P)</code>. Hence this double-and-add algorithm requires <span class="math">t</span> doublings, <span class="math">t</span> additions and <span class="math">t</span> 1-bit table lookup.<br />
This can be extended to <em>windowed</em> double-and-add, i.e. scanning more than a bit per iteration using larger window tables, but the multiplicative depth of the evaluation increases exponentially. We use affine coordinates for doubling/adding points because inverses cost as much as multiplications, i.e. instead of checking that <span class="math">1/x</span> is <span class="math">y</span> we provide <span class="math">y</span> out-circuit and check in-circuit that <span class="math">x\cdot y = 1</span>. However since we start with <span class="math">Q ← ∞</span> it is infeasible to avoid conditional branching since affine formulas are incomplete. Instead, we scan the bits right-to-left and assume that the first bit <code>s_0</code> is 1 (so that we start at <code>Q ← P</code>), we double the input point <code>P</code> instead of the accumulator <code>Q</code> in this algorithm and finally conditionally subtract (using the 1-bit lookup) <code>P</code> if <code>s_0</code> was 0.</p>
<pre><code class="lang-auto">INPUT: s = (s_{t−1},..., s_1, s_0), P ∈ E(Fp).
OUTPUT: sP.
1. Q ← P.
2. For i from 1 to t−1 do
    2.1 If s_i = 1 then Q ← Q + P.
    2.2 P ← 2P.
3. if s_0 = 0 then Q ← Q - P
4. Return(Q).
</code></pre>
<h3><a class="anchor" href="https://ethresear.ch#p-49892-glv-scalar-multiplication-6" name="p-49892-glv-scalar-multiplication-6"></a>GLV scalar multiplication</h3>
<p>However it is well known that if the curve is equipped with an efficient endomorphism then there exists a faster algorithm known as <a href="https://www.iacr.org/archive/crypto2001/21390189.pdf" rel="noopener nofollow ugc">[GLV]</a>.</p>
<p><strong>Example 1 :</strong> suppose that <span class="math">E</span> has Complex Multiplication (CM) with discrimant <span class="math">-D=-3</span>, i.e. <span class="math">E</span> is of the form <span class="math">y^2=x^3+b</span>, with <span class="math">b \in \mathbb{F}_p</span>. This is the case of <code>BN254</code>, <code>BLS12-381</code> and <code>secp256k1</code> elliptic curves used in Ethereum. There is an efficient endomorphism <span class="math">\phi: E \rightarrow E</span> defined by <span class="math">(x,y)\mapsto (\omega x,y)</span> (and <span class="math">\mathcal{O} \mapsto \mathcal{O}</span>) that acts on <span class="math">P \in E[r]</span> as <span class="math">\phi(P)=\lambda \cdot P</span>. Both <span class="math">\omega</span> and <span class="math">\lambda</span> are cube roots of unity in <span class="math">\mathbb{F}_p</span> and <span class="math">\mathbb{F}_r</span> respectively, i.e. <span class="math">\omega^2+\omega+1 \equiv 0 \pmod p</span> and <span class="math">\lambda^2+\lambda+1 \equiv 0 \pmod r</span>.</p>
<p><strong>Example 2 :</strong> suppose that <span class="math">E</span> has Complex Multiplication (CM) with discrimant <span class="math">-D=-8</span>, meaning that the endomorphism ring is <span class="math">\mathbf{Z}[\sqrt{−2}]</span>. This is the case of the <code>Bandersnatch</code> elliptic curves specified in Ethereum Verkle trie. There is an efficient endomorphism <span class="math">\phi: E \rightarrow E</span> whose kernel is generated by a 2-torsion point. The map can be found by looking at 2-isogeneous curves and applying Vélu’s formulas. For Bandersnatch it is defined by <span class="math">(x,y)\mapsto (u^2\cdot \frac{x^2+wx+t}{x+w},u^3\cdot y\cdot \frac{x^2+2wx+v}{(x+w)^2})</span> for some constants <span class="math">u,v,w,t</span> (and <span class="math">\mathcal{O} \mapsto \mathcal{O}</span>) that acts on <span class="math">P \in E[r]</span> as <span class="math">\phi(P)=\lambda \cdot P</span> where <span class="math">\lambda^2+2 \equiv 0 \pmod r</span>.</p>
<p>The GLV algorithm starts by decomposing <span class="math">s</span> as <span class="math">s = s_0 + \lambda s_1</span> and then replacing the scalar multiplication <span class="math">s \cdot P</span> by <span class="math">s_0 \cdot P + s_1 \cdot \phi(P)</span>. Because <span class="math">s_0</span> and <span class="math">s_1</span> are guaranteed to be <span class="math">\leq \sqrt{r}</span> (see Sec.4 of <a href="https://www.iacr.org/archive/crypto2001/21390189.pdf" rel="noopener nofollow ugc">[GLV]</a> and Sec.4 of <a href="https://eprint.iacr.org/2015/565.pdf" rel="noopener nofollow ugc">[FourQ]</a> for an optimization trick), we can halve the size of the for loop in the double-and-add algorithm. We can then scan simultaenously the bits of <span class="math">s_0</span> and <span class="math">s_1</span> and apply the <a href="https://crypto.stackexchange.com/questions/99975/strauss-shamir-trick-on-ec-multiplication-by-scalar" rel="noopener nofollow ugc">Strauss-Shamir trick</a>. This results in a significant speed up but only when an endomorphism is available. For example the left-to-right double-and-add would become:</p>
<pre><code class="lang-auto">INPUT: s and P ∈ E(Fp).
OUTPUT: sP.
1. Find s1 and s2 s.t. s = s1 + 𝜆 * s2 mod r 
    1.1 let s1 = (s1_{t−1},..., s1_1, s1_0) 
    1.2 and s2 = = (s2_{t−1},..., s2_1, s2_0)
2. P1 ← P, P2 ← 𝜙(P) and Q ← ∞.
3. For i from t−1 downto 0 do
    3.1 Q ← 2Q.
    3.2 If s1_i = 0 and s2_i = 0 then Q ← Q.
    3.3 If s1_i = 1 and s2_i = 0 then Q ← Q + P1.
    3.4 If s1_i = 0 and s2_i = 1 then Q ← Q + P2.
    3.5 If s1_i = 1 and s2_i = 1 then Q ← Q + P1 + P2.
4. Return(Q).
</code></pre>
<p>Using the efficient endomorphism in-circuit is also possible (see <a href="https://eprint.iacr.org/2019/1021.pdf" rel="noopener nofollow ugc">[Halo, Sec. 6.2 and Appendix C]</a> or <a href="https://github.com/Consensys/gnark/blob/ea53f373f45d2f9ad9cc1639c34359a35f771191/std/algebra/emulated/sw_emulated/point.go#L530" rel="noopener nofollow ugc">[gnark implementation]</a> for short Weierstrass curves and <a href="https://github.com/zhenfeizhang/bandersnatch-glv" rel="noopener nofollow ugc">[arkworks]</a> and <a href="https://github.com/Consensys/gnark/blob/dc04a1d3b221dbe7571b5a8394b55d02c2872700/std/algebra/native/twistededwards/scalarmul_glv.go" rel="noopener nofollow ugc">[gnark]</a> implementations for twisted Edwards). But one should be careful about some extra checks of the decomposition <span class="math">s = s_0 + \lambda s_1 \mod r</span> (not the SNARK modulus). The integers <span class="math">s_0, s_1</span> can possibly be negative in which case they will be reduced in-circuit modulo the SNARK field and not <span class="math">r</span>.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49892-the-fake-glv-trick-7" name="p-49892-the-fake-glv-trick-7"></a>The fake GLV trick</h2>
<p>Remember that we are proving that <span class="math">s\cdot P = Q</span> and not computing it. We can “hint” the result <span class="math">Q</span> and check in-circuit that <span class="math">s\cdot P - Q = \mathcal{O}</span>. Now, if we can find <span class="math">u,v \leq \sqrt{r}</span> such that <span class="math">v\cdot s = u \pmod r</span> then we can check instead that</p>
<p><span class="math">(v\cdot s)\cdot P - v\cdot Q = \mathcal{O}</span></p>
<p>which is equivalent to</p>
<p><span class="math"> u\cdot P - v\cdot Q = \mathcal{O}</span></p>
<p>The thing now is that <span class="math">u</span> and <span class="math">v</span> are “small” and we can, similarly to the GLV algorithm, halve the size of the double-and-add loop and apply the Strauss-Shamir trick.</p>
<p><strong>Solution</strong>: running the half-GCD algorithm (i.e. running GCD half-way) is sufficient to find <span class="math">u</span> and <span class="math">v</span>. We can apply the exact same trick for finding the lattice basis as in the GLV paper (Sec. 4). For completeness we recall the algorithm hereafter.<br />
We apply the extended Euclidean algorithm to find the greatest common divisor of <span class="math">r</span> and <span class="math">s</span> (This gcd is 1 since <span class="math">r</span> is prime.) The algorithm produces a sequence of equations</p>
<p><span class="math">w_i \cdot r + v_i \cdot s = u_i</span></p>
<p>for <span class="math">i = 0, 1, 2, \dots</span>  where <span class="math">w_0 = 1, v_0 = 0, u_0 = r, w_1 = 0, v_1 = 1, u_1 = s</span>, and <span class="math">u_i \geq 0</span> for all <span class="math">i</span>. We stop at the index <span class="math">m</span> for which <span class="math">u_m \geq \sqrt{r}</span> and take <span class="math">u = u_{m+1}</span> and <span class="math">v = -v_{m+1}</span>.<br />
<em>Note:</em> By construction <span class="math">u</span> is guaranteed to be a positive integer but <span class="math">v</span> can be negative, in which case it would be reduced in-circuit modulo the SNARK modulus and not <span class="math">r</span>. To circumvent this we return in the hint <span class="math">u</span>, <span class="math">v</span> and a <span class="math">\texttt{b}=1</span> if <span class="math">v</span> is negative and <span class="math">\texttt{b}=0</span> otherwise. In-circuit we negate <span class="math">Q</span> instead when <span class="math">\texttt{b}=1</span>.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49892-implementation-8" name="p-49892-implementation-8"></a>Implementation</h3>
<p>A generic implementation in the gnark library is available at <a href="https://github.com/Consensys/gnark/feat/fake-GLV" rel="noopener nofollow ugc">gnark.io (feat/fake-GLV branch)</a>. For Short Weierstrass (e.g. P256) look at the <code>scalarMulFakeGLV</code> <a href="https://github.com/Consensys/gnark/blob/62c89cb10cff1413e9d68cce054c7e711d04c726/std/algebra/emulated/sw_emulated/point.go#L1263" rel="noopener nofollow ugc">method</a> in the emulated package and for twisted Edwards (e.g. Bandersnatch/Jubjub) look at the <code>scalarMulFakeGLV</code> <a href="https://github.com/Consensys/gnark/blob/62c89cb10cff1413e9d68cce054c7e711d04c726/std/algebra/native/twistededwards/point.go#L261" rel="noopener nofollow ugc">method</a> in the native package.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-49892-benchmark-9" name="p-49892-benchmark-9"></a>Benchmark</h4>
<p>The best algorithm to implement scalar multiplication in a non-native circuit (i.e. circuit field ≠ curve field) when an efficient endomorphism is <em>not</em> available is an adaptation of <a href="https://www.iacr.org/archive/ches2007/47270135/47270135.pdf" rel="noopener nofollow ugc">[Joye07]</a> (implemented in <a href="https://github.com/Consensys/gnark/blob/fdb2b0de422b1c4fc5c6d08e81e788095ac818a6/std/algebra/emulated/sw_emulated/point.go#L748" rel="noopener nofollow ugc">gnark here</a>).<br />
Next we compare this scalar multiplication with our fake GLV in a PLONKish vanilla (i.e. no custom gates) circuit (scs) over the BN254 curve (Ethereum compatible). We also give benchmarks in R1CS.</p>
<div class="md-table">
<table>
<thead>
<tr>
<th style="text-align: left;">P-256</th>
<th style="text-align: center;">Old (Joye07)</th>
<th style="text-align: right;">New (fake GLV)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><span class="math">[s]P</span></td>
<td style="text-align: center;">738,031 scs <br /> 186,466 r1cs</td>
<td style="text-align: right;">385,412 scs <br /> 100,914 r1cs</td>
</tr>
<tr>
<td style="text-align: left;">ECDSA verification</td>
<td style="text-align: center;">1,135,876 scs <br /> 293,814 r1cs</td>
<td style="text-align: right;">742,541 scs <br /> 195,266 r1cs</td>
</tr>
</tbody>
</table>
</div><blockquote>
<p>Note here that the old ECDSA verification uses Strauss-Shamir trick for computing <span class="math">[s]P+[t]Q</span> while the new version is merely two fake GLV multiplications and an addition.</p>
</blockquote>
<h4><a class="anchor" href="https://ethresear.ch#p-49892-comparison-10" name="p-49892-comparison-10"></a>Comparison</h4>
<p><a href="https://www.p256wallet.org/" rel="noopener nofollow ugc">p256wallet.org</a> is an ERC-4337 smart contract wallet that leverages zk-SNARKs for WebAuthn and P-256 signature verification. It uses <a href="https://github.com/privacy-scaling-explorations/circom-ecdsa-p256" rel="noopener nofollow ugc">PSE/circom-ecdsa-p256</a> to generate the webAuthn proof, and underneath <a href="https://github.com/privacy-scaling-explorations/circom-ecdsa-p256" rel="noopener nofollow ugc">PSE/circom-ecdsa-p256</a> to generate the ECDSA proof on P-256 curve. The github README reports <code>1,972,905 R1CS</code>. Compiling our circuit in R1CS results in <strong><code>195,266 R1CS</code></strong>. This is more than a <strong>10x</strong> reduction, which is not only due to the fake GLV algorithm but also to optimized non-native field arithmetic in gnark.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-49892-other-curves-11" name="p-49892-other-curves-11"></a>Other curves</h4>
<p>Similar results are noticed for other curves in short Weirstrass, e.g. P-384 and STARK curve:</p>
<div class="md-table">
<table>
<thead>
<tr>
<th style="text-align: left;">P-384</th>
<th style="text-align: center;">Old (Joye07)</th>
<th style="text-align: right;">New (fake GLV)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><span class="math">[s]P</span></td>
<td style="text-align: center;">1,438,071 scs</td>
<td style="text-align: right;">782,674 scs</td>
</tr>
<tr>
<td style="text-align: left;">ECDSA verification</td>
<td style="text-align: center;">2,174,027 scs</td>
<td style="text-align: right;">1,419,929 scs</td>
</tr>
</tbody>
</table>
</div><div class="md-table">
<table>
<thead>
<tr>
<th style="text-align: left;">STARK curve</th>
<th style="text-align: center;">Old (Joye07)</th>
<th style="text-align: right;">New (fake GLV)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><span class="math">[s]P</span></td>
<td style="text-align: center;">727,033 scs</td>
<td style="text-align: right;">380,210 scs</td>
</tr>
<tr>
<td style="text-align: left;">ECDSA verification</td>
<td style="text-align: center;">1,137,459 scs</td>
<td style="text-align: right;">732,131 scs</td>
</tr>
</tbody>
</table>
</div><p>and also in twisted Edwards e.g. Jubjub vs. Bandersnatch:</p>
<div class="md-table">
<table>
<thead>
<tr>
<th style="text-align: left;">Jubjub</th>
<th style="text-align: center;">Old (2-bit double-and-add)</th>
<th style="text-align: right;">New (fake GLV)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><span class="math">[s]P</span></td>
<td style="text-align: center;">5,863 scs <br /> 3,314 r1cs</td>
<td style="text-align: right;">4,549  scs <br /> 2,401 r1cs</td>
</tr>
</tbody>
</table>
</div><div class="md-table">
<table>
<thead>
<tr>
<th style="text-align: left;">Bandersnatch</th>
<th style="text-align: center;">Old (GLV)</th>
<th style="text-align: right;">New (fake GLV)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><span class="math">[s]P</span></td>
<td style="text-align: center;">4,781 scs <br />  2,455 r1cs</td>
<td style="text-align: right;">4,712 scs <br /> 2,420 r1cs</td>
</tr>
</tbody>
</table>
</div><h2><a class="anchor" href="https://ethresear.ch#p-49892-acknowledgement-12" name="p-49892-acknowledgement-12"></a>Acknowledgement</h2>
<p>I would like to thank Arnau Cube, Aard Vark, Holden Mui, Olivier Bégassat and Thomas Piellard for fruitful discussions.</p>
            <p><small>2 posts - 2 participants</small></p>
            <p><a href="https://ethresear.ch/t/fake-glv-you-dont-need-an-efficient-endomorphism-to-implement-glv-like-scalar-multiplication-in-snark-circuits/20394">Read full topic</a></p>
]]></content:encoded>
<pubDate>Mon, 09 Sep 2024 19:48:29 +0000</pubDate>
</item>
<item>
<title>Embedded fee markets and ERC-4337 (part 2)</title>
<link>https://ethresear.ch/t/embedded-fee-markets-and-erc-4337-part-2/20384</link>
<guid>https://ethresear.ch/t/embedded-fee-markets-and-erc-4337-part-2/20384</guid>
<content:encoded><![CDATA[
<div> 关键词：ERC-4337、用户操作（UserOps）、捆绑者（Bundlers）、交易费用、链下成本

总结：

本文主要探讨了以太坊生态中ERC-4337模型及其在处理用户操作（UserOps）时的挑战与改进方法。ERC-4337模型概述了捆绑者（Bundlers）的费用市场结构和链上发布成本与链下聚合成本的关联。文章引入了“捆绑者游戏”的概念，强调了捆绑者与用户之间的信息不对称问题，这导致了用户处于明显的不利地位。

文章分析了当前ERC-4337状态，指出P2P mempool尚未在主网上线，主要在Sepolia测试网上进行测试。Kofi开发的工具提供了有关当前状态的统计数据，显示大多数捆绑者倾向于打包多个用户操作，而那些同时提供赞助服务（Paymaster）的大型捆绑者占主导地位。文章讨论了用户为何需要支付费用给捆绑者，并推测随着采用率的提高，未来用户可能需要自行承担费用。

文章还探讨了ERC-4337与其他变体（如结合捆绑者与构建者角色的方案）的比较，强调了保持这两个角色分离的优势，包括促进竞争和增强抗审查性。通过保持分离，可以实现更高效的市场动态，其中竞争促使捆绑者降低费用以吸引用户，从而为用户提供更具竞争力的价格和服务。

最后，文章讨论了Layer2解决方案中的账户抽象（Account Abstraction）和签名聚合技术如何帮助优化交易费用和数据可用性成本。通过使用签名聚合，可以在不牺牲安全性的情况下显著减少交易大小和链上成本。此外，文章提出了一些机制，如使用预言机提供内存池信息或由Layer2自身作为捆绑者，以帮助用户更好地估算费用并确保公平性。

综上所述，文章深入分析了ERC-4337及其变体在处理用户操作时的复杂性和挑战，并提出了几种改进策略，旨在提升用户体验和效率，同时维护系统的安全性和公平性。 <div>
<p>by: Davide Rezzoli (<a class="mention" href="https://ethresear.ch/u/daviderezzoli">@DavideRezzoli</a>) and Barnabé Monnot (<a class="mention" href="https://ethresear.ch/u/barnabe">@barnabe</a>)</p>
<p>Many thanks to Yoav Weiss (<a class="mention" href="https://ethresear.ch/u/yoavw">@yoavw</a>) for introducing us to the problem, Dror Tirosh (<a class="mention" href="https://ethresear.ch/u/drortirosh">@drortirosh</a>) for helpful comments on the draft, and the 4337 team for their support. Reviews ≠ endorsements; all errors are the authors’ own.</p>
<p>This work was done for <a href="https://efdn.notion.site/ROP-7-Economic-models-of-signature-aggregation-in-account-abstraction-ec5390efab864ed49a8535e8bdfff182" rel="noopener nofollow ugc">ROP-7</a>.</p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-49870-introduction-1" name="p-49870-introduction-1"></a>Introduction</h3>
<p>In our previous <a href="https://ethresear.ch/t/embedded-fee-markets-and-erc-4337-part-1/19542">post</a>, we introduced the ERC-4337 model. This model outlines the fee market structure for bundlers and details the cost function related to the on-chain publishing cost and the off-chain (aggregation costs) of a bundle.</p>
<p>We also introduced the concept of the “<em>Bundler Game</em>”. This game will be the primary focus of the second part. Given a set of transactions, a bundler can choose which transactions to include in their bundle. This creates an asymmetry of information between the bundlers and the user, as the user doesn’t know how many transactions will be included in the bundle. This leads to a zero-sum game where the user is at a clear disadvantage.</p>
<p>This research aims to explore methods to improve the UX by ensuring that users do not need to overpay for inclusion in the next bundle. Instead, users should be able to pay a fee based on the actual market demand for inclusion.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49870-current-state-of-erc-4337-2" name="p-49870-current-state-of-erc-4337-2"></a>Current state of ERC-4337</h2>
<p>In today’s market, the P2P mempool is not live on mainnet and it’s being tested on the Sepolia testnet. Companies building on ERC-4337 are currently operating in a private mode, the users connect via an RPC to a private bundler which will than work with a buidler to publish onchain your useroperation. <a href="https://www.bundlebear.com/overview/all" rel="noopener nofollow ugc">Bundle Bear app</a>, developed by Kofi, provides some intriguing statistics on the current state of ERC-4337.</p>
<p>In the <a href="https://www.bundlebear.com/bundlers/all" rel="noopener nofollow ugc">Weekly % Multi-UserOp Bundles</a> metric, we observe the percentage of bundlers creating bundles that include multiple userops. From the beginning of 2024 to June 2024, this percentage has not exceeded 6.6%. This data becomes even more interesting when considering that many bundlers run their own paymasters, entities that sponsor transactions on behalf of users. Notably, the two largest bundlers who also operate as a paymaster, in terms of user operations published, <a href="https://www.bundlebear.com/paymasters/all" rel="noopener nofollow ugc">sponsored 97%</a> of the user operations using their services. The paymaster pays for some parts of the useroperation and the rest is paid by the dapps or other <a href="https://www.coinbase.com/en-de/developer-platform/solutions/account-abstraction-kit" rel="noopener nofollow ugc">entity</a>.</p>
<p>The question that arises is why paymasters, dApps, etc., are paying for the user operations. Will the user pay them back in the future? We can’t be sure what will happen, but my personal guess is that currently, dApps are covering the fees to increase usage and adoption of their apps. Once adoption is high, users will likely have to pay for the transactions themselves. It’s worth mentioning that for the user to pay for a user operation with the current model is not the best option, since a basic ERC-4337 operation costs ~42,000 gas, while a normal transaction costs ~21,000 gas.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49870-variations-on-erc-4337-3" name="p-49870-variations-on-erc-4337-3"></a>Variations on ERC-4337</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-49870-overview-of-erc-4337-4" name="p-49870-overview-of-erc-4337-4"></a>Overview of ERC-4337</h3>
<p>The mempool is still in a testing phase on Sepolia and is not live on the mainnet. Without the mempool, users have limited options for using account abstraction. Users interact with an RPC, which may be offered by a bundler that bundles UserOps, or with an RPC service that doesn’t bundle, similar to services like Alchemy or Infura, which receive and propagate transactions to other bundlers.</p>
<p><img alt="High level of a transaction in ERC-4337 without the mempool" height="111" src="https://ethresear.ch/uploads/default/original/3X/5/c/5cfa750fc581f313b031ca060a05d21cc3379214.jpeg" width="497" /></p>
<p>Once the mempool is live, the transaction flow will resemble the diagram below, which is similar to the current transaction flow. A mempool enhances censorship resistance for users because, unlike the RPC model, it reduces the chances of a transaction being excluded. However, even with a mempool, there is still a risk that an RPC provider might not forward the transaction, but the mempool model is particularly beneficial for users who prefer to run their own nodes, as it mitigates this risk.</p>
<p><img alt="High level of a normal transaction using an EOA" height="136" src="https://ethresear.ch/uploads/default/original/3X/7/7/779c484a5068fcd2a4df86e24c5ede85cb6af781.png" width="631" /></p>
<p><img alt="High level of an userop type of transaction" height="126" src="https://ethresear.ch/uploads/default/original/3X/f/9/f9ab02c182e4af4e72324eddfc31b93c5555f115.jpeg" width="621" /></p>
<p>While bundlers have the potential to act as builders, we prefer to keep the roles separate due to the competitive landscape. Bundlers would face significant competition from existing, sophisticated builders, making building less attractive and potentially less profitable. As a result, bundlers are more incentivized to collaborate with established builders rather than building independently and risking losses.</p>
<p>Combining the roles of bundler and builder into a single entity implies significant changes to the current system. Bundlers would need to compete with existing <a href="https://etherscan.io/dashboards/block-producers" rel="noopener nofollow ugc">sophisticated builders</a>, or alternatively, current builders will need to horizontally integrate and assume the bundler role as well. The latter scenario, while more plausible, raises concerns about market concentration and the potential negative impact on censorship resistance.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49870-bundlers-and-builders-as-two-different-entities-5" name="p-49870-bundlers-and-builders-as-two-different-entities-5"></a>Bundlers and builders as two different entities</h3>
<p>With the users connecting directly to an RPC, everything runs in a more private environment, which doesn’t help with market competition. In the near future the mempool will be on the mainnet increasing competition.</p>
<p>Using a mempool, in which userops are public to different bundlers increases competition, in the case of non native account abstraction having a separation between bundler and builder is needed, in the case of native account abstraction the separation might not be needed since the builder can interpret the userops as normal transactions.</p>
<p>For our model we believe that having a separation between the bundler and the builder also offers some advantages, especially in terms of competition and censorship resistance. Imagine a scenario where all the bundlers are offering a cost <span class="math">\textbf{v}</span> for getting included in their bundle. There will be a bundler who wants to attract more users to achieve higher profits, so they will offer a cost <span class="math">\textbf{v’}</span>  where <span class="math">\textbf{v’} &lt; \textbf{v}</span>  with enough competition among bundlers, <span class="math">\textbf{v'}</span> will get close to <span class="math">\omega</span>, the aggregation cost for the bundle. In this case, the bundlers who can search more efficiently and have better hardware to include more transactions in a bundle will earn higher fees and in return makes the useroperation for the user cheaper.</p>
<p>This could lead to the following outcome: In a <strong>competitive environment</strong>, bundlers will lower their prices to be selected by users, who will, in turn, seek the lowest price for the inclusion of their user operation in a bundle. This competition will create a system where the bundler who offers the best price will be selected more often than the bundler who is only trying to maximize their profit by creating smaller bundles. Separating the roles of the bundler and builder can also enhance censorship resistance. A bundler can create a bundle of aggregated user operations and send it to different builders. If the bundle includes operations that could be censored, a non-censoring builder can accept it and proceed with construction. However, it’s worth noting that from a user’s perspective, this setup could increase costs, as the introduction of a bundler adds an additional party, leading to higher expenses.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49870-rip-7560-6" name="p-49870-rip-7560-6"></a>RIP-7560</h3>
<p>Native account abstraction isn’t a novel concept; it’s been under research for years. While ERC-4337 is gaining traction, its implementation outside the protocol offers distinct advantages alongside trade-offs. Notably, existing EOAs can’t seamlessly transition to SCWs, and various types of censorship-resistant lists are harder to utilize. As previously mentioned, the gas overhead of a userOp cost escalates significantly compared to a normal transaction. <a href="https://github.com/ethereum/RIPs/blob/196f28d2164f30333b503481e7da954d4bf32ea3/RIPS/rip-7560.md" rel="noopener nofollow ugc">RIP-7560</a> won’t inherently resolve the ongoing issue concerning off-chain costs, but it substantially reduce transaction expenses. From the initial ~42000 gas, it’s possible to reduce the cost by <a href="https://youtu.be/sZ1UO4VN1GI?si=x7Tu22Oqxr7x-KAb&amp;t=554" rel="noopener nofollow ugc">~20000 gas</a>.</p>
<p><img alt="High level of a type4 transaction with RIP-7560" height="136" src="https://ethresear.ch/uploads/default/original/3X/f/a/fadf929aca9a2378a70f5456501dedf5da00358b.jpeg" width="491" /></p>
<h3><a class="anchor" href="https://ethresear.ch#p-49870-layer2s-account-abstraction-7" name="p-49870-layer2s-account-abstraction-7"></a>Layer2s Account Abstraction</h3>
<p>Account abstraction can be utilized in Layer 2 (L2) solutions. Some L2s already implement it natively, while others follow the L1 approach and are waiting for a new proposal similar to RIP-7560. In L2, the L1 is used for data availability to inherit security, while most of the computation occurs off-chain on the L2, providing cheaper transactions and scalability.</p>
<p><img alt="High level of Account abstraction in Layer 2" height="136" src="https://ethresear.ch/uploads/default/original/3X/b/2/b2a8d1ebfceef37ab62f0db834e7ee6135441741.jpeg" width="611" /></p>
<p>In scenarios where computation on L2 is significantly cheaper than the cost of calldata for data availability (DA) on the mainchain, the use of signature aggregation proves highly beneficial. For instance, pairing for BLS on the mainnet is facilitated by the <a href="https://www.evm.codes/precompiled" rel="noopener nofollow ugc"><em>0x08</em></a> precompile from the EVM, which costs approximately ~45000k gas. Consequently, using BLS on L1 is more expensive than traditional transactions.</p>
<p>Compression techniques on L2s are already being used, such as 0-byte compression, which reduces the cost from ~188 bytes to ~154 bytes for an ERC20 transfer. With signature aggregation, the compression efficiency can be further enhanced by using a single signature, reducing the size to ~128 bytes.</p>
<p>In Layer 2s, signature aggregation is a crucial innovation that enhances both transaction efficiency and cost-effectiveness. By combining multiple signatures into a single one, the overall data payload is significantly reduced, which lowers the costs associated with data availability on Layer 1. This advancement not only improves scalability but also reduces transaction costs for users, making the system more economical and efficient.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49870-signature-aggregation-economics-in-layer2s-8" name="p-49870-signature-aggregation-economics-in-layer2s-8"></a>Signature Aggregation economics in Layer2s</h2>
<p>When using an L2 service, the user incurs several costs, including a fee for the L2 operator, a cost based on network congestion, and the cost of data availability on L1.</p>
<p>From a previous research on ”<a href="https://barnabe.substack.com/p/understanding-rollup-economics-from#footnote-3-48535841" rel="noopener nofollow ugc"><strong>Understanding rollup economics from first principles</strong></a>”, we can outline the costs a user faces when using L2 services as follows:</p>
<p>When a user interatcs with a layer 2 he has some costs that we can define as follow:</p>
<ul>
<li><strong>User fee</strong> = L1 data publication fee + L2 operator fee + L2 congestion fee</li>
<li><strong>Operator cost</strong> = L2 operator cost + L1 data publication cost</li>
<li><strong>Operator revenue =</strong> User fees + MEV</li>
<li><strong>Operator profit = Operator revenue - Operator cost</strong> = L2 congestion fee + MEV</li>
</ul>
<p>In the case of non-native account abstraction, an additional entity, the bundler, may introduce a fee for creating bundles of userops.</p>
<p>Considering the bundler, the costs and profits are extended as follows:</p>
<ul>
<li><strong>User fee</strong> = L1 data publication fee + L2 operator fee + L2 congestion fee + Bundler Fee</li>
<li><strong>Bundler Cost</strong> = Quoted(L1 data publication fee + L2 operator fee + L2 congestion fee)</li>
<li><strong>Bundler Revenue</strong> = User fee</li>
<li><strong>Bundler profit</strong> = Bundler Revenue - Bundler cost = Difference between L1 and L2 costs and quoted prices from the bundler + Bundler fee</li>
<li><strong>Operator Cost</strong> = L1 data publication fee + L2 operator fee</li>
<li><strong>Operator profit = Operator revenue - Operator cost</strong> = L2 congestion fee + MEV</li>
</ul>
<p>The bundler earns its fee from the user for their services, while the remainder of the user’s payment covers the L2 operator’s costs. If the user is unaware of the bundle size, estimating the actual cost of sending userops becomes challenging, potentially leading to the bundler charging higher fees than the one necessary to cover the operator cost.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49870-incentive-alignment-in-l2-9" name="p-49870-incentive-alignment-in-l2-9"></a>Incentive Alignment in L2</h3>
<p>The interaction between the bundler and L2 helps address this issue, as L2s are incentivized to keep user costs low due to competition. Overcharging users can drive them to switch to other L2s offering fairer prices.</p>
<p>Let’s redefine our model by introducing the operator. The user bids to the bundler for inclusion in the next L2 block by bidding a value <span class="math">V</span>. The user aims to minimize the data publication fee, while the bundler seeks to maximize their fee or gain a surplus from L2 interaction costs and user fees.</p>
<p>The costs associated with creating a bundle and publishing it on-chain can be divided into two parts:</p>
<p><strong>On-chain cost function:</strong> A bundler issuing bundle <span class="math">\mathbf{B}</span> when the base fee is <span class="math">r</span> expends a cost:</p>
<div class="math">
C_\text{on-chain}(\mathbf{B}, r) = F \times r + n \times S \times r
</div>
<p><strong>Aggregated cost function:</strong> The bundler has a cost function for aggregating <span class="math">n</span> transactions in a single bundle <strong><span class="math">\mathbf{B}</span></strong> with base fee of <span class="math">r</span>:</p>
<div class="math">
C_\text{agg}(\mathbf{B}, r) = F' \times r + n \times S' \times r + n \times \omega
</div>
<p>with  <span class="math">S' &lt; S</span> the reduced size of a transaction and the pre-verification gas use <span class="math">F' &gt; F</span>, which contains the publication and verification of the single on-chain aggregated signature.</p>
<p>If the user can obtain a reliable estimate for <span class="math">n</span>, they can calculate their cost using the <code>estimateGas</code> function, available in most L2 solutions. Having a good estimation can make the user bid accordingly without having to overestimate their bid for inclusion. This function determines the necessary cost to ensure inclusion. Having a good estimate for <span class="math">n</span> and the <code>estimateGas</code> function can avoid the user to pay for a higher <code>preVerificationGas</code>. In the next section, we will explore various mechanisms to ensure a reliable estimation of <span class="math">n</span>.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49870-layer2s-operate-an-oracle-10" name="p-49870-layer2s-operate-an-oracle-10"></a>Layer2s operate an oracle</h3>
<p>The oracle’s role is to monitor the mempool and estimate the number of transactions present. The process works as follows: the Layer 2 deploys an oracle to check the mempool and then informs the user about the number of transactions in the mempool. This enables the user to estimate their bid for inclusion in a bundle. The Layer 2 can request the bundler to include at least a specified number of transactions (<span class="math">n</span>) in a bundle, or else the bundle will be rejected. Once the bundler gathers enough transactions to form a bundle, it sends the bundle to the Layer 2, which then forwards it to the mainnet as calldata for data availability.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/5/155c138c7fac1f1d415836ca20e488c9ad49fa73.jpeg" title="Watcher proposal"><img alt="Watcher proposal" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/1/5/155c138c7fac1f1d415836ca20e488c9ad49fa73_2_538x500.jpeg" width="538" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#p-49870-layer2s-with-shared-sequencer-11" name="p-49870-layer2s-with-shared-sequencer-11"></a>Layer2s with shared sequencer</h3>
<p>An interesting approach is to have multiple Layer 2 (L2) networks running a shared sequencer. This setup can provide a more accurate estimate of the mempool, as the sequencer reaches an agreement through consensus facilitated by the shared sequencer.</p>
<p>In this configuration, different L2 networks operate independently but share a common sequencer. At regular intervals, these networks check the number of user operations (userops) in the shared mempool. The shared sequencer helps synchronize and aggregate data from these networks. Once they reach an agreement, the information is communicated to the user, allowing them to bid based on the number of userops present.</p>
<p>This approach offers several advantages. Firstly, it provides a decentralized method to determine the number of userops in the mempool, enhancing resistance to collusion. Secondly, it eliminates the single point of failure that could occur if only one system were managing the communication between the user and the mempool. Thirdly, the shared sequencer ensures consistency and reduces discrepancies between the different L2 solutions.</p>
<p>By leveraging the shared sequencer, this method ensures a robust and reliable system for estimating and communicating the state of the mempool to users, thus improving the overall efficiency and security of the process.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/6/d6c85557ef46c934ff99f11c86e081107333e050.jpeg" title="Shared Sequencer"><img alt="Shared Sequencer" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/d/6/d6c85557ef46c934ff99f11c86e081107333e050_2_486x500.jpeg" width="486" /></a></div><p></p>
<p>In the two explained approaches by using an oracle, there is a potential attack vector where an adversary could generate multiple user operations in the mempool, knowing that they will revert if aggregated together. As a result, the oracle sees that there are <span class="math">n</span>  transactions and requires a large bundle, but the bundler cannot create the bundle. This issue could stall the network for many blocks.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49870-layer2s-operate-their-own-bundler-12" name="p-49870-layer2s-operate-their-own-bundler-12"></a>Layer2s operate their own bundler</h3>
<p>In this proposal, the Layer 2 itself assumes the role of the bundler, while another entity handles the aggregation of signatures (this could be current bundler services). The process works as follows: the Layer 2 operates its own bundler, and users send their operations (userops) to the mempool. The Layer 2 selects some of these userops from the mempool and sends them “raw” to the aggregator, compensating the aggregator for aggregating the signatures. Once the aggregator produces the bundle, it sends it to the bundler, which then forwards it to the mainnet as calldata for data availability.</p>
<p>The main idea is that the Layer 2 handles the collection of userops and then outsources the aggregation to another entity. The Layer 2 pays for the aggregation and charges the user a fee for the service.</p>
<p>There are two different options:</p>
<ol>
<li>
<p><strong>Flat Fee Model:</strong> The bundler (Sequencer) selects some transactions and charges the user a flat fee. This flat fee is calculated similarly to current Layer 2 transactions, predicting the future cost of L1 data publication. Alternatively, the Layer 2 could charge the user a flat fee based on the cost of bundling <span class="math">n</span>  aggregated userops,  the layer 2 still have to predict how many transactions will be present in the bundle he will contruct to correctly quote the user, this can be made in the same way is now where the . As it is now where the l2 charge the best comeptitive price to the user that it is in the Layer 2’s best interest to keep the prices as competitive as possible for the user.<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/a/2ad21c4d7917c11d2c90e9f0f2b4e027466730f7.png" title="Flat Fee"><img alt="Flat Fee" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/2/a/2ad21c4d7917c11d2c90e9f0f2b4e027466730f7_2_477x500.png" width="477" /></a></div><p></p>
</li>
<li>
<p><strong>Requesting Refunds:</strong> If the Layer 2 wants to enhance its credibility, it could enable automatic refunds. This would involve a mechanism that checks how many userops are published in a single block and whether the transactions could have been aggregated. If a userop that could have been aggregated wasn’t, and no automatic refund was issued, the user can request a refund. In this scenario, the Layer 2 could stake some assets, and if the refund isn’t provided, the user could enforce the refund, ensuring fairness and accountability.<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/8/28b9021118bfcafbcc024f1efe0dcd3c1184c5de.png" title="Request Refund"><img alt="Request Refund" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/2/8/28b9021118bfcafbcc024f1efe0dcd3c1184c5de_2_477x500.png" width="477" /></a></div><p></p>
</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#p-49870-conclusion-13" name="p-49870-conclusion-13"></a>Conclusion</h2>
<p>In these two different posts, we outline the difficulties users experience when bidding to be included in the next bundle. In the first part, we presented the ERC-4337 model, explaining the costs a bundler incurs when posting a bundle on-chain and the associated off-chain costs. We also outlined the fee markets for bundlers and began discussing the issue of formatting the bundler. Users experience difficulties with bidding due to a lack of knowledge about the number of transactions present in the mempool at the time of bundling.</p>
<p>In the second part, we explained ERC-4337 and RIP-7560. We then discussed why signature aggregation is more likely to occur on Layer 2 solutions rather than directly on Layer 1. We demonstrated how Layer 2 solutions could address the asymmetric knowledge that users experience in different ways. The first one is to use oracles to signal to the user how many transactions are present in the mempool, with this approach the users knows how much they should bid and can force the bundler to make larger bundles. The third approach which is the simplest is that the L2 acts as a bundler and outsources the aggregation to a third party and lets the users pay a fee for it.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/embedded-fee-markets-and-erc-4337-part-2/20384">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 05 Sep 2024 10:23:10 +0000</pubDate>
</item>
<item>
<title>Timestamp Ordering in MCP for Timing Games</title>
<link>https://ethresear.ch/t/timestamp-ordering-in-mcp-for-timing-games/20372</link>
<guid>https://ethresear.ch/t/timestamp-ordering-in-mcp-for-timing-games/20372</guid>
<content:encoded><![CDATA[
<div> 关键词：MCP、MEV、POD、公平性、安全性和去中心化

总结：

文章主要探讨了在以太坊中引入多个并发提议者（MCP）机制的重要性以及其对解决现有共识问题的潜在影响。MCP旨在通过分散区块提议过程来减少单一提议者在某一时间槽中的垄断权力，从而解决由基于领导者的共识机制带来的问题，如某些交易的短期审查。

文中提出了经济秩序的多提案者模型，其中考虑了确定性区块排序，即根据交易的优先费用或执行标志对区块进行排序。然而，文章也指出了一种可能的策略游戏——利用时间游戏和抢先行为的机会，这可能导致提议者通过延迟区块提议来最大化奖励。为了应对这一问题，文章提出采用部分有序数据集（POD），一种基于客户端记录的交易时间戳的排序方法。这种机制允许在保证公平性的同时，避免了严格的交易顺序，并通过惩罚恶意操作来保护诚实节点。

POD还引入了一种新的数据结构，允许客户端与网络中的验证者交互，请求特定轮次的交易记录，并通过识别证书来确认交易的最终化。这种方法有助于实现交易的去中心化，防止中央化风险，并确保网络的安全性。

最后，文章讨论了POD在缓解最大可提取价值（MEV）问题方面的潜力，通过改变交易排序策略，限制了恶意行为，特别是前向运行和夹击攻击的可能性。同时，POD机制也引发了关于如何检测不良行为的新问题，以及如何提供更好的激励措施来促进非集中化的思考。 <div>
<blockquote>
<p>Thanks to <a class="mention" href="https://ethresear.ch/u/julian">@Julian</a> and <a href="https://twitter.com/_ddiaconescu_" rel="noopener nofollow ugc">@denisa</a> for the corrections, suggestions and discussions!</p>
</blockquote>
<p>Multiple Concurrent Proposers (MCP) has recently become a significant topic of discussion within the community, particularly following the introduction of the <a href="https://x.com/danrobinson/status/1820506643739615624" rel="noopener nofollow ugc">BRAID protocol</a> and the rise of DAG consensus. Max’s argument in favor of MCP for Ethereum centers on the monopoly created by leader-based <a href="https://ethresear.ch/t/execution-consensus-separation/19964">consensus mechanisms</a>, where the leader for a given slot is granted substantial monopolistic power. This concentration of power leads to issues such as short censorship for some transactions.</p>
<p>In leader-based consensus, the designated leader for each slot has the exclusive authority to propose blocks, which allows them to exploit their position for profit maximization, such as through transaction reordering or frontrunning. MCP aims to mitigate these issues by decentralizing the block proposal process, reducing the influence any single proposer can exert over the network during a given slot.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49846-multiple-concurrent-proposers-economic-order-1" name="p-49846-multiple-concurrent-proposers-economic-order-1"></a>Multiple Concurrent Proposers Economic Order</h1>
<p>Let <span class="math">n</span> represent the number of validators in the network. A subset of validators maintains a local chain, denoted by <span class="math"> k &lt; n</span>. The protocol at some step will need to pick the union of all local blockchains at slot <span class="math">i</span> and an ordering rule must be applied between transactions of each local chain.</p>
<p><strong>Deterministic Block Ordering</strong>: A deterministic rule is applied to order the blocks and its transactions. In the context of <a href="https://www.youtube.com/live/PhsJnEnsLN4?si=_Wd_RzjXLzgdyeaZ" rel="noopener nofollow ugc">MEV-SBC ‘24</a> event, <a href="https://www.youtube.com/watch?v=SBOGdofF4u8" rel="noopener nofollow ugc">Max proposes</a> two approaches:</p>
<ol>
<li><strong>Sorting by Priority Fee</strong>: Blocks are sorted based on the priority fee of transactions. MEV (Maximal Extractable Value) taxes can be applied, where a percentage of the priority fee is extracted and redistributed by the application. This approach is detailed in the proposal <a href="https://www.paradigm.xyz/2024/06/priority-is-all-you-need" rel="noopener nofollow ugc">“Priority is All You Need”</a>.</li>
<li><strong>Execution Flags</strong>: Transactions can set an “execution flag” that indicates specific actions, such as interacting with a particular liquidity pool (e.g., trading ETH/USDC in the UNIv5 pool). When the block ordering rule encounters a transaction with such a flag, it pulls all flagged transactions attempting to interact with that pool and executes them as a batch.</li>
</ol>
<h1><a class="anchor" href="https://ethresear.ch#p-49846-timing-games-with-frontrunning-incentive-2" name="p-49846-timing-games-with-frontrunning-incentive-2"></a>Timing Games with Frontrunning Incentive</h1>
<p>Let <span class="math">p</span> be a proposer participating in the MCP protocol, who is responsible for proposing a block in their local chain during slot <span class="math">i</span>. We acknowledge that there exists an inherent delay and processing time required to propose this block. Specifically, the protocol permits a maximum allowable delay of <span class="math">\Delta</span> time units before <span class="math">p</span> incurs penalties.</p>
<p><span class="math">p</span> may strategically opt to delay their block proposal until <span class="math">\Delta - \epsilon</span> (where <span class="math">\epsilon &gt; 0 </span>) time units. This delay enables <span class="math">p</span> to potentially exploit a frontrunning opportunity by observing and computing a partial order of transactions submitted by other proposers. By strategically placing their block proposal just before the misslot penalty (no block has been proposed and it’s no going to be accepted for slot <span class="math">i</span>), <span class="math">p</span> could include transactions with higher gas fees, a situation that provides a clear incentive for engaging in frontrunning behavior and the main incentive for playing timing games in this post.</p>
<p>Under the current deterministic protocol rules, such a timing strategy is incentivized as it allows proposers to maximize their rewards through manipulation of transaction order. This situation underscores the need for an effective mechanism. However, a more robust solution may involve revisiting the transaction ordering rules to eliminate this concrete incentive for timing games that lead to such exploitative behaviors, thereby ensuring a fairer and more secure protocol.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49846-partially-ordered-dataset-pod-3" name="p-49846-partially-ordered-dataset-pod-3"></a>Partially Ordered Dataset (POD)</h1>
<p>One of the main concerns regarding MCP is the absence of a clearly defined method for determining the order of transactions. It remains uncertain how the sequence and the underlying criteria for ordering will be established, as well as how the influence of clients will be exercised—whether through mechanisms such as auctions, latency considerations, or the risk of spam attacks, as highlighted by <a href="https://www.youtube.com/watch?v=SBOGdofF4u8" rel="noopener nofollow ugc">Phil at SBC '24</a>.</p>
<p>The team of Common Prefix has conducted a thorough <a href="https://www.commonprefix.com/static/clients/flashbots/flashbots_report.pdf" rel="noopener nofollow ugc">analysis of various consensus protocols</a>, including leader-based, inclusion list, and leaderless consensus models, with a focus on their resistance to censorship. As a result of their research, they developed the concept of a Partially Ordered Dataset. In this model, the order of transactions is determined by the timestamps recorded by the clients, which may lead to a lack of strict ordering when two transactions are recorded simultaneously. The implications of relinquishing strict ordering in transaction processing have not been extensively explored in the existing literature, or at least, I am not aware of any comprehensive studies on the matter.</p>
<p>A POD is a finite sequence of pair <span class="math">\{(r, T), …, (r’, T’)\}</span> s.t. <span class="math">r</span> is round (slot) and <span class="math">T</span> a set of transactions.</p>
<p>A round is perfect <span class="math">r_{perf}</span> if no new transactions can appear with recorded round <span class="math">r_{rec} \leq r_{perf}</span>, which means there is no conflict in the ordering before <span class="math">r_{perf}</span>.</p>
<p>A <strong>POD protocol</strong> exposes the following methods.</p>
<ul>
<li>input event <code>write(tx)</code> : Clients call <code>write(tx)</code> to write a transaction <code>tx</code> .</li>
<li>output event <code>write_return(tx, π)</code> : after <code>write(tx)</code> the protocol outputs <code>write_return(tx, π)</code>, where <code>π</code> is a record certificate.</li>
<li>input event <code>read_perfect()</code>: Clients call <code>read_perfect()</code> to read the transactions in the bulletin.</li>
<li>output event <code>read_perfect_return(r, D, Π)</code> : after <code>read_perfect()</code> protocol outputs <code>read_perfect_return(r, D, Π)</code>, where <code>r</code> is a round, called the past perfect round, <code>L</code> is a set of transactions, <code>D</code> is a POD, and <code>Π</code> is a past-perfect certificate. For each entry <code>(r', T)</code> in <code>D</code>, we say that transactions in <code>T</code> became finalized at round <code>r'</code>.</li>
<li>input event <code>read_all()</code> : returns all transactions up to the current round without past-perfection guarantees, hence it can return faster than <code>read_perfect()</code>.</li>
<li>output event <code>read_all_return(D, Π)</code></li>
<li><code>identify(π, Π) → P' ⊆ P</code> : Clients call <code>identify(π, Π) → P' ⊆ P</code> to identify the set <code>P'</code> of parties who vouched for the finalization of a transaction, where <code>Π</code> is a POD and <code>π</code> is the certificate returned by <code>write_return(tx, π)</code>.</li>
</ul>
<p>The properties of Liveness and Security are detailed in the original work, and the following will be utilized in subsequent arguments:</p>
<p>Fair punishment: No honest replica gets punished as a result of malicious operation. If <code>identify(π, Π) → P'</code>, where <code>π</code> is a record certificate for transaction <code>tx</code> and <code>Π</code> is a past-perfect certificate for a POD <code>D</code>, can only be created if all parties in <code>P'</code> sign <code>tx</code> and <code>D</code>.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/4/74ea9a2fbbcf46cdb5da2f6898d698da4c404c6a.png" title="Construction of Partially Ordered Datasets"><img alt="Construction of Partially Ordered Datasets" height="330" src="https://ethresear.ch/uploads/default/optimized/3X/7/4/74ea9a2fbbcf46cdb5da2f6898d698da4c404c6a_2_690x330.png" width="690" /></a></div><p></p>
<p>The construction of the POD is as follows: The client will send a transaction to all the validators in the network and will have to wait for <span class="math">n - f</span> signatures to confirm his transaction has been received by the network, where <span class="math">f</span> is the amount of allowed byzantine validators. Once the client received the signature he will record the median of all the signatures he has received, as there is going to be some latency and difference between the validators when they received the transaction.</p>
<p>For the reading set of transactions for some round the client will have two options:</p>
<ul>
<li>Believe in synchrony on the txs received: Request all the recorded transactions from the validators for some specific round <span class="math">r</span>. Once obtains the  <span class="math">n- f</span> signatures of all the transactions computes the median of the set of transactions based on their timestamps.</li>
<li>Past-perfect guarantees, no-synchrony believer: Assume <span class="math">r_{perf}</span> to be the minimum of the received <span class="math">r</span> values, then we will not have any transaction with lower timestamp. Now takes the union of all the upcoming transactions. Now the client will have to wait some <span class="math">\delta</span> time to ensure through the gossip mechanism there is no lower <span class="math">r_{perf}</span> and no more transaction for the upcoming round.</li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#p-49846-pods-mitigating-mev-in-mcp-4" name="p-49846-pods-mitigating-mev-in-mcp-4"></a>PODs mitigating MEV in MCP</h1>
<p>Adopting Partially Ordered Datasets (PODs) as the primary data structure for MCP introduces a novel approach that hasn’t been extensively studied, particularly regarding its potential to mitigate the types of MEV games previously described.</p>
<p>In PODs, transactions are ordered deterministically based on their timestamps. While this approach necessitates handling cases where multiple transactions share the same timestamp—or evaluating the likelihood of such occurrences—it fundamentally alters the dynamics of the fronturunning incentive of timing games previously described against other proposers block transactions.</p>
<p>Consider a scenario in slot <span class="math">m</span> where a malicious proposer attempts to front-run or sandwich another transaction. Under the previous deterministic ordering, which was based on auctions and priority fees, such attacks were feasible because proposers could manipulate their position in the ordering by outbidding others or exploiting latency. However, with timestamp-based ordering as implemented in PODs, this strategy changes significantly. An open question is still to know which strategies can be applied with PODs or timestamp ordering to extract MEV and if they are worse in wellfare of the network compared with the described game.</p>
<p>In this new setup, being the last proposer in a slot would actually place that proposer in the final position within the transaction order, limiting their ability to engage in front-running or sandwiching assuming honesty in all nodes. Instead, they would only be able to perform back-running, which is generally considered less harmful than front-running or sandwiching. This shift in ordering strategy could effectively reduce the risk of these more dangerous forms of MEV exploitation.</p>
<p>If a malicious validator attempts to manipulate the order of transactions by bribing proposers, slashing should be applied to the validator. By imposing such penalties, the protocol discourages malicious behavior and ensures that the integrity of the transaction ordering process is maintained. One of the future next questions it’s how can we detect a bad behaviour in the transaction record, maybe applying Turkey’s Method it’s a posible option and assume that outliers are malicious records.</p>
<p>However, the situation is more complex than it appears. The shift to a new game for validators, where transaction ordering is influenced by latency, introduces additional challenges. Validators may now engage in latency games, where geographical proximity to other validators or network nodes becomes a crucial factor in gaining an advantage. To mitigate this, it is essential to ensure that validators are well decentralized across different regions.</p>
<p>Decentralizing validators geographically helps reduce the impact of latency-based advantages. Validators clustered in the same location could lead to centralization risks, where a few validators might dominate the network due to their low-latency connections. This centralization could undermine the fairness of transaction ordering and potentially reintroduce the risk of censorship.</p>
<p>Moreover, validators are incentivized to avoid sharing the same location because doing so decreases the uniqueness of the transactions they can access for a possible backrunning and taking such opportunities. The more validators operate from the same region, the fewer unique transactions each can capture, leading to lower profits from transaction fees, as these would have to be split among more validators. This dynamic encourages validators to spread out, fostering a more decentralized and resilient network that is better protected against latency-based games and the centralization of power. However, the current incentive is still weak and future work will reside in how to provide better incentives for non-centralization.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/timestamp-ordering-in-mcp-for-timing-games/20372">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 03 Sep 2024 08:17:06 +0000</pubDate>
</item>
<item>
<title>Interpreting MPT branch node values</title>
<link>https://ethresear.ch/t/interpreting-mpt-branch-node-values/20368</link>
<guid>https://ethresear.ch/t/interpreting-mpt-branch-node-values/20368</guid>
<content:encoded><![CDATA[
<div> 关键词：MPT、分支节点、NULL、空字符串、终结节点

总结:

本文探讨了在默克尔树（MPT）中区分两种不同类型的分支节点情况。第一种情况是分支节点被视为非“终结”节点，其第17项列表应为空（NULL），表示该节点继续指向其他子节点。而第二种情况则是当分支节点作为终结节点使用时，第17项列表实际上包含了数据值，即使这个值恰好是空字符串。

为了区分这两种不同的节点类型，关键在于理解它们在列表中的预期内容。非终结节点的列表应该在该位置上留有空位或NULL表示后续数据的存在，而终结节点则会填充实际的数据值。尽管在随后的RLP编码过程中，NULL会被表示为空字符串，但原始列表中NULL和非NULL（即使是空字符串）的区别是本质的。因此，识别节点类型的关键在于查看列表本身的结构而非其编码后的形式。 <div>
<p>Consider a branch node for an MPT.<br />
Suppose the 17’th item in the branch node list is supposed to be NULL, because the branch node is not a “terminator” node. Ethereum documentation says NULL is encoded as the empty string.<br />
Suppose the 17’th item in the list is supposed to be a value because the branch node is a terminator node. Suppose this value happens to be the empty string.<br />
How to distinguish these two cases?<br />
Note this question should be independent of RLP encoding, which only concerns how we encode the list. I’m asking what’s in the list itself, before considering how the list is subsequently encoded.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/interpreting-mpt-branch-node-values/20368">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sat, 31 Aug 2024 14:57:04 +0000</pubDate>
</item>
<item>
<title>Exploring Verifiable Continuous Sequencing with Delay Functions</title>
<link>https://ethresear.ch/t/exploring-verifiable-continuous-sequencing-with-delay-functions/20362</link>
<guid>https://ethresear.ch/t/exploring-verifiable-continuous-sequencing-with-delay-functions/20362</guid>
<content:encoded><![CDATA[
<div> 关键词：Ethereum、Verifiable Delay Functions（VDF）、Transaction Deadlines、Ethereum作为全球时钟、Sequencer

总结：

文章探讨了在去中心化的设置中，如何解决时间同步的问题，特别是在不可信的第二层（L2）序列器环境中。在这样的环境下，墙上的时钟可能在不同机器间漂移，代理可能会谎报本地时间，而且很难区分恶意行为与不一致的时钟或网络延迟。以太坊被视为一个每约12秒跳动一次的全球时钟，其共识协议软性强制规定，过早或过晚生成的区块和验证不会被认为是有效的。

文章提出了一种机制，用于在去中心化汇总中实现对序列器的及时性、安全性和非剥削性排序的强制执行，而无需额外的共识协议、诚实多数假设或利他主义。这种机制包括三个关键原语：

1. 客户端优先级交易顺序，
2. 以太坊作为具有12秒周期的全球时钟，
3. 可验证延迟函数（VDF）。

通过引入交易截止日期、利用以太坊作为时间基准以及使用VDF，文章提供了一种方法来确保序列器遵守时间规定，同时防止提取用户价值的行为（如夹击攻击），并在分布式环境中保持系统的可靠性和安全性。文章还展示了MR-MEV-Boost案例研究，这是一种对MEV-Boost的修改，允许基于多个轮次的预确认交易，同时应用了上述构建来减少提案者的时机游戏行为。 <div>
<p><em>Thanks to Conor, Lin and Swapnil from the Switchboard team, Cecilia and Brecht from the Taiko team, Alex Obadia, Justin Drake, Artem Kotelskiy and the Chainbound team for review.</em></p>
<h2><a class="anchor" href="https://ethresear.ch#p-49826-abstract-1" name="p-49826-abstract-1"></a>Abstract</h2>
<p>Agreeing on time in a decentralized setting can be challenging: wall clocks may drift between machines, agents can lie about their local times, and it is generally hard to distinguish between malicious intent and just unsynchronized clocks or network latencies.</p>
<p>Ethereum can be thought of as a global clock that ticks at a rate of 1 tick per ~12 seconds. This tick rate is soft-enforced by the consensus protocol: blocks and attestations produced too early or too late will not be considered valid. But what should we do in order to achieve a granularity lower than 12 seconds? Do we always require a consensus protocol to keep track of time?</p>
<p>We want to explore these questions in the context of untrusted L2 sequencers, who don’t have any incentive to follow the L2 block schedule that is currently maintained by trusted L2 sequencers, and will likely play various forms of timing games in order to maximize their revenue.</p>
<p>In this article, we introduce mechanisms to enforce the timeliness, safety and non-extractive ordering of sequencers in a decentralized rollup featuring a <strong>rotating leader mechanism</strong>, without relying on additional consensus, honest majority assumptions or altruism. To do so, we use three key primitives:</p>
<ol>
<li>Client-side ordering preferences,</li>
<li>Ethereum as a global 12s-tick clock,</li>
<li>Verifiable Delay Functions.</li>
</ol>
<p>Lastly, we show the case study of MR-MEV-Boost, a modification of MEV-Boost that enables a variation of based preconfirmations, where the same construction explored can be applied to reduce the timing games of the proposer.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49826-rationale-2" name="p-49826-rationale-2"></a>Rationale</h2>
<p>Rollup sequencers are entities responsible for ordering (and in most cases, executing) L2 transactions and occasionally updating the L2 state root on the L1. Currently, centralized sequencers benefit from the reputational collateral of the teams building them to maintain five properties:</p>
<ul>
<li><strong>Responsiveness</strong>: responding to user transactions with soft commitments / preconfirmations in a <em>timely</em> manner. We want to highlight that this definition includes the timely broadcast of unsafe heads on the rollup peer-to-peer network.</li>
<li><strong>Non-equivocation (safety)</strong>: adhering to preconfirmation promises when submitting the ordered batch on the L1, which is what will ultimately determine the total ordering of transactions.</li>
<li><strong>Non-extractive ordering</strong>: not extracting MEV from users by front-running or sandwiching, or by accepting bribes for front-running privileges.</li>
<li><strong>Liveness</strong>: posting batches to L1 and updating the canonical rollup state regularly.</li>
<li><strong>Censorship-resistance:</strong> ensuring that no valid transactions are deliberately excluded by the sequencer regardless of the sender, content, or any external factors.</li>
</ul>
<p>In this piece we are concerned with how the first four properties can be maintained in a permissionless, untrusted setting. Note that censorship-resistance is ensured by construction: by introducing multiple organizationally distinct sequencers in different geographies and jurisdictions we have a strong guarantee that any transaction will be accepted eventually.</p>
<p>Consider a decentralized sequencer set <span class="math">S := \{S_1,\dots,S_n\}</span>  with a predictable leader rotation mechanism and a sequencing window corresponding to a known amount of L1 slots. For simplicity, let’s assume <span class="math">S_{i}</span> is the current leader and <span class="math">S_{i+1}</span> is the next one. At any point in time, only one sequencer is active and has a lock over the rollup state.</p>
<p>Here are two strategies that sequencer <span class="math">S_i</span> can explore to maximize its expected value:</p>
<p><strong>1. Delaying the inclusion of transactions</strong></p>
<p>Suppose a user sends a transaction to <span class="math">S_i</span> at a certain L2 slot. Then, the sequencer could wait some time before inserting the transaction into a block in order to extract more MEV with sandwich attacks in collaboration with searchers or by directly front-running the user. In particular, <a href="https://www.youtube.com/watch?v=01dnINiLhAk&amp;t=287s" rel="noopener nofollow ugc">since MEV grows superlinearly with time</a>, it’s not in the sequencer’s best interest to commit early to a transaction. The worst case scenario would be the sequencer delaying inclusion until the sequencer rotation <span class="math">^1</span>.</p>
<p><strong>2. Not publishing unsafe heads in the rollup peer-to-peer network</strong></p>
<p>In this setting the sequencer has low incentives to publish the unsafe heads in the rollup network: since L2 blocks are signed by the sequencer (e.g. in <a href="https://docs.optimism.io/builders/node-operators/configuration/consensus-config#p2psequencerkey" rel="noopener nofollow ugc">Optimism</a>), they act as a binding commitment which can be used by users to slash it in case of equivocations.</p>
<p>This has a major downstream consequence on the UX of the rollup: both the next sequencer and users need to wait until a batch is included to see the latest transactions. For users it means they won’t know the status of their transactions in a timely manner, while the next sequencers risks building blocks on invalid state.</p>
<p>We will now explore mechanisms to mitigate these behaviours and introduce slashing conditions for sequencers.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49826-primitive-1-transaction-deadlines-3" name="p-49826-primitive-1-transaction-deadlines-3"></a>Primitive 1: Transaction Deadlines</h2>
<p>We introduce a new EIP-2718 transaction type with an additional field:</p>
<ul>
<li><code>deadline</code> - <code>uint256</code> indicating the last L2 block number for which the transaction is considered valid.</li>
</ul>
<p>This idea is not entirely new. For instance, the <a href="https://limechain.tech/" rel="noopener nofollow ugc">LimeChain</a> team has explored this in their <a href="https://github.com/LimeChain/based-preconfirmations-research/blob/cfc3830c685965fad5e5843533c5586dcb92e873/docs/preconfirmations-for-vanilla-based-rollups.md#preconfirmation-deadline" rel="noopener nofollow ugc">Vanilla Based Sequencing</a> article. However, in our variant the <code>deadline</code> field is signed as part of the transaction payload and it is not expressed in L1 slots.</p>
<p>The reasoning behind it is that the sequencer cannot tamper with either the <code>deadline</code> field or <code>block.number</code> (because it is a monotonically increasing counter), and therefore it is easy to modify the L2 derivation pipeline to attribute a fault in case the sequencer inserts the user transaction in a block where <code>block.number &gt; deadline</code>.</p>
<p>This approach mitigates problem <span class="hashtag-raw">#1</span>. However, it does not in any way solve the <em>responsiveness</em> issue, since sequencers can still delay proposing the block in order to extract more MEV.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49826-primitive-2-ethereum-as-a-global-clock-4" name="p-49826-primitive-2-ethereum-as-a-global-clock-4"></a>Primitive 2: Ethereum as a Global Clock</h2>
<p>A simple rotating sequencer design would be one where <span class="math">S_i</span> loses the power to settle batches after the end of its sequencing window <span class="math">W_i</span>, which is dictated by an L1 smart contract. However, the sequencer still needs some time to post the batch with the latest L2 blocks. We therefore introduce an <em>inclusion window</em> that is shifted <span class="math">n \geq 1</span> slots ahead of <span class="math">W_i</span>, where <span class="math">S_i</span> still has time to land rollup batches on L1 with the last L2 blocks, even if the responsibility of sequencing has shifted to <span class="math">S_{i+1}</span>.</p>
<p>In case of any safety fault, the sequencer should be slashed. If the sequencer has not managed to post all their assigned L2 blocks by the end of its inclusion window, it will forego all associated rewards. Optionally, there could also be penalties for liveness faults. This also helps with the problem of collaboration with the next sequencer, by ensuring that the latest blocks will be known to it within <span class="math">n\cdot12</span> seconds. Ideally, we’d like to keep <span class="math">n</span> as small as possible with a value of <span class="math">1</span>.</p>
<p>There are still some potential issues here: getting a transaction included on Ethereum is probabilistic, meaning that you can’t be sure that a transaction you send will actually be included in time. In this context it means that the last batch sent by an honest leader may not be included in the L1 by the end of its inclusion window. This can be helped with two approaches:</p>
<ul>
<li>A “based” setup, where the sequencer is also the L1 block proposer and can include any transactions right up to the point they have to propose, or</li>
<li>Using proposer commitments with a protocol like <a href="https://boltprotocol.xyz" rel="noopener nofollow ugc">Bolt</a>. We expand more on this in the <em>”Further work”</em> section below.</li>
</ul>
<p>Note that we assume there is a registry smart contract that can be consulted for the currently active sequencer, i.e. it implements some leader election mechanism and takes care of sequencer bonds along with rewards and penalties. It is up to the rollup governance to decide whether the registry can be fully permissionless or if it should use an allowlist. In case of any misbehaviour, governance would be used to temporarily or permanently remove the sequencer from the allowlist.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49826-primitive-3-verifiable-delay-functions-5" name="p-49826-primitive-3-verifiable-delay-functions-5"></a>Primitive 3: Verifiable Delay Functions</h2>
<p><a href="https://medium.com/iovlabs-innovation-stories/verifiable-delay-functions-8eb6390c5f4" rel="noopener nofollow ugc">Verifiable Delay Functions</a> (VDFs henceforth) are a cryptographic primitive that allows a prover to show a verifier that a certain amount of time was spent running a function, and do it in a way that the verifier can check the result quickly.</p>
<p>For instance, consider a cryptographic hash function <span class="math">h</span> and define the application</p>
<div class="math">
H(n,s) := (h \circ \underset{n\ times}\dots \circ h)(s),
</div>
<p>where <span class="math">s</span> is a byte array an <span class="math">n</span> is a natural number.</p>
<p>Composing (or chaining) hash functions like SHA-256 cannot be trivially sped up using parallel computations, but the solution lacks efficient verification <span class="math">^2</span> as the only way to verify the result is to recompute the composition of functions. This solution appeared as a naïve VDF in <a href="https://eprint.iacr.org/2018/601.pdf" rel="noopener nofollow ugc">Boneh’s paper</a>, and for this reason it is referred to as <em>weak</em>.</p>
<p>Another example of VDF is <a href="https://people.csail.mit.edu/rivest/pubs/RSW96.pdf" rel="noopener nofollow ugc">iterated squaring over a group of hidden order</a>, with which it is possible to construct time-lock puzzles. We’ll explore the usage of the latter in the next sections.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49826-why-vdfs-tho-6" name="p-49826-why-vdfs-tho-6"></a>Why VDFs tho?</h3>
<p>VDFs are very useful in the context of sequencing because they can act as a <em>proof of elapsed time</em> for the duration of the block (specifically <code>block_time</code> / <code>max_adversary_speedup</code>, see <em>“Security Considerations”</em>). Consider the following algorithm for the block production pipeline:</p>
<ol>
<li>At the beginning of L2 block <span class="math">N</span>, the sequencer starts computing a VDF that takes an L2 block time (or slightly less) to compute for honest players, using the previous block hash as its input.</li>
<li>After the end of the L2 slot the sequencer builds a block <span class="math">B_N</span> where the header contains the result of the VDF, denoted <span class="math">V_N</span>. We call this <em>sealing</em> a block. This means the block hash digest contains <span class="math">V_N</span>.</li>
</ol>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/8/6802ae3a20554489b1de7ccb7a9ecda502a79c39.png" title=""><img alt="" height="317" src="https://ethresear.ch/uploads/default/optimized/3X/6/8/6802ae3a20554489b1de7ccb7a9ecda502a79c39_2_690x317.png" width="690" /></a></div><p></p>
<p>This algorithm has the nice property of creating a chain of VDF computations, in some sense analogous to <a href="https://solana.com/news/proof-of-history" rel="noopener nofollow ugc">Solana’s Proof of History</a> from which we inherit the security guarantees. What does this give us in the sequencer context? If we remember that a sequencer has a certain deadline by which it has to post batches set by the L1 slot schedule, we can have the L1 enforce that <em>at least</em> some number of L2 blocks need to be settled. This has two downstream results:</p>
<ul>
<li>The sequencer <em>must</em> start producing and sealing blocks as soon as their sequencing window starts. Pairing this with the transaction deadline property results in an upper bound of time for when a transaction can be confirmed. If they don’t follow the block schedule set by the VDF and the L1, they risk not being able to post <em>any</em> batch.</li>
<li>We mitigate problem <span class="hashtag-raw">#2</span> by taking away the incentive to withhold data (not considering pure griefing attacks): this is because the sequencer cannot tamper with an existing VDF chain, which would require recomputing all the subsequent VDFs and result in an invalid batch.</li>
</ul>
<p>In general, for the sake of this post we will consider a generic VDF, provided as a “black box” while keeping the hash chain example in mind which currently has stronger guarantees against ad-hoc hardware such ASICs. See <em>“Security Considerations”</em> below for more insights.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49826-proving-correct-vdfs-7" name="p-49826-proving-correct-vdfs-7"></a>Proving correct VDFs</h3>
<p>If a sequencer provides an invalid VDF in an L2 block header it should be slashed, and ideally we’d like to ensure this at settlement time. However, recalculating a long hash chain on the EVM is simply unfeasible due to gas costs.</p>
<p>How to show then that the number of iterations of the VDF is invalid? One way could be to enforce it optimistically (or at settlement, in case of ZK-rollups) by requiring a valid VDF chain output in the derivation pipeline of the rollup. In case of equivocation in an optimistic rollup the sequencer can be challenged using fraud proofs.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49826-hardware-requirements-8" name="p-49826-hardware-requirements-8"></a>Hardware requirements</h3>
<p>Since by definition VDFs cannot be sped up using parallelism, it follows that computing a VDF can be done by only using a single core of a CPU, and so it does in our block production algorithm.</p>
<p>This makes it different and way more lightweight compared to most Proof-of-Work consensus algorithms such as Bitcoin’s which requires scanning for a value such that, when hashed with SHA-256, the hash begins with a certain number of zero bits.</p>
<p>It’s also worth to note that modern CPUs are optimized to compute the SHA-256 hash function. Since 2016 Intel, starting with the <em>Goldmount</em> family of chips, is offering <a href="https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sha-extensions.html" rel="noopener nofollow ugc">SHA Extensions</a> in the <em>Core</em> and <em>Xeon</em> line-ups on selected models which introduces three new instructions specialized in computing different steps of the hash function algorithm more efficiently.</p>
<p>Lastly, <a href="https://www.man.com/single-core-stagnation-and-the-cloud" rel="noopener nofollow ugc">single-core performance has stagnated over the years</a> indicating that there is a minor benefit in investing in the latest generation of CPUs, thus lowering down the requirements of the system.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49826-case-study-mr-mev-boost-9" name="p-49826-case-study-mr-mev-boost-9"></a>Case Study: MR-MEV-Boost</h2>
<p><a href="https://ethresear.ch/t/based-preconfirmations-with-multi-round-mev-boost/20091">Multi-Round-MEV-Boost</a>, is a modification of MEV-Boost that enables based preconfirmations by running multiple rounds of MEV-Boost auctions within a single L1 slot. The usage of this primitive is to output after each round a based rollup block built by L2 block builders. As shown in the article, this approach inherits the L1 PBS pipeline and mitigates some of the negative externalities of based preconfirmations as a result.</p>
<p>Like MEV-Boost, this fork relies on the opted-in proposer to be an auctioneer which ends the sealed auction by calling the <code>getHeader</code> (<a class="inline-onebox" href="https://ethereum.github.io/builder-specs/#/Builder/getHeader" rel="noopener nofollow ugc">Builder-API</a>) endpoint of the relays. After having signed the sealed bid, the <code>getPayload</code> (<a class="inline-onebox" href="https://ethereum.github.io/builder-specs/#/Builder/submitBlindedBlock" rel="noopener nofollow ugc">Builder-API</a>) is called by the proposer to receive the actual content of the winning bid and to publish the block in the based rollup network.</p>
<p>In the original protocol, the end of the auction usually coincides with the end of the L1 slot (more precisely, <a href="https://mevboost.pics/" rel="noopener nofollow ugc">near one second after it</a>); delaying it results in a high risk of not being able to broadcast the block in time to gather all the needed attestation and forgo all its associated rewards. As such, a block time is proposed every twelve seconds with consistency, enforced by Ethereum consensus.</p>
<p>In contrast, given it consists of multiple rounds happening <em>during</em> the slot, in MR-MEV-Boost an <em>untrusted proposer is incentivized to end the auction seconds later or earlier <span class="math">^{3}</span> according to the incoming bids,</em> in order to extract more more MEV. In the worst case, MR-MEV-Boost will reflect L1 block times. Another consequence of this is an inconsistent slot time for the based rollup. This can be seen as a much more serious form of timing games.</p>
<p>In the article, the discussed possible solutions to this problem are the following:</p>
<ol>
<li>Introduce user incentives: if users determine that a proposer is misbehaving, they stop sending transactions to said proposer.</li>
<li>Introduce a committee (consensus) to attest to timeliness and maintain slot durations.</li>
</ol>
<p>We now argue that a trustless solution that strongly limits the proposer without requiring actions from the user does exist, and it leverages the same construction we used for the VDF-powered block production algorithm in the context of decentralized sequencing.</p>
<p>The construction is fairly simple and consists of computing a VDF that lasts <span class="math">x := 12/r</span> seconds, where <span class="math">r</span> is the number of rounds in an L1 slot (the L2 block time). The proposer must calculate this VDF using the previous based rollup block hash as public input and, at the end of the round, sending it along with the body of a modified <code>getPayload</code> call. The output of the VDF is then stored in the rollup block header and if invalid can result in slashing the proposer after a successful fraud proof.</p>
<p>With this approach the amount of time a proposer can delay the end of a round is limited: for instance if the first auction ended one second later then during the last round it won’t be able to provide three seconds of computation for the VDF but two, resulting in an invalid block and consequent slashing <span class="math">^4</span>. This is because in order to start computing a valid VDF, it requires the previous block hash as its input, implying a sealed block.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49826-security-considerations-10" name="p-49826-security-considerations-10"></a>Security Considerations</h2>
<p><strong>Are VDFs really safe for this purpose?</strong><br />
Suppose an adversary owns hardware which is capable of computing the VDF faster compared to the baseline of honest players <em>without getting noticed</em> (otherwise the number of iterations for the VDF is adjusted by the protocol). Then, the faster the attacker (<code>max_adversary_speedup</code>), the less our construction would constrain the space of its possible actions. In particular, the sequencer would be able to commit a bit later to blocks and be able to re-organize some of them for extracting more value.</p>
<p>However, given we don’t need the “fast proving” property, hash-chains have proven to be robust with Solana’s Proof of History and will continue to be at least in the short-term. Also, our security requirements will not be as strict as something that <a href="https://ethresear.ch/t/statement-regarding-the-public-report-on-the-analysis-of-minroot/16670">needs to be enshrined in Ethereum</a> forever.</p>
<p>Some solutions and directions to get stronger safety guarantees can be found in the <em>”Further work”</em> section below.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49826-current-limitations-11" name="p-49826-current-limitations-11"></a>Current limitations</h2>
<p><strong>Sequencer credibility</strong></p>
<p>As with many new services which leverage (re)staking, the credibility of the sequencer has an upper bound which is the amount it has staked: if a MEV opportunity exceeds that, then a rational untrusted actor would prefer to get slashed and take the MEV reward.</p>
<p><strong>Leader rotation can be a critical moment</strong></p>
<p>As discussed in the batcher and registry smart contract section, the inclusion window is shifted of one slot forward at minimum compared to the sequencing window. This is needed because of the time required to settle the last batch before rotating leader, but leaves an additional slot time of at least 12 seconds in which the sequencer has room to re-organize the last L2 blocks before publishing them on the rollup peer-to-peer network. As a consequence, liveness is harmed temporarily because <span class="math">S_{i+1}</span> might be building blocks on invalid state if it starts to sequence immediately.</p>
<p>Lastly, one additional slot might not be enough to settle a batch according to recent data on <a href="https://ethresear.ch/t/slot-inclusion-rates-and-blob-market-combinatorics/19817">slot inclusion rates for blobs</a>. This can be mitigated by leveraging new inclusion preconfirmation protocols, as explained below.</p>
<p><strong>Sequencer last-look</strong></p>
<p>Our construction makes very difficult for a sequencer to reorg a block after it has been committed to, however it doesn’t solve front-running in its entirety. In particular, the sequencer may extract value from users transactions while building the block with associated <code>deadline</code> field. A possible solution along with its limitations is explored in the section below.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49826-conclusion-12" name="p-49826-conclusion-12"></a>Conclusion</h2>
<p>In this article, we explored mechanisms to enforce the timeliness, safety, and non-extractive ordering of untrusted L2 sequencers in a decentralized rollup environment.<br />
The primitives discussed ensure that sequencers can act more predictably and fairly, mitigating issues such as transaction delays and data withholding. Moreover, these techniques can reduce trust assumptions for existing single-sequencer rollups, aligning with the concept of rollups functioning as <a href="https://vitalik.eth.limo/general/2024/06/30/epochslot.html#what-should-l2s-do" rel="noopener nofollow ugc">“servers with blockchain scaffolding”</a>. These findings provide a robust framework for the future development of decentralized, secure rollup architectures.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49826-further-work-13" name="p-49826-further-work-13"></a>Further work</h2>
<p><strong>Trusted Execution Environments (TEEs) to ensure the sequencer is not running an ASIC</strong></p>
<p>A <a href="https://en.wikipedia.org/wiki/Trusted_execution_environment" rel="noopener nofollow ugc">Trusted Execution Environment</a> is a secure area of a CPU, often called <em>enclave</em>, that helps the code and data loaded inside it be protected with respect to confidentiality and integrity.<br />
Its usage in blockchain protocols is an active area of research, with the main concerns being trusting the hardware manufacturer and the <a href="https://en.wikipedia.org/wiki/Software_Guard_Extensions" rel="noopener nofollow ugc">various vulnerabilities found in the past</a> of some implementations (here’s the <a href="https://x.com/_markel___/status/1828112469010596347" rel="noopener nofollow ugc">latest</a>).<br />
Depending on the use case these trust assumptions and vulnerabilities might be a deal-breaker. However, in our setting we just need a guarantee that the sequencer is not using specialized hardware for computing the VDF, without caring about possible leakage of confidential data from the enclave or manipulation of the wall clock / monotonic clock.</p>
<p><strong>Adapt existing anti-ASICs Proof-of-Work algorithms</strong></p>
<p>The <a href="https://www.getmonero.org/resources/about/" rel="noopener nofollow ugc">Monero</a> blockchain, launched in 2014 as a privacy and untraceable-focused alternative to Bitcoin, uses an ASIC-resistant Proof-of-Work algorithm called <a href="https://github.com/tevador/RandomX" rel="noopener nofollow ugc">RandomX</a>. Quoting their <code>README</code>:</p>
<blockquote>
<p>RandomX is a proof-of-work (PoW) algorithm that is optimized for general-purpose CPUs. RandomX uses random code execution (hence the name) together with several memory-hard techniques to minimize the efficiency advantage of specialized hardware.</p>
</blockquote>
<p>The algorithm however leverages <a href="https://github.com/tevador/RandomX/blob/102f8acf90a7649ada410de5499a7ec62e49e1da/README.md#cpu-performance" rel="noopener nofollow ugc">some degree of parallelism</a>; it is an interesting research direction whether it can adapted into a single-core version, leading to a new weak-VDF.<br />
This approach, while orthogonal to using a TEE, can potentially achieve the same result which is having a guarantee that the sequencer is not using sophisticated hardware.</p>
<p><strong>Time-lock puzzles to prevent front-running</strong></p>
<p>As mentioned in the <em>“Current limitations”</em> section, our construction doesn’t limit the problem of sequencer front-running the users. Luckily, this can be solved by requiring users to encrypt sensitive transactions using <a href="https://people.csail.mit.edu/rivest/pubs/RSW96.pdf" rel="noopener nofollow ugc">time-lock puzzles</a>, as we will show in more detail in a separate piece. However, this solution doesn’t come free: encrypted transactions or encrypted mempools can incentive spamming and statistical arbitrage, <a href="https://collective.flashbots.net/t/it-s-time-to-talk-about-l2-mev/3593" rel="noopener nofollow ugc">especially when the protocol fees are not very high</a>.</p>
<p><strong>Inclusion Preconfirmations and Data Availability layers</strong></p>
<p>Batch submissions to an L1 contract could be made more efficient by leveraging some of the new preconfirmations protocol like <a href="https://boltprotocol.xyz" rel="noopener nofollow ugc">Bolt</a> by Chainbound or <a href="https://docs.primev.xyz/concepts/what-is-mev-commit" rel="noopener nofollow ugc">MEV-Commit</a> by Primev to have guaranteed inclusion in the same slot. In particular, sequencing windows should end precisely in the slot before one where the proposer is running the aforementioned protocols in order to leverage inclusion commitments.</p>
<p>Additionally, the batch could be posted into an efficient and lightweight Data Availability layer run by proposers to enforce a deadline of a configurable amount of seconds in the beginning of the slot, otherwise the sequencer would be slashed.</p>
<p><img alt="" height="190" src="https://ethresear.ch/uploads/default/original/3X/b/e/bed5956f14947f6e30a081e3064cd2a196897c95.png" width="328" /></p>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#p-49826-footnotes-14" name="p-49826-footnotes-14"></a>Footnotes</h2>
<ol>
<li>More precisely, if an operator controls multiple subsequent sequencers it could delay inclusion until the last sequencer rotation.</li>
<li>In Solana, the verification of a SHA-256 chain is actually parallelised but requires dividing a block associated to a ~400ms computation into 32 shreds which are forwarded to the rest of the validators as soon as they’re computed. As such, verification is sped up by computing the intermediate steps of the hash chain in parallel.</li>
<li>In general, the proposer will end some rounds earlier as a side effect of delaying other rounds. For example, it could force a longer last round to leverage possible L1 &lt;&gt; L2 arbitrage opportunities.</li>
<li>There is an edge case where the proposer might not be able to compute all the VDFs even if honest, and it is due to the rotation mechanism: since the public input of the VDF must be the previous rollup block hash, during rotation the next leader will need some time before hearing the block from the rollup network, potentially more than 1s. This could lead the next proposer to be late in computing the VDFs.<br />
To reduce this risk, the next proposer could rely on various parties to receive this information such as streaming services and/or trusted relays.</li>
</ol>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/exploring-verifiable-continuous-sequencing-with-delay-functions/20362">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 30 Aug 2024 14:29:49 +0000</pubDate>
</item>
<item>
<title>PeerDas Documentation</title>
<link>https://ethresear.ch/t/peerdas-documentation/20361</link>
<guid>https://ethresear.ch/t/peerdas-documentation/20361</guid>
<content:encoded><![CDATA[
<div> 关键词：PeerDAS、数据可用性、协议整合、安全保证、加密社区

总结:

这篇文章的主要内容围绕着PeerDAS——即将被集成到以太坊共识层中的数据可用性解决方案。其核心目标是向加密社区清晰地阐述PeerDAS所使用的加密技术，以此激发新的创新与改进，同时明确阐述PeerDAS的安全和效率保证。

文章首先强调了理解PeerDAS安全性的重要性，特别是随着它即将成为以太坊共识层的一部分。为了实现这一目标，文章提供了一个易于加密社区理解的PeerDAS加密技术描述，旨在为未来可能的改进奠定基础。

接下来，文章提出了一个关键的定理（定理1），即在可接受的加密假设下，PeerDAS被认为是一个在代数群模型中符合特定定义的数据可用性采样方案。这个定理是文章关于PeerDAS安全性的主要声明，表明了其在理论上是安全可靠的。

最后，文章邀请加密社区对文档提出反馈，以便进一步优化和改进，体现了作者对社区合作和持续发展的重视。这表明PeerDAS的发展不仅基于当前的技术理解，还考虑到了未来的适应性和改进空间。 <div>
<p>Joint work with <a class="mention" href="https://ethresear.ch/u/b-wagn">@b-wagn</a>, <a href="https://eprint.iacr.org/2024/1362.pdf" rel="noopener nofollow ugc">A Documentation of Ethereum’s PeerDAS</a></p>
<p>The long-term vision of the Ethereum community includes a comprehensive data availability protocol using polynomial commitments and tensor codes. As the next step towards this vision, an intermediate solution called PeerDAS is about to integrated, to bridge the way to the full protocol. With PeerDAS soon becoming an integral part of Ethereum’s consensus layer, understanding its security guarantees is essential.</p>
<p>The linked document aims to describe the cryptography used in PeerDAS in a manner accessible to the cryptographic community, encouraging innovation and improvements, and to explicitly state the security guarantees of PeerDAS. We focus on PeerDAS as described in Ethereum’s consensus specifications [<a href="https://github.com/ethereum/consensus-specs/commit/54093964c95fbd2e48be5de672e3baae8531a964" rel="noopener nofollow ugc">Eth24a</a>, <a href="https://github.com/ethereum/consensus-specs/tree/dev/specs/_features/eip7594" rel="noopener nofollow ugc">Eth24b</a>].</p>
<p>Our intention is two-fold: first, we aim to provide a description of the cryptography used in PeerDAS that is accessible to the cryptographic community, potentially leading to new ideas and<br />
improvements that can be incorporated in the future. Second, we want to explicitly state the security and efficiency guarantees of PeerDAS. In terms of security, this document justifies the following claim:<br />
<strong>Theorem 1</strong> (Main Theorem, Informal): <em>Assuming plausible cryptographic hardness assumptions, PeerDAS is a secure data availability sampling scheme in the algebraic group model, according to the definition in [<a href="https://eprint.iacr.org/2023/1079" rel="noopener nofollow ugc">HASW23</a>].</em></p>
<p>We hope to receive feedback from the community to make further improvements to this document</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/peerdas-documentation/20361">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 30 Aug 2024 12:56:35 +0000</pubDate>
</item>
<item>
<title>Accessible Encryption for Ethereum Rollups with Fairomon</title>
<link>https://ethresear.ch/t/accessible-encryption-for-ethereum-rollups-with-fairomon/20349</link>
<guid>https://ethresear.ch/t/accessible-encryption-for-ethereum-rollups-with-fairomon/20349</guid>
<content:encoded><![CDATA[
<div> 关键词：Fairomon、Monomer、Fairblock、FairyRing、MPC加密

总结:

本文介绍了Fairomon，一种结合了Fairblock和Monomer技术的特别精灵型宝可梦。Fairomon旨在通过Fairblock提供的阈值多点计算（MPC）加密功能，为Monomer构建的以太坊卷积提供内置加密解决方案。以下是Fairomon的主要工作原理和可能的应用：

1. **FairyRing与密钥生成**：FairyRing使用去中心化的密钥生成方式，为每个周期（每100个区块）生成主秘密密钥（MSK）。从MSK中派生出主公钥（MPK），并将其传输至Monomer链用于加密请求的交易。同时，MSK被分割成参与网络的FairyRing验证者的等份。

2. **阈值身份基加密（IBE）**：Fairomon支持阈值IBE，允许用户或开发人员根据特定条件（如区块高度、资产价格、智能合约调用、ZK证明验证或治理投票结束）编程解密交易。这种解密可以通过“ID”触发，这些“ID”可以是链上条件、链上/下链标识符或属性，某些钱包可以证明其所有权。

3. **安全应用可能性**：MPC加密使许多以前无法在卷积中实现的应用成为可能，包括加密内存池、抗审查排序、以及DeFi和游戏应用中的加密订单、无领袖NFT拍卖、身份受限内容和最高手赢的纸牌游戏（如黑杰克）。

4. **交易流程**：用户将加密交易和解密条件提交给应用程序，链接收加密交易并在内存池中处理。交易按照目标高度排序，并在x/pep模块内部对区块内的顺序进行承诺。当达到目标高度或解密条件时，应用程序链接收来自Fairyring链的解密密钥，然后对加密交易进行解密并执行。

5. **架构集成**：Fairomon通过与Monomer链的集成展示了其工作流程，确保了加密交易的安全性和功能性，同时利用了Monomer的OP堆栈和ABCI接口优势。

Fairomon和Fairblock的结合为以太坊生态系统提供了强大的加密功能，使得敏感数据处理更加安全可靠，同时为开发者打开了创新应用的大门。 <div>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/f/5fa5f78d2b4b44708e03133cf29d1de097113e36.jpeg" title="690x435"><img alt="690x435" height="435" src="https://ethresear.ch/uploads/default/optimized/3X/5/f/5fa5f78d2b4b44708e03133cf29d1de097113e36_2_690x435.jpeg" width="690" /></a></div><p></p>
<p>Co-authored by <a class="mention" href="https://ethresear.ch/u/pememoni">@pememoni</a> and <a class="mention" href="https://ethresear.ch/u/shakeshack">@shakeshack</a>. With special thanks to the rest of the Fairblock team!</p>
<p>Fairomon is a special fairy type pokemon that combines the work of Fairblock and Monomer - a framework that enables builders to create Ethereum rollups with built-in encryption with minimal lift.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49798-background-1" name="p-49798-background-1"></a>Background</h1>
<p>Monomer is a rollup framework that enables Cosmos SDK app chains to be deployed as rollups on Ethereum. Internally, Monomer is built on top of the OP stack relying on it for chain derivation and settlement while supporting an ABCI interface for a Cosmos SDK app chain to be deployed on top. Fairblock provides threshold MPC encryption that can be utilized in Monomer rollups through a module built for Cosmos SDK chains.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/3/2311821ac2a3b134e2df081bbf12f2d71f2c31cc.png" title="451x500"><img alt="451x500" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/2/3/2311821ac2a3b134e2df081bbf12f2d71f2c31cc_2_451x500.png" width="451" /></a></div><p></p>
<p>Fairblock enables blockchain developers to integrate pre-execution encryption. This pre-execution encryption is made possible through their threshold MPC network that delivers identity-based encryption (IBE), and soon custom encryption schemes, to partner chains. Fairblock’s MPC network, called Fairyring, generates threshold encryption and decryption keys for each supported Monomer rollup, while the rollups themselves receive and process encrypted transactions natively.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49798-how-it-works-2" name="p-49798-how-it-works-2"></a>How it Works</h1>
<p>FairyRing uses decentralized key generation to issue a master secret key (MSK) for each epoch (every 100 blocks). From each MSK, a master public key (MPK) can be derived. Once the MPK is derived, it is relayed to a Monomer chain where it will be used to encrypt each requested transaction. In parallel, the MSK is split into equal shares for the amount of FairyRing validators participating in the network. For each request for decryption, FairyRing validators use their share of the MSK to collectively derive the associated private keys.</p>
<p>In threshold IBE, users or developers can program the decryption conditions for transactions. Onchain conditions that could trigger decryption could be a block height, the price of an asset, a smart contract call, verification of a ZK proof, or the end of a governance poll, for example. Identity-based encryption allows for the programmability of decryption and allows for decryption to be triggered by “IDs,” which can be either onchain conditions or on/offchain identifiers or attributes that certain wallets prove ownership of.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49798-whats-possible-with-fairomon-3" name="p-49798-whats-possible-with-fairomon-3"></a>What’s Possible with Fairomon</h1>
<p>MPC encryption can make a number of previously inaccessible applications possible within rollups, most notably encrypted mempools, censorship-resistant sequencing, and DeFi and gaming apps such as encrypted orders, leaderless NFT auctions, ID-gated content, and highest-hand-wins card games like blackjack.</p>
<p>The transaction flow for an application is as follows:</p>
<ul>
<li>User submits an encrypted tx and decryption condition (e.g. target height) to an app</li>
<li>Chain receives encrypted txs in mempool</li>
<li>Encrypted txs are sorted by target heights and ordering within a block is committed to inside of the integrated x/pep module</li>
<li>When target height or decryption condition is reached, the app chain receives decryption key from the Fairyring chain</li>
<li>Encrypted txs are decrypted and executed inside the BeginBlock method of the x/pep module</li>
</ul>
<p>See the architecture diagram below for a detailed description of how Fairyring integrates with a Monomer appchain.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/2/723cc342b05947059263449d26d5e63a0010c14a.png" title="690x147"><img alt="690x147" height="147" src="https://ethresear.ch/uploads/default/optimized/3X/7/2/723cc342b05947059263449d26d5e63a0010c14a_2_690x147.png" width="690" /></a></div><p></p>
<p>Monomer links:</p>
<ul>
<li><a href="https://github.com/polymerdao/monomer" rel="noopener nofollow ugc">Github</a></li>
<li><a href="https://github.com/polymerdao/monomer/tree/main/doc" rel="noopener nofollow ugc">Docs</a></li>
</ul>
<p>Fairblock links:</p>
<ul>
<li>
<p><a href="https://www.fairblock.network/" rel="noopener nofollow ugc">Website</a></p>
</li>
<li>
<p><a href="https://github.com/Fairblock" rel="noopener nofollow ugc">Github</a></p>
</li>
<li>
<p><a href="https://docs.fairblock.network/docs/basics/overview" rel="noopener nofollow ugc">Docs</a></p>
</li>
</ul>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/accessible-encryption-for-ethereum-rollups-with-fairomon/20349">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 28 Aug 2024 15:45:36 +0000</pubDate>
</item>
<item>
<title>Outdated encryption stored on blockchain</title>
<link>https://ethresear.ch/t/outdated-encryption-stored-on-blockchain/20346</link>
<guid>https://ethresear.ch/t/outdated-encryption-stored-on-blockchain/20346</guid>
<content:encoded><![CDATA[
<div> 关键词：区块链、数据存储、安全性、加密算法、SHA-1

总结:

区块链技术在医疗、建筑等领域的应用中，强调其能提供安全的数据存储。但若用于加密的算法（如SHA-1）被证明存在安全漏洞，即所谓的“被破解”，这会引发对存储在其上的数据安全性的担忧。因为一旦加密算法被破解，理论上就可以通过计算手段获取原本被加密保护的数据内容，使得原本被认为安全的数据突然间变得公开。

为避免这一情况导致的数据泄露，有几种可能的应对策略：

1. **重新加密**：最直接的方法是重新加密受影响的数据。这意味着使用新的、安全的加密算法对数据进行加密，从而将原有的加密层去除或覆盖。这样，即使原加密算法存在漏洞，新的加密层也能保护数据的安全。

2. **数据迁移**：将数据从使用不安全算法的区块链系统转移到使用更安全算法的新系统上。这种方法需要确保数据迁移过程中的安全性，防止在迁移过程中数据被窃取或损坏。

3. **更新系统配置**：对于仍在运行的系统，可以考虑更新系统配置，启用或升级到更安全的加密算法版本。这通常需要对现有系统进行一定程度的改造和测试，以确保过渡过程平稳无误。

4. **定期审计与评估**：建立一套定期对使用的加密算法进行安全审计和评估的机制，以便及时发现并应对潜在的安全威胁。这包括监控新出现的安全漏洞，以及评估现有系统的安全性。

5. **法律与合规性**：在数据保护和隐私法规日益严格的背景下，确保数据处理符合相关法律法规的要求，对于维护数据安全至关重要。这可能涉及数据加密、访问控制、备份策略等多个方面。

综上所述，面对加密算法被证明存在安全风险的情况，关键在于采取有效的措施来保护数据的安全，这包括但不限于重新加密、数据迁移、系统配置更新、定期安全审计与评估，以及遵守相关的法律与合规要求。 <div>
<p>Please pardon my ignorance. I’ve read several publications related to blockchain being used in healthcare, construction and the like. Many of these publications state that blockchain allows the storage of secured data.</p>
<p>My question is this: If data is “securely” stored on blockchain (I assume encrypted) and the encryption algorithm LATER (after long-term usage) is proven to be “cryptographically broken” (e.g., SHA-1) …</p>
<ul>
<li>does this not mean all “secured” data on the blockchain using that algorithm is suddenly public?</li>
<li>are there steps that can be taken to re-encrypt the data to avoid the massive leak of data?</li>
</ul>
<p>Kind regards.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/outdated-encryption-stored-on-blockchain/20346">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 28 Aug 2024 12:21:08 +0000</pubDate>
</item>
<item>
<title>Does multi-block MEV exist? Analysis of 2 years of MEV Data</title>
<link>https://ethresear.ch/t/does-multi-block-mev-exist-analysis-of-2-years-of-mev-data/20345</link>
<guid>https://ethresear.ch/t/does-multi-block-mev-exist-analysis-of-2-years-of-mev-data/20345</guid>
<content:encoded><![CDATA[
<div> 关键词：多区块MEV、MEV-Boost支付、序列长度、序列位置、自相关性

总结:

本文通过分析自合并以来的提案构建数据和MEV-Boost支付数据，旨在识别多区块MEV的模式。主要发现如下：

1. **序列频率**：观察到相同构建者提出多个连续区块的序列比随机蒙特卡罗模拟预测的要少。最长的序列为25个区块。

2. **支付增加**：对于更长连续序列的平均MEV-Boost支付增加，从单个区块的大约0.05 ETH增加到九个连续区块的大约0.08 ETH。

3. **序列位置价值**：在较长序列中，每个区块位置的平均支付略有增加，这可能表明构建者愿意为较后的区块支付更多费用以确保捕获早先准备的机会。

4. **自相关性**：MEV-Boost支付之间存在较低的自相关性，这意味着历史数据无法预测未来的MEV，也未显示出低MEV或高MEV的时期。

5. **专业化趋势**：没有证据表明构建者根据基础费用波动环境进行专业化。在当前的PBS机制下，创建多区块MEV机会存在固有风险，这可能是导致这种现象的原因之一。

通过这些发现，研究揭示了合并后多区块MEV的出现频率低于预期，支付模式与序列长度有关，但未显示系统性的多区块策略应用，以及构建者并未显示出对特定波动环境的偏好。 <div>
<h1><a class="anchor" href="https://ethresear.ch#p-49786-does-multi-block-mev-exist-analysis-of-2-years-of-mev-data-1" name="p-49786-does-multi-block-mev-exist-analysis-of-2-years-of-mev-data-1"></a>Does multi-block MEV exist? Analysis of 2 years of MEV Data</h1>
<p><em>by <a href="https://x.com/pascalstichler" rel="noopener nofollow ugc">Pascal Stichler</a> (<a href="https://www.ephema.io/" rel="noopener nofollow ugc">ephema labs</a>)</em></p>
<p><em>Many thanks to <a href="https://x.com/nero_eth" rel="noopener nofollow ugc">Toni</a>, <a href="https://x.com/_julianma" rel="noopener nofollow ugc">Julian</a>, <a href="https://x.com/sui414" rel="noopener nofollow ugc">Danning</a> and <a href="https://x.com/marc_nitzsche" rel="noopener nofollow ugc">Marc</a> for feedback and especially to <a href="https://x.com/barnabemonnot" rel="noopener nofollow ugc">Barnabé</a> for nudging the research in the first place and continuous feedback.</em></p>
<h2><a class="anchor" href="https://ethresear.ch#p-49786-tldr-2" name="p-49786-tldr-2"></a><em>TL;DR</em></h2>
<ul>
<li>We looked at proposer-builder data and MEV-Boost payment data since the merge (September 2022) to identify patterns of multi-block MEV.</li>
<li>We observe fewer multi-slot sequences of builders than a random Monte Carlo simulation would predict. The longest observed multi-slot sequence is 25 slots.</li>
<li>Average MEV-Boost payments increase for longer consecutive sequences by the same builder from ~0.05 ETH for single slots to ~0.08 ETH for nine consecutive slots.</li>
<li>In longer sequences, the payment per slot increases slightly with later slots. This indicates that builders bid higher to get longer sequences or the first slot after a longer sequence.</li>
<li>There is a weak positive autocorrelation between subsequent MEV-Boost payments. This contradicts the hypothesis that there are generally periods of low and high MEV.</li>
<li>Comparing builders with periods of low and high base fee volatility shows a low correlation. This indicates that no builder specialization based on base fee volatility has developed yet.</li>
</ul>
<p><em>The detailed results can be found in the Jupyter notebook on <a href="https://github.com/ephema/MEVBoost-Analysis/blob/762b7626c57cc6a1c350059b41e272a70cda49cf/%5Bephema%5D_MEV_Boost_Multi_Slot_MEV_Analysis.ipynb" rel="noopener nofollow ugc">Github </a>or <a href="https://colab.research.google.com/drive/1kKM-da6xP7St8puzPuyn1Ndag6a6wsg3?usp=sharing" rel="noopener nofollow ugc">Google Colab</a>.</em></p>
<h2><a class="anchor" href="https://ethresear.ch#p-49786-background-3" name="p-49786-background-3"></a>Background</h2>
<p>Multi-block Maximal Extractable Value (MMEV) occurs when one party controls more than one consecutive block. It was first introduced in 2021 by [<a href="https://arxiv.org/pdf/2109.04347" rel="noopener nofollow ugc">1</a>] as k-MEV and further elaborated by [<a href="https://eprint.iacr.org/2022/445.pdf" rel="noopener nofollow ugc">2</a>]. It is commonly assumed that controlling multiple slots in a sequence allows to capture significantly more MEV than controlling them individually. This derives from MEV accruing superlinearly over time. The <a href="https://collective.flashbots.net/t/multi-block-mev/457" rel="noopener nofollow ugc">most discussed</a> multi-block MEV strategies include <a href="https://eprint.iacr.org/2022/445.pdf" rel="noopener nofollow ugc">TWAP oracle manipulation attacks</a> on DEXes and producing forced liquidations by price manipulation.</p>
<p>After the merge, [<a href="https://arxiv.org/pdf/2303.04430" rel="noopener nofollow ugc">3</a>] have looked into the first four months of data on multi-block MEV and summarized it as <em>“preliminary and non-conclusive results, indicating [that] builders employ super-linear bidding strategies to secure consecutive block space"</em>.</p>
<p>With the recent Attester-Proposer-Separation (APS) and pre-confirmation discussions, multi-block MEV has become more of a pressing issue again as it might be prohibitive for some of the proposed designs (For a more in-depth overview, we’ve created a <a href="https://miro.com/app/board/uXjVK07aBCU=/?share_link_id=220296247588" rel="noopener nofollow ugc">diagram of recently proposed mechanism designs</a> and also <a href="https://x.com/mikeneuder" rel="noopener nofollow ugc">Mike Neuder</a> lately gave a <a href="https://www.youtube.com/watch?v=ToVi-zsiE4M" rel="noopener nofollow ugc">comprehensive overview</a>).</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49786-methodology-4" name="p-49786-methodology-4"></a>Methodology</h2>
<p>In order to get a better understanding of the historical prevalence of multi-block MEV, we decided to look at all slots from the Merge in September ‘22 until May ‘24 (totalling roughly 4.3 million slots) and analyze the corresponding data on validators and builders and on MEV-boost payments (if applicable). The scope was to identify patterns of unusual consecutive slot sequences and accompanying MEV values. <a href="https://mevboost.pics/data.html" rel="noopener nofollow ugc">The data</a> has been kindly provided by Toni Wahrstätter and contains information per slot on relay, builder pubkey, proposer pubkey and MEV-Boost value as well as a builder pubkey and validator pubkey mapping. In the labeling of validators for our purposes staking pool providers such as Lido or Rocket Pool are treated as one entity.</p>
<p>MEV-Boost payments are used as a proxy for the MEV per block. We acknowledge that this is only a non-perfect approximation. The ascending MEV-Boost first-price auction by its nature of being public essentially functions like a second price + 1 wei auction (thanks to Julian for pointing this out!). Hence, we strictly speaking only get an estimate of the intrinsic value of the second highest bidder. However, as [<a href="https://arxiv.org/pdf/2405.01329" rel="noopener nofollow ugc">4</a>] have observed more than 88% of MEV-Boost auctions were competitive and [<a href="https://arxiv.org/pdf/2407.13931" rel="noopener nofollow ugc">5</a>] concluded that the average profit margin per top three builder is between 1% and 5.4%, further indicating a competitive market between the top builders. Based on this, despite the limitations we deem it feasible to use the MEV-Boost payments as an approximation for the generated MEV per block.</p>
<p>To establish a baseline of expected multi-slot sequences, a Monte Carlo simulation was conducted. In this simulation, builders were randomly assigned to each slot within the specified time period, based on their observed daily market share during that period. The frequency of consecutive slots, ranging in length from 1 to 25 (the longest observed sequence in the empirical data), was recorded. This procedure was repeated 100 times, and the average was taken. We decided to use daily market shares for the main analysis as in the investigated time period market shares have strongly shifted [4]. For comparison we also ran the analysis on monthly and overall market shares.</p>
<p>Further, base fee volatility data has been included to cross-check effects of low and high-volatility periods. Previous research (e.g. [<a href="https://arxiv.org/pdf/2305.19150" rel="noopener nofollow ugc">6</a>] &amp; [<a href="https://arxiv.org/pdf/2401.01622" rel="noopener nofollow ugc">7</a>]) has focused on token price volatility effects based on CEX-prices. As we are interested in low- and high-MEV environments, we deem base fee volatility for our use case more fitting, as it is driven by empty or full blocks which are at least partially a result of the prevalence of MEV opportunities.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49786-empirical-findings-5" name="p-49786-empirical-findings-5"></a>Empirical Findings</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-49786-finding-1-fewer-multi-slot-sequences-exist-than-assumed-by-random-distribution-6" name="p-49786-finding-1-fewer-multi-slot-sequences-exist-than-assumed-by-random-distribution-6"></a>Finding 1: Fewer multi-slot sequences exist than assumed by random distribution</h3>
<p><strong></strong></p><div class="lightbox-wrapper"><strong><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/6/b6ab4921507e70c619d5121b5abf611e67e2138f.png" title=""><img alt="" height="426" src="https://ethresear.ch/uploads/default/optimized/3X/b/6/b6ab4921507e70c619d5121b5abf611e67e2138f_2_533x426.png" width="533" /></a></strong></div><br />
<em>Figure 1: Comparison of statistically expected vs. observed multi-slot sequences (note that slots &gt; 25 have been summarized in slot 25 for brevity)</em><p></p>
<p>Firstly, the prevalence of multi-slot sequences with the same builder proposing the block was investigated to determine if they are more common than would be expected by chance.</p>
<p>Comparing the results of the Monte Carlo simulation as a baseline in expected distribution (blue) with the observed distribution (orange), it can be seen that significantly fewer multi-slot sequences occur than expected (Figure 1). The longest observed sequence was 25 slots and the longest sequence with the same validator (Lido) and builder (BeaverBuild) was 11 consecutive slots on March 4th, 2024 (more details with descriptive statistics in the <a href="https://colab.research.google.com/drive/1kKM-da6xP7St8puzPuyn1Ndag6a6wsg3#scrollTo=5bje4mIWzELq" rel="noopener nofollow ugc">notebook</a>). Running the same simulation on monthly or total market shares in the time period, the observation shifts to having more longer sequences than expected, however we attribute this to the statistical effect of changing market shares. A detailed analysis can be run in the <a href="https://colab.research.google.com/drive/1kKM-da6xP7St8puzPuyn1Ndag6a6wsg3#scrollTo=mz4CTqCQInTv" rel="noopener nofollow ugc">notebook</a> or be provided upon request.</p>
<p>In the next step, to understand this in a more-fine-grained manner, the values are compared for each of the top 10 builders based on market shares. Therefore, for each builder, the difference between expected and observed occurrences of multi-slot sequences are plotted with the size of the bubble indicating the delta in Figure 2. The expected occurrences are based on the results of the Monte Carlo simulation. Red bubbles indicate a positive deviation (more observed slots than expected), while blue indicates a negative deviation. Green dots indicate values in line with the expectation. In Figure 2 it is shown in absolute numbers, in the <a href="https://colab.research.google.com/drive/1kKM-da6xP7St8puzPuyn1Ndag6a6wsg3#scrollTo=cd07f078-f646-450c-b610-9e91012111f2&amp;line=3&amp;uniqifier=1" rel="noopener nofollow ugc">notebook</a> it can also be seen on a relative scale.</p>
<p><strong></strong></p><div class="lightbox-wrapper"><strong><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/9/e94454973c4bf844c96e0a1735409a130a0983dd.png" title=""><img alt="" height="419" src="https://ethresear.ch/uploads/default/optimized/3X/e/9/e94454973c4bf844c96e0a1735409a130a0983dd_2_628x419.png" width="628" /></a></strong></div><br />
<em>Image 2: Deviations between expected (Monte Carlo simulation) and observed multi-slot frequencies per builder</em><p></p>
<p>It can be observed in the relative as well as in the absolute deviation that for the top builders there are more single slot sequences than expected with the exception of ETH-Builder, f1b and Blocknative. For multi-slot sequences with two or more slots, almost all top 10 builders have less than expected. This shows that the trend is not limited to singular entities but derives more from the general market structure.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49786-finding-2-payments-for-multi-slot-sequences-are-higher-on-average-than-for-single-slots-7" name="p-49786-finding-2-payments-for-multi-slot-sequences-are-higher-on-average-than-for-single-slots-7"></a>Finding 2: Payments for multi-slot sequences are higher on average than for single slots</h3>
<p>To understand if multi-slot sequences are valuable, we looked into MEV-Boost payments and compared single-slot to multi-slot sequences (Figure 3).</p>
<p><strong><img alt="" height="342" src="https://ethresear.ch/uploads/default/original/3X/b/5/b570ac276a9b9dc76883e6e89489c8792b0186e3.png" width="467" /></strong><br />
<em>Figure 3: Average MEV-Boost payments per Sequence Length</em></p>
<p>It can be observed that in accordance with previous work of [3], we observe higher average MEV payouts for longer consecutive sequences (from about 0.05 ETH for single slot sequences to around 0.08 ETH for sequences with nine consecutive slots). Note that the gray numbers in Figure 3 provide the sample size for each slot length. So it can be observed that the longer the sequence, almost linearly the average MEV-boost payment per slot in the sequence rises. At this stage of the research we can only speculate why this is the case. It could be driven by a higher value in longer consecutive sequences, but also by alternative effects. For example, Julian rightfully pointed out it could also be driven by an increasing intrinsic value for the second highest-bidder due to accumulating MEV in private order flow and the intrinsic valuation of the winning bidder remains constant. Or as Danning suggested, it might be driven by certain types of proprietary order flow (e.g. CEX-DEX arbitrage) being more valuable in certain time periods (e.g. volatile periods) leading to more consecutive sequences as well as higher MEV-Boost payments on average. For a more comprehensive answer and a more in-depth understanding, an analysis on the true block value (builder profits plus proposer payments) and potentially on individual tx level is necessary. We leave this open for future research.</p>
<p>This trend also holds when plotting the average payments for each individual builder. The results on this are shown in the <a href="https://colab.research.google.com/drive/1kKM-da6xP7St8puzPuyn1Ndag6a6wsg3#scrollTo=e673f535-1bad-41aa-b617-fcdeee234f01&amp;line=3&amp;uniqifier=1" rel="noopener nofollow ugc">notebook</a>.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49786-finding-3-per-slot-payments-also-increase-with-longer-sequences-8" name="p-49786-finding-3-per-slot-payments-also-increase-with-longer-sequences-8"></a>Finding 3: Per Slot Payments also increase with longer sequences</h3>
<p>Supplementary to the absolute average payment, we also looked into the payment per slot position in longer sequences (Figure 4). E.g. how much was on average paid for the third position in a longer sequence.</p>
<p><strong><img alt="" height="333" src="https://ethresear.ch/uploads/default/original/3X/3/9/3910c4ca760a17b0ae0a9ec76bb90d27155b5e42.png" width="428" /></strong><br />
<em>Figure 4: Average MEV-Boost payments per Sequence Position</em></p>
<p>Also in the payment per slot analysis a similar trend can be observed, however less prevalent. This suggests that there is slight value in longer sequences, however builders are not willing to bid significantly more for longer consecutive sequences or the first slot after a longer sequence.</p>
<p>This indicates for us that, at least so far, multi-slot strategies are not applied systematically. In this case, we expect builders would need to pay significantly higher values for later slots to ensure to capture the MEV opportunity prepared earlier.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49786-finding-4-low-auto-correlation-between-consecutive-mev-boost-payments-9" name="p-49786-finding-4-low-auto-correlation-between-consecutive-mev-boost-payments-9"></a>Finding 4: Low auto-correlation between consecutive MEV-Boost payments</h3>
<p><strong></strong></p><div class="lightbox-wrapper"><strong><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/d/9db050ae9f5aa540ea6f3c5b6270dee27e111380.png" title=""><img alt="" height="321" src="https://ethresear.ch/uploads/default/optimized/3X/9/d/9db050ae9f5aa540ea6f3c5b6270dee27e111380_2_533x321.png" width="533" /></a></strong></div><br />
<em>Figure 5: Auto-correlation of MEV-Boost Payments</em><p></p>
<p>We examined auto-correlation in the MEV boost payments to understand if historical MEV data allows us to forecast future MEV and to see if there are low- and high-MEV periods (Figure 5).</p>
<p>Overall, it can be observed that within the first few slots the correlation strongly decreases until an offset of 2 to 3 slots (we tested for Pearson Correlation Coefficient, Spearman’s Rank Correlation Coefficient and Kendall’s Rank Correlation Coefficient). Based on this we can conclude that not more than one to three slots in advance the MEV value can be moderately predicted based on historical data.</p>
<p>Further interesting observations can be made. As expected, the Spearman and Kendall correlation coefficients are significantly higher than the Pearson correlation coefficient, underlining that the data is not following a normal distribution but being skewed and having large outliers. Additionally, it is interesting to note that for the Pearson correlation coefficient, the complete data set and the top 50% quantile dataset behave similarly, which is not the case for the Spearman and Kendall coefficients. This might be an indicator that the rank ordering for the lower 50% quantile can be more reliably predicted, further underlying that high MEV values are volatile and spiky, hence difficult to predict.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49786-finding-5-no-indication-of-builder-specialization-on-low-or-high-base-fee-volatility-environment-10" name="p-49786-finding-5-no-indication-of-builder-specialization-on-low-or-high-base-fee-volatility-environment-10"></a>Finding 5: No indication of builder specialization on low- or high base fee volatility environment</h3>
<p>Previous research (e.g. [6] &amp; [7]) has found that certain builders specialize in low- or high token price volatility environments, with volatility being measured on CEX-price changes. Further, [5] observe that different builders have different strategies with some focusing on high-value blocks while others on gaining market share in low-MEV blocks.</p>
<p>Complementary, to determine whether low or high base fee volatility impacts (multi-block) MEV, we analyzed changes in base fee data to identify periods of high volatility. The base fee fluctuations are driven by whether the gas usage in the previous block was below or above the gas target, as defined by <a href="https://eips.ethereum.org/EIPS/eip-1559" rel="noopener nofollow ugc">EIP-1559</a>. To identify high volatility environments, we employed two methods: (i) a more naive approach that calculated price changes per slot, classifying the highest and lowest (negative) 10% of these changes as high volatility periods, with the remaining 80 % of slots being categorized as low volatility. Consequently, high volatility blocks occur following a block with either minimal or significant MEV and/or priority tips. (ii) Secondly, the Garman-Klass volatility [<a href="https://arxiv.org/pdf/0807.3492" rel="noopener nofollow ugc">8</a>] was calculated on an epoch basis, with slots in the top 20% of GK values designated as high volatility. This approach allows us to examine longer periods characterized by minimal or significant MEV and/or priority tips.</p>
<p>Initial correlation analysis shows only a low correlation between low and high volatile periods and the respective builders (<a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V" rel="noopener nofollow ugc">Cramér’s V</a> for the naive approach 0.0664 and for the Garman-Klass 0.0772). This indicates that there seems to be no builder specialization based on the volatility environment of the base fee. So, it can be observed that in contrast to token price volatility for base price volatility there seems to not have a specialization of builders developed (yet). Further research is needed to elaborate on this first finding.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49786-limitations-11" name="p-49786-limitations-11"></a>Limitations</h2>
<p>The research presented here is intended as an initial exploratory analysis of the data rather than a comprehensive study. It is important to note several limitations that affect the scope and conclusions of this analysis. Firstly, it is limited by the considered data set being publicly available MEV-Boost payments data. This leaves out roughly 10 % of non-MEV-Boost facilitated blocks and it does not reflect potential private off-chain agreements. Additionally, the data was partially incomplete and in other parts contained duplicate information (see the <a href="https://colab.research.google.com/drive/1kKM-da6xP7St8puzPuyn1Ndag6a6wsg3#scrollTo=0d986969-2492-49ac-ad92-8ff78e2a7fe1&amp;line=2&amp;uniqifier=1" rel="noopener nofollow ugc">notebook</a> for details). Further, missed slots have been excluded so far, a more detailed analysis in the future might focus on the particular effects missed slots have on the subsequent MEV. Lastly, as outlined in the methodology section, using MEV-Boost payments is only a proxy for captured MEV and the competitive metric used in [4] is only partially applicable for our use case.</p>
<p>As outlined in section Finding 2 it currently can only be speculated about the causation of the increasing average MEV-Boost payouts. Furthermore, running the analysis on the true block value (proposer payment plus builder profits) might generate further insights and solidify the research findings.</p>
<p>On the frequency analysis, the approach contains somewhat a chicken and egg-problem. The Monte Carlo simulation is run on market shares, while the market shares potentially derive from multi-slot sequences. We see a daily time window as an appropriate balance between precision and the need to filter out isolated effects, although this can be critically challenged.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49786-conclusions-12" name="p-49786-conclusions-12"></a>Conclusions</h2>
<p>Analyzing block meta-data since the merge, we observe that multi-slot sequences occur less frequently than statistically expected. Further, we observe that the average payments for longer multi-slot sequences increase with the sequence length. Similarly, the payments per slot position in longer sequences also slightly rise. This might indicate that there is generally value in longer consecutive sequences. However, considering the only slight increase in value and the fewer observed multi-slot sequences than expected we so far see no indication of deliberate multi-slot MEV strategies being deployed. Also on individual builder level we currently don’t observe strong deviations from expected distributions. This may also stem from the fact that in the current PBS mechanism, with MEV-Boost operating as a just-in-time (JIT) block auction, creating multi-block MEV opportunities carries inherent risk. This risk arises as creating these opportunities typically requires an upfront investment, and the opportunity might be captured by a competing builder in the next slot, assuming no off-chain collusion between the proposer and builder. This element of risk is a critical factor that could be eliminated by some of the proposed changes to the mechanism (e.g. some APS designs), making it an essential consideration when defining future mechanisms.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49786-references-13" name="p-49786-references-13"></a>References</h2>
<p>[1] Babel K, Daian P, Kelkar M, Juels A. Clockwork finance: Automated analysis of economic security in smart contracts. <em>In 2023 IEEE Symposium on Security and Privacy (SP)</em> 2023 May 21 (pp. 2499-2516). IEEE.</p>
<p>[2] Mackinga T, Nadahalli T, Wattenhofer R. Twap oracle attacks: Easier done than said?. <em>In 2022 IEEE International Conference on Blockchain and Cryptocurrency (ICBC)</em> 2022 May 2 (pp. 1-8). IEEE.</p>
<p>[3] Jensen JR, von Wachter V, Ross O. Multi-block MEV. arXiv preprint arXiv:2303.04430. 2023 Mar 8.</p>
<p>[4] Yang S, Nayak K, Zhang F. Decentralization of Ethereum’s Builder Market. arXiv preprint arXiv:2405.01329. 2024 May 2.</p>
<p>[5] Öz B, Sui D, Thiery T, Matthes F. Who Wins Ethereum Block Building Auctions and Why?. arXiv preprint arXiv:2407.13931. 2024 Jul 18.</p>
<p>[6] Gupta T, Pai MM, Resnick M. The centralizing effects of private order flow on proposer-builder separation. arXiv preprint arXiv:2305.19150. 2023 May 30.</p>
<p>[7] Heimbach L, Pahari V, Schertenleib E. Non-atomic arbitrage in decentralized finance. arXiv preprint arXiv:2401.01622. 2024 Jan 3.</p>
<p>[8] Meilijson I. The Garman-Klass volatility estimator revisited. arXiv preprint arXiv:0807.3492. 2008 Jul 22.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/does-multi-block-mev-exist-analysis-of-2-years-of-mev-data/20345">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 28 Aug 2024 10:31:36 +0000</pubDate>
</item>
<item>
<title>An Automatic Technique to Detect Storage Collisions and Vulnerabilities within Solidity Smart Contract</title>
<link>https://ethresear.ch/t/an-automatic-technique-to-detect-storage-collisions-and-vulnerabilities-within-solidity-smart-contract/20328</link>
<guid>https://ethresear.ch/t/an-automatic-technique-to-detect-storage-collisions-and-vulnerabilities-within-solidity-smart-contract/20328</guid>
<content:encoded><![CDATA[
<div> 关键词：存储碰撞、漏洞检测、Solidity智能合约、静态分析、Ethereum网络

总结:

本文提出了一种改进的、全面的技术，旨在检测Ethereum Solidity智能合约中的存储漏洞和碰撞。该技术基于高级静态分析方法，目标是识别部署在以太坊网络上的智能合约中的存储碰撞问题，尤其是复杂的代理合约，如ERC-2535（钻石/多面体代理）、ERC-1822、升级代理模式等。相比于现有技术仅通过合约字节码检测存储碰撞，本文方法利用源代码进行准确分析，同时考虑动态数组、映射变量及复杂嵌套结构的存储布局。

文中通过示例展示了存储碰撞可能导致的问题，即在合同升级后，原本不同数据类型的变量由于存储槽位重叠而发生数据错乱。为解决此问题，作者计划开发一套自动化工具，自动检测智能合约中的所有状态变量并计算其存储槽位布局。此外，工具将扩展功能，精确分析复杂变量及其元素的存储槽位，以及映射变量的键值预测，以提高检测精度和覆盖范围。最终，将实现一个碰撞检测器，能识别任何类型的状态变量及其关联变量或值的潜在碰撞或冲突。

通过此解决方案，开发者可以在部署前确保智能合约无存储碰撞风险，同时帮助检测已部署合约中深层存储的碰撞问题，保护价值数百万美元的资产免受攻击。 <div>
<p>Storage collisions and vulnerabilities within Ethereum smart contracts can lead to unexpected issues like freezing funds, escalating privileges, and financial asset theft. A storage collision occurs when two different storage structs unintentionally use same storage slot(s), or the slot layout is changed during the upgrade of implementation contract. These collision vulnerabilities have been detected in large numbers (worth millions of dollars) in a <a href="https://www.ndss-symposium.org/ndss-paper/not-your-type-detecting-storage-collision-vulnerabilities-in-ethereum-smart-contracts/" rel="noopener nofollow ugc">recent study</a> within smart contracts deployed on the Ethereum network.</p>
<p>In this topic, we propose a more accurate and complete technique to detect storage vulnerabilities and collisions in Solidity smart contracts. And encourage the Ethereum community to <strong>provide feedback on the proposed technique</strong>.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49742-introduction-1" name="p-49742-introduction-1"></a>Introduction</h3>
<p>We are working on a solution based on advanced static analysis techniques that can identify vulnerabilities within the deep storage of Ethereum Solidity smart contracts. We aim to detect storage collisions in proxy contracts deployed on the Ethereum network like ERC-2535 (Diamond/Multi-Facet Proxy), ERC-1822, upgrade proxy pattern, etc., as complex proxy contracts are more likely to experience a storage collision, like during the upgrade of implementation or facet contracts.</p>
<p><a href="https://www.ndss-symposium.org/ndss-paper/not-your-type-detecting-storage-collision-vulnerabilities-in-ethereum-smart-contracts/" rel="noopener nofollow ugc">N. Ruaro et al.</a> analyzed Ethereum contracts using contract bytecode to detect storage collisions and reported 14,891 vulnerable contracts. Their technique was able to identify storage slot types correctly with an accuracy of 87.3%. Whereas, we aim to build a solution that will use source code to accurately analyze the storage layout and slot types of the contract. Furthermore, we will also analyze dynamic arrays, mapping variables, and complex nested structs in our analysis.</p>
<p>Suppose a collision occurs on the state variables’ base slots, our approach will allow us to identify the impact of the collision on dynamic arrays and mapping variables declared consecutively, and arrays data type or mappings key types are same which is a common practice in large contracts like gaming contracts.</p>
<p>As shown in the below example code, the slot layout was changed during the contract upgrade, and since <code>token_uris</code> and <code>token_version</code> have same key types and data types, both variables will return each other’s data after the upgrade due to collision.</p>
<pre><code class="lang-auto">library ImplementationStorage1 {
    struct AddressSlot {
        address owner; // slot n+0
        mapping(uint256 =&gt; string) token_uris; // slot n+1
        mapping(uint256 =&gt; string) token_versions; // slot n+2
    }

    function getAddressSlot(bytes32 slot) internal pure returns (AddressSlot storage r) {
        assembly {
            r.slot := slot
        }
    }
}

// updated code
library ImplementationStorage2 {
    struct AddressSlot {
        address owner; //slot n+0
        mapping(uint256 =&gt; string) token_versions; // slot n+1 (shld be token_uris)
        mapping(uint256 =&gt; string) token_uris; // slot n+2 (shld be token_versions)
    }

    function getAddressSlot(bytes32 slot) internal pure returns (AddressSlot storage r) {
        assembly {
            r.slot := slot
        }
    }
}
</code></pre>
<p><code>token_uris</code> accessing <code>token_versions</code> and vice-versa after the upgrade.</p>
<pre><code class="lang-auto">       (before upgrade)                        (after upgrade)   
      _________________                      _________________
     |     Proxy       |                     |     Proxy       |
     |_________________|                     |_________________|
     | * IMPLEMENT_SLOT| --&gt; NFTManager1     | * IMPLEMENT_SLOT| --&gt; NFTManager2
     | * ADMIN_SLOT    |                     | * ADMIN_SLOT    |
     |_________________|                     |_________________|
     | + upgradeTo()   |                     | + upgradeTo()   |
     | + changeAdmin() |                     | + changeAdmin() |
     |_________________|                     |_________________|
              |                                       |
              v                                       v
      _________________                       _________________
     |   NFTManager1   |                     |   NFTManager2   |
     |_________________|                     |_________________|
     | - owner         |                     | - owner         |
     | - token_uris    | **** collision **** | - token_versions|
     | - token_versions| **** collision **** | - token_uris    |
     |_________________|                     |_________________|

</code></pre>
<p>We plan to build a technology that will automatically detect all storage collisions within a Solidity smart contract.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-49742-methodology-2" name="p-49742-methodology-2"></a>Methodology</h4>
<p>We have structured our development plan into three distinct phases, outlined as follows:</p>
<ul>
<li><strong><strong>Automatic State Variable Detector and Slot Layout Calculator</strong></strong></li>
</ul>
<p>In this phase, we focus on developing an automatic state variable detector and slot layout calculator. This component will facilitate the identification of state variables within smart contracts and determine their corresponding slot layout. By automating this process, we aim to streamline the initial analysis procedures.</p>
<p>Sample output of Slot Calculator</p>
<pre><code class="lang-auto">slot 0 - mapping ds.selectorToFacetAndPosition[bytes4] = FacetAddressAndPosition;
slot 1 - mapping ds.facetFunctionSelectors[address] = FacetFunctionSelectors;
slot 2 - address [] ds.facetAddresses;
slot 3 - mapping ds.supportedInterfaces[bytes4] = bool;
slot 4 - address ds.contractOwner;
slot 5 - mapping ds.tempSelectorsNested[uint256] = FacetAddressAndPosition;
slot 6 - FacetAddressAndPosition [] ds.FacetAddressAndPositionArray;
slot 7 - mapping ds.tempMapping[uint256] = uint256;
slot 8 - mapping ds.tempMapping2[address] = uint256;
</code></pre>
<ul>
<li><strong><strong>Mapping Keys Analyzer and Slot Calculator of Complex Variables</strong></strong></li>
</ul>
<p>Building upon the foundation established in phase 1, in this phase we will first extend the slot calculator capability to calculate the slots of complex variables and their entries (for all data types) i.e. slots of mapping keys, dynamic array, complex struct, mappings with complex struct as value.</p>
<p>This component will also include the approximation of all keys used in mapping variables for saving data using advanced static analysis techniques. By accurately approximating keys and calculating entries, we seek to enhance the precision and breadth of storage slot calculation methodology, which will help detect storage collision within deep storage data of a smart contract.</p>
<ul>
<li><strong><strong>Collision Detector for State Variables and Complex Variables All Entries Slots</strong></strong></li>
</ul>
<p>The final phase of our methodology focuses on implementing a collision detector for both state variables and complex variable slots. This critical component will identify any potential collisions or conflicts within any type of state variables and their associated variable(s)/value(s) slots. By detecting and addressing collisions, we aim to ensure the integrity and reliability of smart contracts.</p>
<p>We aim to develop a robust and comprehensive methodology for smart contract storage collision detectors, by systematically progressing through above discussed three development phases.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-49742-conclusion-3" name="p-49742-conclusion-3"></a>Conclusion</h4>
<p>The development of our solution will allow developers to ensure that their contract has no potential storage collisions before deployment. It will also be able to detect storage collisions within deep storage of deployed smart contracts and can help in securing contracts worth millions of dollars.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/an-automatic-technique-to-detect-storage-collisions-and-vulnerabilities-within-solidity-smart-contract/20328">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 23 Aug 2024 09:35:11 +0000</pubDate>
</item>
<item>
<title>A Note on Equivocation in Slot Auction ePBS</title>
<link>https://ethresear.ch/t/a-note-on-equivocation-in-slot-auction-epbs/20331</link>
<guid>https://ethresear.ch/t/a-note-on-equivocation-in-slot-auction-epbs/20331</guid>
<content:encoded><![CDATA[
<div> 关键词：槽拍卖、ePBS、分叉选择安全性、构建者欺骗、执行数据可用性

总结:
这篇文章探讨了在以太坊并行区块链系统(ePBS)中实现槽拍卖的潜在问题和解决方案。主要关注点在于分叉选择安全性和构建者欺骗。

1. **槽拍卖与ePBS**: 在槽拍卖中，信标提案者承诺特定构建者，该构建者将在适当时间提交执行负载。这与区块拍卖不同，后者要求提案者承诺执行负载哈希。构建者可能通过提交多个负载来欺骗系统，即构建者欺骗。

2. **构建者欺骗的影响**: 构建者欺骗可能导致分叉选择不安全，因为这可能导致不确定的执行负载被纳入链中。文章提出两种草稿方案来解决这个问题。

3. **草稿方案1：投票执行负载哈希**:
   - 提案涉及将“PAYLOAD_PRESENT”替换为“执行负载哈希”，并在没有达到共识的情况下，让诚实的下一轮验证者使用空块作为头部。
   - 这种方法确保了诚实构建者的构建安全，但可能引发免费数据可用性问题。

4. **草稿方案2：假装负载不存在**:
   - 如果下一轮提议者观察到至少两个冲突的负载，那么它会忽略任何空或满块的分叉选择权重。
   - 这种方法避免了数据可用性问题，但也可能使构建者更容易进行游戏。

5. **结论与进一步研究**:
   - 这两种草稿方案都试图在槽拍卖中保持与区块拍卖相同的分叉选择安全性，尽管它们各有优缺点。
   - 文章鼓励有兴趣的参与者继续探索同时解决这两个问题的设计。 <div>
<p><em>Thanks to Francesco D’Amato, Barnabé Monnot, Mike Neuder, and Thomas Thiery for feedback and review. Thanks again to Francesco for coming up with the second proposal.</em></p>
<p>Whether we want to implement slot auctions into ePBS is an <a href="https://www.notion.so/Arguments-in-Favor-and-Against-Slot-Auctions-in-ePBS-c7acde3ff21b4a22a3d41ac4cf4c75d6?pvs=21" rel="noopener nofollow ugc">active discussion area</a>, and support for slot auctions was signaled in the <a href="https://youtu.be/fQx_UbaPX-E?si=C8ALtI4zOSmFjRpN" rel="noopener nofollow ugc">seventh ePBS breakout call</a>. Currently, the ecosystem lacks knowledge about the fork choice safety of slot auctions in the <a href="https://ethereum-magicians.org/t/eip-7732-enshrined-proposer-builder-separation-epbs/19634" rel="noopener nofollow ugc">current ePBS proposal</a>. This note presents two strawman proposals to start discussing the forkchoice safety of slot auction ePBS.</p>
<p>This note presupposes the reader is familiar with the ePBS proposal (<a href="https://ethereum-magicians.org/t/eip-7732-enshrined-proposer-builder-separation-epbs/19634" rel="noopener nofollow ugc">EIP-7732</a>).  An essential part of this EIP is that a <em>payload boost</em> is applied to a beacon block if the <a href="https://ethresear.ch/t/payload-timeliness-committee-ptc-an-epbs-design/16054#proposer-initiated-splitting-18">Payload-timeliness committee (PTC)</a> reaches a quorum. If an execution payload is seen on time by a majority of the PTC, the beacon block that corresponds to the execution payload receives additional fork-choice weight (Reveal Boost). If the PTC observes a timely message from the builder stating that it withholds its payload, the additional fork-choice weight is given to the parent block of the beacon block corresponding with the withhold message (Withholding Boost).</p>
<p>In <a href="https://mirror.xyz/0x03c29504CEcCa30B93FF5774183a1358D41fbeB1/CPYI91s98cp9zKFkanKs_qotYzw09kWvouaAa9GXBrQ" rel="noopener nofollow ugc">slot auction</a> ePBS, the beacon proposer does not commit to an execution payload hash, unlike in block auction ePBS. Instead, it commits to a specific builder that can submit an execution payload when it is time to reveal. The first problem is that a builder could submit multiple execution payloads. In this note, we will refer to this as a builder equivocation.</p>
<p>In block auction ePBS, something similar to equivocation is possible. The builder could wait for at least one PTC member to vote <code>PAYLOAD_ABSENT</code> and then release a withhold message and an execution payload to split the PTC’s view such that none of the three vote options (<code>PAYLOAD_ABSENT</code>, <code>PAYLOAD_WITHHELD</code>, <code>PAYLOAD_PRESENT</code>) reaches the <a href="https://discord.com/channels/595666850260713488/874767108809031740/1272916231250382939" rel="noopener nofollow ugc">quorum of 50%</a> of the votes.</p>
<p>In block auction ePBS, this equivocation does not benefit the builder much. If the PTC does not reach a quorum, no payload boost is applied, and the honest next-slot validator will take the payload as head. If the builder equivocates, the protocol does not need to guarantee Builder Reveal Safety since the builder does not act as the protocol expects. Still, the builder does not have the flexibility to submit a different execution payload since the beacon block commits to the execution payload hash.</p>
<p>It could be that the builder is incentivized to play a <a href="https://arxiv.org/abs/2305.09032" rel="noopener nofollow ugc">timing game</a> and eventually decides that it is best if the block were withheld. The builder could submit a withhold message and see if the PTC will reach a quorum on <code>PAYLOAD_WITHHELD</code>. If the PTC does not seem to do so, and the PTC also has not yet reached a quorum on <code>PAYLOAD_ABSENT</code>, the builder reveals its payload after all. This attack seems difficult to pull off, but it allows the builder to check whether it can renege on its promised payment to the proposer while still landing its payload on-chain if it has to pay (assuming an honest next-slot proposer).</p>
<p>In slot auction ePBS, a builder may be more incentivized to equivocate because it can change the contents of its execution payload. For example, the builder could broadcast a particular execution payload, but a short time later, a significant MEV opportunity appears, and the builder now wants to broadcast a new execution payload.</p>
<p>Preventing equivocations in slot auction ePBS would be desirable because equivocations would cause insecurity in fork choice. Specifically, we want to obtain the following properties with minimal changes.</p>
<blockquote>
<p><img alt=":bulb:" class="emoji only-emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/bulb.png?v=12" title=":bulb:" width="20" /><strong>Desiderata</strong></p>
<ol>
<li>If the builder reveals precisely one timely execution payload, it should retain the same Builder Reveal Safety guarantees as in block auction eBPS</li>
<li>If the builder reveals multiple timely and equivocating execution payloads,<br />
a. no execution payload should go on-chain,<br />
b. but the Unconditional Payment should be as strong as in block auction ePBS</li>
</ol>
</blockquote>
<p>Should slashing or a penalty be applied to equivocating execution payload messages? This question is relevant to block and slot auction ePBS, although the potential benefits of equivocation are likely to be higher in slot auction ePBS. Since ePBS still allows local block construction, it seems unwise to apply harsh slashing or penalties if there is equivocation because this may disincentivize local block construction. Moreover, since it is not clear that there are significant gains to be made from equivocating execution payloads, and if gains are to be made, slashing or penalties do not qualitatively change this, so slashing or penalties are not immediately necessary.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49750-proposal-1-vote-for-execution-payload-hash-1" name="p-49750-proposal-1-vote-for-execution-payload-hash-1"></a>Proposal 1: Vote for Execution Payload Hash</h2>
<p>The first strawman proposal to obtain these properties involves changing the block auction ePBS fork-choice specification as follows.</p>
<blockquote>
<p><img alt=":bulb:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/bulb.png?v=12" title=":bulb:" width="20" /> <strong>Proposal 1: Vote for Execution Payload Hash</strong></p>
<ol>
<li>Replace <code>PAYLOAD_PRESENT</code> with <code>execution_payload_hash</code></li>
<li>If no PTC quorum is reached, let the honest next-slot validator use an empty block as its head instead of a full block.</li>
</ol>
</blockquote>
<p>A PTC member would now vote for the <code>execution_payload_hash</code> it has observed instead of simply voting whether a payload is present. Reveal boost is applied if a quorum is reached on <code>execution_payload_hash</code>. Intuitively, this is necessary for slot auctions since the PTC now indicates which execution payload should be used if the block is full and not just that the block is full.</p>
<p>It seems like desideratum 1—the same Builder Reveal Safety as in block auction ePBS—is immediately satisfied since an honest builder does not release equivocating execution payloads. A PTC member’s <code>execution_payload_hash</code> vote functions the same as a <code>PAYLOAD_PRESENT</code> vote.</p>
<p>If the builder equivocates but the PTC still reaches a quorum on <code>execution_payload_hash</code>, then the execution payload will make it on-chain in the same way a payload would have made it on-chain if the builder did not equivocate. I believe this is fine because the builder released an equivocating payload that did not split the view of the PTC (sufficiently). This indicates that this equivocating payload is a minor threat to the fork-choice security. Although this outcome contradicts desideratum 2a, the timely requirement in desideratum 2 should be read as the execution payload intends to split the view of the PTC sufficiently.</p>
<p>If the builder equivocates and the PTC does not reach a quorum, then the next-slot honest proposer should see an empty block as its head. The builder loses some of its Builder Reveal Safety because it could be that the builder reveals only one payload (does not equivocate), yet the PTC does not reach a quorum. However, Builder Reveal Safety is not very strong in block auction ePBS either because a next-slot rational proposer would prefer to build on an empty block than a full block since these are more valuable (the ex-post reorg safety is low if reveal boost is not applied). Changing the default next-slot honest proposer behavior from seeing a full block to an empty block as its head does not change much in Builder Reveal Safety, and the system then satisfies desideratum 2.</p>
<p>What if the next-slot proposer is dishonest? The builder could collude with the next-slot proposer and broadcast messages such that the PTC does not reach a quorum and include an execution payload late. This is similar to the attack in block auction eBPS, where a builder tries to get Withhold Boost to apply but releases an execution payload if it does not succeed. The builder and next-slot proposer collusion allows the builder to play aggressive timing games while ensuring Builder Reveal Safety. These timing games come at the expense of the execution validation time of the attesting committee. It is not immediately apparent what this attack would gain for the builder and next-slot proposer collusion since the builder timing game gain comes almost entirely from the next-slot proposer’s revenues.</p>
<p>The downside of this proposal is the problem of free data availability. The PTC could now reach a quorum on an <code>execution_payload_hash</code>. These PTC votes would end up on-chain, and an adversary could use them to show that a piece of data was available to the PTC. Yet the adversary would not have to pay the base fee needed to provide the data on-chain; it only has to pay the proposer to commit to the adversary as the builder.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49750-proposal-2-pretend-payload-absent-2" name="p-49750-proposal-2-pretend-payload-absent-2"></a>Proposal 2: Pretend Payload Absent</h2>
<p>The second strawman proposal does not suffer from the free data availability problem and achieves the desiderata as follows.</p>
<blockquote>
<p><img alt=":bulb:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/bulb.png?v=12" title=":bulb:" width="20" /> <strong>Proposal 2: Pretend Payload Absent</strong><br />
If the next-slot proposer/attesters observe(s) at least two equivocating payloads, it/they assign(s) no additional fork-choice weight to any empty or full block</p>
</blockquote>
<p>The behavior of a PTC member does not change from the block auction ePBS specification. However, suppose a proposer sees that the block producer in the previous slot released equivocating execution payloads. In that case, it ignores the fork-choice weight the PTC may have given to any fork.</p>
<p>If the builder is honest, this does not change its Builder Reveal Safety since the system works exactly as it does in block auction ePBS. Desideratum 1 is thus immediately satisfied.</p>
<p>If the builder equivocates, an honest-but-rational proposer will choose to build on an empty block since it allows the proposer to extract the MEV from two slots of time instead of one. The attesters will not object to this since they observed the equivocating payloads and assigned no additional fork-choice weight to any forks. Therefore, if the next-slot proposer and attesters are honest, desideratum 2 is also satisfied.</p>
<p>The next-slot proposer could collude with the builder. The builder could equivocate, and the next-slot proposer could choose to build on a full block. Similarly to the collusion situation described in the first proposal, though, the gain that a builder gets from this equivocation seems to primarily come from the profits the next-slot proposer could make. It is not clear that the joint utility of the collusion increases by enough to justify the collusion.</p>
<p>A builder and a next-slot proposer could collude to ensure an execution payload does not become canonical. Consider a builder that submits an execution payload, and the PTC reaches a quorum on whether this payload is timely. Later, the builder regrets the contents of its execution payload and aims to remove it from the canonical chain. It could then release an equivocation payload so the next-slot proposer will not build on the undesirable execution payload. This is similar to a builder not revealing its block in block auction ePBS.</p>
<p>In conclusion, these strawman proposals seem to achieve the same fork-choice safety under slot auctions as under block auctions with minimal changes. While the first proposal has a problem with free data availability, the second proposal may be more susceptible to builder games, such as reorging its execution payload. The lack of free data availability and being less susceptible to builder games are advantages of slot auctions in ePBS. Further research on a design that simultaneously solves both problems would be very valuable. If you are interested in working on (slot auctions in) ePBS, please see this <a href="https://www.notion.so/ePBS-EIP-7732-tracker-9f85f7b086994bd79192bc72bae703a1?pvs=21" rel="noopener nofollow ugc">page</a>!</p>
            <p><small>2 posts - 2 participants</small></p>
            <p><a href="https://ethresear.ch/t/a-note-on-equivocation-in-slot-auction-epbs/20331">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 23 Aug 2024 16:52:02 +0000</pubDate>
</item>
<item>
<title>The Role of the P2P Market in ePBS</title>
<link>https://ethresear.ch/t/the-role-of-the-p2p-market-in-epbs/20330</link>
<guid>https://ethresear.ch/t/the-role-of-the-p2p-market-in-epbs/20330</guid>
<content:encoded><![CDATA[
<div> 关键词：ePBS、两层市场、P2P市场、可信中立性、价值反映

总结:

文章探讨了以太坊并行区块系统(ePBS)中的两层市场结构及其对生态系统的影响。ePBS采用了一种双层拍卖机制，其中大型区块构建者倾向于使用直接连接的点对点(P2P)市场，而小型构建者则依赖于更传统的P2P市场。文章重点讨论了P2P市场在可信中立性中的作用和价值反映能力。

1. **P2P市场的角色与价值**：P2P市场允许任何人设定底价，有助于发现市场中潜在的高价值区块构建者，尤其是在可信中立性方面。它通过允许提案者与构建者直接互动，减少了对第三方的信任依赖，同时提高了构建者参与度，特别是对于那些不经常参与拍卖的小型构建者。

2. **价值反映的挑战**：尽管P2P市场在信任无虞方面表现出色，但其在价值反映方面的表现较差。由于网络需要抵御分布式拒绝服务(DOS)攻击，因此无法处理大量投标，导致构建者可能需要策略性地进行投标，早期投标不能被取消，这可能影响市场效率。

3. **替代方案：bid curation relay**：文章提出了一种名为“bid curation relay”的外部解决方案，该方案允许提案者连接到具有高信誉的节点，以发现潜在的构建者。这种模式下，信任要求较低，且能提供更好的价值反映，通过允许频繁投标、投标取消等功能，提高市场效率。

4. **可信中立性的重要性**：可信中立性是指确保分配执行负载构建权利的实体是基于最高估值的构建者。P2P市场或bid curation relay等机制的存在有助于实现这一目标，通过促进构建者的多样性和高效匹配，确保提案者能够公平地选择最有价值的构建者。

5. **实施与未来工作**：尽管P2P市场存在一些局限性，但其易于实施且无需硬叉，允许客户端自由迭代。因此，从实用性和未来潜在的MEV-Burn优化角度来看，实施P2P市场是一个有益的决策。进一步的工作可以细化P2P市场的规则，并探索构建者RPC端点的链上注册机制，以优化其功能和效率。

通过上述分析，可以看出P2P市场在ePBS中扮演着关键角色，旨在增强可信中立性、促进价值发现与提高市场效率。尽管面临挑战，通过适当的机制设计和优化，P2P市场可以为以太坊生态系统带来显著益处。 <div>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/1/d178209b1bebdec57cfa0665bfacea6970512e8c.jpeg" title="role_of_p2p"><img alt="role_of_p2p" height="389" src="https://ethresear.ch/uploads/default/optimized/3X/d/1/d178209b1bebdec57cfa0665bfacea6970512e8c_2_690x389.jpeg" width="690" /></a></div><br />
<em>A two-tier auction market: the right resembles the less sophisticated publicly observable P2P market, and the left resembles the more sophisticated private RPC market.</em><p></p>
<p><em>Thanks to Potuz, Barnabé Monnot, Terence Tsao, and Thomas Thiery for comments and discussion.</em></p>
<p>The current ePBS proposal, <a href="https://eips.ethereum.org/EIPS/eip-7732" rel="noopener nofollow ugc">EIP-7732</a>, suggests operating a two-tier market where builders can bid to obtain the execution payload construction rights. Large block builders are expected to use the pull-based direct connection market. This market allows for lower latency and more flexibility for the builder, as the builder only needs to commit to its bid once the proposer asks for it. This market, however, requires the proposer to connect to the builder’s RPC and actively pull bid(s) from it. Smaller builders who lack this connectivity with the validator set can use the push-based P2P market. This market has stricter rules for what bidders can do but does not need the proposer to pull bid(s) from it since bids are pushed to the proposer.</p>
<p>This note explores the role of the P2P market in ePBS. Although there has been some <a href="https://ethresear.ch/t/builder-bidding-behaviors-in-epbs/20129">initial</a> <a href="https://hackmd.io/@potuz/HyhN0Nt9A" rel="noopener nofollow ugc">exploration</a> on the topic, this note presents a clear counterfactual of a world where the P2P market were not included in EIP-7732. This note also emphasizes <a href="https://collective.flashbots.net/t/tee-boost/3741/5" rel="noopener nofollow ugc">multiplexing</a>—the ability of proposers to discover builders—as the most important aspect of the P2P market.</p>
<p>The three arguments in favor of the P2P market that the author has seen in previous work are: 1) it allows anyone to set a floor price for the auction, 2) it can be used for MEV-Burn in future protocol upgrades, and 3) it lowers entry barriers for new entrants or long-tail builders.</p>
<p>The first argument is that allowing anyone to bid via the publicly observable P2P market gives all validators the ability to set a floor price for the auction. Validators can bid based on the block that they could locally build. Builders must then bid at least above the bid of these validators to obtain the execution payload construction rights. It has been argued that this is valuable if a cartel of builders intends to keep bids low. The floor price, however, would not break up a cartel. Although proposers would make slightly more revenue in this case, it is unclear what the value of such a floor price is to the protocol.</p>
<p>The beacon proposer selling the rights may be the ideal party to set a reserve price. As I argue in <a href="https://mirror.xyz/0x03c29504CEcCa30B93FF5774183a1358D41fbeB1/8aCbi_a-Gh5DWnkJWstm8zA5fvtoQB-QR5we7C8XC90" rel="noopener nofollow ugc">this post</a>, a proposer may want to put a higher reserve price than its valuation for the execution payload construction rights to attract higher bids from builders. The P2P market allows the proposer to signal its reserve price to the market. In this sense, the P2P market allows the validator and other participants to express their preferences.</p>
<p>The second argument states that the P2P market may facilitate <a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590">MEV-Burn</a> in future protocol upgrades. MEV-Burn aims to decouple the rewards from selling execution payload construction rights from being a validator. This has numerous benefits; for example, it decreases the value of using a staking service provider (SSP) since MEV-Burn decreases the variance of validator payoffs. MEV-Burn requires that builder bids be legible to the protocol. Most designs achieve this by having a committee that observes the best available bids. If ePBS would only have the direct connection market, the MEV-Burn designs need to be revisited since a proposer selling the execution rights is incentivized to understate the amount that will be burnt. Still, the P2P market is expected to only reflect a small portion of the value of the execution payload construction rights, hence even ePBS with the P2P market may not be satisfactory for an effective MEV-Burn solution.</p>
<p>The last reason for the P2P market is that it would allow builders from which proposers are unlikely to pull bids to still compete in the market. Proposers may be unlikely to pull bids from builders that infrequently participate in the auction because they are very specialized or from new builders unknown to the proposer. This could be because proposers have an outdated whitelist of builders from which to pull bids. Allowing these proposers to participate in the push-based P2P market will result in more builder diversity in block construction, which may benefit the protocol.</p>
<p>This last reason is what we will explore in this post. Specifically, what does the Ethereum ecosystem gain by enshrining the push-based P2P market aside from an out-of-protocol solution that facilitates small builders’ participation in the market?</p>
<p>Shea Ketsdever recently released a post on <a href="https://collective.flashbots.net/t/tee-boost/3741" rel="noopener nofollow ugc">TEE-Boost</a>, an adaptation of MEV-Boost that uses Trusted Execution Environments. In this post, she highlights the different roles a relay plays. One of the roles is multiplexing, allowing proposers to discover builders who may want to participate in the auction.</p>
<p>The ePBS P2P market aims to achieve multiplexing. In the context of ePBS, multiplexing has at least two facets: trustlessness and value reflection. Trustlessness is important because ePBS removes the trust that proposers and builders must place in a relay to facilitate the fair exchange. Value reflection is essential because a multiplexing tool that poorly reflects the value bidders assign to the auctioned item will not efficiently match an auctioneer with the correct bidder.</p>
<p>The ePBS P2P market scores very well on the trustlessness front. Neither a proposer nor a builder must trust anyone since bids are broadcast via the P2P network, and the winning bid is committed to on-chain. The P2P market, however, scores poorly on the value reflection front. Since the P2P network must be DOS resistant, it cannot handle too many bids, so bidders will likely not be allowed to bid as often as they could in MEV-Boost, meaning they have to be strategic about when they bid. Moreover, early bids will not be able to be canceled, which could lead to strategic builders only winning via the P2P market if the valuation of other builders that operate via the direct connection market has decreased (<a href="https://www.youtube.com/watch?v=-PXGPFFneMI" rel="noopener nofollow ugc">adverse selection</a>). Finally, the value reflection of the P2P market relative to the RPC market will worsen as the RPC market becomes more sophisticated while the P2P market becomes stale.</p>
<p>How would an out-of-protocol actor facilitate multiplexing if ePBS were deployed? In MEV-Boost, relays facilitate multiplexing because submitting blocks to relays is (largely) permissionless, and relays are well-connected to validators. In ePBS, a relay - from no one referred to as a bid curation relay - would look different. A bid curation relay could open an RPC endpoint that proposers connect to and host an auction where builders submit bids, like in MEV-Boost. Bids, however, do not need to contain transaction data since the bid curation relay would not be responsible for the fair exchange problem that is solved via ePBS. Bids in ePBS are a bid value and the hash of the execution payload. A proposer then pulls the highest bid from the bid curation relay and, if it so desires, commits to the highest bid via the in-protocol ePBS system. A winning builder then sees this in-protocol commitment and publishes the block via ePBS.</p>
<p>It becomes clear that the trust assumptions that proposers and builders must place in a bid curation relay are vastly lower than in MEV-Boost. Essentially, the proposer and builders must trust the bid curation relay to forward the highest-paying bid when the proposer asks for it. The bid curation relay is not trusted with the block contents (<a href="https://collective.flashbots.net/t/tee-boost/3741" rel="noopener nofollow ugc">builder privacy</a> is preserved) and is not responsible for unconditional payment (<a href="https://collective.flashbots.net/t/tee-boost/3741" rel="noopener nofollow ugc">data availability and validation</a> are enforced via the protocol).</p>
<p>The ePBS relay scores worse on the trustlessness front than the P2P market since the proposer and builders must trust the relay not to censor its bids. On the other hand, the value reflection of such a bid curation relay could be far better. The relay could offer bid cancellations and high-frequency bidding to builders. Moreover, relays could invest in latency reductions and charge for this, as some do in MEV-boost. If a relay successfully reduces latency, more prominent builders may connect to it. This means the value reflection of relays relative to directly connected builders may remain stable or improve over time.</p>
<p><a href="https://collective.flashbots.net/t/tee-boost/3741/5?u=julian" rel="noopener nofollow ugc">Shea also highlights</a> another option that has been discussed widely before: next to the P2P market; there could be an on-chain registry of builders. There could be a smart contract that any builder could write its RPC endpoint to. Any validator could then see the available RPC endpoints and pull bids from it during its slot. This alternative scores well on the trustlessness front since no trust is required, and it scores well on the value reflection point since it allows all builders to compete on a similar level. The proposer could pull from this registry every time it is supposed to propose a block.</p>
<p>Why do we care about multiplexing? Multiplexing contributes to the credible neutrality of the network. In the context of ePBS, credible neutrality may mean something like this: the builder with the highest valuation for the execution payload construction rights is allocated these rights. If proposers were to rely solely on directly connected builders, some long-tail builders who happened to have an exceptionally high value for a specific block might be excluded. If proposers rely on bid curation relays, they may not forward the highest-paying bid because they prefer to forward another bid for whatever reason. If proposers rely on an on-chain registry of builders, it may not connect to the newer or smaller builders.</p>
<p>Allowing multiplexing to contribute to credible neutrality is a trade-off between trustlessness and value reflection. If a completely trustless market is so poor at value reflection that it never surfaces a winning bid, it does not contribute much to credible neutrality. If a perfectly value-reflecting market puts a lot of trust in one party, the benefit of credible neutrality is also nonexistent.</p>
<p>To conclude, the P2P market is easy to implement, and its maintenance does not require a hard fork so clients can iterate freely. Although the P2P market only contributes a little to the core functionality of ePBS, there are virtually no downsides to implementing it, and it is a nice feature that may benefit some users and could be beneficial for proposers as it increases their revenues and may be helpful for MEV-Burn in the future. Further work could specify the P2P market rules and how an on-chain registry of builder RPC endpoints could work.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/the-role-of-the-p2p-market-in-epbs/20330">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 23 Aug 2024 16:01:14 +0000</pubDate>
</item>
<item>
<title>[Feedback Needed] A technique to Automatically Detect Storage Collisions and Vulnerabilities within Solidity Smart Contract</title>
<link>https://ethresear.ch/t/feedback-needed-a-technique-to-automatically-detect-storage-collisions-and-vulnerabilities-within-solidity-smart-contract/20328</link>
<guid>https://ethresear.ch/t/feedback-needed-a-technique-to-automatically-detect-storage-collisions-and-vulnerabilities-within-solidity-smart-contract/20328</guid>
<content:encoded><![CDATA[
<div> 关键词：存储碰撞、漏洞检测、Solidity智能合约、静态分析技术、安全性提升

总结：

本文提出了一种基于高级静态分析技术的解决方案，旨在检测Ethereum Solidity智能合约中的存储漏洞和碰撞。与先前研究使用合约字节码来检测存储碰撞不同，本方案将采用源代码来准确分析合约的存储布局和槽类型。该方法不仅识别存储碰撞，还考虑动态数组、映射变量以及复杂嵌套结构体的影响。

具体而言，当基础槽位发生碰撞时，该方案能识别紧随其后的动态数组和映射变量之间的碰撞影响，特别是当这些变量具有相同的数据类型或键类型时，如大型游戏合约中常见的情况。通过精确分析存储布局和槽类型，该技术能够检测到存储碰撞并避免因升级实施或特性合约导致的状态变量基槽布局改变而产生的数据错误。

此外，该方法计划自动检测智能合约中的所有状态变量及其对应的槽布局，扩展到更复杂的变量（如映射键、动态数组和复杂结构体）的槽计算，并最终实现状态变量和复杂变量所有条目的碰撞检测器。通过这三个阶段的系统开发，旨在为智能合约提供全面的存储碰撞检测能力，确保部署前的安全性，并帮助锁定数百万美元级别的合同免受潜在攻击。

最后，该方案的实施将使开发者能够在部署前验证其合同不存在潜在的存储碰撞风险，同时对已部署的智能合约进行定期审计以发现并修复存储碰撞问题，从而显著提高智能合约的整体安全性和可靠性。 <div>
<p>Storage collisions and vulnerabilities within Ethereum smart contracts can lead to unexpected issues like freezing funds, escalating privileges, and financial asset theft. A storage collision occurs when two different storage structs unintentionally use same storage slot(s), or the slot layout is changed during the upgrade of implementation contract. These collision vulnerabilities have been detected in large numbers (worth millions of dollars) in a <a href="https://www.ndss-symposium.org/ndss-paper/not-your-type-detecting-storage-collision-vulnerabilities-in-ethereum-smart-contracts/" rel="noopener nofollow ugc">recent study</a> within smart contracts deployed on the Ethereum network.</p>
<p>In this topic, we propose a more accurate and complete technique to detect storage vulnerabilities and collisions in Solidity smart contracts. And encourage the Ethereum community to <strong>provide feedback on the proposed technique</strong>.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49742-introduction-1" name="p-49742-introduction-1"></a>Introduction</h3>
<p>We are working on a solution based on advanced static analysis techniques that can identify vulnerabilities within the deep storage of Ethereum Solidity smart contracts. We aim to detect storage collisions in proxy contracts deployed on the Ethereum network like ERC-2535 (Diamond/Multi-Facet Proxy), ERC-1822, upgrade proxy pattern, etc., as complex proxy contracts are more likely to experience a storage collision, like during the upgrade of implementation or facet contracts.</p>
<p><a href="https://www.ndss-symposium.org/ndss-paper/not-your-type-detecting-storage-collision-vulnerabilities-in-ethereum-smart-contracts/" rel="noopener nofollow ugc">N. Ruaro et al.</a> analyzed Ethereum contracts using contract bytecode to detect storage collisions and reported 14,891 vulnerable contracts. Their technique was able to identify storage slot types correctly with an accuracy of 87.3%. Whereas, we aim to build a solution that will use source code to accurately analyze the storage layout and slot types of the contract. Furthermore, we will also analyze dynamic arrays, mapping variables, and complex nested structs in our analysis.</p>
<p>Suppose a collision occurs on the state variables’ base slots, our approach will allow us to identify the impact of the collision on dynamic arrays and mapping variables declared consecutively, and arrays data type or mappings key types are same which is a common practice in large contracts like gaming contracts.</p>
<p>As shown in the below example code, the slot layout was changed during the contract upgrade, and since <code>token_uris</code> and <code>token_version</code> have same key types and data types, both variables will return each other’s data after the upgrade due to collision.</p>
<pre><code class="lang-auto">library ImplementationStorage1 {
    struct AddressSlot {
        address owner; // slot n+0
        mapping(uint256 =&gt; string) token_uris; // slot n+1
        mapping(uint256 =&gt; string) token_versions; // slot n+2
    }

    function getAddressSlot(bytes32 slot) internal pure returns (AddressSlot storage r) {
        assembly {
            r.slot := slot
        }
    }
}

// updated code
library ImplementationStorage2 {
    struct AddressSlot {
        address owner; //slot n+0
        mapping(uint256 =&gt; string) token_versions; // slot n+1 (shld be token_uris)
        mapping(uint256 =&gt; string) token_uris; // slot n+2 (shld be token_versions)
    }

    function getAddressSlot(bytes32 slot) internal pure returns (AddressSlot storage r) {
        assembly {
            r.slot := slot
        }
    }
}
</code></pre>
<p><code>token_uris</code> accessing <code>token_versions</code> and vice-versa after the upgrade.</p>
<pre><code class="lang-auto">       (before upgrade)                        (after upgrade)   
      _________________                      _________________
     |     Proxy       |                     |     Proxy       |
     |_________________|                     |_________________|
     | * IMPLEMENT_SLOT| --&gt; NFTManager1     | * IMPLEMENT_SLOT| --&gt; NFTManager2
     | * ADMIN_SLOT    |                     | * ADMIN_SLOT    |
     |_________________|                     |_________________|
     | + upgradeTo()   |                     | + upgradeTo()   |
     | + changeAdmin() |                     | + changeAdmin() |
     |_________________|                     |_________________|
              |                                       |
              v                                       v
      _________________                       _________________
     |   NFTManager1   |                     |   NFTManager2   |
     |_________________|                     |_________________|
     | - owner         |                     | - owner         |
     | - token_uris    | **** collision **** | - token_versions|
     | - token_versions| **** collision **** | - token_uris    |
     |_________________|                     |_________________|

</code></pre>
<p>We plan to build a technology that will automatically detect all storage collisions within a Solidity smart contract.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-49742-methodology-2" name="p-49742-methodology-2"></a>Methodology</h4>
<p>We have structured our development plan into three distinct phases, outlined as follows:</p>
<ul>
<li><strong><strong>Automatic State Variable Detector and Slot Layout Calculator</strong></strong></li>
</ul>
<p>In this phase, we focus on developing an automatic state variable detector and slot layout calculator. This component will facilitate the identification of state variables within smart contracts and determine their corresponding slot layout. By automating this process, we aim to streamline the initial analysis procedures.</p>
<p>Sample output of Slot Calculator</p>
<pre><code class="lang-auto">slot 0 - mapping ds.selectorToFacetAndPosition[bytes4] = FacetAddressAndPosition;
slot 1 - mapping ds.facetFunctionSelectors[address] = FacetFunctionSelectors;
slot 2 - address [] ds.facetAddresses;
slot 3 - mapping ds.supportedInterfaces[bytes4] = bool;
slot 4 - address ds.contractOwner;
slot 5 - mapping ds.tempSelectorsNested[uint256] = FacetAddressAndPosition;
slot 6 - FacetAddressAndPosition [] ds.FacetAddressAndPositionArray;
slot 7 - mapping ds.tempMapping[uint256] = uint256;
slot 8 - mapping ds.tempMapping2[address] = uint256;
</code></pre>
<ul>
<li><strong><strong>Mapping Keys Analyzer and Slot Calculator of Complex Variables</strong></strong></li>
</ul>
<p>Building upon the foundation established in phase 1, in this phase we will first extend the slot calculator capability to calculate the slots of complex variables and their entries (for all data types) i.e. slots of mapping keys, dynamic array, complex struct, mappings with complex struct as value.</p>
<p>This component will also include the approximation of all keys used in mapping variables for saving data using advanced static analysis techniques. By accurately approximating keys and calculating entries, we seek to enhance the precision and breadth of storage slot calculation methodology, which will help detect storage collision within deep storage data of a smart contract.</p>
<ul>
<li><strong><strong>Collision Detector for State Variables and Complex Variables All Entries Slots</strong></strong></li>
</ul>
<p>The final phase of our methodology focuses on implementing a collision detector for both state variables and complex variable slots. This critical component will identify any potential collisions or conflicts within any type of state variables and their associated variable(s)/value(s) slots. By detecting and addressing collisions, we aim to ensure the integrity and reliability of smart contracts.</p>
<p>We aim to develop a robust and comprehensive methodology for smart contract storage collision detectors, by systematically progressing through above discussed three development phases.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-49742-conclusion-3" name="p-49742-conclusion-3"></a>Conclusion</h4>
<p>The development of our solution will allow developers to ensure that their contract has no potential storage collisions before deployment. It will also be able to detect storage collisions within deep storage of deployed smart contracts and can help in securing contracts worth millions of dollars.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/feedback-needed-a-technique-to-automatically-detect-storage-collisions-and-vulnerabilities-within-solidity-smart-contract/20328">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 23 Aug 2024 09:35:11 +0000</pubDate>
</item>
<item>
<title>Mechan-stein (alt. Franken-ism)</title>
<link>https://ethresear.ch/t/mechan-stein-alt-franken-ism/20321</link>
<guid>https://ethresear.ch/t/mechan-stein-alt-franken-ism/20321</guid>
<content:encoded><![CDATA[
<div> 关键词：Ethereum、block production、MEV、validator sophistication、chain neutrality

总结：

文章主要讨论了如何优化以太坊区块构建过程，以实现三个关键设计目标：鼓励区块构建者的竞争、限制验证者复杂性的价值以及维护区块链空间的中立性。文章首先概述了区块空间市场设计的基本原则，强调了区块生产中心化、验证去信任和高分散性，以及防止审查的重要性。

接着，文章介绍了三种提案机制：
1. **嵌入式块拍卖与MEV销毁（Enshrined PBS & MEV-burn via PTC）**：通过引入即时块拍卖机制，确保任何区块构建者都能参与竞争，同时通过验证者执行的阈值来限制区块构建者的优势。
2. **前置区块拍卖（Ahead-of-time slot auction）**：提前进行区块拍卖，减少验证者复杂性的价值，因为拍卖不是实时进行的。
3. **FOCIL（Focus on Constraints List）**：允许多个共识参与者共同创建区块模板，增加系统中立性，但不涉及MEV的权力分配。

文章提出了结合上述机制的“Mecahn-stein”概念，旨在综合考虑三个设计目标，通过在区块构建过程中引入双重拍卖机制（一个预先拍卖，一个即时拍卖）来平衡区块构建的效率、公平性和中立性。该机制试图通过让区块构建者购买区块构建的权利，同时在区块构建时再次竞拍区块的顶部交易，来实现对区块构建过程的控制与透明度，从而达到鼓励竞争、限制验证者复杂性价值并维护区块链中立性的目的。

文章最后指出，虽然这种结合机制可能面临复杂性、可能扭曲MEV市场以及区块构建者仍然具有一定程度的决策权等挑战，但通过合理设计和实施，可以在一定程度上解决当前存在的问题，为以太坊网络提供更高效、公平和中立的区块构建流程。 <div>
<h1><a class="anchor" href="https://ethresear.ch#p-49714-mechan-stein-alt-franken-ismbrp-classsmallsmall-choose-your-own-adventurehttpsxcomvitalikbuterinstatus1788489148183019929-either-way-just-trying-to-portmanteau-frankenstein-and-mechanismsmallp-1" name="p-49714-mechan-stein-alt-franken-ismbrp-classsmallsmall-choose-your-own-adventurehttpsxcomvitalikbuterinstatus1788489148183019929-either-way-just-trying-to-portmanteau-frankenstein-and-mechanismsmallp-1"></a>Mechan-stein (alt. Franken-ism)<br /><p><small><em>^ <a href="https://x.com/VitalikButerin/status/1788489148183019929" rel="noopener nofollow ugc">choose your own adventure</a> – either way, just trying to portmanteau ‘Frankenstein’ and ‘Mechanism.’</em></small></p></h1>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/d/3d13edb2750f779fb39f38e943038de48a692422.jpeg" title="upload_2936c4a8e65027883c0cacec063f9ea2"><img alt="upload_2936c4a8e65027883c0cacec063f9ea2" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/3/d/3d13edb2750f779fb39f38e943038de48a692422_2_498x500.jpeg" width="498" /></a></div><br />
<sub><em><strong>^“don’t worry bro, just one more auction, i swear. check it out.” h/t Mallesh for the relevant <a href="https://x.com/malleshpai/status/1748026472923623619" rel="noopener nofollow ugc">tweet</a>.</strong></em><br />
</sub><p></p>
<p><span class="math">\cdot</span><br />
<em>by <a href="https://twitter.com/mikeneuder" rel="noopener nofollow ugc">mike</a> – wednesday; august 21, 2024.</em><br />
<sub><em>^hbd <a href="https://en.wikipedia.org/wiki/Bo_Burnham" rel="noopener nofollow ugc">Bo</a>. if you, dear reader, haven’t seen <a href="https://en.wikipedia.org/wiki/Bo_Burnham:_Inside" rel="noopener nofollow ugc">“Inside”</a> or <a href="https://www.youtube.com/watch?v=5XWEVoI40sE" rel="noopener nofollow ugc">“Inside Outtakes,”</a> watching them is your homework assignment.</em></sub><br />
<span class="math">\cdot</span><br />
<em>Many thanks to <a href="https://x.com/barnabemonnot" rel="noopener nofollow ugc">Barnabé</a>, <a href="https://x.com/_julianma" rel="noopener nofollow ugc">Julian</a>, <a href="https://x.com/soispoke" rel="noopener nofollow ugc">Thomas</a>, <a href="https://x.com/jacobykaufmann" rel="noopener nofollow ugc">Jacob</a>, <a href="https://x.com/mteamisloading" rel="noopener nofollow ugc">mteam</a>, <a href="https://x.com/nero_eth" rel="noopener nofollow ugc">Toni</a>, <a href="https://x.com/drakefjustin" rel="noopener nofollow ugc">Justin</a>, <a href="https://x.com/vitalikbuterin" rel="noopener nofollow ugc">Vitalik</a>, <a href="https://x.com/MaxResnick1" rel="noopener nofollow ugc">Max</a>, and <a href="https://x.com/malleshpai" rel="noopener nofollow ugc">Mallesh</a> for discussions around these topics and comments on the draft!</em><br />
<span class="math">\cdot</span><br />
<em>The idea for the combined mechanism explored in <a href="https://ethresear.ch#p-49714-h-2-mechan-stein-9">Part 2</a> of this post came from a Baranbé-led whiteboarding session and accompanying <a href="https://x.com/barnabemonnot/status/1808444733305258047" rel="noopener nofollow ugc">tweet thread</a>. These ideas are also explored in the <a href="https://efdn.notion.site/Block-construction-session-bd611621250f45948eff05fcf6a34067?pvs=4" rel="noopener nofollow ugc">this doc</a>, which inspired <a href="https://github.com/michaelneuder/talks/blob/268e273b55cf2c753b2479c3ebbb826d41811754/misc2024/sbc.pdf" rel="noopener nofollow ugc">this talk</a>.</em><br />
<span class="math">\cdot</span><br />
<strong>tl;dr;</strong> <em>We sketch a high-level framing for Ethereum block construction centered around the design goals of encouraging builder competition, limiting the value of validator sophistication, and preserving the neutrality of block space. We then highlight three proposed mechanisms and how they interface with the established desiderata. We conclude by exploring the potential synergies of combining these designs into a single flow, called <code>Mechan-stein</code>.</em><br />
<span class="math">\cdot</span><br />
<strong>Contents</strong><br />
(1) <a href="https://ethresear.ch#p-49714-h-1-the-building-blocks-pun-intended-of-block-space-market-design-2">The building blocks of block-space market design</a><br />
&nbsp;&nbsp;  <a href="https://ethresear.ch#p-49714-enshrined-pbs-mev-burn-via-ptc-3">Enshrined PBS &amp; MEV-burn via PTC</a><br />
&nbsp;&nbsp;  <a href="https://ethresear.ch#p-49714-execution-auctions-an-attester-proposer-separation-instatiation-5">Execution Auctions (an Attester-Proposer Separation instantiation)</a><br />
&nbsp;&nbsp;  <a href="https://ethresear.ch#p-49714-focil-7">FOCIL</a><br />
(2) <a href="https://ethresear.ch#p-49714-h-2-mechan-stein-9">Mechan-stein</a><br />
&nbsp;&nbsp;  <a href="https://ethresear.ch#p-49714-potential-issues-with-mechan-stein-10">Potenital Issues with Mechan-stein</a><br />
<span class="math">\cdot</span></p>
<p><strong>Related work</strong></p>
<div class="md-table">
<table>
<thead>
<tr>
<th>Article</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ" rel="noopener nofollow ugc"><em>More words on Proposer-Builder Separation</em></a></td>
</tr>
<tr>
<td><a href="https://efdn.notion.site/Block-construction-session-bd611621250f45948eff05fcf6a34067?pvs=4" rel="noopener nofollow ugc"><em>Notes from block construction session</em></a></td>
</tr>
<tr>
<td><a href="https://ethresear.ch/t/burning-mev-through-block-proposer-auctions/14029"><em>MEV-burn</em></a></td>
</tr>
<tr>
<td><a href="https://ethresear.ch/t/payload-timeliness-committee-ptc-an-epbs-design/16054"><em>PTC</em></a></td>
</tr>
<tr>
<td><a href="https://ethresear.ch/t/fork-choice-enforced-inclusion-lists-focil-a-simple-committee-based-inclusion-list-proposal/19870"><em>FOCIL</em></a></td>
</tr>
</tbody>
</table>
</div><hr />
<h1><a class="anchor" href="https://ethresear.ch#p-49714-h-1-the-building-blocks-pun-intended-of-block-space-market-design-2" name="p-49714-h-1-the-building-blocks-pun-intended-of-block-space-market-design-2"></a>[1] The building blocks (pun intended) of block-space market design</h1>
<p>Since before the Merge, <a href="https://github.com/michaelneuder/mev-bibliography" rel="noopener nofollow ugc">much</a> has been (and continues to be) written about Ethereum’s transaction supply chain and block-space market design. I still think Vitalik’s <a href="https://vitalik.eth.limo/general/2021/12/06/endgame.html" rel="noopener nofollow ugc"><em>Endgame</em></a> summarizes the best-case outcome most succinctly with,</p>
<blockquote>
<p><em>“Block production is centralized, block validation is trustless and highly decentralized, and censorship is still prevented.”</em></p>
</blockquote>
<p>We can operationalize each of these statements into a design goal for our system:</p>
<ol>
<li><em>“Block production is centralized.”</em> <span class="math">\rightarrow</span> MEV is a fact of life in financial systems, and some actors will inevitably specialize in its extraction. We can’t expect solo-stakers to run profitable builders, but we can encourage competition and transparency in the MEV markets. When discussing <code>MEV-boost</code>, we usually describe it as aiming to democratize access to MEV for all proposers (which it does extremely well), but one under-discussed element of the existing system is that it <em>encourages builder competition</em> by creating a transparent market for buying block space. There are (and always will be) advantages and economies of scale for being a big builder (e.g., colocation with relays, acquiring exclusive order flow deals, and holding large inventory on various trading venues – for more, see this <a href="https://arxiv.org/pdf/2407.13931" rel="noopener nofollow ugc">recent paper</a> from Burak, Danning, Thomas, and Florian), but anyone can send blocks and compete in the auction. Another important element of <code>MEV-boost</code> is that the auction happens Just-In-Time (JIT) for the block proposal, making <a href="https://ethresear.ch/t/on-attestations-block-propagation-and-timing-games/20272">timing games</a> around the block proposal deadline valuable to the proposer who serves as the auctioneer. Still, the real-time nature of the auction ensures that the builder with the highest value <em>for this specific slot</em> wins the auction (rather than, e.g., the builder with the highest average value for any slot – see <a href="https://arxiv.org/pdf/2408.03116" rel="noopener nofollow ugc">Max &amp; Mallesh’s argument</a> for why ahead-of-time auctions are more centralized). This leads to <strong>design goal <span class="hashtag-raw">#1:</span> encourage builder competition.</strong><a href="https://ethresear.ch#fn1dst"><span class="math">^{[1]}</span></a><a name="fn1"></a></li>
<li><em>“Block validation is trustless and highly decentralized”</em><a href="https://ethresear.ch#fn2dst"><span class="math">^{[2]}</span></a><a name="fn2"></a> <span class="math">\rightarrow</span> Ethereum’s primary focus has been preserving the validator set’s decentralization (why this is important in item <span class="hashtag-raw">#3</span> below). This fundamental tenet instantiates itself in both the engineering/technical design and the economic/incentive design. On the engineering front, the <a href="https://github.com/ethereum/consensus-specs/tree/dev" rel="noopener nofollow ugc">spec</a> is written with the <a href="https://docs.ethstaker.cc/ethstaker-knowledge-base/hardware/hardware-requirements" rel="noopener nofollow ugc">minimum hardware requirements</a> in mind. This constraint ensures that participation in Ethereum’s consensus is <em>feasible</em> given (relatively) modest resources. On the economic level, the goal is to minimize the disparity in financial outcomes between at-home stakers and professional operators. Beyond feasibility, this aims to make at-home staking <em>not too irrational.</em> This double negative is tongue-in-cheek but hopefully conveys the message of trying to ensure there is some economic viability to at-home staking rather than staking through a centralized provider. Another lens for interpreting this is keeping the marginal value of sophistication low. We can’t expect at-home operators to have the exact same rewards as Coinbase and Lido (e.g., because they may have higher network latency), but the centralized staking providers shouldn’t benefit greatly from sophistication. This leads to <strong>design goal <span class="hashtag-raw">#2:</span> limit the value of validator sophistication.</strong></li>
<li><em>“Censorship is prevented.”</em> <span class="math">\rightarrow</span> Credible neutrality is what differentiates crypto-economic systems from FinTech. If centralized entities determine which transactions land on chain and which do not, it’s over. To ensure the anti-fragility and neutrality of Ethereum, we must rely on a <a href="https://collective.flashbots.net/t/decentralized-crypto-needs-you-to-be-a-geographical-decentralization-maxi/1385" rel="noopener nofollow ugc">geographically distributed</a> validators; the validator set is the most decentralized part of the block production pipeline. In my opinion, (i) the main point of having a decentralized validator set is to allow those validators to express different preferences over the transactions that land on chain (“high preference entropy” – <a href="https://ethresear.ch/t/unbundling-staking-towards-rainbow-staking/18683">h/t Dr. Monnot</a>), and (ii) relying on this decentralization is the only way to preserve neutrality of the chain (c.f., <a href="https://ethresear.ch/t/uncrowdable-inclusion-lists-the-tension-between-chain-neutrality-preconfirmations-and-proposer-commitments/19372"><em>Uncrowdable Inclusion Lists</em></a> for more discussion on chain neutrality). This leads to <strong>design goal <span class="hashtag-raw">#3:</span> preserve the neutrality of Ethereum block space.</strong></li>
</ol>
<p>Right. To summarize:</p>
<ol>
<li><em>“Block production is centralized.”</em> <span class="math">\rightarrow</span> <strong>design goal <span class="hashtag-raw">#1:</span> encourage builder competition.</strong></li>
<li><em>“Block validation is trustless and highly decentralized”</em> <span class="math">\rightarrow</span> <strong>design goal <span class="hashtag-raw">#2:</span> limit the value of validator sophistication.</strong></li>
<li><em>“Censorship is prevented.”</em> <span class="math">\rightarrow</span> <strong>design goal <span class="hashtag-raw">#3:</span> preserve the neutrality of Ethereum block space.</strong></li>
</ol>
<p>Ok. This is all great, but let’s talk specifics. Many proposals aim to accomplish some of the design goals above. I am going to focus on three:</p>
<ol>
<li><strong>Enshrined <a href="https://barnabe.substack.com/p/pbs" rel="noopener nofollow ugc">Proposer-Builder Separation</a> &amp; <a href="https://ethresear.ch/t/burning-mev-through-block-proposer-auctions/14029"><code>MEV-burn</code></a> via <a href="https://ethresear.ch/t/payload-timeliness-committee-ptc-an-epbs-design/16054">Payload-Timeliness Committee</a></strong> (abbr. <code>PTC</code> onwards).</li>
<li><strong><a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ" rel="noopener nofollow ugc">Execution Auctions</a>/Attester-Proposer Separation</strong>.</li>
<li><strong><a href="https://ethresear.ch/t/fork-choice-enforced-inclusion-lists-focil-a-simple-committee-based-inclusion-list-proposal/19870/5">Fork-Choice Enforced Inclusion Lists</a></strong> (abbr. <code>FOCIL</code> onwards).</li>
</ol>
<p>This may seem jargon-laden, and I apologize; please check out the links for the canonical article on each topic; for even more legibility, I will present a high-level view of each proposal below.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49714-enshrined-pbs-mev-burn-via-ptc-3" name="p-49714-enshrined-pbs-mev-burn-via-ptc-3"></a>Enshrined PBS &amp; <code>MEV-burn</code> via <code>PTC</code></h3>
<p>This design enshrines a JIT block auction into the Ethereum consensus layer. The diagram below summarizes the block production pipeline <em>during the slot</em>.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/2/22560b81c3e436b0a3524b9c52a1b6b5aa277003.png" title="upload_a40f44ea2cb5821c889733125eb53260"><img alt="upload_a40f44ea2cb5821c889733125eb53260" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/2/2/22560b81c3e436b0a3524b9c52a1b6b5aa277003_2_423x500.png" width="423" /></a></div><p></p>
<ol>
<li><strong>The builder bids</strong> in the auction by sending <code>(block header, bid value)</code> pairs to the proposer and the committee members.</li>
<li><strong>The proposer commits</strong> to the highest bid value by signing and publishing the winning bid.</li>
<li><strong>The committee enforces</strong> that the proposer selected a sufficiently high bid according to their view.</li>
<li><strong>The builder publishes</strong> the block.</li>
<li><strong>The committee enforces</strong> the timeliness of the builder’s publication.</li>
</ol>
<h4><a class="anchor" href="https://ethresear.ch#p-49714-analysis-4" name="p-49714-analysis-4"></a>Analysis</h4>
<ul>
<li><code>PTC</code> allows the protocol (through the enforcement of the committee) to serve as the trusted third-party in the <a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=208b22c7a094ada20736593afcc8c759c7d1b79c" rel="noopener nofollow ugc">fair-exchange</a> of the sale of the block building rights. <code>MEV-burn</code> (maybe more aptly denoted as “block maximization” because burning isn’t strictly necessary for the bids) asks the attesters to enforce a threshold for the bid selected as the winner by the proposer.</li>
<li><span><code>PTC</code> primarily implements <strong>design goal <span class="hashtag-raw">#1:</span> encourage builder competition.</strong></span> <code>PTC</code> enshrines <code>MEV-boost</code>, fully leaning into creating a competitive marketplace for block building. As in <code>MEV-boost</code>, the real-time block auction allows any builder to submit bids and encourages competition during each slot. Additionally, the JIT auction and bid-threshold enforcement of <code>MEV-burn</code> reduces the risk of multi-slot MEV by forcing each auction to take place during the slot. Lastly, <code>PTC</code> and other ePBS designs historically were aimed at <a href="https://ethresear.ch/t/why-enshrine-proposer-builder-separation-a-viable-path-to-epbs/15710#reasons-to-enshrine-4">removing relays</a>. With bid thresholds from <code>MEV-burn</code>, the <a href="https://ethresear.ch/t/relays-in-a-post-epbs-world/16278">bypassability of the protocol</a> becomes less feasible (even if the best builder bypasses, the second best can go through the protocol and ensure their bid wins).</li>
<li><span><code>PTC</code> marginally addresses <strong>design goal <span class="hashtag-raw">#2:</span> limit the value of validator sophistication.</strong></span> By creating an explicit market for MEV-aware blocks, <code>PTC</code> ensures that all validators can access a large portion of the MEV available in their slot. <code>MEV-burn</code> also smooths out the variance in the validator rewards. However, one of the major limitations of this auction design is the “value-in-flight” (h/t Barnabé for <a href="https://www.youtube.com/watch?v=KHw7gdJ14uQ" rel="noopener nofollow ugc">coining</a> the <a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ" rel="noopener nofollow ugc">term</a>) problem of the auction taking place during the slot. Because the value of the sold item changes dramatically throughout a slot, the auctioneer’s role benefits from sophistication. Beyond simple <a href="https://dataalways.mirror.xyz/-m0-bp3aZpcqa15_QbMX3MD1v9xg7VCcfGtZBR7I9Bg" rel="noopener nofollow ugc">timing games</a>, more exotic strategies around the fork-choice rule (e.g., using extra fork-choice weight to <a href="https://ethresear.ch/t/on-attestations-block-propagation-and-timing-games/20272">further delay block publication</a>, h/t Toni) are possible, and we are just starting to see these play out.</li>
<li><span> <code>PTC</code> does not address <strong>design goal <span class="hashtag-raw">#3:</span> Preserve the neutrality of Ethereum block space.</strong></span> Neither <code>PTC</code> nor PBS generally are designed to encourage censorship resistance. The fact that a few builders account for most of Ethereum’s blocks is not surprising, and we should not count on those builders to uphold the credible neutrality of the chain (even if they are right now). While it is true that <code>PTC</code> aims to maintain a decentralized validator set, the fact that the full block is sold counter-acts that effect by still giving discretionary power of the excluded transactions to the builder (e.g., consider the hypothetical where 100% of validators are at-home stakers (maximally decentralized), but they all outsource to the same builder <span class="math">\implies</span> the builder fully determines the transactions that land onchain).</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-49714-execution-auctions-an-attester-proposer-separation-instatiation-5" name="p-49714-execution-auctions-an-attester-proposer-separation-instatiation-5"></a>Execution Auctions (an Attester-Proposer Separation Instatiation)</h3>
<p>In contrast to the JIT block auction enabled by <code>PTC</code>, this design enshrines an ahead-of-time slot auction into the Ethereum consensus layer. A <a href="https://mirror.xyz/0x03c29504CEcCa30B93FF5774183a1358D41fbeB1/CPYI91s98cp9zKFkanKs_qotYzw09kWvouaAa9GXBrQ" rel="noopener nofollow ugc">slot auction</a> still allocates the entire block to the winner of the auction, but they no longer need to commit to the specific contents of the block when bidding (e.g., they are buying future block space) – this allows the auction to take place well in advance of the slot itself. The diagram below summarizes the block production pipeline <em>32 slots ahead of time</em> (the 32 is just an arbitrary number; you could run the auction any time in advance or even during the slot itself; the key distinction is the fact that the bids don’t contain commitments to the contents of the block).</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/6/36fa041386a05b37b3dad9e959dad6c329d506ba.png" title="upload_ba33d4610c47000f0ac60a5273f91f61"><img alt="upload_ba33d4610c47000f0ac60a5273f91f61" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/3/6/36fa041386a05b37b3dad9e959dad6c329d506ba_2_505x500.png" width="505" /></a></div><p></p>
<p>N.B., the first three steps are nearly identical to the <code>PTC</code> process. The only differences are (a) the auction for the <code>Slot N+32</code> block production rights takes place during <code>Slot N</code> and (b) the bid object is a single <code>bid value</code> rather than the <code>(block header, bid value)</code> tuples. The actual building and publication of the block happen during <code>Slot N+32</code>, and <code>Execution Auctions</code> are agnostic to that process.</p>
<ol>
<li><strong>The builder bids</strong> in the auction by sending <code>bid value</code> to the proposer and the committee members.</li>
<li><strong>The proposer commits</strong> to the highest bid value by signing and publishing the winning bid.</li>
<li><strong>The committee enforces</strong> that the proposer selected a sufficiently high bid according to their view.</li>
</ol>
<h4><a class="anchor" href="https://ethresear.ch#p-49714-analysis-6" name="p-49714-analysis-6"></a>Analysis</h4>
<ul>
<li><code>Execution Auctions</code> allow the protocol (through the enforcement of the committee) to serve as the trusted third party in the <a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=208b22c7a094ada20736593afcc8c759c7d1b79c" rel="noopener nofollow ugc">fair-exchange</a> of the sale of the block building rights for a future slot.</li>
<li><span><code>Execution Auctions</code> primarily support <strong>design goal <span class="hashtag-raw">#2:</span> limit the value of validator sophistication.</strong></span> With the real-time auction of <code>PTC</code>, we described how the value-in-flight problem results in value from the sophistication of the validators who conduct the auction. In <code>Execution Auctions</code>, the auction occurs apriori, making the value of the object sold less volatile. The validator conducting the auction has a much simpler role that doesn’t benefit from timing games in the way they do in the JIT auction, thereby reducing their value from sophistication.</li>
<li><span><code>Execution Auctions</code> do not address <strong>design goal <span class="hashtag-raw">#1:</span> encourage builder competition.</strong></span> By running the auction ahead of time, the highest value bidder will always be the builder who is best at producing blocks (h/t Max and Mallesh for <a href="https://arxiv.org/pdf/2408.03116" rel="noopener nofollow ugc">formalizing this</a>). The builder may still choose to sell the block production rights on the secondary market, but only at a premium over the amount they can extract.<a href="https://ethresear.ch#fn3dst"><span class="math">^{[3]}</span></a><a name="fn3"></a></li>
<li><span><code>Execution Auctions</code> do not address <strong>design goal <span class="hashtag-raw">#3:</span> Preserve the neutrality of Ethereum block space.</strong></span> <code>Execution Auctions</code> are <em>not designed to encourage censorship resistance</em>. We fully expect the future block space and builder markets to remain centralized. Another major concern with <code>Execution Auctions</code> is the risk of multi-slot MEV. Because the auction is not real-time, it is possible to acquire multiple consecutive future slots and launch multi-slot MEV strategies without competing in any auction during the slot itself. (We could try to mitigate this by making the look-ahead only a single slot – e.g., <code>Slot N+1</code> auction during <code>Slot N</code>, but this may open up the same value-in-flight issues around JIT block auctions. More research is needed (and actively being done h/t Julian) here.)</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-49714-focil-7" name="p-49714-focil-7"></a>FOCIL</h3>
<p>This design allows multiple consensus participants to construct lists of transactions that must be included in a given slot. In contrast to the previous designs, this <em>is not</em> an auction and <em>does not</em> aim to enshrine a MEV marketplace into the protocol. Instead, the focus here is improving the system’s neutrality by allowing multiple parties to co-create a template (in the form of a set of constraints) for the produced block. The diagram below describes the block production process <em>during the slot itself.</em></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/c/bc6cae7b07f724344d704a6bd035e33e82f7500f.png" title="upload_badb6db529bdfb2640abe1ce4d767dd2"><img alt="upload_badb6db529bdfb2640abe1ce4d767dd2" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/b/c/bc6cae7b07f724344d704a6bd035e33e82f7500f_2_455x500.png" width="455" /></a></div><p></p>
<ol>
<li><strong>The IL committee publishes</strong> their inclusion lists to the builder (clumping this together with the proposer for this diagram because the builder must follow the block template) and the attesters.</li>
<li><strong>The builder publishes</strong> a block that includes an aggregate view of the ILs they received and conforms to the constraints therein.</li>
<li><strong>The attesters enforce</strong> the block validity conditions, which now check that the builder included a sufficient threshold of observed inclusion lists.</li>
</ol>
<h4><a class="anchor" href="https://ethresear.ch#p-49714-analysis-8" name="p-49714-analysis-8"></a>Analysis</h4>
<ul>
<li><code>FOCIL</code> increases the protocol’s neutrality by allowing multiple validators to express their preferences in the block co-creation.</li>
<li><span><code>FOCIL</code> primarily contributes to <strong>design goal <span class="hashtag-raw">#3:</span> preserve the neutrality of Ethereum blockspace.</strong></span> This is the direct goal; more inputs to the block construction seems like a no-brainer (very much in line with the latest thread of <a href="https://www.youtube.com/watch?v=mJLERWmQ2uw" rel="noopener nofollow ugc">concurrent proposer research</a>). Critically, <code>FOCIL</code> intentionally does not give any MEV power to the inclusion list constructors (see <a href="https://ethresear.ch/t/uncrowdable-inclusion-lists-the-tension-between-chain-neutrality-preconfirmations-and-proposer-commitments/19372"><em>Uncrowdability</em></a> for more) to avoid the economic capture of that role. In particular, <code>FOCIL</code> <em>does not</em> aim to constrain the builder’s ability to extract MEV generally; the builder can still reorder and insert transactions at will in their block production process. Instead, it’s their ability to <em>arbitrarily exclude</em> transactions, which <code>FOCIL</code> reduces.</li>
<li><span><code>FOCIL</code> does not address <strong>design goal <span class="hashtag-raw">#1:</span> encourage builder competition.</strong></span> <code>FOCIL</code> is agnostic to the exact block production process beyond enforcing a block template for transactions that cannot be excluded arbitrarily.</li>
<li><span><code>FOCIL</code> does not address <strong>design goal <span class="hashtag-raw">#2:</span> limit the value of validator sophistication.</strong></span> <code>FOCIL</code> is agnostic to the exact block production process beyond enforcing a block template for transactions that cannot be excluded arbitrarily.</li>
</ul>
<p>Right. That was the “vegetable eating” portion of this article. The critical takeaway is <strong>each of the above proposals primarily addresses one of the cited design goals, but none address all three simultaneously.</strong> This makes it easy to point out flaws in any specific design.<br />
…<br />
You probably see where we are going with this. Let’s not bury the lede. What if we combine them? Each serves a specific role and operates on a different portion of the slot duration; why not play it out?</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49714-h-2-mechan-stein-9" name="p-49714-h-2-mechan-stein-9"></a>[2] Mechan-stein</h1>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/0/508d35ee9a4052135205628aa738a64cbcdd4c51.png" title="upload_bc314953eed471a97f9afd50b068bb14"><img alt="upload_bc314953eed471a97f9afd50b068bb14" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/5/0/508d35ee9a4052135205628aa738a64cbcdd4c51_2_440x500.png" width="440" /></a></div><p></p>
<p>With the groundwork laid, we can ~nearly~ combine the three mechanisms directly. There is one issue, however, which arises from both auctions selling the same object – the proposing rights for <code>Slot N+32</code>. The resulting bids in the first auction (the slot auction sale of <code>Slot N+32</code> during <code>Slot N</code>) would thus not carry any economic meaning because bidders would be competing for the slot but would then be forced sellers by the time the slot arrived. To resolve this, the second auction (which happens JIT during the slot) could instead be a Top-of-Block auction (e.g., the first 5mm gas consumed in the block). There are many articles exploring the Top-of-Block/Rest-of-Block split (sometimes called block prefix/suffixes) (see, e.g., <a href="https://ethresear.ch/t/how-much-can-we-constrain-builders-without-bringing-back-heavy-burdens-to-proposers/13808">here</a>, <a href="https://github.com/bharath-123/pepc-boost-relay" rel="noopener nofollow ugc">here</a>, <a href="https://ethresear.ch/t/state-lock-auctions-towards-collaborative-block-building/18558">here</a>), so we won’t go into the details of the consensus changes required to facilitate this exchange. Taking its feasibility for granted, the double-auction design of Mechan-stein makes more sense.<br />
- <strong>Auction 1 during <code>Slot N</code></strong> sells the block proposing rights for <code>Slot N+32</code> and is conducted by the proposer of <code>Slot N</code>.<br />
- <strong>Auction 2 during <code>Slot N+32</code></strong> sells the Top-of-Block to a (potentially different) builder who specifies the specific set of transactions to be executed first in the block. This auction is conducted just in time by the builder/winner of Auction 1.</p>
<p>With this framing, the winner of Auction 1 effectively bought the option to build (or sell) the Rest-of-Block for <code>Slot  N+32</code> – thus the expected value of the bids in that auction would be the average amount of MEV extractable in the block suffix (aside: this might play nicely with <a href="https://x.com/barnabemonnot/status/1808444762376020121" rel="noopener nofollow ugc">preconfs</a>). The diagram below shows the flow at a high level (leaving off many back-and-forths for legibility).</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/d/8dd0b952112b0f09ac63c9b1a5fba3e4c718dc60.jpeg" title="upload_95efb121f762fdc00a15aecb64fe6e54-1"><img alt="upload_95efb121f762fdc00a15aecb64fe6e54-1" height="373" src="https://ethresear.ch/uploads/default/optimized/3X/8/d/8dd0b952112b0f09ac63c9b1a5fba3e4c718dc60_2_690x373.jpeg" width="690" /></a></div><p></p>
<ol>
<li><strong>The <code>Slot N</code> proposer auctions off</strong> the <code>Slot N+32</code> proposing rights.</li>
<li><strong>The <code>Slot N</code> attesters enforce</strong> the bid threshold of the slot auction.</li>
<li><em>[32 slots later]</em> <strong>The <code>Slot N+32</code> IL committee publishes</strong> their ILs.</li>
<li><strong>The <code>Slot N+32</code> builder auctions off</strong> the Top-of-Block for <code>Slot N+32</code>.</li>
<li><strong>The <code>Slot N+32</code> <code>PTC</code> enforces</strong> the bid threshold of the Top-of-Block auction.</li>
<li><strong>The <code>Slot N+32</code> <code>PTC</code> enforces</strong> the timeliness of the block publication from the winning builder.</li>
<li><strong>The <code>Slot N+32</code> attesters enforce</strong> the IL threshold of the final block.</li>
</ol>
<p>Yeah, yeah – it’s a lot of steps, but the pitch is pretty compelling.</p>
<ul>
<li><span>Mechan-stein addresses <strong>design goal <span class="hashtag-raw">#1:</span> encourage builder competition.</strong></span> The permissionless, JIT Top-of-Block auction helps mitigate the risk of multi-slot MEV in <code>Execution Auctions</code> by <em>forcing</em> the slot auction winner to sell a portion of the block or at least pay a threshold to build the full block themselves.</li>
<li><span>Mechan-stein addresses <strong>design goal <span class="hashtag-raw">#2:</span> limit the value of validator sophistication.</strong></span> The role of an average validator in block production is now the simple combination of (1) conducting the ahead-of-time slot auction and (2) publishing their inclusion list when part of an IL committee. This greatly reduces the power bestowed on the validator because (1) they are now conducting an auction apriori (thus, latency and timing games play a smaller role) and (2) the inclusion list intentionally does not generate much value for MEV-carrying transactions (because it only guarantees inclusion rather than ordering).</li>
<li><span>Mechan-stein addresses <strong>design goal <span class="hashtag-raw">#3:</span> preserve the neutrality of Ethereum block space.</strong></span> By allowing many participants to co-create the set of constraints enforced on the builder of each block, high preference entropy is achieved without unduly benefiting the transactions that land in an inclusion list, as block builders can still reorder and insert at their leisure. However, the builder’s ability to exclude is limited, removing some of their monopolist power over the transactions in the block.</li>
</ul>
<p>The combined mechanism creates a set of checks and balances where the weaknesses of one design in isolation are the strengths of another. Everything is perfect, right?</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49714-potential-issues-with-mechan-stein-10" name="p-49714-potential-issues-with-mechan-stein-10"></a>Potential issues with Mechan-stein</h3>
<p>It might not be only rainbows and butterflies. Without being comprehensive (neither in the list of potential issues nor the responses to said issues), let’s run down a few of the most obvious questions with Mechan-stein and some initial counter-points.</p>
<ul>
<li><span><strong>Point <span class="hashtag-raw">#1</span></strong> – complexity, complexity, complexity.</span> This could (and maybe should) count for multiple points (h/t Mallesh for the relevant <a href="https://x.com/malleshpai/status/1748026472923623619" rel="noopener nofollow ugc">tweet</a>). Each of these proposals involves massive changes to the consensus layer of Ethereum with wide-ranging impact (particularly on the fork-choice rule). The devil is truly in the details, and getting something like this spec’ed out and implemented would be an immense research and engineering lift – let’s just say <a href="https://en.wikipedia.org/wiki/Occam%27s_razor" rel="noopener nofollow ugc">William of Ockham</a> would not be impressed.
<ul>
<li><span><strong>Counter-point <span class="hashtag-raw">#1</span></strong> – building the future of finance in a permissionless and hyper-financialized world wasn’t going to be simple (“Rome wasn’t built in a day”).</span> It shouldn’t be shocking that there doesn’t seem to be a silver bullet for building an MEV-aware, decentralized, credibly neutral blockchain. Maybe eating the complexity now can leave the chain in a more stable equilibrium. Also, there may be significant synergies in combining designs (e.g., using the same committee for <code>FOCIL</code> and <code>PTC</code>). You could probably do a subset of Mechan-stein and still get some benefits (e.g., <code>FOCIL</code> + <code>PTC</code>).</li>
</ul>
</li>
<li><span><strong>Point <span class="hashtag-raw">#2</span></strong> – how may the ahead-of-time slot auction distort the MEV market?</span> Mostly just reciting <a href="https://arxiv.org/pdf/2408.03116" rel="noopener nofollow ugc">Max and Mallesh’s</a> argument (3rd time referencing that paper in this article lol). By removing the real-time nature of the initial auction, you bias it towards a winner-take-all for the best builder (or the “Best Block Space Future Value Estimator™”). I’d say this is similar in spirit to the Phil Daian view of making the competition as deterministic as possible (e.g., <a href="https://youtu.be/SBOGdofF4u8?t=620" rel="noopener nofollow ugc">“deterministic vs statistical opportunities”</a>).
<ul>
<li><span><strong>Counter-point <span class="hashtag-raw">#2</span></strong> – that is the point of still having the <code>PTC</code> conduct a JIT Top-of-Block auction.</span> I think this feels reasonable. However, there is still a slight edge that the auctioneer (who may be a builder themselves) has in the JIT auction, which is they can benefit from the sophistication and latency investments as they are the auctioneer and a participant. As mentioned above, you could consider skipping the <code>Execution Auctions</code> part of Mechan-stein and just going with <code>FOCIL</code> + <code>PTC</code> (or even leave <code>MEV-boost</code> alone as the primary PBS market and just do <code>FOCIL</code>). (h/t Justin for pointing out that you could try to do <code>Execution Auctions</code> where multiple proposers (more than one auction winner) are selected – another combined mechanism that tries to mitigate the multi-slot MEV risk.)</li>
</ul>
</li>
<li><span><strong>Point <span class="hashtag-raw">#3</span></strong> – there is still power in being the block producer.</span> As pointed out in this <a href="https://ethresear.ch/t/fork-choice-enforced-inclusion-lists-focil-a-simple-committee-based-inclusion-list-proposal/19870/3">comment</a> and <a href="https://ethresear.ch/t/fork-choice-enforced-inclusion-lists-focil-a-simple-committee-based-inclusion-list-proposal/19870/4">its response</a> on the <code>FOCIL</code> post, there is still some discretionary power in being the block builder. Namely, they can choose which ILs they exclude from their aggregate up to some protocol-enforced tolerance. This notion of having an IL “aggregator” is the main difference between <code>FOCIL</code> and a leaderless approach like <a href="https://www.youtube.com/watch?v=mJLERWmQ2uw" rel="noopener nofollow ugc">Braid</a>.
<ul>
<li><span><strong>Counter-point <span class="hashtag-raw">#3</span></strong> – this seems like a fundamental feature.</span> Again, I find myself leaning on Phil’s comment and mental model for “how economic power expresses itself in the protocol.” In a distributed system with network latency and geographic decentralization, some parties will have advantages over others. Suppose the protocol doesn’t explicitly imbue some participants with power during some period (e.g., by electing a leader). In that case, that power will still manifest somewhere else, likely in a more implicit (thus more sophisticated) way. This is more of a meta point, and I am happy to be convinced otherwise.</li>
</ul>
</li>
</ul>
<p>All right, going to cut it here; hope you found it interesting. Lot’s to think on still.</p>
<p><em>thank for reading <img alt=":heart:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/heart.png?v=12" title=":heart:" width="20" /> -mike</em></p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#p-49714-footnotes-11" name="p-49714-footnotes-11"></a>footnotes</h3>
<p><span class="math">^{[1]}</span><a name="fn1dst"></a>: It is worth noting that, conditioned on having strong censorship resistance properties, the difference between a monopolist builder and a competitive marketplace of builders isn’t so vital. As discussed with Barnabé and Julian, perhaps a more important property is the “replace-ability” of a monopolist builder if they begin abusing their power. All else being equal, I still prefer the outcome where we have multiple builders, even if just for the memetic reality of having a single block builder looks highly centralized, even if the other consensus participants heavily constrain them. Hence, builder competition still feels like a fair desiderata.<a href="https://ethresear.ch#fn1"><img alt=":leftwards_arrow_with_hook:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/leftwards_arrow_with_hook.png?v=12" title=":leftwards_arrow_with_hook:" width="20" />︎</a></p>
<p><span class="math">^{[2]}</span><a name="fn2dst"></a>: Vitalik pointed out that when he originally wrote this, he was referring more to the act of validating the blocks (e.g., by verifying a ZK proof) rather than explicitly participating in consensus. The name “validator” denotes someone who engages in consensus, which has been a nomenclatural pain point since the launch of the beacon chain. Despite this, I still like the framing of keeping some form of consensus participation decentralized (mainly as a means to better chain neutrality), so I will slightly abuse the naming confusion. xD <a href="https://ethresear.ch#fn2"><img alt=":leftwards_arrow_with_hook:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/leftwards_arrow_with_hook.png?v=12" title=":leftwards_arrow_with_hook:" width="20" />︎</a></p>
<p><span class="math">^{[3]}</span><a name="fn3dst"></a>: It is worth noting that validators could also choose to only sell their block at a premium in the more general case through the use of the <a href="https://writings.flashbots.net/the-cost-of-resilience" rel="noopener nofollow ugc"><code>min-bid</code></a> feature of <code>MEV-boost</code>. See more on <code>min-bid</code> from <a href="https://mirror.xyz/0x03c29504CEcCa30B93FF5774183a1358D41fbeB1/8aCbi_a-Gh5DWnkJWstm8zA5fvtoQB-QR5we7C8XC90" rel="noopener nofollow ugc">Julian</a> and <a href="https://hackmd.io/@dataalways/resilience" rel="noopener nofollow ugc">Data Always</a>. <a href="https://ethresear.ch#fn3"><img alt=":leftwards_arrow_with_hook:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/leftwards_arrow_with_hook.png?v=12" title=":leftwards_arrow_with_hook:" width="20" />︎</a></p>
            <p><small>3 posts - 3 participants</small></p>
            <p><a href="https://ethresear.ch/t/mechan-stein-alt-franken-ism/20321">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 21 Aug 2024 13:23:40 +0000</pubDate>
</item>
<item>
<title>L2 sequencer proving on weak hardware; parallelization and decentralization</title>
<link>https://ethresear.ch/t/l2-sequencer-proving-on-weak-hardware-parallelization-and-decentralization/20313</link>
<guid>https://ethresear.ch/t/l2-sequencer-proving-on-weak-hardware-parallelization-and-decentralization/20313</guid>
<content:encoded><![CDATA[
<div> 关键词：Linea、Risc0、zkVM、p2p网络、证明时间

总结:
文章主要探讨了如何通过使用Risc0和zkVM技术来减少区块链验证时间，并同时实现去中心化。首先，作者指出大部分L2（Layer 2）序列器采用闭源方式运行，这需要强大的集中式计算能力。为了实现去中心化，需要接受去中心化网络固有的延迟和噪音。

文章进一步解释了Risc0与zkVM结合使用的方法。zkVM限制了一次能证明的最大计算周期数，大约为16.78百万周期。通过递归将大型程序分割成多个子程序（称为Risc0中的“段”），并分别证明这些子程序，最后聚合证明以模拟整个程序的一次性证明。例如，要证明一个1百万周期的程序，理论上需要分割成大约60个段。

文章提出了分段、并行证明和去中心化策略。通过适当的节点配置，即使在较弱的硬件上，也可以快速证明大型程序。假设有一个去中心化的验证计算网络，可以实现至少840个节点来处理证明工作，并且考虑到冗余因素，这个网络可以处理相当于10个类似于Linea的L2区块链的证明任务。

结论：
通过上述方法，理论上可以显著减少证明时间，并同时提供去中心化的解决方案。然而，为了验证这一设想的有效性，需要实际测试这种设置。如果成功，10,000个较弱节点组成的去中心化网络可以处理多达10个类似Linea的L2区块链的验证任务。 <div>
<p><a href="https://ethresear.ch/t/vortex-building-a-prover-for-the-zk-evm/14427">Linea’s sequencer</a> proves a 30m gassed block of transactions in 5 minutes. Here’s its setup:</p>
<blockquote>
<ul>
<li>On a 96 cores machine with 384 GB of RAM (hpc6a.48xlarge on AWS)</li>
<li>In 5 minutes (only including the inner-proof)</li>
</ul>
</blockquote>
<p>So is it possible to reduce the proving time and, at the same time, obtain decentralization guarantees? We have an idea.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49694-overview-1" name="p-49694-overview-1"></a>Overview</h3>
<p>Almost all of the L2 sequencers are closed-source, intellectual property, and thus protected behind centralized setups. To cram that much power into an entity requires a great deal of justification today. To decentralize the flow, on the other hand, one has to accept certain amounts of delay and noise usually found in decentralized compute networks.</p>
<h4><a class="anchor" href="https://ethresear.ch#p-49694-zkvms-recursion-and-risc0s-approach-2" name="p-49694-zkvms-recursion-and-risc0s-approach-2"></a>zkVMs, recursion, and Risc0’s approach</h4>
<p>Any zkVM toolset puts a certain upper bound on the maximum number of cycles(roughly speaking 1 cycle equals 1 operation) it can prove in one go. This is usually done for efficiency reasons. For <a href="https://github.com/risc0" rel="noopener nofollow ugc">Risc0</a>, a RISC-V general zkVM, it is 2^24 ~ 16.78m cycles. With recursion, proving infinitely sized programs are made possible. So the solution is to divide a large program into individual sub-programs(called segment in Risc0 jargon) and have them proved one by one and aggregate the proofs into a final proof as if the whole program was proved in one go. For example, consider proving a 1b cycles program. With 16.78M maximum segment size limit, one ends up proving 60 segments. The upper bound for segment size limit is not the end of story however and one can customize it into a well-known range of [2^13 - 2^24]. Each segment limit size needs specific memory requirements shown on Table 1:<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/7/b790b7f5fb18cec94d0e621383844425862ba9fb.png" title="Screenshot from 2024-07-18 14-37-34"><img alt="Screenshot from 2024-07-18 14-37-34" height="387" src="https://ethresear.ch/uploads/default/optimized/3X/b/7/b790b7f5fb18cec94d0e621383844425862ba9fb_2_690x387.png" width="690" /></a></div><br />
Extrapolating Table 1’s values, we get 50m cycles for a program that needs 384gb of memory, in order to be proved in Risc0. Recall that Linea’s prover uses 384gb of memory to generate proofs. This is a naive 1-1 translation, but we can treat it as baseline for further testing. So, with this assumption, should one write Linea’s sequencer logic in Risc0, she would end up with a program that is 50m cycles long. Doubling cycles to ~90m, to account for aggregation won’t hurt here.<p></p>
<h4><a class="anchor" href="https://ethresear.ch#p-49694-segmentation-parallel-proving-and-decentralization-3" name="p-49694-segmentation-parallel-proving-and-decentralization-3"></a>Segmentation, parallel proving, and decentralization</h4>
<p>Recursion is a powerful idea in zkVM proving. With recursion once can get to prove seemingly large programs very quickly assuming she has a prove-ready network of machines. Table 2 shows a segmented prove session for a 90m cycles program on a pretty weak machine(8+ years old, Intel core i7 5500U(2C 4T), 16gb memory):<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/0/9/091af39f1eb4b3465f1de18222eed6c4d1051edb.png" title="Screenshot from 2024-07-18 14-47-06"><img alt="Screenshot from 2024-07-18 14-47-06" height="235" src="https://ethresear.ch/uploads/default/optimized/3X/0/9/091af39f1eb4b3465f1de18222eed6c4d1051edb_2_690x235.png" width="690" /></a></div><p></p>
<p>As you can see, different segment size limits result in varied proving regimes. In Table 2, two columns are colored in green, 2^18 and 2^19. Consulting Table 1, we would get 2gb and 4gb of required memory to prove them respectively. These columns are sweet spots for any zkVM proving network whose nodes are presumably weak. Focusing on the 2^19 segment size limit, to prove a 90m cycles program, one would need at least 168 nodes in order to prove the program in 4 minutes and 9 seconds. But 168 nodes is a faulty assumption. In reality, if a p2p network is to undertake the proving job, it needs to have redundancy values of 1:4 and above. The redundancy accounts for noise that is a feature of any p2p network. With 1:4 redundant nodes, 1 in every 5 nodes is assumed to be honest and the rest are time wasters. So, a 1:4 redundant p2p network needs at least of 840 nodes to get the job done.<br />
Assuming that the proving network is p2p, one can expect to obtain decentralized guarantees en route.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49694-conclusion-4" name="p-49694-conclusion-4"></a>Conclusion</h3>
<p>Here we introduced an imaginary setup to decentralize and improve L2 sequencer proving times. If the claim turns out to be legit, we would expect to improve the overall proving time for any zkVM application area. In addition, the setup provides decentralization guarantees as a side effect. While everything looks nice, we, at <a href="https://github.com/WholesumNet" rel="noopener nofollow ugc">Wholesum network</a> would like to put this setup to test and see if it works in action. If successful, a p2p verifiable compute network of 10,000 weak nodes can handle up to 10 Linea like L2s.</p>
<p>A somewhat more expanded version of this post is also available <a href="https://github.com/WholesumNet/docs/blob/779942cf6f650d24fcedf2d8da5a6dd2033a9fee/parallelization/parallelized-proving/report.pdf" rel="noopener nofollow ugc">here</a>.</p>
<p>We appreciate your feedback.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/l2-sequencer-proving-on-weak-hardware-parallelization-and-decentralization/20313">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 20 Aug 2024 15:18:35 +0000</pubDate>
</item>
<item>
<title>On Proposer Timing Games and Economies of Scale</title>
<link>https://ethresear.ch/t/on-proposer-timing-games-and-economies-of-scale/20309</link>
<guid>https://ethresear.ch/t/on-proposer-timing-games-and-economies-of-scale/20309</guid>
<content:encoded><![CDATA[
<div> 关键词：提案者时机游戏、经济规模、验证器市场份额、累积投票、网络影响

总结：

文章主要探讨了在以太坊等区块链系统中，验证器如何通过“提案者时机游戏”来影响区块构建过程，以及经济规模对这类游戏的影响。关键发现如下：

1. **累积投票与时机选择**：在任何给定时刻，累积投票百分比代表了网络上已确认的投票比例。提案者需要确保在他们决定提交区块时，累积投票百分比至少达到40%，以避免区块被后续提案者重新组织。

2. **经济规模影响**：拥有更高验证器市场份额的提案者可以更长时间地推迟区块提交，因为他们的投票量足够大，可以在最后阶段补充足够的支持，而不会面临被重新组织的风险。具体而言，每增加1%的验证器市场份额，提案者可以额外延迟大约0.03秒。

3. **市场占有率与延时关系**：一个拥有30%市场占有率的提案者理论上可以比5%市场占有率的提案者多延迟约0.8秒，同时仍能保持安全的区块提交时间，避免被重新组织。

4. **策略与风险**：高市场份额的节点可以通过更晚提交区块来最大化自己的利益，这要求节点间有协调机制，确保在提交区块前，未投票的验证器将支持该区块，而不是其父区块。这体现了经济规模带来的优势和相应的风险管理策略。

5. **网络影响与限制**：过度依赖提案者时机游戏可能会对网络稳定性产生负面影响，包括导致验证器间的不协调或竞争，以及潜在的系统效率下降。因此，研究和实践应侧重于减少此类游戏的负面影响，鼓励公平和稳定的网络运作。

通过上述分析，文章强调了经济规模在区块链网络中的作用，以及在设计和优化网络机制时需要考虑的复杂因素，以平衡效率、公平性和安全性。 <div>
<h1><a class="anchor" href="https://ethresear.ch#p-49689-on-proposer-timing-games-and-economies-of-scale-1" name="p-49689-on-proposer-timing-games-and-economies-of-scale-1"></a>On Proposer Timing Games and Economies of Scale</h1>
<p><a href="https://timing.pics">Timing games</a> are a known phenomenon (<a href="https://eprint.iacr.org/2023/760">[1]</a>, <a href="https://arxiv.org/abs/2305.09032">[2]</a> and <a href="https://ethresear.ch/t/deep-diving-attestations-a-quantitative-analysis/20020">[3]</a>). The concern is that proposer timing games come with a negative impact on the network.</p>
<p>In the following, I want to show how the success of playing proposer timing games is also a function of economies of scale.</p>
<p><strong>The main finding is:</strong><br />
<em><strong> → An entity with 30% market share can delay 0.8s longer than a 5% entity.</strong></em><br />
<em><strong> → For every 1% increase in validator market share, the delay in block proposals can increase by 0.03 seconds without facing additional reorg risk.</strong></em></p>
<p><img alt="tgeos" height="318" src="https://ethresear.ch/uploads/default/original/3X/c/5/c58fec82d848d1fb7ae0352e8c50c4e8071253c5.png" width="456" /></p>
<p><em>Special thanks to <a href="https://x.com/weboftrees">Anders</a>, <a href="https://x.com/mikeneuder">Mike</a> and <a href="https://x.com/casparschwa">Caspar</a> for feedback!</em></p>
<h2><a class="anchor" href="https://ethresear.ch#p-49689-introduction-2" name="p-49689-introduction-2"></a>Introduction</h2>
<p>A proposer must gather at least 40% of these votes to ensure their block is accepted and not reorged by the following proposer. It’s 40% because that’s the proposer boost threshold. Blocks with less than 40% attestations can be reorged by the next proposer leveraging proposer boost. The challenge for a timing-gamer lies in determining the optimal time to propose (or call getHeader). An economically rational validator would want to wait as long as possible (providing the builder with the longest possible time window) without risking a reorg.</p>
<p>First, let’s revisit the following chart from <a href="https://ethresear.ch/t/deep-diving-attestations-a-quantitative-analysis/20020">this analysis</a>:<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/2/42fe46361f7b2a22bd61c0195f719a57df04d64d.png" title="42fe46361f7b2a22bd61c0195f719a57df04d64d"><img alt="42fe46361f7b2a22bd61c0195f719a57df04d64d" height="304" src="https://ethresear.ch/uploads/default/optimized/3X/4/2/42fe46361f7b2a22bd61c0195f719a57df04d64d_2_690x304.png" width="690" /></a></div><p></p>
<p><strong>~80% of all attestations are seen until second 5 in the slot</strong>. The <strong>40% threshold is reached somewhere around second 3.8</strong>. Thus, assuming zero latency, a block published at second 3.8 should still be able to receive 60% of attestations.</p>
<p><strong>In the following, we refer to this curve as <span class="math">C(t)</span>.</strong></p>
<h2><a class="anchor" href="https://ethresear.ch#p-49689-initial-setup-3" name="p-49689-initial-setup-3"></a>Initial Setup</h2>
<p>The core idea is to determine how the cumulative votes cast by validators evolve over a slot and how a proposer’s control over a portion of these validators may influence the optimal timing of their block proposal.</p>
<p>Given that <span class="math">C(t)</span> represents the cumulative percentage of votes cast by time <span class="math">t</span>, the proposer controls <span class="math">x\%</span> of validators, and needs to ensure that they can still reach at least 40% by the time they propose, we start with the following condition:</p>
<div class="math">
x + (1 - C(t)) \times (1 - x) \geq 0.4
</div>
<p>In this equation:</p>
<ul>
<li><strong><span class="math">(1 - C(t)) \times (1 - x)</span>:</strong> The remaining uncast votes from validators not controlled by the proposer, which could support the proposer’s block.</li>
</ul>
<blockquote>
<p>Note that <strong><span class="math">x \times C(t)</span></strong> would be the portion of votes from the proposer’s validators already included in <span class="math">C(t).</span></p>
</blockquote>
<p>Two assumptions are important to stress:</p>
<ul>
<li><strong>Coordination</strong>: It is assumed that validators coordinate when attesting, e.g. using a central oracle that provides the commands.</li>
<li><strong>Honest Validators</strong>: All validators who have not yet voted at the time of the block proposal will vote for the proposed block (and not the parent block). See <a href="https://github.com/ethereum/consensus-specs/blob/b2f2102dad0cd8b28a657244e645e0df1c0d246a/specs/phase0/validator.md#phase-0----honest-validator">honest validator specs</a>.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-49689-simplifying-the-equation-4" name="p-49689-simplifying-the-equation-4"></a>Simplifying the Equation</h3>
<p>We rearrange the initial equation to find the threshold for <span class="math">C(t)</span>, the cumulative percentage of votes that can be cast before the proposer must act:</p>
<div class="math">
x + (1 - C(t)) \times (1 - x) \geq 0.4
</div>
<p>Expanding and simplifying:</p>
<div class="math">
(1−C(t))×(1−x) = 1 - x - C(t) + C(t) \times x 
</div>
<div class="math">
1 - C(t) + C(t) \times x \geq 0.4
</div>
<p>Finally, solving for <span class="math">C(t)</span>:</p>
<div class="math">
C(t) \leq \frac{0.6}{1 - x}
</div>
<p>Find the complete derivation <a href="https://hackmd.io/L0A6zeBZSzGew2Ni0AzFVQ">here</a>.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49689-interpretation-5" name="p-49689-interpretation-5"></a>Interpretation</h3>
<p>This simplified equation <span class="math">C(t) \leq \frac{0.6}{1 - x}</span> means that the proposer can safely propose as long as the cumulative attestations <span class="math">C(t)</span> remain below the threshold defined by <span class="math">\frac{0.6}{1 - x}</span>.</p>
<ul>
<li><strong><span class="math">C(t)</span>:</strong> The cumulative percentage of votes cast by time <span class="math">t</span>.</li>
<li><strong><span class="math">x \%</span>:</strong> The percentage of total validators controlled by the proposer.</li>
<li><strong><span class="math">0.4</span>:</strong> The 40% threshold needed to secure a majority (<span class="math">1-0.4=0.6)</span>.</li>
</ul>
<p>The equation ensures that the proposer, with their share of validators, can still influence the outcome favorably by proposing before the cumulative attestations exceed this threshold.</p>
<p>A node operator with many validators can risk a few seconds more than a small-size operator, knowing that their own validators will never vote against them.</p>
<p><strong>The following chart shows the effects of economies of scale and answers the question of <em>how long a node operator with <em>x%</em> market share can maximally wait until the point it won’t be able to receive at least 40% of all attestations anymore</em>.</strong></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/2/e24d2c92d6d6e9a6965edcac0b21f454c6404795.png" title="timing_games_proposer_share"><img alt="timing_games_proposer_share" height="383" src="https://ethresear.ch/uploads/default/optimized/3X/e/2/e24d2c92d6d6e9a6965edcac0b21f454c6404795_2_690x383.png" width="690" /></a></div><p></p>
<p>The “<em>seconds in slot</em>” values on the y-axis are <code>attestation_seen</code> timestamps that are not corrected by the time required for block propagation and verification. Since those numbers are just constants impacting the absolute values on the y-axis, this doesn’t matter in making the relative impact of market share on the limits of timing games visible.</p>
<p><strong>We can see that a node operator with 30% of the market share can potentially wait 0.8 seconds longer than a node operator with 5% market share while risking the same.</strong></p>
<h2><a class="anchor" href="https://ethresear.ch#p-49689-in-python-6" name="p-49689-in-python-6"></a>In Python</h2>
<p>Using Python, we can calculate the latest “safe” proposal time for different percentages of validator control. Here’s the key part of the implementation:</p>
<pre><code class="lang-python">import numpy as np
from scipy.interpolate import interp1d

# Provided cumulative attestation data (seconds, % of casted attestations)
data = [
     (0.791, 0.0005390835579514825),
     # (additional data points omitted for brevity)
     (2.228, 0.05444743935309973),
     (2.464, 0.10835579514824797),
     (2.639, 0.16226415094339622),
     (2.777, 0.21617250673854446),
     (2.932, 0.27008086253369273),
     (3.104, 0.323989218328841),
     (3.308, 0.3778975741239892),
     (3.627, 0.43180592991913747),
     (4.069, 0.4857142857142857),
     (4.25, 0.539622641509434),
     (4.407, 0.5935309973045823),
     (4.576, 0.6474393530997304),
     (4.723, 0.7013477088948787),
     (4.898, 0.7552560646900269),
     (5.039, 0.8091644204851752),
     (5.245, 0.8630727762803234),
     (5.521, 0.9169811320754717),
     (6.187, 0.9708894878706199)
]

# Extracting the times and cumulative attestation percentages
times = np.array([point[0] for point in data])
cumulative_attestations = np.array([point[1] for point in data])

# Interpolating the cumulative attestation function
cumulative_attestation_func = interp1d(times, cumulative_attestations, kind='linear', fill_value="extrapolate")

# Function to calculate the latest time a proposer with x% control can safely propose a block
def calculate_latest_proposal_time(x):
    threshold = 0.5 / (1 - x)
    
    for t in np.linspace(times[0], times[-1], 1000):
        if cumulative_attestation_func(t) &gt; threshold:
            return t
    return None

</code></pre>
<h1><a class="anchor" href="https://ethresear.ch#p-49689-conclusion-7" name="p-49689-conclusion-7"></a>Conclusion</h1>
<p>By understanding and calculating the relationship between validator market share and cumulative attestations, proposers can optimize their proposal timing to minimize the likelihood of reorgs while maximizing profits.</p>
<p>Such strategies could be improved by checking which CL client the subsequent validator runs, or, even simpler, the slot index in an epoch. Based on that information one can better estimate the chances of getting reorged (e.g. if it’s Teku, Nimbus, Lodestar, or the last slot in an epoch, then the reorg probability is significantly lower because no honest reorg strategy is implemented).</p>
<p>Pushing proposer timing games to their limits has a <a href="https://ethresear.ch/t/on-attestations-block-propagation-and-timing-games/20272">negative impact on attesters</a> and can have cascading effects: If validators realize they miss out on profits because they vote for the wrong block too often, they might start delaying their attestation.</p>
<p><strong>Ultimately, pushing timing games to their limits can have a detrimental impact on the network. Furthermore, validator coordination that goes beyond running multiple validators from a single node shouldn’t be tolerated/supported. Now, it is important to follow/contribute to block construction research and find ways to <a href="https://eips.ethereum.org/EIPS/eip-7716">reduce the profitability of timing games</a> or prevent them entirely.</strong></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/on-proposer-timing-games-and-economies-of-scale/20309">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 20 Aug 2024 05:48:59 +0000</pubDate>
</item>
<item>
<title>Decentralized and Verifiable Cloud Service on Ethereum</title>
<link>https://ethresear.ch/t/decentralized-and-verifiable-cloud-service-on-ethereum/20292</link>
<guid>https://ethresear.ch/t/decentralized-and-verifiable-cloud-service-on-ethereum/20292</guid>
<content:encoded><![CDATA[
<div> 关键词：Ethereum、去中心化云服务、验证性、计算密集型服务、区块链探索器

总结:

本文提出了一种基于以太坊的去中心化验证云服务协议，旨在为Web2或Web3应用提供计算密集型服务。该协议通过将前端和后端组件全栈迁移至去中心化云端，推动构建完全去中心化、可验证的Web3应用程序成为可能。其核心设计包括：

1. **服务合约**：以类似gRPC protobuf的方式在以太坊上存在，定义服务和方法，供用户调用。

2. **服务提供商注册与质押**：服务提供者需注册并质押以参与服务，多个提供者为每项服务提供服务。

3. **用户请求流程**：用户发起请求（如AI推理）时，首先通过以太坊可验证轻客户端获取可用服务提供商列表，随机选择几方并发执行请求，并验证结果一致性。

4. **仲裁机制**：当结果不一致时，触发链上仲裁过程，确保至少有一个诚实节点的存在，以保证服务正确性。

5. **收费机制**：支持订阅模型、链上支付机制和免费服务模型，以适应不同需求和预算。

此协议旨在降低成本、提高速度、增强信任度、实现可验证性，并最终推动构建更去中心化的Web3应用生态系统，例如去中心化的区块链探索器、AI平台和云游戏服务。通过集成零知识证明等技术，进一步保护用户隐私。 <div>
<p><em>by <a href="https://x.com/0x_1cc" rel="noopener nofollow ugc">KD.Conway</a></em></p>
<h2><a class="anchor" href="https://ethresear.ch#p-49659-tldr-1" name="p-49659-tldr-1"></a>TL;DR</h2>
<ul>
<li>
<p>We propose a decentralized and verifiable cloud service protocol on Ethereum, which can provide computationally intensive service to all web2 or web3 applications, making decentralized ChatGPT, decentralized blockchain explorer reality. By migrating the full stack, including frontend and backend components, to the decentralized cloud, we move toward fully decentralized and verifiable end-to-end Web3 applications.</p>
</li>
<li>
<p>The protocol operates under a minority trust assumption, requiring only one honest node to guarantee service quality. Additionally, the correctness of the cloud service is verifiable on Ethereum.</p>
</li>
<li>
<p>With near-zero on-chain costs, our decentralized cloud service platform can be even more affordable than traditional centralized options.</p>
</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-49659-protocol-overview-2" name="p-49659-protocol-overview-2"></a>Protocol Overview</h2>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/8/d816db0bb6bbc127b1a5f2fa1f320e7c923dbb77.png" title="ecs"><img alt="ecs" height="331" src="https://ethresear.ch/uploads/default/optimized/3X/d/8/d816db0bb6bbc127b1a5f2fa1f320e7c923dbb77_2_690x331.png" width="690" /></a></div><p></p>
<p>A service contract exists on Ethereum, functioning similarly to a gRPC protobuf. This contract defines the service, and the functions within it specify the methods that can be invoked.</p>
<p>Each service provider must register and stake on the service contract. For each service, multiple providers will be available to offer the service.</p>
<p>When a user initiates a service request, such as requesting an AI inference from an LLM model:</p>
<ul>
<li>
<p>The user first utilizes a verifiable Ethereum light client, such as Helios, to retrieve the list of available service providers from the on-chain service contract.</p>
</li>
<li>
<p>The user randomly selects several providers from this list.</p>
</li>
<li>
<p>The user then sends off-chain transactions to these selected providers in parallel. These off-chain transactions are essentially the same as calling the corresponding service function in the smart contract, but they use a different chain ID. This specific chain ID indicates that the transaction is intended to call a cloud service rather than perform an on-chain transaction on Ethereum.</p>
</li>
<li>
<p>The service providers execute the required computations in their local environments according to the program defined in the corresponding function in the service contract. They then return the responses to the user. Each response is signed by the service provider and includes the user’s transaction hash and the results.</p>
</li>
<li>
<p>Upon receiving the responses from the selected providers, the user first verifies the signatures and checks the consistency of the results.</p>
<ul>
<li>
<p>If the results are consistent, the service is considered to have functioned correctly, and no further action is required.</p>
</li>
<li>
<p>If there is a discrepancy in the results, this indicates the presence of at least one malicious service provider. In this case, the user submits the providers’ responses to the on-chain arbitration contract. This triggers a process where the service providers must defend the accuracy of their results. The on-chain arbitration process is detailed in the following section.</p>
</li>
</ul>
</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-49659-service-contract-3" name="p-49659-service-contract-3"></a>Service Contract</h2>
<p>The design of the service contract is akin to the design of gRPC. A new service contract corresponds to a new service in gRPC, and the functions defined in the service contract specify the methods that can be invoked. Due to the constraints of smart contracts, we cannot implement complex computations, such as AI computations, directly within the smart contract. Instead, we define a standard for writing a program, which is then uploaded to decentralized DA services, with the program’s hash stored in the on-chain smart contract.</p>
<p>Following the design principle of “Separate Execution from Proving,” there are two implementations for the service program. One is compiled for native execution, optimized for speed, and can leverage multithreaded CPUs and GPUs to accelerate execution. The other implementation is for proving; the service program is compiled into machine-independent code, allowing us to use zkVM (zero-knowledge virtual machine) or fpVM (fraud-proof virtual machine) to generate proofs. This dual-target approach ensures fast execution, while proving is based on the machine-independent code.</p>
<p>For example, consider matrix multiplication. Native execution utilizes GPU computation (e.g., CUDA) for acceleration. During the proving phase, the service program is compiled into machine-independent instructions, which can be executed in zkVM or fpVM. Both implementations ensure consistent execution results.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/1/b1ccfdf9e6f994151293a835a4619dca5e974865.png" title="output"><img alt="output" height="151" src="https://ethresear.ch/uploads/default/optimized/3X/b/1/b1ccfdf9e6f994151293a835a4619dca5e974865_2_690x151.png" width="690" /></a></div><p></p>
<p>When processing user requests, service providers will run the program in the native execution environment and return the results to the users. Only when on-chain arbitration is required will the service providers run the program for proving. This approach allows service providers to handle requests as quickly as possible in most cases.</p>
<p>Additionally, the service program can be configured to read data from trustworthy sources, such as Ethereum or other blockchains, as well as from decentralized, trustworthy data storage providers. This flexibility allows the service program to function as a blockchain explorer, an AI service, or a decentralized search engine.</p>
<p>A demo version of the service contract is shown below.</p>
<pre><code class="lang-solidity">contract Service {

    // address =&gt; web2 domain
    mapping(address =&gt; string) serviceProviderHost;

    address[] serviceProviders;

    // function selector =&gt; programHash
    mapping(bytes4 =&gt; bytes32) programHashs;

    event Request(
        address account,
        bytes4 functionSelector,
        bytes32 programHash,
        bytes input
    );

    function func1(bytes calldata input) public {
        emit Request(msg.sender, this.func1.selector, programHashs[this.func1.selector], input);
    }
}
</code></pre>
<p>Note that <code>func1</code> specifies the method that can be called. When a user wants to call <code>func1</code>, instead of sending an on-chain transaction on Ethereum, the user needs to send an off-chain transaction directly to the service providers. Besides, the user can obtain the list of service provider addresses, along with their corresponding Web2 domains using Ethereum verifiable light client.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49659-onchain-arbitration-4" name="p-49659-onchain-arbitration-4"></a>Onchain Arbitration</h2>
<p>We support multiple proving systems for on-chain arbitration, including zero-knowledge proofs, Trusted Execution Environments (TEE), and fraud-proof systems. For demonstration purposes, we focus on the fraud-proof system, as it offers lower generation costs compared to zero-knowledge proofs and does not require specific hardware. In previous work, we demonstrated the ability to generate fraud proofs for extremely large AI models. For more details, please refer to opML (<a class="inline-onebox" href="https://arxiv.org/abs/2401.17555" rel="noopener nofollow ugc">[2401.17555] opML: Optimistic Machine Learning on Blockchain</a>).</p>
<p>The on-chain arbitration process using the fraud-proof system proceeds as follows:</p>
<ul>
<li>
<p>If a user receives inconsistent results from the service providers, they submit the providers’ responses to the on-chain arbitration contract, initiating an interactive dispute game with all the involved providers.</p>
</li>
<li>
<p>At this point, the service providers must run the proving-version of the service program in their local fraud-proof VMs to generate the fraud proof, which they then submit to the on-chain arbitration contract to defend their results. For more details on the interactive dispute game, refer to the fraud-proof system design.</p>
</li>
<li>
<p>Service providers who supplied incorrect results will lose the dispute game, resulting in their staked amount being slashed. The slashed stake will be distributed to the winners of the dispute game, as well as to the user, as compensation.</p>
</li>
</ul>
<p>This on-chain arbitration mechanism ensures that only one honest node is required to guarantee the correctness of the provided service. As a result, the protocol relies on a minority trust assumption and inherits security from Ethereum. Assuming at least one honest node and the safety of Ethereum, the protocol can guarantee the correctness of the service.</p>
<p>It’s important to note that on-chain arbitration only occurs when some service providers produce incorrect results. In typical cases, no on-chain interaction is needed, which allows the service to operate as quickly as current centralized cloud service providers.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49659-charging-mechanism-5" name="p-49659-charging-mechanism-5"></a>Charging Mechanism</h2>
<p>There are several possible charging mechanisms:</p>
<ul>
<li>
<p><strong>Subscription Model:</strong> This is similar to the Web2 approach, where the charging mechanism can be conducted off-chain. For example, to use ChatGPT via an API for commercial purposes, you would pay OpenAI a monthly fee to access their services. Multiple service providers can offer the service, allowing for competition and choice.</p>
</li>
<li>
<p><strong>On-Chain Payment Mechanism:</strong> Paying for each request on-chain can be costly due to transaction fees. Batching and rolling up these requests and payments can significantly reduce on-chain transaction costs. One possible approach is to use payment channels to pay for requests. Alternatively, service providers could generate service proofs and claim fees as follows:</p>
<ul>
<li>
<p>A service agreement contract specifies the price for each service request.</p>
</li>
<li>
<p>Users first stake funds into the service agreement contract.</p>
</li>
<li>
<p>Service providers can claim their fees by submitting service proofs to the on-chain service agreement contract. To minimize transaction costs, providers can batch and roll up user requests.</p>
</li>
<li>
<p>The on-chain service proof is a zk-proof, which verifies that the service provider has delivered a certain number of responses to users. The provider can then claim the corresponding service fees according to the agreement contract. This proof ensures the correctness of the user’s request transaction signature, the service provider’s response signature, and the transaction nonce.</p>
</li>
</ul>
</li>
<li>
<p><strong>Free Service Model:</strong> Another approach is for companies to cover the service fees by the themselves (currently, the web2 companies pay the cloud service fee by themselves), offering free services to users while generating revenue through other means, such as advertising or VIP services.</p>
</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-49659-advantages-6" name="p-49659-advantages-6"></a>Advantages</h2>
<ul>
<li>
<p>This decentralized cloud service can be cheaper than centralized cloud services while maintaining similar speed.</p>
<ul>
<li>
<p><strong>Cost-Effectiveness:</strong> Decentralized servers can be significantly cheaper than centralized cloud servers. For example, <a href="http://io.net" rel="noopener nofollow ugc">io.net</a> has shown that the cost of decentralized GPUs can be as low as one-third of the cost of AWS. For services with lower security requirements, such as using LLMs for personal queries, using just two nodes is often sufficient. Additionally, a random check mechanism can be adopted, querying one node most of the time and occasionally checking another to verify correctness. This setup can be more cost-effective than centralized platform.</p>
</li>
<li>
<p><strong>Scalability and Speed:</strong> This platform can outperform centralized systems, especially for computationally intensive tasks. A decentralized cloud service platform operates on an N-to-M model (N users with M servers, where the number of servers can be infinite), whereas centralized platforms use an N-to-1 model (N users with a single super server). This allows a decentralized cloud service platform to scale more effectively. For instance, a centralized AI platform like ChatGPT may slow down during peak times because it can’t scale its computing power quickly enough. In contrast, decentralized platform can dynamically distribute the load across many servers, ensuring faster response times even during heavy usage.</p>
</li>
</ul>
</li>
<li>
<p><strong>Trustless and Verifiable:</strong> The protocol operates under a minority trust assumption, requiring only one honest node to guarantee service quality. Additionally, the correctness of the cloud service is verifiable on Ethereum.</p>
</li>
<li>
<p><strong>Censorship-Resilient:</strong> This platform contributes to a more robustly decentralized Web3, enhancing censorship resistance.</p>
</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-49659-toward-fully-decentralized-and-verifiable-web3-application-7" name="p-49659-toward-fully-decentralized-and-verifiable-web3-application-7"></a>Toward Fully Decentralized and Verifiable Web3 Application</h2>
<p>With this protocol, we can move toward fully decentralized and verifiable Web3 applications.</p>
<p><strong>Decentralized and Verifiable Blockchain Explorer:</strong> Currently, blockchain explorers like Etherscan are hosted by centralized entities, and the results they present are not verifiable. If such an explorer were hacked, it could display malicious and misleading information, such as fake transactions or contracts, potentially leading to phishing scams. By migrating the entire blockchain explorer—including both the frontend and backend services—to our platform, we can ensure full verifiability and robust security for the blockchain explorer.</p>
<p><strong>Decentralized, Verifiable, Faster, and Cheaper AI Platform:</strong> This protocol enables the creation of a fully decentralized, verifiable, and cost-effective AI platform. By moving the entire stack, including both frontend and backend services as well as AI computation, to a decentralized cloud, we can build an AI platform that is not only more affordable but also potentially faster than centralized alternatives.</p>
<p><strong>Decentralized Cloud Gaming:</strong> Some games require high-end hardware, such as powerful GPUs and CPUs, leading game companies to move their games to cloud services, reducing the hardware requirements for customers. We can similarly bring Web3 games to our platform. Since our platform is verifiable on Ethereum, game reward settlements can be easily managed through smart contracts.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49659-further-discussion-8" name="p-49659-further-discussion-8"></a>Further Discussion</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-49659-updating-the-state-9" name="p-49659-updating-the-state-9"></a>Updating the State</h3>
<p>In the previous discussion, the service program operates under a stateless design, meaning it does not modify its internal state. However, the data source used by the service program is upgradable. For instance, if a service program uses Ethereum as its data source, users can interact with smart contracts on Ethereum to update the state. The service program can then utilize the latest Ethereum state as its data source, enabling the implementation of a decentralized explorer.</p>
<p>If updating the internal state of the service program is required, a state machine replication network must be established among the service providers. In this case, each service program would correspond to a layer 2 or layer 3 blockchain on Ethereum. When users invoke a method that updates the internal state, they would send the transaction to the corresponding layer 2 or layer 3 blockchain. The service providers would then reach a consensus on the execution results of that transaction and update the internal state accordingly. Periodically, the layer 2 blockchain would roll up the transactions and its latest state back to Ethereum.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49659-verifiable-fhe-10" name="p-49659-verifiable-fhe-10"></a>Verifiable FHE</h3>
<p>To ensure user privacy, Fully Homomorphic Encryption (FHE) can be integrated into our protocol. In this case, the FHE computation would be incorporated into the service program. Instead of sending plaintext data to the service providers, users would encrypt their input and send only the ciphertext, thereby preserving their privacy. Additionally, if on-chain arbitration is triggered, the FHE service program would be compiled into machine-independent instructions, and a fraud proof or zk-proof would be generated to make the FHE computation fully verifiable.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49659-related-work-and-comparison-11" name="p-49659-related-work-and-comparison-11"></a>Related Work and Comparison</h2>
<p><strong>Comparison with Web3URL</strong></p>
<p>Web3URL (<a href="https://w3url.w3eth.io/" rel="noopener nofollow ugc">https://w3url.w3eth.io/</a>) is an interesting project that transforms Ethereum into an unstoppable decentralized web server. Our protocol can be seen as a significant extension of Web3URL. In Web3URL, service functions must be written within smart contracts, which naturally limits large-scale applications. In contrast, our protocol supports complex service programs, such as AI computations, and provides flexible access to large-scale data, making decentralized ChatGPT and decentralized explorers a reality.</p>
<p><strong>Comparison with ICP</strong></p>
<p>The Internet Computer (ICP: <a href="https://internetcomputer.org/" rel="noopener nofollow ugc">https://internetcomputer.org/</a>) hosts decentralized serverless compute, similar to our goal of creating a decentralized cloud service platform. However, we differ from ICP in several key aspects:</p>
<ul>
<li>
<p><strong>Ethereum Integration:</strong> We are building on Ethereum, allowing us to inherit its security features.</p>
</li>
<li>
<p><strong>Higher Security:</strong> We achieve a higher level of security compared to ICP. While ICP operates in a Byzantine Fault Tolerance (BFT) network under a majority trust assumption—requiring that most nodes in the subnet are honest—we adopt an approach similar to rollups, with on-chain arbitration ultimately reverting to Ethereum. This allows us to guarantee correctness under a minority trust assumption, where just one honest node can ensure the integrity of our protocol.</p>
</li>
<li>
<p><strong>Complex Computation:</strong> Following the design principle of “Separate Execution from Proving,” we can handle complex computations natively, such as LLM inference or even fine-tuning. In contrast, service programs in ICP always run within canisters, which significantly limits their applicability for large-scale computations.</p>
</li>
</ul>
<p>If you are interested in this project or have suggestions for improvements, please feel free to reach out to me.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/decentralized-and-verifiable-cloud-service-on-ethereum/20292">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sat, 17 Aug 2024 11:48:03 +0000</pubDate>
</item>
<item>
<title>Censorship Insurance Markets for BRAID</title>
<link>https://ethresear.ch/t/censorship-insurance-markets-for-braid/20288</link>
<guid>https://ethresear.ch/t/censorship-insurance-markets-for-braid/20288</guid>
<content:encoded><![CDATA[
<div> 关键词：BRAID、流动性要求、用户体验、Censorship Insurance（Censorship保险）、市场解决方案

总结:

文章探讨了BRAID（一种旨在通过条件性打赏机制提高以太坊去中心化程度的多提案机制）在提升 censorship resistance（审查抗性）方面的创新，同时也指出了其在用户体验（UX）上的挑战，特别是由于流动性需求导致的问题。文章提出了两种潜在的解决方案：一种是“Proof of Post-State Liquidity”（后状态流动性证明），即用户需要提供证明，证明在交易执行后他们有足够的流动性来支付Censorship保险费用；另一种是引入Censorship Insurance（Censorship保险）市场，用户可以通过购买保险来降低实际支付的费用，并由第三方机构来承担额外的风险。

BRAID机制通过将交易分发到多个子链中并采用特定的排序规则，使得用户能够在不显著增加交易费用的情况下获得更高的去中心化程度。然而，为了确保交易能够被包括在区块链中，用户需要有足够的流动性来支付潜在的Censorship保险费用，这在大额交易时会成为负担。文章提出通过Censorship保险市场，可以降低用户为保证交易成功而需要准备的流动性，同时市场本身也能通过竞争机制调整保险费率，为用户提供更合理的价格。

文章最后指出，虽然Censorship保险市场的构建可能面临初期的挑战，但通过与应用或钱包平台的合作，或者建立基于请求报价（RFQ）的市场机制，可以有效地解决这一问题，从而为用户提供更好的交易体验，同时保持BRAID机制的去中心化优势。 <div>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/f/bf7665c93b36acdfa0cb7c8ed757aa3ef87f101f.jpeg" title="BRAID"><img alt="BRAID" height="500" src="https://ethresear.ch/uploads/default/original/3X/b/f/bf7665c93b36acdfa0cb7c8ed757aa3ef87f101f.jpeg" width="500" /></a></div><p></p>
<p>By: <a href="https://x.com/_jonahb_">Jonah Burian</a> and <a href="https://x.com/BenLevy0">Ben Levy</a></p>
<p><em>Tl;dr: We point out that BRAID’s liquidity requirements lead to poor user UX and suggest censorship insurance markets as a potential solution.</em></p>
<p><em>Thanks to <a href="https://x.com/maxresnick1">Max Resnick</a> and <a href="https://x.com/davidecrapis">Davide Crapis</a> for the feedback.</em></p>
<h2><a class="anchor" href="https://ethresear.ch#p-49651-intro-1" name="p-49651-intro-1"></a>Intro</h2>
<blockquote>
<p>“The greatness of <s>America</s> <em>Ethereum</em> lies not in being more enlightened than any other <s>nation</s> <em>blockchain</em>, but rather in her ability to repair her faults.” <em>- Alexis de Tocqueville</em></p>
</blockquote>
<p>Censorship resistance (CR) is one of the core security properties of a blockchain.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/7/c7e58a93fe9b45a91ecf29a6aefa91567c310262.png" title="CR"><img alt="CR" height="180" src="https://ethresear.ch/uploads/default/optimized/3X/c/7/c7e58a93fe9b45a91ecf29a6aefa91567c310262_2_690x180.png" width="690" /></a></div><p></p>
<p>Ethereum gifts proposers with one-slot monopolies on transaction inclusion, creating a principal-agent problem and a single point of failure. A censoring party can bribe the current proposer to censor a transaction.</p>
<p>There has been considerable work to mitigate this problem. A key insight is that the weak link problem of a single proposer results in weak CR. Multi-proposer schemes like <a href="https://www.youtube.com/watch?v=mJLERWmQ2uw">BRAID</a> and <a href="https://ethresear.ch/t/fork-choice-enforced-inclusion-lists-focil-a-simple-committee-based-inclusion-list-proposal/19870/1">FOCIL</a> can correct this principal-agent problem.</p>
<p>In this piece, we focus on BRAID, a multi-proposer mechanism that has garnered significant recent attention. It aims to increase CR in a capital-efficient way via a conditional tipping mechanism (explained below).</p>
<p>One challenge in this approach, the need for a deterministic ordering rule, is already well understood. <em><strong>In this piece we identify another challenge—liquidity requirements that adversely affect UX—and propose a few potential solutions.</strong></em></p>
<h2><a class="anchor" href="https://ethresear.ch#p-49651-braid-at-a-high-level-2" name="p-49651-braid-at-a-high-level-2"></a>BRAID at a High Level:</h2>
<p>BRAID runs <span class="math">k</span> subchains in parallel, each with a unique proposer. Block <span class="math">n</span> of Ethereum is the union of transactions from block <span class="math">n</span> of the <span class="math">k</span> subchains, with a special ordering rule applied to order this unordered set.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49651-tipping-in-braid-3" name="p-49651-tipping-in-braid-3"></a>Tipping in BRAID</h3>
<p>Bidders submit a conditional twin tip <span class="math">(t,T)</span> which depends on the number of proposers who include the transaction. If only a single proposer includes a transaction, they receive <span class="math">T</span>; if multiple proposers include the transaction, they split <span class="math">t</span>.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49651-tipping-properties-4" name="p-49651-tipping-properties-4"></a>Tipping Properties</h3>
<p>Let <span class="math">ϕ(t,T)</span> be the minimum cost to censor a BRAID transaction. It has been <a href="https://arxiv.org/abs/2301.13321">shown</a> that <span class="math">ϕ(t,T)=kT</span>.</p>
<p>The goal of BRAID is that users will most likely never actually have to pay <span class="math">T</span>; instead, they pay <span class="math">t</span>, which can be much lower than <span class="math">T</span>.</p>
<p>This multi-dimensional tip disentangles the cost of inclusion (for the transacting party) from the cost of censoring such that <span class="math">t&lt;&lt;T</span>.</p>
<p>Simply put, a user get’s <span class="math">kT</span> worth of CR while (usually) only paying <span class="math">t</span>.</p>
<p><strong>How Users Will Tip:</strong></p>
<ul>
<li><span class="math">T</span>: From a user’s perspective, they set <span class="math">T=\frac{V}{k}</span> where <span class="math">V</span> is the value the user places in their transaction not being censored.</li>
<li><span class="math">t</span>: In current BRAID specs, the ordering of transactions depends on <span class="math">t</span>, with more favorable ordering (i.e., coming first) given to those with the highest <span class="math">t</span>. Therefore, a user will choose their <span class="math">t</span> based on where they want to be in the ordering.</li>
</ul>
<p><em>Note that if a user does not care about CR, they can set <span class="math">T=t</span> and send their transaction to just one proposer.</em></p>
<h2><a class="anchor" href="https://ethresear.ch#p-49651-the-ux-challenge-5" name="p-49651-the-ux-challenge-5"></a>The UX Challenge:</h2>
<p>While a user will only pay <span class="math">t</span> for their transaction, they need to have <span class="math">T</span> available to make a credible promise to the protocol that they can pay <span class="math">T</span>. Hence a user needs to have <span class="math">T</span> of additional available liquidity to make a transaction. We saw before that <span class="math">T \propto V</span>: <span class="math">T</span> tends to scale with the value of the transaction. This burdens users with a liquidity requirement.</p>
<p>For example, say a user wants to sell $5M of ETH due to impending interest rate fears and values censorship resistance at $1M. Let’s say there are 4 shards, i.e., <span class="math">k=4</span>. The user needs to have $250k of additional unpledged liquidity available just to exit their position. This hampers the UX of on-chain finance by placing additional and obscure liquidity requirements on participants that scale with the value of their positions.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49651-fixes-6" name="p-49651-fixes-6"></a>Fixes:</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-49651-proof-of-post-state-liquidity-7" name="p-49651-proof-of-post-state-liquidity-7"></a><strong>Proof of Post-State Liquidity</strong></h3>
<p><strong>Idea:</strong> A user submits a transaction with a proof that they will have enough liquidity to pay <span class="math">T</span> if necessary after their transaction. In the case before, the proof will show that the transaction will give the user $1M of liquidity so they could afford the <span class="math">T=</span> $250k if necessary.</p>
<p><strong>Problem:</strong> This assumes that a proposer has a good understanding of the post-state of a transaction. Most financial transactions interact with shared state, and as a result, transaction ordering is needed to know the post-state. This knowledge relies on the final ordering so we can’t include it as an input to the transaction. Even when there is a reasonable lower bound on post-state available liquidity, establishing it would (unrealistically) require bespoke proofs for each transaction type.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49651-censorship-insurance-ci-8" name="p-49651-censorship-insurance-ci-8"></a>Censorship Insurance (CI)</h3>
<p><strong>Idea:</strong> A third party–the CI provider–can sponsor the escrow of <span class="math">T</span> for the transaction. Users will have to pay an insurance premium of <span class="math">rT</span> to the CI provider, where <span class="math">r</span> represents the rate (mostly) based on the likelihood of censorship. CI providers are thus assessing the rewards of censoring the transaction in real time to ensure it is below <span class="math">kT</span>.</p>
<p>To prevent an attack where a user purchases insurance and then only sends their tx to one proposer whom they are colluding with, the CI should be (one of) the relayer(s) for the tx. This mirrors how gas sponsorship works and indeed CI insurance should likely just be included in a gas sponsorship service.</p>
<p>Effectively a user pays a total of <span class="math">t + rT</span> for their transaction and only needs to have <span class="math">t + rT</span> on hand as opposed to <span class="math">T</span>, which is frequently more than <span class="math">t + rT</span>.</p>
<p>An additional benefit of this scheme is that a marketplace of at least two CI providers will conveniently alert users when their <span class="math">T</span> is too low and there is a high risk of censorship because they’ll refuse to censorship-insure the transaction at a reasonable rate.</p>
<p><strong>Problem:</strong> It will be difficult to bootstrap a two-sided marketplace for this from scratch.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49651-ci-market-structure-9" name="p-49651-ci-market-structure-9"></a>CI Market Structure</h3>
<p>In practice applications or wallets will likely claim jurisdiction over this issue. One possible solution to the bootstrapping problem, therefore, is for applications and/or wallets to sign wholesale agreements with CI providers à la PFOF.</p>
<p>While the above solution likely works fine, another option is to create a proper on-chain market with e.g. an RFQ for each transaction whose sender wishes to purchase censorship resistance for.</p>
<p><img alt="snake" height="240" src="https://ethresear.ch/uploads/default/original/3X/0/4/049237e341dc88cd24cde968c71e70ce689c3444.png" width="240" /></p>
<p>This market, fittingly, would benefit from the CR properties of BRAID.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49651-conclusion-10" name="p-49651-conclusion-10"></a>Conclusion</h2>
<p>BRAID is still in its early days as a proposal. The UX issue of liquidity requirements has not been sufficiently explored, though there are promising signs that we can reasonably punt the issue to the application layer. For next steps, we suggest further exploration of the feasibility of CI markets.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49651-previous-work-11" name="p-49651-previous-work-11"></a>Previous work:</h2>
<ul>
<li><a href="https://arxiv.org/abs/2301.13321">Censorship Resistance in On-Chain Auctions</a>: Elijah, Max, Mallesh</li>
<li><a href="https://ethresear.ch/t/concurrent-block-proposers-in-ethereum/18777">Concurrent Block Proposers in Ethereum</a>: Mike, Max</li>
<li><a href="https://blog.duality.xyz/introducing-multiplicity/">Introducing Multiplicity</a>: Duality blog</li>
<li><a href="https://efdn.notion.site/ROP-9-Multiplicity-gadgets-for-censorship-resistance-7def9d354f8a4ed5a0722f4eb04ca73b">ROP-9: Multiplicity gadgets for censorship-resistance</a> RIG</li>
<li><a href="https://www.youtube.com/watch?v=mJLERWmQ2uw">BRAID: Implementing Multiple Concurrent Block Proposers</a>: Max</li>
<li><a href="https://ethresear.ch/t/fork-choice-enforced-inclusion-lists-focil-a-simple-committee-based-inclusion-list-proposal/19870/1">Fork-Choice enforced Inclusion Lists (FOCIL): A simple committee-based inclusion list proposal</a>: Thomas, Barnabé, Francesco and Julian</li>
</ul>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/censorship-insurance-markets-for-braid/20288">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 16 Aug 2024 15:36:38 +0000</pubDate>
</item>
<item>
<title>Ethereum discv5 DHT Network Health Weekly Reports</title>
<link>https://ethresear.ch/t/ethereum-discv5-dht-network-health-weekly-reports/20282</link>
<guid>https://ethresear.ch/t/ethereum-discv5-dht-network-health-weekly-reports/20282</guid>
<content:encoded><![CDATA[
<div> 关键词：ProbeLab团队、Nebula爬虫、discv5 DHT网络、健康报告、每周更新

总结:

本文概述了ProbeLab团队为监控Ethereum CL discv5 DHT网络的关键指标而开发并部署的基础设施。团队通过将Nebula爬虫适配为兼容discv5基于的网络，并收集反映P2P网络DHT层面健康状况的结果。

- **监控与报告**：每周一发布上一周的最新报告，涵盖了网络结构、规模和客户端采用率的概览，以及地理分布、版本更新趋势、基础设施设置（数据中心或非数据中心）、特定网络层协议支持情况等。
  
- **数据分析**：提供关于网络结构和设置变化的实时洞察，易于识别关键结构变化，帮助理解网络的稳健性和多样性，以及特定地区的趋势和优势。

- **贡献与参与**：鼓励社区成员提出他们认为应包含在每周报告中的重要指标，并通过评论或直接与团队联系进行反馈。

- **透明度与公开性**：报告可在线访问，详细展示了数据收集方法、过滤策略、节点分类差异及其与市场中其他工具的比较。

- **可复用资源**：提供的爬虫工具不仅用于生成报告，也便于社区成员根据自身需求进行调整和利用。

本文强调了ProbeLab团队通过持续监控和分析Ethereum CL discv5 DHT网络的关键指标，为维护网络健康和促进社区发展做出的贡献。通过定期更新的报告，不仅提供了当前网络状态的详细视图，也为改进策略和决策提供了数据基础。鼓励社区参与和反馈机制体现了对开放合作和持续优化的承诺。 <div>
<blockquote>
<p><em>Work presented here has been carried out by the <a href="https://probelab.network" rel="noopener nofollow ugc">ProbeLab</a> team and in particular <a class="mention" href="https://ethresear.ch/u/guillaumemichel">@guillaumemichel</a> <a class="mention" href="https://ethresear.ch/u/cortze">@cortze</a> <a class="mention" href="https://ethresear.ch/u/dennis-tra">@dennis-tra</a> and Steph.</em></p>
</blockquote>
<h2><a class="anchor" href="https://ethresear.ch#p-49640-high-level-description-1" name="p-49640-high-level-description-1"></a>High Level Description</h2>
<p>The ProbeLab team has developed and deployed infrastructure to monitor several critical metrics for Ethereum’s CL discv5 DHT network. In particular, we have adapted the Nebula crawler (<a class="inline-onebox" href="https://github.com/dennis-tra/nebula/" rel="noopener nofollow ugc">GitHub - dennis-tra/nebula: 🌌 A network agnostic DHT crawler, monitor, and measurement tool that exposes timely information about DHT networks.</a>) to be compatible with discv5-based networks and are gathering results that reflect the health of the P2P network at the DHT level.</p>
<p>In this post we’re presenting a summary of what is included in the reports, but for a more complete picture of what’s there, head to: <a href="https://probelab.io/ethereum/discv5/2024-29/" rel="noopener nofollow ugc">https://probelab.io/ethereum/discv5/2024-29/</a> for the latest report.</p>
<ul>
<li>
<p>Reports are produced every Monday for the preceding week.</p>
</li>
<li>
<p>The methodology we follow for DHT Crawling, Data Filtering, Node Classification as well as the differences of our tool to alternatives in the space is given in the Methodology section: <a href="https://probelab.io/ethereum/discv5/methodology/" rel="noopener nofollow ugc">https://probelab.io/ethereum/discv5/methodology/</a>.</p>
</li>
<li>
<p>The crawler used to produce the reports can be found (and can be reused) here: <a class="inline-onebox" href="https://github.com/dennis-tra/nebula/" rel="noopener nofollow ugc">GitHub - dennis-tra/nebula: 🌌 A network agnostic DHT crawler, monitor, and measurement tool that exposes timely information about DHT networks.</a>.</p>
</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-49640-why-you-should-care-2" name="p-49640-why-you-should-care-2"></a>Why you should care</h2>
<p>The metrics included in the reports:</p>
<ul>
<li>
<p>give an overview of the network structure, size and client adoption breakdown. This helps in understanding the robustness and diversity of the network,</p>
</li>
<li>
<p>provide accurate geographic distribution of nodes in the network per client implementation over time, which can highlight regional trends and potential vulnerabilities or strengths in specific areas,</p>
</li>
<li>
<p>make it easy to spot drastic changes in the structure and setup of the network,</p>
</li>
<li>
<p>allow for monitoring of new protocol version uptake/adoption, and provide insights on whether there are adoption barriers,</p>
</li>
<li>
<p>reveal the infrastructure setup (e.g., data center-hosted vs non-data center-hosted) and cloud provider distribution per client implementation,</p>
</li>
<li>
<p>show the breakdown of nodes supporting particular network-layer protocols,</p>
</li>
<li>
<p>depict the percentage of reachable vs unreachable node records in the DHT network.</p>
</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-49640-overview-of-results-3" name="p-49640-overview-of-results-3"></a>Overview of Results</h2>
<p>We’re presenting a small fraction of the results given at <a href="https://probelab.io" rel="noopener nofollow ugc">https://probelab.io</a> to give an idea of the metrics listed. Please head there for the complete reports from Week 11 (mid-March), 2024.</p>
<p><strong>Client Diversity</strong></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/3/433c4aecd269c5dfd9291dc03bbf58624414061c.png" title="discv5-agents-overall"><img alt="discv5-agents-overall" height="375" src="https://ethresear.ch/uploads/default/optimized/3X/4/3/433c4aecd269c5dfd9291dc03bbf58624414061c_2_489x375.png" width="489" /></a></div><p></p>
<p><strong>Client Diversity Over Time</strong></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/6/2664514f07b9b5aa0f4dea5319bc82fe047fa27b.png" title="discv5-agents-overall-stacked"><img alt="discv5-agents-overall-stacked" height="375" src="https://ethresear.ch/uploads/default/optimized/3X/2/6/2664514f07b9b5aa0f4dea5319bc82fe047fa27b_2_489x375.png" width="489" /></a></div><p></p>
<p><strong>Agent version adoption over time - Example: Lighthouse</strong></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/0/a081c5b9376d2f17792caaa6b6d91b9f2e4353a0.png" title="discv5-versions-distribution"><img alt="discv5-versions-distribution" height="375" src="https://ethresear.ch/uploads/default/optimized/3X/a/0/a081c5b9376d2f17792caaa6b6d91b9f2e4353a0_2_489x375.png" width="489" /></a></div><p></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/1/61739999c2d736e11817cf8f417008e2c88869dc.png" title="discv5-agents-versions"><img alt="discv5-agents-versions" height="375" src="https://ethresear.ch/uploads/default/optimized/3X/6/1/61739999c2d736e11817cf8f417008e2c88869dc_2_489x375.png" width="489" /></a></div><p></p>
<p><strong>Country distribution of all nodes</strong></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/4/34aa9fb69b64bdf785073d62f018a4c78cbad033.png" title="discv5-geo-agent-all-bars"><img alt="discv5-geo-agent-all-bars" height="375" src="https://ethresear.ch/uploads/default/optimized/3X/3/4/34aa9fb69b64bdf785073d62f018a4c78cbad033_2_489x375.png" width="489" /></a></div><p></p>
<p><strong>Client-specific country distribution - Example: Prysm</strong></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/9/c92579370dcf03c6358536d0a358da2d79aeaa51.png" title="discv5-geo-agents-lines-prysm"><img alt="discv5-geo-agents-lines-prysm" height="375" src="https://ethresear.ch/uploads/default/optimized/3X/c/9/c92579370dcf03c6358536d0a358da2d79aeaa51_2_489x375.png" width="489" /></a></div><p></p>
<p><strong>Cloud provider distribution of all nodes</strong></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/5/55ffef8626b07b1f21845591817184f663c7c88f.png" title="discv5-cloud-agent-all-bars"><img alt="discv5-cloud-agent-all-bars" height="375" src="https://ethresear.ch/uploads/default/optimized/3X/5/5/55ffef8626b07b1f21845591817184f663c7c88f_2_489x375.png" width="489" /></a></div><p></p>
<p><strong>Cloud vs non-cloud distribution of nodes over time</strong></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/7/47638e6a177376cd847949f006ddd6ad91a73368.png" title="discv5-cloud-rate-agent-all-lines"><img alt="discv5-cloud-rate-agent-all-lines" height="375" src="https://ethresear.ch/uploads/default/optimized/3X/4/7/47638e6a177376cd847949f006ddd6ad91a73368_2_489x375.png" width="489" /></a></div><p></p>
<p><strong>Stale Peer Records over time</strong></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/9/d941c8d07f12784b4d26214ce100dcfbb8f1e99e.png" title="discv5-stale-records-mainnet-stacked"><img alt="discv5-stale-records-mainnet-stacked" height="375" src="https://ethresear.ch/uploads/default/optimized/3X/d/9/d941c8d07f12784b4d26214ce100dcfbb8f1e99e_2_489x375.png" width="489" /></a></div><p></p>
<h2><a class="anchor" href="https://ethresear.ch#p-49640-how-to-contribute-4" name="p-49640-how-to-contribute-4"></a>How to contribute</h2>
<p>Overall, we believe this set of results give an accurate view of the structure and health of the discv5 DHT network. We hope you’ll find the reports useful.</p>
<p>If there are important metrics that you believe should be part of these weekly reports, comment below, or get in touch with the team: <a href="https://www.probelab.network/contact" rel="noopener nofollow ugc">probelab.network/contact</a>.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/ethereum-discv5-dht-network-health-weekly-reports/20282">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 15 Aug 2024 16:10:39 +0000</pubDate>
</item>
<item>
<title>Autonomous Competence Identification Protocol</title>
<link>https://ethresear.ch/t/autonomous-competence-identification-protocol/20281</link>
<guid>https://ethresear.ch/t/autonomous-competence-identification-protocol/20281</guid>
<content:encoded><![CDATA[
<div> 关键词：协议、主观决策、共识、DAO、协作工具

总结:
本文探讨了一种旨在优化DAO（去中心化自治组织）和研究与开发过程中的主观决策制定的协议。该协议通过建立一套评分系统来解决常见的治理问题，促进更高效、合作的环境。其核心功能包括：
1. **量化提案者评分**：参与者对决策情境进行定义，进行多轮讨论，提供反馈，直至最终阶段匿名投票，以避免身份相关的偏见。
2. **减少沟通复杂性**：简化讨论流程，自动分配决策执行者，为后续投票系统创建议程，有效应对信息过载和决策延迟问题。
3. **激励参与与客观评价**：通过将个人贡献与组织目标直接关联，确保所有成员的活动都能推动组织向前发展，同时采用非集中化决策机制，避免内部政治影响。
4. **抵抗操纵与中央集权**：设计机制防止影响力累积和Sybil攻击，确保财务贡献与治理权力相匹配，促进组织的自我进化与分散化。
5. **适应性与扩展性**：满足不同背景和兴趣的参与者需求，即使在AI代理和自动化基础设施的背景下，也能保持高效运行。

此协议旨在革新传统组织结构，通过引入现代技术手段，增强组织决策效率和透明度，同时保护组织免受中心化倾向和外部攻击的影响，从而实现其潜力的最大化。 <div>
<p>I’m excited to share my ongoing research on a protocol designed to streamline communication and decision-making around subjective matters, particularly within DAOs and R&amp;D processes. This protocol establishes a ranking system that counters common governance issues, fostering a more collaborative and effective environment.</p>
<p>I’m posting this in the meta-innovation category because it has implications both for DAO/Consensus research and for potential collaboration tools within the Ethereum community.</p>
<p><em>Link to paper in progress: <a class="inline-onebox" href="https://github.com/peersky/papers/blob/main/acid/whitepaper.pdf" rel="noopener nofollow ugc">papers/acid/whitepaper.pdf at main · peersky/papers · GitHub</a></em></p>
<h2><a class="anchor" href="https://ethresear.ch#p-49639-tldr-1" name="p-49639-tldr-1"></a>TL’DR</h2>
<p>The protocol enables subjective decision-making and quantifies proposer ratings. Participants define a context and engage in rounds of discussion, providing and receiving feedback without revealing identities until the round concludes. This mitigates biases like the <a href="https://en.wikipedia.org/wiki/Halo_effect" rel="noopener nofollow ugc">Halo effect</a>, and collusion (sybil attack) risks.</p>
<p>Protocol streamlines discussions and enables autonomously assign competent decision makers as well as create pre-arranged agenda for any follow up voting systems (hence addresses <a href="https://www.sciencedirect.com/science/article/abs/pii/0022053176900405" rel="noopener nofollow ugc">Agenda Manipulation</a>,  ( casually explained in <a href="https://www.youtube.com/watch?v=goQ4ii-zBMw" rel="noopener nofollow ugc">this youtube video</a> ) problem</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49639-motivation-2" name="p-49639-motivation-2"></a>Motivation</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-49639-communication-complexities-hinder-decision-making-3" name="p-49639-communication-complexities-hinder-decision-making-3"></a>Communication Complexities Hinder Decision-Making</h3>
<p>Effective decision-making is hindered by communication complexities.</p>
<ul>
<li>Traditional methods (meetings, chats): don’t scale, leading to information overload and delays.</li>
<li>More stakeholders exponentially increase communication complexity, leaving less time for effective decisions.</li>
<li>Individual contributions can get lost, leading to under-appreciation and high turnover.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-49639-traditional-organizations-are-sub-optimally-managed-4" name="p-49639-traditional-organizations-are-sub-optimally-managed-4"></a>Traditional Organizations are Sub-optimally Managed</h3>
<p>Despite modern networking and project management technologies, the primary, basis of hierarchical communication hasn’t changed much over centuries. Decisions still require large centralization force, which will step in and cut opinions to shape performance capable decision.</p>
<ul>
<li>Centralized decision-making prioritizes efficiency over diverse input, fostering internal politics and biased decisions.</li>
<li>This breeds internal politics, leading to biased decisions that may harm the organization.</li>
<li>Current methods lack objective ways to measure and reward valuable contributions, limiting organizational potential.</li>
<li>Does not let organizations reach their full potential</li>
</ul>
<p>This touches every organization, including Ethereum R&amp;D.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49639-icos-do-not-work-well-for-daos-5" name="p-49639-icos-do-not-work-well-for-daos-5"></a>ICOs do not work well for DAOs</h3>
<p>Research shows that many DAOs are highly centralized, with low participation rates and vulnerability to governance attacks. The incentive structures in Proof of Stake (PoS) and Proof of Work (PoW) systems can lead to centralization.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/e/1edd6c487afe9524d47e660bb04cf7872abb6008.jpeg" title="img"><img alt="img" height="274" src="https://ethresear.ch/uploads/default/optimized/3X/1/e/1edd6c487afe9524d47e660bb04cf7872abb6008_2_690x274.jpeg" width="690" /></a></div><br />
<div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/e/4e7ea5b4ee785ee33c7f3664e698a915d71867f8.jpeg" title="img2"><img alt="img2" height="374" src="https://ethresear.ch/uploads/default/optimized/3X/4/e/4e7ea5b4ee785ee33c7f3664e698a915d71867f8_2_690x374.jpeg" width="690" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#p-49639-cyber-physical-social-systems-6" name="p-49639-cyber-physical-social-systems-6"></a>Cyber-Physical-Social-Systems</h3>
<p>There’s a growing need for DAOs to bridge traditional management with AI agents and automated infrastructure, as highlighted by research in Cyber-Physical-Social Systems (CPSS).</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49639-approach-7" name="p-49639-approach-7"></a>Approach</h2>
<p>The protocol aims to incentivize participation without enabling influence compounding. It builds on a real-world game where participants propose and vote on ideas (like music tracks) without revealing identities until the round ends.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49639-key-requirements-for-the-protocol-8" name="p-49639-key-requirements-for-the-protocol-8"></a>Key requirements for the protocol:</h3>
<ul>
<li><strong>Mission aligned:</strong> Participant activity directly impacts organizational goals.</li>
<li><strong>Highly performant</strong>: Organizations using the protocol should outperform traditional structures.</li>
<li><strong>Centralization resilient</strong>: Financial contributions shouldn’t lead to disproportionate influence.</li>
<li><strong>Multidimensional</strong>: Support diverse participant interests.</li>
<li><strong>Rational</strong>: Function even when agents act in their self-interest.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#p-49639-key-features-9" name="p-49639-key-features-9"></a>Key features:</h3>
<ul>
<li><strong>Competence-based participation</strong>: Participants earn governance rights through demonstrated competence, not just financial contributions.</li>
<li><strong>Sybil attack resistance</strong>: A tournament ladder structure imposes costs and time requirements, making manipulation difficult.</li>
<li><strong>Progressive decentralization</strong>: Organizations can evolve by adding governance layers, increasing overall governance surface area.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#p-49639-current-state-10" name="p-49639-current-state-10"></a>Current State</h2>
<ul>
<li><strong>Research paper in progress</strong>: Seeking feedback and potential co-authors.</li>
<li><strong>Basic prototype and testing</strong>: Exploring use cases beyond music, such as manage-less code writing.</li>
<li><strong>Website with Telegram group</strong>: <a href="https://rankify.it" rel="noopener nofollow ugc">https://rankify.it</a></li>
</ul>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/autonomous-competence-identification-protocol/20281">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 15 Aug 2024 12:41:30 +0000</pubDate>
</item>
<item>
<title>A Threshold Network for “Human Keys” to solve privacy and custody issues</title>
<link>https://ethresear.ch/t/a-threshold-network-for-human-keys-to-solve-privacy-and-custody-issues/20276</link>
<guid>https://ethresear.ch/t/a-threshold-network-for-human-keys-to-solve-privacy-and-custody-issues/20276</guid>
<content:encoded><![CDATA[
<div> 关键词：Mishti网络、阈值网络设计、零知识证明、身份验证、合规性

总结:

本文探讨了在区块链和公钥基础设施（PKI）中，为何不直接将个人映射到密钥，而是通过一种称为Mishti网络的创新阈值网络设计来实现这一目标。Mishti网络的核心理念在于，它允许将个人的知识和属性与高熵伪随机数建立一种碰撞抵抗的映射关系，从而生成密钥。这种设计解决了ZK身份验证、合规性和用户注册过程中的一些关键问题。

首先，Mishti网络通过基于阈值验证的模糊随机函数（tVOPRF）在私有数据上构建了一种解决方案，使得密钥能够从个人的身份信息中生成。这不仅包括生物识别信息，也包括密码、安全问题等人类可记忆的数据，从而避免了仅使用随机数生成密钥可能带来的安全风险。

其次，Mishti网络为解决以太坊面临的用户注册复杂性和隐私保护问题提供了新思路。它通过提供自托管密钥管理方式，确保用户对密钥的唯一控制权，同时支持密钥恢复机制，增强了用户体验。此外，Mishti网络还展示了如何利用其基础加密技术解决ZK身份应用中需要从用户身份生成不可追踪nullifiers的问题，从而保护用户的隐私。

最后，Mishti网络还提出了将同态加密和零知识证明结合，以满足合规需求的方法。通过构造基于阈值椭圆曲线乘法的同态加密方案，Mishti网络允许在ZK证明中包含加密数据，并实现灵活的数据访问控制，这为解决ZK身份验证中的合规性挑战提供了新的可能性。

综上所述，Mishti网络通过创新的阈值网络设计和零知识证明技术，不仅解决了区块链和PKI领域中的身份验证和隐私保护问题，还为合规性管理提供了新的解决方案，展示了其在数字身份管理和合规性控制方面的潜力。 <div>
<h1><a class="anchor" href="https://ethresear.ch#p-49627-introduction-1" name="p-49627-introduction-1"></a>Introduction</h1>
<p>In blockchain and PKI more generally, people are represented by keys. A somewhat strange question to ask might be “why don’t keys represent people?” I will argue this is actually an important question and the crux of major privacy and onboarding challenges. We present a a threshold network design dubbed Mishti Network to derive keys from people rather than arbitrary randomness. This network solves a number of problems in ZK identity, compliance, and onboarding.</p>
<p>What does it mean for a key to be a representation of a person? There are two conditions that should be met:</p>
<ul>
<li>A person’s knowledge and/or attributes can always map to the private key</li>
<li>This person is the sole controller of the key</li>
</ul>
<p>In other words, it is a collision-resistant map of personal data and attributes to a high-entropy pseudorandom number. Without collision resistance, multiple people could have the same key. Without high entropy, the key is not secure. Keys can be both standard private keys or also a nullifier that’s useful for secure ZK credentials.</p>
<p>Human keys are not solely biometrics. They could be from human-friendly data such as security questions, passwords, or any unique knowledge belonging to an individual rather than arbitrary randomness.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49627-solution-oblivious-pseudorandom-function-2" name="p-49627-solution-oblivious-pseudorandom-function-2"></a>Solution: Oblivious Pseudorandom Function</h1>
<p>This solution is based on a threshold verifiable oblivious pseudorandom function (tVOPRF) on private data. An oblivious pseudorandom function (OPRF) takes a private input and computes a pseudorandom function (PRF). PRFs take low-entropy input and create high-entropy output. Adding verifiability via a ZKP makes it into a VOPRF. Verifying individual node contributions is important to decentralizing the network.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49627-why-it-is-helpful-to-ethereum-pki-3" name="p-49627-why-it-is-helpful-to-ethereum-pki-3"></a>Why it is helpful to Ethereum + PKI</h1>
<p>Some of the outstanding issues in Ethereum are onboarding and privacy. Onboarding requires not just simplicity but also self-custody, and recovery. Current onboarding solutions such as social logins and passkeys do not have self-custody (as they can be recovered by web2 accounts), while self-custodial solutions can’t have recovery without extra onboarding step like electing gaurdians.</p>
<p>A similar need is for ZK identity applications that need to derive nullifiers from their users’ identities, in a way nobody can trace back to the user. This is a common need in proof-of-personhood solutions to ensure that each person only has one corresponding nullifier without a central database or key that links users to their nullifiers.</p>
<p>Furthermore, the underlying cryptography and network can be repurposed to tackle another pressing challenge: that of satisfying compliance rules with ZK identity. The same underlying elliptic curve multiplication primitive that underlies this design can be used to construct threshold ElGamal decryption over ZK-friendly curves, which can allow ZK proofs to contain encrypted data with flexible access control.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49627-oblivious-pseudorandom-function-4" name="p-49627-oblivious-pseudorandom-function-4"></a>Oblivious Pseudorandom Function</h1>
<p>To generate keys from identities, an oblivious pseudorandom function (OPRF) can be constructed with distributed EC scalar multiplication. This allows private user data such as security questions, biometrics, passwords, or social security numbers, etc. to deterministically generate secret keys. The resulting pseudorandom value is computationally impractical to reverse despite it being from low-entropy input. One can thereby create wallet or nullifier from any (or a combination) of these low-entropy “human” factors. In the 2HashDH OPRF [1], a server or network’s secret is used to give randomness to the client’s input. The oblivious property prevents any server or set of nodes from seeing see this input.</p>
<p>2HashDH is the following algorithm between a user with a private input <span class="math">x</span> and a server (or network) with a private key <span class="math">s</span>. For a subgroup <span class="math">G</span> of an elliptic curve there are two hash functions:</p>
<p><span class="math">hashToCurve: \{0,1\}^* \rightarrow G</span><br />
<span class="math">hashToScalar: G \rightarrow F_q</span>.</p>
<p>The 2HashDH OPRF proceeds as follows</p>
<ol>
<li>User samples a random mask <span class="math">r</span> and sends <span class="math">M = r * hashToCurve(x)</span></li>
<li>Server multiplies by its secret, returning <span class="math">s * M</span></li>
<li>User computes the output by unmasking the server’s response and hashing it: <span class="math">o = HashToScalar(r^{-1} * s * M)</span></li>
</ol>
<p><span class="math">o</span> is uniformly pseudorandom in <span class="math">F_q</span>, and the server is information-theoretically blinded from the user’s input.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49627-decentralizing-the-server-5" name="p-49627-decentralizing-the-server-5"></a>Decentralizing the server</h2>
<p>To decentralize the OPRF server, only the step with a server must be decentralized:</p>
<blockquote>
<ol start="2">
<li>Server multiplies by its secret, returning <span class="math">s * M</span></li>
</ol>
</blockquote>
<p>For threshold elliptic curve multiplication, first a linear secret sharing, such as Shamir’s scheme, must be used. The secret key is generated through distributed key generation (DKG) such that each node with index <span class="math">i</span> receives share <span class="math">f(i)</span> for some secret polynomial <span class="math">f</span> known to nobody. There is no node at the <span class="math">0</span> index and <span class="math">f(0)</span> is the secret key of the network. The secret key <span class="math">f(0)</span> can be computed by a set <span class="math">Q</span> of <span class="math">t</span> nodes where <span class="math">t</span> is one more than the degree of <span class="math">f</span>.</p>
<p><span class="math">f(0) = \sum_{i \in Q}{L_{0, Q}(i)*f(i)}</span></p>
<p>where <span class="math">L_{0,Q}(i)</span> is the Lagrange basis for index <span class="math">i</span> in set <span class="math">Q</span> evaluated at zero.</p>
<p>Instead of reconstructing <span class="math">f(0)</span>, the nodes can collaborate to construct <span class="math">f(0) * M</span></p>
<p><span class="math">f(0) * M = \sum_{i \in Q}{L_{0, Q}(i)*f(i) * M}</span></p>
<p>This is sufficient for step</p>
<blockquote>
<ol start="2">
<li>Server multiplies by its secret, returning <span class="math">s * M</span></li>
</ol>
</blockquote>
<p>if the nodes are honest. But if one lies, the result will be wrong and there will be no way of knowing who lied. Thus, each node should prove their individual multiplication using a lightweight zero-knowledge DLEQ proof.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49627-other-interesting-use-case-provable-encryption-with-programmable-privacy-6" name="p-49627-other-interesting-use-case-provable-encryption-with-programmable-privacy-6"></a>Other interesting use case: Provable encryption with programmable privacy</h1>
<p>The same decentralized EC scalar primitive can be used not just for VOPRF but also for ElGamal decryption over ZK-friendly curves. This is helpful when identities must be revealed in certain conditions.</p>
<p>For example, many private DeFi protocols are interested in ensuring that bad actors do not get the benefits of anonymity, while the average user typically does. Governments are not satisfied with solely ZK because they need access to user data, but currently the only alternative is honeypots where all user data is stored to be turned over to authorities if needed.</p>
<p>Another use of revealing provably encrypted identities under certain conditions is undercollateralized lending – what if you want an identity or private key to be revealed if a DeFi loan is defaulted on? In this case, you need to prove the proper data is encrypted correctly, then have a smart contract control decryption rights.</p>
<p>To modify this threshold EC point multiplication to such use cases, little is needed.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49627-encryption-7" name="p-49627-encryption-7"></a>Encryption</h3>
<p>ElGamal encryption is client-side:</p>
<ol>
<li>Create an ephemeral keypair <span class="math">(a, A = aG)</span></li>
<li>Encode the message as an EC point <span class="math">P</span></li>
<li>Compute Diffie-Hellman shared secret with network public key: <span class="math">aB</span></li>
<li>Compute the ciphertext <span class="math">(A, aB+P)</span></li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-49627-decryption-8" name="p-49627-decryption-8"></a>Decryption</h3>
<p>Unlike encryption, decryption requires a server or decentralized network.</p>
<ol>
<li>Server/network multiply ephemeral public key <span class="math">A</span> by its secret key <span class="math">b</span> to get <span class="math">bA</span> = <span class="math">aB</span></li>
<li>Decryptor subtracts this value from <span class="math">aB+P</span> to get <span class="math">P</span></li>
</ol>
<p>The server/network’s step can be handled by the same threshold multiplication protocol as before!</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49627-network-setup-and-collusion-protection-9" name="p-49627-network-setup-and-collusion-protection-9"></a>Network Setup and Collusion Protection</h1>
<p>The team at Holonym has implemented this as as an AVS on Eigenlayer called Mishti Network. High reputation is common among Eigenlayer operators despite the permissionless nature, so it is ideal for threshold networks where collusion is a concern. To further mitigate collusion risk, there is the idea of parallel networks:</p>
<p>The asynchronous and homomorphic nature of the computations means users can permissionlessly add nodes outside of Mishti Network that they trust to not collude with Mishti Network. E.g. instead of splitting a secret between Mishti Network, half of the secret is between the Mishti Network and the other half in a semi-trusted node elected by the user. Since the whole network just does an EC multiplication, exactly what its individual does do, nodes and networks can be treated the same. A 2/2 scheme could be done between a semi-trusted node and Mishti network, simply by</p>
<ul>
<li>Adding their public keys to get the joint public key</li>
<li>Adding their responses to get a joint response to the computation</li>
</ul>
<p>Note this requires no consent from the network and is not limited to 2/2 schemes; it can be done with any combination of semi-trusted nodes and/or independent networks via threshold schemes.</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49627-references-10" name="p-49627-references-10"></a>References</h1>
<p>[1] S. Jarecki, A. Kiayias, and H. Krawczyk, “Round-optimal<br />
password-protected secret sharing and T-PAKE in the password only model,” in International Conference on the Theory and Application of Cryptology and Information Security. Springer, 2014 pp. 233–253</p>
<h1><a class="anchor" href="https://ethresear.ch#p-49627-concluding-notes-11" name="p-49627-concluding-notes-11"></a>Concluding Notes</h1>
<p>If you have any ideas on how to improve or elaborate on this network design for either ZK identity, self-custody, or any other relevant use cases, please reply or reach out.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/a-threshold-network-for-human-keys-to-solve-privacy-and-custody-issues/20276">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 14 Aug 2024 23:38:38 +0000</pubDate>
</item>
<item>
<title>On Attestations, Block Propagation, and Timing Games</title>
<link>https://ethresear.ch/t/on-attestations-block-propagation-and-timing-games/20272</link>
<guid>https://ethresear.ch/t/on-attestations-block-propagation-and-timing-games/20272</guid>
<content:encoded><![CDATA[
<div> 关键词：Attestations, Block Propagation, Timing Games, Node Operators, Ethereum Consensus

总结：

文章主要探讨了在以太坊网络中验证者的行为变化、区块传播机制及其对共识的影响。文章通过具体案例研究了节点运营商Lido、Coinbase和Kiln在区块提议时间上的策略与行为，并分析了这些策略如何影响网络共识。

1. **区块构建市场的演变**：文章指出，目前大部分区块构建工作外包给特定的区块构建者，其中两个主要构建者负责生成大约三分之二的区块。Kiln等实体则通过延迟区块提议时间来最大化其策略空间，将其提议时间精确到每槽的3-3.5秒。

2. **区块传播与控制权转移**：虽然提案者仍然负责从转发器接收并传播区块，但实际传播速度往往由转发器决定，因为它们通常拥有更好的网络连接。然而，提案者仍能通过延迟操作来参与时间游戏，影响区块的传播速度。

3. **验证行为的分析**：文章详细分析了不同节点运营商在面对来自不同提案者的区块时的验证行为。例如，Kiln验证者显示出一种独特的“U”型分布，这可能是由于不同的地理位置或客户端软件导致。此外，文章还讨论了验证者如何处理自己的区块，即“本地区块”，以避免区块重组。

4. **协调行为的讨论**：文章探讨了节点运营商之间可能存在的协调行为，以及这种协调行为如何影响网络共识。例如，Kiln验证者被发现试图通过投票支持自己的区块来避免区块重组，这在以太坊社区中被视为不适当的行为。

5. **解决方案与未来方向**：文章提出了通过增加对关联验证者惩罚的措施来对抗复杂协调行为的可能性，以维护网络的公平性和安全性。同时，文章鼓励研究者和开发者继续探索如何平衡提高效率与保护网络免受恶意协调行为的影响。 <div>
<h1><a class="anchor" href="https://ethresear.ch#p-49615-on-attestations-block-propagation-and-timing-games-1" name="p-49615-on-attestations-block-propagation-and-timing-games-1"></a>On Attestations, Block Propagation, and Timing Games</h1>
<p>By now, <a href="https://timing.pics/">proposer timing games</a> are no longer a new phenomenon and have been analyzed, <a href="https://eprint.iacr.org/2023/760">here</a>, <a href="https://arxiv.org/abs/2305.09032">here</a> and <a href="https://ethresear.ch/t/deep-diving-attestations-a-quantitative-analysis/20020">here</a>.</p>
<p>In the following research piece, I want to show the <strong>evolution of <a href="https://timing.pics/">proposer timing games</a></strong> and analyze their impact on attesters. Through a case study of the node operators of Lido, Coinbase, and Kiln, we dive deep into block proposal timing and its impact on Ethereum’s consensus.</p>
<p><img alt="kilnmeme" height="413" src="https://ethresear.ch/uploads/default/original/3X/1/5/152baa9c8da23d4524a4e75101c4a1c0967ebf83.png" width="456" /></p>
<p>As of August 2024, the <strong>block building market is largely outsourced</strong>, with <a href="https://mevboost.pics/">~90%</a> handled by <a href="https://github.com/flashbots/mev-boost">mevboost</a> block builders. In practice, two builders, <a href="https://www.titanbuilder.xyz/">Titan Builder</a> and <a href="https://beaverbuild.org/">Beaverbuild</a>, produce approximately <a href="https://mevboost.pics/">80%</a> of all blocks that make it on-chain.</p>
<p><strong>Kiln is among the entities pushing timing games the furthest</strong>, delaying block proposals to the <strong>3-3.5 second</strong> mark within the slot.</p>
<blockquote>
<p>In today’s environment with mevboost, <strong>block propagation is primarily handled by relays.</strong> Although proposers still propagate the block after receiving it from the relay, relays typically have better network connectivity and can therefore do it faster. <strong>However, the timing remains under the control of proposers</strong>, who can delay their <code>getHeader</code> calls to engage in timing games.</p>
</blockquote>
<p>This chart shows the <strong>evolution of timing games</strong>. We can see that blocks from Kiln validators appear later and later over time.</p>
<p><img alt="proposer_timing_games" class="animated" height="383" src="https://ethresear.ch/uploads/default/original/3X/8/2/82cad8533f90505055f8eced73ae89d774a96111.gif" width="690" /></p>
<p><strong>This comes with an impact on the network: for blocks proposed by Kiln proposers, the missed/wrong head vote rate is significantly higher:</strong><br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/d/8d3a31d4dd9d8856d2baaf1b7ad1528312b72923.png" title="missed_head_votes_over_proposers"><img alt="missed_head_votes_over_proposers" height="316" src="https://ethresear.ch/uploads/default/optimized/3X/8/d/8d3a31d4dd9d8856d2baaf1b7ad1528312b72923_2_690x316.png" width="690" /></a></div><p></p>
<p><a href="https://ethresear.ch/t/deep-diving-attestations-a-quantitative-analysis/20020">Previous analysis</a> showed that <strong>the longer one waits, the higher the expected number of missed head votes</strong> (<em>“80% of attestations seen by the second 5 in the slot”</em>). Kiln proposes blocks very late, causing some attesters to miss them and instead vote for the parent block. <strong>Given that there are approximately 32,000 validators assigned to each slot, this results in about 10% of them voting for the wrong block.</strong></p>
<p>Let’s examine the attesting behavior of three large node operators and compare how they respond to <strong>blocks proposed at different times within a slot.</strong> The chart below illustrates the distribution of correct and timely head votes across the seconds within a slot.<br />
<img alt="attestations_seen_late" class="animated" height="383" src="https://ethresear.ch/uploads/default/original/3X/5/e/5eb241fefdf5cecb08a41d95fbf6d0263dbb573d.gif" width="690" /><br />
For early blocks, we observe that both <strong>Lido and Coinbase display a characteristic “U”-shape</strong> in their voting patterns that might be caused by different geo locations or client software. In contrast, <strong>Kiln shows a single prominent peak</strong> that slightly lags behind the first peaks of Coinbase and Lido. <strong>However, for late blocks, Kiln attesters also show the “U”-shape pattern.</strong></p>
<p><strong>When blocks are first seen at the 4-second mark in the p2p network during a slot, most Lido attesters attest up to 2 seconds earlier than most of the Kiln or Coinbase attesters.</strong> This pattern doesn’t necessarily suggest that Kiln is executing “individual strategies.” Instead, it could be attributed to running different clients or using different geographical locations.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49615-but-who-affects-whom-2" name="p-49615-but-who-affects-whom-2"></a><em><strong>But who affects whom?</strong></em></h3>
<p>In the following chart, we compare a node operator’s performance over different proposers. A bar above y=1, for example, the green bar at Lido, indicates that Lido attesters miss more head votes for blocks from Kiln proposers. At the same time, Lido attesters do better for Lido blocks. The dashed line at 1 indicates the average share in missed head votes over all entities as proposers. A bar below 1 means the specific entity misses fewer head votes in conjunction with the respective proposer compared to the average.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/8/786e634d534692c6bcef1859d4baf99b6490a363.png" title="missed_head_votes_over_proposers_percentage"><img alt="missed_head_votes_over_proposers_percentage" height="316" src="https://ethresear.ch/uploads/default/optimized/3X/7/8/786e634d534692c6bcef1859d4baf99b6490a363_2_690x316.png" width="690" /></a></div><p></p>
<blockquote>
<p>Importantly, it is expected that each node operator does best with its local blocks. This is expected even without a coordination oracle, simply by co-locating nodes.</p>
</blockquote>
<p>To quickly summarize what we see:</p>
<ul>
<li>Most node operators are rather stable across different proposers.</li>
<li><strong>Figment performs significantly worse for Kiln proposers.</strong> The same applies to Lido, Kraken, and EtherFi attesters.</li>
<li><strong>Kiln and Binance are the only entities performing better for Kiln blocks</strong> (which are, as a reminder, very late).</li>
</ul>
<p><strong>Kiln attesters generally do well.</strong> <a href="https://ethresear.ch/t/deep-diving-attestations-a-quantitative-analysis/20020">Earlier analysis</a> showes that Kiln does a more than good job when it comes to running high-performing validators. Refer to <a href="https://ethresear.ch/t/deep-diving-attestations-a-quantitative-analysis/20020">this analysis</a> for further details of Kiln’s attestation performance.</p>
<p><strong>Kiln causes stress.</strong> Now, we know that Kiln blocks cause stress to other attesters but not necessarily to Kiln’s attesters.</p>
<p><strong>Explaining how.</strong> The “<em>how</em>?” is difficult to respond to at this point. A possible explanation might be that Kiln’s validators are heavily co-located, with many validators running on the same machine, or have very dense peering. Another reason might be coordinated behavior across multiple nodes, either through custom peering/private mesh networks or through another additional communication layer connecting their validators. The latter is regarded as more centralizing as it leverages economies of scale even more.</p>
<p>A similar pattern can be observed when examining the (correct &amp; timely) attestation timing of Lido and Coinbase for the blocks proposed by each respective entity (26.07.2024-03.08.2024).<br />
<img alt="attestations_seen_late_by_proposer_misses" class="animated" height="383" src="https://ethresear.ch/uploads/default/original/3X/5/a/5acb3eda53b7f342972637ae3d881d9e7cb44983.gif" width="690" /></p>
<p>Interestingly, Kiln develops a “U”-shape distribution ranging from <span class="math">3.8 \Rightarrow
 6.1</span> for their own late blocks, Lido a peak at 4.2s, and Coinbase a plateau starting at second 4 with a small peak at second 6 in the slot.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49615-prevent-reorgs-of-my-own-proposed-blocks-3" name="p-49615-prevent-reorgs-of-my-own-proposed-blocks-3"></a>“Prevent reorgs of my own proposed blocks”</h2>
<p>Let’s shift our focus to reorged blocks. One strategy from the perspective of a node operator might be to <strong>never</strong> vote for reorging out one’s own block. Simply speaking, “<em>never vote for the parent block as the head if the proposer is me</em>”.</p>
<p>Instead of calling it <em>an entity’s own block</em>, I will use <em>local block</em> in the following.</p>
<p>The following chart shows the percentage of attesters voting for the reorged block vs voting for the parent block. The red part displays the % of all attesters from that entity that voted for a reorged block built by that entity.<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/5/f580dddb61ad6a3e4f577516f312475182d980d7.png" title="votes_for_local_reorged_block"><img alt="votes_for_local_reorged_block" height="316" src="https://ethresear.ch/uploads/default/optimized/3X/f/5/f580dddb61ad6a3e4f577516f312475182d980d7_2_690x316.png" width="690" /></a></div><p></p>
<p>Kiln shows outlier behavior. While most node operators’ attesters correctly vote for the parent block rather than the local block, Kiln’s attesters appear to disregard this norm. <strong>Over 10% of Kiln attesters attempt to keep the local block on-chain by voting for it.</strong> If such strategies are adopted, they might justify the losses from incorrect head votes if they prevent the local block from being reorged. However, these tactics are generally frowned upon within the Ethereum community: “<em>don’t play with consensus</em>”.</p>
<blockquote>
<p>The chart uses 365 days of data. Thus, if some sophisticated strategy was implemented during the last year, the red portion would be proportionately smaller.</p>
</blockquote>
<h2><a class="anchor" href="https://ethresear.ch#p-49615-but-how-do-we-feel-about-any-additional-level-of-coordination-4" name="p-49615-but-how-do-we-feel-about-any-additional-level-of-coordination-4"></a>But how do we feel about any additional level of coordination?</h2>
<p>Regarding coordinated attesting, we, as community, seem to accept that validators run on the same node vote for the same checkpoints.</p>
<p>We probably don’t want any additional efforts that cross the boundaries of physical machines to improve coordination across validators. It’s something that everyone can build that goes beyond <a href="https://github.com/ethereum/consensus-specs/blob/b2f2102dad0cd8b28a657244e645e0df1c0d246a/specs/phase0/validator.md#attesting">what the specs describe</a>. Such coordination could have different forms:</p>
<ul>
<li><strong>Level 1 - Fall-backs &amp; Static Peering</strong>: Have a central fall-back/back-up node for multiple physical machines. This can also be a circuit breaker, some particularly fault-tolerant machine acting as a private relayer for information. Setups with improved peering, private mesh networks, or similar might also fall into this category.</li>
<li><strong>Level 2 - If-else rules</strong>: Have hard-coded rules waiting longer in certain slots. Those would be installed on multiple physical machines, allowing them to “coordinate” based on predefined rules.</li>
<li><strong>Level 3 - Botnet</strong>: Have a centralized oracle that communicates with all validators and delivers the checkpoints to vote for and the timestamp when they should be published.</li>
</ul>
<p>In my opinion, crossing the line into the latter form of coordination (<em>level 2 and 3</em>) is problematic, and node operators should be held accountable. Finally, there may be a <strong>gray area</strong> for strategies involving <strong>static peering</strong> and <strong>private mesh networks</strong>.</p>
<p><strong>Such setups could be used to run (malicious) strategies such as:</strong></p>
<ul>
<li>ensuring to never vote for different checkpoints across multiple physical machines.</li>
<li>ensuring to never vote for reorging out a block from one’s own proposer.</li>
<li>coordinating based on the consecutive proposer (<a href="https://github.com/ethereum/consensus-specs/pull/3034">honest reorg client</a> (y/n)).</li>
<li>censoring attestations of a certain party.</li>
<li>not voting for the blocks of a certain party.</li>
<li>etc.</li>
</ul>
<p><strong>When discussing <em>coordination</em>, it’s important to distinguish between two types:</strong></p>
<ol>
<li>Coordinated behavior that occurs when validators are <strong>run from the same physical machine</strong>.</li>
<li>Coordinated behavior that arises from running the same <strong>modified client software</strong> or relying on the same <strong>centralized oracle</strong>.</li>
</ol>
<p>A potential solution to counter sophisticated coordinated validator behavior is <a href="https://ethereum-magicians.org/t/eip-7716-anti-correlation-attestation-penalties/20137">EIP-7716: Anti-Correlation Penalties"</a>, which proposes to scale penalties with the correlation among validators.</p>
<p><em><strong>Find the code for this analysis <a href="https://github.com/nerolation/timing-games-and-economies-of-scale">here</a>.</strong></em></p>
<h1><a class="anchor" href="https://ethresear.ch#p-49615-more-on-that-topics-5" name="p-49615-more-on-that-topics-5"></a>More on that topics</h1>
<div class="md-table">
<table>
<thead>
<tr>
<th>Title</th>
<th>Author</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://timing.pics">Timing.pics</a></td>
<td>DotPics Website</td>
</tr>
<tr>
<td><a href="https://ethresear.ch/t/timing-games-implications-and-possible-mitigations/17612">Timing Games: Implications and Possible Mitigations</a></td>
<td>Caspar &amp; Mike</td>
</tr>
<tr>
<td><a href="https://ethresear.ch/t/deep-diving-attestations-a-quantitative-analysis/20020">Deep Diving Attestations - A quantitative analysis</a></td>
<td>Toni</td>
</tr>
<tr>
<td><a href="https://www.paradigm.xyz/2023/04/mev-boost-ethereum-consensus">Time, slots, and the ordering of events in Ethereum Proof-of-Stake</a></td>
<td>Georgios &amp; Mike</td>
</tr>
<tr>
<td><a href="https://arxiv.org/abs/2305.09032">Time is Money: Strategic Timing Games in Proof-of-Stake Protocols</a></td>
<td>Caspar et al.</td>
</tr>
<tr>
<td><a href="https://eprint.iacr.org/2023/760">Time to Bribe: Measuring Block Construction Market</a></td>
<td>Toni et al.</td>
</tr>
<tr>
<td><a href="https://ethresear.ch/t/the-cost-of-artificial-latency-in-the-pbs-context/17847">The cost of artificial latency in the PBS context</a></td>
<td>Chorus One</td>
</tr>
<tr>
<td><a href="https://ethresear.ch/t/empirical-analysis-of-the-impact-of-block-delays-on-the-consensus-layer/17888">Empirical analysis of the impact of block delays on the consensus layer</a></td>
<td>Kiln</td>
</tr>
<tr>
<td><a href="https://youtu.be/J_N13erDWKw?t=1061">P2P Presentation on Timing Games (Youtube)</a></td>
<td>P2P_org</td>
</tr>
<tr>
<td><a href="https://www.youtube.com/watch?v=gsFU-inKRQ8">Time is Money (Youtube)</a></td>
<td>Caspar</td>
</tr>
</tbody>
</table>
</div>
            <p><small>6 posts - 3 participants</small></p>
            <p><a href="https://ethresear.ch/t/on-attestations-block-propagation-and-timing-games/20272">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 14 Aug 2024 13:04:48 +0000</pubDate>
</item>
<item>
<title>A Node-Based Solution to Execution Sharding: The KRNL Protocol</title>
<link>https://ethresear.ch/t/a-node-based-solution-to-execution-sharding-the-krnl-protocol/20268</link>
<guid>https://ethresear.ch/t/a-node-based-solution-to-execution-sharding-the-krnl-protocol/20268</guid>
<content:encoded><![CDATA[
<div> 关键词：Web3、KRNL协议、执行碎片化、证明来源（PoP）、去中心化注册表

总结：

本文探讨了Web3生态系统所面临的挑战，包括网络碎片化、可扩展性限制、跨链集成复杂性和安全漏洞。为解决这些问题，文章提出了一种名为KRNL协议的解决方案，它是一个集成引擎，能够无缝地在以太坊交易生命周期中整合无许可和可组合功能，跨越多个区块链网络。KRNL通过将功能转换为称为“内核”的执行碎片来优化资源使用、增强模块化并加速部署，从而改进去中心化应用程序的响应速度并缩短其上市时间。

文章进一步介绍了执行碎片的概念，即通过将智能合约的执行分散到多个区块链网络中，以提高区块链系统的可扩展性和效率。KRNL协议作为这一概念的一部分，旨在成为Web3框架中的关键工具，通过引入社区构建、无许可、可货币化和可组合的执行碎片，实现跨链应用的开发。

此外，文章提出了“内核”作为执行碎片的代表，它们具有状态无状态、轻量级设计、韧性和独立部署能力等特征，旨在提供一个去中心化的云环境，允许用户在分布式环境中运行应用程序。KRNL协议还包括证明来源（PoP）机制，确保内核成功执行后才能执行交易，以及去中心化注册表，用于激活和货币化由社区构建的内核。

最后，文章概述了DeFaaS系统，这是一个利用区块链技术和去中心化API管理的新型去中心化FaaS系统，旨在提供比传统FaaS解决方案更好的可扩展性、灵活性、安全性和可靠性。同时，文章还提出了一个基于以太坊区块链特性的模型，用于执行不同服务并根据服务质量差异性地交付可靠性，以应对多服务网络中的成本降低需求。文章还讨论了去中心化数字身份的实现，提出了一个统一的钱包解决方案，以管理区块链和自主权两种类型的去中心化身份，并通过实际案例进行了验证。 <div>
<p>By <a href="https://x.com/asim_eth" rel="noopener nofollow ugc">Asim Ahmad</a> and <a href="https://x.com/Tahir_Mahmood" rel="noopener nofollow ugc">Tahir Mahmood</a> on behalf of <a href="https://krnl.xyz" rel="noopener nofollow ugc">KRNL</a>.</p>
<p><strong>1. Abstract</strong></p>
<p>The evolution of the Web3 ecosystem confronts pivotal challenges such as network fragmentation, scalability constraints, cross-chain integration complexities, and security vulnerabilities. To address these issues, we introduce the KRNL Protocol—an orchestration and verification engine that seamlessly integrates permissionless and composable functions across multiple blockchain networks within the Ethereum transaction lifecycle. By transforming both on-chain and off-chain functions into execution shards called “kernels,” KRNL offers a distributed runtime environment that optimizes resource utilization, enhances modularity, and accelerates deployment. This approach not only improves the responsiveness of decentralized applications (dApps) but also reduces their time-to-market. Our proposal positions KRNL as part of the fabric of the Web3 framework.</p>
<p><strong>2. Motivation</strong></p>
<p>The Web3 ecosystem faces several significant challenges, including fragmentation, scalability limitations, cross-chain friction, and security concerns.</p>
<p><strong>Fragmentation</strong>: The emergence of numerous Layer 1 and Layer 2 solutions has led to the creation of isolated silos. This fragmentation impedes seamless interaction between applications and smart contracts across different environments, undermining the foundational principle of composability in decentralized systems.</p>
<p><strong>Scalability Constraints</strong>: Ethereum grapples with network congestion and high gas fees. These scalability issues deter the widespread adoption of dApps and erode user experience.</p>
<p><strong>Cross-Chain Friction</strong>: Facilitating interoperability between Ethereum and other blockchains often demands intricate integrations. The absence of standardized cross-chain communication protocols exacerbates development complexities, stifling innovation and efficiency.</p>
<p><strong>Security Vulnerabilities</strong>: Ensuring transaction integrity, provenance, and security in a decentralized manner remains a challenge. The proliferation of bridges and interoperability solutions introduces novel attack vectors, heightening security risks.</p>
<p>To address these challenges, we reimagine the execution paradigm by introducing the concept of kernels - community-built, permissionless, monetizable, and composable execution shards across Web3. We also introduce the KRNL protocol, an orchestration and verification engine that enables smart contracts to integrate execution shards, enriching the logic and state of traditional smart contract operations without the creation of custom infrastructure. With this proposal, we aim to become an essential tool for the development of cross-chain applications.</p>
<p><strong>3. TL;DR</strong></p>
<p>Execution Sharding refers to the approach of dividing and distributing the execution of smart contracts across multiple blockchain networks, or “shards”, to enhance scalability and efficiency in blockchain systems. Instead of executing every transaction on a single chain, execution sharding allows transactions and smart contract states to be distributed across multiple chains, each handling a portion of the overall workload.</p>
<p>Execution sharding is critical for Ethereum’s scalability. The KRNL Protocol integrates permissionless and composable kernels (execution shards) across multiple networks, seamlessly into the native Ethereum transaction lifecycle.</p>
<p>KRNL manages resources to provide a secure and optimal execution environment for smart contracts. This enables a distributed runtime environment that determines transaction outcome based on selected kernels, operating across different environments. KRNL’s open framework enhances modularity, optimizes resources, ensures stable operations, and accelerates deployment, ultimately improving responsiveness and reducing time to market for applications.</p>
<p><strong>4. Introducing Kernels</strong></p>
<p>Within the KRNL Protocol framework, kernels represent execution shards. These kernels transform both on-chain and off-chain functions into modular units characterized by the following attributes:</p>
<ul>
<li><strong>Statelessness</strong>: Kernels maintain no intrinsic state, ensuring flexibility and facilitating seamless migration across environments.</li>
<li><strong>Lightweight Design</strong>: To minimize computational overhead, kernels promote efficient execution.</li>
<li><strong>Resilience</strong>: Engineered to withstand operational failures, ensuring reliable performance.</li>
<li><strong>Independent Deployability</strong>: Allowing for deployment across various environments.</li>
</ul>
<p>The defining features of kernels include:</p>
<ul>
<li><strong>Infrastructure Agnosticism</strong>: Kernels are not tethered to specific infrastructures; they possess the agility to migrate across environments as necessitated.</li>
<li><strong>Enhanced Modularity and Composability</strong>: By deconstructing applications into discrete kernels, modularity is enhanced, enabling permissionless sharing across multiple applications.</li>
<li><strong>Accelerated Deployment</strong>: Simplifying the deployment process improves responsiveness and reduces time-to-market for applications.</li>
</ul>
<p><strong>5. Vision</strong></p>
<p><strong>The Pre-Cloud Paradigm</strong></p>
<p>Before cloud computing, developers bore the burden of constructing, operating, and maintaining all requisite programs and services. This paradigm engendered prohibitive costs, scalability constraints, accessibility challenges, and resource limitations. Cloud computing revolutionized this landscape, introducing managed services where back-end infrastructures are handled by cloud providers.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/f/5fe5cbe3446592ada1eea874797faf006e20d182.png" title="Before and After Cloud Computing"><img alt="Before and After Cloud Computing" height="315" src="https://ethresear.ch/uploads/default/optimized/3X/5/f/5fe5cbe3446592ada1eea874797faf006e20d182_2_690x315.png" width="690" /></a></div><p></p>
<p><strong>KRNL’s Transformative Potential</strong></p>
<p>KRNL seeks to catalyze a comparable paradigm shift within the Web3 domain—a permissionless Web3 cloud environment built by the community through contributions of monetizable kernels. This vision aligns with the Function as a Service (FaaS) model, reimagined to suit the decentralized and heterogeneous fabric of blockchain ecosystems.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/0/3/03a9e2f49a0d71e30f39b7ce9368173d25a7b5a6.jpeg" title="Before and After KRNL"><img alt="Before and After KRNL" height="301" src="https://ethresear.ch/uploads/default/optimized/3X/0/3/03a9e2f49a0d71e30f39b7ce9368173d25a7b5a6_2_690x301.jpeg" width="690" /></a></div><p></p>
<p><strong>Functions as a Service (FaaS) in the Web3 Context</strong></p>
<p>FaaS is a category of cloud computing services that provide a platform enabling customers to develop, run and manage applications without the complexity of building and maintaining the infrastructure associated with developing and launching an app. Examples of a traditional FaaS include AWS Lambda, Google Cloud Functions, Microsoft Azure Functions, etc.</p>
<p>The conventional FaaS model does not fit well in distributed and heterogeneous blockchain environments, where each blockchain is a silo and not efficient in the context of the whole Web3 ecosystem. To adapt this concept to Web3, it is essential to ensure decentralized registry, management, and execution of kernels.</p>
<p><strong>6. Core Concepts</strong></p>
<p><strong>The Computing Engine</strong></p>
<p>KRNL enhances an Ethereum Remote Procedure Call (RPC) node with a verification and orchestration-enabled computing engine. This engine abstracts the intricacies associated with integrating smart contract interdependencies.</p>
<p>The computing engine creates an application and technology agnostic framework that offers a runtime environment to user applications in a distributed manner. It sits between a transaction initiated on any chain and its propagation into a block, determining a transaction’s outcome based on the kernels selected. This approach allows for flexible, efficient scaling and optimization of distributed applications.</p>
<p><strong>Proof of Provenance (PoP)</strong></p>
<p>PoP validates that prescribed kernels have run successfully before a transaction is executed, ensuring reliability and security of the KRNL Protocol.</p>
<p>The KRNL Protocol achieves this by utilizing various schemes including a decentralized token authority that issues a signature token, ERC-1271, cryptography and proof systems. The implementation requires the application developer to implement a Software Development Kit (SDK) as well as the token authority. PoP works with existing standards within the Ethereum ecosystem, combining multiple schemes to ensure an anti-fragile system.</p>
<p><strong>Decentralized Registry</strong></p>
<p>An Ethereum based registry for activating and monetizing community built kernels. This registry serves as the definitive repository, maintaining critical information about registered kernels, including their pathways, monetization schemes, and other customizable parameters. Core to the design of KRNL is the concept of a two-sided marketplace where kernels are built and monetized, while being utilized by applications across Web3.</p>
<p><strong>7. Architecture</strong></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/2/d2b12fc5edd69ae351e74dda8a31b3f6e57a5311.png" title="Architecture Overview of the KRNL Protocol"><img alt="Architecture Overview of the KRNL Protocol" height="388" src="https://ethresear.ch/uploads/default/optimized/3X/d/2/d2b12fc5edd69ae351e74dda8a31b3f6e57a5311_2_690x388.png" width="690" /></a></div><p></p>
<p><strong>Use Case Scenario</strong></p>
<p>In a hypothetical scenario, a DeFi protocol on Ethereum would like to allow users to trade RWA assets if they are an approved user on Company 1’s RWA platform (and if not, to reject the transaction from this wallet). Say Company 1 has built an RWA platform on Blockchain 2, with dynamic off-chain metadata corresponding to approved users. Additionally, these users need to have an identity score of X as determined by a on-chain DID smart contract on Blockchain 3. In the past, implementing these solutions across various chains would have required multiple complex integrations and in many cases require direct communication with vendors. However, with KRNL, builders now only need to perform a single, one-time permissionless integration.</p>
<p>There is not currently any application layer that facilitates the conditional logic before state changes are executed, and this is generally built ground-up by builders. Ideally, this would be done in a plug-and-play, permissionless manner that would be reproducible by protocols that want to utilize the RWA platform and identifiers from the DID system.<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/6/869c6dc2fe0c134bfb17f32f1b481fadcd6e2704.png" title="Limitations of Existing Solutions"><img alt="Limitations of Existing Solutions" height="323" src="https://ethresear.ch/uploads/default/optimized/3X/8/6/869c6dc2fe0c134bfb17f32f1b481fadcd6e2704_2_690x323.png" width="690" /></a></div><p></p>
<p><strong>8. Decentralization and Security Considerations</strong></p>
<p><strong>Upholding Decentralization</strong></p>
<p>KRNL leverages the intrinsic decentralization of existing native blockchains. By integrating with a standard Ethereum RPC node, any Ethereum RPC node can function as a KRNL node without interfering with consensus mechanisms of the underlying network. Node operators are incentivized through the accrual of a proportion of fees generated from kernels, fostering a decentralized and participatory ecosystem.</p>
<p><strong>Mitigating Malicious Activities</strong></p>
<p>To preempt and mitigate potential malicious activities, such as replicating KRNL node code to fabricate counterfeit signatures, KRNL employs multiple cryptographic schemes that ensure security by design. The security architecture is flexible, customizable, and predominantly under the control of the dApp developer. This approach ensures that the KRNL Protocol remains permissionless, resilient, and secure.</p>
<p><strong>Explore more in our <a href="https://github.com/KRNL-Labs/krnl-node-sandbox-public" rel="noopener nofollow ugc">KRNL Developer Sandbox</a></strong></p>
<p><strong>Learn more about <a href="https://docs.krnl.xyz/" rel="noopener nofollow ugc">KRNL</a></strong></p>
<p><strong>Supporting Research Papers</strong></p>
<p><a href="https://arxiv.org/html/2404.08151v1" rel="noopener nofollow ugc">Decentralized FaaS over Multi-Clouds with Blockchain based Management for Supporting Emerging Applications</a></p>
<p>DeFaaS is a novel decentralized Function-as-a-Service (FaaS) system proposed to address the limitations of centralized FaaS solutions. This system leverages blockchain technology and decentralized API management to create a distributed FaaS platform that offers improved scalability, flexibility, security, and reliability. DeFaaS is designed to support various distributed computing scenarios beyond FaaS, including decentralized applications (dApps), volunteer computing, and multi-cloud service mesh. The proposed system aims to mitigate issues associated with centralized FaaS, such as vendor lock-in and single points of failure.</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0306457321000340?ref=pdf_download&amp;fr=RR-2&amp;rr=89e00464f80d773d" rel="noopener nofollow ugc">Multi-Service Model for Blockchain Networks</a></p>
<p>Multi-service networks aim to efficiently supply distinct goods within the same infrastructure by relying on a (typically centralized) authority to manage and coordinate their differential delivery at specific prices. In turn, final customers constantly seek to lower costs whilst maximizing quality and reliability. This paper proposes a decentralized business model for multi-service networks using Ethereum blockchain features – gas, transactions, and smart contracts – to execute multiple services at different prices. By employing Ether, to quantify the quality of service and reliability of distinct private Ethereum networks, their model concurrently processes streams of services at different gas prices while differentially delivering reliability and service quality.</p>
<p><a href="https://www.researchgate.net/publication/372662346_Orchestrating_Digital_Wallets_for_On-_and_Off-chain_Decentralized_Identity_Management" rel="noopener nofollow ugc">Qualified Digital Certificates within Blockchain Networks</a></p>
<p>This paper examines decentralized digital identities, which use asymmetric cryptography without centralized oversight, focusing on both on-chain (blockchain) and off-chain (self-sovereign) types. Currently, no single wallet manages both types of decentralized identities. To address this, the paper proposes an orchestration solution for a universal wallet that combines both types and validates it using a real-life use case.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/a-node-based-solution-to-execution-sharding-the-krnl-protocol/20268">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 14 Aug 2024 11:56:25 +0000</pubDate>
</item>
<item>
<title>DoS on block proposers in PoS and block builders in PBS</title>
<link>https://ethresear.ch/t/dos-on-block-proposers-in-pos-and-block-builders-in-pbs/20262</link>
<guid>https://ethresear.ch/t/dos-on-block-proposers-in-pos-and-block-builders-in-pbs/20262</guid>
<content:encoded><![CDATA[
<div> 关键词：Ethereum 2.0、PoS、DoS攻击、区块提议者、保护机制

总结:

在以太坊2.0的权益证明（PoS）系统中，区块提议者在创建下一个区块前约12秒就已知，这为攻击者提供了机会，可能通过拒绝服务（DoS）攻击来阻止提议者创建新块并失去奖励。类似地，这种担忧也出现在Algorand的基于VRF的领导者选举机制中，该机制通过避免此类攻击来解决这一问题。

在以太坊2.0的分片（PBS）系统中，构建者首先揭示密封块投标（承诺），然后稍后揭示块内容。如果未按时揭示内容，构建者将受到惩罚。这意味着攻击者可以通过DoS攻击受害者，导致其因未能及时揭示块而遭受严重处罚，并可能被重复利用。

为了保护免受这类攻击，以太坊2.0采取了多种措施。首先，通过引入信标链和分片技术，网络设计了复杂的共识算法，旨在确保安全性和去中心化。其次，针对区块提议者的DoS攻击，以太坊2.0采用了多重验证和备份机制，确保即使某个提议者遭到攻击或离线，系统也能正常运行并选择其他提议者继续进行区块生成。此外，通过实施惩罚机制，如罚款和出块延迟，以太坊激励节点保持在线和积极参与网络活动，从而减少DoS攻击的有效性。最后，以太坊的网络设计还包括了自我修复能力，能够在检测到攻击行为时自动调整和恢复系统的稳定性和安全性。这些综合措施共同构成了以太坊2.0对DoS攻击的防御策略，确保网络的健壮性和可靠性。 <div>
<ol>
<li>
<p>In Ethereum 2.0 PoS, the block proposer of the next block is known a certain time (~12s) before she creates the block. It might create an opportunity for attackers to DoS the next proposer who will therefore not create the new block and lose the reward. This might be systematically repeated again. We know that something similar was of concern for Algorand PoS and its VRF-based leader election that avoided this kind of attack.</p>
</li>
<li>
<p>In PBS, the builder reveals the sealed block bid (commitment), and then later reveals the block contents. If the contents are not revealed, the builder will be penalized. So, the attacker already knowing the network address of victim can DoS her and cause severe penalties for not revealing the block on time. This might be systematically repeated again.</p>
</li>
</ol>
<p>My question or point to discuss is how Ethereum protects against this kind of attack?</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/dos-on-block-proposers-in-pos-and-block-builders-in-pbs/20262">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 13 Aug 2024 10:28:43 +0000</pubDate>
</item>
<item>
<title>A trustless on-chain anti-MEV solution for Layer2/3</title>
<link>https://ethresear.ch/t/a-trustless-on-chain-anti-mev-solution-for-layer2-3/20260</link>
<guid>https://ethresear.ch/t/a-trustless-on-chain-anti-mev-solution-for-layer2-3/20260</guid>
<content:encoded><![CDATA[
<div> 关键词：MEVless协议、L2链、Tx-Order-commitment、DA-layers、隐私保护

总结:

本文提出了一种解决Layer2矿工提取价值（MEV）问题的方案，称为MEVless协议。该协议的主要创新在于：

1. **用户发送txHash而非完整交易内容**：用户仅需向L2链发送交易哈希值和预付费用以防止分布式拒绝服务（DOS）攻击。

2. **基于提示排序txHash**：链接收这些txHash并根据提供的提示金额进行排序，生成Tx-Order-commitment并广播给其他节点，同时允许用户订阅此承诺。

3. **确保交易执行顺序**：当用户看到订单承诺后，可发送交易内容至L2链。链会接受这些内容并根据先前的承诺顺序打包交易。如果交易内容与先前的哈希不符，则将其置于已承诺交易之后。

4. **去中心化和数据可用性层（DA-layers）**：通过去中心化链节点和使用DA-layers来确保交易安全，防止单个节点不接受交易的情况发生。

5. **隐私保护**：MEVless协议设计使得交易内容在打包前无法被观察，因此用户无需依赖隐私节点或MEVA来保护交易免受攻击，确保了交易的私密性和安全性。

该方案旨在提供一种信任无界的解决方案，消除对特定机构的信任需求，同时通过去中心化和透明的交易流程，增强交易的安全性和隐私保护。 <div>
<p>We have a solution to resolve the Layer2 MEV onchain trustlessly.<br />
Here is the arch:<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/e/aeec6e16bbf3b6864d66ae84ecf5b663f9c55ce5.jpeg" title="image"><img alt="image" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/a/e/aeec6e16bbf3b6864d66ae84ecf5b663f9c55ce5_2_336x500.jpeg" width="336" /></a></div><p></p>
<ol>
<li>Users only send their txHash to the L2 chain with some advance charge (to prevent DOS attack)</li>
<li>The chain accepts these txHashes, sort them based on the amount of tips, and then make a Tx-Order-commitment and broadcast it to the other chain nodes.  Also, user can subscribe this commitment.</li>
<li>When users see the order-commitments, they will send their tx-content to the L2 chain and the DA-layers.</li>
<li>Chain accepts the tx-content from users, and also fetch txs from DA-layers ,  pack them according to the previously promised order. If the tx-content does not match the previously tx-hash, chain will put them behind the txs which made order-commitment.<br />
<strong>All promised txs will be sorted before the unpromised txs.</strong><br />
NOTICE: In this way, the chain may deduct tx-content and pretend not to receive it.  To prevent this situation. We have to:<br />
i. Decentralise chain node.<br />
ii. Use DA to complete the txs if one node does not accept the txs.</li>
</ol>
<p>In this case, we call it MEVless protocol,   it means you don’t have to trust any group and institution.  You do not have to depend on a privacy node, not through MEVA, to protect your transactions from MEV attack.  Because all the attackers(besides miners themselves) cannot see your tx-content when it orders.   Once the tx-content is packed and executed, it must be packed by the previously commitment, attackers cannot front-run and sandwich attack you.</p>
<p>We have developed some of it and you can see the running effect:</p><aside class="onebox githubfolder">
  <header class="source">
      <img class="site-icon" height="32" src="https://ethresear.ch/uploads/default/original/2X/b/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg" width="32" />

      <a href="https://github.com/yu-org/nine-tripods/tree/main/MEVless" rel="noopener nofollow ugc" target="_blank">github.com</a>
  </header>

  <article class="onebox-body">
    <h3><a href="https://github.com/yu-org/nine-tripods/tree/main/MEVless" rel="noopener nofollow ugc" target="_blank">nine-tripods/MEVless at main · yu-org/nine-tripods</a></h3>


  <p><span class="label1">Contribute to yu-org/nine-tripods development by creating an account on GitHub.</span></p>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both;"></div>
</aside>
<aside class="onebox githubfolder">
  <header class="source">
      <img class="site-icon" height="32" src="https://ethresear.ch/uploads/default/original/2X/b/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg" width="32" />

      <a href="https://github.com/VersechainLabs/versechain/tree/mevless" rel="noopener nofollow ugc" target="_blank">github.com</a>
  </header>

  <article class="onebox-body">
    <h3><a href="https://github.com/VersechainLabs/versechain/tree/mevless" rel="noopener nofollow ugc" target="_blank">GitHub - VersechainLabs/versechain at mevless</a></h3>

  <p><a href="https://github.com/VersechainLabs/versechain/tree/mevless" rel="noopener nofollow ugc" target="_blank">mevless</a></p>

  <p><span class="label1">A high performance decentralized modular sequencer for Starknet</span></p>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both;"></div>
</aside>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/f/bfa58e407bea18228ba10bbc90f904aca2c776aa.jpeg" title="image"><img alt="image" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/b/f/bfa58e407bea18228ba10bbc90f904aca2c776aa_2_607x500.jpeg" width="607" /></a></div><p></p>
<p>You can see the txHash order-commitment in the above red box and you can try MEV-attacking these txs when they are completed by tx-contents later, then you will find you cannot insert your tx into their order at all.</p>
            <p><small>9 posts - 3 participants</small></p>
            <p><a href="https://ethresear.ch/t/a-trustless-on-chain-anti-mev-solution-for-layer2-3/20260">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 13 Aug 2024 04:02:59 +0000</pubDate>
</item>
<item>
<title>Enabling truly encrypted DeFi in FHE rollups</title>
<link>https://ethresear.ch/t/enabling-truly-encrypted-defi-in-fhe-rollups/20259</link>
<guid>https://ethresear.ch/t/enabling-truly-encrypted-defi-in-fhe-rollups/20259</guid>
<content:encoded><![CDATA[
<div> 关键词：文章、提取、5个、中文、总结

---

总结: 这篇文章要求从一段文本中提取出五个关键点，并用中文进行总结。首先，需要识别出文本中的核心信息或主题，这通常包括最重要的事实、观点或论点。然后，将这五个关键点归纳出来，确保它们覆盖了文本的主要内容和重点。

具体操作步骤如下：

1. **阅读与理解**：仔细阅读原文，确保理解其主要论点和细节。
2. **识别关键点**：找出文本中最重要、最能代表其主旨的部分，这些可能是论据、结论、主要事件或人物等。
3. **提炼概括**：将这些关键点以简洁的语言表述出来，确保每个关键点都能独立反映原文的一部分核心信息。
4. **整合总结**：将提炼出的五个关键点整合成一个连贯的段落，确保整体内容完整地涵盖了原文的主题和要点。

通过这样的过程，可以有效地对复杂文本进行简化和归纳，帮助读者快速掌握文章的核心内容。 <div>
<p>(topic deleted by author)</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/enabling-truly-encrypted-defi-in-fhe-rollups/20259">Read full topic</a></p>
]]></content:encoded>
<pubDate>Mon, 12 Aug 2024 13:53:24 +0000</pubDate>
</item>
<item>
<title>Proof of Service Integrity (PoSI): Trustless measurement of service integrity</title>
<link>https://ethresear.ch/t/proof-of-service-integrity-posi-trustless-measurement-of-service-integrity/20255</link>
<guid>https://ethresear.ch/t/proof-of-service-integrity-posi-trustless-measurement-of-service-integrity/20255</guid>
<content:encoded><![CDATA[
<div> 关键词：PoSI、服务完整性、去中心化验证、可信计算、智能合约

总结：

本文主要介绍了Proof of Service Integrity（PoSI）协议，一种用于验证离链服务完整性的去中心化验证机制。PoSI协议旨在解决现代区块链架构中增长的服务量、规模和复杂性与传统信任部署模式之间的不兼容问题。其核心目标是确保离链服务的正确性、完整性和可验证性。

1. **服务完整性定义**：PoSI协议关注于测量离链服务的完整性，包括正确部署了授权和验证的软件版本以及未进行未经授权的更改。这与互联网系统中的服务完整性不同，后者通常由服务的所有者或管理者负责，而PoSI允许任何人均可无权限验证服务的完整性。

2. **去中心化验证的重要性**：在区块链系统中，通过精心设计的激励机制和共识算法，确保了链上逻辑的安全性。然而，离链服务由于其在信任环境中运行，且无法在链上归因，因此存在多种风险，如内部威胁、未经授权的修改、审查风险和数据/资金安全风险。

3. **PoSI协议概述**：PoSI协议通过实现认证部署、持续完整性监控和完整性证明来验证服务的完整性。它确保只有经过验证的代码被部署，并定期监控服务以检测未经授权的修改或篡改。用户可以通过无权限接口请求服务完整性证明。

4. **架构工作流程**：PoSI协议涉及三个关键工作流程，包括服务开发者的工作流、运营商的工作流和验证工作流。服务开发者注册服务图像、设置预期指标并触发部署；运营商注册并执行任务，如托管服务图像或执行测量；应用程序或链外链请求服务完整性证明。

5. **多层安全模型**：PoSI协议采用多层次安全模型，结合共识机制、可信计算和经济激励，提供全面的安全保障。该模型要求参与的服务具有开源代码、公开可验证的服务映像、可复现的构建过程和容器化部署。

通过以上五个关键词的总结，我们可以看到PoSI协议在解决离链服务完整性验证方面的重要性和创新性，它通过去中心化的方法，结合多种技术手段，为构建更加安全、可靠和透明的区块链生态系统提供了新的思路和实践路径。 <div>
<h1><a class="anchor" href="https://ethresear.ch#p-49574-proof-of-service-integrity-posi-trustless-measurement-of-service-integrity-1" name="p-49574-proof-of-service-integrity-posi-trustless-measurement-of-service-integrity-1"></a>Proof of Service integrity (PoSI) : Trustless measurement of service integrity</h1>
<h2><a class="anchor" href="https://ethresear.ch#p-49574-tldr-2" name="p-49574-tldr-2"></a>TL;Dr</h2>
<p><strong>Proof of Service Integrity (PoSI)</strong> is a byzantine fault tolerant verification protocol for offchain activities.</p>
<p>It performs three main tasks in a decentralised fashion - <em>deployment</em> of approved service images, <em>measurements</em> of deployed services, and <em>attestation</em> of the integrity of these services in production.</p>
<p>The problem PoSI solves is that offchain services are growing in volume, size and complexity in modern chain architectures, but they are largely centralised and run in trusted environments while handling millions of dollars of transaction flows. This is incompatible with the goals of crypto systems. Permissionless verification of offchain services using PoSI protocol provides a real-time integrated security view for emerging hybrid crypto protocols that have a mix of on-chain and off-chain activities.</p>
<p>Offchain services that are verified by PoSI protocol are called <strong>Integrity Verified services</strong> (IVS).</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49574-preface-3" name="p-49574-preface-3"></a>Preface</h2>
<p>This post builds on the earlier proposal on integrity proofs (<a class="inline-onebox" href="https://ethresear.ch/t/integrity-proofs-to-improve-rollup-security/19437">Integrity proofs to improve rollup security</a>) with the following main differences:</p>
<ol>
<li>Focus on measuring integrity of any off-chain service, rather than just rollup services</li>
<li>Earlier design was TEE-based, current protocol is primarily BFT-based but uses TEEs as a <em>defense-in-depth</em> mechanism.</li>
<li>Changes to the architecture</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#p-49574-prelude-4" name="p-49574-prelude-4"></a>Prelude</h2>
<p>Traditional distributed systems monitoring/observability involves collecting and analyzing data in order to gain insights into the functioning, performance, security and health of software systems and applications. It involves systematically observing and tracking various metrics, events, logs and distributed traces to construct a visual representation of a system’s hardware and software performance and health. While there are multiple types of distributed systems monitoring data, one  dimension in particular that is not measured in traditional web2 distributed systems is service integrity.</p>
<p>For this post, let’s define <em>service integrity</em> as the following:</p>
<ol>
<li>The correct (authorised and verified) software version has been deployed</li>
<li>No unauthorised changes have been made to the deployed software in production.</li>
<li>Anyone can permissionlessly verify proof of <span class="hashtag-raw">#1</span> and <span class="hashtag-raw">#2</span> for any given service either through data provided over a user interface or API, or through verification of a zero-knowledge proof.</li>
</ol>
<p>In internet/online systems (web2), <em>services integrity</em> (particularly <span class="hashtag-raw">#1</span> and <span class="hashtag-raw">#2</span>) is the responsibility of the organisation or entity that centrally owns and manages the distributed service, <em>aka trusted deployments</em>. As a consequence, <span class="hashtag-raw">#3</span> is simply not possible.</p>
<p>When we talk about web3 systems, <em>service integrity</em> becomes paramount. Services are deployed in <em>untrusted environments</em> managed by operators that we do not know or have legal contracts with.</p>
<p>The way this problem has been solved in blockchain-based systems (Proof-of-stake in particular) is through a carefully designed set of incentives to encourage external operators to run the distributed software with desired behaviours, coupled with a clever mechanism for the distributed network to reach a consensus such that if an operator that is part of the consensus set is detected to perform any malicious action, they can be financially penalised (through onchain mechanisms or social governance).</p>
<p>This worked reasonably in the early days of evolution of onchain systems where all the logic for onchain protocols were on smart contracts on a single chain, which was invoked from offchain clients. Censorship resistance was largely handled by allowing anyone to run the Json-RPC nodes (which are the user transaction entry points) that communicate with the other distributed network nodes over P2P protocols. This ensured eventual censorship-resistance.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49574-evolution-of-crypto-protocol-architectures-5" name="p-49574-evolution-of-crypto-protocol-architectures-5"></a>Evolution of crypto protocol architectures</h2>
<p>Recent developments in blockchain systems have seen an explosion in the number of layer-1 and layer-2 chains, and the rise of modular architectures with innovations in application protocols, core infrastructure, scaling and interop solutions, developer &amp; user tools. These innovations are aimed at solving problems with scaling throughput, reducing latency, lowering transaction costs, offering greater sovereignty to builders over design choices, solving for both synchronous and async interoperability, unifying liquidity, mev optimisation, and improving user experience in crypto.</p>
<p>These developments have resulted in increased complexity and sophistication of onchain protocols involving a mix of smart-contract logic and offchain logic. Emerging use cases such as cross-chain swaps involve a mix of smart contract and offchain logic on both the source and destination chains.</p>
<p>Let’s look at a few of the hybrid onchain-offchain architectures in popular crypto protocols.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/9/491a30b1c361107e65615618ea2b29c226245e4d.jpeg" title="Fig 1: Hybrid onchain-offchain design in crypto protocols"><img alt="Fig 1: Hybrid onchain-offchain design in crypto protocols" height="474" src="https://ethresear.ch/uploads/default/optimized/3X/4/9/491a30b1c361107e65615618ea2b29c226245e4d_2_690x474.jpeg" width="690" /></a></div><p></p>
<p>Fig A shows onchain logic on a single chain encoded as smart contracts. The onchain logic is accessed from a regular web or mobile client application through RPC calls.</p>
<p>Fig B shows an example of crypto protocol containing a mix of onchain smart contract logic on a single chain and offchain component attached to it. The offchain component typically either supplies data from an online system (eg price feeds through oracle) or performs compute-heavy operations on behalf of the smart contract (eg co-processor). The offchain component can also be a regular web backend of the dapp, if the app developer chooses to keep a portion of the business logic offchain (which is not uncommon in most modern dapps).</p>
<p>Fig C shows an example of a cross-chain transaction that involves two chains - source and destination chain (e.g., cross-chain swaps or bridging). Here, smart contract logic is present on both the chains, and there are corresponding offchain components.</p>
<blockquote>
<p>The main challenge that is being addressed in this post is that a big proportion of the off-chain components that are part of these hybrid onchain-offchain crypto protocols are run in trusted environments. This is incompatible with the main goals of crypto protocols which are trustlessness, censorship resistance and permission-less participation and verifiability.</p>
</blockquote>
<p>While the onchain components (aka smart contracts) are secured by consensus, economic incentives and permissionless verification, the same cannot be said about offchain services whose actions cannot be attributed onchain.  These services are, in most cases, centralised,  owned and run by trusted entities, but play critical role in the overall transaction workflows. They are vulnerable to censorship, tampering and other kinds of attacks. Note that only the on-chain logic of the crypto protocols is secured by the blockchain consensus, not the supporting off-chain infrastructure and services which have varying levels of trust assumptions. In some cases, it is not even possible to detect malicious actions performed by such offchain components <em>(non-attributable faults)</em>.</p>
<p>Figure 2 shows a non-exhaustive list of popular categories of offchain services that are an integral part of many crypto protocols.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/b/4b764f4c7ce39d7b0d4769c441ff37392c08d505.png" title="Fig 2: Common categories of off-chain services"><img alt="Fig 2: Common categories of off-chain services" height="350" src="https://ethresear.ch/uploads/default/optimized/3X/4/b/4b764f4c7ce39d7b0d4769c441ff37392c08d505_2_690x350.png" width="690" /></a></div><p></p>
<p>What are the types of risks to crypto protocols with such centralised offchain services?</p>
<p><em>Insider Threats</em>: Employees or contractors within the service development team or the cloud platform provider may misuse their privileged access.<br />
<em>Unauthorized Modifications</em>: Malicious actors might attempt to alter the service code logic or configuration without detection, leading to unintended consequences inconsistent with the protocol goals.<br />
<em>Censorship Risks</em>: In case of offchain services, bad actors might attempt to censor certain transactions or user interactions.<br />
<em>Data and Fund Security</em>: There’s a risk of unauthorized access to sensitive data or funds managed by the service. e.g. a dapp backend managing an embedded wallet may view/steal user wallet keys.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49574-we-need-a-decentralised-verification-protocol-6" name="p-49574-we-need-a-decentralised-verification-protocol-6"></a>We need a decentralised verification protocol</h2>
<p>Hence, a critical requirement for the success of these modular hybrid onchain-offchain architectures is the ability to prove offchain service integrity at scale in a decentralised trustless manner, i.e. a <em>Byzantine fault tolerant service integrity verification system</em>.</p>
<p>In this post, we present <em>Proof of Service Integrity</em> (<strong>PoSI</strong>), a verification protocol that performs three main tasks - <em>deployment of publicly-identifiable code images</em>, <em>measurement of the correctness  of code deployed periodically</em>, and  <em>attestation of service integrity</em> in the production environment. These correspond respectively to the properties of <em>correctness</em>, <em>integrity</em> and <em>verifiability</em> for the monitored services. Figure 3 shows the key desired properties and relationships between the PoSI nodes that are part of the verification network, and the monitored services.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/3/135c0d620ec8bd53705bf83f07fd11c3aff47d42.jpeg" title="Fig 3: Verification layer for off-chain services"><img alt="Fig 3: Verification layer for off-chain services" height="163" src="https://ethresear.ch/uploads/default/optimized/3X/1/3/135c0d620ec8bd53705bf83f07fd11c3aff47d42_2_690x163.jpeg" width="690" /></a></div><p></p>
<p>The PoSI nodes that implement the verification protocol itself satisfy the following  properties: 1) <em>Trustless:</em> Service integrity measurements are secure against byzantine attacks by collaborations among the monitoring services and the monitored services. 2) <em>Tamper-proof</em>: The service monitoring service while verifying the tamper-resistance of the monitored services, is itself tamper-resistant  3) <em>Open</em>: The protocol allows anyone to register and provide measurement data , by using cryptographic primitives to ensure that a subset of actors cannot maliciously modify results in their favour.</p>
<p>A formal security model allows us to establish guarantees of accurate service measurements in the presence of malicious actors. The security guarantees of the PoSI protocol are composable with the onchain state commitments on blockchain ledgers to provide a comprehensive view of protocol security which is not possible by just focusing on smart-contract &amp; consensus-based security.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/2/f2a8e22a3c90dfb18ded2bfa3c371ef147452247.png" title="Fig 4: Integrated view of security of crypto protocols with PoSI"><img alt="Fig 4: Integrated view of security of crypto protocols with PoSI" height="201" src="https://ethresear.ch/uploads/default/optimized/3X/f/2/f2a8e22a3c90dfb18ded2bfa3c371ef147452247_2_690x201.png" width="690" /></a></div><p></p>
<h2><a class="anchor" href="https://ethresear.ch#p-49574-proof-of-service-integrity-posi-protocol-overview-7" name="p-49574-proof-of-service-integrity-posi-protocol-overview-7"></a>Proof of Service Integrity (PoSI) protocol overview</h2>
<p>PoSI enables verifiable service integrity through the following:</p>
<p><em>Authenticated Deployment</em>: PoSI ensures that only authorized and verified code is deployed to the production environment. This prevents the introduction of malicious or unauthorized code during the deployment process.</p>
<p><em>Continuous Integrity Monitoring</em>: Once deployed, PoSI nodes continuously monitor the service to detect any unauthorized modifications or tampering. Any discrepancies between the running service and its expected state are immediately detected and reported.</p>
<p><em>Integrity Attestation</em>: Users or dApps can request integrity proofs for any PoSI-enabled service through a permissionless, public interface. Two types of integrity checks can be done on a given service - <em>measurements-based</em> and <em>proof-based</em>. <em>Measurements-based</em> checks involve deriving service integrity from the onchain measurements for the service. <em>Proof-based</em> checks can be done by requesting a SNARK proof of integrity  for the service, which can then be verified either on-chain (SNARK verification) or off-chain (in a web or mobile app).</p>
<p>Services that are verified by PoSI protocol are called <em>Integrity-verified services</em> (IVS).</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49574-architecture-workflows-8" name="p-49574-architecture-workflows-8"></a>Architecture workflows</h2>
<p>PoSI protocol involves the following three workflows:</p>
<ol>
<li>Service developer workflow</li>
<li>Operator workflow</li>
<li>Verification workflow</li>
</ol>
<p>Figure 5 shows an overview of the key actors and actions in the protocol.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/2/929487f88493ab7c774f428b5bed736328ff9f72.jpeg" title="Fig 5: Overview of PoSI protocol"><img alt="Fig 5: Overview of PoSI protocol" height="268" src="https://ethresear.ch/uploads/default/optimized/3X/9/2/929487f88493ab7c774f428b5bed736328ff9f72_2_690x268.jpeg" width="690" /></a></div><p></p>
<p>The architecture of PoSI involves the following components and actors:</p>
<p><strong>1. Human/Organisational actors:</strong></p>
<p><em>Service developer</em>: This refers to the developer and owner of the distributed software service. The service developer is the main ‘customer’ for the integrity-verified service, and is the person or entity that is ready to pay a fee to have their service integrity-verified.</p>
<p><em>Operator:</em> This refers to the provider of the computational infrastructure. The service developer can themselves choose to be the operator by deploying the IVS on a cloud account controlled by them or they can choose to deploy their service on an external operator’s VM through a DePIN service.</p>
<p><strong>2. PoSI Platform:</strong></p>
<p><em>PoSI platform onchain:</em> This contains the core smart contracts of the  protocol.</p>
<p><em>PoSI Offchain</em>: This comprises core offchain services that are part of the protocol.</p>
<p><strong>3. Applications/ Other chains:</strong></p>
<p><em>Application</em>:  A web or mobile application that verifies the proof for an IVS.</p>
<p><em>Other chain:</em> Any other chain can verify the zk proofs generated by the PoSI protocol.</p>
<h3><a class="anchor" href="https://ethresear.ch#p-49574-service-developer-workflow-9" name="p-49574-service-developer-workflow-9"></a>Service developer workflow</h3>
<ol>
<li>Service developer builds the service and registers the service image in a public repository.</li>
<li>Service developer registers the service image along with other service metadata with the PoSI onchain smart contracts. They also deposit rewards amount, along with service level expectations (e.g. frequency of measurements).</li>
<li>Service developer can trigger the PoSI smart contract to trigger the service deployment either on their self-hosted VM, their cloud VM or on a DePIN VM.</li>
<li>The PoSI protocol pays out the rewards to the operators based on the tasks performed by them, from the service developer’s account.</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-49574-operator-workflow-10" name="p-49574-operator-workflow-10"></a>Operator workflow</h3>
<ol>
<li>Operator registers their VM with the PoSI registration service. The operator can choose to perform two kinds of tasks - host <em>service images</em>, or host the <em>PoSI host program</em> that performs measurements on other services. For the former, any regular VM of the configuration required by service developers would be accepted. For the latter, TEE-based VMs will be required.</li>
<li>Note that service developer can choose to deploy their service on their own VM, in which case they need to register it like other external operators. For TEE-based VMs, the quote has to be generated by the operator and submitted to the registration service along with in-enclave generated public key.</li>
<li>Operator stakes the minimum specified tokens as part of registration. If the service developer hosts the service on their own VM, this step is not required.</li>
<li>The PoSI registration service verifies the registered VM and registers it with the PoSI onchain contract. The PoSi registration service itself runs within a TEE enclave.</li>
<li>When the service developer triggers deployment of a service, the PoSI host program retrieves the registered service image from public repository and deploys the service on the service developer (or external operator’s VM based on the configuration).</li>
<li>If an operator has registered to host the PoSI protocol, the PoSI master  deploys the PoSI host program on the operator’s VM. This enables the operator to then perform service measurements on other services.</li>
<li>Based on the specification of the service developer, the operator set is established for verifying that service, which runs the consensus mechanism to determine the final service measurements. The votes of all  operators in the operator set are aggregated and recorded onchain, along with the measurements.</li>
<li>At periodic intervals, measurements of the performance of the verious operators are taken by the PoSI measurement service, and rewards are computed for the operators. Any incorrect measurements attributable to any of the operators in operator set is penalized through slashing of their stake, in a manner defined in the PoSI protocol.</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#p-49574-verification-workflow-11" name="p-49574-verification-workflow-11"></a>Verification workflow</h3>
<ol>
<li>Any web or mobile application can ask the PoSI protocol servers for attestation of any particular service. The PoSI protocol returns the proof to the web/mobile application.</li>
<li>Two kinds of proofs can be requested from the PoSI protocol for a service: <em>state proofs</em> and <em>SNARK-proofs</em>. <em>State proofs</em> simple return the onchain state of a service computed from the measurements submitted by operators. SNARK proofs that are returned by the PoSI protocol can be verified either off-chain within the web/mobile application, or submitted to another on-chain smart contract for verification.</li>
</ol>
<p>An integrated view of the various workflows for the PoSI protocol is shown in figure.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/2/32045a0a10edcc50ab33324a649080621d79a8ac.jpeg" title="Fig 6: Integrated view of PoSI workflows"><img alt="Fig 6: Integrated view of PoSI workflows" height="309" src="https://ethresear.ch/uploads/default/optimized/3X/3/2/32045a0a10edcc50ab33324a649080621d79a8ac_2_690x309.jpeg" width="690" /></a></div><p></p>
<p>Note: Figure 6 shows only a single host program taking the service measurements (for reducing clutter in diagram), but it can be visualised as a set of nodes that participate and arrive at a consensus before posting the measurements on-chain.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49574-conclusion-12" name="p-49574-conclusion-12"></a>Conclusion</h2>
<p>Trustfree measurement of offchain service integrity is an unsolved problem in decentralised networks. <strong>Proof of Service Integrity (PoSI)</strong> addresses this core requirement by providing a secure, byzantine resistant verification layer for offchain services while allowing open participation for operators and service developers to benefit from the protocol. All components of the protocol can be operated by community-run protocol nodes controlled by the onchain protocol smart contracts. PoSI incorporates a layered security model that includes <em>consensus-based</em>, <em>hardware-based</em> and <em>crypto-economic security</em>. PoSI requires the participating offchain services to have open source code, a publicly verifiable service image, reproducible build process and dockerized deployment.</p>
<h2><a class="anchor" href="https://ethresear.ch#p-49574-faq-13" name="p-49574-faq-13"></a>FAQ</h2>
<h3><a class="anchor" href="https://ethresear.ch#p-49574-what-kind-of-services-can-benefit-from-the-posi-protocol-14" name="p-49574-what-kind-of-services-can-benefit-from-the-posi-protocol-14"></a>What kind of services can benefit from the PoSI protocol?</h3>
<p>Any in-protocol or out-of-protocol offchain service can benefit from PoSI protocol. A non-exhaustive list of offchain services was mentioned earlier in the post, and is reproduced here:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/b/4b764f4c7ce39d7b0d4769c441ff37392c08d505.png" title="Fig 7: Popular categories of off-chain services"><img alt="Fig 7: Popular categories of off-chain services" height="350" src="https://ethresear.ch/uploads/default/optimized/3X/4/b/4b764f4c7ce39d7b0d4769c441ff37392c08d505_2_690x350.png" width="690" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#p-49574-what-are-the-alternative-architectures-available-to-secure-offchain-services-15" name="p-49574-what-are-the-alternative-architectures-available-to-secure-offchain-services-15"></a>What are the alternative architectures available to secure offchain services?</h3>
<p>For offchain services to transition from <em>trusted</em> to <em>trust-minimised</em> / <em>trustless</em> architectures, here is a comparison of the various design approaches.</p>
<div class="md-table">
<table>
<thead>
<tr>
<th>Design approach</th>
<th>Description</th>
<th>Pros</th>
<th>Cons</th>
<th>Security model</th>
</tr>
</thead>
<tbody>
<tr>
<td>Consensus-based</td>
<td>Build a BFT-consensus with own operator set</td>
<td>Trustless</td>
<td>It is expensive and cumbersome for a service developer</td>
<td>Depends on size of the operator set</td>
</tr>
<tr>
<td>ZK-based</td>
<td>Build a custom zk circuit or a program that can be proven in a general purpose zkVM</td>
<td>Trustless</td>
<td>Involves rewrite of the service using zk DSLs or using Rust. Expensive to generate zk-proofs</td>
<td>Restricted to what can be proven in zk circuits</td>
</tr>
<tr>
<td>EigenLayer AVS-based</td>
<td>Convert the service into Eigenlayer AVS</td>
<td>Inherit Ethereum security without bootstrapping an operator set</td>
<td>Requires rewrite of the code to comply with AVS protocol. Also AVS can only detect and penalise operator faults if they are observable on-chain.</td>
<td>Economic security</td>
</tr>
<tr>
<td>PoSI IVS-based</td>
<td>Deploy existing code in docker containers with no code rewrite.</td>
<td>Ability to detect non-attributable faults (those that are not normally visible on-chain such as censorship, or unauthorized upgrades of service algorithms). Small, configurable cost.</td>
<td>Services should meet pre-requisites: open-source code, a publicly verifiable service image, a reproducible build process and dockerized deployment</td>
<td>Multi-layered security model incorporating <em>consensus-based</em>, <em>TEE</em>, and <em>crypto-economic security</em> constructs.</td>
</tr>
</tbody>
</table>
</div><h2><a class="anchor" href="https://ethresear.ch#p-49574-credits-16" name="p-49574-credits-16"></a>Credits</h2>
<p>The concept and design for PoSI protocol and Integrity-verified services was initially developed as a collaboration between <a class="mention" href="https://ethresear.ch/u/peshwar9">@peshwar9</a> and <a class="mention" href="https://ethresear.ch/u/mohsinriaz17">@mohsinriaz17</a> with contribution from several others to refine and enhance it.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/proof-of-service-integrity-posi-trustless-measurement-of-service-integrity/20255">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sun, 11 Aug 2024 17:44:55 +0000</pubDate>
</item>
<item>
<title>Pls ignore. New post available</title>
<link>https://ethresear.ch/t/pls-ignore-new-post-available/20251</link>
<guid>https://ethresear.ch/t/pls-ignore-new-post-available/20251</guid>
<content:encoded><![CDATA[
<div> 关键词：文章、提取、5个、中文、总结

---

总结: 这篇文章要求从一段文本中提取出五个关键点，并用中文进行总结。首先，需要识别出文本中的核心信息或主题，这通常包括最重要的事实、观点或论点。然后，将这五个关键点归纳出来，确保它们覆盖了文本的主要内容和重点。

具体操作步骤如下：

1. **阅读与理解**：仔细阅读原文，确保理解其主要论点和细节。
2. **识别关键点**：找出文本中最重要、最能代表其主旨的部分，这些可能是论据、结论、主要事件或人物等。
3. **提炼概括**：将这些关键点以简洁的语言表述出来，确保每个关键点都能独立反映原文的一部分核心信息。
4. **整合总结**：将提炼出的五个关键点整合成一个连贯的段落，确保整体结构清晰，逻辑性强，能够全面地反映原文的主要内容。

通过以上步骤，可以有效地完成从原文到精炼中文总结的过程，确保信息的准确性和完整性。 <div>
<p>(topic deleted by author)</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/pls-ignore-new-post-available/20251">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sun, 11 Aug 2024 08:56:36 +0000</pubDate>
</item>
<item>
<title>Proof of Service integrity</title>
<link>https://ethresear.ch/t/proof-of-service-integrity/20251</link>
<guid>https://ethresear.ch/t/proof-of-service-integrity/20251</guid>
<content:encoded><![CDATA[
<div> 关键词：文章、提取、5个、中文、总结

---

总结: 这篇文章要求从一段文本中提取出五个关键点，并用中文进行总结。首先，需要识别出文本中的核心信息或主题，这通常包括最重要的事实、观点或论点。然后，将这五个关键点归纳出来，确保它们覆盖了文本的主要内容和重点。

为了实现这一目标，第一步是仔细阅读并理解原始文本，确定其主要论点和关键信息。接着，选择五个最能代表文本核心内容的元素进行提炼。这些关键点可能是主要事件、重要人物的观点、统计数据、对比分析或是结论等。

在进行中文总结时，应确保语言简洁明了，同时保持对原始信息的准确传达。总结应该清晰地列出上述五个关键点，并简要阐述每个点与整体文本的关系，以帮助读者快速理解文本的核心内容。在整个过程中，注意保持逻辑连贯性和信息的完整性，使得总结既全面又易于理解。 <div>
<p>(topic deleted by author)</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/proof-of-service-integrity/20251">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sun, 11 Aug 2024 08:56:36 +0000</pubDate>
</item>
<item>
<title>Efficient Data Distribution with Reed-Solomon Codes for Sharded Storage</title>
<link>https://ethresear.ch/t/efficient-data-distribution-with-reed-solomon-codes-for-sharded-storage/20232</link>
<guid>https://ethresear.ch/t/efficient-data-distribution-with-reed-solomon-codes-for-sharded-storage/20232</guid>
<content:encoded><![CDATA[
<div> 关键词：Reed-Solomon编码、区块链存储、数据分布、O(N log n)解码复杂度、M31域

总结：

本文介绍了一种利用Reed-Solomon（RS）编码高效分配N个数据元素至n个节点的方法，旨在解决区块链存储的扩展问题。与简单地将数据放大至bN倍的直觉方法不同，本文提出的方法通过将数据表示为表格并创建数据碎片，实现了O(N log n)的解码复杂度。

首先，对于2-adicity域的情况，我们以BabyBear域为例进行说明。假设有一个长度为N的向量{a_i}，目标是将其在n个服务器间分布，确保任何k个服务器都能恢复原始数据。通过使用RS编码，我们将向量表示为大小为m×k的表格，并构建了一个双变量多项式f(x,y)，其中f(x,y) = ∑_{ij} a_{ij} L_i(x) λ_j(y)。通过快速傅里叶变换（FFT）对表格的每一行进行操作，我们得到了多项式的特定形式，从而可以为每个节点分配唯一的线性组合的列。为了验证数据碎片的有效性，我们引入了替换y=x^m的技巧，这有效地将所有列连接起来，便于进行多项式承诺计算。

对于M31域的情况，我们采用类似的方法，但使用了环傅里叶变换（CFFT）。在环表示中，多项式f(x,y)被定义为f(x,y)=f_0(x)+yf_1(x)，其中f_0(x)和f_1(x)的次数分别为N/2-1。通过对表格的每一行执行CFFT，我们得到了大小为n的m个向量。然后，我们定义f(x,y,u,v) = ∑_{ij} a_{ij} L_i(x,y) Λ_j(u)，其中Λ_j(u)是围绕圆的偶数拉格朗日基函数。通过替换u=x^{m/2}，我们能够将所有一维组件合并为单个多项式，同时保持结构f_0(x)+yf_1(x)，从而实现数据碎片化。

此方法适用于数据恢复、存储以及零知识证明（zk）应用。在数据恢复过程中，任何k个碎片都足够恢复原始数据。在存储方面，这种方法允许直接存储多项式承诺，而不仅仅是中间表示形式的数据。此外，算法描述部分详细介绍了如何生成碎片及其承诺，并将数据分布到集群中的节点上。通过这种方法，即使客户端的带宽有限，也能有效处理大数据文件的分布。

总之，本文通过引入RS编码和FFT/CFFT技术，提供了一种优化的数据分布式存储方案，特别适用于需要整体节点失败保护的区块链系统。尽管多项式承诺的计算复杂度保持不变，但这种方法在数据处理流程中提供了显著的优化，使其成为区块链存储扩展的有力解决方案。 <div>
<h2><a class="anchor" href="https://ethresear.ch#introduction-1" name="introduction-1"></a>Introduction</h2>
<p>This writeup presents an efficient method for distributing N data elements across n nodes using Reed-Solomon (RS) encoding, specifically designed for blockchain <a href="https://ethresear.ch/t/blockchain-sharded-storage-web2-costs-and-web3-security-with-shamir-secret-sharing/18881">sharded storage</a> solutions. We address the challenge of scaling blockchain storage by introducing techniques that achieve O(N log n) decoding complexity, where N is the total amount of data and n is the number of nodes.</p>
<p>A naive approach would be to simply blow up the data from N to bN, where b is the blowup factor. However, this would result in O(N log N) decoding complexity. Our method, by representing data as a table and creating data shards, achieves O(N log n) decoding complexity, which is significantly faster.</p>
<p>It’s crucial to note that we don’t need to apply RS codes to all data together. This is because a node can only go offline as a whole - there can’t be a situation where two nodes lose half of their data each, requiring RS codes to recover the data. A node either loses all its data or provides it entirely.</p>
<p>While our method doesn’t significantly improve the speed of calculating polynomial commitments (which remains O(N log N) for FRI), it greatly optimizes the data encoding-decoding procedure.</p>
<h3><a class="anchor" href="https://ethresear.ch#naive-approach-2" name="naive-approach-2"></a>Naive approach</h3>
<p><img alt="" height="150" src="https://ethresear.ch/uploads/default/original/3X/6/9/691031aaa0990298d3f1755f55e1cc286cb49197.svg" width="600" /></p>
<h3><a class="anchor" href="https://ethresear.ch#our-approach-3" name="our-approach-3"></a>Our approach</h3>
<p><img alt="" height="240" src="https://ethresear.ch/uploads/default/original/3X/6/a/6a6eea46c44a4be6f7311ec8edbab96cf8f81cd0.svg" width="600" /></p>
<h2><a class="anchor" href="https://ethresear.ch#notation-and-definitions-4" name="notation-and-definitions-4"></a>Notation and Definitions</h2>
<p>Before proceeding with the detailed description of our method, let’s define the key terms and symbols used throughout this writeup:</p>
<ul>
<li>N: Total amount of data elements</li>
<li>n: Number of nodes in the network</li>
<li>k: Minimum number of nodes required to recover the original data</li>
<li>b: Blowup factor, defined as b = n/k</li>
<li>m: Number of rows in the data table representation</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#h-2-adicity-fields-case-5" name="h-2-adicity-fields-case-5"></a>2-Adicity Fields Case</h2>
<p>We consider a 2-adic prime field, specifically the BabyBear field with prime p = 15 * 2^27 + 1.</p>
<p>Let’s consider we have a vector <span class="math">{a_i}</span> of N elements of field <span class="math">F_p</span>. We want to distribute this vector among n servers, such that any k servers can recover the original vector. We use Reed-Solomon codes to achieve this.</p>
<p>We represent the vector <span class="math">{a_i}</span> as a table <span class="math">{a_{ij}}</span> of size <span class="math">m \times k</span> with <span class="math">m</span> rows and <span class="math">k</span> columns.</p>
<p>We define a bivariate polynomial <span class="math">f(x,y)</span> to represent our data:</p>
<p><span class="math">f(x,y) = \sum\limits_{ij} a_{ij} L_i(x) \lambda_j(y)</span></p>
<p>where <span class="math">L_i(x)</span> is a Lagrange polynomial of degree <span class="math">m-1</span> and <span class="math">\lambda_j(y)</span> is a Lagrange polynomial of degree <span class="math">k-1</span>.</p>
<p>After performing FFT over each row of the table, <span class="math">f(x,y)</span> takes the following form:</p>
<p><span class="math">f(x,y) = \sum\limits_{ij} b_{ij} L_i(x) y^j = \sum\limits_{j} f_j(x) y^j</span></p>
<p>where <span class="math">f_j(x)=\sum\limits_{i} b_{ij} L_i(x)</span> is a polynomial of degree <span class="math">m-1</span>.</p>
<p>Each node should receive a unique linear combination of the columns of the table. Then we can recover the original vector by solving a system of linear equations. Let’s represent the data shard as <span class="math">f(x,y_0)</span>, where <span class="math">y_0</span> is a fixed value for each shard.</p>
<p><span class="math">f(x,y) - f(x,y_0) = \sum\limits_{j} (y^j - y_0^j) f_j(x) = (y-y_0) q(x,y)</span></p>
<p>where <span class="math">q(x,y)</span> is a quotient polynomial.</p>
<p>We make the substitution <span class="math">y=x^m</span> without loss of any inner polynomial structure. This substitution effectively concatenates all columns of the table, one after another, which is convenient for creating a polynomial commitment.</p>
<p>After the substitution, we get the following polynomial equation to check that the shard is a valid part of the original data:</p>
<p><span class="math"> f(x,x^m) - f(x, y_0) = (x^m - y_0) q(x,x^m) </span></p>
<h2><a class="anchor" href="https://ethresear.ch#circle-fields-case-6" name="circle-fields-case-6"></a>Circle Fields Case</h2>
<p>We now consider the M31 field with p = 2^32 - 1. [HLP24] proposed a method called CFFT (Circular Fast Fourier Transform), which is analogous to FFT but works with polynomials defined on a complex circle.</p>
<p>In the circle representation, the polynomial takes the form <span class="math">f(x,y)=\Re(f(z))</span>, where <span class="math">|z|=1</span>.</p>
<p>Due to the circle constraint <span class="math">|z|^2 = x^2 + y^2 = 1</span>, the polynomial can be represented as:</p>
<p><span class="math">f(x,y) = f_0(x) + y f_1(x)</span></p>
<p>where <span class="math">f_0(x)</span> and <span class="math">f_1(x)</span> are polynomials of degree <span class="math">N/2-1</span>. Note that we have two polynomials of this degree, providing sufficient degrees of freedom.</p>
<p>Let’s represent the data vector <span class="math">{a_i}</span> as a table <span class="math">{a_{ij}}</span> of size <span class="math">m \times k</span> with <span class="math">m</span> rows and <span class="math">k</span> columns. We perform circle FFT (CFFT) on each row of the table, resulting in <span class="math">m</span> vectors of size <span class="math">n</span>.</p>
<p><span class="math">f(x,y,u,v) = \sum\limits_{ij} a_{ij} L_i(x,y) \lambda_j(u,v)</span></p>
<p>It’s important to note that the function <span class="math">f(x,y,u,v)</span> is defined on a torus: <span class="math">x^2+y^2=1</span>, <span class="math">u^2+v^2=1</span>.</p>
<p>Let’s consider <span class="math">f(x,y,u,v)</span> as <span class="math">v</span>-even function. This approach is not useful directly for SNARKs, because then we have even constraint on function values and next row could be dependent on the previous one. However, it’s useful for data distribution.</p>
<p>Then</p>
<p><span class="math">f(x,y,u,v) = f(x,y,u) = \sum\limits_{ij} a_{ij} L_i(x,y) \Lambda_j(u) </span>,<br />
where <span class="math">\Lambda_j(u)</span> is even Lagrange basis on the circle.</p>
<p>After applying CFFT over each row, we get:</p>
<p><span class="math">f(x,y,u) = \sum\limits_{ij} b_{ij} L_i(x,y) u^j = \sum\limits_{j} f_j(x,y) u^j</span></p>
<p>where <span class="math">f_j(x,y)=\sum\limits_{i} b_{i} L_i(x,y)=f_{j,0}(x) + y f_{j,1}(x)</span> and each polynomial is <span class="math">(m/2-1)</span>-ordered.</p>
<p>Let’s consider <span class="math">f(x,y,u_0)</span> as a data shard, where <span class="math">u_0</span> is a fixed value for each shard.</p>
<p><span class="math">f(x,y,u) - f(x,y,u_0) = \sum\limits_{j} (u^j - u_0^j) f_j(x,y) = (u-u_0) q(x,y,u)</span></p>
<p>where <span class="math">q(x,y,u)</span> is a quotient polynomial.</p>
<p>We make the substitution <span class="math">u=x^{m/2}</span> in <span class="math">f(x,y,u)</span>. This substitution does not result in information loss because <span class="math">f_j(x,y)=f_{j,0}(x) + y f_{j,1}(x)</span>, where the degrees of <span class="math">f_{j,0}(x)</span> and <span class="math">f_{j,1}(x)</span> are <span class="math">m/2-1</span>. The resulting polynomial <span class="math">f(x,y,x^{m/2})</span> maintains the structure <span class="math">f_0(x) + y f_1(x)</span> and remains defined on a circle, albeit with each one-dimensional component now of degree <span class="math">N/2-1</span>. This substitution effectively concatenates all columns of the table, similar to the 2-adicity case.</p>
<p>After the substitution, we get the following polynomial equation to check that the shard is a valid part of the original data:</p>
<p><span class="math">f(x,y,x^{m/2}) - f(x,y,u_0) = (x^{m/2}-u_0)q(x,y,x^{m/2})</span></p>
<h2><a class="anchor" href="https://ethresear.ch#applications-7" name="applications-7"></a>Applications</h2>
<h3><a class="anchor" href="https://ethresear.ch#recovering-the-source-data-8" name="recovering-the-source-data-8"></a>Recovering the source data</h3>
<p>Any <span class="math">k</span> shards are enough to recover the original data.</p>
<p><span class="math">f(x,y,u) = \sum\limits_{j} c_{ij} L_i(x,y) \mu(u),</span></p>
<p>where <span class="math">\{\mu_i(u)\}</span> is a Lagrange polynomial basis on the evaluation domain <span class="math">H=\{u_i\}</span>, and <span class="math">u_i</span> are fixed values for each shard.</p>
<p><span class="math"> \mu_i(u) = d_i Z_{H}(u)/(u-u_i),</span><br />
where <span class="math">Z_{H}(u)</span> is a polynomial that is zero at all points of <span class="math">H</span>, <span class="math">d_i</span> is a normalization factor, so</p>
<p><span class="math"> \mu_i(u) = \begin{cases}
1, &amp; u = u_i \\
0 &amp; u \neq u_i
\end{cases}
</span></p>
<p>The source values could be computed as<br />
<span class="math">a_{ij} = f(g^i.x, g^i.y, g^j.x)</span></p>
<h3><a class="anchor" href="https://ethresear.ch#polynomial-storing-9" name="polynomial-storing-9"></a>Polynomial storing</h3>
<p>In some cases, we want to store something directly related to the polynomial commitment of <span class="math">f</span> instead of <span class="math">a_{ij}</span>. This is important for zk applications, like rollups.</p>
<p>Due to the inner structure of coefficient representation, we can represent <span class="math">g(x,y)</span> as <span class="math">f(x,y,x^{m/2})</span>. That means that we can store rollup block data as a set of shards, keeping the source polynomial structure, keeping the source commitment. Then <span class="math">a_{ij}</span> will be some kind of intermediate representation of the committed data.</p>
<h2><a class="anchor" href="https://ethresear.ch#algorithm-description-10" name="algorithm-description-10"></a>Algorithm description</h2>
<pre><code class="lang-python">
def get_shards_and_commitments(data: List[M31], m:int, n:int, k:int, cd:Domain, rd:Domain, xrd:Domain)
    # data is a list of N elements
    # m is the number of rows in the table
    # n is the number of nodes
    # k is the number of nodes required to recover the original data
    # cd is the evaluation domain for the columns
    # rd is the evaluation domain for the rows
    # xrd is evaluation domain for the shards (blown up rows)
    # Returns polynomial commitments and prover data for all the data and shards
    
    # create a table of size m x k, fulfilled row by row
    table = create_table(data, m, k)
    
    # perform cfft on each row of the table
    for row in table:
        row[:] = cfft(row, rd)
    
    # create shards
    shards = [icfft(fit_to_domain_with_zeros(row, rd, xrd), xrd) for row in table]
    
    # convert to col-ordered table
    shards = to_col_ordered(shards)

    # convert table to col_ordered
    table = to_col_ordered(table)

    # compute monomial representation of $f(x,y,x^{m/2})$
    f = concat([cfft(col, cd) for col in table])

    return pcs_monomial_repr(f), [pcs(shard) for shard in shards]

</code></pre>
<h2><a class="anchor" href="https://ethresear.ch#distributing-the-data-over-a-cluster-of-nodes-11" name="distributing-the-data-over-a-cluster-of-nodes-11"></a>Distributing the data over a cluster of nodes</h2>
<p>In practice, the client should deliver the data to <span class="math">n</span> nodes, and the total amount of data is <span class="math">bN</span>. However, for big files, it could be inefficient due to the client’s limited bandwidth.</p>
<p>Instead of this client-centralized approach, <span class="math">b</span> nodes could deliver <span class="math">N \cdot (1-1/k)</span> total data to <span class="math">k-1</span> nodes. There is no bottleneck at the client side (the client sends just <span class="math">N</span> data to one node), but total network data bandwidth is <span class="math">b N \cdot (2-1/k) \approx 2bN</span>.</p>
<p><img alt="" height="493" src="https://ethresear.ch/uploads/default/original/3X/3/b/3be891968cd29eb75294d6219b0d063ddb3bb8f2.svg" width="195" /></p>
<p>There is no valuable computational overhead to compute the shards vectors because with fft or cfft each node can perform a unique coset shift instead of blowup (and the sum of all shifted evaluation domains is the evaluation domain for the blowup).</p>
<p><img alt="" height="250" src="https://ethresear.ch/uploads/default/original/3X/2/0/2015d84ea59b9b3aea04f0aa12ff6cc33087367f.svg" width="340" /></p>
<h2><a class="anchor" href="https://ethresear.ch#conclusion-12" name="conclusion-12"></a>Conclusion</h2>
<p>We have extended our method of data representation to the M31 field, providing a robust framework for efficient data distribution in blockchain storage. By representing data as a table and using FFT/CFFT techniques, we achieve O(N log n) decoding complexity, significantly optimizing the data encoding-decoding procedure. This approach is particularly valuable in blockchain systems where nodes can only fail as a whole, and efficient data recovery is crucial.</p>
<p>While the complexity of polynomial commitment calculations remains O(N log N) for FRI, our method provides substantial benefits in the overall data handling process, making it a promising solution for scalable blockchain storage.</p>
<h2><a class="anchor" href="https://ethresear.ch#references-13" name="references-13"></a>References</h2>
<p><a href="https://eprint.iacr.org/2024/278" rel="noopener nofollow ugc">HLP24</a></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/efficient-data-distribution-with-reed-solomon-codes-for-sharded-storage/20232">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 07 Aug 2024 22:32:41 +0000</pubDate>
</item>
<item>
<title>Aligning DAO contributions with objectives</title>
<link>https://ethresear.ch/t/aligning-dao-contributions-with-objectives/20204</link>
<guid>https://ethresear.ch/t/aligning-dao-contributions-with-objectives/20204</guid>
<content:encoded><![CDATA[
<div> 关键词：DAO、Objective Alignment Engine（OAE）、Jury、Dispute Resolution、Rewards Distribution

文章主要探讨了如何通过建立Objective Alignment Engine（OAE）来优化DAO（去中心化自治组织）中贡献者的激励机制，以确保贡献与DAO的既定目标保持一致。以下是文章的主要内容总结：

1. **动机**：当前的DAO激励机制往往基于规则，如按投票数排名支付固定金额，这可能导致资源分配不合理，无法保证贡献的质量与目标的一致性。引入OAE机制旨在通过客观评估贡献与目标的关联度来改进激励机制。

2. **背景**：在区块链协议中，通常缺乏有效手段将贡献者的激励与网络目标对齐。OAE机制尝试将这一过程纳入协议设计范畴，以实现“利益相关”和“未来决策”等类似解决方案的目标。

3. **假设与基础**：文章假定DAO已经明确定义了其目标，例如以太坊聚焦于安全性，而Optimism则关注可扩展性。基于此明确的目标，可以设计依赖于信息收集而非仅依赖个人偏好的激励机制。

4. **核心组件**：OAE机制的核心包括组建一个能够评估贡献与目标一致性程度的“陪审团”。陪审团成员需要投入代币作为抵押，以获得奖励，并通过机制如SchellingCoin或元市场等确保其报告的真实性。

5. **问题与挑战**：尽管OAE机制提供了一种潜在的解决方案，但仍面临几个挑战，包括明确目标定义的难度、陪审团可能的共谋行为、以及防止操纵预测市场或同伴预测的风险。

总结：
文章提出了Objective Alignment Engine（OAE）的概念，旨在通过构建一个能够评估贡献与DAO目标一致性程度的机制，优化DAO中的激励分配。该机制的核心是组建一个陪审团，负责对贡献进行评价，并通过机制设计确保陪审团的公正性和激励的有效性。然而，实施此类机制仍需解决目标定义的清晰度、陪审团的诚实性和抵抗操纵等问题。通过引入明确的目标定义和有效的激励机制，OAE有望在一定程度上解决DAO治理中的激励对齐问题，促进资源的合理分配和目标的高效实现。 <div>
<h1><a class="anchor" href="https://ethresear.ch#aligning-dao-contributions-with-objectives-1" name="aligning-dao-contributions-with-objectives-1"></a>Aligning DAO contributions with objectives</h1>
<p>In this post, we’re approaching how to align DAO contributions in a setting where a clear goal is already defined.</p>
<p>We will define an Objective Alignment Engine (OAE) as a class of mechanisms that fulfill this objective. We aim to define the contour of such mechanisms so that they optimize resource allocation and provide economic guarantees on the efficacy of incentives.</p>
<p><em>A more complete description along with more concrete examples is available at <a href="https://www.notion.so/r-ag-oae-Objective-Alignment-Engine-6984df3b33cc468e85264a9b975437eb?pvs=21" rel="noopener nofollow ugc">[r.ag.oae] Objective Alignment Engine</a>.</em></p>
<h2><a class="anchor" href="https://ethresear.ch#motivation-2" name="motivation-2"></a>Motivation</h2>
<p>Suppose a DAO where governance contributors are compensated based on a simple rule, like “the top 10 delegates by total votes delegates are paid $10k / month”. As protocol designers, this sounds suboptimal as we have no guarantees that the treasury is spent on the delegates who produce the most useful contributions to governance (e.g. produce the most complete proposals, or vote most consistently). Also, any such rules-based process inevitably becomes gameable under <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law" rel="noopener nofollow ugc">Goodhart’s law</a>.</p>
<p>We’d prefer that contributions were picked individually and reward contributors based on how aligned these contributions are with the overarching goals of the network (e.g., how much are such contributions participating in growth? or decentralization?).</p>
<p>Importantly, we’d also prefer that there is an objective notion of alignment, enabling a mechanism that relies not only on individual preferences but as much as possible on eliciting information (as suggested in <a href="https://ethresear.ch/t/governance-mixing-auctions-and-futarchy/10772">this post on mixing auctions and futarchy</a>).</p>
<h2><a class="anchor" href="https://ethresear.ch#background-3" name="background-3"></a>Background</h2>
<p>On-chain protocols often struggle to align contributor incentives with network goals. While blockchains are designed to optimize resource spending for security, producing alignment with agreed-upon goals is typically left to external governance systems. OAE mechanisms bring contributions and incentives within the purview of the protocol designer.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/c/cc4f46fea1dfde8237a19aac23844864e00fd24f.jpeg" title="image"><img alt="image" height="389" src="https://ethresear.ch/uploads/default/optimized/3X/c/c/cc4f46fea1dfde8237a19aac23844864e00fd24f_2_690x389.jpeg" width="690" /></a></div><p></p>
<p>This approach aligns with the “skin in the game” and futarchy-like solutions suggested in <a class="inline-onebox" href="https://vitalik.eth.limo/general/2021/08/16/voting3.html" rel="noopener nofollow ugc">Moving beyond coin voting governance</a>. We’ll rely on the notion that there is a jury that is incentivized to produce a good judgment of whether contributions are aligned and scale this with additional mechanisms.</p>
<h2><a class="anchor" href="https://ethresear.ch#assumptions-objective-definition-4" name="assumptions-objective-definition-4"></a>Assumptions: objective definition</h2>
<p>A central assumption that we take is that the DAO has a clearly defined objective. While this is theoretically difficult to achieve in a decentralized setting, most protocol values and visions are set initially by the core team and steered by a Foundation.</p>
<p>For example, Ethereum focuses today on <a href="https://ethereum.org/en/roadmap/vision/" rel="noopener nofollow ugc">Scalability, Security, and Sustainability</a>, whereas Optimism has the <a href="https://optimism.io/vision" rel="noopener nofollow ugc">Superchain vision</a>.</p>
<p>In the rest of this post, we assume an existing process produces a clear definition of an objective <span class="math">o</span> (hence, the <em>Objective</em> part of the Alignment Engine).</p>
<p>The existence of such an objective enables designing mechanisms that rely only on eliciting information from participants, namely whether a contribution is aligned or not with the objective.</p>
<h2><a class="anchor" href="https://ethresear.ch#alignment-engine-5" name="alignment-engine-5"></a>Alignment engine</h2>
<h3><a class="anchor" href="https://ethresear.ch#jury-6" name="jury-6"></a>Jury</h3>
<p>Once an objective is defined, we want to set up a jury that can review any contribution and evaluate how aligned it is with the objective. This is the central part of this design.</p>
<p>The main function of the jury is to produce ratings “aligned” / “misaligned” on contributions that are produced on the protocol.</p>
<p>To produce alignment within the jury itself, we rely on mechanisms that incentivize truthful reporting but don’t rely on verifiable outcomes (like, BTC/USD quote). Possible such mechanisms are SchellingCoin or <a href="https://arxiv.org/abs/2306.04305" rel="noopener nofollow ugc">self-resolving prediction markets for unverifiable outcomes</a> (Srinivasan et al, 2023).</p>
<p>To enable incentivization and notably negative rewards, we expect jurors to stake tokens ($ALIGN) and receive token emissions as rewards.</p>
<h3><a class="anchor" href="https://ethresear.ch#dispute-resolution-7" name="dispute-resolution-7"></a>Dispute resolution</h3>
<p>Here we assume that most contributions can be unequivocally qualified as “aligned” or “misaligned” (ie there is a <em>clear</em> way to rate most contributions, as long as <span class="math">o</span> is well defined).</p>
<p>But equivocal cases will inevitably appear. When a contestable result is produced, a dispute resolution mechanism needs to be enforced (either an external one like a Kleros court or an Augur-style ALIGN token fork).</p>
<h3><a class="anchor" href="https://ethresear.ch#calibration-8" name="calibration-8"></a>Calibration</h3>
<p>In general, a juror can be an agent making use of any tools available, including AI and prediction markets, to produce the best evaluations. But this leaves open the question of how to incentivize jurors to get better at their jobs so the jury doesn’t degenerate into a static committee.</p>
<p>If part of the contributions have a ground truth to which their ratings can be compared (e.g. growth contributions that aim at increasing a key metric like TVL for a DeFi protocol or fees for an L2), jurors can be rewarded accordingly. This way, the mechanism can still leverage objective outcomes to improve its accuracy (or <em>be <a href="https://www.overcomingbias.com/p/meta-jury-markets" rel="noopener nofollow ugc">calibrated</a></em>).</p>
<h3><a class="anchor" href="https://ethresear.ch#scaling-9" name="scaling-9"></a>Scaling</h3>
<p>Armed with such a jury, DAO contributions can theoretically be evaluated. To handle large numbers of contributions, two scaling options are available:</p>
<ul>
<li>Prediction markets: bettors predict jury decisions, creating “Aligned” and “Misaligned” tokens.</li>
<li>Peer prediction: raters evaluate contributions, with a small percentage reviewed by the jury.</li>
</ul>
<p>Spam protection through staked curation or auctions ensures only valuable contributions are evaluated.</p>
<h2><a class="anchor" href="https://ethresear.ch#rewards-distribution-10" name="rewards-distribution-10"></a>Rewards distribution</h2>
<p>With contribution evaluation in place, the last bit is to distribute contribution rewards to incentivize the most aligned contributions to be produced in the future.</p>
<p>Aligned contributions receive rewards from treasury or token emissions, proportional to their alignment rating. Highly aligned contributions may be automatically implemented in proposal-like scenarios</p>
<p>This produces a positive feedback loop where:</p>
<ol>
<li>Better-aligned contributions receive more rewards</li>
<li>This incentivizes more aligned contributions in the future</li>
<li>The protocol becomes more resistant to misaligned or captured governance over time.</li>
</ol>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/e/7e67510d59620761b9c2941d8a4067b859bc6ebb.jpeg" title="image"><img alt="image" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/7/e/7e67510d59620761b9c2941d8a4067b859bc6ebb_2_543x500.jpeg" width="543" /></a></div><p></p>
<h2><a class="anchor" href="https://ethresear.ch#attacks-11" name="attacks-11"></a>Attacks</h2>
<p>Some potential limitations and related attacks include:</p>
<ol>
<li><strong>Equivocal objective definition.</strong> Attackers may exploit ambiguous objectives to reward misaligned contributions. This can be mitigated by updating the objective when the DAO observes that equivocation happens.</li>
<li><strong>Jurors collusion and bribing.</strong> This can be countered with staking mechanisms, reputation systems, random juror selection, or shielded voting.</li>
<li><strong>Peer prediction and prediction markets manipulation.</strong> Usual caveats and mitigations apply.</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#questions-12" name="questions-12"></a>Questions</h2>
<p>Such OAE mechanisms rely on the existence of an objective <span class="math">o</span>. We haven’t answered how such an objective can be defined in a general setting. There is an argument that leaving it to regular token-voting just pushes the problem around and the overall mechanism inherits some of the issues of both sub-mechanisms. However, it appears that splitting the problem in two has benefits, as, once an objective is defined, more deterministic outcomes can be achieved through mechanism design.</p>
<p>Also, other kinds of mechanisms can be devised that rely on subjective evaluations. Including subjective evaluations might render objective definition superfluous. But relying on a jury whose jurors input their own preferences leaves the question open of how the jury achieves legitimacy. A solution would be to rely on a measure of juror reputation, as pioneered by Backfeed.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/aligning-dao-contributions-with-objectives/20204">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 01 Aug 2024 23:46:47 +0000</pubDate>
</item>
<item>
<title>ShardDAG: Ordering and Exploitation in Sharded Blockchains</title>
<link>https://ethresear.ch/t/sharddag-ordering-and-exploitation-in-sharded-blockchains/20203</link>
<guid>https://ethresear.ch/t/sharddag-ordering-and-exploitation-in-sharded-blockchains/20203</guid>
<content:encoded><![CDATA[
<div> 关键词：Ethereum、shardDAG、cross-shard transactions、data availability、transaction ordering

总结:
文章探讨了在状态分片区块链中如何通过shardDAG（状态分片有向无环图）架构来解决跨分片交易数据可用性和交易顺序问题。shardDAG架构旨在通过将不同分片链连接成一个有向无环图，为跨分片交易提供可执行的处理顺序，从而限制操纵、利用和审查行为。关键点如下：

1. **数据可用性**：为了确保跨分片交易数据的可用性，shardDAG架构引入了一种机制，要求验证者参与同步链，该链聚合并最终确定来自不同分片的更新状态。这确保了数据能够在分片间流动。

2. **交易和跨分片交易的顺序**：shardDAG通过链接分片块来定义分片块之间的部分顺序，这为跨分片交易提供了可执行的处理顺序。这种顺序防止了跨分片交易被恶意插入以进行利用或审查。

3. **验证数据接收**：为了确保分片接收和处理所需的数据，shardDAG提出了“父条件”和“同步父条件”，这些条件要求分片块的子图包含来自多个分片的数据，从而确保数据接收。

4. **经济激励**：shardDAG设计考虑了经济激励，通过“子条件”确保分片块能在同步链中最终确定，以此来鼓励分片分发其交易数据。

5. **处理限制与优先级**：在处理容量受限的情况下，shardDAG允许通过优先级费用和最大效率价值（MEV）来选择要包含在分片块中的交易和跨分片交易，同时保证未处理的交易和跨分片交易在后续块中得到处理。

综上所述，shardDAG架构通过确保数据可用性和合理的交易顺序，为状态分片区块链提供了一种有效的方式来减少审查和利用风险，同时通过经济激励机制促进数据共享和分发，从而增强系统的整体安全性和公平性。 <div>
<h2><a class="anchor" href="https://ethresear.ch#tldr-1" name="tldr-1"></a>tl;dr</h2>
<p>Ethereum’s design has moved away from state sharding; however, L2 architectures like zkSharding provide a unified protocol in which L2 dApps are composable yet scalable via state sharding, avoiding the need for state fragmentation emerging across distinct L2s. However, sharded systems are not without challenges. In particular, state sharding amplifies MEV exploitation and censorship problems that exist in non-sharded blockchains.</p>
<p>We propose a shardDAG architecture for state sharded blockchains or multi-chain systems, combining protocol rules, rewards and penalties that constrain transaction exploitation [<a class="inline-onebox" href="https://arxiv.org/abs/1904.05234" rel="noopener nofollow ugc">[1904.05234] Flash Boys 2.0: Frontrunning, Transaction Reordering, and Consensus Instability in Decentralized Exchanges</a>] and external influences like regulatory censorship  [<a href="https://www.mevwatch.info/" rel="noopener nofollow ugc">https://www.mevwatch.info/</a>, <a class="inline-onebox" href="https://home.treasury.gov/news/press-releases/jy0916" rel="noopener nofollow ugc">U.S. Treasury Sanctions Notorious Virtual Currency Mixer Tornado Cash | U.S. Department of the Treasury</a>]. Constraints on exploitation and censorship are achieved using a DAG architecture that links shard blocks to each other. The DAG provides an enforceable order in which cross-shard transactions must be processed by each shard, thereby constraining manipulation of transaction processing order.</p>
<h2><a class="anchor" href="https://ethresear.ch#motivation-2" name="motivation-2"></a>Motivation</h2>
<p>State sharded blockchains inherit magnified MEV exploitation and censorship problems that exist in non-sharded blockchains because transaction completion can require block proposers in many distinct shards, and each block proposer could exploit or censor transactions. Further, more severe transaction exploits are possible via inserting other exploitative transactions in intermediate blocks that occur between starting and finishing transaction processing.</p>
<p>To understand this, the example below demonstrates a simple exploit scenario.</p>
<p><em><strong>Exploit Example</strong></em></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/b/db208f14e2f3355a7d1d888bbc8b90e969102b97.jpeg" title="BasicExploit"><img alt="BasicExploit" height="445" src="https://ethresear.ch/uploads/default/optimized/3X/d/b/db208f14e2f3355a7d1d888bbc8b90e969102b97_2_690x445.jpeg" width="690" /></a></div><br />
Figure 1: Two shard chains. Blocks 0 and 1 of shard <em>A</em> each contain cross-shard transactions <em>t</em> and <em>u</em>  respectively, whose destinations are shard <em>B</em>. Suppose <em>t</em> can be exploited if in shard <em>B</em> <em>u</em> is processed earlier than <em>t</em>. Then the system is dangerous for <em>t</em>’s user without enforceable ordering rules that ensure <em>t</em> must be processed before <em>u</em> in shard <em>B</em>.<p></p>
<h2><a class="anchor" href="https://ethresear.ch#why-cross-shard-transaction-data-availability-matters-3" name="why-cross-shard-transaction-data-availability-matters-3"></a>Why Cross-Shard Transaction Data Availability Matters</h2>
<p>Punishment for censoring a cross-shard transaction (CST), or processing in an incorrect, exploitative order can only be enforced provided that</p>
<p>i) It can be established that all the required data was available to the shard, and</p>
<p>ii) The shard subsequently failed to process the data correctly.</p>
<p>Therefore, a mechanism is required for establishing <em>verifiable</em> cross-shard (or cross-chain, or cross-rollup) transaction data availability. The broad steps in achieving this are illustrated in Fig. 2. Preventing exploitation requires enforceable rules for ordering the processing of transactions and CSTs; however, enforcing processing order requires that each shard receives the CSTs that it is required to process. To be able to receive CSTs, that CST data must be available. Thus, constraining exploitation rests upon ensuring CST data availability.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/b/7bee56e632f92d068ac45ccb5bf63a201b085725.jpeg" title="ShardDAG StrategySteps"><img alt="ShardDAG StrategySteps" height="154" src="https://ethresear.ch/uploads/default/optimized/3X/7/b/7bee56e632f92d068ac45ccb5bf63a201b085725_2_690x154.jpeg" width="690" /></a></div><br />
Figure 2: Goal: ShardDAG ordering aims to constrain manipulation, exploitation and censorship of transactions and cross-shard transactions. Step 3: These constraints require enforceable protocol rules for ordering the processing of transactions and cross-shard transactions. Step 2: Fairly enforcing ordering rules requires on-chain acknowledgement of receipt of cross-shard transactions. Step 1: Receipt of cross-shard transactions requires cross-shard transaction data availability.<p></p>
<h2><a class="anchor" href="https://ethresear.ch#step-3-a-preview-how-dags-provide-order-4" name="step-3-a-preview-how-dags-provide-order-4"></a>Step 3. A Preview: How DAGs Provide Order</h2>
<p>Our solution to the transaction and cross-shard transaction ordering problem involves linking shard chains into a shard directed acyclic graph or shardDAG, and then ordering processing according to the partial order specified within shard block subgraphs.</p>
<p>ShardDAG ordering is previewed in Fig. 3. The distinct shard chains are connected to form a shardDAG, providing an enforceable ordering of cross-shard transactions amongst the shard chains. Unlike in Fig. 1, in Fig. 3’s shardDAG, an exploitative CST in a later block cannot be processed before a CST in an earlier block.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/8/48bb4910acfa00365d8e0b97b45be64b0e32e732.jpeg" title="ShardDAG NonOverloaded Ordering"><img alt="ShardDAG NonOverloaded Ordering" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/4/8/48bb4910acfa00365d8e0b97b45be64b0e32e732_2_193x500.jpeg" width="193" /></a></div><br />
Figure 3: Unlike the distinct shard chains in Fig. 1, the shardDAG (top) depicted here defines a partial ordering of shard blocks that fall under any particular block (here shard B block 2) and the cross-shard transactions that the blocks contain. (Middle) A Hasse diagram can be constructed to visualise a partial ordering of the shard blocks (for clarity lines connecting blocks have not been included). The shardDAG is topologically sorted (bottom) to produce a block containing an ordered set of transactions and CSTs. In general many topological sorts are possible, the block builder selects one, likely based on MEV.<p></p>
<h2><a class="anchor" href="https://ethresear.ch#a-sharddag-for-data-availability-5" name="a-sharddag-for-data-availability-5"></a>A ShardDAG for Data Availability</h2>
<p>To establish cross-shard transaction data availability, the simple set of shard chains in Fig. 1 is extended to become a shardDAG that is crafted to incentivize data sharing. In this system all validators participate in a synchronization chain which aggregates and finalises state updates from shards that are each operated by distinct subsets of the total validator set. Transaction and CST processing is performed within shards only, hence the synchronization chain is not a processing bottleneck. Here the details of the synchronization chain are restricted to its involvement in the shardDAG—the broader function of the synchronization chain in the sharded system is beyond the scope of this post.</p>
<p>To form a shardDAG, shard blocks include links to other shard blocks in the form of:</p>
<ul>
<li><strong>a hash to the previous shard block in the same shard, as in a typical blockchain,</strong></li>
<li><strong>a set of hashes to other shards blocks in other shards,</strong></li>
<li><strong>a hash to a (valid) synchronization block, equal to or later than the most recent synchronization block already used by prior shard blocks that are included in the subgraph.</strong></li>
</ul>
<p>The formation of a shardDAG is illustrated in Fig. 4, where for clarity only edges in the subgraph of the white block are shown. The thick arrows are the white block’s hashes to other blocks.</p>
<p>The following is a central concept in the function of the shardDAG.</p>
<p><strong>When a shard <span class="math">A</span> creates a shard block that includes the hash <span class="math">h</span> of another shard block or synchronization block, this inclusion acts as an acknowledgement that shard <span class="math">A</span> has received the block headers and outboxes of cross-shard transactions for <span class="math">h</span> and <span class="math">h</span><em>’s</em> entire subgraph in the shardDAG.</strong></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/2/b2ba419c59a01907d08a616eb2b464634114c1d3.jpeg" title="SubGraphLowDetail"><img alt="SubGraphLowDetail" height="469" src="https://ethresear.ch/uploads/default/optimized/3X/b/2/b2ba419c59a01907d08a616eb2b464634114c1d3_2_690x469.jpeg" width="690" /></a></div><br />
Figure 4: Illustration of the white block’s subgraph in the shard DAG. The white block’s header contains a list of hashes to other shard blocks (thick white arrows), and well as a single hash to a synchronization block (thick grey arrow). Thin grey edges trace the subgraph of the white block, beyond the blocks explicitly included in its header.<p></p>
<h2><a class="anchor" href="https://ethresear.ch#step-2-enforcing-cst-receipt-6" name="step-2-enforcing-cst-receipt-6"></a>Step 2. Enforcing CST Receipt</h2>
<h3><a class="anchor" href="https://ethresear.ch#enforcing-cst-receipt-via-shard-chains-7" name="enforcing-cst-receipt-via-shard-chains-7"></a>Enforcing CST Receipt via Shard Chains</h3>
<p>To enforce shards to continually acknowledge receipt of new shard block data, the protocol specifies conditions on block validity. Suppose we have a shard block <span class="math">b_i</span> and <span class="math">b_i</span><em>’s</em> prior shard block <span class="math">b_{i-1}</span> in the same shard as <span class="math">b_i</span>.</p>
<ul>
<li><strong>[PARENT CONDITION]: For <span class="math">b_i</span> to be a valid shard block, the graph difference of <span class="math">b_i</span><em>’s</em> subgraph minus <span class="math">b_{i-1}</span><em>’s</em> subgraph must contain shard blocks created by more than <span class="math">F&gt;1</span> shards, where <span class="math">F</span> is a system parameter controlling the branching of the DAG.</strong></li>
</ul>
<p><em>Example:</em></p>
<p><em>In Fig. 4, the subgraph of the white shard A block only contains two blocks that are not in the subgraph of the previous shard A block, i.e. the white block itself, and the middle shard B block. If in this example F=1, then the white block is valid; however, if F&gt;1 then the white block is invalid.</em></p>
<h3><a class="anchor" href="https://ethresear.ch#enforcing-cst-receipt-via-the-synchronization-chain-8" name="enforcing-cst-receipt-via-the-synchronization-chain-8"></a>Enforcing CST Receipt Via the Synchronization Chain</h3>
<p>The parent condition enforces receipt of CSTs, but does not guarantee that each CST reaches its destination so that transactions complete. Without additional rules it is possible (though unlikely) for sets of shards to create shard blocks whose subgraphs do not span all shards and therefore do not acknowledge receipt of CSTs from all shards, as illustrated in Fig. 6.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/9/c95bce910ccff1c8dbd9ffbaa913b546c5db043d.jpeg" title="consensusParentCondition"><img alt="consensusParentCondition" height="444" src="https://ethresear.ch/uploads/default/optimized/3X/c/9/c95bce910ccff1c8dbd9ffbaa913b546c5db043d_2_690x444.jpeg" width="690" /></a></div><br />
Figure 6: Despite the parent condition for valid shard blocks, it is possible (though unlikely) for shard subgraphs to not acknowledge receipt of CSTs from some other shards via shard block edges, indicated by the vertical dashed line. However, the synchronization parent condition eventually forces all shards to acknowledge all CSTs via synchronization block edges. Here the synchronization parent condition forces shard 4 to acknowledge receipt of the red CST (via the red edges) and therefore process it, because the dashed blue edge exceeds the limit (here <em>S</em>=2) of consecutive synchronization block hashes. For clarity only the subset of synchronization blocks edges that are relevant to illustrating the above point are shown.<p></p>
<p>This is unlikely to occur in the shardDAG; however, the synchronization chain is used to ensure that it <em>cannot</em> occur via a further block validity condition:</p>
<ul>
<li><strong>[SYNCHRONIZATION PARENT CONDITION]: A valid shard block <em>b</em> cannot have more than <span class="math">S</span> prior blocks from the same shard using the same synchronization block hash.</strong></li>
</ul>
<p>The value of <span class="math">S</span> should be chosen depending on the ratio of rates of synchronization block to shard block creation. It is expected that synchronization blocks will be produced at a slower rate compared to shard blocks.</p>
<p>A malicious shard can only produce <span class="math">S</span> shard blocks before being forced to acknowledge receipt of new shard blocks via the synchronization chain. In Fig. 5, the red CST shard block will eventually be included in a synchronization block, in a worst case scenario waiting until a shard 1 validator becomes the synchronization block proposer. Thus, eventually all shards will acknowledge receiving the red CST, including the red CST’s destination shard, as indicated by the red arrows. The dashed blue arrow indicates that shard 4 block 3 would be invalid if it used this hash because more than <span class="math">S</span> (here 2) consecutive shard blocks would hash to the same synchronization block.</p>
<p>In this way, economically motivated validators (and especially synchronization block proposers) are motivated to share data so that finality can be reached and economic rewards can be distributed.</p>
<h2><a class="anchor" href="https://ethresear.ch#step-1-enforcing-cst-data-availability-via-dag-edges-between-shards-9" name="step-1-enforcing-cst-data-availability-via-dag-edges-between-shards-9"></a>Step 1. Enforcing CST Data Availability Via DAG Edges Between Shards</h2>
<p>While the parent, and synchronization parent conditions force shards to acknowledge receipt of data, these rules do not force shards to <em>distribute</em> shard block data and establish data availability. Thus, the protocol specifies a rule on shard block finality to align data availability with economic incentives.</p>
<ul>
<li><strong>[CHILD CONDITION]: For a shard block <span class="math">b</span> to be finalised within the synchronization chain, within the subgraph of any synchronization block, <span class="math">b</span> must have child shard blocks created by more than <span class="math">F</span> shards.</strong></li>
</ul>
<p>When a block satisfies the child condition its CSTs have been acknowledged as received by more than <em>F</em> other shards and the shard has therefore distributed its CST data.</p>
<p>The child condition is illustrated in Fig. 5. For a shard block to acquire child shard blocks, other (honest) shards must first receive its subgraph data. Thus, shards are economically incentivised to possess and distribute data in their subgraphs.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/b/1b0c569adc1421ca246f774a9b776cda2aaa6465.jpeg" title="ChildConditionEthREsearch"><img alt="ChildConditionEthREsearch" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/1/b/1b0c569adc1421ca246f774a9b776cda2aaa6465_2_688x500.jpeg" width="688" /></a></div><br />
Figure 5: Illustration of the child condition for the subgraph of the upper left synchronization block. In this example <em>F</em>=2. The white block in finalised because it has child shard blocks from three shards, indicated by thick white arrows. In contrast, the bottom right shard block is not finalised because it only has one child shard block indicated by the thick grey arrow.<p></p>
<h2><a class="anchor" href="https://ethresear.ch#step-3-an-enforceable-dag-partial-order-of-transaction-and-cst-processing-10" name="step-3-an-enforceable-dag-partial-order-of-transaction-and-cst-processing-10"></a>Step 3. An Enforceable DAG Partial Order of Transaction and CST Processing</h2>
<p>The shardDAG provides a verifiable, enforceable ordering of transactions and CSTs, which constrains exploits and guarantees (eventual) transaction processing. Transactions and cross-shard transactions must be processed in an order consistent with the partial order of the shard blocks that they are each created in.</p>
<p>Suppose that shard <em>B</em> creates a new shard block <em>b</em>. As illustrated in Fig.3, the the steps involved in ordering the processing of CSTs and transactions are:</p>
<ol>
<li>First <span class="math">b</span><em>’s</em> hashes (DAG edges) to other shard blocks and a synchronization block are chosen. Hashes are only chosen if corresponding subgraph CST data is available, otherwise correct ordering cannot be known and penalties may ensue.</li>
<li>The protocol rules described earlier require that the validator creating and proposing <span class="math">b</span> has all the CST data from <span class="math">b</span><em>’s</em> subgraph, call these <span class="math">T</span>. The set of pending CSTs <span class="math">P</span> whose destination is shard <span class="math">B</span>, and which have not already been processed in an existing shard <span class="math">B</span> block are extracted from <span class="math">T</span> and any new transactions are added to <span class="math">P</span>.</li>
<li>The set of pending transactions and CSTs, <span class="math">P</span> are (partially) ordered according to the shardDAG ordering of the shard blocks that they were created in, retaining the order of multiple CSTs created within a single block. <span class="math">P</span> is topologically sorted to create a totally ordered set of transactions and CSTs.</li>
<li>Block size limits may constrain the number of transactions and cross-shard transactions included in a shard block. If this occurs, it is optional to introduce priority of transactions and CSTs as illustrated in Fig. 7, whereby block proposers select transactions and CSTs to include based on priority fees, and MEV. However, this comes at the cost of potentially allowing exploitative transaction insertion. Pending transactions and CSTs must be processed if allowed by block size limits; any unused block space must be too small to contain any unprocessed transaction or CST.</li>
<li>New transactions that do not fit into block processing can be included in a shard block’s outbox of CSTs. Such transactions enter the shardDAG for ordering and will therefore be processed in a later block along with other pending transactions and CSTs not included in <span class="math">b</span>.</li>
</ol>
<p>Validators should only sign a proposed block once they have verified that the block proposer has followed this protocol ordering. If an invalid ordering is used then the block is invalid and/or the signing validators are subjected to penalties. We reiterate that because ordering rules involve only on-chain data, data availability of CSTs and block headers enables any validator or node to verify correctness of ordering.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/0/9/09afd7f7cf81fd6dacfe88bcc2f4b6c7eab46c0f.jpeg" title="ShardDAG Overloaded Ordering Only"><img alt="ShardDAG Overloaded Ordering Only" height="407" src="https://ethresear.ch/uploads/default/optimized/3X/0/9/09afd7f7cf81fd6dacfe88bcc2f4b6c7eab46c0f_2_690x407.jpeg" width="690" /></a></div><br />
Figure 7: An extension of Fig.3 when shard B is overloaded and unprocessed transactions and CSTs exceed maximum block size (left). The block builder selects transactions and CSTs to remove from the topological sort for the new block (middle), so that the remaining transactions and CSTs do not exceed block size limits (right). Removed CSTs will be processed in later blocks. This removal of transactions and CSTs is expected to be based on priority fees and MEV. Unprocessed new transactions (b2’) may be included in an outbox as data so that they enter shardDAG ordering for processing in a later block, like b0 and b1 in earlier blocks, but these outboxed transactions are not processed in the current block.<p></p>
<h2><a class="anchor" href="https://ethresear.ch#summary-11" name="summary-11"></a>Summary</h2>
<p>In state-sharded blockchains, censorship and insertion of exploitative transactions part-way through transaction processing can be constrained by shardDAG transaction and CST ordering. These shardDAG constraints are derived from ordering, which enforces processing of earlier transactions and CSTs before later ones. ShardDAG ordering rests upon economic incentives that motivate validators to suitably participate in the shardDAG to receive block rewards and avoid penalties.</p>
<p>DAGs are a natural tool to be used in ordered systems. The shardDAG broadens the use of DAGs in blockchain, beyond their more common application in consensus mechanisms. The shardDAG has been presented here in a unified state sharded system, but the ideas can be applied to sets of distinct rollups or blockchains.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/sharddag-ordering-and-exploitation-in-sharded-blockchains/20203">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 01 Aug 2024 23:12:07 +0000</pubDate>
</item>
<item>
<title>Inclusion List Timing Constraints</title>
<link>https://ethresear.ch/t/inclusion-list-timing-constraints/20198</link>
<guid>https://ethresear.ch/t/inclusion-list-timing-constraints/20198</guid>
<content:encoded><![CDATA[
<div> 关键词：包括 "包括"、"时间限制"、"安全考虑"、"可行性"、"不同设计比较"

总结：

本文深入探讨了在以太坊中实现包括列表（ILs）所涉及的各种权衡，重点关注时间、安全性和可实施性方面的限制。ILs允许块构建者承诺在特定区块中包含交易，但其实现需要考虑到多种因素：

1. **时间限制**：每个以太坊“槽”固定为12秒，块构建者需在此时间窗口内准备区块。IL的创建和传播需要严格的时间控制，确保不会影响链的活性。

2. **安全性**：IL的存在要求诚实验证者能够识别并拒绝不符合要求的区块。这涉及到如何平衡验证器的责任和效率，避免DoS攻击的可能性。

3. **可行性**：IL的实现可能涉及多个参与方，包括构建者、验证者和执行者。需要定义明确的依赖关系，确保所有参与者都能履行其角色，同时考虑到网络的传播时间和节点验证时间。

4. **不同设计比较**：文章讨论了几种IL设计，如EIP-7547和FOCIL，每种设计都有其优势和局限性，特别是在时间、安全性和可实施性方面的权衡。

5. **内容争端**：IL可能与其他机制（如较短的槽时间、更高的gas限制、分布式验证技术或活跃验证服务）产生冲突，需要权衡以优化整个系统的性能和稳定性。

总的来说，实现有效的IL系统需要细致地规划和协调，确保所有关键组件在有限的时间框架内高效协同工作，同时保持系统的安全性和可靠性。 <div>
<p>Special thanks to <a class="mention" href="https://ethresear.ch/u/julian">@Julian</a>, <a class="mention" href="https://ethresear.ch/u/barnabe">@barnabe</a> and <a class="mention" href="https://ethresear.ch/u/manav2401">@manav2401</a> for the reviews</p>
<h2><a class="anchor" href="https://ethresear.ch#background-1" name="background-1"></a>Background</h2>
<p>Inclusion list have been an active topic since the <a href="https://notes.ethereum.org/@vbuterin/pbs_censorship_resistance" rel="noopener nofollow ugc">early</a> <a href="https://ethresear.ch/t/how-much-can-we-constrain-builders-without-bringing-back-heavy-burdens-to-proposers/13808">days</a>. <a href="https://notes.ethereum.org/@fradamt/forward-inclusion-lists" rel="noopener nofollow ugc">Various</a> <a href="https://notes.ethereum.org/@fradamt/H1TsYRfJc" rel="noopener nofollow ugc">designs</a> have emerged over time, each with inevitable trade-offs concerning <strong>What can be constrained within a single Ethereum slot?</strong>.<br />
This post explores these trade-offs from the perspectives of <strong>different actors</strong> involved in ILs and defines the dependencies required for each actor to fulfill their role in integrating ILs into the protocol. We will compare and contrast multiple designs, focusing on the limitations related to <strong>timing, security, and feasibility</strong>.</p>
<p>First, we will outline some definitions.</p>
<h2><a class="anchor" href="https://ethresear.ch#il-definitions-2" name="il-definitions-2"></a>IL Definitions</h2>
<p><strong>Slot Time</strong>: In the context of Ethereum, a slot refers to a fixed interval currently set at 12 seconds. During each slot, the proposer/builder proposes a block, attesters vote on the block, and an aggregator aggregates the votes. The proposer of subsequent slot includes aggregated votes in their block, and the cycle repeats. Today out-of-protocol builders have an ~8-second window to prepare for the next slot’s block. All actions are synchronized with these validator duty intervals, and <strong>IL should not extend the current slot time</strong>.</p>
<p><strong>Inclusion List:</strong> An inclusion list (IL) is a list of transactions that a block proposer commits to including in a block. Depends on the conditional vs unconditional constraint, if these transactions are not included in the block, then the block cannot be considered canonical, assuming honest attesters who will vote against the block. The IL consists of the following options and requirements.</p>
<ol>
<li><strong>Satisfactory Requirement</strong>:
<ul>
<li><strong>Conditional</strong>: The IL does not need to be satisfied if the target block is full.
<ul>
<li><strong>Forward-Looking</strong>: If the IL cannot be satisfied in the current target block, does it still apply to subsequent blocks? <a href="https://ethresear.ch/t/cumulative-non-expiring-inclusion-lists/16520">More in this post</a></li>
</ul>
</li>
<li><strong>Unconditional</strong>: The IL needs to be satisfied. This typically means the IL has its own gas limit.</li>
</ul>
</li>
<li><strong>Satisfactory Time</strong>:
<ul>
<li><strong>Same Slot IL</strong>: The IL is satisfied within the same slot, similar to users sending a transactions wanting to be included on chain. With sufficient base fee and tip, we can expect the transaction to be included the slot of. For example, an IL transaction for slot <code>n+1</code> is satisfied in slot <code>n+1</code>.</li>
<li><strong>Next Slot IL</strong>: The IL is satisfied in the subsequent slot with one slot delay. For example, an IL transaction for slot <code>n+1</code> is satisfied in slot <code>n+2</code>.</li>
</ul>
</li>
<li><strong>IL constructor</strong>: The actor responsible for preparing and broadcasting the IL to the network. This role can be fulfilled by a single entity (like a proposer) or by a committee where the protocol reaches consensus on individual ILs from its members. The consensus of IL may be reached by IL aggregate which represents IL committee’s vote.</li>
<li><strong>IL Gas Limit</strong>: IL gas limit has an implication on the size of IL which dirrectly affects the network propagation time and node’s verification time.</li>
<li><strong>IL Ordering In Block</strong>: When the IL becomes part of the block, the transactions may be required to be placed in a specific order. This order could be:
<ul>
<li><strong>Top of the Block</strong>: Transactions are placed at the beginning of the block.</li>
<li><strong>Anywhere in the Block</strong>: Transactions are placed anywhere within the block.</li>
<li><strong>Bottom of the Block</strong>: Transactions are placed at the end of the block.</li>
</ul>
</li>
<li><strong>Liveness Guarantee</strong> The IL must be made available to the block builder to avoid stalling the chain’s liveness. The delivery method of the IL to the builder varies based on the trust model. If a single person constructs the IL, stricter requirements may be necessary, such as additional attester validation along with the block.</li>
<li><strong>No Free DA</strong> An IL that has not been satisfied in execution cannot be part of the consensus, as it would grant free DA. Free DA has to be tightly coupled with consensus and should not be mistaken for free bandwidth or temporary data storage. While nodes can use a small amount of bandwidth or store temporary data with anti-dos measures in place, this should not be conflated with free DA.</li>
</ol>
<p><strong>Block Builder</strong>: The actor tasked with fulfilling the IL and broadcasting the resulting product (ie. a block that fulfills the IL) over the network. In the case of a solo validator, the block proposer serves as the block builder, and the product is the execution payload of the block. For a MEV-boost validator, the block builder handles the fulfillment, which returns the signed header to proposer, and the relay broadcasts the final block to the network. It is often the case that the block proposer cannot verify the satisfactory fulfillment of the IL when signing the header request. Relays have to verify the payload satisfies IL ahead of time or assume optimistic.</p>
<p><strong>IL Transaction Invalidation</strong>: Transactions in an IL may become <strong>not includable</strong> at the time of inclusion due to invalidations, such as an incorrect nonce or insufficient balance. This situation can arise under different conditions. For example, when multiple parties are involved in constructing their version of ILs, the transactions from each party might render each other not includable. Similarly, if one party constructs the IL while another party broadcasts the block at the same moment, there can also be invalidations, leading to mutual exclusion of the IL transactions and block transactions.</p>
<p><strong>Head Block</strong>: Often referred to as the parent block, the IL should be constructed on top of the chain’s head from the perspective of the node. The builder, responsible for constructing the block and satisfying the IL, should also build on top of the head block in order to make sure that block and inclusion list are aligned.<br />
<strong>Constraint</strong>: If an IL is built on head <code>a</code>, then to satisfy the IL, the builder’s block must also be built on top of head <code>a</code>.</p>
<h2><a class="anchor" href="https://ethresear.ch#il-timings-3" name="il-timings-3"></a>IL Timings</h2>
<p><strong>IL Preparation Time</strong>: This is the time required for a party to prepare the IL, which is constructed on top of the head block. The larger the IL may require longer time to prepare.</p>
<p><strong>IL Propagation Time</strong>: This is the time required for the IL to propagate across the network to other nodes. Factors influencing this time include the size of the IL, the number of ILs (committee size), and the network’s gossip rules.</p>
<p><strong>IL Verification Time</strong>: This is the time required to verify the IL. The IL must be valid, otherwise builders can get grieved. In some scenarios, attesters must verify the IL before considering the current slot block as the head (. In other cases, the proposer must verify the IL before proposing the next slot block. The point is that some parties must verify the IL beforehand, and it’s crucial to consider who is bearing this cost.</p>
<p><strong>Block Preparation Time</strong>: This is the time required to build an execution block. The block can be constructed either by the proposer or the builder. The IL’s satisfactory requirements must be met in the block. This means the block builder must verify the IL, parent block and ensure that the block satisfies IL requirements.</p>
<p><strong>Block Propagation Time</strong>: This is the time taken for a block to be transmitted across the network and received by all participants. It’s crucial that the block is received and verified by attesters promptly, as delays can lead to the block not being considered as the head of the chain, increasing the risk of reorg.</p>
<p><strong>Block Verification Time</strong>: This is the time taken for a node to verify the block and IL. The focus here is on execution verification time, as consensus verification is typically fast. A block must be verified as execution valid and meet the IL requirements before it can be considered the head of the chain.</p>
<p>Based on the timing definition provided, we can outline the following dependencies:</p>
<ul>
<li>The parent head block <code>n</code> must be released before attestation cut off. The difference is between start of the slot. Head release time = <span class="math">T_{HR}</span></li>
<li>The head block must be propagated to peers on time. Head propagation time = <span class="math">T_{HP}</span></li>
</ul>
<ul>
<li>The IL constructor must see and validate the parent head block before creating the IL. Head validator time = <span class="math">T_{HV}</span></li>
<li>The IL must be constructed and released using for example a local mem pool. IL construction time = <span class="math">T_{ILC}</span></li>
<li>The IL must propagate through the network to reach the builders. IL propagation time = <span class="math">T_{ILP}</span></li>
<li>The block builder needs to verify the IL before submitting a bid.  IL verification time = <span class="math">T_{ILV}</span>
<ul>
<li>This requirement may change in the context of slot auctions.</li>
</ul>
</li>
<li>The proposer must see the bids before submitting a block. Bid propagation time = <span class="math">T_{BP}</span></li>
<li>The attester must verify the block <code>n+1</code> before considering it as the head. We can reuse head verification time above.</li>
</ul>
<p>In short, we could summarize: A single Ethereum slot should not exceed the following durations, ensuring that the end-to-end IL is applied, and the block remains canonical on the chain: <span class="math">SLOT &gt;= T_{HR}+T_{HP}+2 * T_{HV}+T_{ILC}+T_{ILP}+T_{ILV}+T_{BP}</span></p>
<h1><a class="anchor" href="https://ethresear.ch#different-versions-of-il-4" name="different-versions-of-il-4"></a>Different versions of IL</h1>
<p>Different versions of IL have varying constraint trade-offs. Some examples taken from <a href="https://eips.ethereum.org/EIPS/eip-7547" rel="noopener nofollow ugc">EIP-7547</a> and <a href="https://ethresear.ch/t/fork-choice-enforced-inclusion-lists-focil-a-simple-committee-based-inclusion-list-proposal/19870">FOCIL</a>.</p>
<h4><a class="anchor" href="https://ethresear.ch#eip-7547-in-mev-boost-5" name="eip-7547-in-mev-boost-5"></a>EIP-7547 in MEV-Boost</h4>
<ul>
<li>The block builders for slot <code>n</code> constructs a block for slot <code>n</code> after verifying the block for slot <code>n-1</code>.</li>
<li>The block proposer of slot <code>n</code> constructs an IL for slot <code>n+1</code> after verifying the block for slot <code>n-1</code>.</li>
<li>The IL for slot <code>n+1</code> and the block for slot <code>n</code> may invalidate each other if they are sent by different parties.</li>
<li>The block proposer/builder of slot <code>n+1</code> requires the IL and the block for slot <code>n</code> to build a block.</li>
<li>The block proposer of slot <code>n+1</code> needs the IL and the block for slot <code>n</code> to build an IL.</li>
<li>Attesters for slot <code>n+1</code> need the IL and the block for slot <code>n</code> to attest to the block. The block for slot <code>n+1</code> must link to a valid IL <code>n+1</code>, or it cannot be canonical.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#eip-7547-in-epbs-eip-7732-6" name="eip-7547-in-epbs-eip-7732-6"></a>EIP-7547 in ePBS (EIP-7732)</h4>
<ul>
<li>The block proposer of slot <code>n</code> selects the builder’s bid of slot <code>n</code> after verifying the execution block for slot <code>n-1</code>.</li>
<li>The block proposer of slot <code>n</code> constructs an IL for slot <code>n+1</code> after verifying the execution block for slot <code>n-1</code>.</li>
<li>Since the bid commits to the transactions, the IL for slot <code>n+1</code> and the bid for slot <code>n</code> may conflict. This is different in slot auction.</li>
<li>The builder reveals the execution block at slot <code>n</code>’s 6-seconds mark.</li>
<li>Subsequent block builders require the execution block at slot <code>n</code> and the IL for slot <code>n+1</code> to place bids for slot <code>n+1</code>. This is different in slot auction.</li>
<li>Attesters for slot <code>n+2</code> verify that the execution block for slot <code>n+1</code> satisfies the IL and is valid. We gain an extra slot time for validation due to <a href="https://ethresear.ch/t/advantage-of-pipelining-consensus-and-execution-delayed-execution/19668">delayed execution property</a>.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#focil-in-mev-boost-ignoring-il-aggregation-step-7" name="focil-in-mev-boost-ignoring-il-aggregation-step-7"></a>FOCIL in MEV-Boost (Ignoring IL Aggregation Step)</h4>
<ul>
<li>The block builder of slot <code>n</code> constructs a block for slot <code>n</code> after verifying the block for slot <code>n-1</code>.</li>
<li>The IL committee builds the IL for slot <code>n</code> after verifying the block for slot <code>n-1</code>.</li>
<li>The IL committee for slot <code>n</code> releases the IL during slot <code>n-1</code>.</li>
<li>Attesters for slot <code>n</code> lock their view on the ILs.</li>
<li>The builder of slot <code>n</code> includes the IL transactions into the block for slot <code>n</code></li>
<li>At the start of slot <code>n</code>, the proposer requests the builder’s head, signs it, and broadcasts it.</li>
<li>Attesters for slot <code>n</code> verify that the block satisfies the IL committee’s requirements according to their locked view in slot <code>n-1</code>.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#focil-in-epbs-same-slot-version-8" name="focil-in-epbs-same-slot-version-8"></a>FOCIL in ePBS (Same Slot Version)</h4>
<ul>
<li>The block proposer of slot <code>n</code> selects the builder’s bid for slot <code>n</code> after verifying the execution block for slot <code>n-1</code>.</li>
<li>The IL committee for slot <code>n+1</code> constructs the IL for slot <code>n+1</code> after the builder reveals the execution block for slot <code>n</code>.</li>
<li>Builders for slot <code>n+1</code> verify the IL and make bids for slot <code>n+1</code>.</li>
<li>The block proposer of slot <code>n+1</code> selects the builder’s bid for slot <code>n+1</code>.</li>
<li>Attesters for slot <code>n+2</code> verify that the execution block for slot <code>n+1</code> satisfies the IL and is valid, providing close to an extra slot time due to delayed execution.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#focil-in-epbs-next-slot-version-9" name="focil-in-epbs-next-slot-version-9"></a>FOCIL in ePBS (Next Slot Version)</h4>
<ul>
<li>The block proposer of slot <code>n</code> selects the builder’s bid for slot <code>n</code> after verifying the execution block for slot <code>n-1</code>.</li>
<li>The IL committee for slot constructs the IL for slot <code>n+2</code> after the builder reveals the execution block for slot <code>n</code>.</li>
<li>Builders for slot <code>n+2</code> verify the IL and make bids for slot <code>n+2</code>.</li>
<li>The block proposer of slot <code>n+2</code> selects the builder’s bid for slot `n+2.</li>
<li>Attesters for slot <code>n+3</code> verify that the execution block for slot <code>n+2</code> satisfies the IL and is valid, providing close to an extra slot time due to delayed execution.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#il-contentions-10" name="il-contentions-10"></a>IL Contentions</h2>
<p>ILs may compete with initiatives as the following:</p>
<p><strong>Shorter Slot Time Contentions with IL</strong>: With shorter slot times, ILs may not be constructed and fulfilled on time. A proposer that cannot fulfill an IL results in a liveness fault. One way to address this is to extend the IL satisfactory rule to the next slot or to multiple subsequent slots, but this approach introduces risks of denial-of-service (DoS) attacks and more transaction invalidation concerns. There is a trade-off here.</p>
<p><strong>Higher Gas Limit Contentions with IL</strong>: With a higher block gas limit, it takes longer to verify the block, which reduces the time available to construct the IL after verifying the block. Additionally, with a higher IL gas limit, it takes longer to propagate and verify the IL, reducing the time available to fulfill the IL by building the block.</p>
<p><strong>DVT Contentions with IL</strong>: Distributed Validator Technology (DVT) requires more exchanges between validators before signing. This process includes beacon chain duties such as attesting, proposing, and submitting ILs. These additional exchanges require time, and there is a need to ensure that the IL, especially in more complex forms, does not make DVT operations impractical.</p>
<p><strong>AVS Contentions with IL</strong>: Active Validator Service (AVS) also require more actions from validators. The specific details depend on the AVS implementation, but generally, requiring more time from validators to perform certain tasks can create contention with fulfilling IL obligations.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/inclusion-list-timing-constraints/20198">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 01 Aug 2024 16:02:42 +0000</pubDate>
</item>
<item>
<title>Cross-rollup Synchronous Atomic Execution</title>
<link>https://ethresear.ch/t/cross-rollup-synchronous-atomic-execution/20193</link>
<guid>https://ethresear.ch/t/cross-rollup-synchronous-atomic-execution/20193</guid>
<content:encoded><![CDATA[
<div> 关键词：Radius、同步原子执行、共享分发器、数据可用性层、验证层

总结:
这篇文章详细阐述了Radius设计的跨rollup组合体同步原子执行解决方案。该方案旨在提升跨rollup交易的协同性和用户体验，通过引入共享分发器，允许不同rollup之间创建共享的序列层，为用户提供便捷服务。共享分发器负责协调和确保多个交易同时原子性执行，提高执行效率并减少延迟。

关键点包括：
1. **同步原子执行**：允许多个交易并发执行，以降低总延迟时间，提供即时的交易体验。
2. **安全与便捷**：通过快速执行组合交易并确保其原子性，用户可以更高效地规划后续操作，同时减少对网络关键参与者（如共享分发器和执行者）的信任需求。
3. **架构设计**：提出了一种基于共享分发器、数据可用性层和验证层的架构，确保交易的安全性和执行效率。
4. **角色与责任**：明确定义了各实体（如用户、共享分发器、执行者等）的角色和职责，确保系统稳定运行。
5. **实施与验证**：通过具体场景展示如何实现和验证同步原子执行过程，包括交易流程、智能合约功能、协调机制以及验证逻辑。

此解决方案通过优化跨rollup交易的执行流程，增强了区块链网络的整体性能和用户体验，同时确保了交易的安全性和可信度。 <div>
<ul>
<li><em>by Hankyung Ko(<a href="https://ethresear.ch/u/hankyungko">@HankyungKo</a>) and Chanyang Ju(<a href="https://ethresear.ch/u/wooju">@wooju</a>), Researcher at <a href="https://twitter.com/radius_xyz" rel="noopener nofollow ugc">Radius</a> . Thanks to</em> <a href="https://twitter.com/Hyunxukee" rel="noopener nofollow ugc"><em>Tariz</em></a> <em>and</em> <a href="https://twitter.com/ZeroKnight_eth" rel="noopener nofollow ugc"><em>AJ</em></a> <em>for reviewing this post.</em></li>
<li><em>Your feedback and opinions are highly valued.</em></li>
</ul>
<p><em>Radius has designed a synchronous atomic execution solution for cross-rollup composability. This development is driven by our commitment to support rollups seeking improved composability and enhanced user experience. We will enable rollups to create their own shared sequencing layer, offering this as a service to make it widely accessible. By doing so, we ensure that atomic execution of bundled transactions is coordinated effectively across participating rollups.</em></p>
<h1><a class="anchor" href="https://ethresear.ch#h-1-introduction-1" name="h-1-introduction-1"></a>1. Introduction</h1>
<hr />
<p>Synchronous atomic execution allows multiple transactions from different rollups to be executed simultaneously and atomically in an all-or-nothing manner, significantly reducing latency compared to sequential execution. A naive approach to executing multiple cross-rollup transactions require each transaction to be finalized sequentially on L1. For <span class="math">n</span> transactions, the total latency would be <span class="math">n</span> times the L1 finalization period. In contrast, <strong><code>synchronous atomic execution</code></strong> enables all transactions to be executed at the same time, significantly reducing latency.</p>
<p>While executing transactions simultaneously can reduce latency, it may raise concerns about security. For example, in a bundled transaction involving minting-and-burning across different rollups, there’s a risk that the burn could fail while the mint succeeds. To address this, we’ve designed our system to verify the atomicity of bundled transactions faster than the time it takes for block finalization. This approach ensures that security is maintained even with simultaneous execution. Our innovation improves composability across multiple rollups, providing a seamless, efficient, and secure user experience with real-time, all-or-nothing execution of cross-rollup transactions without delays.</p>
<p>To implement this convenient and secure solution, Radius introduces a shared sequencer for rollups to guarantee the atomic execution of bundled transactions. Users create bundled transactions that depend on transactions across multiple rollups, and the shared sequencer manages the sequencing of bundled transactions for successful execution.</p>
<blockquote>
<p><img alt=":point_right:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/point_right.png?v=12" title=":point_right:" width="20" /> It’s important to note that this shared sequencer is not a single entity controlled by Radius, but rather a set formed by aggregating existing sequencers from each rollup. A leader is selected from this set through a predefined process (<a href="https://docs.theradius.xyz/testnets/loggia-testnet-with-radius-avs/decentralized-sequencing" rel="noopener nofollow ugc">reference</a>) to manage sequencing.</p>
<p>To prevent potential power abuse by the shared sequencer, Radius employs decentralized sequencing techniques, including encrypted mempool (<a href="https://ethresear.ch/t/mev-resistant-zk-rollups-with-practical-vde-pvde/12677">PVDE</a> and <a href="https://ethresear.ch/t/radius-skde-enhancing-rollup-composability-with-trustless-sequencing/19185">SKDE</a>). The shared sequencer has two main functions: determining transaction order and enforcing transaction reverts to maintain bundle atomicity.</p>
<p>This article details on how our architecture addresses the second function, ensuring atomicity. Preventing potential abuse of the first function, transaction ordering, is also crucial. Radius addresses this concern through the encrypted mempool, ensuring that the shared sequencer cannot abuse its power regarding transaction ordering.</p>
</blockquote>
<h2><a class="anchor" href="https://ethresear.ch#h-1-requirements-of-the-synchronous-atomicity-solution-2" name="h-1-requirements-of-the-synchronous-atomicity-solution-2"></a>1) Requirements of the synchronous atomicity solution</h2>
<ul>
<li><strong><code>Convenience</code>:</strong> Users can achieve greater benefits by utilizing atomic execution of cross-rollup bundled transactions, without compromising security.
<ul>
<li><strong>Bundle transactions</strong>: Users can bundle and execute multiple transactions across different rollups as one.</li>
<li><strong>Atomic execution:</strong> The bundle transaction is guaranteed to execute simultaneously without failures. If a failure occurs, it is guaranteed to fail simultaneously.</li>
<li><strong>Fast execution for bundle while maintaining security</strong>: Transactions are guaranteed to be executed faster than sequential execution (where each transaction waits for the previous one to finish). This allows users to strategize their next transactions more quickly. Additionally, atomic execution is cryptographically verified before finalization, ensuring that the security of the transactions is maintained even though they are executed more quickly.</li>
</ul>
</li>
<li><strong><code>Security</code></strong>
<ul>
<li><strong>Minimized trust level</strong>: Aims to minimize the amount of trust users must place in key network participants like the shared sequencer and executors by:
<ul>
<li>Minimizing the number of parties that need to be trusted.</li>
<li>Minimizing the duration for which trust is necessary.</li>
<li>Ensuring early detection and verification of any malicious behavior by the shared sequencer and executors before blockchain finalization.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#h-2-main-idea-3" name="h-2-main-idea-3"></a>2) Main Idea</h2>
<p>We propose an architecture for synchronous atomic execution using the <code>shared sequencer</code>, <code>data availability (DA)</code>, and a <code>verification layer</code>.</p>
<h3><a class="anchor" href="https://ethresear.ch#why-shared-sequencer-4" name="why-shared-sequencer-4"></a>Why shared sequencer?</h3>
<ul>
<li>To satisfy the desired properties of convenience and security, it is necessary to handle bundled transactions across multiple rollups. Therefore, we propose a new entity called the shared sequencer, which is responsible for confirming the blocks of multiple rollups.</li>
<li>The shared sequencer receives a cross-chain bundle and determines the block order for atomic execution.</li>
<li>The shared sequencer is responsible to ensure the atomic execution of bundled transactions before confirming the block.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#new-responsibility-of-the-executor-5" name="new-responsibility-of-the-executor-5"></a>New responsibility of the executor</h3>
<ul>
<li><strong>The executor, in agreement with the shared sequencer, has an added constraint:</strong> it must execute the transaction list committed by the shared sequencer.</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/5/553162b8f7a0148ceedbd7bddbf19ce91d944f58.png" title="SAE_figure1"><img alt="SAE_figure1" height="130" src="https://ethresear.ch/uploads/default/optimized/3X/5/5/553162b8f7a0148ceedbd7bddbf19ce91d944f58_2_690x130.png" width="690" /></a></div><br />
[Figure 1] The definition of roles and responsibilities of shared sequencer and executors<p></p>
<h3><a class="anchor" href="https://ethresear.ch#conditions-for-synchronous-atomicity-6" name="conditions-for-synchronous-atomicity-6"></a>Conditions for synchronous atomicity</h3>
<ul>
<li>Synchronous atomicity for bundle transaction is achieved if the following two conditions are independently verified:
<ol>
<li>All bundled transaction in the block committed by the shared sequencer should be atomic.</li>
<li>The executor executes the same block as committed by the shared sequencer.</li>
</ol>
</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#h-3-our-contributions-7" name="h-3-our-contributions-7"></a>3) Our Contributions</h2>
<ul>
<li><strong>Designed a synchronous atomic execution solution</strong>:
<ul>
<li><strong>Security requirements definition</strong>: We have defined the security requirements for each entity involved in synchronous atomic execution, ensuring that all components operate securely and reliably.</li>
<li><strong>Architecture design</strong>: We have designed a robust architecture that ensures security and efficiency. This architecture includes:
<ul>
<li><strong>User’s bundle transaction</strong>: We defined the structure and format of bundle transactions that users can create. These bundled transactions enable users to execute multiple transactions across different rollups simultaneously, ensuring atomic execution.</li>
<li><strong>The bundler contract</strong>: We designed and implemented the bundler contract, which is responsible for handling and processing bundled transactions. This contract is called by the shared sequencer, and performs several critical functions:
<ul>
<li>Acts as a gateway smart contract for users to call the actual contracts they intend to execute.</li>
<li>Allows the shared sequencer to enforce transaction reverts to guarantee the atomicity of the bundle transactions.</li>
<li>Verifies the legitimacy of transactions initially created by the user.</li>
<li>Charges transaction fees to users.</li>
</ul>
</li>
<li><strong>Coordination process for the shared sequencer</strong>:  We developed a coordination process for the shared sequencer, which includes interaction with the full nodes (simulators) of each rollup. This process ensures that the shared sequencer can effectively manage and sequence transaction across multiple rollups, guaranteeing atomic execution.</li>
<li><strong>Verification logic</strong>: We defined the verification logic to ensure that all transactions within a bundle meet the defined security requirements before finalization.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Demonstrated the feasibility of the architecture:</strong>
<ul>
<li><strong>Implementation</strong>: We have implemented the entire architecture, demonstrating its feasibility and effectiveness. Our implementation includes all components of the synchronous atomic execution solution, from the user’s bundle transaction creation to the coordination process for the shared sequencer.
<ul>
<li>The user’s bundled transaction is signed using MetaMask.</li>
<li>Implemented on two Polygon CDKs:
<ul>
<li>Each Polygon CDK has an API that responds to the shared sequencer’s simulation requests.</li>
<li>Each Polygon CDK has a deployed Bundler contract.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Demo</strong>: The demo scenario involves transferring tokens from Rollup A to Rollup B. In this scenario, the bundled transaction consists of two operations: burning the wrapped token on Rollup A and mint it on Rollup B. This demonstrates the practical application of our solution. (<a href="https://x.com/radius_xyz/status/1809120936270123468" rel="noopener nofollow ugc">Check out our Demo here!</a>)</li>
</ul>
</li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#h-2-definition-8" name="h-2-definition-8"></a>2. Definition</h1>
<hr />
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/0/b/0bec5c1463e929939d860e125c34e05cbc6b4c34.jpeg" title="SAE_figure2"><img alt="SAE_figure2" height="243" src="https://ethresear.ch/uploads/default/optimized/3X/0/b/0bec5c1463e929939d860e125c34e05cbc6b4c34_2_690x243.jpeg" width="690" /></a></div><br />
[Figure 2] Overview of transaction flow<p></p>
<blockquote>
<p><img alt=":pushpin:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/pushpin.png?v=12" title=":pushpin:" width="20" /> <strong>The proposed architecture is based on the following assumptions.</strong></p>
<ul>
<li><strong>Scenario</strong>
<ul>
<li>Each Bundle Tx consists of two transactions: a <strong>Burn</strong> transaction and a <strong>Mint</strong> transaction of ERC20 contract (rToken), occurring on different chains (inspired by Hyperlane bridge scenario).</li>
</ul>
</li>
<li><strong>Rollups</strong>
<ul>
<li>Each rollup has a simulation API implemented.</li>
<li>The <code>Radius’ Bundler contract</code> is deployed on each rollup.</li>
<li>The ERC20 rToken contract is also implemented on each rollup.</li>
<li>The execute function of the Radius’ Bundler contract is accessible only by whitelisted <code>shared sequencers</code>.</li>
<li>The ERC20 contract (rToken) grants burn and mint access rights to the Radius’ Bundler contract.</li>
</ul>
</li>
<li><strong>Incentives and Penalties</strong> (Future work)
<ul>
<li>There are sufficient incentives for correct behavior and penalties for incorrect behavior for the <code>shared sequencer</code> and <code>Executor</code>.</li>
</ul>
</li>
</ul>
</blockquote>
<h2><a class="anchor" href="https://ethresear.ch#h-1-operational-roles-and-security-requirements-of-entities-9" name="h-1-operational-roles-and-security-requirements-of-entities-9"></a>1) Operational Roles and Security Requirements of Entities</h2>
<p>In this section, we define the correct behavior and adversarial behavior of each entity in the architecture. The adversarial behaviors defined here will be analyzed in Section 4.</p>
<ul>
<li><strong><code>User</code>:</strong> The entity that generates cross-rollup bundled transactions and sends them to the shared sequencer for atomic execution.
<ul>
<li><strong>Adversarial behaviors</strong>
<ul>
<li>Creates invalid Bundle Tx:
<ul>
<li>The value of the BURN Tx and the MINT Tx do not match.</li>
<li>Insufficient account balance for the tokens intended to be burned.</li>
<li>Lacks the ability to pay the gas fee required for executing the transaction on at least one chain.</li>
<li>Incorrectly signs the Bundle Tx.</li>
</ul>
</li>
<li>Calls the MINT function without the shared sequencer’s assistance:
<ul>
<li>Attempts to execute the MINT Tx without creating a Bundle Tx.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong><code>Shared sequencer</code>:</strong> The entity responsible for receiving bundled transactions from users, creating and submitting blocks for multiple rollups, and ensuring the atomic execution of bundled transactions.
<ul>
<li><strong>Adversarial behaviors</strong>
<ul>
<li>Calls bundler contract without user’s consent.</li>
<li>Fails to verify whether the Bundle Tx is executed atomically across all rollups.</li>
<li>Forces the valid Bundle Tx to revert unnecessarily.</li>
<li>Sends a different transaction list to the executor than the one committed to the DA after confirming the block.</li>
</ul>
</li>
</ul>
</li>
<li><strong><code>Executor</code>:</strong> The entity specific to each rollup that executes the transaction list determined by the shared sequencer and uploads the resulting blocks to the Data Availability layer.
<ul>
<li><strong>Adversarial behaviors</strong>
<ul>
<li>Does not execute the transaction list as confirmed and provided by the shared sequencer.</li>
</ul>
</li>
</ul>
</li>
<li><strong><code>Shared Prover</code>:</strong> The entity that generates zero-knowledge proofs to validate the atomic execution of bundled transactions across different chains based on data from the Data Availability layer.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#h-2-additional-components-10" name="h-2-additional-components-10"></a>2) Additional Components</h2>
<ul>
<li><strong><code>Simulator</code>:</strong> The simulator refers to the full node of each rollup that the shared sequencer communicates with to validate the atomicity of bundled transactions before committing the block. This entity could be the same as the executor mentioned above.</li>
<li><strong><code>Data Availability Layer (DA)</code>:</strong> The DA is a layer for storing data committed by the shared sequencer and executor to prove their honesty. The shared prover uses this information to verify the honesty of both entities.
<ul>
<li>Given a reliable <code>DA</code>, if the shared sequencer and executor each commit the minimum necessary information for the verification of synchronous atomicity to the DA, it can be quickly <code>verified</code> based on that information.</li>
</ul>
</li>
<li><strong><code>Verification layer</code>:</strong> The verification layer is responsible for verifying the proofs generated by the shared prover and assisting with the appropriate actions if verification fails. This layer can either be part of the settlement layer or a dedicated layer focused solely on verification.</li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#h-3-synchronous-atomic-execution-architecture-11" name="h-3-synchronous-atomic-execution-architecture-11"></a>3. Synchronous atomic execution architecture</h1>
<hr />
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/a/ea8cbd6739d8f45ec741d82a13f6f55f94907b38.jpeg" title="SAE_figure3"><img alt="SAE_figure3" height="349" src="https://ethresear.ch/uploads/default/optimized/3X/e/a/ea8cbd6739d8f45ec741d82a13f6f55f94907b38_2_690x349.jpeg" width="690" /></a></div><br />
[Figure 3] The process of synchronous atomic execution architecture<p></p>
<p>The architecture of Radius’s synchronous atomic execution ensures that bundled transactions are executed in an all-or-nothing manner within the same cycle, coordinated by the shared sequencer. Initially, the shared sequencer’s coordination is trusted optimistically, allowing each transaction to be executed independently on its respective rollup. Subsequently, the atomicity of these transactions is verified before the rollup blocks are finalized on L1.</p>
<p>It can be divided into three main components: <strong>the bundler contract</strong>, <strong>the coordination process</strong>, and <strong>the verification process</strong>. This section will describe each of these components in detail.</p>
<h2><a class="anchor" href="https://ethresear.ch#h-1-smart-contract-for-bundle-transaction-radiuss-bundler-contract-12" name="h-1-smart-contract-for-bundle-transaction-radiuss-bundler-contract-12"></a>1) Smart <strong>Contract for Bundle Transaction (Radius’s Bundler contract)</strong></h2>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/0/b/0bec5c1463e929939d860e125c34e05cbc6b4c34.jpeg" title="SAE_figure2"><img alt="SAE_figure2" height="243" src="https://ethresear.ch/uploads/default/optimized/3X/0/b/0bec5c1463e929939d860e125c34e05cbc6b4c34_2_690x243.jpeg" width="690" /></a></div><br />
[Figure 4] Overview of transaction flow<p></p>
<p>We introduce a new smart contract called <code>Radius’s bundler contract</code>, designed to handle and process the users’ bundled transactions. It acts as a gateway to execute the users’ intended contracts.</p>
<p>For example, as shown in the figure, suppose a user creates a bundled transaction that includes calling the Burn function of the rToken contract on Rollup A and the Mint function of the rToken contract on Rollup B. The shared sequencer receives this bundle and wraps it into a transaction that calls the Radius’s bundler contract. Each rollup then processes the transaction through a series of verification via the Radius contract, ultimately executing the user’s intended contract calls.</p>
<h3><a class="anchor" href="https://ethresear.ch#key-features-of-the-bundler-contract-13" name="key-features-of-the-bundler-contract-13"></a>Key features of the Bundler contract</h3>
<ul>
<li>Acts as a gateway smart contract for users to call the actual contracts they intend to execute.</li>
<li>Allows the shared sequencer to enforce transaction reverts to guarantee the atomicity of the bundle transactions.</li>
<li>Verifies the legitimacy of transactions initially created by the user.</li>
<li>Charges transaction fees to users.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#how-is-the-bundler-contract-implemented-14" name="how-is-the-bundler-contract-implemented-14"></a>How is the Bundler contract implemented?</h3>
<p>The Bundler contract includes the following functions:</p>
<ul>
<li><code>execute</code>: Called by the shared sequencer, this function executes the user’s transaction after a series of verifications.</li>
<li><code>deposit</code>: Allows users to deposit transaction fees in advance.</li>
<li><code>withdraw</code>: Allows users to withdraw their deposited funds.</li>
<li><code>addWhitelist</code>: Adds a sequencer to the whitelist.</li>
<li><code>removeWhitelist</code>: Removes a specific sequencer from the whitelist</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#how-is-the-execute-function-implemented-15" name="how-is-the-execute-function-implemented-15"></a>How is the <code>execute</code> function implemented?</h3>
<blockquote>
<p><img alt=":pushpin:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/pushpin.png?v=12" title=":pushpin:" width="20" /> The input parameters for the execute function are as follows:</p>
<ul>
<li><code>from</code>: User address</li>
<li><code>bundle_tx_list</code>: Information of all transactions within the Bundle Tx</li>
<li><code>index</code>: Current transaction’s index within the <code>bundle_tx_list</code></li>
<li><code>bundle_tx_signature</code>: User’s signature for the Bundle Tx</li>
<li><code>revert_flag</code>: Flag for enforcing revert
<ul>
<li>The sequencer includes a <strong>“revert_flag”</strong> in the data, which is set by the sequencer to forcibly revert the user’s transaction. If this value is set to true, the Bundler contract will revert the user’s transaction. This mechanism is designed to ensure the atomicity of the transactions defined in the bundle, preventing the execution of the remaining transactions if even one included in the bundle fails to execute.</li>
</ul>
</li>
</ul>
</blockquote>
<ol>
<li>Access control:
<ul>
<li>Verify that the call is made by a shared sequencer listed in the whitelist.</li>
</ul>
</li>
<li>Check <code>revert_flag</code>:
<ul>
<li>If <code>revert_flag</code> == true, forcibly reverts the transaction.</li>
</ul>
</li>
<li>Verify user’s transaction:
<ul>
<li>Decode the transaction’s data field.</li>
<li>Ensure the user’s deposit is greater than the transaction fee.</li>
<li>Verify the user’s bundled transaction signature.</li>
<li>Check the Bundle Transaction validity
<ul>
<li>(In this scenario) Verify that the values to be minted and burned are identical.</li>
</ul>
</li>
</ul>
</li>
<li>Deduct transaction fee from user’s deposit (Exception handling required for early reverts):
<ul>
<li>Transfer the transaction fee to the shared sequencer from the contract’s deposited assets.</li>
<li>Deduct the transaction fee from the user’s deposit.</li>
</ul>
</li>
<li>Execute user’s intended contract:
<ul>
<li>Call the contract that the user intended to execute.</li>
</ul>
</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#h-2-coordination-process-for-the-shared-sequencer-16" name="h-2-coordination-process-for-the-shared-sequencer-16"></a>2) Coordination process for the shared sequencer</h2>
<p>Radius’s shared sequencing technique separates the roles of sequencing and execution. The shared sequencer is responsible for deciding the block that atomically executes the user’s bundle transactions, while the block is built by each rollup’s executor. Therefore, if it can be ensured that the shared sequencer has coordinated the atomic execution of the bundle transactions and the executor has executed the block as determined by the shared sequencer, synchronous atomic execution is achieved.</p>
<p>Coordination involves requesting simulations to the full nodes of each rollup for the respective transaction lists, collecting the simulated results, and, if some transactions within the bundle need to be reverted to maintain atomicity, forcibly reverting the remaining transactions to produce a transaction list that will be executed atomically. In other words, if the simulation results of the two transaction defined in the bundle are not the same (i.e., one is a revert and the other is a success), the transaction that yields a successful result is modified by setting its <code>revert_flag</code> to 1 to forcibly revert it. The transaction list is then updated with the modified transaction.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/d/4d77403de3cd9cacd9e96500481eda982fd6f53f.jpeg" title="SAE_figure5"><img alt="SAE_figure5" height="493" src="https://ethresear.ch/uploads/default/optimized/3X/4/d/4d77403de3cd9cacd9e96500481eda982fd6f53f_2_690x493.jpeg" width="690" /></a></div><br />
[Figure 5] Example of a Simulation Process<p></p>
<p>After finalizing the block, the shared sequencer commits to it. Later, the block information committed by the shared sequencer can be compared with the block executed by the executor to verify that the executor executed the block as agreed. The atomicity of the bundle transactions can be verified by examining the receipt committed by the executor after building the block, which shows the success status of each transaction and ensures that the shared sequencer did not forcibly revert all transactions. These two processes can be verified off-chain using ZKP systems such as RiscZero or SP1. This process is detailed in section 3.3.</p>
<h2><a class="anchor" href="https://ethresear.ch#h-3-verification-process-for-the-shared-prover-17" name="h-3-verification-process-for-the-shared-prover-17"></a>3) Verification process for the shared prover</h2>
<p>We design a zk prover called  “Shared Prover” which allows us to verify that the shared sequencer and executor acted in accordance with the protocol’s intentions. The shared prover generates proof for the <strong>atomicity of the bundle transactions</strong> and their <strong>valid execution result</strong> according to the commitment.</p>
<p>We leverage the DA layer to facilitate the sharing of the sequencer’s commitments and execution data across different chains. Utilizing the DA layer for data storage offers enhanced transparency and accessibility. Based on the DA (Data Availability) data, it can be confirmed that the user’s bundle transactions were executed atomically, and the validity of the execution result on different chains can be verified.</p>
<p>The shared sequencer communicates with simulators to finalize the the list of transactions to be performed while ensuring atomicity, and the executor processes this list and then it uploads the resulting data to the DA layer.</p>
<h3><a class="anchor" href="https://ethresear.ch#information-stored-in-the-da-18" name="information-stored-in-the-da-18"></a>Information stored in the DA</h3>
<ul>
<li>
<p><strong>Shared sequencer’s transactions list commitment</strong></p>
<p>To settle transaction list, shared sequencer commits the following data with signature to the DA:</p>
<ul>
<li><code>chain ID</code></li>
<li><code>block height</code></li>
<li><code>transaction MPT root</code></li>
<li><code>bundle transaction list</code></li>
</ul>
</li>
<li>
<p><strong>Executed Block data</strong></p>
<p>After a block is executed on each chain, the entity who executed the block uploads the results to the DA.</p>
</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#atomicity-proofs-shared-sequencers-honesty-19" name="atomicity-proofs-shared-sequencers-honesty-19"></a>Atomicity proofs (Shared sequencer’s honesty)</h3>
<p>The shared prover retrieves the block execution results stored in the DA by the executor. In the atomicity proof, the following aspects are verified using zero-knowledge proofs:</p>
<ul>
<li>
<p><strong>All-or-Nothing execution</strong></p>
<ul>
<li>
<p>The receipt status for the bundle transactions must have the same value.</p>
<p><span class="math">assert! (\text{receipt}(\text{tx}_A).\text{status} = \text{receipt}(\text{tx}_B).\text{status})</span></p>
</li>
</ul>
</li>
<li>
<p><strong>Prevention of arbitrary manipulation of the shared sequencer’s revert flag</strong></p>
<ul>
<li>
<p>There is a potential attack where a valid bundled transaction is reverted entirely, collecting fees from the user without executing the transaction. To prevent this, the atomicity proof checks that not all revert flags for the transactions are set to 1.</p>
</li>
<li>
<p>Therefore, revert flags cannot be set to 1 simultaneously.</p>
<p><span class="math">\text{assert!}~(\text{tx}_A.\text{revert_flag} \times \text{tx}_B.\text{revert_flag} = 0)</span></p>
</li>
</ul>
</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#proof-of-rollup-executors-honesty-20" name="proof-of-rollup-executors-honesty-20"></a>Proof of Rollup executor’s honesty</h3>
<p>To verify that the executor has honestly executed the block according to the transaction list committed be the shared sequencer, the shared prover also includes the relationship between the values committed by the shared sequencer and the MPT root of the transactions uploaded by the executor.</p>
<h1><a class="anchor" href="https://ethresear.ch#h-4-security-analysis-21" name="h-4-security-analysis-21"></a>4. Security analysis</h1>
<hr />
<ul>
<li><strong>Individual Transaction Validity</strong>: The validity of individual transactions (e.g., insufficient user’s balance) is verified by the Bundler Contract, and transactions will fail if they do not pass this check.</li>
<li><strong>Bundle Transaction Validity</strong>: The validity of the bundled transaction (e.g., discrepancy between mint and burn amounts) is also verified by the Bundler Contract, and the bundle will fail if it does not pass this check.</li>
<li><strong>Atomicity of Successful Bundle Transactions</strong>: The shared sequencer is responsible for ensuring the atomicity of successfully executed bundled transactions. The honesty of the shared sequencer is verified by the shared prover and the verification layer.</li>
<li><strong>Honesty of the Executor</strong>: If the shared sequencer has legitimately determined and committed the blocks for each rollup, the honesty of the executor is verified by the shared prover and the verification layer.</li>
<li><strong>Prevention of Manipulation</strong>: Additionally, neither the shared prover nor the rollup executor can manipulate the user’s bundle transaction. The user’s signature prevents such tampering.</li>
<li><strong>Integrity of Proofs and Verification</strong>: The shared prover and the verification layer perform proofs and verification based on authenticated data available in the Data Availability (DA) layer. As a result, they cannot alter the proof content. The worst they can do is to withhold performance.</li>
</ul>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/cross-rollup-synchronous-atomic-execution/20193">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 01 Aug 2024 08:57:29 +0000</pubDate>
</item>
<item>
<title>Preconfirmation Bidding Increased Block Values on Holesky</title>
<link>https://ethresear.ch/t/preconfirmation-bidding-increased-block-values-on-holesky/20190</link>
<guid>https://ethresear.ch/t/preconfirmation-bidding-increased-block-values-on-holesky/20190</guid>
<content:encoded><![CDATA[
<div> 关键词：Mev-commit、Holesky、预确认、竞标行为、区块价值

总结:
文章主要探讨了自7月10日以来，Mev-commit 0.4.3版本在Holesky tesnet上启用预确认功能的情况。在这段时间里，预确认功能参与度逐渐增加，目前有1个转接器、3个提供者、9个竞标者参与其中。从7月10日至7月29日期间，提供者共发布了807次预确认，涉及415个Holesky区块。预确认交易数量较少（815次），但价值相对较高，平均每个预确认交易的价值为0.0049 ETH，而平均优先费用仅为0.0045 ETH。这表明预确认交易对整体区块价值的贡献显著。

此外，文章还指出，预确认交易平均为区块贡献的价值（0.0093 ETH）远高于常规区块（0.0044 ETH）。这表明预确认机制可以有效提升区块价值，从而增加验证者的奖励。同时，文章也提到，由于Holesky作为测试网与主网存在差异，其预确认功能的使用情况可能与主网有所不同。为了进一步研究和优化预确认机制，计划进行更多在模拟主网环境下的测试，并邀请更多的参与者加入到Mev-commit生态系统中。 <div>
<h1><a class="anchor" href="https://ethresear.ch#tldr-1" name="tldr-1"></a>TLDR</h1>
<ul>
<li>Since July 10, mev-commit 0.4.3 has enabled over 800 execution preconfirmations on Holesky, with increasing network participation.</li>
<li>Providers issued 807 preconfirmations across 415 blocks. Bidders sent 4.24 ETH worth of bids.</li>
<li>Average mev-commit block value was 0.0093 ETH compared to 0.0044 ETH for a vanilla block.</li>
<li>Average preconfirmation bid was 0.0049 ETH, slightly higher than the average priority fee of 0.0045 ETH.</li>
<li>Data shows that preconf bids contribute significantly to overall block value, despite limited participation for the nascent network.</li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#overview-2" name="overview-2"></a>Overview</h1>
<p>Since July 10, mev-commit 0.4.3 has been facilitating execution preconfirmations on Holesky tesnet. There has been an upwards trend of participation with currently 1 relay, 3 providers, 9 bidders, and <a href="https://validators.mev-commit.xyz/" rel="noopener nofollow ugc">27,000 validators</a> participating. From July 10 to July 29 (Holesky block range 1902173 to 2027932), providers have issued 807 preconfirmations across 415 Holesky blocks. Some examples of preconfirmation blocks:</p>
<ul>
<li><a href="https://holesky.etherscan.io/block/1943039" rel="noopener nofollow ugc">1943039</a> with 21 preconfs and .016 ETH worth of bids</li>
<li><a href="https://holesky.etherscan.io/block/1986732" rel="noopener nofollow ugc">1986732</a> with 7 preconfs and .04 ETH worth of bids</li>
<li><a href="https://holesky.etherscan.io/block/1986963" rel="noopener nofollow ugc">1986963</a> with 5 preconfs and .022 ETH worth of bids</li>
</ul>
<p>There are two caveats to these initial results. The first is that network participation is still growing. As more actors onboard or opt in to the network, the flow of preconfs is likely to increase. The second caveat is that Holesky does not have the same competitive use cases for preconfs as mainnet, and does not mirror mainnet Ethereum transacting behavior as closely as desired.</p>
<p>The notebook used for analytics <a href="https://github.com/Evan-Kim2028/preconf_analytics/blob/e6fdb9886c600315d531b59cb13e6efccc7d56bd/notebooks/preconfs.ipynb" rel="noopener nofollow ugc">can be found here</a>. The data for these results can be replicated using the <a href="https://github.com/primev/mev_commit_sdk_py" rel="noopener nofollow ugc">mev-commit-sdk-py</a> repository to collect mev-commit events powered by Hypersync indexer. There is also <a href="http://explorer.testnet.mev-commit.xyz/app/discover" rel="noopener nofollow ugc">an explorer</a>, which is currently in development.</p>
<h1><a class="anchor" href="https://ethresear.ch#bidding-behavior-3" name="bidding-behavior-3"></a>Bidding Behavior</h1>
<p>We observe 815 preconfirmation transactions, indicating a niche but valuable market segment compared to 4 million regular transactions. This significant difference suggests preconfs are currently used by a smaller subset of users who are testing preconfirmations.</p>
<p>A total of 9 bidders participated, sending 4.24 ETH in bids compared to 0.13 ETH in priority fees, with an average preconfirmation bid of 0.005 ETH versus 0.00016 ETH for priority fees on the same transaction, indicating a heavier bidding preference for preconfirmations over priority fees.<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/0/4/04d6296e99153e73f2e018b8c21b727582d0c89e.png" title="image"><img alt="image" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/0/4/04d6296e99153e73f2e018b8c21b727582d0c89e_2_563x500.png" width="563" /></a></div><p></p>
<p>Overall, priority fees totaled 544 ETH with the average preconfirmation bid being 0.0049 ETH, slightly higher than the average priority fee of 0.0045 ETH.</p>
<h1><a class="anchor" href="https://ethresear.ch#block-value-4" name="block-value-4"></a>Block Value</h1>
<p>We hypothesized that preconfs would add an increase in block value, resulting in higher validator rewards per mev-commit block. On average, we observe 1.95 preconfirmation transactions per block compared to 42.3 total transactions. Average mev-commit block value was 0.0093 ETH compared to 0.0044 ETH for a vanilla block.</p>
<p><img alt="image" height="474" src="https://ethresear.ch/uploads/default/original/3X/d/9/d97a557eefe85c83cef80122c55b8695d60307b1.png" width="542" /></p>
<p>One limitation in comparing mev-commit blocks to vanilla Holesky blocks is that there are only ~400 mev-commit blocks compared to ~50,000 Holesky blocks. This is primarily due to the nascent mev-commit network participation rates. Additionally the average bid amount at 0.005 ETH seems on the higher side for Holesky blocks and may not accurately reflect mainnet amounts. However accurately pricing preconfirmations is a difficult task and has to be balanced with the presence of mev spikes on mainnet that can greatly skew results. We are actively researching how to price preconfirmations more effectively.</p>
<p>We illustrate the block revenue breakdown over several days in the chart below for mev-commit blocks, showing the breakdown between preconfirmation bids and priority fees:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/0/c04ec97bff2be44d3e1f79915157487def6eb685.png" title="image"><img alt="image" height="409" src="https://ethresear.ch/uploads/default/optimized/3X/c/0/c04ec97bff2be44d3e1f79915157487def6eb685_2_690x409.png" width="690" /></a></div><p></p>
<p>Preconfirmation bids significantly contributed to increasing block value. On days such as July 11th, 18th, and 24th, preconfirmation bids markedly boosted total block value, highlighting their substantial impact.</p>
<p>The charts below illustrate an outsized impact that preconfirmation bids on block value:</p>
<ul>
<li><strong>Preconf Bids per Block</strong>: Despite a smaller number of transactions, preconfirmation bids are consistently higher, often reaching up to 0.02 ETH.</li>
<li><strong>Priority Fees per Block</strong>: While more frequent, priority fees are generally lower, seldom exceeding 0.01 ETH.</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/3/43fe06dfdd10788ec3344bc9e8592e3773cf34a6.png" title="image"><img alt="image" height="312" src="https://ethresear.ch/uploads/default/optimized/3X/4/3/43fe06dfdd10788ec3344bc9e8592e3773cf34a6_2_690x312.png" width="690" /></a></div><p></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/0/1078d70fc113dfd6d3a0d5a291ef56a81a12f361.png" title="image"><img alt="image" height="317" src="https://ethresear.ch/uploads/default/optimized/3X/1/0/1078d70fc113dfd6d3a0d5a291ef56a81a12f361_2_690x317.png" width="690" /></a></div><p></p>
<p>A notable example is block <a href="https://holesky.etherscan.io/block/1943039" rel="noopener nofollow ugc">1943039</a>, which had the highest number of preconfs with 21 out of 48 transactions. In this block, preconf bid revenue was 0.008 ETH, vastly outpacing the 0.0009 ETH from priority fees.</p>
<p>These observations demonstrate that even a few preconfirmation transactions can substantially enhance block value due to their higher bid amounts.</p>
<h1><a class="anchor" href="https://ethresear.ch#limitations-5" name="limitations-5"></a>Limitations</h1>
<p>As mentioned earlier, the caveats to our initial findings is that Holesky is a testnet and does not have the same types of competitive opportunities as Mainnet. Users tend to have less urgency on Holesky and this is reflected in smaller block sizes and lower priority fees.</p>
<p>As a result, the preconf bids may not have the same relationship to priority fees on mainnet compared to testnet and may not accurately reflect the user’s true bidding preferences since testnet tokens are being used.</p>
<h1><a class="anchor" href="https://ethresear.ch#closing-remarks-6" name="closing-remarks-6"></a>Closing Remarks</h1>
<p>This report initially touches on some preconfirmation bidding behavior observed through early mev-commit usage and offers insights into how preconf bids can increase validator rewards. We plan to follow up with a more detailed report on mev-commit protocol details such as the decay mechanism, rewards and slashing, settlement process, and revenue.</p>
<p>We plan to onboard more bidders, providers and validators into the mev-commit ecosystem and conduct more tests in an environment that mimics mainnet more closely. We invite you to participate starting at <a class="inline-onebox" href="https://docs.primev.xyz/get-started/welcome-to-primev" rel="noopener nofollow ugc">Welcome to Primev - Documentation</a></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/preconfirmation-bidding-increased-block-values-on-holesky/20190">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 01 Aug 2024 02:58:46 +0000</pubDate>
</item>
<item>
<title>Affiliated AMMs and permissionless solving for uniform price batch auctions</title>
<link>https://ethresear.ch/t/affiliated-amms-and-permissionless-solving-for-uniform-price-batch-auctions/20187</link>
<guid>https://ethresear.ch/t/affiliated-amms-and-permissionless-solving-for-uniform-price-batch-auctions/20187</guid>
<content:encoded><![CDATA[
<div> 关键词：MEV、批拍卖、AMM、公正执行、分散化

总结:

本文探讨了通过批拍卖机制降低矿工提取费用(MEV)的可能性及其潜在应用。MEV是区块链交易中的一个重要议题，它涉及到交易排序、删除和插入带来的经济利益。批拍卖通过确保交易执行顺序的中立性以及统一清算价格来尝试解决这一问题。

文章首先强调了批拍卖的概念和其在减少MEV方面的潜力。批拍卖允许在区块构建阶段进行交易批量处理，以确保所有交易在同一时间以相同价格执行，这有助于接近理想状态下的公平市场环境。

接着，文章提出了“公正执行”概念，即批拍卖合同执行时不依赖于交易顺序，且统一清算价格保证了交易双方获得一致的价格。CoW协议是当前实现批拍卖机制的一个实例，但作者指出，仅依靠批拍卖机制并不足以完全消除MEV，因为仍然存在数据被忽略或插入的风险。因此，文章提出了一种折衷策略，即在正常市场条件下，通过激励机制让特权行为者不进行数据篡改，并享受仅有的微小优势。

文章进一步讨论了批拍卖系统与自动做市商（AMM）的集成，尤其是引入“附属AMM”，这些AMM可以参与批拍卖并设定特定的交易规则，以此来减少流动性提供者的损失和MEV。

此外，文章还提出了“无许可解决”策略，即允许任何实体执行批拍卖，以最大化去中心化程度。这需要建立信任执行环境，确保诚实解决者能够通过最大化收入来优化执行过程，从而防止恶意操纵价格的行为。

最后，文章讨论了批拍卖系统的实施细节，如费用覆盖、流动性迁移和监管机制等，并强调了系统稳定性的关键因素，包括市场流动性、价格波动范围以及解决者之间的竞争平衡。文章认为，通过引入多级AMM和附属AMM，以及优化解决者激励机制，批拍卖系统可以在一定程度上降低MEV，从而促进更公平、更透明的交易环境，为Dex交易提供更多便利，最终提升区块链的价值。 <div>
<p>The idea of mitigating MEV through batch auctions is as old as the concept of MEV itself. They can both be traced back to <a href="https://www.reddit.com/r/ethereum/comments/2d84yv/miners_frontrunning/" rel="noopener nofollow ugc">this Reddit post</a> from August 2014<sup class="footnote-ref"><a href="https://ethresear.ch#footnote-49420-1" id="footnote-ref-49420-1">[1]</a></sup>. Today, ten years later, this is still an ongoing discussion (see for instance <a href="https://ethresear.ch#references">[UNIX]</a>, <a href="https://ethresear.ch#references">[COW]</a>). To what extent can we reduce MEV by using batch auctions? Can we build batch auction protocols that are better than those existing today? In this article we propose an optimistic point of view for the first question and a candidate affirmative answer to the second question through concrete mechanisms.</p>
<p><strong>Note:</strong> This article was primarily conceived while working at Flashbots Research for a year spanning 2023 and 2024. Special thanks to Christoph Schlegel.</p>
<p>The article assumes that the reader is familiar with the concepts of MEV, batch auctions and AMMs in the context of decentralized exchange (DEX).</p>
<h3><a class="anchor" href="https://ethresear.ch#a-cooperative-endeavour-between-traders-1" name="a-cooperative-endeavour-between-traders-1"></a>A cooperative endeavour between traders</h3>
<p>In the long term, on-chain traders will use those DEX protocols that are most convenient for them. Ideally, they would trade at market prices with no fee other than the gas cost, equalling the gas price of a transfer. Even though Ethereum’s reality is far from this, in my opinion we shouldn’t rush to dismiss the possibility. Carefully designed smart contracts jointly with off-chain mechanisms might help traders get close to this ideal<sup class="footnote-ref"><a href="https://ethresear.ch#footnote-49420-2" id="footnote-ref-49420-2">[2]</a></sup>. It is likely that these clever designs are not out there yet.</p>
<p>A very powerful idea is that the smart contract that settles trade orders does so in batches, enforcing uniform clearing prices. This has two fundamental properties:</p>
<p>(a) The execution does not depend on the ordering of the trade orders.</p>
<p>(b) Uniform clearing prices ensure that a user trading in one direction receives the same price as the other users trading in that same direction, and is a direct counterpart to the users trading in the other directions, with no room for intermediaries between them.</p>
<p>The most famous (or even the only) live system implementing this mechanism on a blockchain is CoW protocol <a href="https://ethresear.ch#references">[COW]</a> (see also <a href="https://ethresear.ch#references">[SPEEDEX]</a>). Unfortunately, the sole existence of a uniform price batch auction mechanism is not enough to reach the ideal situation described above. MEV occurs through the reordering, censoring, or insertion of data by a privileged player. While according to (a) reordering trade orders has no effect in our case, it is still possible that privileged players censor and insert data. As a matter of fact, for the players who write a transaction or a block, it will always be physically possible to ignore some data and to include their own data, maybe even pretending that this new data was produced by someone else. Therefore, here we will simply abandon the search of a protocol that logically guarantees no censorship nor privileged insertions. Nevertheless, we will not rule out the possibility of a mechanism such that in normal market conditions and assuming wide adoption, the privileged players will be incentivized not to censor, and enjoy only a marginal advantage from last moment inclusions.</p>
<p>While CoW protocol has achieved an interesting degree of adoption, it represents only a small fraction of Ethereum DEX activity —<a href="https://defillama.com/aggregators/chains/ethereum" rel="noopener nofollow ugc">around 1% these days</a>. CoW runs a centralized solving protocol.</p>
<h3><a class="anchor" href="https://ethresear.ch#uniform-clearing-prices-and-walrasian-equilibrium-2" name="uniform-clearing-prices-and-walrasian-equilibrium-2"></a>Uniform clearing prices and Walrasian equilibrium</h3>
<p>A trade order can be understood as a mathematical function of the clearing prices. The output of the function is the traded amounts for each asset. Given a set of trade orders, there is typically only a limited set of valid clearing price vectors, maybe even only one. This situation perfectly corresponds to the concept of Walrasian equilibrium in a pure exchange market (see <a href="https://ethresear.ch#references">[RGGM]</a>, <a href="https://ethresear.ch#references">[FY]</a> and references therein). A Walrasian equilibrium is a vector of prices at which the supply of each good equals the demand for that good.</p>
<p>Under very mild hypothesis, we can guarantee the existence of at least one equilibrium. The computational problem of finding equilibrium price vectors translates to the search of fixed points of a certain mapping.</p>
<h3><a class="anchor" href="https://ethresear.ch#affiliated-amms-3" name="affiliated-amms-3"></a>Affiliated AMMs</h3>
<p>Decentralized exchange predominantly occurs through automated market makers (AMMs). Many researchers and industry actors have pointed out that AMM liquidity providers (LPs) typically receive worse prices than what the market has to offer at each time. This phenomenon is usually referred to as loss vs. rebalancing (LVR) and described as MEV suffered by the liquidity providers <a href="https://ethresear.ch#references">[LVR]</a>, <a href="https://ethresear.ch#references">[WLVR]</a>.</p>
<p>There is a natural mechanism to attack this issue that no one seems to have considered yet<sup class="footnote-ref"><a href="https://ethresear.ch#footnote-49420-3" id="footnote-ref-49420-3">[3]</a></sup>. Special AMMs may participate in a uniform price batch auction just like any other trader. These would be the affiliated AMMs. They would allow certain swaps depending on their state and execution price, only admitting the price from the batch. We can think of the allowed swaps as preprogrammed trade orders. To implement this, the contract that executes batches should be prepared to call affiliated swaps passing the batch prices. From now on, let us call <em>W</em> the smart contract that executes batches, i.e. the main contract of the system under consideration. Specially designed affiliated AMMs may be added <em>a posteriori</em> following specifications determined by the <em>W</em> contract<sup class="footnote-ref"><a href="https://ethresear.ch#footnote-49420-4" id="footnote-ref-49420-4">[4]</a></sup>. It is possible that affiliated AMMs benefit by only allowing swaps coming from <em>W</em>. However, we do not need to discuss it at this point: the scheme allows to decouple the problem of choosing a specific AMM design. A multiplicity of them may coexist, and liquidity migration can happen seamlessly at any time. The existence of multiple AMMs affiliated to the same <em>W</em> contract does not entail liquidity fragmentation.</p>
<p>Assuming wide adoption of the <em>W</em> contract and low incidence of censorship, we have clearing prices that are actual market prices, thus mitigating LVR and MEV.</p>
<p>
<br />
</p><p><img alt="diagram" height="451" src="https://ethresear.ch/uploads/default/original/3X/4/8/4824aef76acd0078806fd0c5418d95c44555a648.png" width="642" /></p>
<p></p>
<h3><a class="anchor" href="https://ethresear.ch#permissionless-solving-4" name="permissionless-solving-4"></a>Permissionless solving</h3>
<p>Once we have truly accepted that we cannot enforce censorship resistance for trade orders at code level, we may reasonably conjecture that the best we can do is to open the gates as much as possible in order to minimize censorship and democratize the system. The proposal is to let <em>W</em> allow anyone to execute a batch. The block proposers, as always, will exercise their right to choose the transactions they prefer, possibly through a PBS mechanism <a href="https://ethresear.ch#references">[PBS]</a>, <a href="https://ethresear.ch#references">[MEV-BOOST]</a>. This feature achieves the maximum degree of decentralization possible at smart contract level for a batch auction system. The auction will occur at block building level. This is analogous to the usual permissionless access to AMMs, which is only regulated by the PBS apparatus or whatever mechanism adopted by the block proposers. Another example is UniswapX: their reactors allow anyone to be a <a href="https://docs.uniswap.org/contracts/uniswapx/guides/createfiller" rel="noopener nofollow ugc">filler</a>, though they don’t enforce uniform clearing prices.</p>
<p>Let us explain why it is reasonable to expect that this mechanism will work well, i.e., that potential price manipulations by censoring orders are expected to be under control. The flow of the reasoning is as follows. We will first imagine the system flourished, running a large portion of Ethereum’s DEX volume. We will try to visualize this scenario and assess whether it is stable or if we should expect frequent price manipulations. Let us list some properties of the flourished scenario:</p>
<p><strong>(1)</strong> Since there are many important tokens on Ethereum blockchain, we expect to have a main cluster of several tokens interconnected by swaps at each batch. This is desirable because it means the liquidity in one pair can benefit traders in other pairs (e.g., an order in pair A/B can be settled against orders in pairs B/C and C/A).</p>
<p><strong>(2)</strong> By looking at how prices vary, it turns out that very-short-term volatility is easy to estimate. Only as an example, on a normal day the price of ETH measured in USD typically varies less than 0.1% in a 12s period, with some larger jumps occasionally. Uninformed traders may use this kind of magnitude for the slippage tolerance. Furthermore, public tools that monitor real time price movements can aid users to reduce the slippage tolerance depending on their preferences. Meanwhile, informed traders doing statistical arbitrage or plain arbitrage are expected to use very low values for the slippage tolerance when trading liquid assets. This will set a tight bound on the bounty that a malicious solver can obtain by deviating the price.</p>
<p><strong>(3)</strong> We may assume the existence of honest solvers. As usual, the batch that generates more income for the block proposer should make it to the chain. Honest solvers will aim to maximize that income by maximizing inclusion. They will frequently need to discard some orders for various reasons, such as limited block space, or computation deadlines. As a result, we will often have more than one honest proposed batch. Trusted execution environments can be useful in increasing the transparency of honest solvers.</p>
<p>When a malicious solver tries to manipulate the price, they have to beat the best honest solution. To this end, they will censor every order in one direction for a given pair A/B exceeding certain price threshold. By doing so, they will not only miss out on the gas fees of the censored orders, but also on orders in other pairs due to operating away from the market equilibrium prices (recall (1)). Because of this and (2) it is possible that in most cases it will not be profitable to manipulate the prices of the batch. In addition, we may have other off-chain mechansims to further prevent malicious solving. One such mechanism can be to use private channels between traders and honest solvers in certain cases.</p>
<h3><a class="anchor" href="https://ethresear.ch#mev-a-zoom-out-analysis-5" name="mev-a-zoom-out-analysis-5"></a>MEV: a zoom-out analysis</h3>
<p>Total MEV extraction from Ethereum has been stable during the last two years, at levels above 250 kETH per year<sup class="footnote-ref"><a href="https://ethresear.ch#footnote-49420-5" id="footnote-ref-49420-5">[5]</a></sup>. During this period, there haven’t been many innovations generating optimism about MEV reduction. This has led many people to believe that such levels of MEV are inevitable. The fundamental economic reason for the existence of MEV can be summarized by the concept of block proposer monopoly. If traders want to improve their situation, they need to coordinate by adopting a mechanism that gives them more bargaining power, a trade union. This is the principle underlying the concrete proposals presented here. A system that integrates the different types of liquidity and unifies the execution prices helps traders coordinate their orders around true market prices as described in (2).</p>
<p>Reducing the incidence of MEV would be a great achievement, since it would allow the DEX volume to grow. On-chain trading would become more convenient than centralized alternatives in many cases, thus increasing the global value of the blockchain.</p>
<h3><a class="anchor" href="https://ethresear.ch#final-remarks-6" name="final-remarks-6"></a>Final remarks</h3>
<p>(I) The above description of <em>W</em> is incomplete. Possibly the most important undefined aspect is how to cover gas and trade fees (by <em>gas fee</em> we mean the cost of gas usage as if it were a transfer). What kind of regulations should <em>W</em> implement regarding the operational cost or trade fees? Can the system work well at zero trade fee? See footnote [2]. These questions don’t seem very easy to answer. Fortunately, we will be able to continue iterating theory and practice.</p>
<p>(II) If there are non-affiliated AMMs coexisting with <em>W</em>, the solvers of <em>W</em> can extract profit from them. Every time there is a price movement, it will be possible to find surplus-generating solutions. To find them, they need to consider non-affiliated AMMs as virtual agents of the batch, following a procedure explained in <a href="https://ethresear.ch#references">[FY]</a>. This mathematical fact should act as an attractor of liquidity from traditional to affiliated AMMs.</p>
<h3><a class="anchor" href="https://ethresear.ch#references-7" name="references-7"></a>References</h3>
<p><strong>[COW]</strong> CoW protocol, <a class="inline-onebox" href="https://docs.cow.fi/cow-protocol" rel="noopener nofollow ugc">CoW Protocol | CoW Protocol Documentation</a>;</p>
<p>Felix Leupold, <em>Gnosis Protocol v2 Fighting the MEV Crisis with Batch Auctions one CoW at a time</em>, <a href="https://www.youtube.com/watch?v=6MfcZGVeQsQ" rel="noopener nofollow ugc">https://www.youtube.com/watch?v=6MfcZGVeQsQ</a></p>
<p><strong>[CF]</strong> Andrea Canidio, Robin Fritsch, <em>Arbitrageurs’ profits, LVR, and sandwich attacks: batch trading as an AMM design response</em>, <a class="inline-onebox" href="https://arxiv.org/abs/2307.02074" rel="noopener nofollow ugc">[2307.02074] Arbitrageurs' profits, LVR, and sandwich attacks: batch trading as an AMM design response</a></p>
<p><strong>[FB2]</strong> Philip Daian, Steven Goldfeder, Tyler Kell, Yunqi Li, Xueyuan Zhao, Iddo Bentov, Lorenz Breidenbach, Ari Juels, <em>Flash Boys 2.0: Frontrunning, Transaction Reordering, and Consensus Instability in Decentralized Exchanges</em> <a class="inline-onebox" href="https://arxiv.org/abs/1904.05234" rel="noopener nofollow ugc">[1904.05234] Flash Boys 2.0: Frontrunning, Transaction Reordering, and Consensus Instability in Decentralized Exchanges</a></p>
<p><strong>[FY]</strong> Sergio Yuhjtman, Flashbots, <em>Walraswap: a solution to uniform price batch auctions</em>, <a class="inline-onebox" href="https://arxiv.org/abs/2310.12255" rel="noopener nofollow ugc">[2310.12255] Walraswap: a solution to uniform price batch auctions</a></p>
<p><strong>[LVR]</strong> Jason Milionis, Ciamac C. Moallemi, Tim Roughgarden, Anthony Lee Zhang, <em>Automated Market Making and Loss-Versus-Rebalancing</em>, <a href="https://arxiv.org/pdf/2208.06046" rel="noopener nofollow ugc">https://arxiv.org/pdf/2208.06046</a></p>
<p><strong>[MEV-BOOST]</strong> Flashbots, <em>MEV-Boost in a Nutshell</em>, <a href="https://boost.flashbots.net/" rel="noopener nofollow ugc">https://boost.flashbots.net/</a></p>
<p><strong>[PBS]</strong> Ethereum Foundation, <em>Proposer-builder separation</em>, <a class="inline-onebox" href="https://ethereum.org/en/roadmap/pbs/" rel="noopener nofollow ugc">Proposer-builder separation | ethereum.org</a></p>
<p><strong>[RGGM]</strong> Geoffrey Ramseyer, Mohak Goyal, Ashish Goel, David Mazières,</p>
<p><em>Augmenting Batch Exchanges with Constant Function Market Makers</em>, <a class="inline-onebox" href="https://arxiv.org/abs/2210.04929" rel="noopener nofollow ugc">[2210.04929] Augmenting Batch Exchanges with Constant Function Market Makers</a></p>
<p><strong>[SPEEDEX]</strong> Geoffrey Ramseyer, Ashish Goel, and David Mazières, <em>SPEEDEX: A Scalable, Parallelizable, and Economically Efficient Decentralized EXchange</em>, <a href="https://www.usenix.org/conference/nsdi23/presentation/ramseyer" rel="noopener nofollow ugc">https://www.usenix.org/conference/nsdi23/presentation/ramseyer</a>, <a class="inline-onebox" href="https://stellar.org/blog/developers/building-speedex-a-novel-design-for-decentralized-exchanges" rel="noopener nofollow ugc">Stellar | Building SPEEDEX – A Novel Design for a Scalable Decentralized Exchange</a></p>
<p><strong>[UNIX]</strong> Hayden Adams, Noah Zinsmeister, Mark Toda, Emily Williams, Xin Wan, Matteo Leibowitz, Will Pote, Allen Lin, Eric Zhong, Zhiyuan Yang, Riley Campbell, Alex Karys, Dan Robinson, <em>UniswapX</em> <a href="https://uniswap.org/whitepaper-uniswapx.pdf" rel="noopener nofollow ugc">https://uniswap.org/whitepaper-uniswapx.pdf</a></p>
<p><strong>[WLVR]</strong> Cow DAO, <em>What is Loss-Versus-Rebalancing (LVR)?</em>, <a class="inline-onebox" href="https://cow.fi/learn/what-is-loss-versus-rebalancing-lvr" rel="noopener nofollow ugc">What is Loss-Versus-Rebalancing (LVR)? - CoW DAO</a></p>
<hr class="footnotes-sep" />

<ol class="footnotes-list">
<li class="footnote-item" id="footnote-49420-1"><p>Though the origin of the name MEV and its systematic study started at <a href="https://ethresear.ch#references">[FB2]</a>. <a class="footnote-backref" href="https://ethresear.ch#footnote-ref-49420-1">↩︎</a></p>
</li>
<li class="footnote-item" id="footnote-49420-2"><p>An example of a scenario close to this ideal is to allow a small trade fee that is sublinear in the traded amount. <a class="footnote-backref" href="https://ethresear.ch#footnote-ref-49420-2">↩︎</a></p>
</li>
<li class="footnote-item" id="footnote-49420-3"><p>The same general approach can be found in <a href="https://ethresear.ch#references">[CF]</a>, as is apparent from the title. However the concrete mechanism described there is different from the one proposed here. Additionally, <a href="https://ethresear.ch#references">[RGGM]</a> studies uniform price batches where AMMs swap at the prices of the batch. <a class="footnote-backref" href="https://ethresear.ch#footnote-ref-49420-3">↩︎</a></p>
</li>
<li class="footnote-item" id="footnote-49420-4"><p>This can be implemented without calling ERC20 approvals between contracts. <a class="footnote-backref" href="https://ethresear.ch#footnote-ref-49420-4">↩︎</a></p>
</li>
<li class="footnote-item" id="footnote-49420-5"><p>Most of the extracted MEV is being distributed through MEV-BOOST. See some numbers at <a href="https://mevboost.pics/" rel="noopener nofollow ugc">https://mevboost.pics/</a> and <a href="https://eigenphi.io/" rel="noopener nofollow ugc">https://eigenphi.io/</a>. <a class="footnote-backref" href="https://ethresear.ch#footnote-ref-49420-5">↩︎</a></p>
</li>
</ol>
            <p><small>2 posts - 2 participants</small></p>
            <p><a href="https://ethresear.ch/t/affiliated-amms-and-permissionless-solving-for-uniform-price-batch-auctions/20187">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 31 Jul 2024 22:17:33 +0000</pubDate>
</item>
<item>
<title>Ethereum + Industry of Integrations (IOI)</title>
<link>https://ethresear.ch/t/ethereum-industry-of-integrations-ioi/20167</link>
<guid>https://ethresear.ch/t/ethereum-industry-of-integrations-ioi/20167</guid>
<content:encoded><![CDATA[
<div> 关键词：IPSME、IOI、Ethereum、智能合约、集成

文章主要探讨了通过将集成系统（Integration of Integrations, IOI）概念应用于以太坊（Ethereum）网络的可能性。以下是文章的主要要点总结：

1. **IPSME与IOI**：IPSME是一种由社区开发的集成系统演进架构，而IOI则提出，只要两个系统的接口（APIs）已知且可访问，就能通过特定方式创建翻译来实现集成，并且这种集成可以商业化。

2. **以太坊与智能合约**：文章指出，以太坊已经支持发布/订阅（Pub/Sub）机制，这是实现IOI概念的基础。通过将集成逻辑封装为智能合约，可以在以太坊上创建可重用的集成，从而可能实现原开发者对集成的商业化。

3. **去中心化集成**：通过将集成作为智能合约执行，可以减少与外部系统集成的复杂性，特别是通过传递性（transitivity），即如果A与B集成，B与C集成，那么理论上A可以直接与C集成，减少了对中心化或去中心化Oracle服务的需求。

4. **智能合约的潜力**：文章讨论了智能合约在协议通信中的强大能力，认为它们不仅能够实现自动化集成，还能够在无需信任第三方的情况下确保数据的一致性和完整性。

5. **对Oracle服务的影响**：通过智能合约实现的集成，理论上可以降低对依赖Oracle服务的需求，因为智能合约能够提供一种更直接、更安全的方式来获取和处理外部数据。

总结：将集成系统（如IPSME定义的IOI）概念应用到以太坊网络中，通过利用智能合约实现集成，不仅能够简化与外部系统的集成过程，提高效率，同时还能通过集成的可重用性和潜在的商业化途径，降低对传统Oracle服务的依赖，实现更加去中心化、安全和高效的集成解决方案。 <div>
<p>(This is my first post here, so I hope that I have not missed any protocol)</p>
<p>I’ve been integrating systems using IPSME, which lead to the Industry of Integrations concept <a href="https://root-interface.se/IOI" rel="noopener nofollow ugc">IOI</a>. IPSME defines an evolutionary architecture for integrations developed by the community.</p>
<p>Demos of my work can be found here:<br />
            
</p>
<p>The concept of the IOI is such that: if any two system interfaces (APIs) are known and accessible, that (via the conventions of <a href="https://ipsme.dev" rel="noopener nofollow ugc">IPSME</a>) a translation can be created integrating the two APIs …​ And! That that translation can be monetized.</p>
<p>The Ethereum blockchain is often linked to Oracles or Oracles services that are off-chain. AFAIK, Ethereum already support a pubsub (the basis for IPSME). The question is then if an IOI can be created within the Ethereum network. Namely, can integrations to external services be smart contracts so that integrations are on-chain and are re-usable by other developers. Is it possible that smart contract integration contains the logic so that the original developer can possibly monetize off building the integration. If the integrations can be reused, then the complexity for integrating with external systems can be reduced through the property of transitivity i.e., if A integrates with B and B with C, then A is integrated with C.</p>
<p>I’m interested in doing exploratory research to see if IOI can be applied to Web3. I would like to ask here:</p>
<ul>
<li>Does this idea sound feasible with the Ethereum network?</li>
<li>Are smart contacts powerful enough for protocol communication?</li>
<li>Is it correct that Ethereum supports pubsub and can it be utilized for this?</li>
<li>Would this idea alleviate the need for Oracle services/networks?</li>
</ul>
<p>I’m looking forward to your feedback.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/ethereum-industry-of-integrations-ioi/20167">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sun, 28 Jul 2024 19:52:10 +0000</pubDate>
</item>
<item>
<title>ePBS Metagame: SSP peacekeeping and alternative public service</title>
<link>https://ethresear.ch/t/epbs-metagame-ssp-peacekeeping-and-alternative-public-service/20162</link>
<guid>https://ethresear.ch/t/epbs-metagame-ssp-peacekeeping-and-alternative-public-service/20162</guid>
<content:encoded><![CDATA[
<div> 关键词：MEV-Burn、公共利益建设者(PGB)、有机建设者、SSP、社会层

总结:文章深入探讨了MEV-Burn背景下公共利益建设者(PGB)的策略和影响。首先，介绍了五种潜在的MEV-Burn参与者类型，其中重点分析了公共利益建设者与有机建设者的角色及其动机。公共利益建设者通过高燃烧率或排除有害行为为网络做出贡献，而有机建设者则侧重于维护网络的纯净与实用价值。文章还提出了一种可能的提名机制，即验证者在考虑其声誉和社会信誉的同时选择PGB进行区块构建。

其次，文章通过案例研究探讨了SSP如何通过竞标和恶意攻击来维持其市场地位，以及PGB如何通过减少燃烧率和包括所有MEV（包括有害的）来实现盈利并促进网络改进。这表明PGB可以通过牺牲部分利润来增加其被选中构建区块的机会，从而提高其效率并提升社会信誉。

最后，文章强调了MEV-Burn的社会影响，包括可能形成的利益集团和网络中的动态变化。尽管存在潜在的负面影响，但文章认为有道德的建设者和验证者将找到方法来抵消不法行为，并保持网络的平衡与健康。文章呼吁继续对这一领域进行深入研究，以全面理解MEV-Burn带来的复杂社会影响。 <div>
<h3><a class="anchor" href="https://ethresear.ch#introduction-1" name="introduction-1"></a><strong>Introduction</strong></h3>
<p>Alongside Enshrined Proposer-Builder Separation, MEV-Burn is expected to produce a major shift in many on-chain economic and social dynamics. From reducing rugpool incentives to making ReOrg’s even less profitable, this rollout is sure to provide some tangible improvements to the Ethereum protocol, but what seems to get all too frequently overlooked is the on-chain behavioral impact.</p>
<p>If you haven’t already, I would <em>strongly</em> recommend taking a peek at both <a href="https://ethresear.ch/t/dr-changestuff-or-how-i-learned-to-stop-worrying-and-love-mev-burn/17384">how i learned to stop worrying and love mev-burn</a> by <a href="https://ethresear.ch/u/mikeneuder">Mike Neuder</a>, and <a href="https://ethresear.ch/t/burn-incentives-in-mev-pricing-auctions/19856">Burn incentives in MEV pricing auctions</a> by <a href="https://ethresear.ch/u/aelowsson">aelowsson</a> as they both inspired me to write this piece and may help decipher some of the more technical details.</p>
<p>As stated in <a href="https://ethresear.ch/u/aelowsson">aelowsson</a>’s <a href="https://ethresear.ch/t/burn-incentives-in-mev-pricing-auctions/19856">Burn incentives in MEV pricing auctions</a> there are five types of potential MEV-Burners within pricing auctions:</p>
<aside class="quote no-group">
<div class="title">
<div class="quote-controls"></div>
<img alt="" class="avatar" height="24" src="https://ethresear.ch/user_avatar/ethresear.ch/aelowsson/48/7611_2.png" width="24" /><a href="https://ethresear.ch/t/burn-incentives-in-mev-pricing-auctions/19856/1">Burn incentives in MEV pricing auctions</a></div>
<blockquote>
<p>(A) Public good builder, (B) For-profit public good builder, (C) Extortion racket, (D) Staker-initiated griefing, (E) Staker-initiated griefing cartel.</p>
</blockquote>
</aside>
<h3><a class="anchor" href="https://ethresear.ch#pgb-overview-2" name="pgb-overview-2"></a><strong>PGB Overview</strong></h3>
<p>Narrowing in on the Public Goods division, there are several ways to be considered a Public Goods Builder; one being burn rates. Simply put, the more profits a builder dedicates to burning, the more ‘ultrasound’ Ethereum gets, the more deflationary Ether is, and ultimately, the more service is done to the public. This could be a strategy chosen over a For-Profit Builder if the operator determines that the potential social credit outweighs the mere profit a For-Profit Builder would contribute. Once again this motivation would apply to another type of Public Good Builder, an ‘Organic’ Builder. Instead of contributing high burn rates, this builder would rather exclude toxic MEV such as Sandwitching or even swear off censorship, creating a stronger Ethereum protocol rather than just making Ether more economically plausible. Both would be critical to maintaining the integrity and utility of Ethereum, and both can be used in harmony or to different degrees.</p>
<p>In order for a PGB to be nominated to build a block, both a validator would need to value its reputation/social credit over its slot profit and a builder would go through the trouble to construct a public service block. Due to the random nature of POS, even if a validator were to choose a PGB for its block construction it would <em>not</em> experience a epoch-over-epoch profit increase, but a builder may. This is touched on later, but the social credit a PGB acquires could bring in more <em>business</em> while the randomness of validation ensures that a validator would not.</p>
<p>Some of these public service examples have already been theorized, and many tend to restrict public good opportunities to <em>just</em> these examples. If you take anything away from this article, know that this ePBS social layer may be more dynamic than many predict it to be.</p>
<h3><a class="anchor" href="https://ethresear.ch#case-study-3" name="case-study-3"></a><strong>Case Study</strong></h3>
<p>When choosing a block builder, validators have a choice; a choice of image; a choice of profit; and ultimately a choice that will contribute to the ePBS social layer. Once again, public service can be attractive to many builders still establishing their image. Potential for this public service can be found even in the ePBS vulnerability of Staker-initiated griefing. (theorized by <a href="https://ethresear.ch/u/aelowsson">aelowsson</a>) Simply put, SSPs will do anything to remain the most profitable staking model, even if they need to sabotage or grief competing validators. By outbidding other builders, SSP-sponsored builders can achieve the slot held by an opposing validator and tank opposing SSP rewards. This then reduces the average user rewards for competing SSPs and drives users (and fees) to their own platform. One example of alternative public service resides within SSP peacekeeping. By reducing burn rates and including all MEV (including toxic) a for-profit model can be derived. Using liquidity developed via this method can then be used to outbid suspected SSP griefers or even direct bidding aggression at any SSP-sponsored builder, further enforcing ePBS and preventing large validators and builders from being run together again. However, looking past technical difficulties such as identifying SSP builders, this method also proves to be inefficient, as it offers minimal public credit, due to the fact it leverages harmful building practices to enforce SSP unity.</p>
<h3><a class="anchor" href="https://ethresear.ch#pgb-efficiency-4" name="pgb-efficiency-4"></a><strong>PGB Efficiency</strong></h3>
<p>The efficiency of a public goods builder or PGB is multidimensional, to say the least. Two sections I’ll touch on are popularity efficiency and network improvement-based efficiency.</p>
<p>PBE or Population-Based Efficiency is the simpler of the two and can be represented by the following equation:</p>
<div class="math">
E = \frac{pq}{ra}
</div>
<p>This equation combines a popularity index (<span class="math">\frac{q}{a}</span>) and a profit margin (<span class="math">\frac{p}{r}</span>); where <span class="math">E</span> is relative efficiency, <span class="math">p</span> is average slot profits, <span class="math">q</span> is average builder pick rate, <span class="math">r</span> is average slot revenue and a is average builder pick rate.</p>
<p>This equation represents the efficiency of a PGB’s ‘social marketing’. By sacrificing some slot profit a PGB can increase its pick rates and potentially increase its epoch-over-epoch profit or even just control more of the builder market. <em>Of course, all the while serving the Ethereum public.</em></p>
<p>Another form of efficiency modeling is NIBE or Network Improvement Based Efficiency. This model bases the efficiency of a PGB on its actual public service. However, the idea is harder to put an equation behind, as each public service action contributes varying improvements to the network and each have their own relative values.</p>
<h3><a class="anchor" href="https://ethresear.ch#the-social-layer-5" name="the-social-layer-5"></a><strong>The Social Layer</strong></h3>
<p>As ePBS rolls out and additional features like MEV burn hit the network, it’s clear that countless social impacts will arise. From SSP sabotage to even the PGB social dynamic, there are many opportunities for cliques, groups, and cartels to form. However, no matter what social problems may arise, any reputable builder will find a way to negate any nefarious builder patterns, and any reputable validators will find a way to support any honest builders. Ultimately, the idea of social credibility and the value of PGB’s may balance the many Immiscible groups that are predicted to form after the introduction of ePBS and MEV burn.</p>
<p>Concluding this paper I would like to, once again, thank both <a href="https://ethresear.ch/u/aelowsson">aelowsson</a> and <a href="https://ethresear.ch/u/mikeneuder">Mike Neuder</a> for their wonderful research on MEV burn and pricing auctions respectively. Between PGB dynamics and ePBS metagame, there’s so much more to uncover in this field, so I would like to end by wrapping up this research on PGB dynamics and the resulting social layer by reminding everyone that there is still a world of research to be done.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/epbs-metagame-ssp-peacekeeping-and-alternative-public-service/20162">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sat, 27 Jul 2024 18:47:18 +0000</pubDate>
</item>
<item>
<title>Rollup-Centric Considerations of Based Preconfimations</title>
<link>https://ethresear.ch/t/rollup-centric-considerations-of-based-preconfimations/20160</link>
<guid>https://ethresear.ch/t/rollup-centric-considerations-of-based-preconfimations/20160</guid>
<content:encoded><![CDATA[
<div> 关键词：基于链、预确认、盈利性、时间性、领导者选举

总结:
本文详细阐述了基于链（Taiko链）的构建与优化策略。首先，文章介绍了基于链的基本架构，包括去中心化提案者同步至L2内存池，以及如何通过预确认机制来提升盈利性和时间效率。预确认允许节点周期性地对较小的序列化批次进行确认，从而减少数据发布成本，并通过设置预确认间隔T来确保L2块的定期发布。

接着，文章讨论了数据发布问题，当前Taiko链采用将所有编码的L2交易列表打包成blob的方式，这导致提案者需要支付整个blob的L1 Gas费用，降低了块的盈利能力。Gwyneth引入预确认后，可以实现更高效的数据打包和发布，通过预确认者批量提交块到L1，同时分离了序列化承诺和数据可用性。

文章还分析了预确认带来的非确定性提案问题，即多个预确认者并行构建链会导致链的非确定性。为解决这一问题，文章提出领导者选举作为解决方案之一，以确保在任何给定时间点只有一个节点有权限更新链的状态。领导者选举有助于维护链的确定性和安全性，同时不影响其基础特性。

最后，文章强调了基于链在继承L1安全性和最终性的同时，面临的一些实际挑战，如提案者的盈利性、链启动时的活力建设以及块时间配置。通过预确认机制的优化，基于链可以在不牺牲安全性和最终性的情况下，提供快速的交易确认时间和高效的盈利能力，从而接近理论上可能达到的最大去中心化水平。

综上所述，本文围绕基于链的构建、预确认机制的实施、非确定性提案的解决方案以及面对的实际挑战，提供了一套全面的优化策略，旨在提升基于链的性能和用户体验，同时保证系统的安全性和稳定性。 <div>
<p><em>Special thanks to <a href="https://x.com/Brechtpd" rel="noopener nofollow ugc">Brecht Devos</a>, <a href="https://twitter.com/linoscope" rel="noopener nofollow ugc">Lin Oshitani</a>, <a href="https://twitter.com/ConorMcMenamin9" rel="noopener nofollow ugc">Conor McMenamin</a>, <a href="https://medium.com/@jonas.bostoen" rel="noopener nofollow ugc">Jonas Bostoen</a>, <a href="https://twitter.com/ccpamatt" rel="noopener nofollow ugc">Christian Matt</a> for the reviews <img alt=":tada:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/tada.png?v=12" title=":tada:" width="20" /></em></p>
<p>TLDR;<br />
This article presents <a href="https://x.com/gwyneth_taiko" rel="noopener nofollow ugc">Gwyneth</a> from the <a href="https://x.com/taikoxyz" rel="noopener nofollow ugc">Taiko Labs</a>.  We outline the Taiko chain setup, discuss the profitability and timeliness of L2 block building, and explore how implementations of preconfirmations can configure blocktime and more efficient data publishing. We also address the issue of nondeterministic proposals caused by multiple preconfirmers through leader election, which affect UX for builders and users. The designs in this article are subject to change.</p>
<h2><a class="anchor" href="https://ethresear.ch#background-the-simplest-taiko-chain-1" name="background-the-simplest-taiko-chain-1"></a>Background: The Simplest Taiko Chain</h2>
<p>At present, Taiko Labs is subsidizing block production by running proposers, effectively burning ETH to maintain a fast and inexpensive network. With that in mind, our effort on preconfirmations needs to be expedited, as we aim to facilitate profitable block building in the community without compromising security and throughput. This is the basic setup of the Taiko chain:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/1/f196c6e6be939ac54173b7e6802df9776c2728b6.png" title="Untitled (4)"><img alt="Untitled (4)" height="294" src="https://ethresear.ch/uploads/default/optimized/3X/f/1/f196c6e6be939ac54173b7e6802df9776c2728b6_2_517x294.png" width="517" /></a></div><p></p>
<ul>
<li>
<p>Decentralized proposers run their <a href="https://docs.taiko.xyz/core-concepts/taiko-nodes/" rel="noopener nofollow ugc">taiko-geth</a> to sync with the L2 mempool.</p>
</li>
<li>
<p>When a batch of Tx constitutes a <strong>profitable block</strong>, the rational proposer <a href="https://github.com/taikoxyz/taiko-mono/blob/b89e97b1cd7795753bba57b8ca6caf8a77e22613/packages/protocol/contracts/L1/ITaikoL1.sol#L14" rel="noopener nofollow ugc">submits this block</a> to L1.</p>
<ul>
<li>The profitable criteria is the total tip collected from all Tx plus their MEV covers the costs to interact with L1 and prover:<br />
<div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/6/261274c74370de26d6ec593f649377533be6ea83.png" title="Screen Shot 2024-07-05 at 12.54.04 PM"><img alt="Screen Shot 2024-07-05 at 12.54.04 PM" height="50" src="https://ethresear.ch/uploads/default/optimized/3X/2/6/261274c74370de26d6ec593f649377533be6ea83_2_690x50.png" width="690" /></a></div></li>
</ul>
</li>
<li>
<p>Taiko smart contract on L1 contains the decentralized ledger of L2 Tx batches. These batches will, inevitably, contain invalid Tx with vanilla proposing strategy, since the sequencing is not coordinated. For example:</p>
<ul>
<li>L2-75 has a Tx transferring 100 ETH from Alice to Bob without Alice’s correct signature</li>
<li>L2-76 and 77 both contain a Tx from Cassy with nonce equaling 9;</li>
</ul>
<p>In such cases,  <a href="https://github.com/taikoxyz/taiko-mono/tree/ec6c179967b9ac93cd967ff3a1fe8b331fdb8256/packages/taiko-client" rel="noopener nofollow ugc">taiko-client</a>, similar to Ethereum’s consensus client, will witness the invalid Txs from the L1 ledger and exclude them from the actual block being synced to L2.</p>
</li>
<li>
<p>Back to L2, each <a href="https://github.com/taikoxyz/taiko-mono/tree/ec6c179967b9ac93cd967ff3a1fe8b331fdb8256/packages/taiko-client" rel="noopener nofollow ugc">taiko-client</a> (fork of Ethereum’s consensus client), witnesses the L1 ledger and applies a deterministic rule that invalidates the above Txs. Subsequently, the client can form a correct batch constituting the next block and construct the blockhash.</p>
</li>
<li>
<p>This blockhash is considered finalized when a prover submits proof of the execution of valid Txs, as well as the exclusion of invalid Txs from the state of the ledger.</p>
</li>
</ul>
<p>As Vitalik noted, a based rollup can be a <a href="https://vitalik.eth.limo/general/2021/01/05/rollup.html" rel="noopener nofollow ugc">“total anarchy”</a> amid the chaos, but it remains functional as long as the decentralized ledger persists and the L2 network maintains synchronization. Taiko will continue to progress by inheriting <strong>L1 security and finality</strong>. However, proposers may still encounter challenges, resulting in a <strong>liveness</strong> issue due to lack of profitability.</p>
<h2><a class="anchor" href="https://ethresear.ch#challenges-and-solutions-2" name="challenges-and-solutions-2"></a>Challenges and Solutions</h2>
<h3><a class="anchor" href="https://ethresear.ch#profitability-timing-game-in-l2-block-building-3" name="profitability-timing-game-in-l2-block-building-3"></a>Profitability &amp; Timing Game in L2 Block Building</h3>
<p>In the diagram below proposer Alice observes L2-75 upon confirming L1-100, and she creates L2-76 with blockhash 0xabc. Proposer Bob, attempting the same, causes a fork with an alternate blockhash 0xf3c. Both submit proposals to L1-100 and pay the current L1 transaction fee. However, since Alice’s transactions were incorporated first, Bob’s transaction reverts due to L1_UNEXPECTED_PARENT(), causing Bob to lose his proposing fee. Alice successfully earns the tip and MEV of L2-76, but she still needs to compensate the prover to validate her block afterward.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/8/c8a72e51d5473fb6e20d1824d69e4a3baf4c6921.png" title="Untitled (5)"><img alt="Untitled (5)" height="303" src="https://ethresear.ch/uploads/default/optimized/3X/c/8/c8a72e51d5473fb6e20d1824d69e4a3baf4c6921_2_517x303.png" width="517" /></a></div><p></p>
<p>An L2 block is proposed to the rollup Contract as a raw transaction batch. Consequently, each node subscribing to the event derives the blockhash in their own execution clients. Despite this, the <strong>rollup state is finalized when the proposal is confirmed on L1 because block hash derivation is deterministic.</strong> We still need a proof to validate the block hash to rollup’s L1 ledger, enabling light clients to fetch the states and users to perform withdrawals. Hence, real-time proving solutions such as SGX are important because they enforce L2 state finality with high probability. Let’s recall:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/6/261274c74370de26d6ec593f649377533be6ea83.png" title="Screen Shot 2024-07-05 at 12.54.04 PM"><img alt="Screen Shot 2024-07-05 at 12.54.04 PM" height="50" src="https://ethresear.ch/uploads/default/optimized/3X/2/6/261274c74370de26d6ec593f649377533be6ea83_2_690x50.png" width="690" /></a></div><p></p>
<p>Solving for MEV is a knapsack problem - the larger the knapsack, the more value extracted. It’s been well-studied that L1 proposers will play <a href="https://ethresear.ch/t/timing-games-implications-and-possible-mitigations/17612">the timing game</a> to extend the MEV solving window as much as possible; the same logics apply to L2. Even worse, because L2 users typically tip much lower in an ecosystem with significantly less liquidity, the current 12s block time on Taiko is far less than enough for anyone to profit, which results in a <strong>liveness issue for decentralized proposing</strong>. This is why Taiko Labs operates an unprofitable proposer to sustain the 12s block time. Without taking measures, the L2 blocktime would be arbitrarily long if rational proposers play the timing game.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/e/9ea87c99a3e43e4879c1d6de369b3bb71d885276.png" title="Untitled (6)"><img alt="Untitled (6)" height="375" src="https://ethresear.ch/uploads/default/optimized/3X/9/e/9ea87c99a3e43e4879c1d6de369b3bb71d885276_2_495x375.png" width="495" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#solving-blocktime-data-publishing-with-preconfirmations-4" name="solving-blocktime-data-publishing-with-preconfirmations-4"></a>Solving Blocktime &amp; Data Publishing with Preconfirmations</h3>
<p>Essentially, we’re facing a conflict in a <strong>UX property of L2 (blocktime) versus decentralized block building</strong>. In centralized L2, timeliness is easily managed by the centralized sequencer, while on L1, the beacon attestation enforces the time to publish the execution payload. Thus, we observe that timeliness must be enforced by some mechanism other than builders in the game. Whoever facilitates preconfirmations could also mandate blocktime.</p>
<p><strong>A preconfirmer can periodically issue preconfirmations to builders for smaller sequenced batches, then batch publish the batches to reduce the data publishing costs.</strong> The periodic issuance of batches now constitutes L2 blocks. The L2 protocol, which allows the preconfirmer to opt in, can facilitate timeliness by ensuring preconfirmed blocks are released every T second. Now, we define <strong>T as the L2 blocktime</strong>, which can be adjusted faster to improve user experience.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/2/d204bb354780b7e25543312984493e2f719d2f72.png" title="Untitled (7)"><img alt="Untitled (7)" height="387" src="https://ethresear.ch/uploads/default/optimized/3X/d/2/d204bb354780b7e25543312984493e2f719d2f72_2_690x387.png" width="690" /></a></div><p></p>
<p>Regarding data publishing, Taiko currently publishes all encoded L2 transaction lists in blobs. This requires the proposer to cover the L1 gas fee for a whole blob regardless how much data is actually necessary, further reducing the block’s profitability. In Gwyneth, preconfirmations will allow for <strong>more batching of L2 blocks into blobs</strong> if the preconfirmer is assigned multiple L1 slots, which also implies the separation of sequencing commitment and data availability:</p>
<ul>
<li><strong>Preconfirmations Issuance ⇒ commit L2 sequencing</strong></li>
<li><strong>Preconfirmations Delivery ⇒ data publishing to L1</strong></li>
</ul>
<p>Now we can characterize the L1 preconfirmer as the de facto L2 proposer, and the existing decentralized sequencer who submits batches as L2 builders - we just migrate the PBS architecture to L2. Moreover, this L2 PBS mechanism can use a similar pipeline as on L1, because the L2 proposer is exactly an L1 validator who runs something like <a href="https://github.com/flashbots/mev-boost.git" rel="noopener nofollow ugc">MEV-boost</a> with a preconfirmation add-on. The new fee model functions as follows:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/7/c7f1262f29380aea940d4c5152693be272949797.png" title="Screen Shot 2024-07-11 at 2.46.47 AM (1)"><img alt="Screen Shot 2024-07-11 at 2.46.47 AM (1)" height="212" src="https://ethresear.ch/uploads/default/optimized/3X/c/7/c7f1262f29380aea940d4c5152693be272949797_2_460x212.png" width="460" /></a></div><p></p>
<p>For clarification, L2 proposers are the preconfirmers who opt into the Gwyneth protocol to propose L2 blocks, and the preconfrmers are the L1 validators who can issue preconfirmations.</p>
<p><img alt="Screen Shot 2024-07-11 at 2.47.22 AM (1)" height="32" src="https://ethresear.ch/uploads/default/original/3X/f/0/f01ebd831084ad709d4805fe4ff1b83327b3ce73.png" width="372" /></p>
<p>Overall, preconfirmations enable Gwyneth blocks to be built in short and steady intervals by decentralized participants, while not compromising profitability. A deficiency of liveness caused by lacking liquidity on L2 will not jeopardize blocktime; in other words, users can always enjoy fast transaction confirmation. It also provides a clear model for L2 MEV compatible with the existing PBS pipeline.</p>
<h3><a class="anchor" href="https://ethresear.ch#decentralized-block-proposing-with-pbs-5" name="decentralized-block-proposing-with-pbs-5"></a>Decentralized Block Proposing with PBS</h3>
<p>We have discussed how preconfirmation benefits L2 proposers. Now, let’s consider <strong>proposal inclusion</strong> from the perspective of L1 validators.</p>
<p>Initially, we have a distinct group of L2 participants who compete to propose the next L2 batch by calling the <code>ProposeBlock</code> function in the Taiko smart contract. Their proposal transactions with encoded L2 batches are exposed in the public mempool, and L1 validators or builders will choose to include these proposals. Apparently, t<strong>he L1 parties can easily capture the transactions, stealing the L2 tip and MEV when producing the L1 block.</strong> We’re revisiting the PBS playbook. Rollup with permissionless sequencing can implement similar mechanisms to mitigate block stealing.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/1/d17ee918d3b13e915483a110c04c22af49e6fe0e.png" title="Untitled (8)"><img alt="Untitled (8)" height="260" src="https://ethresear.ch/uploads/default/optimized/3X/d/1/d17ee918d3b13e915483a110c04c22af49e6fe0e_2_517x260.png" width="517" /></a></div><p></p>
<p>However, there’s no need for mitigation following the <a href="https://ethresear.ch/t/based-preconfirmations/17353">definition</a> of base rollup:</p>
<blockquote>
<p>A rollup is said to be based, or L1-sequenced, when its sequencing is driven by the base L1.</p>
</blockquote>
<p>In other words, all L2 proposers are L1 validators. Given access to both mempools, a builder can incorporate L2 batches in her L1 bundles, which is by far the most <strong>efficient paradigm for Gwyneth block-building</strong></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/8/68eae50a088bf3c631d1fb23e52d49855f0a0ad5.png" title="Untitled (9)"><img alt="Untitled (9)" height="249" src="https://ethresear.ch/uploads/default/optimized/3X/6/8/68eae50a088bf3c631d1fb23e52d49855f0a0ad5_2_517x249.png" width="517" /></a></div><p></p>
<p>Recall also in PBS, validators have a choice to build the block natively without using <a href="https://github.com/flashbots/mev-boost.git" rel="noopener nofollow ugc">MEV-boost</a> connecting to external builders. The L1 validator, who’s also an L2 proposer, can issue consecutive preconfirmations to self-produce L2 blocks until her slot to propose. In this case, we may also omit the separate role of builders, and rewrite the fee model for L2 proposers:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/2/1291f379205c49d484fda26e97169d2671738cdd.png" title="Screen Shot 2024-07-11 at 2.46.17 AM"><img alt="Screen Shot 2024-07-11 at 2.46.17 AM" height="90" src="https://ethresear.ch/uploads/default/optimized/3X/1/2/1291f379205c49d484fda26e97169d2671738cdd_2_517x90.png" width="517" /></a></div><p></p>
<p>With the inclusion model much simplified, we note that the L1 validator who includes the L2 proposal is the deterministic proposer of L2. Given Taiko’s current 12s blocktime, there is a one-to-one correspondence between each L1 and L2 block, hence the state of the chain at any slot is deterministic.</p>
<h3><a class="anchor" href="https://ethresear.ch#nondeterministic-proposer-and-leader-election-6" name="nondeterministic-proposer-and-leader-election-6"></a>Nondeterministic Proposer and Leader Election</h3>
<p>Now, as we decouple the L1-to-L2 block correspondence with preconfirmation, we argue that <strong>nondeterminism is also introduced because, during the L1 epoch, multiple preconfirmers exist to perform sequencing concurrently.</strong></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/2/b24f24e1ef4885e714897d2f95d25f8c1f2582fb.jpeg" title="Untitled (10)"><img alt="Untitled (10)" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/b/2/b24f24e1ef4885e714897d2f95d25f8c1f2582fb_2_660x500.jpeg" width="660" /></a></div><p></p>
<p>If these preconfirmers are the subset of L2 proposers who produce blocks natively, everyone will start building on top of the latest finalized parent. This continues until the set of preconfirmations is settled, updating the head of the chain. Then, a proposer will restart with the new head and <strong>abandon their local ledger, resulting in previous preconfirmed transactions being reverted upon delivery</strong>. If the proposer does not restart and proposes the local fork with data publishing during their slot, <strong>that proposal will also revert</strong>. In such a case, the L2 will miss a slot to update; users will experience the <strong>chain halting</strong> until the next proposer comes on board. The malfunctioning proposer might be slashed depending on the protocol implementation.</p>
<p>Considering builders in the PBS setting, who can send their sequenced batches to all L2 proposers in the current epoch, <strong>the head of the chain will appear nondeterministic</strong> to them, as all proposers will endorse different forks simultaneously. However, only the next-in-line proposer holds the source of truth, since her ledger will be settled first. <strong>Therefore, a rational builder should request preconfirmation only from the next-in-line proposer</strong>. Nonetheless, the protocol cannot prevent a malicious proposer from forcing his fork proposal through a regular transaction on Ethereum.</p>
<p>There are two possible solutions: 1)  <strong>define the ledger held by the next-in-line proposer as canonical, which yields a leader selection protocol;</strong> 2) disable block proposals at the non-preconfirmed L1 slots, then fork proposals will likely be excluded by a rational next-in-line preconfirmer. The latter solution is sub-optimal because we still want to preserve the option of non-preconfirmed block proposals unless there are enough preconfirmers to achieve our desirable liveness.</p>
<h4><a class="anchor" href="https://ethresear.ch#on-leader-election-7" name="on-leader-election-7"></a>On Leader Election</h4>
<p>In a decentralized setting at anytime, <strong>only one L1 validator should have exclusive write access to the L2 state</strong>, even if all opt-in participants can issue preconfirmations. <strong>Such systems are inherently finalized without any external finality gadget</strong>.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/3/23c6258764b5d1c0ec9df8be0a3db5b6cd2132a8.png" title="Untitled (11)"><img alt="Untitled (11)" height="231" src="https://ethresear.ch/uploads/default/optimized/3X/2/3/23c6258764b5d1c0ec9df8be0a3db5b6cd2132a8_2_690x231.png" width="690" /></a></div><p></p>
<p>On the other hand, an L2 builder who’s building the latest Gwyneth chain can only write to preconfirmed L1 block space from the next opt-in validator. Requesting preconfirmations from others is strictly prohibited because that creates a gap in the slot.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/d/3d2b9b203eda188c0ebc6520cdb249d14b114b14.png" title="Untitled (12)"><img alt="Untitled (12)" height="251" src="https://ethresear.ch/uploads/default/optimized/3X/3/d/3d2b9b203eda188c0ebc6520cdb249d14b114b14_2_690x251.png" width="690" /></a></div><p></p>
<p>Essentially, we create a just-in-time market for exchanging L1/L2 block space. Instead of a JIT auction, Some suggest using <a href="https://ethresear.ch/t/execution-tickets/17944">execution tickets</a> for an ahead-of-time auction, which means in the diagram above, the L1-104 proposer can sell L2-79 block space simultaneously while the L1-102 proposer sells L2-78. This establishes a one-to-one correspondence between L1/L2 slots in a more controlled manner, and since it allows all participants to buy and sell these rights, an ahead-of-time auction aligns better with the preconfirmation market. From the L2 perspective, the protocol’s sale of execution tickets can imply new fee models for value-capturing. <a href="https://ethresear.ch/t/preconfirmations-on-splitting-the-block-mev-boost-compatibility-and-relays/19837">XGA-style preconfirmations</a> can be a good implementation.</p>
<h2><a class="anchor" href="https://ethresear.ch#summary-8" name="summary-8"></a>Summary</h2>
<p>Taiko started as a rollup with decentralized proposers, with a protocol that deterministically derives L2 state as long as the ledger is finalized on L1. We realized that based sequencing, which unites L1 and L2 proposers, transforms our framework into something more simple and powerfull. Based sequencing will work, naively, with finality and security inherited from L1.</p>
<p>Based sequencing may not work, in practice, considering builder profitability, bootstrapping liveness, and the configuration of fast blocktime. We discuss preconfirmations to tackle these challenges with some tweaks on timeliness and proposal mechanisms. However, having multiple validators who issue preconfirmations can cause the concurrent building of L2 forks. This introduces nondeterminism for the spectators of chains including builders, exchanges, and users, although fortunately, nondeterministic sequencing does not affect finality - <strong>most obstacles in based sequencing relate to essential UX properties for builders and users.</strong></p>
<p>Despite some controversy, leader election could be a practical middle-ground solution. We anticipate a significant number of L1 proposers opting in as preconfirmations gain adoption. Consequently, <strong>proposer decentralization still remains close to the (at least theoretically) maximal achievable decentralization offered by a vanilla based rollup.</strong></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/0/40dd31cbef98940c5ba5f843d943f08e9ab8a7e2.png" title="Untitled (13)"><img alt="Untitled (13)" height="339" src="https://ethresear.ch/uploads/default/optimized/3X/4/0/40dd31cbef98940c5ba5f843d943f08e9ab8a7e2_2_690x339.png" width="690" /></a></div><p></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/rollup-centric-considerations-of-based-preconfimations/20160">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sat, 27 Jul 2024 14:41:46 +0000</pubDate>
</item>
<item>
<title>Notes on the LVR of FM-AMM</title>
<link>https://ethresear.ch/t/notes-on-the-lvr-of-fm-amm/20151</link>
<guid>https://ethresear.ch/t/notes-on-the-lvr-of-fm-amm/20151</guid>
<content:encoded><![CDATA[
<div> 关键词：FM-AMM、LVR、CEX-DEX、交易成本、流动性池大小

总结:
本文详细介绍了改进后的自动做市商（FM-AMM）的额外特性，通过找到纯粹策略纳什均衡来解决CEX-DEX套利商之间的博弈问题。文中计算了理论设置下的FM-AMM的渐近LVR，并将其性能与Uniswap V2风格的固定费率CPMM进行了对比。观察结果表明，FM-AMM的性能受到价格波动、交易成本和流动性池规模的影响，在特定条件下，它对套利商的损失较小。

文章首先回顾了LVR在自动做市商领域的重要性及其在减少LVR方面的努力。接着，作者详细阐述了FM-AMM的设计，并通过修改原始设计解决了更广泛情况下的批量交易执行问题。随后，通过建立模型分析了具有固定交易成本和零售订单流不存在条件下的FM-AMM性能，发现其纳什均衡存在并具有对称性，且LVR随参与者数量成反比衰减。进一步地，文章考虑了交易成本由参与者决定的情况，指出FM-AMM的优势取决于跳动大小、频率和成本，而数值模拟显示FM-AMM与基于rollup的解决方案相匹配。

最后，通过对比FM-AMM和CPMM的LVR，文章指出FM-AMM在高波动率、低交易成本和较大流动性池的情况下表现出色。同时，文章讨论了在不同参数下的性能比较，发现FM-AMM在L2环境中以及低交易成本的L1环境中具有优势，这解释了之前研究结果的混杂性。此外，文章还提出了未来研究应考虑更宽松条件以提供更全面评估的建议。

总的来说，本文提供了对FM-AMM在不同市场条件下的性能分析，强调了其在特定场景下的优越性，同时也指出了现有模型的局限性和未来研究方向。 <div>
<h1><a class="anchor" href="https://ethresear.ch#h-0-tldr-1" name="h-0-tldr-1"></a>0. TL;DR</h1>
<p>We introduced and detailed the additional features of FM-AMM, as presented in [CF23]. We modeled the game between CEX-DEX arbitrageurs for arbitrage profit on FM-AMM and then solved it by finding the pure strategy Nash equilibrium. Lastly, we calculated the asymptotic LVR of FM-AMM in theoretical settings and compared its performance against the Uniswap V2-style fixed-rate fee CPMM through numerical simulations. Our observations indicated that the performance is heavily influenced by price volatility, transaction costs, and the size of the liquidity pool, with FM-AMM showing a reduced loss to arbitrageurs under specific conditions.</p>
<h1><a class="anchor" href="https://ethresear.ch#h-1-introduction-2" name="h-1-introduction-2"></a>1. Introduction</h1>
<p>Since LVR was introduced in [MMRZ22] and [MMR23], it has quickly become the standard for measuring the performance of AMMs. Numerous attempts have been made to reduce LVR through dynamic fee policies, and this research continues actively. However, batch trade execution has not received much attention, except in [CF23] and [GGMR22]. In [CF23], the authors proposed a function-maximizing automated market maker (FM-AMM), asserting it effectively eliminates LVR, and provided numerical simulations comparing its performance with various Uniswap V3 pools. They later <a href="https://forum.cow.fi/t/4-months-of-cow-amm-what-we-have-learned-and-the-next-steps/2432" rel="noopener nofollow ugc">claimed</a> that CoW-AMM (their implementation of FM-AMM) performed well in live settings too, which led to <a href="https://x.com/0x94305/status/1813690004331438306" rel="noopener nofollow ugc">debate</a> on Twitter regarding the legitimacy of their measurement methods. Although the debate focused more on whether markout is a useful metric for measuring performance, the existence of retail order flow and fluctuating transaction costs are also obstacles to precisely comparing their performance. In this article, we analyze the performance of FM-AMM and compare it to CPMM under fixed transaction costs and in the absence of retail order flow conditions like those in [N22] and [E24].</p>
<p>In detail, we slightly modified their design and found a Nash equilibrium in a game where arbitrageurs strategically submit orders to the (slightly modified) FM-AMM to maximize their returns. The game is similar to the liquidity provision game introduced in [MC24], which is a special form of the generalized Tullock contest. The resulting equilibrium has many favorable properties: the solution always uniquely exists, and it is symmetric. Moreover, LVR decays inversely proportionally to the number of participants. This model assumes that the number of arbitrageurs, <span class="math">N</span>, is pre-determined and transaction cost, <span class="math">c</span>, is zero. We proceed to a model where the number of participants is determined endogenously according to <span class="math">c</span>. In this setting, FM-AMM is not always superior; the result now depends on jump size, frequency, and cost. We provide numerical simulation results and suggest that FM-AMM fits well with rollup-based solutions.</p>
<h1><a class="anchor" href="https://ethresear.ch#h-2-fm-amm-3" name="h-2-fm-amm-3"></a>2. FM-AMM</h1>
<p>In this section we fill the omitted details of FM-AMM introduced in [CF23] to handle the more general case. The underlying AMM curve introduced in [CF23] is:</p>
<div class="math">
y_\text{out} = \frac{x_\text{in}}{X + 2x_\text{in}}Y,
</div>
<p>where <span class="math">x_\text{in}</span> is the amount of token <span class="math">X</span> the trader is willing to sell, and <span class="math">y_\text{in}</span> is the amount of token <span class="math">Y</span> that she will receive. However, this is the simplest case where only a single side of order is submitted in batch. Authors of original paper handled the case such that both side of orders exist in the same batch by assuming users only specify the amount of token X to buy or sell. Unfortunately, this is hard to implement in fully on-chain manner since whether the trader has enough capital to buy specified amount of token X is not guaranteed before the batch is settled (Selling is not problematic; we can pull the token from trader and keep it by settlement). We generalize the formula to handle broader range of cases. Let <span class="math">X, Y</span> be reserves of pool, <span class="math">T</span> be total supply of LP tokens before batch settlement, <span class="math">x_\text{in}, y_\text{in}</span> be aggregate amount of each token that traders are willing to sell, and <span class="math">x_\text{mint}, y_\text{mint}</span> be aggregate amount of each token provided from LPs. The fundamental equation we will start with is:</p>
<div class="math">
\begin{align}\begin{bmatrix}x_{\text{mint}} \\y_{\text{mint}}\end{bmatrix}&amp;=x_{\text{mint}} \begin{bmatrix}1 \\p\end{bmatrix}+\begin{bmatrix}0 \\2\alpha\end{bmatrix}\\\begin{bmatrix}x_{\text{in}} \\y_{\text{in}}\end{bmatrix}&amp;=x_{\text{in}} \begin{bmatrix}1 \\p\end{bmatrix}+\begin{bmatrix}0 \\\beta\end{bmatrix}\\\begin{bmatrix}x_1 \\y_1\end{bmatrix}&amp;=\begin{bmatrix}x_0 \cdot \frac{y_0 + \alpha + \beta}{y_0 + 2(\alpha + \beta)} \\y_0 + \alpha + \beta\end{bmatrix}\end{align}
</div>
<p>Here, the <span class="math">p</span> is the clearing price, and <span class="math">\alpha, \beta</span> are the net swap amount for swapping and minting, respectively. In short, among the submitted orders, we swap only part of them, <span class="math">\alpha</span> and <span class="math">\beta</span>, then exchange the rest via p2p without changing the spot price. The fact that</p>
<div class="math">
\begin{bmatrix}x_{\text{mint}} \\y_{\text{mint}} - 2\alpha\end{bmatrix}, \begin{bmatrix}x_{\text{in}} \\y_{\text{in}} - \beta\end{bmatrix}, \begin{bmatrix}x_1 \\y_1\end{bmatrix}
</div>
<p>are all parallel gives us following matrix equation:</p>
<div class="math">
\begin{equation}\begin{bmatrix}2x_0 + 2x_{\text{mint}} &amp; 2x_{\text{mint}} \\2x_{\text{in}} &amp; 2x_{\text{in}} + x_0\end{bmatrix}\begin{bmatrix}\alpha \\\beta\end{bmatrix}=\begin{bmatrix}x_0 y_{\text{mint}} - x_{\text{mint}} y_0 \\x_0 y_{\text{in}} - x_{\text{in}} y_0\end{bmatrix}\end{equation}
</div>
<p>Note that the determinant of matrix in LHS is always strictly positive so above equation is not singular. <span class="math">\alpha, \beta</span>  are:</p>
<div class="math">
\begin{align} (\alpha, \beta) = \left( \frac{\frac{x_{0} y_{mint}}{2} + x_{in} y_{mint} - \frac{x_{mint} y_{0}}{2} - x_{mint} y_{in}}{x_{0} + 2 x_{in} + x_{mint}}, \  \frac{x_{0} y_{in} - x_{in} y_{0} - x_{in} y_{mint} + x_{mint} y_{in}}{x_{0} + 2 x_{in} + x_{mint}}\right) \end{align}
</div>
<p>The clearing price, <span class="math">p_c</span>, is:</p>
<div class="math">
\begin{align} 
p_c = \frac{y_{0} + 2 y_{in} + y_{mint}}{x_{0} + 2 x_{in} + x_{mint}}
\end{align}
</div>
<p><span class="math">x_\text{out}, y_\text{out}</span> are:</p>
<div class="math">
\begin{align}
(x_\text{out}, y_\text{out}) &amp;= 
\left( \frac{y_{in} \left(x_{0} + 2 x_{in} + x_{mint}\right)}{y_{0} + 2 y_{in} + y_{mint}}, \  \frac{x_{in} \left(y_{0} + 2 y_{in} + y_{mint}\right)}{x_{0} + 2 x_{in} + x_{mint}}\right) \\
&amp;= \left(\frac{y_\text{in}}{p_c}, p_c x_\text{in} \right) \\
\end{align} 
</div>
<p>It is straight forward to find <span class="math">x_2, y_2</span>, the reserves after minting LP tokens, and <span class="math">t</span>, the newly issued LP token amount, so we would skip on them here.</p>
<p>Above construction charges no fee. To keep price same even after charging fee, we will take <span class="math">1/(1 + \gamma)</span> portion of input and <span class="math">\gamma</span> portion of output as fee. So the effective fee rate will be   <span class="math">\frac{2 \gamma}{1+ \gamma}</span>, which is approximately <span class="math">2 \gamma</span>. Considering arbitrageurs it may better to take fee fully on input, though.</p>
<h1><a class="anchor" href="https://ethresear.ch#h-3-model-4" name="h-3-model-4"></a>3. Model</h1>
<p>In this section, we describe the model upon which our analysis is based. We model a normal form game involving strategic arbitrageurs. This means that each player is unaware of the bids of others, and all bids are submitted simultaneously. Additionally, each player’s bid is never censored. Although this assumption does not perfectly reflect the current state of blockchains, ongoing cryptographic developments and improved market designs, such as inclusion lists, will help bridge the gap between theory and reality. This formulation is almost the same as that of [CM24]; the only difference is that players now “take” mispriced liquidity instead of providing it to the AMM.</p>
<h2><a class="anchor" href="https://ethresear.ch#h-31-automated-market-maker-5" name="h-31-automated-market-maker-5"></a>3.1. Automated Market Maker</h2>
<p>For the AMM, we will use the FM-AMM introduced in Section 2. Note that the AMM itself is not a player; we assume that the LPs of the AMM are passive investors who will not take any action in the short term.</p>
<h2><a class="anchor" href="https://ethresear.ch#h-32-arbitrageurs-6" name="h-32-arbitrageurs-6"></a>3.2. Arbitrageurs</h2>
<p>We assume that all players are homogeneous. They are risk-neutral and can execute trades of any size and in any direction on CEX without any slippage. Their sole goal is to maximize profit.</p>
<h2><a class="anchor" href="https://ethresear.ch#h-33-strategic-game-of-liquidity-taking-7" name="h-33-strategic-game-of-liquidity-taking-7"></a>3.3. Strategic Game of Liquidity Taking</h2>
<p>First, we solve the game with <span class="math">N</span> players where <span class="math">N</span> is given exogenously, without considering transaction costs. Then, we introduce a strictly positive transaction cost <span class="math">c</span> and derive <span class="math">N</span> from the equilibrium condition. We will restrict our interest to conditions with positive trading fees, which guarantees the uniqueness of the equilibrium. Players observe the pool reserves <span class="math">X</span>, <span class="math">Y</span>, and the external true price <span class="math">P</span>. Then, they submit bids <span class="math">(x_i, y_i)</span>, which are the amounts of tokens to sell to the pool. The clearing price will be:</p>
<div class="math">
\begin{align}
P_c = \frac{Y + 2\sum^N_{i=1} y_i }{X + 2\sum^N_{i=1} x_i} \tag{1} \\
\end{align}
</div>
<p>The utility function is the arbitrage profit after charging the swap fee (and transaction cost, if applicable). The utility of player <span class="math">i</span>, <span class="math">U_i</span>, is:</p>
<div class="math">
\begin{align}
R_i = -(1 + \gamma)(P x_i + y_i) + (1 - \gamma)\left(\frac{P}{P_c}y_i + P_c x_i\right) \tag{2}
\end{align}
</div>
<p>Now, we are ready to find the equilibrium.</p>
<h1><a class="anchor" href="https://ethresear.ch#h-4-equilibrium-analysis-8" name="h-4-equilibrium-analysis-8"></a>4. Equilibrium Analysis</h1>
<h2><a class="anchor" href="https://ethresear.ch#h-41-n-is-determined-exogenously-and-transaction-cost-c-is-zero-9" name="h-41-n-is-determined-exogenously-and-transaction-cost-c-is-zero-9"></a>4.1. <span class="math">N</span> is Determined Exogenously, and Transaction Cost <span class="math">c</span> is Zero</h2>
<p>We first introduce the following lemma:</p>
<div class="math">
\text{Lemma. The player } i\text{'s best response is submitting a bid with at least one 0 component, that is, either } (x_i, 0) \text{ or } (0, y_i).
</div>
<p>The proof is straightforward. Assume <span class="math">(x_i, y_i)</span> and <span class="math">(x'_i, y'_i)</span> result in the same clearing price. Then <span class="math">x_i \leq x'_i</span> if and only if <span class="math">y_i \leq y'_i</span>. Combining these and subtracting the utility of one from the other yields the desired result.</p>
<p>Meanwhile, the first order condition and the profitability condition give us that the best response is, when <span class="math">P_{-i}</span> is defined as <span class="math">P_{-i} = \frac{Y + 2\sum^N_{j \neq i} y_j }{X + 2\sum^N_{j \neq i} x_j}</span>, submitting <span class="math">x_i</span> or <span class="math">y_i</span> such that the following holds:</p>
<div class="math">
\begin{align}
P_c = 
\begin{cases} 
\sqrt{\frac{1 - \gamma}{1 + \gamma} P P_{-i}} &amp; \text{if } \frac{1 - \gamma}{1 + \gamma} P \geq P_{-i} \\
\sqrt{\frac{1 + \gamma}{1 - \gamma} P P_{-i}} &amp; \text{if } \frac{1 + \gamma}{1 - \gamma} P \leq P_{-i}
\end{cases}. \tag{3}
\end{align}
</div>
<p>Otherwise, it is better not to submit any order (i.e., bid). One can think of <span class="math">\frac{1+\gamma}{1-\gamma}P_{-i}</span> and <span class="math">\frac{1-\gamma}{1+ \gamma}P_{-i}</span> as the threshold prices such that arbitrage becomes profitable. Note that this holds for every <span class="math">i</span>, so <span class="math">P_{-i} = P_{-j}</span> for every <span class="math">i</span> and <span class="math">j</span>, which tells us the equilibrium is symmetric and always exists.</p>
<p>From now on, we only consider the external price to be sufficiently higher than the pool’s spot price, <span class="math">Y/X</span>. The opposite case can be solved in a similar manner. It is clear that <span class="math">x_\text{eq} = 0</span> for the case we are dealing with. Then, <span class="math">(3)</span> is equivalent to:</p>
<div class="math">
\begin{align}
\frac{Y + 2Ny_\text{eq}}{X} = \sqrt{\frac{1-\gamma}{1+\gamma}P\cdot \frac{Y + 2 (N-1) y_\text{eq}}{X}} \tag{4}
\end{align}
</div>
<p>Solving <span class="math">(4)</span> yields that</p>
<div class="math">
\begin{align}
y_\text{eq} = \frac{1}{4N^2}\left[ (N - 1) \cdot \frac{1-\gamma}{1+\gamma} \cdot PX -2NY +  \sqrt{(N-1)^2 + 4N \cdot \frac{Y}{X} \cdot \frac{1+\gamma}{1-\gamma} \cdot \frac{1}{P}} \cdot \frac{1-\gamma}{1+\gamma}\cdot PX \right] \tag{5}
\end{align}
</div>
<p>From now on, we will proceed with radical approximations due to its complexity. Although we do not provide any rigorous proof for the validity of such approximations, we will see it works well in the simulations later. Let <span class="math">P_0 = \frac{Y}{X}</span> and <span class="math">\varepsilon = \frac{1-\gamma}{1+\gamma} \cdot \frac{P}{P_0} - 1</span>, that is, the price difference between the threshold price and the external price. Approximating <span class="math">y_\text{eq}</span> with <span class="math">\varepsilon</span> through a Taylor series gives us a simpler form:</p>
<div class="math">
\begin{align}
y_\text{eq} &amp;=  \frac{Y}{4N^2}\left[ (N-1) \cdot (1+ \varepsilon) - 2N +(1+\varepsilon)\sqrt{(N-1)^2 +\frac{4N}{1+\varepsilon}}\right] \tag{6} \\
&amp;\approx \frac{Y}{2(N+1)} \varepsilon + o(\varepsilon^2) \tag{7}
\end{align}
</div>
<p>Using <span class="math">(7)</span>, one can compute the profit of individual arbitrageurs and the total loss of the AMM against arbitrageurs:</p>
<div class="math">
\begin{align}
ARB &amp;\approx L\sqrt{P_0}\cdot\left(\frac{1+\gamma}{2(N+1)^2}\right)\cdot\varepsilon^2 \tag{8} \\
LVR &amp;\approx (1+\gamma)\cdot L\sqrt{P_0}\cdot\left(\frac{N}{2(N+1)^2}\right)\cdot\varepsilon^2 \tag{9}
\end{align}
</div>
<p>Thus, assuming the transaction cost is <span class="math">0</span>, for any <span class="math">N</span>, every <span class="math">N</span> arbitrageur will submit identical bids and they will share the profit equally, while each individual arbitrageur’s profit will decay by <span class="math">O(N^{-2})</span>. Moreover, as <span class="math">N </span> goes to infinity the clearing price <span class="math">P_c</span> converges to threshold price, and therefore the stationary distribution of price discrepancy will be as same as that of fixed fee rate CPMM in [MMR23].</p>
<h2><a class="anchor" href="https://ethresear.ch#h-42-transaction-cost-is-not-free-and-the-number-of-arbitrageurs-is-determined-endogenously-10" name="h-42-transaction-cost-is-not-free-and-the-number-of-arbitrageurs-is-determined-endogenously-10"></a>4.2. Transaction Cost is Not Free, and the Number of Arbitrageurs is Determined Endogenously</h2>
<p>Now we extend the model in 4.1 to a more realistic one by adopting a nonzero transaction cost <span class="math">c</span>. The utility function remains the same as in <span class="math">(2)</span>, except we have an additional term <span class="math">-c</span>. Since this term disappears when we take the derivative, the best response remains the same as long as it is profitable. Thus, the solution is not much different from <span class="math">(7)</span>, except <span class="math">N</span> is replaced with <span class="math">N^{*}</span>, where <span class="math">N^{*}</span> is the largest integer that satisfies <span class="math">L\sqrt{P_0}\cdot\left(\frac{1+\gamma}{2(N^{*}+1)^2}\right)\cdot\varepsilon^2 \geq c</span>. Then, the LVR will be:</p>
<div class="math">
\begin{align}
LVR &amp;\approx (1+\gamma) \cdot  L \sqrt{P_0} \cdot\varepsilon^2 \cdot \frac{N^{*}}{2(N^{*}+1)^2} \tag{10} \\
&amp;\approx cN^{*} \tag{11} \\
&amp;\approx c \left \lfloor \varepsilon\sqrt{\frac{1+\gamma}{2c} \cdot L \sqrt{P_0}}- 1 \right\rfloor \tag{12} \\
&amp;\leq \varepsilon\sqrt{(1+\gamma)2c \cdot L \sqrt{P_0}} \tag{13}
\end{align}
</div>
<h2><a class="anchor" href="https://ethresear.ch#h-43-comparison-with-cpmm-11" name="h-43-comparison-with-cpmm-11"></a>4.3. Comparison with CPMM</h2>
<p>The derivation of the LVR for CPMM has already been studied extensively, so we will simply present the result:</p>
<div class="math">
\begin{align}
LVR_\text{CPMM} \approx \frac{1}{1-\gamma} \cdot L \sqrt{P_0} \cdot \frac{\varepsilon^2}{4}, \tag{14}
\end{align}
</div>
<p>where <span class="math">\gamma</span> is the fee rate taken from the input and <span class="math">\varepsilon</span> is again the price difference between the external price and the threshold price, in this case, <span class="math">\frac{P_0}{1-\gamma}</span>. In short, the LVR of CPMM grows faster than that of FM-AMM as <span class="math">\varepsilon</span> (the price difference) and <span class="math">L\sqrt{P_0}</span> (the initial pool size) grow. From this, we can predict that the performance of FM-AMM will be better in larger pools compared to CPMM.</p>
<p>FM-AMM performance is affected by the transaction cost <span class="math">c</span>, while CPMM is not affected as long as the arbitrageur’s profit is greater than <span class="math">c</span>. This implies that FM-AMM suits well with rollup settings that have longer block times (resulting in higher volatility between blocks) and low transaction costs.</p>
<h1><a class="anchor" href="https://ethresear.ch#h-5-simulations-12" name="h-5-simulations-12"></a>5. Simulations</h1>
<p>Due to the nonzero transaction cost, finding an analytic solution for instantaneous LVR or the stationary distribution of price discrepancy is no longer straightforward. Therefore, we proceed with numerical simulations. You can check the code used <a href="https://github.com/kosunghun317/FMAMM_LVR/tree/main/notebooks" rel="noopener nofollow ugc">here</a>. This code is largely copy-pasted with minor tweaks from <a href="https://github.com/alexnezlobin/simulations/tree/main" rel="noopener nofollow ugc">this source</a>. Swap fees are fixed at 0.3% across all simulations (i.e., <span class="math">\gamma_\text{FMAMM} = 0.0015</span>, <span class="math">\gamma_\text{CPMM} = 0.003</span>).</p>
<h2><a class="anchor" href="https://ethresear.ch#h-51-distribution-of-lvr-13" name="h-51-distribution-of-lvr-13"></a>5.1. Distribution of LVR</h2>
<p>In this section, we observe the distribution of LVR under several cases without iterating over many parameters. Note that the variance is always greater in FM-AMM; this is because the price is not corrected perfectly under the nonzero transaction cost condition.</p>
<p>The conditions of the first case are L1 (12-second block time), $10 transaction cost, with 5% daily volatility and $100M pool size.<br />
<img alt="image" height="450" src="https://ethresear.ch/uploads/default/original/3X/1/0/101edc5f8de31543cda7c084f2d7e98e3e522ebf.png" width="604" /></p>
<p>The second is L1, $10 transaction cost, with 10% volatility and $100M pool size.<br />
<img alt="image" height="450" src="https://ethresear.ch/uploads/default/original/3X/a/4/a49fb0096777435725991cf46954abd43c9a26a8.png" width="597" /><br />
As predicted, FM-AMM outperforms CPMM as volatility increases.</p>
<p>Next one is L1 with congestion; transaction cost went up to $30.<br />
<img alt="image" height="450" src="https://ethresear.ch/uploads/default/original/3X/f/3/f3a2fc79c45044e5ef9c61b01a5ae50b913299d7.png" width="597" /><br />
This fits to our prediction well, too. As tx cost increases FM-AMM loses more than CPMM.</p>
<p>The last result is L1 with congestion, but with smaller liquidity ($10M).<br />
<img alt="image" height="450" src="https://ethresear.ch/uploads/default/original/3X/f/8/f82923769edf0a3f8df22d9c2327ee0473eae369.png" width="597" /><br />
This result is a bit contradictory to our initial guess: usually, smaller liquidity conditions are more favorable to CPMM, as LVR per pool value of FM-AMM increases as the pool value gets smaller. To clarify this, we will run simulations over various parameters and compare the performances.</p>
<h2><a class="anchor" href="https://ethresear.ch#h-52-performance-comparisons-14" name="h-52-performance-comparisons-14"></a>5.2. Performance Comparisons</h2>
<p>Below are the numerical simulations of LVR for CPMM and FM-AMM under various parameters. Swap fees are set at 0.3% for both of them. Blue regions indicate where CPMM performs better, while grey regions indicate where FM-AMM performs better. Note that the results in the low volatility and high-cost regions are not as reliable due to the very few trades occurring in these conditions.</p>
<p>First is the cases for L1:<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/f/3f4b7c4f8afa28b742eb494cce687ca245b9ded3.png" title="image"><img alt="image" height="196" src="https://ethresear.ch/uploads/default/optimized/3X/3/f/3f4b7c4f8afa28b742eb494cce687ca245b9ded3_2_690x196.png" width="690" /></a></div><br />
<div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/c/2c339052835747c3057c1c6d6b4456033db13814.png" title="image"><img alt="image" height="196" src="https://ethresear.ch/uploads/default/optimized/3X/2/c/2c339052835747c3057c1c6d6b4456033db13814_2_690x196.png" width="690" /></a></div><br />
<div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/1/21678b78eb6b694ff0938209d8de97d84a713df9.png" title="image"><img alt="image" height="196" src="https://ethresear.ch/uploads/default/optimized/3X/2/1/21678b78eb6b694ff0938209d8de97d84a713df9_2_690x196.png" width="690" /></a></div><br />
<div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/b/7b8453c6492353e30665d40174ad349c53cc73da.png" title="image"><img alt="image" height="196" src="https://ethresear.ch/uploads/default/optimized/3X/7/b/7b8453c6492353e30665d40174ad349c53cc73da_2_690x196.png" width="690" /></a></div><br />
<div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/a/ba04ccc1440d5207217bb4056c6d9cc2e2d03291.png" title="image"><img alt="image" height="196" src="https://ethresear.ch/uploads/default/optimized/3X/b/a/ba04ccc1440d5207217bb4056c6d9cc2e2d03291_2_690x196.png" width="690" /></a></div><br />
<div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/f/cf2266ce32541a90a009ee96c05f24714ebf7da6.png" title="image"><img alt="image" height="196" src="https://ethresear.ch/uploads/default/optimized/3X/c/f/cf2266ce32541a90a009ee96c05f24714ebf7da6_2_690x196.png" width="690" /></a></div><br />
<div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/a/daec56f47b5ff0e9cfe26e087f4588f0ce99ebe5.png" title="image"><img alt="image" height="196" src="https://ethresear.ch/uploads/default/optimized/3X/d/a/daec56f47b5ff0e9cfe26e087f4588f0ce99ebe5_2_690x196.png" width="690" /></a></div><br />
<div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/f/8f7211259a315e71a052cac3a52055de1eabbda8.png" title="image"><img alt="image" height="196" src="https://ethresear.ch/uploads/default/optimized/3X/8/f/8f7211259a315e71a052cac3a52055de1eabbda8_2_690x196.png" width="690" /></a></div><br />
<div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/c/cc23d7506e2cc23e5b29ae96faee7ed609588d70.png" title="image"><img alt="image" height="196" src="https://ethresear.ch/uploads/default/optimized/3X/c/c/cc23d7506e2cc23e5b29ae96faee7ed609588d70_2_690x196.png" width="690" /></a></div><br />
<div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/2/a2153b4ce8fb4ee42360ccff33df0b311aa457e4.png" title="image"><img alt="image" height="196" src="https://ethresear.ch/uploads/default/optimized/3X/a/2/a2153b4ce8fb4ee42360ccff33df0b311aa457e4_2_690x196.png" width="690" /></a></div><p></p>
<p>Following are the special cases for based rollup (tx cost = $0.05, block time = 12 sec) and typical L2s (tx cost = 0.01, block time = 2 sec), respectively:<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/4/e463cf577f86a2492c5fb4e0f15d30284212bf6d.png" title="image"><img alt="image" height="196" src="https://ethresear.ch/uploads/default/optimized/3X/e/4/e463cf577f86a2492c5fb4e0f15d30284212bf6d_2_690x196.png" width="690" /></a></div><br />
<div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/5/55c066b2d8b4c15d67d1eb5eb9ad1008d4840ecd.png" title="image"><img alt="image" height="196" src="https://ethresear.ch/uploads/default/optimized/3X/5/5/55c066b2d8b4c15d67d1eb5eb9ad1008d4840ecd_2_690x196.png" width="690" /></a></div><p></p>
<h2><a class="anchor" href="https://ethresear.ch#h-53-discussion-15" name="h-53-discussion-15"></a>5.3. Discussion</h2>
<p>It is clear that FM-AMM performs better under certain conditions, including L2s and L1 with low transaction costs, and this partially explains why the results in [CF23] was rather mixed and defer by each pair. Due to its nature of forcing competition over price between arbitrageurs, it performs well even in high volatility conditions. Notably, this is achieved without raising the swap fee, which typically results in losing retail order flow. Thus, FM-AMM can lose less to arbitrageurs while not sacrificing retail order flow.</p>
<h1><a class="anchor" href="https://ethresear.ch#h-6-conclusion-16" name="h-6-conclusion-16"></a>6. Conclusion</h1>
<p>As the authors of [CF23] claimed, FM-AMM indeed achieves superior performance under certain conditions, even without raising swap fees. It suits L2s particularly well. However, our analysis is based on several non-realistic assumptions, especially the (short-term) censorship resistance assumption and simultaneous bid submission. Future research will focus on more relaxed conditions to provide a more comprehensive evaluation.</p>
<h1><a class="anchor" href="https://ethresear.ch#h-7-references-17" name="h-7-references-17"></a>7. References</h1>
<p>[MMRZ22] J. Milionis, C. C. Moallemi, T. Roughgarden, and A. L. Zhang. Automated Market Making and Loss-Versus-Rebalancing, <em>arXiv preprint <a href="https://arxiv.org/abs/2208.06046" rel="noopener nofollow ugc">arXiv:2208.06046</a></em>, 2022.<br />
[MMR23] J. Milionis, C. C. Moallemi, and T. Roughgarden. Automated Market Making and Arbitrage Profits in the Presence of Fees, <em>arXiv preprint <a href="https://arxiv.org/abs/2305.14604" rel="noopener nofollow ugc">arXiv:2305.14604</a></em>, 2023.<br />
[GGMR22] G. Ramseyer, M. Goyal, A. Goel, and D. Mazières. Augmenting Batch Exchanges with Constant Function Market Makers, <em>arXiv preprint <a href="https://arxiv.org/abs/2210.04929" rel="noopener nofollow ugc">arXiv:2210.04929</a></em>, 2022.<br />
[CF23] A. Canidio and A. Fritsch. Arbitrageurs’ profits, LVR, and sandwich attacks: batch trading as an AMM design response, <em>arXiv preprint <a href="https://arxiv.org/abs/2307.02074" rel="noopener nofollow ugc">arXiv:2307.02074</a></em>, 2023.<br />
[CM24] D. Crapis and J. Ma. The Cost of Permissionless Liquidity Provision in Automated Market Makers, <em>arXiv preprint <a href="https://arxiv.org/abs/2402.18256" rel="noopener nofollow ugc">arXiv:2402.18256</a></em>, 2024.<br />
[N22] A. Nezlobin. Ethereum Block Times, MEV, and LP returns, <em>Medium article <a href="https://medium.com/@alexnezlobin/ethereum-block-times-mev-and-lp-returns-5c13dc99e80" rel="noopener nofollow ugc">Ethereum Block Times, MEV, and LP returns</a></em>, 2022<br />
[E24] A. Elsts. CEX/DEX arbitrage, transaction fees, block times, and LP profits, <em>Ethresearch Forum article <a href="https://ethresear.ch/t/cex-dex-arbitrage-transaction-fees-block-times-and-lp-profits/19444">CEX/DEX arbitrage, transaction fees, block times, and LP profits</a></em>, 2024</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/notes-on-the-lvr-of-fm-amm/20151">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 26 Jul 2024 06:21:02 +0000</pubDate>
</item>
<item>
<title>A design for APS-burn in the context of a Decentralized L2</title>
<link>https://ethresear.ch/t/a-design-for-aps-burn-in-the-context-of-a-decentralized-l2/20146</link>
<guid>https://ethresear.ch/t/a-design-for-aps-burn-in-the-context-of-a-decentralized-l2/20146</guid>
<content:encoded><![CDATA[
<h1><a class="anchor" href="https://ethresear.ch#aps-burn-in-the-context-of-a-decentralized-l2-1" name="aps-burn-in-the-context-of-a-decentralized-l2-1"></a>APS-burn in the context of a Decentralized L2</h1>
<h1><a class="anchor" href="https://ethresear.ch#overview-2" name="overview-2"></a>Overview</h1>
<p>We propose a design for Attester-Proposer-Separation that is tailored for the context of a decentralized L2. This design is intended to operate in the context of an L2 with its own validator set, running some sort of BFT consensus protocol, with single slot finality.</p>
<p>This design is based on the <a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ" rel="noopener nofollow ugc">APS-burn design</a> from <a class="mention" href="https://ethresear.ch/u/barnabe">@barnabe</a>, but with some notable differences. It assumes that there are short block times, preferably one second, and no longer than 2 seconds, and that each block is final within the scope of the canonical L2 chain (prior to being finalized on L1) . This design aims to obtain the benefits of APS in an L2 context, while aiming to mitigate censorship, and mitigate the negative externalities of multi-block MEV. These properties are achieved using a sealed-bid auction, similar in principle to the <a href="https://ethresear.ch/t/sealed-execution-auction/20060">Sealed execution auction</a> proposal from Anders, but in an L2 context.</p>
<p>To understand the motivation behind this design, as well as its trade-offs, see the “benefits” and “risks” sections below.</p>
<p>DO NOT read this post if:</p>
<ul>
<li>You are trying to keep up with the important developments in Ethereum and attempting to determine which posts are important and which aren’t. This post is intended for soliciting early feedback on a design that is specific to decentralized rollups, and is not a finalized proposal.</li>
</ul>
<p>DO read this post if:</p>
<ul>
<li>You are trying to decentralize a rollup, and are considering adopting an PoS consensus protocol, and are interested in exploring ideas within the design space.</li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#related-reading-3" name="related-reading-3"></a>Related Reading</h1>
<ul>
<li><a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ" rel="noopener nofollow ugc">More pictures about proposers and builders - Barnabé Monnot</a></li>
<li><a href="https://arxiv.org/abs/2301.13321" rel="noopener nofollow ugc">Censorship Resistance in On-Chain Auctions - Elijah Fox, Mallesh Pai, Max Resnick</a></li>
<li><a href="https://ethresear.ch/t/sealed-execution-auction/20060">Sealed execution auction - Anders Elowsson</a></li>
<li><a href="https://ethresear.ch/t/on-block-space-distribution-mechanisms/19764">On block-space distribution mechanisms - Mike Neuder</a></li>
<li><a href="https://mirror.xyz/0x03c29504CEcCa30B93FF5774183a1358D41fbeB1/CPYI91s98cp9zKFkanKs_qotYzw09kWvouaAa9GXBrQ" rel="noopener nofollow ugc">Block vs. Slot Auction PBS - Julian Ma<br />
</a></li>
</ul>
<p>MEV Burn related reading:</p>
<ul>
<li><a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590">MEV burn—a simple design</a></li>
<li><a href="https://ethresear.ch/t/the-price-is-right-realigning-proposer-builder-incentives-with-predictive-mev-burn/18656">The price is right: Realigning proposer-builder incentives with predictive MEV-burn</a></li>
<li><a href="https://ethresear.ch/t/dr-changestuff-or-how-i-learned-to-stop-worrying-and-love-mev-burn/17384">Dr. changestuff or: how i learned to stop worrying and love mev-burn</a></li>
<li><a href="https://ethresear.ch/t/in-a-post-mev-burn-world-some-simulations-and-stats/17092">In a post MEV-Burn world - Some simulations and stats</a></li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#description-4" name="description-4"></a>Description</h1>
<p>We propose a method whereby the right to propose a future block is obtained via an on-chain auction.</p>
<p>For every slot <span class="math">n</span>, the auction for block proposal rights starts at slot <span class="math">n - t</span> and runs for <span class="math">k</span> slots. The auction closes at slot <span class="math">(n - t) + k</span>. During the period between <span class="math">n - t</span> and <span class="math">(n - t) + k</span>, bids are submitted to an on-chain smart contract. Each bid specifies an amount of some defined token that will be burned as part of the block that will be proposed at slot <span class="math">n</span>. The winning bid is the bid that burns the most tokens.</p>
<p>During the auction between slot <span class="math">n - t</span> and <span class="math">(n - t) + k</span>, bids are posted on-chain as sealed commitments. After the auction closes at slot <span class="math">k</span>, there is a buffer period of <span class="math">b</span> blocks in which no new bids are accepted by the smart contract for slot <span class="math">n</span>. After this buffer period, and up to slot <span class="math">n</span>, bidders post their opened commitments, which reveal the amount they are bidding. The block that is proposed to the network at slot <span class="math">n</span>, must be from the same address specified in the highest bid in the auction for slot <span class="math">n</span>, and also burn the amount of tokens specified in the bid.</p>
<p>Each bid is composed of the height of the slot being bidded on, the address that will propose the block, and an amount of MEV that will be burned in the block.</p>
<h3><a class="anchor" href="https://ethresear.ch#mitigating-multi-block-mev-5" name="mitigating-multi-block-mev-5"></a>Mitigating multi-block MEV</h3>
<p>By incorporating a sealed bid auction, we can mitigate concerns around multi-block MEV. One of the main concerns with various APS designs is that it allows bidders to bid on block proposal rights for a contiguous segment of slots. If a bidder knows that they have the rights to slot <span class="math">n</span>, then they can bid higher than anyone else for slot <span class="math">n+1</span>, because they know that they can employ lucrative multi-block MEV strategies such as censoring price oracle updates or censoring sell orders on a trading pair to drive up the price etc.</p>
<p>In order to mitigate this concern, it is imperative that the bidders have no guarantee of having won the auction for slot <span class="math">n</span> while the auction for slot <span class="math">n+1</span> is open.</p>
<p>As an illustrative example, consider the following instantiation where bidders bid for the right to propose a block 12 slots in the future <span class="math">(t = 12)</span>, and they have 4 slots in which to submit bids <span class="math">(k = 4)</span>, followed by a buffer phase in which the on-chain auction will not accept bids <span class="math">(b = 2)</span> followed by the reveal phase.</p>
<p>As you can see from the following visualization, is we assume that all bids for the slot <span class="math">n</span> auction are revealed at slot <span class="math">(n - t) + k + b</span> then the bidder for slot <span class="math">n</span> only finds out that they have won block proposal rights for slot <span class="math">n</span> after the auction for slot <span class="math">n+1</span> and slot <span class="math">n+2</span> have already closed.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/2/12de37869ca318f21a59cb84c3ddab8f309c90ee.png" title="L2_APS-burn"><img alt="L2_APS-burn" height="280" src="https://ethresear.ch/uploads/default/optimized/3X/1/2/12de37869ca318f21a59cb84c3ddab8f309c90ee_2_690x280.png" width="690" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#censorship-6" name="censorship-6"></a>Censorship</h3>
<p>Censorship is a concern with any on-chain auction (ref: <a href="https://arxiv.org/abs/2301.13321" rel="noopener nofollow ugc">Censorship Resistance in On-Chain Auctions</a>). Obviously block builders are highly incentivized to censor any transactions to the on-chain auction that that carry bids that aren’t their own, which means that the only bids that will make it to the on-chain contract are from block builders that already have proposal rights to slots, as these builders will likely only include their own transactions to the auction contract.</p>
<p>The only way to fully mitigate censorship is through some form of <a href="https://eips.ethereum.org/EIPS/eip-7547" rel="noopener nofollow ugc">inclusion lists</a>, or a design that facilitates <a href="https://ethresear.ch/t/concurrent-block-proposers-in-ethereum/18777">multiple concurrent block proposers</a>. We propose that this sort of mechanism is an integral part of this design, but the exact details of the mechanism employed are out of scope for this piece.</p>
<p>However, even without an inclusion list / MCP mechanism, censorship of auction transactions becomes prohibitively expensive quite quickly. This is because every transaction that is censored has associated transaction fees that can be collected by some other block builder, which they can use to increase their bids with. The censoring block builder will therefore incur a competitive disadvantage for every bid they censor. Moreover, the censoring block builder will incur the cost of each bid they censor for every block they propose, resulting in a linear increase in cost over time. In other words, If <span class="math">n</span> blocks are proposed, and <span class="math">k</span> transactions are censored per block, the total cost incurred by the censoring block builder becomes:</p>
<p><span class="math">CoC=n\times\sum_{i=1}^{k}C_{i}</span></p>
<h3><a class="anchor" href="https://ethresear.ch#collateralization-and-penalties-7" name="collateralization-and-penalties-7"></a>Collateralization and Penalties</h3>
<p>This design requires that bidders are collateralized in order to submit bids, and that this collateral is slashed under certain circumstances:</p>
<ul>
<li>If bid commitments are not revealed, this can incur penalties. The reason for this is to prevent bidders from submitting multiple bids and then just revealing them conditionally based on what other bidders reveal (as detailed in <a href="https://arxiv.org/pdf/2301.12532" rel="noopener nofollow ugc">this paper</a> - h/t <a class="mention" href="https://ethresear.ch/u/quintuskilbourn">@quintuskilbourn</a> for this). Obviously censorship resistance is important in order to prevent these penalties from being used for griefing attacks.</li>
<li>If the winner of an auction for slot <span class="math">n</span>, does not propose a block for slot <span class="math">n</span>, they are slashed.</li>
<li>If the winner of an auction for slot <span class="math">n</span>, equivocates and proposes more than one block for slot <span class="math">n</span>, they are slashed.</li>
<li>If the proposed block is valid, and is from the auction winner, but does not burn the amount of MEV that was stipulated in the winning bid, the collateral is slashed.</li>
</ul>
<p>There are two ways to approach collateralization:</p>
<h4><a class="anchor" href="https://ethresear.ch#h-1-per-bidder-collateralization-8" name="h-1-per-bidder-collateralization-8"></a>1 | Per-Bidder-Collateralization</h4>
<p>This requires that a block builders / bidders posts some collateral on-chain, and that this will be subject to slashing conditions. Once the collateral is posted, the bidder can participate in any number of auctions and submit any number of bids. The collateral can be withdrawn at any stage, but is subject to some defined delay period.</p>
<h4><a class="anchor" href="https://ethresear.ch#h-2-per-bid-bonding-9" name="h-2-per-bid-bonding-9"></a>2 | Per-Bid-Bonding</h4>
<p>Bidders do not need to be collateralized, but each individual bid will require a bond. In the case of the winning bid, the bond is returned when the block for the slot is delivered. In the case of not winning the bid, the bond is returned only if the bid commitment was revealed.</p>
<p>As a side note: per-bid-bonding can also potentially be used to prevent bids being revealed earlier through some side-channel, by allowing anyone to cancel their bid before the auction closes and withdraw their bond if they reveal the pre-image. Once the auction is closed, then only the original bidder can withdraw the bond.</p>
<p>There are subtle trade-offs between the two approaches:</p>
<ul>
<li>
<p>Per-bid-bonding could potentially be more centralizing, as it favors better capitalized bidders. With a slot <span class="math">n+t</span> auction with a per-bid bond of <span class="math">S</span>, then bidders will need <span class="math">t \times S</span> to participate in every auction.</p>
</li>
<li>
<p>On the other hand, this potentially improves censorship resistance to a degree, as the same bidder can bid from different addresses, reducing the scope for targeted censorship of specific rival block builders.</p>
</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#preventing-bids-from-being-revealed-early-10" name="preventing-bids-from-being-revealed-early-10"></a>Preventing bids from being revealed early</h3>
<p>It’s not entirely clear what the incentives would be for bidders to reveal their bids early, but the effect of revealing bids early will undermine the value of a sealed-bid auction, and will allow for multi-block MEV strategies to be employed. We can imagine a scenario whereby somebody constructs a mechanism employing ZKPs to allow bidders to reveal their bids, in order to understand if their bid is lower than another bid, which would give them the option to bid higher. This could be a useful tool for participants in the auction.</p>
<p>To mitigate against the risks of bidders revealing their bids early, it should be impossible, or very hard, to prove what the bid was. There are a number of ways of accomplishing this:</p>
<h4><a class="anchor" href="https://ethresear.ch#using-threshold-encryption-11" name="using-threshold-encryption-11"></a>Using threshold encryption</h4>
<p>The validators will use distributed-key-generation (DKG) to create a threshold encryption key, which is part of the headers for every block. The BFT round leader will also be responsible for collecting the keys from validators, posting the encryption key, and also gossipping the decryption key at the right time, so that it can also be included in the block headers. This will allow bidders to encrypt their bids when they are posted on-chain. It will also allow them to decrypt their bids locally to ascertain if they have won the block proposal rights for slot <span class="math">n</span>. At this stage it should be deterministically known to all parties who have won the slot <span class="math">n</span> auction.</p>
<p>Upon receiving a new block for slot <span class="math">n</span>, validators will examine the amount of MEV burned in the block as well as the address of the proposer. They will take these two pieces of data and encrypt them using the threshold encryption key for the auction for slot <span class="math">n</span>. If there is a bid that exactly matches the ciphertext, and that bid is from the proposer that is proposing the block, and is correctly collateralized, and most importantly, if there is no higher bid in the auction, then that block is accepted. This construction can be strengthened by imposing slashing conditions on entities that propose blocks that do not have a winning bid associated with it.</p>
<p>The benefit of this approach is that it precludes any possibility of revealing bids early, assuming an honest majority of validators. However, it does add some extra complexity to the consensus layer, as well as the overhead of establishing clear and reliable public transmission of threshold encryption keys.</p>
<h4><a class="anchor" href="https://ethresear.ch#using-a-verifiable-delay-function-12" name="using-a-verifiable-delay-function-12"></a>Using a Verifiable Delay Function</h4>
<p>In order to reveal a commitment, the smart contract must verify an accompanying Verifiable Delay Function (VDF) proof. The VDF ensures that any bid must take at least <span class="math">d</span> seconds to produce a proof for. While there is nothing to stop bidders revealing their bids, it makes it difficult for bidders to prove what they bid, as the proof will take approximately <span class="math">d</span> seconds to produce.</p>
<p>There are multiple VDF schemes that can be employed. Such a scheme was proposed by Nomadic Labs (see <a href="https://eprint.iacr.org/2023/977.pdf" rel="noopener nofollow ugc">Timed Commitments Revisited</a>).</p>
<p>Note that in this scheme, the commit binding is deterministic, so not completely resilient to revealing bids. In the specific scheme, if the bidder shares the values used to generate the commitment (i.e., <span class="math">G</span>, <span class="math">g</span>, <span class="math">e</span>, <span class="math">k</span>, and <span class="math">ct</span>), others can reproduce the commitment <span class="math">\psi</span>, thereby revealing the bid. Further work is needed to understand the complexity involved in doing this in a ZKP, in order to understand whether the complexity is sufficient to discourage revealing of bids. If needed, we would change the scheme to use a key derivation function that is suboptimal for use within zk circuits, resulting in inefficient proof generation, and therefore a similar level of effort required to create the actual VDF proof.</p>
<p>Note that while it is possible to just use VDFs by themselves without the complexity of a commit-reveal scheme, this has the drawback of allowing bidders to produce multiple VDFs concurrently in order to retain the option of conditional bidding.</p>
<h1><a class="anchor" href="https://ethresear.ch#risks-concerns-13" name="risks-concerns-13"></a>Risks / Concerns</h1>
<h4><a class="anchor" href="https://ethresear.ch#reduced-competitiveness-in-bidding-14" name="reduced-competitiveness-in-bidding-14"></a>Reduced Competitiveness in Bidding</h4>
<p>Bids are a bet on averages, this can potentially have more centralizing effects than a JIT block auction, because it precludes any opportunistic MEV strategies that capitalize on MEV spikes, which could prevent block builders that exist on these strategies from participating. Also, because it is a bet on averages, the system may favor the most well capitalized block builders.</p>
<p>Also, because we are using a sealed-bid auction, participants are not bidding in response to each other’s bids. This removes the natural competitiveness that drives up prices, and so the overall level of bidding is likely to be somewhat lower.</p>
<h4><a class="anchor" href="https://ethresear.ch#l2-reorg-resistance-15" name="l2-reorg-resistance-15"></a>L2 reorg resistance</h4>
<p>This design assumes a BFT consensus protocol with single-slot-finality, wherein reorgs do not occur in the normal case. If reorgs are a concern, one can adapt the above design to include a second buffer phase at the end of the reveal phase but before slot <span class="math">n</span>. This would force any incentivized reorg to be at least as deep as the size of the second buffer phase, making it much more expensive, and so disincentivizing malicious reorgs.</p>
<h1><a class="anchor" href="https://ethresear.ch#benefits-16" name="benefits-16"></a>Benefits</h1>
<h4><a class="anchor" href="https://ethresear.ch#the-benefit-of-aps-is-that-there-is-no-longer-a-requirement-for-mev-boost-relays-17" name="the-benefit-of-aps-is-that-there-is-no-longer-a-requirement-for-mev-boost-relays-17"></a>The benefit of APS is that there is no longer a requirement for mev-boost relays</h4>
<p>The reason is that there is no negotiation between proposers and relayers (in terms of the proposer being the BFT round leader, who proposes blocks to the validator set). In the mev-boost scenario, the relayers are required in order to give some assurance to the builder that the proposer will not unbundle their block and steal the MEV, and also to give assurance to the proposer that the builder will in fact release the block on time, and not cause the proposer to get slashed. This is necessary to maintain PBS (unless ePBS is implemented), without which searcher bots will engage in PGAs which will cause significant and adverse network congestion.</p>
<h4><a class="anchor" href="https://ethresear.ch#it-reduces-the-centralizing-effects-of-mev-on-the-validator-set-18" name="it-reduces-the-centralizing-effects-of-mev-on-the-validator-set-18"></a>It reduces the centralizing effects of MEV on the validator set</h4>
<p>While mev-boost already does this in terms of democratizing access to MEV, there are still some centralizing effects from having MEV flowing to validators. For example, co-locating validator nodes close to relays means that validators can benefit from reduced latency and the higher bids that emerge in the final milliseconds of the slot. This latency advantage has compelling economies of scale for larger staking pools, which drives both economic and geographic centralization.</p>
<h4><a class="anchor" href="https://ethresear.ch#strengthens-pos-tokenomic-design-19" name="strengthens-pos-tokenomic-design-19"></a>Strengthens PoS tokenomic design</h4>
<p>For L2s that maintain their own gas token, APS simplifies the modeling of token rewards and penalties with regards to the validator set. This is because MEV no longer flows to the validators, which makes the validator risk/reward profile more deterministic and easier to reason about. Validators will just receive rewards as designed by the protocol and nothing more, which makes it easier to design PoS tokenomics. APS-burn also acts as a natural token sink, strengthening the tokenomics by having a deflationary effect on the token itself.</p>
<hr />
<h1><a class="anchor" href="https://ethresear.ch#future-work-20" name="future-work-20"></a>Future Work</h1>
<p>As well as soliciting early feedback and peer review, we plan to work on determining how best to model this design so that we can understand the trade-offs in the design choices such as threshold encryption or VDFs, parameterization of the on-chain auction, per-bidder-collateralization or per-bid-bonding, and to understand the extent to which we can confidently predict the behavior of participants.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/a-design-for-aps-burn-in-the-context-of-a-decentralized-l2/20146">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 25 Jul 2024 10:16:16 +0000</pubDate>
</item>
<item>
<title>The case for decentralization increasing efficiency is overstated</title>
<link>https://ethresear.ch/t/the-case-for-decentralization-increasing-efficiency-is-overstated/20140</link>
<guid>https://ethresear.ch/t/the-case-for-decentralization-increasing-efficiency-is-overstated/20140</guid>
<content:encoded><![CDATA[
<p>Block-building on Ethereum has become quite centralized. 90% of blocks are auctioned off through MEV-Boost. Numerous solutions have been proposed, including anonymous inclusion lists and execution tickets. People are concerned about this, both for essentially ideological reasons, and for reasons of efficiency. Blockchains have an ethos of being open to all people, whether or not that is maximally efficient. There is a tradeoff between efficiency (in the sense of getting each block built in the most efficient way, by the most efficient builders) and “fairness”, or including all transactions, if people’s use of the chain is unaffected by the degree of centralization. If blockchain users are concerned their transactions will eventually be sanctioned and rendered worthless, they may avoid that blockchain, or avoid cryptocurrencies altogether. Thus, seemingly inefficient decentralization may be optimal for the blockchain as a whole, and would be unanimously preferred by all blockbuilders to the present equilibrium.</p>
<p>I am concerned, however, that the efficiency case is overstated. Imagine there is a firm so efficient at MEV extraction that they build all of the blocks on chain. If them doing so would cause people to leave the blockchain altogether, then they are incentivized to not bid on some blocks at all.</p>
<p>In “<a href="https://ethresear.ch/t/on-block-space-distribution-mechanisms/19764">On block-space distribution mechanisms</a>”, Neuder, Garavmidi, and Roughgarden propose execution-tickets as a mechanism for distributing block-building rights, using a proportional all-pay auction. Bidders buy lottery tickets for the right to build a block. In the example given, they have two buyers, buyer 1 with value 4, and buyer 2 with value 2. Under a perfectly efficient system, buyer one always gets the block, at price 2+epsilon. Under their all-pay system, buyer 1 bids 8/9th and buyer 2 bids 4/9th, with them receiving the block rights 2/3rds and 1/3rd of the time, respectively.</p>
<p>Under the description of the example, however, this necessarily <em>cannot</em> improve efficiency. If excessive centralization would scare away some users from using the chain at all, the winning monopolist is incentivized to give away some of the block. The value of efficiency is already reflected in their valuations. If you assume that their valuation is always higher, then you are assuming that there is no efficiency case whatsoever. You only have an ideological case, which is fine on its own terms — but you should not mix and match arguments which overstate your case. Note too that we are only caring about one side of the ledger, those who want their transactions to be included. Mightn’t it be possible that some people are repulsed by crypto’s shady reputation?</p>
<p>Nor should this necessarily apply in cases of monopolistic competition. To simply not bid is not the only way to redistribute blocks. If it were the case, the main block-builders would indeed be stuck in a prisoner’s dilemma — they could choose not to bid, but they would have to all do it. If, however, the winners hold another auction for the block, with some of the fairness raising characteristics as before, they can decentralize to the extent which is optimal for them.The drawback is that now the builders internalize a smaller portion of the gains. There is a free-rider problem with decentralization. However, as the market becomes more decentralized, doubtless people will be less concerned about censorship.</p>
<p>The efficiency argument for decentralization therefore much smaller than it would appear. There should probably be a split between allocatively efficient auctions for blocks, and allocatively fair but inefficient markets. What is the right split between the two? It is highly unlikely that it is optimal to only sell blocks in one way all the time.</p>
<p>I think that this is an ideal question for a prediction market. The right amount of decentralization is a macro question. You’re not going to be able to A/B test it in a couple days. Your choices are trying to influence people’s choice in the long run, and answer the question: what is the long run amount of decentralization that maximizes the amount of capital put on the chain. Is there any better use of prediction markets than this? I am somewhat agnostic to the exact method of determining the split — and have no opinions whatsoever as to the proper proportion.</p>
<p><em>This post was first posted on my blog <a href="https://nicholasdecker.substack.com/p/the-case-for-blockchain-decentralization" rel="noopener nofollow ugc">here</a>. Thank you for reading this, please tell me if you disagree.</em></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/the-case-for-decentralization-increasing-efficiency-is-overstated/20140">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 24 Jul 2024 18:48:55 +0000</pubDate>
</item>
<item>
<title>Builder Bidding Behaviors in ePBS</title>
<link>https://ethresear.ch/t/builder-bidding-behaviors-in-epbs/20129</link>
<guid>https://ethresear.ch/t/builder-bidding-behaviors-in-epbs/20129</guid>
<content:encoded><![CDATA[
<p>Special thanks to <a class="mention" href="https://ethresear.ch/u/soispoke">@soispoke</a> for the review</p>
<h1><a class="anchor" href="https://ethresear.ch#background-1" name="background-1"></a>Background</h1>
<p>Builder bidding strategies in the MEV-Boost world have been studied extensively over some time. Numerous <a href="https://arxiv.org/html/2312.14510v3" rel="noopener nofollow ugc">excellent resources</a>, <a href="https://arxiv.org/abs/2407.13931" rel="noopener nofollow ugc">literature</a>, <a href="https://ethresear.ch/t/game-theoretic-model-for-MEV-Boost-auctions-mma/16206">game-theoretic models</a>, and <a href="https://collective.flashbots.net/t/MEV-Boost-builder-bids-archive/3561" rel="noopener nofollow ugc">archives</a> capture the current builder bidding behaviors on how to win block building right for an Ethereum slot. Today, builder bidding war for MEV-Boost is a complex interplay between latencies, relays, and strategy effectiveness. In this post, we argue that builder bidding strategies become simpler in ePBS world and we highlight the key differences in how bidding strategies change under the new ePBS market space rules, strategy limitations, and reduced latency benefits in ePBS.</p>
<h1><a class="anchor" href="https://ethresear.ch#market-spaces-2" name="market-spaces-2"></a>Market Spaces</h1>
<p>Here, we summarize three types of market spaces. The first one is MEV-Boost. The second and third ones are ePBS. MEV-Boost is push + pull based market space, meaning the builders push the bids to the relays, and the proposer pulls the bids from the relays. ePBS contains two types of market spaces: the P2P Bid Gossip Netwok, which is push-based, and the Builder RPC Endpoint, which is pull-based.</p>
<ul>
<li><strong>MEV-Boost market space</strong>
<ul>
<li><strong>Push + pull-based</strong>: The builders push bids to the relay, and the proposer pulls the bids from the relay.</li>
</ul>
</li>
<li><strong>ePBS market spaces</strong>
<ul>
<li><strong>P2P market space</strong>
<ul>
<li><strong>Push-based</strong>. The builder pushes the bid to the p2p network.</li>
</ul>
</li>
<li><strong>Builder RPC market space</strong>
<ul>
<li><strong>Pull-based</strong>. The proposer pulls the bids from the builder RPC end points.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>We define the following market space characteristics given how the consensus <a href="https://github.com/ethereum/consensus-specs/pull/3828" rel="noopener nofollow ugc">spec</a> is written today. Builder-API is still <strong>TBD</strong> for ePBS.</p>
<h2><a class="anchor" href="https://ethresear.ch#mev-boost-market-space-3" name="mev-boost-market-space-3"></a>MEV-Boost Market Space</h2>
<ul>
<li><strong>Open auction</strong>: Builders that subscribe to the relay’s feed can see the every builder’s latest bid.</li>
<li><strong>Continuous auction</strong>: Builders can bid multiple times and cancel previous bids.</li>
<li><strong>Auction termination</strong>: The auction terminates when the proposer calls <code>getHeader</code> and when the relay returns the header to the proposer to sign. The relay may delay the header response for a timing game. This means the relayer has the final control over when the auction terminates.</li>
<li><strong>Profit sharing</strong>: Some relays take the difference between the winning bid and the second-highest bid received from builders. This difference goes to the relay, with a portion potentially refunded to the builder. This transforms the auction dynamic into a second-price auction. However, not all relays adopt this approach, and complete trust in the relay is mandatory.</li>
<li>We assume the market space doesn’t verify block contents from the builder, hence it is an <a href="https://github.com/michaelneuder/optimistic-relay-documentation/blob/4fb032e92080383b7b5d8af5675ef2bf9855adc3/towards-ePBS.md" rel="noopener nofollow ugc"><strong>optimistic market space</strong></a>. The only delay is when the builder sends the block to the relay.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#epbs-p2p-market-space-4" name="epbs-p2p-market-space-4"></a>ePBS P2P Market Space</h2>
<ul>
<li><strong>Open auction</strong>: Anyone can subscribe and listen to the P2P network for gossiped builder bids.</li>
<li><strong>Single bid auction</strong>: To prevent DOS attacks on the P2P network, the current spec only allows builders to submit a single bid and above a certain minimum value. Any subsequent bid will be dropped by the nodes. There is no cancellation support over the P2P network.</li>
<li><strong>Auction termination</strong>: The auction terminates when the proposer proposes the block which includes the builder’s bid. The proposer could play a timing game here and has the final control over when the auction terminates.</li>
<li><strong>Profit sharing</strong>: The bid specifies the value, and the proposer gets the full value on the consensus layer as long as the consensus block that includes the bid remains canonical. There’s no profit sharing with 3rd parties.</li>
<li>The market space is still <strong>optimistic</strong> and doesn’t need to verify the execution contents at inclusion time. If the execution block later becomes invalid or fails to reveal, the proposer still gets unconditional payment. The only delay here is the builder sending the bid to the P2P network. This delay is argubly <strong>longer</strong> than using a relay in MEV-Boost market space.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#epbs-builder-rpc-market-space-5" name="epbs-builder-rpc-market-space-5"></a>ePBS Builder RPC Market Space</h2>
<p>Note: The <a href="https://github.com/ethereum/builder-specs" rel="noopener nofollow ugc">Builder API</a> is undefined at this moment. This section is based on what we think the ePBS Builder API might look like, but it’s highly subjective to change and open for feedback. Below outlines one version of Builder API which we have been thinking.</p>
<ul>
<li><strong>Private auction</strong>: Only the proposer can request a bid from the builder. The proposer will sign the <code>getHeader</code> request using the builder’s public key. The builder’s bid remains private until requested by the proposer. Builders can’t sniff other builders’ bids unless the builder API allows this or the builder voluntarily opens their bids to the public.</li>
<li><strong>Single</strong> (maybe multiple?) <strong>bid auction</strong>: Builders allow proposers to request a bid once, and any subsequent requests will result in an error. Builders may also allow proposers to request bids multiple times without error; this specific detail is undefined, and it’s unclear what the Nash outcome is here. If builders allow multiple requests, then the builder must ensure previous bids are canceled.</li>
<li><strong>Auction termination</strong>: The auction terminates when the proposer requests the header and the proposer receives the header. The builder can play a timing game, but this may backfire and lead to the proposer using another builder’s bid. Builder timing game will not work here, but proposer timing games are still relevant.</li>
<li><strong>Profit sharing</strong>: Same as the P2P market space.</li>
<li>The market space is still <strong>optimistic</strong>, and the delay here is the builder returning the bid to the proposer. This delay is shorter than the P2P market space and likely the same as MEV-Boost if the builder is well co-located.</li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#builder-bidding-profiles-under-epbs-6" name="builder-bidding-profiles-under-epbs-6"></a>Builder Bidding Profiles under ePBS</h1>
<p>In the <a href="https://arxiv.org/abs/2312.14510" rel="noopener nofollow ugc">Strategic Bidding Wars in On-chain Auctions</a>, four profiles of builder behavior are listed in MEV-Boost auction:</p>
<ul>
<li><strong>Naive Behavior</strong>: Aggressively updates bids based on their valuation as long as the aggregated signal surpasses their profit margin.</li>
<li><strong>Adaptive Behavior</strong>: Monitors the current highest bid and places a bid if able to outbid by a small constant. Defaults to the naive strategy if unable to outbid.</li>
<li><strong>Last Minute Behavior</strong>: Reveals valuation at the final possible moment before auction termination to minimize the reaction window for other players.</li>
<li><strong>Bluff Behavior</strong>: Initially places high bids (bluff) and later reverts to actual valuation, leveraging bid cancellation to compel other players to disclose their valuations.</li>
</ul>
<p>Given the new market space in ePBS, we will examine which strategies are viable under the auction rules.</p>
<h3><a class="anchor" href="https://ethresear.ch#p2p-market-space-7" name="p2p-market-space-7"></a>P2P Market Space</h3>
<ul>
<li><strong>Naive, Adaptive, and Bluff Behaviors</strong>: These strategies are harder to execute since bids can only be sent once. The builder might use different staked addresses, each sending one bid. However, this requires staking on the consensus layer for each address, assuming payment is handled on the consensus layer. Additionally, bluffing is not possible because bids cannot be canceled.</li>
<li><strong>Last Minute Behavior</strong>: This is the <strong>only</strong> possible strategy. Builders will reveal their valuation at the final moment before auction termination to minimize the reaction window for other players.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#builder-rpc-market-space-8" name="builder-rpc-market-space-8"></a>Builder RPC Market Space</h3>
<ul>
<li><strong>Naive, Adaptive, Bluff, and Last Minute Behavior</strong>: For similar reasons to the P2P market space, these strategies are not possible. Additionally, the auction is private, meaning builders cannot see each other’s bids. Most importantly, the auction has shifted from push-based to pull-based, so the builder no longer has control over when to submit bids. The only way for builders to get their bids to the proposer is through the proposer’s request.</li>
</ul>
<p>We conclude that builders’ bidding strategies are heavily limited under ePBS. For P2P, only last-minute bidding is possible. For Builder RPC, builders can only respond to the proposer as it is a pull-based model.</p>
<h1><a class="anchor" href="https://ethresear.ch#market-space-considerations-9" name="market-space-considerations-9"></a>Market Space Considerations</h1>
<p>We add a few more concerns in this section that was emphasized in the MEV-Boost market space but may no longer be relevant in ePBS market space.</p>
<h2><a class="anchor" href="https://ethresear.ch#latency-and-dos-concerns-10" name="latency-and-dos-concerns-10"></a>Latency and DOS Concerns</h2>
<p>Different market spaces impose varying latency constraints. In the P2P market space, builders push bids to the proposers, and the market operates as a large P2P gossip network constrained by anti-DOS measures. With 1 million validators, the worst-case scenario could mean 1 million bids. Due to these concerns, rules like disallowing multiple bids and ensuring bids are above certain values are necessary. The P2P network is inherently slow, so we don’t foresee serious bidders using it to win bids. However, the P2P market space is valuable for maintaining a good <strong>baseline for competitive bids</strong> that isn’t latency-sensitive. If builders using RPC collude to drive bid prices low, an <strong>altruistic builder</strong> over P2P can ensure the bid value baseline remains healthy and competitive with minimal effort. The baseline P2P bid value may also be used for burning in future iterations, as it only requires a 1/n honest assumption.</p>
<p>In the builder RPC market space, which is pull-based, latency matters significantly. Instead of two latencies (global and individual) defined in the MEV-Boost market space, there’s only one individual delay to consider: how fast the builder can return the bids to the proposer. Delaying the return of <code>getHeader</code> may result in proposer missing builder’s bid.</p>
<h2><a class="anchor" href="https://ethresear.ch#auction-interval-uncertainty-11" name="auction-interval-uncertainty-11"></a>Auction Interval Uncertainty</h2>
<p>The auction interval uncertainty becomes clearer in ePBS because MEV-Boost middleware and relays no longer control the timing of when the block gets returned to the proposer or released to the network. The proposer either uses the pushed bids from the P2P network or pulls bids from the builders RPC. The proposer has the final say on the auction interval cut-off. From the builder RPC market space perspective, it will keep updating its bids until the proposer requests them.</p>
<h3><a class="anchor" href="https://ethresear.ch#new-bluff-behavior-under-epbs-12" name="new-bluff-behavior-under-epbs-12"></a>New: Bluff Behavior under ePBS</h3>
<p>In ePBS, proposers or builders may attempt to bluff other builders. This may not be scalable given the nature of the single bid auction over P2P and the fact that every builder is a validator and needs to have a stake on the beacon chain. One bluff strategy is for the proposer of next slot to reveal a high value P2P bid, intentionally stating that this is the bid it will include for the next slot unless others can beat it. This helps set the base price and forces everyone else to beat it. However, the proposer doesn’t have to include its bid.</p>
<p>Although it’s obvious that anyone can see that the bid comes from the proposer and just ignore it, the proposer may use sybil validators to perform the same bluff. However, it’s still unclear how scalable this strategy is, given that one bid equals one validator.</p>
<h1><a class="anchor" href="https://ethresear.ch#open-questions-13" name="open-questions-13"></a>Open questions</h1>
<p>The current ePBS market space design and requirements leave some open questions. We will summarize the open questions here for feedback:</p>
<ul>
<li>
<p><strong>P2P Market Space Conditions</strong>:</p>
<ul>
<li>Every builder can only submit one bid, and the subsequent bids get dropped. Are there any advantages to allowing multiple bids here? If yes, then how many?</li>
<li>Every builder’s bid needs to be above a certain value to deter DOS attacks. What should the value be?
<ul>
<li>We can look at current or past empirical data here.</li>
</ul>
</li>
<li>There’s a tradeoff between the number of bids allowed and the minimal values. If we set the values high, we may allow multiple bids.</li>
<li>Is there a strong argument for requiring bid cancellation?</li>
</ul>
</li>
<li>
<p><strong>Builder RPC Market Space’s Builder API Interface</strong>:</p>
<ul>
<li>What does the Builder API interface look like?
<ul>
<li>We want to leverage the existing Builder API and aim for minimal changes.</li>
<li>When the proposer makes a header request to the builder, what should the request look like? Can we use the current get header request with a signature, or should we modify it?</li>
<li>Do we allow multiple getHeader requests, such as continuous polling from the proposer, or do we enforce a common standard?</li>
</ul>
</li>
<li>What kind of auction is most ideal?
<ul>
<li>Sealed second-price auction may be most ideal.</li>
<li>How to design this over Builder API?</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Comparing MEV-Boost Market Space to ePBS Market Space</strong>:</p>
<ul>
<li>Do we lose anything in the ePBS market space that is important to maintain from the MEV-Boost market space?</li>
</ul>
</li>
<li>
<p><strong>Implications of staking pools also bidding:</strong></p>
<ul>
<li>Pools that hold a significant chunk of validators could be in a privileged position for submitting bids and manipulating the market extensively compared to a builder that doesn’t hold as many keys.
<ul>
<li>Is there an advantage to this asymmetry?</li>
<li>Will we see staking pools and builders teaming up, and how will this dynamic play out?</li>
</ul>
</li>
</ul>
</li>
</ul>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/builder-bidding-behaviors-in-epbs/20129">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 23 Jul 2024 13:44:07 +0000</pubDate>
</item>
<item>
<title>Enabling standardized on chain executions through modular accounts</title>
<link>https://ethresear.ch/t/enabling-standardized-on-chain-executions-through-modular-accounts/20127</link>
<guid>https://ethresear.ch/t/enabling-standardized-on-chain-executions-through-modular-accounts/20127</guid>
<content:encoded><![CDATA[
<div> 关键词：标准化执行、Modular Accounts、Verifiable Credentials (VCs)、Zero-Knowledge Proofs (ZKPs)、Validation Module

总结:<br />
这篇文章讨论了通过Modular Accounts实现区块链上标准执行的过程。它提出了一种框架，结合ERC 7579提案，将用户身份验证和交易授权分离，使用zk证明验证VC以确保操作的合法性和隐私。该系统利用智能合约、账户抽象和接口检测来提升安全性、隐私和用户体验。文章列举了几个关键应用领域，如DeFi、DAOs和供应链管理，以展示这种标准化执行的价值。未来，文章还关注了账户灵活性和安全性的增强，如P-256椭圆曲线签名验证、Keystore合同和passkeys在提高用户体验中的作用。整体而言，这个框架旨在为大规模企业服务提供一个安全、合规和高效的区块链执行环境。 <div>
<p><strong>Enabling standardized on chain executions through Modular Accounts</strong></p>
<p><strong>Introduction</strong></p>
<p>This blogpost is intended to be an extension from a previous work “ <a href="https://ethresear.ch/t/self-sovereign-identity-and-account-abstraction-for-privacy-preserving-cross-chain-user-operations-across-roll-ups/19599">Self-sovereign identity and account abstraction for privacy preserving cross chain user operations across roll ups</a> ” with the intent of proposing a system implementation of network features. In the previous work I tried to envision a system combining a three-layered architecture that I will briefly summarize here:</p>
<ol>
<li>An application layer comprises wallets and other service apps to facilitate the generation and management of Verifiable Credentials, set a permission logic for compiling user operation objects through apps.</li>
<li>A network layer based on different L2s, which include a Keystore contract and Smart Contract Accounts. This layer is responsible for generating Zero-Knowledge Proofs (ZKPs) and Merkle proofs for Sequencers. The Keystore contract manages encryption keys and user authentication, ensuring the correct key pairing for Verifiable Credentials and Operations. Smart Contract Accounts verify user operations, by validating ZK cryptographic proofs to ensure the integrity of the signatures of the transactions before they are executed.</li>
<li>A sequencing layer which interconnects L2s with Ethereum main-net and manages the execution of batches of transactions anchoring Roll-up IDs to sequencing networks batching, validation cross chain atomic transactions via the Keystore roll-up, and the Roll-up contract within Ethereum’s slots.</li>
</ol>
<p>Today, I am trying to focus on some elements on the 1 and 2 layer, sitting in the conjunction between External Owned Accounts and Contract Accounts trying to envision an implementation pathway for the adoption of standardized on chain execution.</p>
<p><strong>The concept</strong></p>
<p>To facilitate the adoption of blockchain based services globally there is a need for standardizing secure, privacy and regulatory-compliant on chain executions to scale. As we move towards a more decentralized future, ensuring cyber security, data minimization from origination to processing, and improving UX in executing on chain operations is crucial.</p>
<p>This blog post introduces a framework based on the ERC 7579 proposal, which integrates a module to lavage onchain verifiable credentials and zero-knowledge (zk) proofs in the context of modular smart accounts. This framework aims to standardize onchain executions by separating user authentication and transaction authorization while preserving privacy and regulatory requirements throughout the transaction lifecycle.</p>
<p>The core function of the system described involves validating zk proofs generated by VCs to authenticate users and authorize operations. This makes the Validation Module the most appropriate choice, as it is designed to validate user operations before they are executed.</p>
<p>The Validation Module allows for checking the validity and authenticity of zk proofs, ensuring that only legitimate user operations are processed.</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/c/1c3ddecf471befecea0769a9e94397e93d1a0bb7.png" title=""><img alt="" height="153" src="https://ethresear.ch/uploads/default/optimized/3X/1/c/1c3ddecf471befecea0769a9e94397e93d1a0bb7_2_455x153.png" width="455" /></a></div><p></p>
<p>For reference, the framework considers a minimal set of ERC for implementation:</p>
<ul>
<li>
<p><a href="https://eips.ethereum.org/EIPS/eip-7579#modules">ERC 7579</a> - Minimal Modular Smart Accounts: to set a module to validate user operations by verifying ZK proofs derived from verifiable credentials (VCs) ensuring that user operations are authenticated and authorized before execution.</p>
</li>
<li>
<p><a href="https://eips.ethereum.org/EIPS/eip-1271">ERC 1271</a> - Standard Signature Validation Method for Contracts: to standardize how smart contracts validate signatures, defining the function to verify the validity of a signature, crucial for transaction authorization.</p>
</li>
<li>
<p><a href="https://eips.ethereum.org/EIPS/eip-4337">ERC 4337</a> - Account Abstraction Using Alt Mempool: to abstract account management and operations, enabling more complex and user-friendly interactions allowing smart contract accounts to handle user operations and transaction executions.</p>
</li>
<li>
<p><a href="https://eips.ethereum.org/EIPS/eip-165">ERC 165</a> - Standard Interface Detection: to allow contracts to declare support for certain interfaces and enabling smart contracts to query and interact with other contracts that implement specific interfaces.</p>
</li>
</ul>
<p><strong>High-level process flow</strong></p>
<p>The framework leverages the strengths of different proposals to create a robust, secure, and privacy-preserving onchain execution environment.</p>
<p>I list here a high-level overview:</p>
<ol>
<li>Users issue on chain merklized Verifiable Credentials (VCs) through a contract identifier operated by an issuer. These credentials are stored in the user’s identity wallet (EOA)</li>
<li>Users generate and validate ZK proofs derived from their VCs through a contract verifier to access service apps and compile User Operation objects.</li>
<li>Smart Contract Account entry point perform canonical verification loop according ERC 4337.</li>
<li>The validation module verifies the zk proofs against the VCs in the execution loop and upon successful validation, the module calls the isValidSignature function as defined by ERC 1271 to authorize entry point to executeUserOps.</li>
<li>Smart contract Account entry point executes onchain operations and distributes the fees to the Bundler address.</li>
</ol>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/0/8054bf5b0b25390eb19a159f9c698f66c091eb2f.jpeg" title=""><img alt="" height="237" src="https://ethresear.ch/uploads/default/optimized/3X/8/0/8054bf5b0b25390eb19a159f9c698f66c091eb2f_2_643x237.jpeg" width="643" /></a></div><p></p>
<p><strong>Market &amp; Business Considerations</strong></p>
<p>This framework offers significant value for large-scale enterprise services requiring validation logic before authorizing onchain operations. By integrating SSI and zk proofs, enterprises can ensure privacy and regulatory compliance while enhancing security and efficiency. The separation of authentication and authorization further strengthens the system’s robustness.</p>
<p>Here a short list of valuable use cases that would fit nicely with this logic:</p>
<ol>
<li><strong>Trading</strong>: A DeFi platform allows users to interact with various financial services such as lending, borrowing, and trading. The platform issues VCs containing user identity and KYC information. Whenever a user wants to execute a financial transaction, they submit a zk proofs on VC and compile user operations objects along as operation request. The Validation Module verifies the zk proof against the stored VC and upon successful verification, the entry point is executing the operations on chain.</li>
<li><strong>DAOs</strong>: a decentralized voting system allows DAO members to vote for grants allocation privacy preserving and reducing conflict of interest. Voters authenticate themselves using VCs and zk proofs to cast their votes and upon the voting completion the entry point executes grants allocations on chain according to the voting results.</li>
<li><strong>Supply chain</strong>: a management system tracks the manufacturing and logistics of goods, participants in the supply chain authenticate using VCs and zk proofs to update and access the status of goods up to the distributor and at the moment of the sale the entry point execute onchain rewards, or payment premiums, to the different members.</li>
</ol>
<p><strong>Conclusion &amp; further thoughts</strong></p>
<p>When we look forward to what kind of functionalities we would like to have as user for the future I believe there has been a big trend in providing standardization through native abstraction, modularity, and functional collaboration between EOAs and SCAs. Considering the future road map of Ethereum in Pectra, <a href="https://github.com/ethereum/EIPs/blob/master/EIPS/eip-7702.md">EIP 7702</a> sets the way for a new transaction type which enables account properties of both externally owned accounts (EOAs) and Smart Contract Accounts (SCAs). Furthering looking ahead, there is a way to set “native account abstraction” by <a href="https://github.com/ethereum/RIPs/blob/master/RIPS/rip-7560.md">RIP 7560</a> and <a href="https://eips.ethereum.org/EIPS/eip-7562">ERC 7562</a> which align with ERC 4337 rules at least on validation rules.</p>
<p>A side of this is also important <a href="https://eips.ethereum.org/EIPS/eip-7212">EIP 7212</a> proposes a precompiled contract to perform signature verifications using the secp256r1 elliptic curve. This curve, also known as P-256, is widely supported in modern devices like Apple’s Secure Enclave, Webauthn, and Android Keychain, and Passkeys. This would allows more efficient and flexible management of accounts by transaction signs in mobile devices. Both EIP 7702 and EIP 7212 are potentially taking place in Pectra.</p>
<p>Network harmonization is a key capability of every growing community, in this context personally I see value in further exploring the use of keystore contracts, session keys and passkeys as additional features to provide security, flexibility and usability to the product experience.</p>
<p>Passkeys can be used to authenticate users providing a smoother UX while combining strong user authentication. Session keys can improve security of communication at application level, meaning when users are interacting with dapps for using market services, and keystore contract could represent an reliable solution for setting a functional framework for operating key sessions.</p>
<p>The shared focus on improving account flexibility and security through different methods highlights the committment of the community’s to address scalability, usability, and security concerns within the Ethereum network. In this setting collaboration between EOAs and SCAs, and additional features as passkeys keystore contracts can live in a modular setting where specific modules enforce optional rules for executions, and different cryptographic tecniques could ensure data minimization for controllers and processors.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/enabling-standardized-on-chain-executions-through-modular-accounts/20127">Read full topic</a></p>
]]></content:encoded>
<pubDate>Mon, 22 Jul 2024 12:43:52 +0000</pubDate>
</item>
<item>
<title>Diseconomies of Scale: Anti-Correlation Penalties (EIP-7716)</title>
<link>https://ethresear.ch/t/diseconomies-of-scale-anti-correlation-penalties-eip-7716/20114</link>
<guid>https://ethresear.ch/t/diseconomies-of-scale-anti-correlation-penalties-eip-7716/20114</guid>
<content:encoded><![CDATA[
<div> 关键词：Diseconomies of Scale、Anti-Correlation Penalties、EIP-7716、Proposer Timing Games、Centralization

总结:
这篇文章讨论了以太坊共识机制中的中心化问题，尤其是经济规模效应导致的不平衡和集中风险。EIP-7716提案旨在通过实施反相关惩罚来抵消这些影响，通过调整奖励和罚款，鼓励小型和分散的参与者，同时抑制大型运营商可能滥用的优势。反相关惩罚会根据验证者之间行为的相关性动态调整，例如对源投票错误的惩罚会根据网络中未参与或错误投票的总余额来调整。文章还提到，虽然惩罚可能会对单个验证者造成短期影响，但长期来看，它有利于提高网络的抗风险能力并促进去中心化。 <div>
<h1><a class="anchor" href="https://ethresear.ch#diseconomies-of-scale-anti-correlation-penalties-eip-7716-1" name="diseconomies-of-scale-anti-correlation-penalties-eip-7716-1"></a>Diseconomies of Scale: Anti-Correlation Penalties (EIP-7716)</h1>
<blockquote>
<p>Special thanks to <a href="https://x.com/dapplion">DappLion</a> and <a href="https://x.com/VitalikButerin">Vitalik</a> for their collaborative effort on the overall concept, and <a href="https://x.com/weboftrees">Anders</a> and <a href="https://x.com/_julianma">Julian</a> for their valuable feedback on this post!</p>
</blockquote>
<p>Ethereum relies on a decentralized set of validators to ensure properties like credible neutrality and censorship resistance. Validators <a href="https://ethereum.org/en/staking/">stake</a> a certain amount of ETH to participate in Ethereum’s consensus and secure the network. In return, validators receive rewards directly from the protocol (<a href="https://ethresear.ch/t/faq-ethereum-issuance-reduction/19675">#issuance</a>) as well as execution layer rewards when proposing a block, which include transaction fees and <a href="https://ethereum.org/en/developers/docs/mev/">MEV</a> from the blocks they propose (<a href="https://boost.flashbots.net/">#mevboost</a>). As of today, thousands, if not tens of thousands, of small-sized entities run validators from their homes despite several disadvantages. These include the risk and responsibility of operating and maintaining a node, the technical burden associated with setup and upkeep, potential downtime, and the lack of a liquid staking token that would otherwise provide flexibility and liquidity.</p>
<p>With the ongoing maturation of Ethereum’s PoS, we’ve encountered various <strong>centralizing forces</strong> inherent to the current protocol:</p>
<ul>
<li><strong>EL Reward Variance</strong>: While attestation rewards are distributed fairly evenly, the rewards for proposing a block can vary significantly. This variation arises because <a href="https://ethereum.org/en/developers/docs/mev/">MEV</a> is extremely spiky, resulting in a few outlier blocks with proposer profits exceeding 10 ETH. Large operators running many validators have better chances of capturing these “juicy” blocks. Although over many years the earnings of individual validators should average out, the future remains uncertain. Assuming 1 million validators and 2,628,000 slots per year, the probability of being selected as a proposer is ~0.0001%. On average, a validator can expect to propose <span class="math">\frac{1,000,000}{365.25 \times 7200} = 2.628</span> blocks per year (there are 7200 slots per day). From April 2023 to April 2024, the percentage of blocks with more than 10 ETH was 0.004041%. Statistically, a single validator will eventually propose a block with more than 10 ETH of MEV, but it’s unknown whether this will happen this year or in ten years, and by then, MEV issues might be resolved. While solo stakers literally participate in a lottery, large operators can average their profits and plan for the future with greater certainty.<br />
Over 1 year, the probability of a random validator getting at least one block with &gt;10 ETH profits is 0.1%:</li>
</ul>
<div class="math">
P(\text{at least one 10} \, \text{ETH} \, \text{block}) = 1 - (1 - 0.0004041)^n = 1 - (1 - 0.0004041)^{2.628}
</div>
<p>If you control 1% of all validators (~10k validators), the probability of getting at least one block with more than 10 ETH of MEV climbs to approximately 99.99% over one year.</p>
<p>The following chart shows the <strong>cumulative sum of MEV-Boost payments</strong> on the y-axis and the <strong>cumulative number of MEV-Boost payments</strong> on the x-axis. We can see that 90% of all blocks distribute around 44% of the total value, leaving 56% to be distributed to the lucky 10% of proposers.</p>
<div align="center">
<p>
  </p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/e/aed0a28f0f91206549c0993488907698504ef2dc.png" title="Bk31ssG_R.png"><img height="428" src="https://ethresear.ch/uploads/default/optimized/3X/a/e/aed0a28f0f91206549c0993488907698504ef2dc_2_600x428.png" width="600" /></a></div>
<p></p>
</div>
<ul>
<li>
<p><strong>Reorgs</strong>: “<em><a href="https://ethresear.ch/t/change-fork-choice-rule-to-mitigate-balancing-and-reorging-attacks/11127">Honest reorgs</a></em>” occur when the proposer of slot <span class="math">n_{1}</span> orphans the block of the proposer of slot <span class="math">n_{0}</span> because that block hasn’t received at least 40% of the slot’s committee members’ votes. By using <a href="https://notes.ethereum.org/@casparschwa/H1T0k7b85">proposer boost</a>, these “<em>weak</em>” blocks (those with less than 40% attestations) can be reorged by the next proposer to penalize the previous proposer for poor performance, such as being late and therefore rugging some attesters for their correct head votes. Reorgs can have centralizing forces and the more stake an entity holds, the more strategically it can decide whether to reorg a particular block. Large-scale operators have more safety because they can ensure their own validators never vote to reorg their own blocks. Essentially, all nodes of an entity can coordinate to always vote for the current slot’s block rather than its parent if the current block comes from that entity. This coordination potentially allows large entities to risk broadcasting their block later in the slot while still having a high probability of the block becoming canonical. <a href="https://ethresear.ch/t/deep-diving-attestations-a-quantitative-analysis/20020">Analysis has shown</a> that <strong>by second 4</strong> of the slot, <strong>40% of all attestations</strong> for that slot <strong>have been seen</strong>. A large operator, who controls many validators and knows that these validators will never vote to reorganize its blocks, can slightly delay block propagation without significantly increasing its risk. The same principle applies when a single entity owns consecutive slots. In theory, this entity could wait until the end of the slot (or even longer) before publishing its block. Then, it could use the next slot to solidify that weak block into the chain by leveraging proposer boost.</p>
</li>
<li>
<p><strong>Proposer Timing Games</strong>: <a href="https://timing.pics/">Proposer timing games</a> (also see <a href="https://eprint.iacr.org/2023/760">[1]</a>, <a href="https://arxiv.org/abs/2305.09032">[2]</a>) is a term that summarizes a strategy applied by some block proposers in which they delay their proposal to give the builder more time for extracting MEV. This leads to increased profits for the proposer but <a href="https://ethresear.ch/t/deep-diving-attestations-a-quantitative-analysis/20020">evidently</a> comes with a negative impact on other proposers and especially attesters. Proposer timing games are risky because late blocks have an increased chance of being reorged. In general, large-size operators face lower risks when playing timing games than small-size entities. This stems from the fact that larger operators are on average more sophisticated and have better connectivity in the P2P network: What might be a late block for an Australian validator (go <a href="https://x.com/sassal0x">sassal</a> <img alt=":crown:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/crown.png?v=12" title=":crown:" width="20" />) might be just in time for a US-based Coinbase validator. Thus, the lower the latency, the more a validator can risk delaying.</p>
</li>
</ul>
<div class="math">
\textbf{The above symptoms are all exacerbated by one thing, namely...}
</div>
<div class="math">
\underline{\mathbf{economies\ of\ scale.}}
</div>
<p><a href="https://en.wikipedia.org/wiki/Economies_of_scale">Economies of scale</a> are nothing new and the crypto landscape isn’t immune either. Looking at Wikipedia, it is defined as “<em>the cost advantages that enterprises obtain due to their scale of operation […]</em>”, and the same applies to Ethereum staking:</p>
<div align="center">
  <div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/5/e548616102366792be17847ac823b351c9f94170.png" title="ryZMFz-_R.png"><img height="359" src="https://ethresear.ch/uploads/default/optimized/3X/e/5/e548616102366792be17847ac823b351c9f94170_2_400x359.png" width="400" /></a></div>
</div>
<p>Large operators like Coinbase, Kraken, or Kiln can leverage economies of scale to make staking even more profitable. This allows them to offer rewards competitive with those of solo stakers, even after taking their cut. To illustrate this, consider a simple example (the exact numbers are not important here):</p>
<div class="md-table">
<table>
<thead>
<tr>
<th>Entity</th>
<th>Validators on one Node</th>
<th>Hardware Costs</th>
<th>Other Costs</th>
<th>Total Cost ($)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Solo Staker</td>
<td>1 validator</td>
<td>1 Intel NUC ($ 1,200)</td>
<td>$ 5,000</td>
<td>$ 6,200</td>
</tr>
<tr>
<td>Coinbase</td>
<td>1,000 validators</td>
<td>1 Intel NUC ($ 1,200)</td>
<td>$ 50,000</td>
<td>$ 51,200</td>
</tr>
</tbody>
</table>
</div><blockquote>
<p>For more realistic numbers, refer to the latest EthStaker survey published <a href="https://paragraph.xyz/@ethstaker/staking-survey-2024">here</a>. By allocating 10x as much in <em>Other Costs</em> for large operators, we account for the increased complexity of setting up multiple validators on one machine. This is realistic enough for the point we’re making here.</p>
</blockquote>
<p>We can see the effects of economies of scale: the machine used by Coinbase will generate 1,000 times the profits compared to solo stakers, while the costs are only eight times higher. As a result, the ROI for large-scale operators is significantly better.</p>
<p>Using one hardware device for multiple validators is just one piece of the puzzle. Others include:</p>
<ul>
<li>Cloud service provider</li>
<li>ISPs</li>
<li>Geographical locations</li>
<li>Maintenance responsibilities</li>
<li>Client software</li>
<li>And many more…</li>
</ul>
<p>In all these categories, the goal is maximum diversity to minimize the risk of external factors degrading or damaging the network. Despite this goal, economically rational players might prefer a one-stop-shop solution, such as running a Lighthouse + Geth node on a Google/AWS/Hetzner instance located in central Europe, maintained by a dedicated team of specialists. While this setup may perform well in terms of efficiency, Ethereum should not create incentives that further exacerbate centralization.</p>
<div align="center">
<p>
  <img height="392" src="https://ethresear.ch/uploads/default/original/3X/b/a/bac447c3b4a914807987ea637a010bceb40febef.png" width="400" />
</p>
</div>
<p><strong>But who is large-scale and whose not?</strong></p>
<p>The protocol itself does not know which validator is operated by which entity. From the protocol’s perspective, a Coinbase validator looks the same as a solo staker. Therefore, to prevent correlations from emerging, we cannot simply scale rewards and penalties based on the market share of the entity behind a validator. For more on this topic, I recommend Barnabé’s post, <em><a href="https://barnabe.substack.com/p/seeing-like-a-protocol">“Seeing Like a Protocol”</a></em>.</p>
<p>Fortunately, <strong>economies of scale inherently come with correlations</strong>, which is something the protocol can be made aware of. Leveraging economies of scale may linearly scale with correlations, thus we can implement rules to dynamically scale economic incentives and steer the validator set toward diversification.</p>
<div align="center">
<p>
  </p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/9/697934a0c70e654fbc603e570f422269f8116551.png" title="BJbZ1XWuR.png"><img height="358" src="https://ethresear.ch/uploads/default/optimized/3X/6/9/697934a0c70e654fbc603e570f422269f8116551_2_400x358.png" width="400" /></a></div>
<p></p>
</div>
<p>Penalizing correlated faults isn’t something new to Ethereum. In the current <a href="https://eth2book.info/capella/part2/incentives/slashing/">slashing</a> mechanism, a “malicious” validator is initially penalized by a reduction of 1/32 of their effective balance when they are slashed. After being halfway through the withdrawal period, they are subject to an additional penalty (the <em><a href="https://eth2book.info/capella/part2/incentives/slashing/">correlation penalty</a></em>) that scales with the number of validators (specifically their stake) who were slashed around the same time (+/- 18 days). Therefore, a solo staker accidentally voting for two different head blocks, which is a slashable offense, would lose significantly less than a party with a 20% market share (assuming all 20% collectively fail).</p>
<p>In the end, the goal must be to incentivize validators to diversify their setup. As shown in the following example, we want validators to reduce their correlation with other validators, making the whole network more robust against external influences.</p>
<div align="center">
<p>
  </p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/7/678299142da4a042b47e07270b8ccfc7c644e393.png" title="ry5bkuMO0.png"><img height="229" src="https://ethresear.ch/uploads/default/optimized/3X/6/7/678299142da4a042b47e07270b8ccfc7c644e393_2_690x229.png" width="690" /></a></div>
<p></p>
</div>
<h2><a class="anchor" href="https://ethresear.ch#eip-7716-2" name="eip-7716-2"></a>EIP-7716</h2>
<p>The goal of “<a href="https://eips.ethereum.org/EIPS/eip-7716">EIP-7716: Anti-Correlation Attestation Penalties</a>” is to get us closer to diseconomies of scale. The more homogeneous an entity’s staking setup, the more it should be penalized while non-correlated setups profit from the proposed changes.</p>
<p>With anti-correlation penalties, the previous “<em>economies of scale vs number of validators</em>” might be more like the following:</p>
<div align="center">
<p>
  </p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/0/e0a2bcab530fcb98384c27b1765434ef0121fdd3.png" title="S179FsfOA.png"><img height="359" src="https://ethresear.ch/uploads/default/optimized/3X/e/0/e0a2bcab530fcb98384c27b1765434ef0121fdd3_2_400x359.png" width="400" /></a></div>
<p></p>
</div>
<p>Anti-correlation penalties were first described by Vitalik in an <a href="https://ethresear.ch/t/supporting-decentralized-staking-through-more-anti-correlation-incentives/19116">ethresearch post</a>. After some <a href="https://ethresear.ch/t/analysis-on-correlated-attestation-penalties/19244">initial analyses</a> and a more <a href="https://ethresear.ch/t/a-concrete-proposal-for-correlated-attester-penalties/19341">concrete proposal</a> available, the <a href="https://eips.ethereum.org/EIPS/eip-7716">EIP</a> is now at the point where everyone is invited to look into the inner workings of correlated penalties and leave feedback.</p>
<p>In short, the EIP proposes to multiply the missed (source) vote penalty by a penalty factor that ranges from 0 to 4 but equals 1 on average <em>(notably, this is important to not touch the issuance policy)</em>.</p>
<h4><a class="anchor" href="https://ethresear.ch#how-does-7716-work-3" name="how-does-7716-work-3"></a>How does 7716 work?</h4>
<div class="md-table">
<table>
<thead>
<tr>
<th>Variable</th>
<th>Symbol</th>
<th>Description</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>penalty_factor</code></td>
<td><span class="math">p</span></td>
<td>Penalty scaling factor</td>
<td>dynamic</td>
</tr>
<tr>
<td><code>net_excess_penalties</code></td>
<td><span class="math">p_{exc}</span></td>
<td>Net excess penalties</td>
<td>dynamic</td>
</tr>
<tr>
<td><code>non_participating_balance</code></td>
<td><span class="math">balance_{non\_attesting}</span></td>
<td>Sum balance of not/wrong attesting validators</td>
<td>dynamic</td>
</tr>
<tr>
<td><code>PENALTY_ADJUSTMENT_FACTOR</code></td>
<td><span class="math">p_{adjustment}</span></td>
<td>Penalty adjustment factor</td>
<td>2**12</td>
</tr>
<tr>
<td><code>MAX_PENALTY_FACTOR</code></td>
<td><span class="math">p_{max}</span></td>
<td>Maximum penalty factor</td>
<td>4</td>
</tr>
<tr>
<td><code>EXCESS_PENALTY_RECOVERY_RATE</code></td>
<td><span class="math">p_{recovery}</span></td>
<td>Rate by which the excess penalty decreases</td>
<td>1</td>
</tr>
</tbody>
</table>
</div><blockquote>
<p>The final values for the constants are still to be decided.</p>
</blockquote>
<p>The penalty factor <span class="math">p</span> scales the slot penalties to a maximum of <code>MAX_PENALTY_FACTOR</code>, or down. It’s determined the following:</p>
<div class="math">
p = \min\left(\frac{balance_{non\_attesting}\ \times\ p_{adjustment}}{\max(p_{excess},\ 0.5)\times\ balance_{total}\ +\ 1},\ p_{max} \right)
</div>
<p>and from the <a href="https://github.com/ethereum/consensus-specs/blob/816d338bd09ffc8e83097c4db1764ba834f3adca/specs/_features/correlated_penalties/beacon_chain.md">pyspec implementation</a>; h/t <a href="https://x.com/dapplion">dapplion</a>:</p>
<pre><code class="lang-python">penalty_factor = min(
    ((total_balance - participating_balance) * PENALTY_ADJUSTMENT_FACTOR) // 
    (max(self.net_excess_penalties, 0.5) * total_balance + 1),
    MAX_PENALTY_FACTOR
)
</code></pre>
<p>The formula calculates the penalty factor as a ratio of the “penalty weight” of non-attesting validators to the net excess penalty scaled by the balance of all validators. A higher penalty adjustment factor increases the sensitivity of the penalty factor. Conversely, a higher net excess penalty leads to a lower penalty factor.</p>
<p>Finally, this is how the <code>penalty_factor</code> variable would be used:</p>
<pre><code class="lang-auto">def get_flag_index_deltas(state: BeaconState, flag_index: int) -&gt; Tuple[Sequence[Gwei], Sequence[Gwei]]:
    """
    Return the deltas for a given ``flag_index`` by scanning through the participation flags.
    """
    ...
    for index in get_eligible_validator_indices(state):
        base_reward = get_base_reward(state, index)
        if index in unslashed_participating_indices:
            if not is_in_inactivity_leak(state):
                reward_numerator = base_reward * weight * unslashed_participating_increments
                rewards[index] += Gwei(reward_numerator // (active_increments * WEIGHT_DENOMINATOR))
        elif flag_index != TIMELY_SOURCE_FLAG_INDEX:
            # [New in correlated_penalties]
            slot = committee_slot_of_validator(state, index, previous_epoch)
            penalty_factor = compute_penalty_factor(state, slot) 
            penalties[index] += Gwei(penalty_factor * base_reward * weight // WEIGHT_DENOMINATOR)
    return rewards, penalties
</code></pre>
<p>We check if we are dealing with source votes (line 12), derive the slot in which the validator was supposed to vote (line 14), compute the penalty factor (line 15), and multiply it by the base reward (line 16).</p>
<blockquote>
<p>Although source votes might be a good starting point, the concept can be equally appied to head and target votes.</p>
</blockquote>
<p>The <span class="math">p_{exc}</span> is updated at the end of each slot using:</p>
<div class="math">
p_{exc} = \max(p_{recovery},\ p_{exc} + p) - p_{recovery}
</div>
<p>Which equals to:</p>
<pre><code class="lang-python">net_excess_penalties = max(
        EXCESS_PENALTY_RECOVERY_RATE, 
        net_excess_penalties + penalty_factor
    ) - EXCESS_PENALTY_RECOVERY_RATE
</code></pre>
<p>We can observe the following dynamics:</p>
<ul>
<li>If the balance of non-attesting validators increases, the penalty factor also increases.</li>
<li>If the balance of non-attesting validators remains the same, the penalty factor approaches 1.</li>
<li>If the balance of non-attesting validators decreases, the penalty factor can go below 1 for a while and then approach 1 afterward.</li>
</ul>
<p>When the <code>non_participating_balance</code> continuously increases for several rounds, the <code>penalty_factor</code> and the <code>net_excess_penalties</code> also increase. This continues until the <code>non_participating_balance</code> stops increasing. Then, the <code>net_excess_penalties</code> and the <code>penalty_factor</code> start decreasing together.</p>
<p>With the <code>net_excess_penalties</code> keeping track of the excess penalties of past epochs, the formula can self-regulate what constitutes a “large” number of misses and what does not.</p>
<p>This mechanism ensures that the sum of penalties doesn’t change with this EIP—only the distribution does.</p>
<h2><a class="anchor" href="https://ethresear.ch#some-faqs-4" name="some-faqs-4"></a>Some FAQs</h2>
<h3><a class="anchor" href="https://ethresear.ch#h-1-wouldnt-that-be-even-worse-for-solo-stakers-5" name="h-1-wouldnt-that-be-even-worse-for-solo-stakers-5"></a>1. Wouldn’t that be even worse for solo stakers?</h3>
<p>No. Solo stakers, commonly referred to as small-scale operators or individuals running 1-10 validators from home, are expected to behave in a very uncorrelated manner compared to larger operators. Although factors like geographical location and client software can impact correlations, solo stakers are likely to be offline at different times than large-scale operators such as Coinbase or Kraken. As a result, the penalties solo stakers receive are smaller than those in the current system. In contrast, if a large-scale operator’s staking setup has a bug causing all their validators to fail to attest, the correlation is clear and the penalties are higher.</p>
<p>This expectation was first confirmed in an <a href="https://ethresear.ch/t/analysis-on-correlated-attestation-penalties/19244">initial analysis</a> on anti-correlation penalties, which showed that solo stakers and Rocketpool operators would have been better off, while large-scale operators would have received higher penalties on average.</p>
<h3><a class="anchor" href="https://ethresear.ch#h-2-wouldnt-that-discourage-people-from-using-minority-clients-6" name="h-2-wouldnt-that-discourage-people-from-using-minority-clients-6"></a>2. Wouldn’t that discourage people from using minority clients?</h3>
<p>No. The opposite is the case, as using the majority client leads to even greater correlations. For example, if the Lighthouse client has a bug that causes attesters to vote for the wrong source, then the correlation is super high, and so is the penalty. On the other hand, all Lodestar attesters failing is regarded as a much smaller collective fault. In the case of minority clients being expected to have more bugs, then this would also balance out better because if it’s only a small minority, then the correlation penalty is more forgiving than if it had been some majority client.</p>
<p>No. In fact, it encourages using minority clients. Using the majority clients leads to higher correlations. For example, if the Lighthouse client has a bug causing attesters to vote for the wrong source, the correlation is high and the penalty increases. Conversely, if all Lodestar attesters fail, it is considered a much smaller collective fault. The correlation penalty is more forgiving to a small minority than to a majority client. So, even if minority clients are expected to have more bugs, correlation penalties can steer validators toward using them.</p>
<h3><a class="anchor" href="https://ethresear.ch#h-3-why-would-it-benefit-decentralization-7" name="h-3-why-would-it-benefit-decentralization-7"></a>3. Why would it benefit decentralization?</h3>
<p>Anti-correlation penalties effectively differentiate between small and large operators without relying on validators that have consolidated their stake or other out-of-protocol solutions. By introducing economic incentives for diversified behavior, we benefit small players who are already “anti-correlated” while encouraging large players to reduce the impact of external factors such as one-node setups, or cloud providers on their staking operations.</p>
<h3><a class="anchor" href="https://ethresear.ch#h-4-wouldnt-that-just-lead-to-big-parties-investing-in-increased-fault-tolerance-while-even-increasing-the-correlations-8" name="h-4-wouldnt-that-just-lead-to-big-parties-investing-in-increased-fault-tolerance-while-even-increasing-the-correlations-8"></a>4. Wouldn’t that just lead to big parties investing in increased fault tolerance while even increasing the correlations?</h3>
<p>If big parties invest in increased fault tolerance, it’s still beneficial. Enhancing fault tolerance is difficult and expensive. At some point, it becomes cheaper to invest in anti-correlation than in further fault tolerance improvements. While large operators might have to move validators from popular cloud platforms to different environments, solo stakers running their nodes from home can continue as they are. Anything that costs large operators money but is free for solo stakers (=diseconomies of scale) fosters decentralization.</p>
<div align="center">
<p>
  </p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/5/25f018bc53924b32c0a7c1deb29b0a0f3075daf1.jpeg" title="HyW84oMdA.jpg"><img height="216" src="https://ethresear.ch/uploads/default/optimized/3X/2/5/25f018bc53924b32c0a7c1deb29b0a0f3075daf1_2_690x216.jpeg" width="690" /></a></div>
<p></p>
</div>
<p>The main argument is, that no matter how big operators react, either going for anti-correlation or, doing the opposite, putting all validators on a single extremely robust node, both cost money and reduce their APY. As fault tolerance has its limits, there is no escape from harsher penalties other than diversifying.</p>
<div align="center">
<p>
  </p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/2/72c96a75749cc73c7789fac6f4fd97834924d1d5.png" title="BJL374W_0.png"><img height="500" src="https://ethresear.ch/uploads/default/optimized/3X/7/2/72c96a75749cc73c7789fac6f4fd97834924d1d5_2_328x500.png" width="328" /></a></div>
<p></p>
</div>
<h3><a class="anchor" href="https://ethresear.ch#h-5-this-sounds-super-dangerous-as-i-could-be-penalized-for-32-eth-just-by-missing-a-single-source-vote-9" name="h-5-this-sounds-super-dangerous-as-i-could-be-penalized-for-32-eth-just-by-missing-a-single-source-vote-9"></a>5. This sounds super dangerous, as I could be penalized for 32 ETH just by missing a single source vote.</h3>
<p>This is incorrect since the penalty_factor variable is capped at 4 (to be analyzed). Capping ensures that the correlation penalties never get out of hand.</p>
<h3><a class="anchor" href="https://ethresear.ch#h-6-why-only-focus-on-source-votes-and-not-do-the-same-for-head-and-target-votes-10" name="h-6-why-only-focus-on-source-votes-and-not-do-the-same-for-head-and-target-votes-10"></a>6. Why only focus on source votes and not do the same for head and target votes?</h3>
<p>This is a good question and since research on that topic is still at the very beginning this isn’t decided yet. An argument against head and target votes is the fact that they depend on external factors: as shown in <a href="https://ethresear.ch/t/analysis-on-correlated-attestation-penalties/19244">previous analysis</a>, head votes are sensitive to proposer timing games. So, if those timing games become more and more the standard, less well-connected validators (oftentimes solo stakers) could potentially be worse off. However, <a href="https://ethresear.ch/t/analysis-on-correlated-attestation-penalties/19244">this</a> analysis showed that it would be the opposite and in the long run, small-size stakers would profit from anti-correlation penalties. The same applies to target votes that are harder to get right in the first slot of an epoch compared to every other slot. Nevertheless, in the long run, this should smooth out across validators, allowing us to do anti-correlation penalties for all parts of an attestation, source-, target-, and head votes.</p>
<h2><a class="anchor" href="https://ethresear.ch#useful-links-11" name="useful-links-11"></a>Useful links:</h2>
<div class="md-table">
<table>
<thead>
<tr>
<th>Url</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a class="inline-onebox" href="https://eips.ethereum.org/EIPS/eip-7716">EIP-7716: Anti-correlation attestation penalties</a></td>
<td>EIP-7716 (draft)</td>
</tr>
<tr>
<td><a class="inline-onebox" href="https://ethresear.ch/t/a-concrete-proposal-for-correlated-attester-penalties/19341">A concrete proposal for correlated attester penalties</a></td>
<td>Original Proposal</td>
</tr>
<tr>
<td><a class="inline-onebox" href="https://ethereum-magicians.org/t/eip-7716-anti-correlation-attestation-penalties/20137">EIP-7716: Anti-correlation attestation penalties - EIPs - Fellowship of Ethereum Magicians</a></td>
<td>EthMagicians Post</td>
</tr>
<tr>
<td><a class="inline-onebox" href="https://github.com/dapplion/anti-correlation-penalties-faq">GitHub - dapplion/anti-correlation-penalties-faq: Anti correlation penalties FAQ</a></td>
<td>EthBerlin Project</td>
</tr>
<tr>
<td><a class="inline-onebox" href="https://github.com/igorline/lighthouse/pull/1">Impement anti-correlation attestation penalties eip by igorline · Pull Request #1 · igorline/lighthouse · GitHub</a></td>
<td>Lighthouse Implementation</td>
</tr>
<tr>
<td><a class="inline-onebox" href="https://ethresear.ch/t/analysis-on-correlated-attestation-penalties/19244">Analysis on ''Correlated Attestation Penalties''</a></td>
<td>Quantitative Analysis</td>
</tr>
</tbody>
</table>
</div>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/diseconomies-of-scale-anti-correlation-penalties-eip-7716/20114">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sat, 20 Jul 2024 08:56:40 +0000</pubDate>
</item>
<item>
<title>EIP 648 for Parallel Rollup</title>
<link>https://ethresear.ch/t/eip-648-for-parallel-rollup/20103</link>
<guid>https://ethresear.ch/t/eip-648-for-parallel-rollup/20103</guid>
<content:encoded><![CDATA[
<div> 关键词：EIP 648, 交易类型, 并行处理, 范围限制, 兼容性问题

总结:<br />
EIP 648是一项2017年的提议，旨在通过引入新的交易类型来提升以太坊网络中交易的并发处理。该提案允许用户指定读写范围，非重叠范围的交易可并行执行。然而，它存在一些问题。首先，用户可能难以预知交易将访问的具体账户，这称为合同预知识问题。其次，它采用悲观并发控制，以账户为单位，可能导致粗粒度的同步，限制了实际的并行性。因此，虽然EIP 648有其优点，但在实际应用到如Rollup等技术时，需要解决这些兼容性和性能优化的问题。 <div>
<h3><a class="anchor" href="https://ethresear.ch#what-is-eip-648-1" name="what-is-eip-648-1"></a>What is EIP 648?</h3>
<p>EIP 648 was proposed back in 2017. The proposal introduces a new transaction type to facilitate parallel transaction processing.</p>
<p>The new type allows users to specify both the read and write ranges of accounts that the transaction will access. Transactions with no overlapping ranges will be executed in parallel. If the ranges overlap, transactions will conflict unless they are all read-only.</p>
<h3><a class="anchor" href="https://ethresear.ch#issues-2" name="issues-2"></a>Issues</h3>
<p>While the proposal is favorable for its simplicity in some cases, it also has some issues that make it difficult to use directly in rollups.</p>
<ol>
<li>
<p><strong>Contract Pre-knowledge:</strong> It is not always straightforward for users to know exactly which accounts their transactions will access beforehand.</p>
</li>
<li>
<p><strong>Account-level:</strong> The approach uses pessimistic concurrency control at the account level. The statements of the ranges act as synchronization primitives, but this has very coarse concurrency granularity (account-level), which can seriously reduce parallelism in many cases.</p>
</li>
</ol>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/eip-648-for-parallel-rollup/20103">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 19 Jul 2024 07:14:22 +0000</pubDate>
</item>
<item>
<title>Commit-Boost: Proposer Platform to Safely Make Commitments</title>
<link>https://ethresear.ch/t/commit-boost-proposer-platform-to-safely-make-commitments/20107</link>
<guid>https://ethresear.ch/t/commit-boost-proposer-platform-to-safely-make-commitments/20107</guid>
<content:encoded><![CDATA[
<div> 关键词：Commit-Boost, Ethereum, Validator Sidecar, Proposer Commitments, Open-Source

总结:<br />
Commit-Boost是一个由社区驱动的开放源代码项目，旨在为Ethereum开发一个通用的验证者平台，以处理与承诺相关的事务。它解决了当前碎片化问题，通过统一标准，让核心开发者能处理升级和故障情况，同时支持MEV-Boost和其他承诺协议。Commit-Boost不依赖于单一侧车，允许验证者安全地参与不同类型的承诺，减少风险和复杂性。团队由非营利实体支持，致力于透明度、安全性和持续发展，没有VC资金，确保软件作为公共利益存在。目前处于MVP阶段，正在进行测试，预计Q3末将进行审计并逐步完善功能。 <div>
<p><em>The following post is an introduction to some and an update to others on a community effort called <a href="https://x.com/Commit_Boost" rel="noopener nofollow ugc">Commit-Boost</a>. Much of this has already been discussed in various public domains / presentations / documentation. Thank you to all the countless teams that already have contributed / committed to contributing to this effort including researchers, validators, builders, relays, client teams, consulting firms, teams building commitment protocols, L2s, restaking platforms, shared sequencers, wallets, and countless others. Please reach out if you would like to contribute to this effort for Ethereum.</em></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/d/adc1d52f519c3c2bb0d61cd00f9c796e249eba81.png" title="Commit-Boost"><img alt="Commit-Boost" height="375" src="https://ethresear.ch/uploads/default/optimized/3X/a/d/adc1d52f519c3c2bb0d61cd00f9c796e249eba81_2_408x375.png" width="408" /></a></div><p></p>
<p><strong>TL;DR</strong></p>
<ul>
<li>Due to the risks developing for Ethereum, core development, and its validator set, a group of teams / individuals are working on developing a public good called Commit-Boost</li>
<li><a href="https://github.com/Commit-Boost/commit-boost-client" rel="noopener nofollow ugc">Commit-Boost</a> is an open-source public good that is fully compatible with <a href="https://github.com/flashbots/mev-boost" rel="noopener nofollow ugc">MEV-Boost</a> but acts as a light-weight validator platform to safely make commitments</li>
<li>Specifically, Commit-Boost is a new Ethereum validator sidecar that is focused on standardizing the last mile of communication between validators and proposer commitment protocols</li>
<li>Commit-Boost has been designed with safety and modularity at its core, with the goal of not limiting the market downstream including stakeholders, flows, proposer commitments, enforcement mechanisms, etc.</li>
<li>While we should always be skeptical of out-of-protocol solutions that directly impact infrastructure this close to the Ethereum protocol layer, if we are going to rely on these solutions, we believe they should be developed, sustained, and governed in a way that encompasses many of the views <a href="https://collective.flashbots.net/t/toward-an-open-research-and-development-process-for-mev-boost/464" rel="noopener nofollow ugc">previously voiced</a> by the community. We have tried to embrace this and strive to model Commit-Boost after it</li>
</ul>
<p><strong>Background</strong></p>
<ul>
<li>Proposer commitments have been an important part of Ethereum’s history. Today, we already see the power of commitments where over 90% of validators give up their autonomy and make a wholesale commitment that outsources block building to a sophisticated actor called a block builder</li>
<li>However, most are starting to agree on a common denominator: in the future, beacon proposers will face a broader set of options of what they may “commit" to–be it inclusions lists or preconfs or other types of commitments such as long-dated blockspace futures–compared to just an external or local payload they see today</li>
<li>A <a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ" rel="noopener nofollow ugc">post</a> from Barnabe captures this well; during block construction, the validator “…creates the specs, or the template, by which the resulting block must be created, and the builders engaged by the proposer are tasked with delivering the block according to its specifications”</li>
<li>While this all seems great, the challenge is that many teams building commitments are creating new sidecars driving fragmentation and risks for Ethereum</li>
<li>For Ethereum, there are going to be significant challenges and increased risks during upgrades if there are a handful of sidecars that validators are running</li>
<li>For validators, these risks potentially take us to a world where proposers will need to make decisions on which teams to “bet on” and which sidecars they will need to run to participate in what those teams are offering</li>
<li>For homestakers, this is difficult and they likely will be unable to participate in more than one of these commitments</li>
<li>For sophisticated actors, this increases the attack vector and operational complexity as more and more sidecars are required to be run</li>
<li>Another side effect of this is validators are somewhat locked into using a specific sidecar due to limited operational capacity and the switching costs of running a different sidecar (i.e., vendor lock-in). The higher the switching costs, the more embedded network effects could become if these sidecars only support certain downstream actors / proposer commitment protocols</li>
<li>This also could create a dynamic where core out-of-protocol infrastructure supporting Ethereum which should be a public good, starts being used for monetization, distribution, or other purposes</li>
<li>Due to these dynamics, various teams and individuals across the community are driving the development and testing of open-source / public good software called Commit-Boost. This effort includes researchers, validators, builders, relays, client teams, consulting firms, protocols building commitments, L2s, restaking platforms, and countless others across the community</li>
</ul>
<p><strong>Commit-Boost Overview</strong></p>
<p>Commit-Boost is a community-driven, open-source project developing an unopinionated validator platform to enable safe interactions with commitments. Some of its features include:</p>
<ul>
<li>Unification: Core devs will be able to interact and work with one standard during Ethereum forks / upgrades / when and if things go wrong</li>
<li>Backward compatibility + more: Commit-Boost is not only backward compatible with MEV-Boost, but will improve the life of validators who only run MEV-Boost through increased reporting, telemetry / other off-the-shelf tools validators can employ</li>
<li>Opt-in without running more sidecars: Commit-Boost will allow proposers who want to opt into other commitments do so without having to run multiple sidecars</li>
<li>Robust support: Commit-Boost the software is supported by a not-for-profit entity. This team will be focused on security and robustness through policies and procedures with follow-the-sun type models where there is support 24/7 if / when things go wrong. This team will also be focused on testing and adjustments needed during hard forks and have a team to interact with to help during adoption, improvements, and sustainment</li>
<li>Not VC-backed public good: This team and effort will not be VC-backed. There is no monetization plan. The entity will not be able to sell itself and will not start any monetizable side businesses</li>
</ul>
<p><strong>Robustness, Sustainability, and Security</strong></p>
<ul>
<li>Commit-Boost is being developed as a fully open-source project with contributions from teams across the Ethereum tech stack including from validators, client teams, relays, builders, consulting firms, researchers, and many others. This effort with input and support from these teams will help develop a robust product integrating many perspectives</li>
<li>Commit-Boost will go through code reviews and audits once fully developed</li>
<li>As noted below, there also will be a full-time team that helps maintain and upgrade the software with their core focus on 100% uptime and when there are bugs, robust processes to quickly address and fix</li>
<li>The software stack is also built with the validator at the core and includes off-the-shelf tools for monitoring as well as reducing and proactively addressing any risks that may arise</li>
<li>Last, this public good software will have minimal, but critical open governance around future upgrades with input across the Ethereum</li>
</ul>
<p><strong>Team Supporting / Governance of Commit-Boost Software</strong></p>
<ul>
<li>Entity supporting the software: Not-for-profit entity</li>
<li>Multiple-person team: Multiple devs that focus on transparency, sustainment / development, and research with an initial focus around Commit-Boost the software</li>
<li>Transparency: Open-source <a href="https://github.com/Commit-Boost/commit-boost-client" rel="noopener nofollow ugc">repo</a> and governance calls (see below)</li>
<li>Sustainment / Development: 24/7 follow-the-sun coverage and highly engaged with client teams around upgrades / early in getting testnet support</li>
<li>Research: Helping with open-source research across Ethereum</li>
<li>Governance: This is still a WIP, but at a minimum will run a Commit-Boost, ACD-like calls (first one coming soon) to engage with stakeholders and drive consensus on upgrades / help coordinate around hard forks. A credibly neutral community member will lead these calls / this process that has experience with running governance processes over critical software within the Ethereum community</li>
<li>Funding: All grants</li>
</ul>
<p><strong>Where Will the Grants Come From</strong></p>
<p>The team is in the process of applying for grants from across the ecosystem. We are initially applying to a few organizations across the community that are supporting grants across research organizations and firms focused on PBS and staking. If teams are interested in providing a grant, feel free to comment below / reach out.</p>
<p><strong>Technical Roadmap</strong></p>
<p>Commit-Boost is currently in the MVP phase with <a href="http://holesky.beaconcha.in/slot/2022891" rel="noopener nofollow ugc">testing</a> underway in Holesky with multiple validators. This includes the full functionality of a PBS Module implementing MEV-Boost with additional telemetry and metrics collection. We are continuing the development and feature set of Commit-Boost targeting production-ready software and audits kicking off at the end of Q3. More details are in the Commit-Boost <a href="https://github.com/Commit-Boost/commit-boost-client/issues" rel="noopener nofollow ugc">repo</a> and we are keen to get feedback / engage with the community around these.</p>
<p>Some near-term high-level highlights from the roadmap include:</p>
<ul>
<li>Optimized and functional MEV-Boost module including multiple metrics for reporting and extensions such as configurable timing for get_header / get_payload calls</li>
<li>Pre-made dashboards on Grafana for all core services</li>
<li>Improved reliability and integrations for incident response</li>
<li>R&amp;D / spec signing mechanism to fit as many validator set-ups as possible</li>
<li>Expanding modularity and optionality (i.e., supporting different types of signatures and modules)</li>
</ul>
<p><strong>Commit-Boost Design Principles</strong></p>
<ul>
<li>Built for validators: Platform that not only can help validators today (i.e., can improve the lives of validators even if they just run an MEV-Boost module) but allows validators to be ready for the market of tomorrow (i.e., preconfs, inclusion lists, etc)</li>
<li>Neutrality: No opinions, the platform will be proposer commitment agnostic, relay agnostic, transaction flow agnostic, etc. The goal is to build a platform that doesn’t limit the design space downstream while reducing risks of fragmentation for validators and Ethereum</li>
<li>Unified: Validators run one core sidecar with the ability to opt into many different commitments</li>
<li>Safety: Open-source code developed with input by the community with community reviews / audits</li>
<li>Reduce risks: Modularized and transparency are core to reducing risk / overhead for the proposer to manage commitments and their broader operational processes</li>
<li>Values aligned: Public good with no plans for monetization. We will continuously ask ourselves: would Vitalik run Commit-Boost and can this be designed in a way to increase the decentralization of Ethereum block construction</li>
</ul>
<p><strong>From the Perspective of the Proposer</strong></p>
<p>More details on what it takes to run Commit-Boost as a node operator can be found <a href="https://commit-boost.github.io/commit-boost-client/get_started/overview" rel="noopener nofollow ugc">here</a>. Please note that this has not been finalized and over the next few weeks we will be making updates (see roadmap / milestones above).</p>
<ul>
<li>Run a single sidecar with support for MEV-Boost and other proposer commitments protocols, such as precons / other commitments</li>
<li>Out-of-the-box support for metrics reporting and dashboards to have clear insight around what is happening in your validator seen through dashboards such as Grafana</li>
<li>Plug-in system to add custom modules, i.e., receive a notification on Telegram if a relay fails to deliver a block</li>
<li>Standardized way to provide a signature to opt into a commitment</li>
<li>Creates constraints / condition sets and pass these constraints downstream</li>
</ul>
<p><strong>From the Perspective of the Proposer Commitment Protocol / Module Creator</strong></p>
<p>More details on what it takes to build a module / metrics can be found <a href="https://commit-boost.github.io/commit-boost-client/category/developing" rel="noopener nofollow ugc">here</a>. Please note that this has not been finalized and over the next few weeks we will finalize moving parts that impact module creators (see roadmap / milestones above).</p>
<ul>
<li>A modular platform to develop and distribute proposer commitments protocols</li>
<li>A single API to interact with validators</li>
<li>Support for hard-forks and new protocol requirements</li>
</ul>
<p><strong>Architecture of Commit-Boost</strong></p>
<p>More details can be found in the Commit-Boost <a href="https://commit-boost.github.io/commit-boost-client/" rel="noopener nofollow ugc">documentation</a>. However, below is a schematic of Commit-Boost. This proposed architecture allows proposers to run one sidecar, but still retain the ability to opt into a network of proposer commitment modules. More specifically, with this middleware, the proposer will only need to (in the case of delegation / light weight commitments) run one sidecar and limit their responsibilities to only selecting which module / proposer commitment protocol they would like to subscribe to.</p>
<p>It is important to note that the below depiction contains just a few examples of proposer commitment modules that can run on Commit-Boost. The design space for modules is completely open / not gated by the Commit-Boost software and proposers will be responsible for opting into the commitments they wish to subscribe to (i.e., a proposer is responsible for which modules they will subscribe to).</p>
<p><strong>Terminology</strong></p>
<ul>
<li>Proposer: entity, staking pool NoOp, or DVT cluster with the right to propose the next block</li>
<li>Commitment: a constraint or condition that the proposer choses and agrees to via a signature</li>
<li>Key Manager: some proposers use key managers or remote signers as part of their proposer / validator duties. Please note, that Commit-Boost is being designed in a way where it does not require validators to run key managers and working on solutions for monolithic set-ups</li>
<li>Consensus Client: for example, Lighthouse or Teku (see <a href="https://ethereum.org/en/developers/docs/nodes-and-clients/" rel="noopener nofollow ugc">here</a> for more details)</li>
<li>Commitment Modules: community-built modules allowing proposers to make commitment, including some of the logic of the proposer commitment protocol</li>
<li>Signer API: The signer API is one of the core components around Commit-Boost. This is used to provide signatures from the proposer to the proposer commitment protocol. This is still in the design but proxy signatures will be used in nearly all cases (there are some outlier cases). For more details on the API please see <a href="https://commit-boost.github.io/commit-boost-client/api/" rel="noopener nofollow ugc">here</a>. For an example of how to communicate with the Signer API, please see <a href="https://commit-boost.github.io/commit-boost-client/developing/commit-module" rel="noopener nofollow ugc">here</a></li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/a/ca4fdf7f738261cf46b1505dc56198da182592dc.png" title="Schematic"><img alt="Schematic" height="411" src="https://ethresear.ch/uploads/default/optimized/3X/c/a/ca4fdf7f738261cf46b1505dc56198da182592dc_2_690x411.png" width="690" /></a></div><p></p>
<p>Using this as a middleware instead of direct modification to the consensus client or running a sidecar per commitment will allow for each component to be sustained independently and will provide for cross proposer commitment compatibility. This will also allow for a bit of time for the market to play out, but via a public good, standardize the last mile of communication to help address the risks (discussed in the background section above) developing. Once the market does play out, and the community is able to observe some dynamics (the good and the bad), we can and should push for CL changes.</p>
<p><strong>Resources</strong></p>
<ul>
<li><a href="https://github.com/Commit-Boost" rel="noopener nofollow ugc">Commit-Boost Repo</a></li>
<li>Commit-Boost <a href="https://commit-boost.github.io/commit-boost-client/" rel="noopener nofollow ugc">documentation</a></li>
<li>List of presentations</li>
<li>Original post on ETH Research, read more <a href="https://ethresear.ch/t/based-proposer-commitments-ethereum-s-marketplace-for-proposer-commitments/19517">here</a></li>
<li>First presentation to the community can be found <a href="https://www.youtube.com/watch?v=jrm4ZUoj9xY&amp;list=PLJqWcTqh_zKHDFarAcF29QfdMlUpReZrR&amp;index=11" rel="noopener nofollow ugc">here</a></li>
<li>Second presentation at zuBerlin can be found <a href="https://streameth.org/zuberlin/watch?session=66681afef9b8e98b1ec95fdd" rel="noopener nofollow ugc">here</a></li>
<li>zuBerlin Devnet notion can be found <a href="https://twisty-wednesday-4be.notion.site/ZuBerlin-Preconfs-Devnet-b693047f41e7407cadac0170a6711dea" rel="noopener nofollow ugc">here</a></li>
<li>Mev-Boost Community call <a href="https://www.youtube.com/watch?v=UgoFjNkkTac" rel="noopener nofollow ugc">here</a></li>
<li>Espresso / One Balance Sequencing day here (this will be updated when the link is ready)</li>
<li>Gattaca MEV Day here (this will be updated when the link is ready)</li>
</ul>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/commit-boost-proposer-platform-to-safely-make-commitments/20107">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 19 Jul 2024 14:33:05 +0000</pubDate>
</item>
<item>
<title>What is "RealTPS" in Blockchain</title>
<link>https://ethresear.ch/t/what-is-realtps-in-blockchain/20098</link>
<guid>https://ethresear.ch/t/what-is-realtps-in-blockchain/20098</guid>
<content:encoded><![CDATA[
<div> 关键词：区块链、TPS、透明度、交易定义、测量工具

总结:<br />这篇文章讨论了区块链中TPS（交易每秒）指标的不透明性问题。传统上，TPS被模糊地定义和测量，与区块链的透明性原则相冲突。文章指出，应以确认交易（ConfirmedTransactions）为基础来计算TPS，质疑现有的高TPS宣称可能未经详细说明。文章提到Hyperledger的Caliper和Quorum推荐的测量工具存在局限，如只考虑单节点性能，未充分反映分布式网络特性。作者团队开发了新的工具AnTPS，旨在提供透明的多节点、多账户测量，支持不同场景和环境，以改进对区块链TPS的准确评估。 <div>
<p>Authors: <a href="https://x.com/Kyungmin7984" rel="noopener nofollow ugc">@Kyungmin</a> <a href="https://github.com/bicoCrypto" rel="noopener nofollow ugc">@bicoCrypto</a> <a href="https://github.com/solmingming" rel="noopener nofollow ugc">@solmingming</a></p>
<h2><a class="anchor" href="https://ethresear.ch#tldr-1" name="tldr-1"></a>TL;DR</h2>
<p>The current concept of TPS (Transactions Per Second) in blockchain is being disclosed in an ambiguous and opaque manner, conflicting with blockchain’s core value of transparency. This article reconsiders the definition of transactions in blockchain, compares theoretical figures with actual measurements, evaluates existing measurement tools, introduces our self-developed tool, and proposes a more accurate and transparent TPS measurement method.</p>
<h2><a class="anchor" href="https://ethresear.ch#problem-2" name="problem-2"></a>Problem</h2>
<p>The biggest change in the transition from Web2 to Web3 is decentralization. This has led to improved system accessibility and increased information transparency. However, there is still opaque information in the blockchain ecosystem, with TPS being a prime example.</p>
<p>In transaction processing systems, especially financial systems, TPS is a crucial performance indicator. However, the TPS information currently provided in blockchain is limited to simple figures, with detailed information about measurement methods and processes remaining opaque.</p>
<p>While blockchain smart contracts are operated transparently through verification and auditing, we still rely on the foundation’s system for the blockchain nodes themselves, lacking verification procedures similar to smart contracts.</p>
<h2><a class="anchor" href="https://ethresear.ch#tps-in-traditional-web2-3" name="tps-in-traditional-web2-3"></a>TPS in traditional Web2</h2>
<p>When discussing blockchain TPS, VISA’s processing capability is often mentioned as a comparison. <a href="https://www.reddit.com/r/nanocurrency/comments/82438o/visa_is_capable_of_performing_24000_transactions/" rel="noopener nofollow ugc">VISA officially announced a processing capability of 24,000 TPS</a>, but <a href="https://news.bitcoin.com/no-visa-doesnt-handle-24000-tps-and-neither-does-your-pet-blockchain/" rel="noopener nofollow ugc">this has been questioned</a>:</p>
<p>In centralized Web2 systems, it’s difficult to verify such issues. However, blockchain (Web3) systems are decentralized and their code is managed as open source, making it possible to verify TPS.</p>
<h2><a class="anchor" href="https://ethresear.ch#tps-in-web3-4" name="tps-in-web3-4"></a>TPS in Web3</h2>
<p>In blockchain systems with public nodes and permissionless nodes, anyone can participate in the network, operate nodes, and access the system. Even without connecting to the mainnet or testnet, the source code is publicly available, allowing independent network construction or modification after forking.</p>
<p>Ethereum and most EVM-compatible blockchains publish high TPS figures. For example, Avalanche C-Chain is introduced as capable of achieving 4,500 TPS. However, information on how this figure was measured is not provided.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/4/748ee3c646e8c2964dd294eefe5aa1491ce65b45.jpeg" title="Image"><img alt="Image" height="378" src="https://ethresear.ch/uploads/default/optimized/3X/7/4/748ee3c646e8c2964dd294eefe5aa1491ce65b45_2_690x378.jpeg" width="690" /></a></div><p></p>
<h2><a class="anchor" href="https://ethresear.ch#time-to-define-transaction-5" name="time-to-define-transaction-5"></a>Time to Define Transaction</h2>
<p>In EVM blockchains, the term “Transaction” is used in various contexts:</p>
<ul>
<li>SendTransaction: Simply refers to the act of sending a transaction, without guaranteeing the final state or completeness of the transaction.</li>
<li>PendingTransaction: The state where a transaction is waiting in the node’s memory pool (Mempool).</li>
<li>QueuedTransaction: Similar to Pending, waiting in the node’s memory pool, but distinguished in the serialization process through Nonce.</li>
<li>ConfirmedTransaction: The state where a transaction receipt has been issued, indicating the transaction has succeeded or failed.</li>
</ul>
<p>We believe that TPS should be calculated based on ConfirmedTransactions when measuring. Based on this, we propose the following formula for calculating TPS:<br />
TPS = BlockGasLimit / (TxGasUsed * BlockCreationTime)</p>
<p>Currently, Avalanche C-Chain’s BlockGasLimit is 15,000,000<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/9/c909ced7994a754fc7875e8dd1730093faedad3e.png" title="Tx Gaslimit"><img alt="Tx Gaslimit" height="225" src="https://ethresear.ch/uploads/default/optimized/3X/c/9/c909ced7994a754fc7875e8dd1730093faedad3e_2_690x225.png" width="690" /></a></div><p></p>
<p>Even assuming the simplest transaction (TxGasUsed = 21,000) and the shortest block creation time (BlockCreationTime = 1 second), the theoretical maximum TPS is 715. This shows a significant difference from the officially announced 4,500 TPS. (The actual measured value would naturally be even lower)</p>
<p>We speculate that this difference may occur due to:</p>
<ul>
<li>The transaction standard used in TPS calculation may not be ConfirmedTransaction</li>
<li>The Avalanche version that achieved 4,500 TPS may differ from the version currently used in public nodes</li>
<li>Differences in TPS measurement methods and methodologies</li>
</ul>
<p>Such opaque information raises questions about the reliability and accuracy of TPS figures.<br />
Monad has published a critical analysis of these limitations of blockchain TPS: <a href="https://www.monad.xyz/wtf-is-tps" rel="noopener nofollow ugc">WTF is TPS?</a></p>
<h2><a class="anchor" href="https://ethresear.ch#tps-benchmark-tools-6" name="tps-benchmark-tools-6"></a>TPS Benchmark Tools</h2>
<p>There are currently two main blockchain TPS benchmark tools in use:</p>
<ol>
<li><a href="https://github.com/hyperledger/caliper" rel="noopener nofollow ugc">Hyperledger Caliper</a>: Developed by the Hyperledger Foundation</li>
<li><a href="https://github.com/drandreaskrueger/chainhammer" rel="noopener nofollow ugc">ChainHammer</a>: Recommended by Quorum (a private blockchain developed by ConsenSys)<br />
Note: ChainHammer’s most recent commit was 2 years ago, making it essentially outdated.</li>
</ol>
<p>Caliper is written in JavaScript and is a highly complete project. However, there are doubts about whether it is optimized for measuring “blockchain” TPS:</p>
<ol>
<li>TPS measurement on a single node:<br />
<div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/7/67bd0b50a563472ac93dafc3fa2e17effa0919e8.png" title="Figure2"><img alt="Figure2" height="389" src="https://ethresear.ch/uploads/default/optimized/3X/6/7/67bd0b50a563472ac93dafc3fa2e17effa0919e8_2_690x389.png" width="690" /></a></div></li>
</ol>
<p>The core of blockchain is distributed storage of data through consensus. However, Caliper only conducts TPS measurements on a single node, which can measure the TPS of individual nodes but does not accurately reflect the TPS of the entire blockchain network. (The transaction propagation process is not considered)</p>
<ol>
<li>Limitation of measurement from a single account:<br />
In EVM, EOA (Externally Owned Account) has a Nonce value, causing transactions to be processed sequentially.<br />
While 2024 was predicted to be the era of parallel EVM, current parallel processing technology still proceeds in an optimistic manner, requiring re-execution in case of conflicts. (Cases requiring re-execution can hardly be considered true parallel execution.)<br />
Therefore, execution from a single account versus multiple accounts can have a significant impact on TPS.</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#antpsan-ti-tps-7" name="antpsan-ti-tps-7"></a>AnTPS(An-ti TPS)</h2>
<p>To improve these limitations, we have developed our own blockchain benchmark tool, AnTPS, using Golang: <a href="https://github.com/decipherhub/AnTPS" rel="noopener nofollow ugc">Github</a><br />
Features include:<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/0/f0cff11ba6f328b500e85b78f194763580ca940c.png" title="image"><img alt="image" height="283" src="https://ethresear.ch/uploads/default/optimized/3X/f/0/f0cff11ba6f328b500e85b78f194763580ca940c_2_690x283.png" width="690" /></a></div><p></p>
<ul>
<li>Transparently providing measurement environment/results.</li>
<li>Conducting measurements on at least two or more nodes.</li>
<li>Supporting measurement cases for both single and multiple accounts.</li>
<li>Supporting various scenarios during measurement (ERC20/721/1155/NativeToken).</li>
<li>Supporting not only local environment measurements but also Cloud environments through IaC.</li>
</ul>
<p>Our goal is to overcome the limitations of existing tools while providing information transparently.<br />
We welcome your opinions and feedback. Thank you.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/what-is-realtps-in-blockchain/20098">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 19 Jul 2024 03:52:11 +0000</pubDate>
</item>
<item>
<title>Based Preconfirmations with Multi-round MEV-Boost</title>
<link>https://ethresear.ch/t/based-preconfirmations-with-multi-round-mev-boost/20091</link>
<guid>https://ethresear.ch/t/based-preconfirmations-with-multi-round-mev-boost/20091</guid>
<content:encoded><![CDATA[
<div> 关键词：基于预确认、多轮MEV-Boost、L1 PBS管道、外部性、区块链安全

总结:
本文探讨了基于预确认的 rollup 模型存在的问题，尤其是其12秒的区块时间所导致的负面效应。作者提出了多轮MEV-Boost（MR-MEV-Boost）的概念，通过在一个槽位内运行多次MEV-Boost拍卖来实现快速预确认，从而保持了基于预确认rollup的优点如同步兼容性和L1抗审查性，同时减轻了预确认带来的负面影响。文章分析了MR-MEV-Boost的优势和挑战，如减少了延迟竞赛、缓解拥堵、改进定价机制，并讨论了用户和协议设计中的公平交换问题。尽管存在一些限制，如较慢的预确认速度和对-relayer负担的增加，但该方案为解决基于预确认的rollup问题提供了新的思路。 <div>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/b/fb1798cc79775d7958124717d6ba5cc97c1aa008.jpeg" title="image"><img alt="image" height="394" src="https://ethresear.ch/uploads/default/original/3X/f/b/fb1798cc79775d7958124717d6ba5cc97c1aa008.jpeg" width="690" /></a></div><p></p>
<p>By <a href="https://twitter.com/linoscope" rel="noopener nofollow ugc">Lin Oshitani</a> (<a href="https://switchboard.nethermind.io/" rel="noopener nofollow ugc">Nethermind Switchboard</a>, <a href="https://www.nethermind.io/nethermind-research" rel="noopener nofollow ugc">Nethermind Research</a>). Many thanks to <a href="https://twitter.com/ConorMcMenamin9" rel="noopener nofollow ugc">Conor</a> for the detailed back-and-forth on crafting this document and to <a href="https://www.linkedin.com/in/aikaterini-panagiota-stouka/" rel="noopener nofollow ugc">Aikaterini</a>, <a href="https://x.com/ElenaPetreska0x" rel="noopener nofollow ugc">Elena</a>, <a href="https://twitter.com/smartprogrammer" rel="noopener nofollow ugc">Ahmad</a>, <a href="https://twitter.com/aj_jalan" rel="noopener nofollow ugc">Anshu</a>, <a href="https://twitter.com/swp0x0" rel="noopener nofollow ugc">Swapnil</a>, <a href="https://twitter.com/tkstanczak" rel="noopener nofollow ugc">Tomasz</a>, <a href="https://twitter.com/totorovirus" rel="noopener nofollow ugc">Jinsuk</a>, <a href="https://twitter.com/0xQuintus" rel="noopener nofollow ugc">Quintus</a>, <a href="https://x.com/ceciliaz030" rel="noopener nofollow ugc">Ceciliaz</a>, and <a href="https://twitter.com/Brechtpd" rel="noopener nofollow ugc">Brecht</a> for the helpful comments and/or review. This work was partly funded by Taiko. The views expressed are my own and do not necessarily reflect those of the reviewers or Taiko.</p>
<h1><a class="anchor" href="https://ethresear.ch#tldr-1" name="tldr-1"></a>TL;DR</h1>
<p>As we outlined in our previous post <a href="https://ethresear.ch/t/strawmanning-based-preconfirmations/19695">Strawmanning Based Preconfirmations</a>, naive implementations of based preconfirmations introduce many negative externalities that require thoughtful consideration.</p>
<p>In this post, we will expand on the negative externalities of based preconfirmations by examining them through the lens of the L1 PBS pipeline. Then, we propose <em>multi-round MEV-Boost</em>, a modification of MEV-Boost that enables based preconfirmations by running multiple rounds of MEV-Boost auctions within a single slot. This approach inherits the L1 PBS pipeline and mitigates the negative externalities of based preconfirmations as a result.</p>
<h1><a class="anchor" href="https://ethresear.ch#motivation-2" name="motivation-2"></a>Motivation</h1>
<p>As Justin Drake <a href="https://ethresear.ch/t/based-rollups-superpowers-from-l1-sequencing/15016#:~:text=a%20based%20rollup%20is%20one%20where%20the%20next%20L1%20proposer%20may%2C%20in%20collaboration%20with%20L1%20searchers%20and%20builders%2C%20permissionlessly%20include%20the%20next%20rollup%20block%20as%20part%20of%20the%20next%20L1%20block">defines</a> in the original post, based rollups are rollups “where the next L1 proposer may, in collaboration with L1 searchers and builders, permissionlessly include the next rollup block as part of the next L1 block”. Using the <a href="https://flashbots.mirror.xyz/bqCakwfQZkMsq63b50vib-nibo5eKai0QuK7m-Dsxpo" rel="noopener nofollow ugc">MEV supply chain</a> diagram, based rollups can be illustrated below:<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/c/ac29f931cf332c1b1b38334158920d6807f499f1.png" title="Based Rollups (5) (1)"><img alt="Based Rollups (5) (1)" height="346" src="https://ethresear.ch/uploads/default/optimized/3X/a/c/ac29f931cf332c1b1b38334158920d6807f499f1_2_690x346.png" width="690" /></a></div><p></p>
<p>Notice that the L2 transactions, represented as the red line, go through the same process as the L1 transactions, represented as the black line. By effectively “piggybacking” the L1 PBS pipeline, based rollups provide two key benefits:</p>
<ul>
<li><strong>Benefit 1</strong>: Since no additional actors (and thus no additional choke points) are introduced for L2 sequencing, based rollups fully inherit L1 censorship resistance, liveness, and credible neutrality.</li>
<li><strong>Benefit 2</strong>: Since the L1 and L2 transactions are sequenced by the same entity (the builder), based rollups enable not only synchronous L2-L2 composability but also synchronous L1-L2 composability.</li>
</ul>
<p>Based rollups are great. They solve L2 fragmentation and sequencer decentralization while enabling L1 composability and inheriting L1’s censorship resistance, liveness, and credible neutrality. They are the only rollups that can have these properties simultaneously.</p>
<p>However, they have one large drawback: they also inherit the 12-second L1 block time. To address the slow confirmation time, Justin introduced <a href="https://ethresear.ch/t/based-preconfirmations/17353">base preconfirmations</a>. In this approach, L1 proposers can opt into providing preconfirmations for based rollup L2 transactions, as shown below:<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/e/6e65221a830715cead4428b2724688ce66d7520c.png" title="Based Preconf (2) (1)"><img alt="Based Preconf (2) (1)" height="346" src="https://ethresear.ch/uploads/default/optimized/3X/6/e/6e65221a830715cead4428b2724688ce66d7520c_2_690x346.png" width="690" /></a></div><p></p>
<p>Since providing preconfirmations requires technical sophistication, most based preconfirmation designs include a delegation mechanism that allows validators to outsource the preconfirmation duty to a designated preconfer, as illustrated below:<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/9/f9346fd068da5aa3ae0e57fceccc6c2af223b958.png" title="Based Preconf with delegation (2)"><img alt="Based Preconf with delegation (2)" height="346" src="https://ethresear.ch/uploads/default/optimized/3X/f/9/f9346fd068da5aa3ae0e57fceccc6c2af223b958_2_690x346.png" width="690" /></a></div><p></p>
<p>Notice that L2 and L1 transactions no longer share the block-building pipeline. As such, the benefits of based rollups are diminished:</p>
<ul>
<li>On benefit 1: We introduced an additional choke point to the system, the preconfer, which can censor L2 transactions or degrade L2 liveness by going down. As a result, the inheritance of L1 censorship resistance and liveness are degraded.</li>
<li>On benefit 2: We now have two parallel block-building entities: one for L1 (the builder) and another for L2 (the preconfer). Consequently, L1-L2 composability now requires coordination between the builder and the preconfer. This adds complexity and can lead to <em>builder-preconfer integration</em>, where the proposer delegates not only their preconfirmation right but also the whole block-building right to the preconfer ahead of their slot.</li>
</ul>
<p>In summary, by introducing preconfirmations, we lost the below structure:<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/c/ac29f931cf332c1b1b38334158920d6807f499f1.png" title="Based Rollups (5) (1)"><img alt="Based Rollups (5) (1)" height="346" src="https://ethresear.ch/uploads/default/optimized/3X/a/c/ac29f931cf332c1b1b38334158920d6807f499f1_2_690x346.png" width="690" /></a></div><p></p>
<p>As a result, many of the benefits of based rollups are diminished.</p>
<p>So, what if we keep this pipeline but run it multiple times within a slot to achieve fast preconfirmations? This brings us to the main contribution of this document: <em>Multi-round MEV-Boost</em>.</p>
<h1><a class="anchor" href="https://ethresear.ch#multi-round-mev-boost-3" name="multi-round-mev-boost-3"></a>Multi-round MEV-Boost</h1>
<p>At a high level, Multi-round MEV-Boost, or <em>MR-MEV-Boost</em> (pronounced “<em>mister-mev-boost</em>”, h/t <a href="https://twitter.com/ConorMcMenamin9" rel="noopener nofollow ugc">Conor</a> for the idea on the pronounciation :)) for short, works as follows:</p>
<ul>
<li>Split each slot into a fixed number of <em>rounds, e</em>.g., 4 rounds with 3 seconds each.</li>
<li>Within each round, run a single MEV-Boost auction. As a result of the auction, a single <em>partial block</em> (a.k.a <em>partial payload</em>) will be signed and published, i.e., the partial block will be <em>preconfirmed</em>.</li>
<li>The full block is created and published at the end of the slot. The full block should contain the partial blocks in the exact order they were preconfirmed without inserting any transactions before or in between.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#refresher-mev-boost-4" name="refresher-mev-boost-4"></a>Refresher: MEV-Boost</h2>
<p>Before diving deeper into the proposed protocol, let’s quickly review today’s <a href="https://docs.flashbots.net/flashbots-mev-boost/introduction" rel="noopener nofollow ugc">MEV-Boost</a> PBS pipeline used in L1 Ethereum.<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/e/fe199d4c844c9230a675292724deb29c9e03a3df.png" title="MEV-Boost"><img alt="MEV-Boost" height="500" src="https://ethresear.ch/uploads/default/original/3X/f/e/fe199d4c844c9230a675292724deb29c9e03a3df.png" width="609" /></a></div><p></p>
<ol>
<li>Builders send the <code>header</code>, <code>payload</code>, and <code>bid</code> to the relayer.</li>
<li>The relayer checks the validity (the <code>bid</code> is correct, the <code>payload</code> does not contain invalid transactions, etc), stores the <code>payload</code>, and then sends the <code>header</code> and <code>bid</code> to the proposer.</li>
<li>The proposer selects the header with the highest bid, signs it, and then sends the signed header to the relayer.</li>
<li>The relayer propagates the signed header and corresponding payload to the network.</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#protocol-description-5" name="protocol-description-5"></a>Protocol Description</h2>
<p>In this section, we describe the MR-MEV-Boost protocol.</p>
<h3><a class="anchor" href="https://ethresear.ch#protocol-flow-overview-6" name="protocol-flow-overview-6"></a>Protocol Flow Overview</h3>
<p>To incentivize proposers to provide preconfirmations, we introduce a <em>preconf transaction</em>, where the payment of a <em>preconf tip</em> is conditioned on being preconfirmed. It will include the following information on top of the transaction payload itself:</p>
<ul>
<li><code>tip</code>: The preconfirmation tip paid for being preconfirmed.</li>
<li><code>target_slot</code>: The latest slot in which the preconf transaction can be included.</li>
<li><code>target_round</code>: The latest round within the <code>target_slot</code> in which the preconf transaction can be included.</li>
</ul>
<p>The <a href="https://ethresear.ch#preconf-transaction">Preconf Transaction</a> section will discuss the encoding and enforcement of these conditions.</p>
<p>Using this new transaction type, MR-MEV-Boost works as follows:<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/a/7a4dc3f4c38b4184016390e2e48dc44e5630da74.png" title="MR-MEV-Boost"><img alt="MR-MEV-Boost" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/7/a/7a4dc3f4c38b4184016390e2e48dc44e5630da74_2_252x500.png" width="252" /></a></div><p></p>
<ol>
<li>Users submit preconf transactions to the builders. The submission can be through any order flow pipeline used in current L1, such as:
<ul>
<li>Public mempool.</li>
<li>Private order flow.</li>
<li>Order flow auctions on <a href="https://mevblocker.io/" rel="noopener nofollow ugc">MEVBlocker</a>, <a href="https://docs.flashbots.net/flashbots-protect/mev-share" rel="noopener nofollow ugc">MEV-Share</a>, <a href="https://writings.flashbots.net/the-future-of-mev-is-suave" rel="noopener nofollow ugc">SUAVE</a>, etc.</li>
</ul>
</li>
<li>The builders build <code>partial_payload</code>s. The partial payload built by the builders should only include preconf transactions with <code>target_slot</code> and <code>target_round</code> at or after the current block/round. To commit to this, the builder signs the <code>merkle_root</code> (denoted as <code>sig_b</code> ) and becomes subject to builder slashing condition 1, described in the <a href="https://ethresear.ch#slashing-conditions">slashing condition section</a>.</li>
<li>The relayer checks the validity (e.g., the <code>bid</code> is correct, the <code>partial_payload</code> does not contain invalid transactions, etc.), stores the <code>partial_payload</code>, and then sends the <code>merkle_root</code> and <code>bid</code> to the proposer.</li>
<li>Proposer signs (denoted as <code>sig_p</code>) and returns the selected <code>merkle_root</code> together with the current <code>round</code>.</li>
<li>The relay publishes the selected <code>partial_payload</code> and the associated round and signatures to the preconf network. Note that the preconf network is different from the existing L1 p2p network. Only entities interested in the preconfirmed state (partial block builders, relays, full-node providers, etc.) must subscribe to the preconf network.</li>
<li>End users—or, more precisely, the L2/L1 full nodes to which they are connected—verify that the <code>merkle_root</code> is signed by the proposer and is associated with the current <code>round</code>. Upon confirmation, they accept the <code>partial_payload</code> as preconfirmed and execute it to update to the latest preconfirmed state.</li>
<li>
<ol>
<li>to 6. is repeated multiple rounds within the slot. The number of rounds within each slot will be fixed. The final round will run concurrently with the full block MEV-Boost auction at 8.-11.</li>
</ol>
</li>
<li>to 11. The MEV-Boost auction is conducted for the full block. An important difference with the usual L1 MEV-Boost auction is that the <code>merkle_proofs</code> are propagated from the builder to the proposer. These proofs prove that the <code>partial_payload</code>s are included in the full block in the order they were preconfirmed without any other transaction being inserted before or between them. By validating these proofs, the proposer can ensure that the proposer slashing condition 2, described in the <a href="https://ethresear.ch#slashing-conditions">slashing conditions section</a>, is not violated without needing to trust the relayer (h/t to <a href="https://twitter.com/Brechtpd" rel="noopener nofollow ugc">Brecht</a> for the idea of using Merkle proofs here).</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#preconf-transaction-7" name="preconf-transaction-7"></a>Preconf Transaction</h3>
<p>Let’s consider the encoding of the preconf transactions. For L2s, the additional fields can be introduced as custom encodings of the transactions. For L1, they can be implemented through an <a href="https://eips.ethereum.org/EIPS/eip-4337" rel="noopener nofollow ugc">ERC-4337</a>-style entry point contract that wraps the contract calls with additional information.</p>
<p>To enforce the expiration, the L2 execution layer (or <a href="https://github.com/ethereum-optimism/specs/blob/b1c9b7985b65bd2d065a414f5ad0552f36e48540/specs/protocol/derivation.md#deriving-the-transaction-list" rel="noopener nofollow ugc">derivation layer</a>) and L1 entry point contract will filter out preconf transactions with expired <code>target_slot</code>. On the other hand, since the L1 is unaware of the concept of rounds, expiration based on <code>target_round</code> will be enforced via builder slashing condition 1, explained in the next section.</p>
<h3><a class="anchor" href="https://ethresear.ch#slashing-conditions-8" name="slashing-conditions-8"></a>Slashing Conditions</h3>
<p>To ensure that the full block matches with the preconfed partial blocks, the proposer will be subject to:</p>
<ul>
<li><strong>Proposer slashing condition 1</strong>: The proposer must not sign two conflicting <code>merkle_root</code>s within the same round.</li>
<li><strong>Proposer slashing condition 2</strong>: The final <code>full_payload</code> should contain all the <code>partial_payload</code>s in the order they are signed and published without any other transaction being inserted before or in between.</li>
</ul>
<p>Furthermore, to crypto-economically enforce the expiration of preconf transactions, the builder will be subject to:</p>
<ul>
<li><strong>Builder slashing condition 1:</strong> Each <code>partial_payload</code> must only include preconf transactions with <code>target_slot</code> and <code>target_round</code> at or after the current slot/round.</li>
</ul>
<p>We impose this condition on the builder rather than the proposer because the proposer does not see the partial payload when signing. An alternative approach would be to make this a proposer slashing condition and require the relayer to ensure the condition is not violated. However, this would necessitate the proposer trusting the relayer to avoid being slashed rather than only relying on the relayer to avoid missing a slot, as is currently done in L1 MEV-Boost.</p>
<h3><a class="anchor" href="https://ethresear.ch#user-actions-9" name="user-actions-9"></a>User Actions</h3>
<p>To mitigate the <a href="https://ethresear.ch/t/strawmanning-based-preconfirmations/19695#problem-4-fair-exchange-7">fair exchange problem</a>, wallets or full nodes to which end users are connected should take the actions below:</p>
<ul>
<li><strong>User action 1</strong>: Stop submitting preconf transactions if preconfirmed <code>partial_payload</code>s are not published in a timely manner. For example, if we have 4 rounds in a slot, then stop submitting preconf transactions if a <code>partial_payload</code> is not published every 3 seconds.</li>
<li><strong>User action 2</strong>: Set <code>target_slot</code> and <code>target_round</code> to a reasonably close block and round (e.g., one or two rounds ahead). By doing so, builders are required to respond in a timely manner to preconfirmation transactions to avoid the preconfirmation transactions being invalidated.</li>
</ul>
<p>More on how the fair exchange is addressed is described in the <a href="https://ethresear.ch#analysis">analysis section</a>.</p>
<h3><a class="anchor" href="https://ethresear.ch#l1-l2-composability-10" name="l1-l2-composability-10"></a>L1-L2 Composability</h3>
<p>Since the partial payloads can contain both L1 and L2 transactions, builders can ensure L1-L2 composability by including L1-L2 transaction bundles in the partial payloads.</p>
<h3><a class="anchor" href="https://ethresear.ch#non-opted-in-slots-11" name="non-opted-in-slots-11"></a>Non-opted-in Slots</h3>
<p>When the current L1 slot’s proposer has not opted in as a preconfer, L1 transactions will be proposed by the current proposer, while L2 transactions will be proposed by the next opted-in preconfer in the lookahead (we follow Justin’s <a href="https://ethresear.ch/t/based-preconfirmations/17353#:~:text=proposer%20lookahead%E2%80%94higher%20precedence%20for%20smaller%20slot%20numbers">original based preconfirmation design</a> here). This results in two simultaneous MEV-Boost auctions: the usual L1 MEV-Boost auction signed by the current L1 proposer and the MR-MEV-Boost auction signed by the next preconfer. As a result, L1-L2 composability and L1 preconfirmation will be lost during these non-opted-in slots. Note that this limitation applies to all off-protocol preconfirmation designs.</p>
<h1><a class="anchor" href="https://ethresear.ch#analysis-12" name="analysis-12"></a>Analysis</h1>
<p>In this section, we will perform an initial analysis of the proposed protocol and identify its drawbacks.</p>
<h3><a class="anchor" href="https://ethresear.ch#have-we-solved-the-problems-13" name="have-we-solved-the-problems-13"></a>Have we solved the problems?</h3>
<p>Let’s revisit the problems raised in the <a href="https://ethresear.ch/t/strawmanning-based-preconfirmations/19695">Strawmanning Based Preconfirmations</a> post and see if and how MR-MEV-Boost addresses them.</p>
<p><strong>Problem 1: Latency race</strong></p>
<p>Latency races are when searchers fight to be the first to access the preconfer, leading to colocation or vertical integration. With MR-MEV-Boost, this issue is largely mitigated by preconfirming batches and conducting auctions within the batch, as it promotes competition based on price rather than speed. It is generally acknowledged that batch auctions help reduce latency wars compared to continuous first-come, first-served ordering, as described in <a href="https://academic.oup.com/qje/article/130/4/1547/1916146" rel="noopener nofollow ugc">this paper</a> and <a href="https://ethresear.ch/t/latency-arms-race-concerns-in-blockspace-markets/14957">this post</a>.</p>
<p><strong>Problem 2: Congestion</strong></p>
<p>Congestion issues arise when searchers flood the rollup with probabilistic arbitrage attempts. With MR-MEV-Boost, this issue is mitigated as searchers are incentivized to participate in auctions rather than resort to spam.</p>
<p><strong>Problem 3: Tip pricing</strong></p>
<p>The MEV-Boost auction will handle the tip pricing of the preconfirmation. Furthermore, by introducing batching and auctions within the batch, proposers can price the preconfirmation tips more effectively (hence capturing revenue) than by providing a continuous stream of per-transaction preconfirmations.</p>
<p><strong>Problem 4: Fair exchange</strong></p>
<p>Let’s see how MR-MEV-Boost addresses the <a href="https://ethresear.ch/t/strawmanning-based-preconfirmations/19695#problem-3-tip-pricing-6">fair exchange problem</a>, where the proposer withholds publishing preconfirmations to the user. Note that preconfers are incentivized to withhold preconf promises as much as possible to maximize their opportunity to reorder and insert transactions, thereby increasing their MEV.</p>
<p>There are two cases to consider:</p>
<ul>
<li>If the proposer withholds preconfirming partial payload (i.e., stops signing <code>merkle_root</code>s of <code>partial_payload</code>s), users will stop sending preconfirmation requests (user action 1), reducing the proposer’s order flow and, consequently, revenue.</li>
<li>If the proposer intentionally publishes empty partial payloads, pending preconf transactions will expire after a few rounds (user action 2 and builder slashing condition 1), reducing the proposer’s order flow and, consequently, revenue.</li>
</ul>
<p>In summary, end users monitor and enforce proposers’ honest behavior by linking the proposers’ revenue to the timely preconfirmation of partial payloads.</p>
<p>A potential alternative would be to introduce a committee to monitor and attest to the timely releases of partial payloads. However, this would require additional trust assumption to an external committee unless we enshrine the protocol into the L1. More on enshrinement in the <a href="https://ethresear.ch#future-direction">future direction section</a>.</p>
<p><strong>Problem 5: Liveness</strong></p>
<p>With existing based preconfirmation designs where preconfirmation duties are delegated ahead of the slot, liveness relies on this single external entity for the duration of the preconfer’s slot(s). On the other hand, with MR-MEV-Boost, liveness concerns are reduced as we do not introduce such “lock-in” to a specific entity before the slot. If some builders or relayers are unavailable, others can step in to maintain functionality. Moreover, even if the entire multi-round MEV-Boost pipeline fails, proposers still have the option to construct their own partial blocks and preconfirm them independently.</p>
<p><strong>Problem 6: Early auctions</strong></p>
<p>Early auctions are not introduced as preconfirmations are provided through the MEV-Boost JIT auctions.</p>
<h2><a class="anchor" href="https://ethresear.ch#round-interval-14" name="round-interval-14"></a>Round Interval</h2>
<p>How short can each round in MR-MEV-Boost be? If it is too long, it will degrade the user experience; if it is too short, it will impose excessive network and hardware requirements on builders and relays, thus hurting decentralization.</p>
<p>In each round, the relayer has two tasks:</p>
<ul>
<li>(A) Run the partial block auction.</li>
<li>(B) Propagate the partial block.</li>
</ul>
<p>Task (A) consists of the time it takes the builder to construct the block, the time it takes the relay to validate the block, and two network round-trips: one between the builder and the relay and another between the relay and the proposer. Assuming that <a href="https://x.com/SheaKetsdever/status/1808509437700665543" rel="noopener nofollow ugc">block construction</a>, validation, and network round-trips take 500 milliseconds each, we get a ballpark estimate of 2 seconds.</p>
<p>For task (B), considering L1 allocates 4 seconds for block propagation and 8 seconds for consensus, and no consensus is needed for partial blocks, a good upper bound for propagation time is 4 seconds. In practice, it should be much shorter because only block builders, relays, and full-node providers need to receive these partial blocks, and they have better network bandwidth and lower latency than average validators. Let’s assume 1-2 seconds for this analysis.</p>
<p>Combining 2 seconds for (A) and 1-2 seconds for (B) gives us 3-4 seconds per round.</p>
<p>These estimates are highly approximate, and further research and analysis are needed. Additionally, making the interval too short will intensify latency races toward the end of the batch duration, as described <a href="https://ethresear.ch/t/latency-arms-race-concerns-in-blockspace-markets/14957#auction-designs-for-transaction-ordering-2">here</a>, and should be considered.</p>
<h2><a class="anchor" href="https://ethresear.ch#drawbacks-15" name="drawbacks-15"></a>Drawbacks</h2>
<p>Next, we will outline the drawbacks of this protocol when compared to existing based preconfirmation designs, such as the one described in the <a href="https://ethresear.ch/t/based-preconfirmations/17353">original post</a>. An analysis of more general drawbacks of preconfirmations will be reserved for future work.</p>
<h3><a class="anchor" href="https://ethresear.ch#no-speed-of-light-continuous-preconfirmations-16" name="no-speed-of-light-continuous-preconfirmations-16"></a>No Speed-of-light Continuous Preconfirmations</h3>
<p>MR-MEV-Boost does not provide speed-of-light preconfirmations with hundreds of milliseconds latency, like <a href="https://docs.arbitrum.io/how-arbitrum-works/sequencer" rel="noopener nofollow ugc">Arbitrum’s first-come-first-serve sequencer</a>. Instead, it offers preconfirmations in batches with a few seconds of latency between them, similar to <a href="https://docs.optimism.io/connect/resources/glossary#time-slot" rel="noopener nofollow ugc">Optimism’s approach</a>.</p>
<p><a href="https://solana.com/" rel="noopener nofollow ugc">Solana</a> and <a href="https://www.jito.wtf/" rel="noopener nofollow ugc">Jito</a> provide an interesting case study on continuous versus batched preconfirmations. In Solana’s “continuous block building,” the leader streams (i.e., preconfirms) processed transactions continuously. Combined with Solana’s fixed low fee, continuous block building led to network spamming and latency races, causing validators to <a href="https://www.jito.network/blog/solving-the-mev-problem-on-solana-a-guide-for-stakers/" rel="noopener nofollow ugc">waste 58% of their time processing failed arbitrages</a>. Jito addressed this by introducing a 200ms “speed bump” (batches) and a mev-geth style bundle auction for batches, achieving an 80% share with their Jito validator client. This example indicates that that continuous preconfirmations are likely unsustainable due to spam and that batching and some auction for each batch are required. For more details, watch this informative talk by Zano Sherwani, co-founder of Jito, <a href="https://www.youtube.com/watch?v=c-O_JZI2QAA" rel="noopener nofollow ugc">here</a>.</p>
<h3><a class="anchor" href="https://ethresear.ch#relay-burden-17" name="relay-burden-17"></a>Relay Burden</h3>
<p>MR-MEV-Boost introduces additional burdens to the relays without incentives. Instead of managing a single round of MEV-Boost auctions, relayers must handle multiple rounds within a single slot, each within a limited timeframe. If the cost of operating a relayer increases too much, it may lead to further relayer centralization and <a href="https://collective.flashbots.net/t/builderelay/2688/1" rel="noopener nofollow ugc">builder-relay integration</a>, or alternatively, no relayer may opt to support MR-MEV-Boost. Relayer incentives are a <a href="https://www.gate.io/learn/articles/the-pursuit-of-relay-incentivization/1257" rel="noopener nofollow ugc">long-lasting problem</a> in L1, and MR-MEV-Boost likely worsen this situation.</p>
<p>One way to mitigate the issue is to incorporate <a href="https://frontier.tech/optimistic-relays-and-where-to-find-them" rel="noopener nofollow ugc">optimistic relay</a> schemes to reduce the relayer’s operational costs. With this approach, relayers optimistically assume the honesty of the block-builder and skip the validation work for payloads sent from the builder. If the builder is later found to deviate from honest behavior, their collateral will be used to refund the proposer. Optimistic relaying would be especially helpful as it allows relayers to bypass the need to validate the based rollup transactions when verifying partial blocks.</p>
<p>Another potential solution is for the proposers to share the preconfirmation tip revenue with the relay to compensate for the additional workload.</p>
<h3><a class="anchor" href="https://ethresear.ch#blob-efficiency-18" name="blob-efficiency-18"></a>Blob Efficiency</h3>
<p>So far, we have blurred the line between L1 and L2 preconfirmations. This is somewhat intentional, as L2 transactions are included within L1 transactions. However, there are cases where the difference becomes important.</p>
<p>Consider a scenario where the L2 transactions within a round cannot fill an entire blob. If we only support preconfirmations for L2 transactions by preconfirming the L1 transactions that contain them, we face a problem. Proposers would either have to preconfirm partially filled blob transactions at the end of the round or wait for another round to collect enough transactions to fill the blob.</p>
<p>One solution is to allow proposers to commit to a batch of L2 transactions without linking them to a specific L1 transaction. This would let the builder of the final full block aggregate the L2 transactions into one or more L1 blobs at the end of the slot.</p>
<h3><a class="anchor" href="https://ethresear.ch#issues-with-mev-boost-19" name="issues-with-mev-boost-19"></a>Issues with MEV-Boost</h3>
<p>MR-MEV-Boost inherits the existing L1 MEV-Boost pipeline, which also means that we inherit many of MEV-Boost’s downsides, such as <a href="https://arxiv.org/pdf/2305.19037" rel="noopener nofollow ugc">reliance on a handful of relays and builders</a>. However, based rollups aim to inherit the security of L1, not to exceed it. Therefore, being “as bad as” L1 is the best we can do as a based solution.</p>
<h1><a class="anchor" href="https://ethresear.ch#future-direction-20" name="future-direction-20"></a>Future Direction</h1>
<p>MR-MEV-Boost can be generalized as <em>partial-block preconfirmations</em>, where the proposer incrementally builds the block by committing to and publishing partial blocks during their slot.</p>
<p>One future direction would be to enshrine partial-block preconfirmations into the L1 protocol to achieve faster block times. This aligns with Vitalik’s <a href="https://vitalik.eth.limo/general/2024/06/30/epochslot.html" rel="noopener nofollow ugc">recent post</a> and offers several benefits over off-protocol designs like MR-MEV-Boost:</p>
<ul>
<li>Removes “non-opted-in” proposers, enabling L1 preconfirmations and L1-L2 composability for all slots.</li>
<li>Fully utilizes Ethereum’s validator set, potentially introducing lightweight <a href="https://ethresear.ch/t/payload-timeliness-committee-ptc-an-epbs-design/16054">PTC</a>-like attestations for timely partial payload releases.</li>
<li>Opens doors to increase the block times without degrading UX, which may help enable <a href="https://ethereum.org/en/roadmap/single-slot-finality/" rel="noopener nofollow ugc">single-slot finality</a>.</li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#related-work-21" name="related-work-21"></a>Related Work</h1>
<p>In his <a href="https://dba.xyz/were-all-building-the-same-thing/" rel="noopener nofollow ugc">latest post</a>, Jon Charbonneau explains in great detail how based rollups/preconfirmations work and the centralization risk of based preconfirmations.</p>
<p>Furthermore, partial-block preconfirmations are closely related to <a href="https://ethresear.ch/t/how-much-can-we-constrain-builders-without-bringing-back-heavy-burdens-to-proposers/13808">inclusion list</a> research, as both can be viewed under the broader concept of “partial-block building,” where different parts of a block are constructed at different times by different entities. For example, the <a href="https://research.eigenlayer.xyz/t/mev-boost-liveness-first-relay-design/15" rel="noopener nofollow ugc">MEV-Boost++ proposal</a> from Kyodo (EigenLayer) resembles MR-MEV-Boost, as both enable early commitment to partial blocks by imposing additional slashing conditions on the proposer.</p>
<h1><a class="anchor" href="https://ethresear.ch#conclusion-22" name="conclusion-22"></a>Conclusion</h1>
<p>We introduce MR-MEV-Boost, a design that enables based preconfirmations by running multiple rounds of MEV-Boost auctions within a single slot. By inheriting the L1 PBS pipeline, MR-MEV-Boost mitigates many of the negative externalities of based preconfirmations while retaining the benefits of based rollups.</p>
<p>At <a href="https://switchboard.nethermind.io/" rel="noopener nofollow ugc">Nethermind Switchboard</a>, we actively research and tackle the open challenges of based preconfirmations. We are also collaborating closely with Taiko to develop <a href="https://github.com/NethermindEth/Taiko-Preconf-AVS/blob/6b21d85d329986a2a9725048e56be3a45d463dcc/Docs/design-doc.md" rel="noopener nofollow ugc">a PoC for based preconfirmations</a> compatible with L2 PBS, including MR-MEV-Boost. Stay tuned for more updates!</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/based-preconfirmations-with-multi-round-mev-boost/20091">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 18 Jul 2024 06:47:13 +0000</pubDate>
</item>
<item>
<title>A Note On Securely Finding Minimum Mean Cycle</title>
<link>https://ethresear.ch/t/a-note-on-securely-finding-minimum-mean-cycle/20073</link>
<guid>https://ethresear.ch/t/a-note-on-securely-finding-minimum-mean-cycle/20073</guid>
<content:encoded><![CDATA[
<div> 关键词：隐私保护、Minimum Mean Cycle (MMC)、Multi-Party Computation (MPC)、Karp算法、Chaturvedi-McConnell方法

总结:
本文研究了在执行图优化算法，如Minimum Mean Cycle (MMC)，同时保护用户和公司之间敏感信息的问题。先前的工作中，Aly等人的方法基于Karp算法，但存在周期检测问题。作者提出了一种改进的协议，解决了这个问题并提高了效率。新协议使用MP-SPDZ实现，时间复杂度从O(|V|^5)降低到O(|V|^3)，空间复杂度从O(|V|^4)减小到O(|V|^2)。研究着重于在保证隐私的同时，提高MM <div>
<p>This study is supported by an Ethereum Foundation R&amp;D grant and is a collaborative work with Enrico ( <a class="mention" href="https://ethresear.ch/u/enricobottazzi">@enricobottazzi</a> ), Masato ( <a class="mention" href="https://ethresear.ch/u/0xvon">@0xvon</a> ) and Nam ( <a class="inline-onebox" href="https://github.com/namnc" rel="noopener nofollow ugc">namnc (Nam Ngo) · GitHub</a> ) from Ethereum Foundation.</p>
<p><strong>Abstract</strong></p>
<p>Executing graph optimization algorithms such as the Minimum Mean Cycle (MMC) while preserving privacy has significant potential for handling sensitive information between users and companies. For example, it enables multilateral netting to solve the Minimum Cost Flow (MCF) problem without disclosing mutual debts, making it highly relevant for processes like netting among multinational corporations. Aly et. al. [2] proposed an algorithm using Multi-Party Computation (MPC) to execute the MMC problem. However, this approach is based on Karp’s algorithm [1], which was found by Chaturvedi et al. [3] to occasionally fail to detect cycles. In this study, we propose a revised protocol that corrects this issue and enhances its efficiency. We implemented our protocol using MP-SPDZ and confirmed that it correctly identifies the MMC, similar to traditional protocols. Our findings indicate that our proposed protocol operates correctly and more efficiently than Aly’s protocol, which reduces the time/round complexity from <span class="math">O(|V|^5)</span> to <span class="math">O(|V|^3)</span> and the space complexity from <span class="math">O(|V|^4)</span> to <span class="math">O(|V|^2)</span>. Furthermore, we discuss potential improvements for even more efficient algorithms.</p>
<h1><a class="anchor" href="https://ethresear.ch#h-1-introduction-1" name="h-1-introduction-1"></a>1. Introduction</h1>
<h3><a class="anchor" href="https://ethresear.ch#h-1-1-importance-of-graph-theory-optimization-2" name="h-1-1-importance-of-graph-theory-optimization-2"></a>1-1. Importance of Graph Theory Optimization</h3>
<p>Graph theory optimization problems play a crucial role in various domains, from computer science and engineering to economics and finance. These problems involve finding the most efficient way to navigate, connect, or utilize network structures, and solutions to these problems have far-reaching implications for improving systems and processes.</p>
<p>One of the representative problems in graph theory optimization is the Minimum Cost Flow (MCF) problem, which aims to find the least costly way to send a certain amount of flow through a network. The MCF problem is foundational in numerous applications, providing critical insights and optimizations.</p>
<p>In the financial sector, particularly in Netting, the Minimum Cost Flow (MCF) problem is often addressed to optimize the settlement of transactions and reduce systemic risk. Netting involves aggregating multiple financial obligations to streamline transactions, minimize risk, and enhance efficiency. However, one of the critical challenges in this context is maintaining the privacy and confidentiality of sensitive financial data. Traditional methods for solving the MCF problem may require exposing transaction details, leading to significant privacy concerns and potential security risks.</p>
<p>Beyond netting, the MMC problem and its solutions have a wide array of applications across various fields:</p>
<ul>
<li><strong>Network Security:</strong> Enhancing security measures by optimizing the flow of information and resources while minimizing potential points of vulnerability.</li>
<li><strong>Supply Chain Management</strong>: Streamlining logistics and distribution networks to reduce costs and improve delivery times.</li>
<li><strong>Urban Planning</strong>: Developing efficient transportation systems by optimizing traffic flow and reducing congestion.</li>
</ul>
<p>The Minimum Mean Cycle (MMC) problem is a crucial component in solving the MCF problem. The MMC problem focuses on identifying cycles in directed graphs with the minimum average weight, which is essential for detecting inefficient paths and optimizing network performance. By incorporating the MMC problem into the solution of the MCF problem, we can achieve more accurate and efficient outcomes.</p>
<p>To address the privacy concerns inherent in solving the MCF problem, we explore the use of Multi-Party Computation (MPC) to securely solve the MMC problem. MPC is a cryptographic approach that allows multiple parties to collaboratively compute a function over their inputs while keeping those inputs private. By applying MPC techniques, we can solve the MMC problem without exposing sensitive data, thus preserving the privacy of financial transactions and other confidential information.</p>
<h3><a class="anchor" href="https://ethresear.ch#h-1-2-previous-work-3" name="h-1-2-previous-work-3"></a>1-2. Previous Work</h3>
<p>Aly et al. [2] proposed a method to solve Karp’s MMC algorithm [1] using Multi-Party Computation. However, this approach has some problems and suffers from significant computational complexity and time consumption. Additionally, the Karp’s algorithm [1] was found by Chaturvedi et al . [3] to occasionally fail to detect cycles.</p>
<h3><a class="anchor" href="https://ethresear.ch#h-1-3-our-contribution-4" name="h-1-3-our-contribution-4"></a>1-3. Our Contribution</h3>
<p>in this study, we propose a novel approach that not only addresses these shortcomings but also offers a more efficient and practical solution for securely solving the MMC problem using MPC. Our proposed protocol aims to reduce computational and time complexities, enhance cycle detection accuracy, and ensure robust privacy protection. Our experimental results demonstrate a significant improvement in efficiency, with a reduction in time complexity from <span class="math">O(|V|^5)</span> to <span class="math">O(|V|^3)</span> and space complexity from <span class="math">O(|V|^4)</span> to <span class="math">O(|V|^2)</span>.</p>
<h1><a class="anchor" href="https://ethresear.ch#h-2-minimum-mean-cycle-problem-5" name="h-2-minimum-mean-cycle-problem-5"></a>2. Minimum Mean Cycle Problem</h1>
<p>Minimum Mean Cycle Problem and its solution is defined by Karp in 1978 [1].</p>
<h3><a class="anchor" href="https://ethresear.ch#h-2-1-problem-definition-6" name="h-2-1-problem-definition-6"></a>2-1. Problem Definition</h3>
<p>Given a connected graph <span class="math">G(V,E)</span> where <span class="math">V</span> is a set of nodes and <span class="math">E</span> is a set of edges, with defining these parameters:</p>
<ul>
<li><span class="math">c_{i,j} \in C</span> denotes the <strong>cost</strong> on the edge <span class="math">(i,j)</span>.</li>
<li><span class="math">d^k(i)</span> denotes the minimum cost from node <span class="math">s</span> to <span class="math">i</span> that contains exactly <span class="math">k</span> edges.</li>
</ul>
<p>First of all, for any cycle <span class="math">X</span>, the mean cycle is defined by:</p>
<div class="math">
\begin{equation}
\mu (X) = \frac{\sum_{uv \in X} c_{uv}}{|X|}
\end{equation}
</div>
<p>Thus, the minimum mean cycle is:</p>
<div class="math">
\begin{equation}
\mu ^* = \min_{cycle X} \mu (X)
\end{equation}
</div>
<p>Minimum Mean Cycle (MMC) Problem is the problem to find this <span class="math">\mu ^*</span>.</p>
<h3><a class="anchor" href="https://ethresear.ch#h-2-2-efficient-mmc-7" name="h-2-2-efficient-mmc-7"></a>2-2. Efficient MMC</h3>
<p>The MMC problem is known as NP-hard, and Karp introduces an efficient algorithm for solving it. The solution is followed by 2 steps.</p>
<p>The first step, we call it as <strong>Walk</strong>, is to calculates <span class="math">d^k(i)</span>, which denotes minimum cost from node <span class="math">s</span> to <span class="math">i</span> that contains exactly <span class="math">k</span> edges. Walk can be computed via the recurrence:</p>
<div class="math">
\begin{equation}
d^k(j) = \min_{(i,j) \in E} d^{k-1}(i)+c_{ij}
\end{equation}
</div>
<p>Initially, <span class="math">d^0(j)=\infty</span>, except for the source node <span class="math">d^0(s)=0</span></p>
<p>The second step is to calculate the minimum mean cycle by:</p>
<div class="math">
\begin{equation}
\mu^* = \min_{j \in V} \max_{0 \leq k \leq |V|-1} \left[ \frac{d^V(j) - d^k(j)}{|V| - k} \right]
\end{equation}
</div>
<p>See Karp’s paper [1] for a proof of equation (4). Overall algorithmic complexity is <span class="math">O(|V| \cdot |E|)</span>, and the first step has a significant impact on the entire algorithm.</p>
<h1><a class="anchor" href="https://ethresear.ch#h-3-alys-secure-mmc-protocol-8" name="h-3-alys-secure-mmc-protocol-8"></a>3. Aly’s Secure MMC Protocol</h1>
<p><strong>Notation</strong></p>
<ul>
<li><span class="math">[a]</span> denotes secret shared or encrypted values of <span class="math">a</span></li>
<li><span class="math">[z] = _{[c]} [x]:[y]</span> denotes the assignment that if <span class="math">[c]</span> is one, <span class="math">[x]</span> is assigned to <span class="math">[z]</span> or <span class="math">[y]</span> otherwise.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#h-3-1-protocol-9" name="h-3-1-protocol-9"></a>3-1. Protocol</h3>
<p>Aly et. al. [2] provide algorithmic solutions to MMC problem in a secure multi-party and distributed setting. This protocol is constructed by 2 sub-protocols:</p>
<ol>
<li><span class="math">walk([C],[b]) \rightarrow [A],[walks]</span></li>
<li><span class="math">mmc([A],[walks]) \rightarrow [\text{min-cost}], [\text{min-cycle}]</span></li>
</ol>
<p>This corresponds to Steps 1 and 2 of Karp’s Algorithm in section 2.</p>
<p>In first sub-protocol, we have two inputs. The cost matrix <span class="math">[C]_{ij}</span> denotes the cost of edge <span class="math">(i,j)</span>. It represents <span class="math">[\infty]</span> for non-existing edges. The viable matrix <span class="math">[b]_{ij}</span> denotes 1 if edge <span class="math">(i,j)</span> doesn’t exist, and 0 otherwise.</p>
<p>From these inputs, we outputs two values. One is the 2-dimensional walk cost matrix <span class="math">[A]</span> which <span class="math">[A]_{jk}</span> records <span class="math">d^k(j)</span>. The other is 4-dimensional walk path matrix <span class="math">[walks]</span> which <span class="math">[walks]_{ijkl}</span> records the number of times the edge <span class="math">(i,j)</span> is traversed by the shortest walk of length <span class="math">k</span> from <span class="math">s</span> to <span class="math">l</span>. The algorithm is detailed as Protocol 3-1.</p>
<hr />
<p><strong>Protocol 3-1. Aly’s Walk Protocol</strong></p>
<hr />
<p><strong>Input</strong>: A matrix of shared costs <span class="math">[C]_{ij}</span> for <span class="math">i,j \in \{1,2,...,|V|\}</span>, a binary matrix on viable adges <span class="math">[b]_{ij}</span> for <span class="math">i,j \in \{1,2,...,|V|\}</span>.</p>
<p><strong>Output</strong>: A matrix of walk costs <span class="math">[A]_{ik}</span> for <span class="math">i \in \{1,2,...,|V|\}</span> and <span class="math">k \in \{0,1,...,|V|\}</span>, a wak matrix <span class="math">[walks]_{ijkl}</span> for <span class="math">i,j,k,l \in \{1,2,...,|V|\}</span> encoding these walks.</p>
<ol>
<li><span class="math">[A] \leftarrow [\infty], [A]_{00} \leftarrow [0], [C] \leftarrow [C] + [\infty](1-[b])</span></li>
<li><strong>for</strong> <span class="math">k \leftarrow 1</span> to <span class="math">|V|+1</span> do
<ol>
<li><strong>for</strong> <span class="math">j \leftarrow 1</span> to <span class="math">|V|</span> do
<ol>
<li><strong>for</strong> <span class="math">i \leftarrow 1</span> to <span class="math">|V|</span> do
<ol>
<li><span class="math">[c] \leftarrow [A]_{ik-1} + [C]_{ij} &lt; [A]_{jk}</span></li>
<li><span class="math">[A]_{jk} \leftarrow _{[c]} [A]_{ik-1} + [C]_{ij} : [A]_{jk}</span></li>
<li><span class="math">[walks]_{..kj} \leftarrow _{[c]} [walks]_{..k-1i} : [walks]_{..kj}</span></li>
<li><span class="math">[walks]_{ijkj} \leftarrow _{[c]} [walks]_{ijkj} + 1 : [walks]_{ijkj}</span></li>
</ol>
</li>
<li><strong>end</strong></li>
</ol>
</li>
<li><strong>end</strong></li>
</ol>
</li>
<li><strong>end</strong></li>
</ol>
<hr />
<p>In second sub-protocol, we have two outputs. <span class="math">[\text{min-cost}]</span> is the minimum mean cost. <span class="math">[\text{min-cycle}]</span> denotes the 2-dimensional cycle matrix which <span class="math">[\text{min-cycle}]_{jk}</span> is 1 if edge <span class="math">(j,k)</span> is included in the cycle achieving <span class="math">\mu ^*</span> and 0 otherwise. Here, <span class="math">\text{min-cycle}</span> is s-j path with <span class="math">|V|</span> edges whose cost is <span class="math">d^{|V|}(j)</span>, minus the s-j path with k edges whose cost is <span class="math">d^{k}(j)</span>. The details are provided as protocol 3-2. We note that we use the theorem that <span class="math">\frac{a}{b}&gt;\frac{c}{d} \iff ad&gt;bc</span> to make a comparison of <span class="math">\frac{d^V(j) - d^k(j)}{|V| - k}</span> without calculating the inverse.</p>
<hr />
<p><strong>Protocol 3-2. Aly’s MMC Protocol</strong></p>
<hr />
<p><strong>Input:</strong> A matrix of walk costs <span class="math">[A]_{ik}</span> for <span class="math">i \in \{1,2,...,|V|\}</span> and <span class="math">k \in \{0,2,...,|V|\}</span>, a walk matrix <span class="math">[walks]_{ijkl}</span> for <span class="math">i,j,k,l \in \{1,2,...,|V|\}</span> encoding these walks.</p>
<p><strong>Output</strong>: The cost of the minimum mean cycle <span class="math">[\text{min-cost}]</span>, a matrix with the minimum mean cycle <span class="math">[\text{min-cycle}]_{ij}</span> for <span class="math">i,j \in \{1,2,...,|V|\}</span></p>
<ol>
<li>
<p><strong>for</strong> <span class="math">j \leftarrow 1</span> to <span class="math">|V|</span> do</p>
<ol>
<li><span class="math">[\text{max-cycle}],[\text{max-cost}] \leftarrow \phi</span></li>
<li><strong>for</strong> <span class="math">k \leftarrow |V|</span> to <span class="math">1</span> do
<ol>
<li><span class="math">[\text{a-num}] \leftarrow [A]_{j(|V|+1)} - [A]_{jk}</span></li>
<li><span class="math">[\text{a-den}] \leftarrow |V|-k</span></li>
<li><span class="math">[c] \leftarrow [\text{k-num}] \cdot [\text{a-den}] &lt; [\text{a-num}] \cdot [\text{k-den}]</span></li>
<li><span class="math">[\text{k-num}] \leftarrow _{[c]} [\text{a-num}]  : [\text{k-num}]</span></li>
<li><span class="math">[\text{k-den}] \leftarrow _{[c]} [\text{a-den}]  : [\text{k-den}]</span></li>
<li><span class="math">[\text{max-cycle}] \leftarrow _{[c]} [walks]_{..|V|j} - [walks]_{..kj} : [\text{max-cycle}]</span></li>
<li><span class="math">[\text{max-cost}] \leftarrow _{[c]} [A]_{jk} : [\text{max-cost}]</span></li>
</ol>
</li>
<li><strong>end</strong></li>
<li><span class="math">[c] \leftarrow [\text{j-num}] \cdot [\text{k-den}] &lt; [\text{k-num}] \cdot [\text{j-den}]</span></li>
<li><span class="math">[\text{j-num}] \leftarrow _{[c]} [\text{k-num}]  : [\text{j-num}]</span></li>
<li><span class="math">[\text{j-den}] \leftarrow _{[c]} [\text{k-den}]  : [\text{j-den}]</span></li>
<li><span class="math">[\text{min-cycle}] \leftarrow _{[c]} [\text{max-cycle}] : [\text{min-cycle}]</span></li>
<li><span class="math">[\text{min-cost}] \leftarrow _{[c]} [\text{max-cost}] : [\text{min-cost}]</span></li>
</ol>
</li>
<li>
<p><strong>end</strong></p>
</li>
</ol>
<hr />
<p><strong>Complexity</strong></p>
<p>This method requires <span class="math">O(|V|^5)</span> time/round complexity, from the conditional assignments to <span class="math">|V| \times |V|</span> elements in <span class="math">[walks]</span> matrix for <span class="math">|V|^3</span> loops (line i-3~4 of Protocol 1). And this method requires <span class="math">O(|V|^4)</span> space complexity, due to 4-dimensional <span class="math">[walks]</span> matrix.</p>
<h3><a class="anchor" href="https://ethresear.ch#h-3-2-problem-in-alys-protocol-10" name="h-3-2-problem-in-alys-protocol-10"></a>3-2. Problem in Aly’s Protocol</h3>
<p>Aly’s protocol implements Karp algorithm [1] in the secure manner. In karp’s alrogithm, we determine <span class="math">\text{min-cycle}</span> like s-j path with <span class="math">|V|</span> edges whose cost is <span class="math">d^{|V|}(j)</span>, minus the s-j path with k edges whose cost is <span class="math">d^{k}(j)</span>. However, Chaturvedi and McConnell [3] provides an counterexample which the cycle couldn’t detected with this method. Furthermore, they prove the following lemma.</p>
<p><strong>Lemma 1</strong><br />
Let <span class="math">j</span> be a vertex such that there exists <span class="math">k</span>, where <span class="math">j</span> and <span class="math">k</span> are a minimizing pair. Every cycle on the length <span class="math">|V|</span> edge progression from <span class="math">s</span> to <span class="math">j</span> of cost <span class="math">d^{|V|}(j)</span> is a cycle of minimum mean cost. (See the proof on their paper [3].)</p>
<p>This lemma means that the cycle can be detected by traversing the edge progression from the last edge and marking the vertices visited by the walk until a previous marked vertex is encountered, from s-j path with <span class="math">|V|</span> edges whose cost is <span class="math">d^{|V|}(j)</span>.</p>
<h1><a class="anchor" href="https://ethresear.ch#h-4-cm-based-secure-mmc-protocol-11" name="h-4-cm-based-secure-mmc-protocol-11"></a>4. CM-based Secure MMC Protocol</h1>
<p><strong>Notation</strong></p>
<ul>
<li><span class="math">[a]</span> denotes secret shared or encrypted values of <span class="math">a</span></li>
<li><span class="math">[z] = _{[c]} [x]:[y]</span> denotes the assignment that if <span class="math">[c]</span> is one, <span class="math">[x]</span> is assigned to <span class="math">[z]</span> or <span class="math">[y]</span> otherwise.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#h-4-1-protocol-12" name="h-4-1-protocol-12"></a>4-1. Protocol</h3>
<p>We propose a protocol that converts the minimum mean cycle detection from Aly’s protocol to one with Lemma1. In addition, a few changes have been made to the data structure. We name it “<strong>CM-based Securely MMC Protocol</strong>”, taking the initials of Chaturvedi and McConnell, who proposed Lemma 1.</p>
<p>CM-based protocol is constructed by 3 sub-protocols:</p>
<ol>
<li><span class="math">walk([C],[b]) \rightarrow [A],[ep]</span></li>
<li><span class="math">mmc</span>
<ol>
<li><span class="math">mmcn([A],[ep]) \rightarrow [\text{min-cost}],[\text{minimizing-node}]</span></li>
<li><span class="math">extract\text{-}cycle([\text{minimizing-node}],[ep]) \rightarrow [\text{min-cycle}]</span></li>
</ol>
</li>
</ol>
<p>Here, Aly’s second sub-protocol is divided into CM-based second and third sub-protocols.</p>
<p>In fist sub-protocol, For the most part, it is the same as Protocol 3-1, with one difference: Instead of <span class="math">[walks]</span>, we record the edge progression in a 2-dimensional matrix called <span class="math">[ep]</span>, which <span class="math">[ep]_{jk}</span> means the edge that passes before one of <span class="math">j</span> in the shortest s-j path with k edges. This change eliminates the need for extra <span class="math">|V|^2</span> loops to update <span class="math">[walks]_{..kj}</span>. The algorithm is detailed as Protocol 4-1.</p>
<hr />
<p><strong>Protocol 4-1. CM-based Walk Protocol</strong></p>
<hr />
<p><strong>Input</strong>: A matrix of shared costs <span class="math">[C]_{ij}</span> for <span class="math">i,j \in \{1,2,...,|V|\}</span>, a binary matrix on viable adges <span class="math">[b]_{ij}</span> for <span class="math">i,j \in \{1,2,...,|V|\}</span>.</p>
<p><strong>Output</strong>: A matrix of walk costs <span class="math">[A]_{ik}</span> for <span class="math">i \in \{1,2,...,|V|\}</span> and <span class="math">k \in \{0,1,...,|V|\}</span>, a matrix of walk edge progressions <span class="math">[ep]_{ij}</span> for <span class="math">i,j \in \{1,2,...,|V|\}</span>.</p>
<ol>
<li><span class="math">[A] \leftarrow [\infty], [A]_{00} \leftarrow [0], [C] \leftarrow [C] + [\infty](1-[b])</span></li>
<li><strong>for</strong> <span class="math">k \leftarrow 1</span> to <span class="math">|V|+1</span> do
<ol>
<li><strong>for</strong> <span class="math">j \leftarrow 1</span> to <span class="math">|V|</span> do
<ol>
<li><strong>for</strong> <span class="math">i \leftarrow 1</span> to <span class="math">|V|</span> do
<ol>
<li><span class="math">[c] \leftarrow [A]_{ik-1} + [C]_{ij} &lt; [A]_{jk}</span></li>
<li><span class="math">[A]_{jk} \leftarrow _{[c]} [A]_{ik-1} + [C]_{ij} : [A]_{jk}</span></li>
<li><span class="math">[ep]_{jk} \leftarrow _{[c]} i : [ep]_{jk}</span></li>
</ol>
</li>
<li><strong>end</strong></li>
</ol>
</li>
<li><strong>end</strong></li>
</ol>
</li>
<li><strong>end</strong></li>
</ol>
<hr />
<p>In (a) of the 2nd sub-protocol, instead of computing <span class="math">[\text{min-cycle}]</span>, we detect the node <span class="math">j</span> that achieves mmc. We call it minimizing node.</p>
<p>The algorithm is detailed as Protocol 4-2-a.</p>
<hr />
<p><strong>Protocol 4-2-a. CM-based MMCN Protocol</strong></p>
<hr />
<p><strong>Input:</strong> A matrix of walk costs <span class="math">[A]_{ik}</span> for <span class="math">i \in \{1,2,...,|V|\}</span> and <span class="math">k \in \{0,2,...,|V|\}</span>, a matrix of walk progressions <span class="math">[ep]_{ij}</span> for <span class="math">i,j \in \{1,2,...,|V|\}</span>.</p>
<p><strong>Output</strong>: The cost of the minimum mean cycle <span class="math">[\text{min-cost}]</span>, the node achieving the minimum mean cycle <span class="math">[\text{minimizing-node}]</span>.</p>
<ol>
<li>
<p><strong>for</strong> <span class="math">j \leftarrow 1</span> to <span class="math">|V|</span> do</p>
<ol>
<li>
<p><span class="math">[\text{max-cost}] \leftarrow \phi</span></p>
</li>
<li>
<p><strong>for</strong> <span class="math">k \leftarrow |V|</span> to <span class="math">1</span> do</p>
<ol>
<li><span class="math">[\text{a-num}] \leftarrow [A]_{j(|V|+1)} - [A]_{jk}</span></li>
<li><span class="math">[\text{a-den}] \leftarrow |V|-k</span></li>
<li><span class="math">[c] \leftarrow [\text{k-num}] \cdot [\text{a-den}] &lt; [\text{a-num}] \cdot [\text{k-den}]</span></li>
<li><span class="math">[\text{k-num}] \leftarrow _{[c]} [\text{a-num}]  : [\text{k-num}]</span></li>
<li><span class="math">[\text{k-den}] \leftarrow _{[c]} [\text{a-den}]  : [\text{k-den}]</span></li>
<li><span class="math">[\text{max-cost}] \leftarrow _{[c]} [A]_{jk} : [\text{max-cost}]</span></li>
</ol>
</li>
<li>
<p><strong>end</strong></p>
</li>
<li>
<p><span class="math">[c] \leftarrow [\text{j-num}] \cdot [\text{k-den}] &lt; [\text{k-num}] \cdot [\text{j-den}]</span></p>
</li>
<li>
<p><span class="math">[\text{j-num}] \leftarrow _{[c]} [\text{k-num}]  : [\text{j-num}]</span></p>
</li>
<li>
<p><span class="math">[\text{j-den}] \leftarrow _{[c]} [\text{k-den}]  : [\text{j-den}]</span></p>
</li>
<li>
<p><span class="math">[\text{minimizing-node}] \leftarrow _{[c]} j : [\text{minimizing-node}]</span></p>
</li>
<li>
<p><span class="math">[\text{min-cost}] \leftarrow _{[c]} [\text{max-cost}] : [\text{min-cost}]</span></p>
</li>
</ol>
</li>
<li>
<p><strong>end</strong></p>
</li>
</ol>
<hr />
<p>In (b) of the 2nd sub-protocol, from <span class="math">[\text{minimizing-node}]</span>, we construct a back pointer which indicates s-j path with <span class="math">|V|</span> edges whose cost is <span class="math">d^{|V|}(j)</span> and extract a cycle from the back pointer. Compared to Protocol 3-2, instead of expanding <span class="math">[\text{min-cycle}]</span> directly from <span class="math">[walks]</span>, the additional protocol is required. We follow Lemma 1 and consider any cycle included in the back pointer as a minimum mean cycle. The algorithm is detailed as Protocol 4-2-b.</p>
<hr />
<p><strong>Protocol 4-2-b. CM-based Extract-Cycle Protocol</strong></p>
<hr />
<p><strong>Input:</strong> A minmizing node <span class="math">[\text{minimizing-node}]</span>, a matrix of walk progressions <span class="math">[ep]_{ij}</span> for <span class="math">i,j \in \{1,2,...,|V|\}</span>.</p>
<p><strong>Output</strong>: A matrix with the minimum mean cycle <span class="math">[\text{min-cycle}]_{ij}</span> for <span class="math">i,j \in \{1,2,...,|V|\}</span></p>
<ol>
<li><span class="math">[\text{backpointers}]_{0} \leftarrow [\text{minimizing-node}]</span>, <span class="math">[\text{next-index}] \leftarrow [\text{minimizing-node}]</span></li>
<li><strong>for</strong> <span class="math">k \leftarrow |V|</span> to <span class="math">1</span> do
<ol>
<li><span class="math">[\text{val}] \leftarrow [0]</span></li>
<li><strong>for</strong> <span class="math">j \leftarrow 0</span> to <span class="math">|V|-1</span> do
<ol>
<li><span class="math">[match] = j == [\text{next-index}]</span></li>
<li><span class="math">[\text{val}] = _{[\text{match}]} [ep]_{jk}:[\text{val}]</span></li>
<li><span class="math">[\text{match-index-matrix}]_{jk} = [match]</span></li>
</ol>
</li>
<li><strong>end</strong></li>
<li><span class="math">[\text{next-index}] = [\text{val}]</span></li>
<li><span class="math">[\text{backpointers}].append([\text{val}])</span></li>
</ol>
</li>
<li><strong>end</strong></li>
<li><strong>for</strong> <span class="math">i \leftarrow 0</span> to <span class="math">|V|-1</span> do
<ol>
<li><span class="math">[\text{counter}] \leftarrow [0]</span></li>
<li><strong>for</strong> <span class="math">k \leftarrow 0</span> to <span class="math">|V|</span> do
<ol>
<li><span class="math">[\text{counter}] = [\text{counter}] + [\text{match-index-matrix}]_{ik}</span></li>
</ol>
</li>
<li><span class="math">[c] = [\text{counter}] &gt;= 2</span></li>
<li><span class="math">[\text{cycle-node}] = _{[c]} i : [\text{cycle-node}]</span></li>
</ol>
</li>
<li><strong>end</strong></li>
<li><span class="math">[\text{min-cycle}] \leftarrow [0],[\text{counter}] \leftarrow [0]</span></li>
<li><strong>for</strong> <span class="math">k \leftarrow |V|</span> to <span class="math">1</span> do
<ol>
<li><span class="math">[\text{edge-from}] \leftarrow [\text{backpointers}]_k</span></li>
<li><span class="math">[c] = [\text{edge-from}] [\text{cycle-node}]</span></li>
<li><span class="math">[\text{counter}] = [\text{counter}] + [c]</span></li>
<li><span class="math">[c_0] = [\text{counter}] + 1</span></li>
<li><strong>for</strong> <span class="math">j \leftarrow 0</span> to <span class="math">|V|-1</span> do
<ol>
<li><span class="math">[c_1] = [\text{match-index-matrix}]_{jn-k}</span></li>
<li><span class="math">[c_2] = [c_0]*[c_1]</span></li>
<li><strong>for</strong> <span class="math">i \leftarrow 0</span> to <span class="math">|V|-1</span> do
<ol>
<li><span class="math">[c_3] = [\text{match-index-matrix}]_{jn-k+1}</span></li>
<li><span class="math">[\text{min-cycle}]_{ji} = [\text{min-cycle}]_{ji} + ([c_2] * [c_3])</span></li>
</ol>
</li>
<li><strong>end</strong></li>
</ol>
</li>
<li><strong>end</strong></li>
</ol>
</li>
<li><strong>end</strong></li>
</ol>
<hr />
<p><strong>Complexity</strong><br />
This ****method requires <span class="math">O(|V|^3)</span> multiplications or communication rounds, from the conditional assignments of <span class="math">[A],[ep],[\text{min-cycle}]</span> for <span class="math">|V|^3</span> loops (line i-2~3 of Protocol 4-1 and like iii-3 of Protocol 4-2-b). And this method requires <span class="math">O(|V|^2)</span> space complexity, largely due to 2-dimensional matrixes. A table comparing the Complexity of each protocol is shown in Table 4-1 below.</p>
<p><strong>Table 4-1. Complexity Analysis of Secure MMC Protocols</strong></p>
<div class="md-table">
<table>
<thead>
<tr>
<th></th>
<th>multiplications/communication rounds complexity</th>
<th>space complexity</th>
</tr>
</thead>
<tbody>
<tr>
<td>Aly’s</td>
<td><span class="math">O(|V|^5)</span></td>
<td><span class="math">O(|V|^4)</span></td>
</tr>
<tr>
<td>CM-based</td>
<td><span class="math">O(|V|^3)</span></td>
<td><span class="math">O(|V|^2)</span></td>
</tr>
</tbody>
</table>
</div><h3><a class="anchor" href="https://ethresear.ch#h-4-2-implementation-13" name="h-4-2-implementation-13"></a>4-2. Implementation</h3>
<p>We have implemented CM-based Securely MMC protocol in naive secret sharing scheme using Python MP-SPDZ. And we confirmed that the minimum mean cycle was found reliably in a number of random edges, including the counterexamples shown by Chaturvedi et al [3].</p>
<h1><a class="anchor" href="https://ethresear.ch#h-5-conclusion-14" name="h-5-conclusion-14"></a>5. Conclusion</h1>
<p>In this study, we have proposed a more efficient protocol for solving the Minimum Mean Cycle (MMC) problem using Multi-Party Computation (MPC). Our CM-based approach not only addresses but also significantly improves upon the issues identified in Aly’s protocol. Specifically, our protocol reduces the time/round complexity from <span class="math">O(|V|^5)</span> to <span class="math">O(|V|^3)</span> and the space complexity from <span class="math">O(|V|^4)</span> to <span class="math">O(|V|^2)</span> compared to Aly’s protocol.</p>
<p>Despite these advancements, the complexity remains super-quadratic in terms of the number of nodes, which can pose practical challenges for very large graphs. To mitigate this limitation, we propose the following strategies:</p>
<ul>
<li>By exposing the graph’s topography, we can optimize the edge search to include only the minimum necessary edges, thereby reducing the time/round complexity to <span class="math">O(|V|^2 \cdot |E|)</span>. This approach, however, requires a trade-off with some degree of privacy.</li>
<li>Implementing simpler algorithms that provide approximate or sub-optimal solutions, such as Greedy Algorithms and Distributed Algorithms, can further enhance practicality. These algorithms can significantly reduce computational overhead while delivering sufficiently accurate results for many applications.</li>
</ul>
<p>In summary, our protocol offers a substantial improvement over existing methods, paving the way for more efficient and practical solutions to the MMC problem in secure computation settings. Future work will focus on refining these strategies to further balance the trade-offs between efficiency, accuracy, and privacy.</p>
<h1><a class="anchor" href="https://ethresear.ch#reference-15" name="reference-15"></a>Reference</h1>
<ol>
<li>Richard M. Karp, “A characterization of the minimum cycle mean in a digraph”, Discrete Mathematics, Volume 23, Issue 3, 1978, Pages 309-311, ISSN 0012-365X, <a href="https://doi.org/10.1016/0012-365X(78)90011-0" rel="noopener nofollow ugc">https://doi.org/10.1016/0012-365X(78)90011-0</a>.</li>
<li>Aly, A., Van Vyve, M. (2015). Securely Solving Classical Network Flow Problems. In: Lee, J., Kim, J. (eds) Information Security and Cryptology - ICISC 2014. ICISC 2014. Lecture Notes in Computer Science(), vol 8949. Springer, Cham. <a class="inline-onebox" href="https://doi.org/10.1007/978-3-319-15943-0_13" rel="noopener nofollow ugc">Securely Solving Classical Network Flow Problems | SpringerLink</a></li>
<li>Mmanu Chaturvedi, Ross M. McConnell, “A note on finding minimum mean cycle”, Information Processing Letters, Volume 127, 2017, Pages 21-22, ISSN 0020-0190, <a class="inline-onebox" href="https://doi.org/10.1016/j.ipl.2017.06.007" rel="noopener nofollow ugc">Redirecting</a>.</li>
</ol>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/a-note-on-securely-finding-minimum-mean-cycle/20073">Read full topic</a></p>
]]></content:encoded>
<pubDate>Mon, 15 Jul 2024 07:09:02 +0000</pubDate>
</item>
<item>
<title>Sealed execution auction</title>
<link>https://ethresear.ch/t/sealed-execution-auction/20060</link>
<guid>https://ethresear.ch/t/sealed-execution-auction/20060</guid>
<content:encoded><![CDATA[
<div> 关键词：密封拍卖、执行提案人、密封投标、公开竞标、MEV问题。

总结:<br />
本文提出了一种名为密封执行拍卖（SEA）的新机制，旨在分离执行提案权和验证者角色，避免MEV问题。该机制包括两个阶段：第一阶段，建设者匿名提交密封投标；第二阶段，建设者公开投标，最高投标者支付第二高投标作为费用。通过这种方式，可以防止建设者与提案者之间的勾结，确保拍卖的公正性。文章还讨论了可能的合谋情况及应对策略，如对提案者错过区块的惩罚和后续拍卖流程设计。总的来说，SEA为区块链执行权拍卖提供了一个潜在的解决方案，以实现更有效的分离和减少激励冲突。 <div>
<h1><a class="anchor" href="https://ethresear.ch#sealed-execution-auction-1" name="sealed-execution-auction-1"></a>Sealed execution auction</h1>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/e/3e0cf08f2021a1cbbe0156d52c8482dba0a00ba6.jpeg" title="Sealed execution auction"><img alt="Sealed execution auction" height="394" src="https://ethresear.ch/uploads/default/optimized/3X/3/e/3e0cf08f2021a1cbbe0156d52c8482dba0a00ba6_2_690x394.jpeg" width="690" /></a></div><p></p>
<p>By <a href="https://x.com/weboftrees">Anders</a>.</p>
<p><em>While working on the <a href="https://ethresear.ch/t/mev-resistant-dynamic-pricing-auction-of-execution-proposal-rights/20024">dynamic pricing auction</a> I though of another way to hold the auction that also seems interesting. Posting a rough sketch here, although I am not yet certain of its viability. Thanks to <a href="https://x.com/drakefjustin">Justin</a>, <a href="https://x.com/barnabemonnot">Barnabé</a> and <a href="https://x.com/terencechain">Terence</a>.</em></p>
<h2><a class="anchor" href="https://ethresear.ch#introduction-2" name="introduction-2"></a>Introduction</h2>
<p>In the process of enshrining proposer–builder separation (<a href="https://github.com/ethereum/EIPs/pull/8711">ePBS</a>), it has been <a href="https://mirror.xyz/barnabe.eth/LJUb_TpANS0VWi3TOwGx_fgomBvqPaQ39anVj3mnCOg">suggested</a> that attesting and execution proposing should be more fully separated. Proposals such as <a href="https://ethresear.ch/t/execution-tickets/17944">execution tickets</a> (ETs) and <a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ">execution auctions</a> (EAs) strive to allocate the right to propose execution blocks to entities other than the validators. This also facilitates <a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590">MEV burn</a>. There have been concerns (<a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590/4">1</a>, <a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590/23">2</a>, <a href="https://ethresear.ch/t/dr-changestuff-or-how-i-learned-to-stop-worrying-and-love-mev-burn/17384/3">3</a>) around insufficient early bidding in the MEV pricing auctions with a base fee floor used in EA. By <a href="https://ethresear.ch/t/burn-incentives-in-mev-pricing-auctions/19856">considering the staking metagame</a>, this issue is potentially resolved, but the resulting attester–builder integration can then by itself be <a href="https://ethresear.ch/t/burn-incentives-in-mev-pricing-auctions/19856#risks-associated-with-attester-builder-integration-14">problematic</a>. There is also a general concern that the decided-upon auction design will <a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ">induce MEV</a>, and no definite specification among <a href="https://ethresear.ch/t/on-block-space-distribution-mechanisms/19764#preliminaries-12">several alternatives</a> for the auction design in ET. For this reason, it seems fruitful to explore an auction that facilitates true separation and does not induce MEV. One such mechanism recently proposed is the <a href="https://ethresear.ch/t/mev-resistant-dynamic-pricing-auction-of-execution-proposal-rights/20024">MEV resistant dynamic pricing auction</a>. In the context of Vickrey auctions of execution rights, <a href="https://forum.arbitrum.foundation/t/constitutional-aip-proposal-to-adopt-timeboost-a-new-transaction-ordering-policy/25167">Timeboost</a> under consideration by Arbitrum can also be mentioned.</p>
<p>This post proposes a <a href="https://en.wikipedia.org/wiki/Vickrey_auction">Vickrey</a> slot auction in two rounds to select a forthcoming execution proposer (akin to EA), referred to as a sealed execution auction (SEA). Staked builders make sealed bids for the right to propose an execution block. Bids are observed by attesters and then collated by the beacon proposer. In subsequent steps, builders reveal their bids, attesters observe the revealed bids, and the proposer once again collates them. The right to propose a forthcoming execution block is awarded to the highest bidder, paying according to the second-highest bid, with the payment burned.</p>
<h2><a class="anchor" href="https://ethresear.ch#auction-3" name="auction-3"></a>Auction</h2>
<h3><a class="anchor" href="https://ethresear.ch#staked-builders-4" name="staked-builders-4"></a>Staked builders</h3>
<p>Builders are staked at a level sufficient for the protocol to penalize them if they fail to reveal committed bids. The stake can also serve as a deposit account to pay for winning bids, or this account can be managed separately.</p>
<h3><a class="anchor" href="https://ethresear.ch#sealed-bids-5" name="sealed-bids-5"></a>Sealed bids</h3>
<p>Figure 1 gives an overview of the auction. In the first round, each builder has the opportunity to make one sealed bid over a public P2P layer. There might be a small fee for making a bid, as a further anti-Sybil measure. Attesters observe the sealed bids that have come in at time <span class="math">T_1</span>. Around two seconds later, at <span class="math">T_2</span>, the beacon proposer collates sealed bids (including any bids it finds after <span class="math">T_1</span>), and broadcasts them in a structure. This structure may be a beacon block if the auction proceeds over two slots (see <a href="https://ethresear.ch/t/sealed-execution-auction/20060#timeline-15">Timeline</a>). At <span class="math">T_3</span>, attesters observe the structure and make sure that all previously observed bids at <span class="math">T_1</span> have been included. If the bids were included in a beacon block, they will attest to the block contingent on correct and timely collation. If not included in a beacon block and the proposer equivocates on the structure, the subsequent block must be rejected.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/9/e9efaf529cceda171c770e9160ed477ff7093303.png" title="Figure 1"><img alt="Figure 1" height="347" src="https://ethresear.ch/uploads/default/optimized/3X/e/9/e9efaf529cceda171c770e9160ed477ff7093303_2_690x347.png" width="690" /></a></div><p></p>
<p><strong>Figure 1.</strong> Sealed execution auction. Staked builders submit sealed bids before <span class="math">T_1</span> and the proposer collates them at <span class="math">T_2</span>. At <span class="math">T_3</span> attesters ensure that all bids they observed at <span class="math">T_1</span> are part of the collated structure. Builders unseal the bids after <span class="math">T_3</span> and attesters observe them at <span class="math">T_4</span>. The proposer then collates bids in a beacon block at <span class="math">T_5</span> and attesters attests to the block at <span class="math">T_6</span> contingent on correct collation. The highest unsealed bid wins, paying a fee corresponding to the second highest bid. The fee is burned. Builders that did not unseal their bids are penalized.</p>
<h3><a class="anchor" href="https://ethresear.ch#revealed-bids-6" name="revealed-bids-6"></a>Revealed bids</h3>
<p>In the second round, after the <span class="math">T_3</span> deadline, builders unseal their bids. They should not release before <span class="math">T_3</span>, because then the proposer can collude with other builders to release a bid structure with some bids placed after other bids were revealed. However, they do not need to observe the proposer’s structure before release, and can proceed right after the <span class="math">T_3</span> mark.</p>
<p>Attesters observe unsealed bids at <span class="math">T_4</span>. The proposer collates all unsealed bids it can find, including them in the beacon block at around <span class="math">T_5</span>. It may also include bids that were never unsealed, so that the associated builder can be penalized (this is a strict requirement in the single-slot design, because then the sealed bids have not been included in a previous beacon block). The highest bid is selected as the forthcoming execution proposer, and the second highest bid value is deducted from the winner’s balance and burned. At <span class="math">T_6</span>, attesters attest to the beacon block, contingent on a correct collation by the beacon proposer.</p>
<h2><a class="anchor" href="https://ethresear.ch#rationale-7" name="rationale-7"></a>Rationale</h2>
<p>Collusion between builders and proposers to reduce the burn as in the MEV burn design is arguably resolved; without stakers actively burning each others’ MEV revenue.</p>
<ul>
<li>There is no longer a stable equilibrium to rely on for colluding parties, such as late bidding.</li>
<li>The proposer no longer has leverage to punish early bidders by electing another builder.</li>
<li>Chiseling at a cartel is trivial, simply by truthful bidding.</li>
<li>Every bid fulfills a real purpose, as opposed to early bids in MEV pricing auctions.</li>
<li>There is no avenue for discouragement attacks, since there is no substantial proposer revenue to remove.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#penalization-8" name="penalization-8"></a>Penalization</h2>
<p>Several actions must be penalized. If the proposer omits an observed sealed bid in the first round or an observed revealed bid in the second round, the proposer’s block must be rejected by attesters. If the proposer fails to release the structure of the sealed bids in the first round or the revealed bids in the second round in a timely manner (reaching attesters before <span class="math">T_3</span> and <span class="math">T_6</span> respectively), the proposer’s block must also be rejected by attesters.</p>
<p>It is possible that a builder made a mistake and will be unable to pay for its bid, if the bid is higher than its staked amount. This will be penalized by burning some proportion of the stake, for example corresponding to the amount of the actual winning bid, some fixed amount of ETH, or its entire stake. In any case, if its unbacked bid is the highest, the builder will not win the auction. The second highest bid will instead be selected as the execution proposer, paying the third highest bid, etc. If the bid underpinning the fee (normally the second highest bid) lacks funds, the bid below it will be set to underpin the fee.</p>
<h2><a class="anchor" href="https://ethresear.ch#builder-proposer-collusion-and-possible-remedies-9" name="builder-proposer-collusion-and-possible-remedies-9"></a>Builder–proposer collusion and possible remedies</h2>
<p>A potential cause for concern is the following scenario: a builder determines that it would not like to unseal its bid (potentially after observing other builders’ unsealed bids). It does not want to subject itself to a penalty, so it colludes with the proposer to have it miss the slot. Is this a cause for concern? This ultimately depends on if the builder benefits more by <em>not</em> revealing its bid than the proposer loses from a missed proposal. This could be the case when bidding for the right to propose the current or next slot, and the expected MEV falls drastically between bid commit and reveal (i.e., a <a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ">value-in-flight</a> problem). Another potential cause for concern is if the value instead increases drastically. The proposer might then pose an ultimatum to the winning builder: “send me some part of expected profits, or I will fail to propose”. A failed proposal would leave the builder without rights for the slot. An <a href="https://en.wikipedia.org/wiki/Ultimatum_game">ultimatum game</a> emerges. The other builders might also be inclined to pay the proposer, in order to starve off competition, and the winning builder would then also need to pay the proposer to ensure it proposes.</p>
<p>While the outlined collusion scenarios may be a bit speculative, it can still be interesting to explore possible remedies. A few directions then spring to mind:</p>
<h4><a class="anchor" href="https://ethresear.ch#h-1-penalize-beacon-proposers-for-missed-beacon-blocks-10" name="h-1-penalize-beacon-proposers-for-missed-beacon-blocks-10"></a>1. Penalize beacon proposers for missed beacon blocks</h4>
<p>Proposers already lose out on revenue if they miss their block. However, this loss might not be a sufficient deterrent. It would therefore be beneficial to also penalize proposers if they miss their blocks. Otherwise, if the penalty applied to a builder is substantially higher than the loss from missed proposals for the proposer, that builder penalty will be less meaningful. Builders could seek to collude to let the proposer take the fall. In essence, if the value to the builder, its competitors, or the proposer, of having the builder not win the auction, is higher than the loss to the proposer of not proposing, then collusion or an ultimatum game may emerge.</p>
<h4><a class="anchor" href="https://ethresear.ch#h-2-require-subsequent-beacon-proposers-to-conclude-the-auction-11" name="h-2-require-subsequent-beacon-proposers-to-conclude-the-auction-11"></a>2. Require subsequent beacon proposers to conclude the auction</h4>
<p>Is it possible to have the next beacon proposer conclude the auction? This depends to some extent on the <a href="https://ethresear.ch/t/sealed-execution-auction/20060#timeline-15">Timeline</a> of the auction.</p>
<ul>
<li><strong>Single-slot design:</strong> In the single-slot design, attesters do not signal if they rejected a block because of an incorrect initial structure, a late structure, or an incorrect or missing beacon block. A way to deal with this is that the next proposer presents the correct outcome of the auction, in its own view, and that the attesters of <span class="math">n+2</span> either reject or confirm the new block based on the proposed outcome. But this means that these attesters must also have tracked events in the previous slot as they unfolded, and any split views (e.g., from a rather late sealed builder bid) may persist for several blocks in a row.</li>
<li><strong>Two-slot design:</strong> If the auction commences over two slots, there will be an agreed-upon set of committed sealed bids, or the first beacon block will have been rejected. The second step of the auction can then be concluded in a subsequent slot without requiring attesters to have observed the commit-phase. The requirement is to still have attesters make an observation of unsealed bids sometime before the proposer deadline. But that point need not necessarily be taken from the earlier slot. A benefit is that this might starve off split views.</li>
</ul>
<p>One thing to note is that if a builder finds it worthwhile to pay the first proposer to not propose, in order to avoid revealing a bid without being penalized, it might be willing to pay also a second proposer for not proposing. However, the price will go up, and the number of potential collusion partners scheduled to propose in a row may not be too large. It should also be noted that when auctioning off rights for slot <span class="math">n+i</span>, there is a requirement that the delay until the conclusion of the auction does not surpass <span class="math">i</span>. In other words, it will only be possible to repeat a failed auction around <span class="math">i</span> times. Note that this requirement is also due to the fact that the order in which auctioned off execution rights are provided cannot be altered ex post, since the expected MEV for slots may vary.</p>
<h4><a class="anchor" href="https://ethresear.ch#h-3-skip-the-beacon-proposal-reveal-12" name="h-3-skip-the-beacon-proposal-reveal-12"></a>3. Skip the beacon proposal reveal</h4>
<p>Is it possible to skip the beacon proposal reveal? If all bids are unsealed, the outcome will be evident to every participant. The mechanism can then be designed such that the winning builder safely can propose its block at the assigned slot, even if a proposer has not collated the outcome and presented a winner. The previous option 2 is focused on concluding the auction via a beacon proposal in time before the execution proposal, but the point here is that the auction does not need to be concluded by the proposer as long as the outcome is evident to the builder and can be verified by attesters when the builder proposes its block. The sealed bids must then have been included in a beacon block, as in the two-slot design.</p>
<p><a href="https://en.wikipedia.org/wiki/Threshold_cryptosystem">Threshold decryption</a> via a committee of attesters (h/t Barnabé) is one option here. The bids are decrypted by a committee, and the winner made evident to the builders/forthcoming proposer and attesters. There would still be liveness concerns, but collusion would be more difficult. It can be noted that as long as all builders unseal their bids in a timely manner (even without threshold decryption), the winning builder can proceed with the proposal. Always penalizing builders that do not unseal their bids before <span class="math">T_4</span> could then seem sufficient, but the issue is that split views would emerge in potential designs. In any case, the outcome would also at some point need to be included in a block, to process payment and penalties.</p>
<h4><a class="anchor" href="https://ethresear.ch#h-4-auction-of-a-future-slot-to-reduce-value-in-flight-13" name="h-4-auction-of-a-future-slot-to-reduce-value-in-flight-13"></a>4. Auction of a future slot to reduce value-in-flight</h4>
<p>The Vickrey auction is truthful, allowing builders to bid their true value at the commit deadline. Since value-in-flight is the most likely cause for collusion, auctioning off a slot further removed from the present will temper the issue.</p>
<h4><a class="anchor" href="https://ethresear.ch#auctioning-off-multiple-slots-14" name="auctioning-off-multiple-slots-14"></a>Auctioning off multiple slots</h4>
<p>Note that to avoid having a failed beacon proposal result in a missing execution proposal, there is also the option to sell the right to two execution proposals in the subsequent slot (with builders bidding their <a href="https://en.wikipedia.org/wiki/Vickrey_auction">inverse demand curve</a> and paying according to the second and third highest bids).</p>
<h2><a class="anchor" href="https://ethresear.ch#timeline-15" name="timeline-15"></a>Timeline</h2>
<p>This section presents two hypothetical timelines for the auction, either when only including unsealed bids in the beacon block (single-slot auction) or when including both sealed and unsealed bids in separate beacon blocks (two-slot auction).</p>
<h3><a class="anchor" href="https://ethresear.ch#single-slot-auction-16" name="single-slot-auction-16"></a>Single-slot auction</h3>
<p>Example of a slot auction with a tight schedule enacted mostly during a single slot <span class="math">n</span>, auctioning off execution proposal rights for a later slot <span class="math">n+i</span>.</p>
<div class="md-table">
<table>
<thead>
<tr>
<th><span class="math">T_x</span></th>
<th>Time</th>
<th>Overview</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="math">T_1</span></td>
<td>4s</td>
<td><strong>Sealed bid deadline</strong></td>
<td>Attesters of slot <span class="math">n+1</span> observe all sealed bids. Builders must have broadcast them some time before this point to ensure eligibility.</td>
</tr>
<tr>
<td><span class="math">T_2</span></td>
<td>6s</td>
<td><strong>Proposer collates bids</strong></td>
<td>The proposer of slot <span class="math">n+1</span> releases a structure containing all sealed bids it can find.</td>
</tr>
<tr>
<td><span class="math">T_3</span></td>
<td>8s</td>
<td><strong>Attesters observe collation</strong></td>
<td>Attesters of slot <span class="math">n+1</span> observe the proposer’s structure to ensure it contains all bids they had seen at <span class="math">T_1</span> and that the release of this structure is timely.</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><span class="math">T_4</span></td>
<td>10s</td>
<td><strong>Revealed bid deadline</strong></td>
<td>Attesters of slot <span class="math">n+1</span> observe unsealed bids. Builders must have broadcast them some time before this point (but after <span class="math">T_3</span>) to ensure eligibility.</td>
</tr>
<tr>
<td><span class="math">T_5</span></td>
<td>0s (12s)</td>
<td><strong>Proposer collates in beacon block</strong></td>
<td>The proposer of slot <span class="math">n+1</span> includes every unsealed bid it can find in the  block, also indicating sealed bids that were never unsealed. A winner is declared.</td>
</tr>
<tr>
<td><span class="math">T_6</span></td>
<td>4s (12+4s)</td>
<td><strong>Attesters confirm collation</strong></td>
<td>Attesters of slot <span class="math">n+1</span> confirm that the proposer fulfilled its role and collated bids in a timely manner by attesting to the block.</td>
</tr>
</tbody>
</table>
</div><p>Note that builders can unseal their bids directly after <span class="math">T_3</span>. This should allow attesters of slot <span class="math">n+1</span> to observe revealed bids at 10s. However, if needed, the entire schedule could be pushed back slightly.</p>
<h3><a class="anchor" href="https://ethresear.ch#two-slot-auction-17" name="two-slot-auction-17"></a>Two-slot auction</h3>
<p>Here is an example of a schedule for the two-slot auction:</p>
<div class="md-table">
<table>
<thead>
<tr>
<th><span class="math">T_x</span></th>
<th>Time</th>
<th>Overview</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="math">T_1</span></td>
<td>10s</td>
<td><strong>Sealed bid deadline</strong></td>
<td>Attesters of slot <span class="math">n+1</span> observe all sealed bids. Builders must have broadcast them some time before this point to ensure eligibility.</td>
</tr>
<tr>
<td><span class="math">T_2</span></td>
<td>0s (12s)</td>
<td><strong>Proposer collates bids</strong></td>
<td>The proposer of slot <span class="math">n+1</span> includes all sealed bids it can find in its beacon block.</td>
</tr>
<tr>
<td><span class="math">T_3</span></td>
<td>4s (12+4s)</td>
<td><strong>Attesters confirm collation</strong></td>
<td>Attesters of slot <span class="math">n+1</span> confirm that the proposer fulfilled its role and collated bids in a timely manner by attesting to the block.</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><span class="math">T_4</span></td>
<td>8s (12+8s)</td>
<td><strong>Revealed bid deadline</strong></td>
<td>Attesters of slot <span class="math">n+2</span> observe unsealed bids. Builders must have broadcast them some time before this point (but after <span class="math">T_3</span>) to ensure eligibility.</td>
</tr>
<tr>
<td><span class="math">T_5</span></td>
<td>0s (12+12s)</td>
<td><strong>Proposer collates in beacon block</strong></td>
<td>The proposer of slot <span class="math">n+2</span> includes every unsealed bid it can find in the  block, potentially indicating sealed bids that were never unsealed. A winner is declared.</td>
</tr>
<tr>
<td><span class="math">T_6</span></td>
<td>4s (12+12+4s)</td>
<td><strong>Attesters confirm collation</strong></td>
<td>Attesters of slot <span class="math">n+2</span> confirm that the proposer collated all unsealed bids by attesting to the block.</td>
</tr>
</tbody>
</table>
</div>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/sealed-execution-auction/20060">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sat, 13 Jul 2024 16:30:07 +0000</pubDate>
</item>
<item>
<title>Staking Rights Auctions</title>
<link>https://ethresear.ch/t/staking-rights-auctions/20059</link>
<guid>https://ethresear.ch/t/staking-rights-auctions/20059</guid>
<content:encoded><![CDATA[
<div> 关键词：Staking Rights Auctions, Ethereum, Issuance, MEV, Inflation Control

总结: 这篇文章讨论了作者过去为Cartesi和CTSI提出的一种名为"Staking Rights Auctions"的机制，旨在解决以太坊发行（包括ether的质押和MEV问题）中的挑战。该机制通过拍卖赋予节点操作员参与权益，用户可以根据风险偏好支付不同的价格，从而实现更精细的激励分配。文章提到，这有助于平衡MEV差距、控制通胀并允许用户表达不同的投资时间周期。尽管需要进一步研究和生态系统的参数决策，但作者认为这个系统能提高参与度，同时限制通胀，促进更健康的网络环境。 <div>
<p>This post was written by <a class="mention" href="https://ethresear.ch/u/pedroargento">@pedroargento</a> but his account seems unable to post it, so he asked me to do it. I’m not too aware of the necessities/constraints of the ether issuance debate - but this seems quite interesting for defi protocols as well. Anyway, here it goes:</p>
<hr />
<p>Hey everyone,</p>
<p>A friend of mine just watched the “What’s the Issue with Issuance?” talk by Christine Kim, Caspar Schwarz-Schilling, and Ansgar Dietrichs at EthCC. They said that the key points discussed around Ethereum’s issuance reminded them of a proposal I wrote in the past for Cartesi and CTSI.</p>
<p>The significant issues mentioned were the high (and growing) percentage of ether staked, how having too much ether staked isn’t necessarily beneficial for the network, how LST providers might be in a "winner take all ‘’ situation and etc. Both Ansgar and Justin Drake suggested aiming for around 20-25% staked ether (ball park estimates).</p>
<p>It seems to me that the auction mechanism I proposed for the CTSI staking economy could really help to address these issues, by making the staking system much more expressive. The idea also allows participants to pay negative issuance for the right to earn MEV, which not only tackles the problem of excessive ether staking, but might also helps to balance MEV discrepancies.</p>
<p>I’m not an expert in this research area, but based on the feedback from the EthCC talk, it seems like my proposal aligns well with the direction Ethereum is aiming to take. I’m sharing this here on the Ethereum Research forum in hopes that it can contribute to the ongoing conversation and possibly offer a viable solution to the current challenges with Ethereum’s issuance models.</p>
<p>Looking forward to your thoughts and feedback!</p>
<h1><a class="anchor" href="https://ethresear.ch#staking-rights-auctions-1" name="staking-rights-auctions-1"></a>Staking Rights Auctions</h1>
<p>A popular solution to reward users for staking is to mint new tokens and distribute them among stakers. Besides the obvious incentive to gain extra tokens, the inflation created penalizes those who choose not to participate. The challenge is how to measure the opportunity costs of users and how to choose the appropriate issuance amount to achieve a target participation rate, while avoiding exceedingly high inflation rates.</p>
<p>Some projects have a fixed emission rate while others have a dynamic inflation function, which is higher when the participation is below desired and lower otherwise. There are three key problems with these methods:</p>
<ul>
<li>
<p>You need strong assumptions about users’ risk preferences to tailor the parameters of the function;</p>
</li>
<li>
<p>Users have little information about the mining income they will get as it depends on the number of total staked funds.</p>
</li>
<li>
<p>The methods don’t allow for differentiation between players with different risk preferences;</p>
</li>
<li>
<p>It is hard to determine a balanced inflation target.</p>
</li>
</ul>
<p>As a countermeasure to these three issues, I’m proposing a staking system based on a novel mechanism called staking rights, detailed in the sections below.</p>
<h2><a class="anchor" href="https://ethresear.ch#the-mechanism-of-staking-rights-2" name="the-mechanism-of-staking-rights-2"></a>The Mechanism of Staking Rights</h2>
<p>Staking rights give node operators the right to participate in staking. Without the rights, operators cannot be selected in the lottery that chooses the node that will generate the next block.</p>
<p>Rights are transitory. At the end of each staking cycle, a set of rights expires and ceases to exist. Conversely, new rights are created and made available for purchase through an auction.</p>
<p>Staking rights always have a final value of 1 token, which is delivered to the account that purchased it at the precise time of their expiry. When users buy a staking right for a price of less than 1 token, the difference between the price paid and the unit value is proportional to their perceived opportunity of the staking right. In that case, the difference is minted and locked in staking together with the price paid, totaling 1 token staked per right sold.</p>
<p>Here is an example. Suppose that the desired staking participation rate is 50% of the circulating supply of 1 thousand tokens. In this case, the system creates and auctions 500 staking rights, each scheduled to pay 1 token at the end of the cycle.</p>
<blockquote>
<p>Circulating supply: 1000<br />
Target participation: 500 (50%)<br />
Staking rights issued: 500<br />
Auction price = 0.97</p>
</blockquote>
<p>Assume that each staking right is sold for <span class="math">0.97</span> in the auction, thereby generating <span class="math">0.03</span> new tokens. The staking rights buyer at the end of the staking cycle would be rewarded 1 token obtaining a <span class="math">3.09\%</span> return <span class="math">(0.03/0.97)</span>. The total inflation generated for the network would be <span class="math">15</span> tokens (<span class="math">0.03</span> per right * <span class="math">500</span> rights) or <span class="math">1.5\%</span>.</p>
<p>With this system, the user knows exactly how much return they will get for their staked tokens, independent of how many rights are sold or how many other stakers exist. There are also no assumptions about risk preferences, buyers will state them through bidding. This method also allows for bigger differentiation between users: instead of asking for a binary decision (stake or not to stake), we allow users to signal at what price they would be willing to stake.</p>
<p>The system can offer staking rights with different staking cycle periods: 2 weeks, 1 month, 3 months, etc. This achieves two objectives (1) differentiate between users who are willing to stake long term from short term players and, mainly, (2) decrease volatility in token emission. After all, if all staking cycles end at the same time, all new staking rights will be subjected to the same market conditions that may not represent the average behavior of stakers.</p>
<p>With different staking periods, in each cycle only a small number of staking rights will need to be created to replace the expired ones. This is because in each cycle there is going to be a mix of active staking rights bought at different points in time.</p>
<p>User risk preferences can be stated in the form of a discount rate, the rate used to convert future values (promises of payouts) to the present. The discount rate is the income that makes one indifferent between gaining money in the present or in the future. For example, with a discount rate of 10% a year, one would be indifferent between receiving 100 dollars today or 110 dollars a year from now.</p>
<p>The discount rate of a user can be translated to a staking right value using it to compute the present value of all incentives that can be paid by staking the right.</p>
<p>Staking rights give the owner three sources of incentives, provided that the owner remained active within the network:</p>
<ul>
<li>
<p>Staking right’s unit value (paid at the end of the cycle)</p>
</li>
<li>
<p>Block producer’s tips</p>
</li>
<li>
<p>Mine extractable Value</p>
</li>
</ul>
<p>Below is an example of staking rights holder cash flows for a six month locked period.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/8/28063c3ac2b9e1cd18ed0e7aa69be283c8125261.png" title="Chart"><img alt="" height="385" src="https://ethresear.ch/uploads/default/optimized/3X/2/8/28063c3ac2b9e1cd18ed0e7aa69be283c8125261_2_624x385.png" title="Chart" width="624" /></a></div><p></p>
<p>The price to be paid for the staking can be easily calculate based on the return demanded by the staker</p>
<blockquote>
<p>Given:<br />
a staking right in a staking cycle of 12 weeks that pays rewards every 2 weeks<br />
<span class="math">MEV_t</span> the expected mine reward for time <span class="math">t</span><br />
<span class="math">NF_t</span> the expected network fees for time <span class="math">t</span><br />
<span class="math">UV</span> the staking right unit value<br />
<span class="math">i</span> the 2-week return expected by the user<br />
The price <span class="math">P</span> will be calculated as:</p>
<div class="math">
P= \frac {UV} {(1+i)^6} + \sum_ {t=1}^{6} \frac {MEV_t + NF_t} {(1+i)^t}
</div>
</blockquote>
<p>The staking rights can be sold through a closed price auction of Nth price, which means that the higher bid wins the token but will pay the price of the highest loser bid. For example, If 500 tokens are sold and the 501st highest bid was 0.98, all 500 tokens will cost 0.98. This type of auction, also known as a Vickrey auction (or Dutch auction), ensures all players bid their true valuation of the staking right, revealing their true risk preferences.</p>
<blockquote>
<p><strong>Proof</strong><br />
Its not 100% applicable to this specific auction, but a classical proof from Game Theory can give the intuition why the paid price being the lowest winning bid incentivizes truthfully reporting:</p>
<p>Given user <span class="math">i</span> has a valuation <span class="math">B_i</span> for a staking right. They can bid <span class="math">B_+ &gt;B_i</span> or <span class="math">B_- &lt; B_i</span> and the <span class="math">N</span>-th price of the auction ends up being <span class="math">B_n</span>.</p>
<p>If they bids <span class="math">B_+</span> there are two possibilities:</p>
<ol>
<li>
<p><span class="math">B_n &lt; B_i</span></p>
</li>
<li>
<p><span class="math">B_+ &gt; B_n &gt; B_i</span></p>
</li>
</ol>
<p>In (1) they would get <span class="math">(B_i - B_n)</span> independent of bidding <span class="math">B_+</span> or <span class="math">B_i</span> and in (2) they would lose <span class="math">(B_i — B_+)</span> that would be larger than <span class="math">(B_n — B_i)</span>. In neither case they have incentive to bid <span class="math">B_+</span>.</p>
<p>If they bids <span class="math">B_-</span> there are two possibilities:</p>
<ol start="3">
<li>
<p><span class="math">B_n &lt; B_-</span></p>
</li>
<li>
<p><span class="math">B_- &lt; B_n &lt; B_i</span></p>
</li>
</ol>
<p>In (3) they would get <span class="math">(B_i — B_n)</span> independent of bidding <span class="math">B_-</span> or <span class="math">B_i</span> and in (4) they would not get the token, making it better to bid <span class="math">Bi</span> and have the chance to win.</p>
<p>In all possible cases there is no incentive to bid <span class="math">B_+</span> or <span class="math">B_-</span>, making <span class="math">B_i</span> the dominant Nash-Bayesian equilibrium.</p>
</blockquote>
<p>This system also allows for deflation, if the value of the auction ends up above 1 unit. This would make sense if people are expecting such a high reward from the fees and MEV that they are willing to burn a certain amount of tokens in order to participate.</p>
<h2><a class="anchor" href="https://ethresear.ch#inflation-control-mechanisms-3" name="inflation-control-mechanisms-3"></a>Inflation Control Mechanisms</h2>
<p>Besides the burning possibility, its possible to add parameters in the auction to help manage inflation. Although its unclear to me at this time how those parameters could be decided by the Ethereum ecosystem, I’m presenting them anyway. Contributions are welcomed as always.</p>
<p><strong>First</strong>. Auction reserve prices: In the worst-case scenario, where all rights are sold in the auction with a price close to zero, the inflation will be the number of rights sold, divided by the total supply (50% in our previous example).</p>
<p>A reserve price means that only bids above a certain value will be considered valid. If we choose a reserve price of 0.7, the worst-case scenario in our example would be an inflation of 15%.</p>
<p>With a reserve price, it is possible to choose an acceptable inflation range and guarantee it will be complied with at all times.</p>
<p><strong>Second</strong>. The number of issued tokens: The number of tokens directly affects the inflation. If only 100 tokens are issued (out of a total supply of one thousand), the worst-case scenario for inflation would be 10%.</p>
<p>These two variables need to be controlled dynamically in order to make sure the inflation is never higher than a previously determined ceiling. The number of tokens issued will depend not only on the target participation rate but also on the value of bids from the auction. This number will be capped so that the total newly minted tokens are limited to the maximum inflation. The total newly minted tokens can be calculated as the difference between the face value and the highest bid not honored (the Dutch auction price) times the number of tokens issued.</p>
<blockquote>
<p>Let <span class="math">CAP</span> be the maximum number of minted ETH desired</p>
<p>Let <span class="math">N_ {max}</span> be the maximum number of staking rights necessary to achieve the target participation rate</p>
<p>Let <span class="math">B(i)</span> be the <span class="math">i</span>-th largest bid from the auction results</p>
<p>Let <span class="math">N</span> be the number of staking rights issued</p>
<p><span class="math">N</span> will be chosen as the result of the optimization problem:</p>
<div class="math">
\begin{aligned}
\max_{} \quad &amp; N\\
\textrm{s.t.} \quad &amp; N * (1-B(N+1)) \le CAP\\
\quad &amp; N \le N_ {max} \\
\end{aligned}
</div>
</blockquote>
<p>More precisely, suppose that we sort all the bids made during the auction in decreasing order and plot them as in the figure below.</p>
<p><img alt="m1" height="318" src="https://ethresear.ch/uploads/default/original/3X/f/4/f4dd92342507c13ba5e872d205c15c90bf308b60.png" width="477" /></p>
<p>Then N staking rights will be issued in order to preserve the maximum number of ETH issued (CAP). Therefore, we can dynamically choose the minimum value B(N+1) such that the inflation is within the predetermined bounds.</p>
<p>The deflation case is depicted in the figure below:</p>
<p><img alt="m2" height="318" src="https://ethresear.ch/uploads/default/original/3X/7/3/736de5fac6d3efefd1b613297221521da4f9d64b.png" width="459" /></p>
<p>It is important to note that there is no way around the tradeoff between participation rate and inflation, to control the later there is the need to sacrifice the former. The advantage brought by the system of staking rights auction is that we maximize participation, while limiting the inflation and allowing workers to express their economic preferences.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/staking-rights-auctions/20059">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sat, 13 Jul 2024 14:53:13 +0000</pubDate>
</item>
<item>
<title>Bitfinity: A new sharded blockchain</title>
<link>https://ethresear.ch/t/bitfinity-a-new-sharded-blockchain/20054</link>
<guid>https://ethresear.ch/t/bitfinity-a-new-sharded-blockchain/20054</guid>
<content:encoded><![CDATA[
<div> 关键词: 
1. 基于阈值BLS签名
2. 分片（sharding）
3. 无缝跨-shard通信
4. 跨链结算
5. Bitcoin-EVM桥接

总结:
Bitfinity是一个运用了分片技术的新型区块链，通过阈值BLS签名实现线性可扩展性和高速度。它将代币与EVM处理器分离，支持跨-shard通信，实现了顺畅的跨链资产转移，包括与比特币的连接。作为比特币和其它资产的Layer Two，Bitfinity特别适合部署复杂Solidity智能合约，提供极致的性能和效率。<br /><br />总结: Bitfinity利用分片和阈值签名技术，打造高性能的EVM，连接比特币和其他资产，实现跨链交易和智能合约部署。 <div>
<p>Using threshold BLS signatures we design a new blockchain that implements sharding, separating tokens from EVM processors. Bitfinity is linearly scalable and fast.<br />
Bitfinity implements seamless cross-shard communication.</p>
<aside class="onebox pdf">
  <header class="source">

      <a href="https://github.com/bitfinity-network/whitepapers/blob/163145326e321c87956b2f881159f73b7a6409fb/Bitfinity_Network.pdf" rel="noopener nofollow ugc" target="_blank">github.com</a>
  </header>

  <article class="onebox-body">
    <a href="https://github.com/bitfinity-network/whitepapers/blob/163145326e321c87956b2f881159f73b7a6409fb/Bitfinity_Network.pdf" rel="noopener nofollow ugc" target="_blank"><span class="pdf-onebox-logo"></span></a>

<h3><a href="https://github.com/bitfinity-network/whitepapers/blob/163145326e321c87956b2f881159f73b7a6409fb/Bitfinity_Network.pdf" rel="noopener nofollow ugc" target="_blank">Bitfinity_Network.pdf</a></h3>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both;"></div>
</aside>
<p>
Using sharding techniques we also implement seamless cross-chain settlement and can bridge over Bitcoin to the EVM.</p>
<aside class="onebox allowlistedgeneric">
  <header class="source">
      <img class="site-icon" height="32" src="https://ethresear.ch/uploads/default/original/3X/9/6/966040c1403c32d7669b160eb35a33dc860db193.png" width="32" />

      <a href="https://bitfinity.network/" rel="noopener nofollow ugc" target="_blank">bitfinity.network</a>
  </header>

  <article class="onebox-body">
    

<h3><a href="https://bitfinity.network/" rel="noopener nofollow ugc" target="_blank">Bitfinity EVM</a></h3>

  <p>Bitfinity is a blazingly-fast, next-gen EVM, serving as a Layer Two for Bitcoin and other assets - utilising threshold signature schemes and built on the IC. Use Bitfinity to deploy advanced Solidity smart contracts.</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both;"></div>
</aside>

            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/bitfinity-a-new-sharded-blockchain/20054">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 12 Jul 2024 23:34:33 +0000</pubDate>
</item>
<item>
<title>Searcher Competition in Block Building</title>
<link>https://ethresear.ch/t/searcher-competition-in-block-building/20044</link>
<guid>https://ethresear.ch/t/searcher-competition-in-block-building/20044</guid>
<content:encoded><![CDATA[
<div> 关键词：MEV奖励、验证者、搜索者、合作博弈论、核心

总结:<br />本文探讨了在区块链中，验证者和搜索者之间的MEV（矿工提取价值）收益分配。通过合作博弈论模型，研究了验证者（具有否决权）与可替代或互补的搜索者之间的互动。核心部分分析了可能的支付向量，发现当搜索者竞争激烈时，验证者的奖励增加，符合理论预测。在随机模型中，高概率下搜索者独立发现机会时，验证者独占全部收益；而在低概率或搜索者互补情况下，验证者可能得到零支付。研究还扩展到了有限大小区块的情况。实证结果证实了理论的预测。 <div>
<p>In a new paper with Christoph Schlegel (<a class="mention" href="https://ethresear.ch/u/jcschlegel">@jcschlegel</a>), Benny Sudakov and Danning Sui(<a class="mention" href="https://ethresear.ch/u/sui414">@sui414</a>), we look at the distribution of MEV rewards between the validator and searchers. We model the interaction between all players using tools from cooperative game theory. Namely, for any coalition of players, we define a (maximum achievable) value the coalition can derive by creating the best block together. The validator is a special player, that is needed to create any value. In other words, it has a veto power. However, searchers are the ones that find (arbitrage) opportunities which derive a value. Searchers can be substitutes or complements of each other into finding opportunities. The outcome of this interaction is payoff vector, specifying how much each player gets. In the core of the game payoffs are such that any coalition gets paid at least as much as the value they produce themselves.<br />
First, we study a structure of the core, which is always non-empty set of payoff vectors. Then, we focus on the searcher-optimum allocation and show that each searcher obtains its marginal contribution. In a stochastic model, where each opportunity is independently found with the same probability by each searcher, we show that if this probability is mildly high in the number of searchers, validator gets all rewards. In other words, core is just a single payoff vector. While if this probability is low, with a constant probability the validator can get zero payment, as the searchers are complements of each other. We extend some results to the blocks with bounded size.<br />
On the empirical side, we observe that if there is a high competition of searchers, validator rewards are increasing (in absolute terms), which aligns with our theoretical predictions.<br />
For more details check out the paper: <a class="inline-onebox" href="https://arxiv.org/abs/2407.07474" rel="noopener nofollow ugc">[2407.07474] Searcher Competition in Block Building</a>. Any feedback is welcome.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/searcher-competition-in-block-building/20044">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 11 Jul 2024 11:00:11 +0000</pubDate>
</item>
<item>
<title>L2 Asset Interoperability via Two-way Canonical Bridges</title>
<link>https://ethresear.ch/t/l2-asset-interoperability-via-two-way-canonical-bridges/20039</link>
<guid>https://ethresear.ch/t/l2-asset-interoperability-via-two-way-canonical-bridges/20039</guid>
<content:encoded><![CDATA[
<div> 关键词：L2资产、两向桥接、ERC-1155接口、L2结算合同、跨链流动性

总结:<br />
文章讨论了L2资产桥接问题，强调了现有的L2解决方案如L2之间共享结算层的局限性，导致生态系统碎片化。为解决这一问题，提出了一种两向桥接的概念，即资产能在L2和L1之间双向流动。L2结算合同作为记录，采用ERC-1155接口，用户通过发送资产到系统地址实现转移。返回L2时，使用特殊函数进行销毁并存入。这种设计确保资产安全，用户自行承担风险。同时，用户可利用快速流动性桥，而跨链流动性提供者则用于资产重平衡。该机制还可扩展至L3。总的来说，两向桥接旨在打破链间壁垒，促进资产流动性与互操作性。 <div>
<h2><a class="anchor" href="https://ethresear.ch#motivation-1" name="motivation-1"></a>Motivation</h2>
<p>One key problem with the L2 scaling solutions is that assets natively minted on L2s can only be used on the L2 of issuance but it cannot be bridged back to L1 or other L2s, without utilizing external bridges. This creates fragmentation. At the time of writing, there is already half as much natively-minted assets ($12b) on Eth L2s compared to canonical bridged assets ($24b), according to L2Beat.</p>
<p>Shared settlement layers only solve this problem for L2s using the same shared settlement layer. The ecosystem remain fragmented once more shared settlement layer show up.</p>
<p>We propose <strong>two-way canonical bridges</strong> as a solution, where L2-minted assets can be <strong>reverse-canonically bridged</strong> to L1. It is simply an ERC-1155-like interface that an L2 settlement contracts adopt, plus additional precompiles added to the L2 execution environment.</p>
<h2><a class="anchor" href="https://ethresear.ch#two-way-canonical-bridges-2" name="two-way-canonical-bridges-2"></a>Two-way Canonical Bridges</h2>
<p>Below is a highlevel description of two-way canonical bridging.</p>
<ul>
<li>The L2 settlement contract becomes the ledger of record for all native assets issued on it (that have been reverse-canonically-bridged). The settlement contract (on L1) shall implement the ERC-1155 interface, where the asset id field denotes the L2 asset address.</li>
<li>To send an L2-native asset to an L1 address, the L2 users simply send the asset to a prespecified system address, which shall results in the L2 settlement contract on L1 issuing ERC-1155 tokens to itself. Next, L2-&gt;L1 call mechanisms can be utilized to move the newly-issued asset to any desired destination. This is done within the same L2 transaction.</li>
<li>To send a reverse-canonically-wrapped asset back to its L2 of origin, a special <code>burnAndDeposit</code> function on the L2 settlement contract can be called.</li>
<li>Since the L2 settlement contract is an ERC-1155 contract, L1 EOAs and other L2s can simply hold assets or wrap them as normal. This requires the L2 canonical bridge to support wrapping of ERC-1155 assets.</li>
<li>In normal usage, it is expected that the only holders of the ERC-1155 tokens issued by an L2 settlement contract are other L2 settlement contracts. This means that the state overhead on L1 is small.</li>
</ul>
<p>Additional consideration:</p>
<ul>
<li>The safety of an asset is maintained without additional trust assumptions because the L2 settlement contract acts as the ledger of record for all outstanding assets (those owned by other L1 addresses).</li>
<li>It is assumed that any assets that is reverse-canonically-bridged to L1 addresses is done at the risk of the user initiating the bridging.</li>
<li>In practice, end-users can utilize fast liquidity bridges while crosschain liquidity providers utilize the two-way canonical bridges to rebalance.</li>
<li>This mechanism can extend to L3s on L2s. An asset issued on an L3 can be reverse canonically-bridged to L2 and then reverse canonically-bridged back to L1. We’d need the 1155 ids on the settlement contract to be able to represent the 1155 asset id on L2 alongside with the asset address–this can be done via hashing for example.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#acknowledgements-3" name="acknowledgements-3"></a>Acknowledgements</h3>
<p>Thanks to Shumo Chu for review and comments.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/l2-asset-interoperability-via-two-way-canonical-bridges/20039">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 10 Jul 2024 22:20:35 +0000</pubDate>
</item>
<item>
<title>MEV resistant dynamic pricing auction of execution proposal rights</title>
<link>https://ethresear.ch/t/mev-resistant-dynamic-pricing-auction-of-execution-proposal-rights/20024</link>
<guid>https://ethresear.ch/t/mev-resistant-dynamic-pricing-auction-of-execution-proposal-rights/20024</guid>
<content:encoded><![CDATA[
<div> 关键词：MEV抵抗、动态定价拍卖、执行提案权、随机抽取机制（RANDAO）、票池拍卖（ET）

总结:
这篇文章提出了一种MEV抵抗的动态定价拍卖机制，用于销售执行提案权。这种机制旨在减少 Beacon 验证者在提案选择过程中的影响，通过公开的P2P层接受建造者购买订单，订单由共识层的债务账户支持。购买过程由 attesters 观察并确保其有效性，避免了过多的MEV。设计有执行票拍卖(ETA)和集体铸造两种版本，其中 ETA 利用RANDAO随机排序。文章讨论了价格调整策略、动态定价的复杂性以及如何平衡价格变化与市场需求。尽管存在多块交易MEV和审查抵抗等未解决的问题，但该机制为执行提案权的拍卖提供了一个潜在的、MEV抵抗的解决方案。 <div>
<h1><a class="anchor" href="https://ethresear.ch#mev-resistant-dynamic-pricing-auction-of-execution-proposal-rights-1" name="mev-resistant-dynamic-pricing-auction-of-execution-proposal-rights-1"></a>MEV resistant dynamic pricing auction of execution proposal rights</h1>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/9/3903954c39a134bc9b9fc6b919977da400390b97.jpeg" title=""><img alt="" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/3/9/3903954c39a134bc9b9fc6b919977da400390b97_2_500x500.jpeg" width="500" /></a></div><p></p>
<p><em>Execution proposal of marriage between EA and ET through an auction sequenced by RANDAO (she said yes).</em></p>
<p>By <a href="https://x.com/weboftrees">Anders</a>. Special thanks to <a href="https://x.com/barnabemonnot">Barnabé</a> for helping me improve the clarity of this post. Thanks also for valuable feedback to <a href="https://x.com/soispoke">Thomas</a>, <a href="https://x.com/_julianma">Julian</a>, and <a href="https://x.com/fradamt">Francesco</a>.</p>
<h2><a class="anchor" href="https://ethresear.ch#h-1-introduction-2" name="h-1-introduction-2"></a>1. Introduction</h2>
<h3><a class="anchor" href="https://ethresear.ch#h-11-background-3" name="h-11-background-3"></a>1.1 Background</h3>
<p>As part of the effort to enshrine proposer–builder separation (<a href="https://ethresear.ch/t/minimal-epbs-beacon-chain-changes/18653">ePBS</a>), the role of beacon validators as execution proposers has come under <a href="https://mirror.xyz/barnabe.eth/LJUb_TpANS0VWi3TOwGx_fgomBvqPaQ39anVj3mnCOg">scrutiny</a>. <a href="https://ethresear.ch/t/execution-tickets/17944">Execution tickets</a> (ET), first introduced as <a href="https://www.youtube.com/watch?v=IrJz4GZW-VM">attester–proposer separation</a>, is a mechanism for selecting the execution proposer by random draw from a ticket pool, aiming to detach beacon validators from the selection process. However, the mechanism for selling tickets has not been settled, with several <a href="https://ethresear.ch/t/on-block-space-distribution-mechanisms/19764#preliminaries-12">alternatives</a> under consideration. A notable <a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ">concern</a> is that the sale of execution tickets may induce maximal extractable value (MEV). If the mechanism is administered by the consensus layer and the beacon proposer is given too much influence over the price or over the selection of purchasers, the design risks repeating one of the issues it was intended to resolve, with a new source of MEV becoming a concern. An execution layer vending machine raises similar <a href="https://x.com/barnabemonnot/status/1805859642213269739">questions</a>. Therefore, a MEV resistant auction mechanism could be desirable if pursuing ETs.</p>
<p><a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ">Execution auctions</a> (EA) is a related mechanism for selecting a future execution proposer, omitting the ticket pool. It  relies on a <a href="https://ethresear.ch/t/burn-incentives-in-mev-pricing-auctions/19856">MEV pricing auction</a>, where bidders first make bids that set a <a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590">floor to the MEV burn</a>, and finally bid through tips in order to be selected by the proposer. Concerns have been raised (<a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590/4">1</a>, <a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590/23">2</a>, <a href="https://ethresear.ch/t/dr-changestuff-or-how-i-learned-to-stop-worrying-and-love-mev-burn/17384/3">3</a>) regarding the viability of MEV pricing auctions due to insufficient bid incentives in the initial phase. It has <a href="https://ethresear.ch/t/burn-incentives-in-mev-pricing-auctions/19856">recently been suggested</a> that this concern is resolved by considering the staking metagame, in which stakers must bid early to deprive other stakers of revenue. However, this resolution implies that EAs will lead to increased staker–builder integration, which might also be a <a href="https://ethresear.ch/t/burn-incentives-in-mev-pricing-auctions/19856#risks-associated-with-attester-builder-integration-14">cause for concern</a>. For this reason, it seems fruitful to explore an alternative auction mechanism also when selecting the execution proposer without leveraging a ticket pool.</p>
<h3><a class="anchor" href="https://ethresear.ch#h-12-overview-of-proposal-4" name="h-12-overview-of-proposal-4"></a>1.2 Overview of proposal</h3>
<p>This post introduces a dynamic pricing auction with MEV resistance to sell execution proposal rights. Builders hold reserves in a debit account and place binding purchase order for a ticket (ET) or an execution proposal slot (similar to EA). The final price adapts dynamically based on the total outstanding as well as currently incoming orders/tickets, with some similarities to, e.g., <a href="https://github.com/ethereum/EIPs/blob/f93b530c60dc7a88e5b811f9cbdf865ecc1b9b97/EIPS/eip-1559.md">EIP-1559</a>, and the payment is burned. Orders are delimited at the slot level through attester observations to remove agency from the beacon proposers facilitating the auction, thus inducing less new MEV. This produces a high aggregate MEV burn. In one version of the design, dubbed execution ticket auction (ETA), orders that came in during the same slot are sequenced for proposal by leveraging the <a href="https://eth2book.info/capella/part2/building_blocks/randomness/#the-randao">RANDAO</a>. In another version only applicable to ETs, orders that came in during the same slot are minted collectively into tickets. Due to the current limitations of the RANDAO, the mechanism is only capable of auctioning off proposal rights at least one epoch in advance.</p>
<h2><a class="anchor" href="https://ethresear.ch#h-2-purchase-process-5" name="h-2-purchase-process-5"></a>2. Purchase process</h2>
<p>Figure 1 presents the proposed purchase mechanism. Builders send purchase orders (for one ticket/execution slot at a time) over a public P2P layer. They specify a maximum price and hold a debit account within consensus to guarantee that their purchase orders are backed by sufficient funds. This account is funded using a separate transaction (see the discussion).</p>
<p>Beacon attesters observe all orders up to an observation deadline, enacted for example 2 seconds before the slot boundary. The beacon proposer collects all orders (there will be one purchase order per slot on average), including orders they may have found during the last few seconds of their slot. Orders are added as a group to the beacon block and will later be popped from a virtual first-in first-out (FIFO) queue scheduled across blocks. This queue may be just one slot long, depending on implementation.</p>
<p>Attesters reject the block if the beacon proposer fails to include a purchase order that they observed. The mechanism thus far has similarities to MEV pricing auctions (e.g., <a href="https://ethresear.ch/t/committee-driven-mev-smoothing/10408">MEV smoothing</a>, <a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590">MEV burn</a>, <a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ">EA</a>), but attesters are tasked with simply observing all purchases, instead of setting a bid floor. Another design that might come to mind is inclusion lists (ILs) in the style of <a href="https://ethresear.ch/t/fork-choice-enforced-inclusion-lists-focil-a-simple-committee-based-inclusion-list-proposal/19870">FOCIL</a>, but there is no new active participant in the form of an IL committee.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/0/702bb78973cad1e335953a439305f91bf34e2132.png" title="Figure 1"><img alt="Figure 1" height="424" src="https://ethresear.ch/uploads/default/optimized/3X/7/0/702bb78973cad1e335953a439305f91bf34e2132_2_690x424.png" width="690" /></a></div><p></p>
<p><strong>Figure 1.</strong> Schematic overview of the purchase process. Orders in blue, backed by builders’ debit accounts, are observed by attesters (purple arrows). Beacon proposers subsequently add all incoming orders to the beacon block (dark red arrow). A validity check is performed to ensure that orders are fully backed. Orders are finally processed—using either RANDAO to determine the sequence in cases where several orders came in during the associated slot (yellow), or otherwise using collective minting (red). In ETA, orders are directly queued for proposal.</p>
<p>Once a slot’s orders have been added to the beacon block, a validity check is performed on builders that included at least one new order (cyan in Figure 1). If a builder’s outstanding (not yet processed) orders across the queue are not fully backed by its debit account, all the builder’s pending orders are discarded. A penalty may also be applied. Orders are  priced directly upon being added or, e.g., at the time of sequencing, as described in Section 4. The determined purchase price is charged from the debit account and burned. The remaining ETH of the purchase order is subsequently virtually released such that it can be used to back new purchase orders. Orders are then sequenced and either queued for proposal (yellow arrow) or added to the ticket pool (red arrow), as described in Section 3.</p>
<h2><a class="anchor" href="https://ethresear.ch#h-3-sequencing-process-6" name="h-3-sequencing-process-6"></a>3. Sequencing process</h2>
<p>The purchase orders from the same slot are added unsequenced to the beacon block. The subsequent sequencing of orders from the same slot varies between designs.</p>
<h3><a class="anchor" href="https://ethresear.ch#h-31-et-minting-of-execution-tickets-7" name="h-31-et-minting-of-execution-tickets-7"></a>3.1 ET – Minting of execution tickets</h3>
<p>The natural strategy for ETs is <em>collective minting</em>, wherein all orders from the same slot mint a ticket at the same time, as indicated by the red arrow in Figure 1. The RANDAO used for ETA in the next subsection could also be applied to ETs using the same setup (dashed yellow arrow). However, the only real benefit (which remains marginal) is to facilitate a more even replenishment of the ticket pool.</p>
<h3><a class="anchor" href="https://ethresear.ch#h-32-eta-orders-sequenced-by-randao-8" name="h-32-eta-orders-sequenced-by-randao-8"></a>3.2 ETA – orders sequenced by RANDAO</h3>
<p>Purchase orders that came in during the same slot can be sequenced directly by the RANDAO, completely skipping a ticket pool. Perhaps <em>execution ticket auction</em> (ETA) would be a proper moniker. Indeed, with this design, a buyer will have an <em>Estimated Time of Arrival</em> for their order, which suitably cannot be precisely known beforehand if there is more than one order in the slot. Barnabé’s discussion (<a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ">1</a>, <a href="https://x.com/barnabemonnot/status/1805872045302898807">2</a>) on the topic of ETs and determinism is relevant here.</p>
<p>Orders can only be sequenced after the RANDAO has been updated. Therefore, there is an initial ineligibility window <span class="math">W</span> during which orders cannot lead to an execution proposal. The RANDAO updates every 32 slots, but the proposed mechanism does not guarantee a new order every slot; in fact, the mode will be zero orders in a slot. Consequently, the safe distance between auction and slot proposal will need to be somewhat longer than 32 slots. Sequenced orders can be understood as sitting in a second FIFO queue while waiting to propose. Note that ETA could set the queue to hold as many proposal rights as the ticket pool, if desirable.</p>
<h2><a class="anchor" href="https://ethresear.ch#h-4-dynamic-pricing-9" name="h-4-dynamic-pricing-9"></a>4. Dynamic pricing</h2>
<h3><a class="anchor" href="https://ethresear.ch#h-41-ticket-saturation-and-delta-10" name="h-41-ticket-saturation-and-delta-10"></a>4.1 Ticket saturation and delta</h3>
<p>The exploration of dynamic pricing will refer to processed orders as “tickets”, although in the ETA design these are just sitting in the ordered queue waiting to propose. The protocol strives to ensure that there are <span class="math">\hat{T}</span> outstanding tickets at any time. The price of a new ticket should be determined by the current number of outstanding tickets <span class="math">T</span> as well as the current supply of purchases and purchase orders <span class="math">T_p</span>, measured over some window of length <span class="math">W_T</span>, which in some versions can be only one slot long.</p>
<p>Define the ticket saturation as <span class="math">T_s=T-\hat{T}</span>. If <span class="math">T_s&lt;0</span>, there are too few tickets, and the protocol would in general like to sell more than one ticket per slot. If <span class="math">T_s&gt;0</span>, there are too many, and it would in general like to sell fewer than one. The delta <span class="math">T_{\delta}=T_p-W_T</span> gives purchase orders relative to an expectation of one ticket per slot, which is the rate at which tickets are consumed by execution proposers. If <span class="math">T_{\delta}&lt;0</span>, the protocol is selling fewer than one ticket per slot and would in general like to sell more. If <span class="math">T_{\delta}&gt;0</span>, it sells more than one and would in general like to sell fewer.</p>
<p>If both <span class="math">T_s</span> and <span class="math">T_{\delta}</span> are negative, the protocol should decrease the ticket price to sell more tickets. If both <span class="math">T_s</span> and <span class="math">T_{\delta}</span> are positive, it should increase the price to sell fewer. The less trivial question is how to approach a situation when one of the variables is negative and the other is positive, how to window sales, and how quickly to adjust the price.</p>
<h3><a class="anchor" href="https://ethresear.ch#h-42-dynamic-pricing-mechanism-11" name="h-42-dynamic-pricing-mechanism-11"></a>4.2 Dynamic pricing mechanism</h3>
<h4><a class="anchor" href="https://ethresear.ch#h-421-overview-12" name="h-421-overview-12"></a>4.2.1 Overview</h4>
<p>The price of tickets adjusts on a relative basis, just like in EIP-1559, gradually shifting by some proportion of the current price each slot. To improve MEV resistance and adapt to the problem at hand, three differences to EIP-1559 however seem useful: (1) the price should depend on orders included in the current slot, not only the preceding; (2) the block should never be “full”, lest the ticket price becomes very high; (3) the mechanism should be “two-dimensional” in the sense that it accounts for both ticket saturation and delta.</p>
<p>This subsection begins by exploring the simplest realization of such a pricing mechanism, which will then gradually be expanded. In the simplest design, <span class="math">W_T=1</span>, and orders can be priced directly when added to the beacon block. If there is one new order (<span class="math">T_{\delta}=0</span>) and the number of outstanding tickets is as desired (<span class="math">T_s=0</span>), the price stays the same. If there are many new orders (a sudden spike in the expected MEV), the pricing mechanism will hike the price substantially. For example, if 100 orders were to come in, the purchase price for them could rise by orders of magnitude; the exact specification would need to be determined based on other auction paramters such as the size of the ticket pool. Builders will of course track incoming orders in real time and update their estimate of the final purchase price. Therefore, even during a sudden rise in expected MEV, there will only be new orders up to the point where the deduced price matches expected MEV.</p>
<p>As another option, <span class="math">W_T</span> can be longer, setting the price <span class="math">W</span> slots after orders have been added to the beacon block. In Figure 1, <span class="math">W=3</span>. An asymmetric window spanning 4 slots up to and including the processing slot is then an option. The most important benefit is MEV resistance during spikes, as will be further discussed in Section 4.3. Other potential benefits include better pricing granularity, a more complete picture when pricing orders, and the marginal simplification in ETA from pricing and sequencing orders at the same. Of course, it can be argued that the picture already is “complete” in the sense that builders can indicate expected MEV already at the current slot, albeit they may not be fully equipped to evaluate incoming orders in real-time. It can also be argued that <span class="math">W&gt;0</span> and <span class="math">W_T&gt;1</span> needlessly increase uncertainty and analytical complexity for builders as well as developers. As an example, builders may place an order several slots before a spike, but still need to pay closer to the real expected value of the MEV they are about to receive (priced closer to proposal time).</p>
<h4><a class="anchor" href="https://ethresear.ch#h-422-equations-13" name="h-422-equations-13"></a>4.2.2 Equations</h4>
<p>A rudimentary example will now be provided. Should this general mechanism be pursued, the exact price controller would have to be determined by reasoning about how quickly the price should adapt to changes in the willingness to buy tickets, sensitivity to ticket saturation, interplay between saturation and delta, sensitivity to MEV induction (see the next subsection), and by running simulations of the purchase process.</p>
<p>Ticket saturation and delta from the previous subsection is first weighed by window length and desired number of outstanding tickets</p>
<div class="math">
w_s=\frac{T_s}{c_s\hat{T}}, \quad w_{\delta}=\frac{T_{\delta}}{c_{\delta}W_T},
</div>
<p>using the constants <span class="math">c_s=2^3</span> and <span class="math">c_{\delta}=2^6</span>. The percentage change <span class="math">w</span> to the ticket price applied each slot (minting <span class="math">n</span> orders) is</p>
<div class="math">
w=(1+w_s)(1+w_{\delta})^k.
</div>
<p>This post uses <span class="math">k=2</span>, ensuring a non-linear price response as <span class="math">T_{\delta}</span> grows. This can be particularly relevant at shorter windows <span class="math">W_T</span>. Setting <span class="math">k=3</span> is also viable. The constant <span class="math">c_{\delta}</span> can then alternatively be increased to offer better pricing granularity at a lower ticket delta, while still offering some guarantees regarding the maximum number of orders that may come in during one slot. The price <span class="math">p</span> updates from its level at the previous slot <span class="math">p_0</span> to its level at the present slot <span class="math">p_1</span> as</p>
<div class="math">
p_1=w \times p_0.
</div>
<h4><a class="anchor" href="https://ethresear.ch#h-423-visualizations-14" name="h-423-visualizations-14"></a>4.2.3 Visualizations</h4>
<p>Figure 2 illustrates what a pricing schedule according to <span class="math">w</span> would look like for the outlined equations, with <span class="math">\hat{T}=4096</span> and <span class="math">W_T=32</span>. The yellow band stipulates no price change (<span class="math">w=1</span>), and passes through the intersection of the black lines, which correspond to a neutral ticket delta (x-axis) and saturation (y-axis). There have been suggestions of much <a href="https://www.youtube.com/watch?v=IrJz4GZW-VM">higher</a> <span class="math">\hat{T}</span>. This issue relates to a wide range of <a href="https://ethresear.ch/t/economic-analysis-of-execution-tickets/18894">considerations</a> that are not the focus of this post.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/8/383433ceb5b2bfb3db8a88907125d3d913dc871b.png" title="Figure 2"><img alt="Figure 2" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/3/8/383433ceb5b2bfb3db8a88907125d3d913dc871b_2_668x500.png" width="668" /></a></div><p></p>
<p><strong>Figure 2.</strong> Rudimentary example for <span class="math">W_T=32</span> of a percentage change in ticket price  that varies with delta in ticket sales and the overall saturation of tickets in the pool. Black lines indicate a neutral delta (one ticket sold per slot) and saturation (<span class="math">T=\hat{T}</span>).</p>
<p>Figure 3 instead shows a pricing schedule when <span class="math">W_T=1</span> using the same equation and settings as previously. If no orders come in during the measured slot, <span class="math">T_{\delta}=-1</span>. Note that the colormap is log-scaled to capture the large increase in <span class="math">w</span> that is instituted if 64 orders were to come in during a single slot. When <span class="math">T_W=32</span> (Figure 2), a large jump in orders would affect the price for 32 consecutive slots (assuming an asymmetric window), before the purchase takes place, and so <span class="math">w</span> will naturally be lower on a per-slot basis.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/f/4f34c0d1471b29367616a9d71dc0a1fe156cfd73.png" title="Figure 3"><img alt="Figure 3" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/4/f/4f34c0d1471b29367616a9d71dc0a1fe156cfd73_2_659x500.png" width="659" /></a></div><p></p>
<p><strong>Figure 3.</strong> Rudimentary example for <span class="math">W_T=1</span> of a percentage change in ticket price that varies with delta in ticket sales and the overall saturation of tickets in the pool. Black lines indicate a neutral delta (one ticket sold in the slot) and saturation (<span class="math">T=\hat{T}</span>).</p>
<p>The relative change at <span class="math">W_T=1</span> for different <span class="math">T_{\delta}</span> is shown in Figure 4, at a neutral ticket saturation (<span class="math">T_s=0</span>). The price change instituted with this setting for between 0 to 4 orders is {0.969, 1, 1.031 1.063 1.096}. The same granularity can be preserved at lower quantities of orders while further raising the price at higher quantities, by increasing <span class="math">k</span>.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/1/b1cd5d40cffb62dcaa44cc90cb317c9a4f0ebb76.png" title="Figure 4"><img alt="Figure 4" height="325" src="https://ethresear.ch/uploads/default/optimized/3X/b/1/b1cd5d40cffb62dcaa44cc90cb317c9a4f0ebb76_2_690x325.png" width="690" /></a></div><p></p>
<p><strong>Figure 4.</strong> Rudimentary example for <span class="math">W_T=1</span>, focusing on the relative price change <span class="math">w</span> across <span class="math">T_{\delta}</span> at a neutral saturation. If 60 orders come in during a single slot, the price rises sharply.</p>
<p>Figure 5 instead plots the response at <span class="math">T_{\delta}=-1</span> across <span class="math">T_s</span>. In other words, it shows how the price would change if no purchase orders are registered.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/8/885e3dfacabd2f85c9ee4480a30e67576cb30f5b.png" title="Figure 5"><img alt="Figure 5" height="312" src="https://ethresear.ch/uploads/default/optimized/3X/8/8/885e3dfacabd2f85c9ee4480a30e67576cb30f5b_2_690x312.png" width="690" /></a></div><p></p>
<p><strong>Figure 5.</strong> Rudimentary example for <span class="math">W_T=1</span>, focusing on the relative price change <span class="math">w</span> across <span class="math">T_s</span> when no purchase order comes in.</p>
<h3><a class="anchor" href="https://ethresear.ch#h-43-slot-surge-pricing-15" name="h-43-slot-surge-pricing-15"></a>4.3 Slot surge pricing</h3>
<p>In the outlined pricing mechanism, there is a remaining opportunity for the beacon proposer to derive some MEV at shorter windows <span class="math">T_W</span>. This happens during a sudden spike in interest for purchasing tickets between the point where attesters have observed purchase orders and the slot boundary.</p>
<p>Let <span class="math">n_a</span> be the equilibrium quantity of orders that would have come in during a slot if a spike happened before the attester observation deadline (purple arrows in Figure 1). Builders keep track of incoming orders and calculate the current ticket price, which when compared to the updated expected MEV <span class="math">V_e</span> produces <span class="math">n_a</span> orders. If a spike comes in after the attester deadline, the proposer has exclusivity and could (be paid to) include only a subset of the orders <span class="math">n_p</span>. The surplus MEV for the proposer emerges from providing a lower expected purchase price for each order it lets through. This is a monopoly pricing regime, wherein the proposer sells spots at a price approaching <span class="math">V_e-p_1</span>. It determines <span class="math">n_p</span> to maximize its revenue <span class="math">R(n_p)</span>, in accordance with the revenue function:</p>
<div class="math">
\text{Maximize} \quad R(n) = n (V_e(n) - p_1(n)).
</div>
<p>Here, <span class="math">p_1(n)</span> is based on the price equation provided in the previous subsection. Also note that if many purchase orders come in, <span class="math">V_e</span> might gradually fall (if there is a temporary spike); hence <span class="math">V_e(n)</span>. If the potential price increase between beacon slots is set to be more moderate, while prices still can surge from a high quantity of purchased tickets within a single slot, the proposer’s potential revenue would be reduced. The proposer can then sell fewer spots at a lower price. One way to do this is to set the price in the current slot as previously</p>
<div class="math">
p_1=w \times p_0,
</div>
<p>but to not incorporate the full price change when setting the value <span class="math">p^*_0</span> that will be used as <span class="math">p_0</span> when pricing the next slot</p>
<div class="math">
p^*_0=\left(1+\frac{1-w}{c_w}\right) \times p_0.
</div>
<p>The constant <span class="math">c_w</span> is set above 1, e.g., <span class="math">c_w=2</span>. During a spike in expected value up to a new baseline <span class="math">V_e</span>, the price would then theoretically stay rather fixed (at a new higher level) for subsequent slots, with the number of orders in each slot gradually decreasing, until it proceeds at the regular pace of one purchase order per slot. Yet note that if <span class="math">V_e</span> rises from a temporary opportunity, there will be a bit more MEV for the proposer to extract still, because a lot of the value can depend on getting in early. This also depends on if the mechanism is ET or ETA and the size of the ticket pool. The discussion offers some further thoughts on the proposer’s ability to extract MEV.</p>
<p>As a concluding remark, it should always be remembered that a big ticket pool acts to temper fluctuations in the expected value of tickets. The buyer does not necessarily buy the right to sell tickets within the next couple of epochs, but rather within the next couple of hours, days, weeks or months, depending on the setting for <span class="math">\hat{T}</span>—and it turns out that when measured over longer periods, the level of the MEV has been <a href="https://youtu.be/IrJz4GZW-VM?feature=shared&amp;t=1241">very stable</a> in Ethereum.</p>
<h3><a class="anchor" href="https://ethresear.ch#h-44-the-role-of-a-maximum-price-16" name="h-44-the-role-of-a-maximum-price-16"></a>4.4 The role of a maximum price</h3>
<p>Each buyer assigns a max price to the order. This is the value that needs to be backed by the debit account. If the max price is insufficient at the time of pricing, such that the actual price is higher, the builder does not receive a ticket/slot. Yet builders could make unbacked orders to starve off competitors, which would bring down the purchase price. It seems desirable to not force builders to analyze the balances of every competitor to determine which bids are real and which are “fake”. One simple way to avoid such a situation is to penalize builders for placing orders that turn out to be unbacked at the time of purchase. This can potentially be combined with setting a validity rule requiring some minimum max price, either relative to the prevailing price at bid time, or/and as a fixed overall minimum.</p>
<p>Penalizing builders however exacerbates another potential issue. During an unforeseen spike in expected MEV, there are circumstances where a builder could “liquidate” its competitors’ bids if the current purchase price is close to their stipulated maximum. A builder could enter new bids forcing other builders out, to penalize them and gain cheaper tickets. For this reason, the mechanism could reduce gameability and the risks as well as improve capital efficiency for builders by stipulating an absolute maximum purchase price. A builder that bids the absolute maximum is guaranteed to not get liquidated and will always receive a ticket. This does not mean that the protocol will burn less MEV, merely that in times of extremely high expected MEV, there will temporarily be a higher quantity of bids, wherein each order has a lower chance of actually getting one of the desirable profitable slots.</p>
<p>What should the absolute maximum be set to if this path is pursued? In <a href="https://flashbots-data.s3.us-east-2.amazonaws.com/index.html">data</a> provided by <a href="https://www.flashbots.net/">Flashbots</a> spanning 2.7 million blocks between the last quarter of 2022 and the third quarter of 2023, the maximum average <a href="https://hackmd.io/@flashbots/quantifying-REV">REV</a> across 64 slots is 19.5 ETH. The peak average is skewed by a few spurious blocks with REV of several 100 ETH that may have been hard to predict beforehand. This average does therefore not represent a realistic expected MEV for builders bidding many slots in advance. Expand the window by a factor of 4 to 256 and the maximum average falls almost by a factor of 4, to 5.25. Setting the absolute maximum to 5 ETH would thus presumably not influence the auction even in times of extreme market conditions, since that price would hardly ever be reached.</p>
<h2><a class="anchor" href="https://ethresear.ch#h-5-discussion-17" name="h-5-discussion-17"></a>5. Discussion</h2>
<p>A MEV resistant dynamic pricing auction for selling execution proposal rights has been presented, relevant to the research of both ETs and EAs. It seeks to remove agency from the beacon proposer, thus inducing less MEV. This is achieved by having every order result in a sale, and every order coming in during the same slot having the same expected sales price. The execution ticket auction (ETA) sequences orders directly for proposal by leveraging the RANDAO. Orders that came in during the same slot can otherwise be minted collectively into tickets, with sequencing pursued at a later stage in accordance with the ET proposal.</p>
<p>If pursuing this auction mechanism, the dynamic pricing step would require substantial analysis. One sensitive part is the balance between moderating changes in the supply of orders while still offering sufficient pricing granularity. A high <span class="math">k</span> can be useful here. Another potential avenue is to hold the auction less frequently. The expected timing of orders within the slot would also be interesting to study—orders can be placed early to starve off others, or late to gain better information. One could even theorize that some builders will wait until after the attester deadline, and then pay the proposer a small fee for exclusive post-deadline inclusion (the benefit being to avoid race conditions).</p>
<p>Transactions to fund or withdraw from a builder’s debit account would need to be synchronized with the validity check to avoid race conditions. It may be convenient to expand the role of the debit account if it is desirable to subject builders to slashing or penalties at the execution proposal stage. In other words, the debit account might also function as a stake.</p>
<p>Just as with MEV pricing auctions, attesters accepting or rejecting a block based on some observation deadline is potentially sensitive. However, this particular design should hopefully be less so, since there will only be one order on average per block to observe, and less value (even potentially negative) in bidding later in the block. A potential benefit of an auction administered instead at the execution layer is the “endogenous” component, facilitating a higher burn; the value of a ticket increases if the current ticket holder can extract value from future ticket holders through MEV. However, this direction raises gameability concerns if a single actor can come to monopolize the auction (ILs may here be useful). A MEV resistant mechanism, as here proposed, originating at the consensus layer, therefore seems like a viable direction.</p>
<p>It might seem tempting to replicate some facets of the proposed design for transaction processing: making the protocol more MEV resistant by having attesters observe transactions, the protocol sequence them by RANDAO, and the price adjust in a slot-attentive fashion. However, the requirements for transactions are different than for the purchase orders of execution rights analyzed in this post (e.g., time, quantity). Translating the ideas of this post directly to transaction processing might therefore unfortunately be difficult. Yet the proposed mechanism could perhaps lend some inspiration going forward.</p>
<p>It should be noted that multi-block MEV is a separate topic of concern. The proposed mechanism is resistant to inducing MEV at the purchase stage but does not preclude multi-block MEV. This is a general issue and an underexplored topic at this point in time. Censorship resistance is likewise an important problem not addressed by the auction mechanism. Various strategies, such as ILs (<a href="https://ethresear.ch/t/fork-choice-enforced-inclusion-lists-focil-a-simple-committee-based-inclusion-list-proposal/19870">1</a>, <a href="https://ethresear.ch/t/one-bit-per-attester-inclusion-lists/19797">2</a>, <a href="https://ethresear.ch/t/unconditional-inclusion-lists/18500">3</a>), have been proposed. Whether the presented auction mechanism can be one part of an overall architecture that also tackles other issues remains to be explored.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/mev-resistant-dynamic-pricing-auction-of-execution-proposal-rights/20024">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 09 Jul 2024 10:52:26 +0000</pubDate>
</item>
<item>
<title>Deep Diving Attestations - A quantitative analysis</title>
<link>https://ethresear.ch/t/deep-diving-attestations-a-quantitative-analysis/20020</link>
<guid>https://ethresear.ch/t/deep-diving-attestations-a-quantitative-analysis/20020</guid>
<content:encoded><![CDATA[
<h1><a class="anchor" href="https://ethresear.ch#deep-diving-attestations-1" name="deep-diving-attestations-1"></a>Deep Diving Attestations</h1>
<p><em>I want to provide some quantitative stats on…</em></p>
<ul>
<li><em>Head</em>-, <em>target</em>-, and <em>source</em> votes,</li>
<li>The individual node operators’ attestation performance, including the best and worst validators,</li>
<li>Attestation <em>timing</em> and <em>inclusion delay</em>, and</li>
<li>The impact of <em>MEV-Boost, CL clients, Proposer Timing Games</em> and <em>Big Blocks with Blobs</em> on attestation accuracy.</li>
</ul>
<p><img alt="doge" height="458" src="https://ethresear.ch/uploads/default/original/3X/2/a/2a11d5d44000665f0ed449873280783c5e163cd6.png" width="459" /></p>
<p><em>Many thanks to <a href="https://x.com/casparschwa">Caspar</a>, <a href="https://x.com/dapplion">DappLion</a>, <a href="https://x.com/barnabemonnot">Barnabé</a> and <a href="https://x.com/potuz_eth">Potuz</a> for their feedback and review!</em></p>
<h2><a class="anchor" href="https://ethresear.ch#data-2" name="data-2"></a>Data</h2>
<p>I use data ranging from slot 9,169,184 to slot 9,392,415, amounting to 6,975 epochs, 31 days of data.<br />
The goal is to provide some initial results from analyzing attestations, as a warm-up for analyzing correlated attestation penalties (<a href="https://eips.ethereum.org/EIPS/eip-7716">EIP-7716</a>).<br />
Some of the data is collected by myself using custom parsing scripts. Other data was provided by <a href="https://ethpandaops.io/">EthPandaOps</a>. This includes timing data collected from running nodes of <strong>every client</strong> in the regions <strong>Sydney</strong>, <strong>Helsinki</strong>, and <strong>San Francisco</strong>, with all nodes being <strong>subscribed to all subnets</strong>. For classifying CL clients, the <a href="https://github.com/sigp/blockprint">blockprint</a> tool was used.</p>
<blockquote>
<p>Importantly, my solo staker categorization is done very conservatively to avoid confusing professional entities with solo stakers. In total, my dataset contains 8,488 validators classified as solo stakers.</p>
</blockquote>
<p>The code for creating the charts is published in <a href="https://github.com/nerolation/eth-deep-diving-attestations">this repo.</a></p>
<h2><a class="anchor" href="https://ethresear.ch#attestations-3" name="attestations-3"></a>Attestations</h2>
<h3><a class="anchor" href="https://ethresear.ch#the-basics-4" name="the-basics-4"></a>The Basics</h3>
<p><a href="https://eth2book.info/capella/part2/consensus/">Attestations</a> are at the core of Ethereum. Through attesting to past checkpoints, Ethereum’s validators agree on a state to become irreversible (<a href="https://eth2book.info/capella/part2/consensus/casper_ffg/">Casper FFG</a>). Furthermore, validators use attestations to agree upon the tip of the chain, deciding which transactions get confirmed and which don’t (<a href="https://eth2book.info/capella/part2/consensus/lmd_ghost/">LMD GHOST</a>).<br />
Every validator, backed by its stake, participates in every epoch and is randomly assigned a slot, during which it is expected to broadcast its view of the chain through attesting.</p>
<p><strong>An attestation contains three things:</strong></p>
<ul>
<li>A <em>source</em> vote: The block (and all predecessors) to be finalized</li>
<li>A <em>target</em> vote: The block (and all predecessors) to be justified (=pre-finalized)</li>
<li>A <em>head</em> vote: The block seen as the head of the chain.</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/8/98c353dd9366c270a1d3ef3b75c9efaec79c4fcd.png" title="epochslotvalidator"><img alt="epochslotvalidator" height="182" src="https://ethresear.ch/uploads/default/optimized/3X/9/8/98c353dd9366c270a1d3ef3b75c9efaec79c4fcd_2_690x182.png" width="690" /></a></div><p></p>
<p>Since the <a href="https://ethereum.org/en/history/">Deneb hardfork</a> that included <a href="https://eips.ethereum.org/EIPS/eip-7045">EIP-7045</a>, attestations for a slot in epoch N can be included up until the end of epoch N+1. However, <a href="https://eth2book.info/capella/part2/incentives/rewards/">inclusion doesn’t guarantee a reward</a>:<br />
To be rewarded, a validator must ensure its source vote is included within 5 slots. The target vote has to be included within 32 slots to be rewarded. Head votes must be included in the following slot to be eligible for a reward.</p>
<p>As of today, Ethereum counts <a href="https://beaconcha.in/charts/validators">~1.03</a> million validators. This means we have 1.03 million votes every epoch, ~32,000 every slot. In one day, with 225 epochs, there are approximately 225 million attestations. This data grows quite fast.</p>
<p>If the <strong>source vote</strong> is <strong>invalid</strong>, then the <strong>target</strong> and <strong>head vote</strong> <strong>MUST</strong> be <strong>invalid</strong> too.</p>
<p>A slot can be broken down into 3 phases:<br />
<img alt="slottime" height="92" src="https://ethresear.ch/uploads/default/original/3X/e/2/e2bf8c2e61a6f0079f45f24b5648a1d68f960153.png" width="632" /></p>
<ol>
<li>Validators attest when they have seen a block for the current slot or at second 4 in the slot - the attestation deadline. A block broadcasted at second 0 in the slot has 4 seconds to be seen by all relevant validators and collect votes. Late blocks risk not receiving enough attestations and being reorged by a subsequent block.</li>
<li>Between second 4 and 8 in the slot, attestations are <a href="https://eth2book.info/capella/part2/building_blocks/aggregator/">aggregated</a> and broadcasted by selected validators.</li>
<li>Eventually, the subsequent block proposer includes them into its block.</li>
</ol>
<blockquote>
<p>For more in-depth explanations check out this post by Georgios and Mike on “<a href="https://www.paradigm.xyz/2023/04/mev-boost-ethereum-consensus">Time, slots, and the ordering of events in Ethereum Proof-of-Stake</a>”.</p>
</blockquote>
<h3><a class="anchor" href="https://ethresear.ch#definitions-5" name="definitions-5"></a>Definitions</h3>
<p><strong>Missed vs. Failed:</strong></p>
<ul>
<li>A validator can either <strong>miss</strong> its attestation (<em>missed</em>) or attest to a <strong>wrong</strong> checkpoint (<em>failed</em>).</li>
<li><strong>Missed attestations</strong> can happen if the node running the validator is out of sync or offline.</li>
<li><strong>Voting for a wrong checkpoint</strong>, e.g. a wrong head, can have various reasons like receiving a block too late, being out of sync or even having a bug, etc.</li>
<li><strong>Regardless of the reason, a <em>failed</em> vote tells us one important fact about a validator—it is online.</strong></li>
</ul>
<p>In the following, we’ll also need the term <em><strong>“high-performing validator”</strong></em> which is a validator that hasn’t failed to cast a correct and timely head vote over the complete time frame analyzed.</p>
<h3><a class="anchor" href="https://ethresear.ch#attestation-inclusion-delay-6" name="attestation-inclusion-delay-6"></a>Attestation Inclusion Delay</h3>
<p>In the best case, attestations are included in the block of the <strong>next slot</strong>, causing a <strong>delay of 0</strong>. Sometimes, especially when the next proposer is offline or gets reorged, attestations are not included in the next slot. Then, the validator misses out on the rewards from the correct head vote, even though the attestation can still be included in a later block.</p>
<p>The following chart shows the distribution of the inclusion delay over seconds 1-63 and the <strong>clients the attesters were using</strong>.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/d/fd890aacb6b6636f2240881d9dbbaa7b721face8.png" title="correct_head_delay_clients"><img alt="correct_head_delay_clients" height="316" src="https://ethresear.ch/uploads/default/optimized/3X/f/d/fd890aacb6b6636f2240881d9dbbaa7b721face8_2_690x316.png" width="690" /></a></div><p></p>
<ul>
<li>95.85% of attestations are included in the next slot.</li>
<li>~1.2% of attestations are included in the slot after the next.</li>
<li>When a new epoch begins, old attestations are again picked up and finally included.
<ul>
<li>This is weird (<em>but there’ll be an explanation little down below</em>).</li>
<li>Attestations of validators of all clients are affected.</li>
</ul>
</li>
</ul>
<p><strong>This raises the question, “<em>what are clients doing?</em>”</strong></p>
<p>Examples include slots <strong><a href="https://beaconcha.in/slot/9267438#attestations">9267438</a></strong> with a delay of 35 (5250 validators), <strong><a href="https://beaconcha.in/slot/9267425#attestations">9267425</a></strong> with a delay of 52 (1813 validators), or slot <strong><a href="https://beaconcha.in/slot/9267427#attestations">9267427</a></strong> with a delay of 36 slots (1305 validators).</p>
<p>What if those late attestations were already included earlier and were later just included again (h/t <a href="https://github.com/dapplion">dapplion</a>)? To analyze that, we reproduce the above chart but separate by <em><strong>first inclusion</strong></em> and <em><strong>every following inclusion</strong></em>:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/8/a8cfb4ec1e6b9486dbafddb9587eeb55be1b3d1c.png" title="correct_head_delay_reinclusion"><img alt="correct_head_delay_reinclusion" height="316" src="https://ethresear.ch/uploads/default/optimized/3X/a/8/a8cfb4ec1e6b9486dbafddb9587eeb55be1b3d1c_2_690x316.png" width="690" /></a></div><p></p>
<p>First, it is interesting that almost <strong>half of the attestations included with a delay of 1 slots</strong> (note: best is 0) <strong>have already been included in an earlier slot</strong>. This is possible because proposers are free to pick attestations that have already been included in the past 63 slots and include them again. Additionally, a block can contain the same attestations multiple times, aggregated differently.</p>
<p>We can see that the majority of the attestations included in the second hump with a delay of around 35 slots are <strong>reincluded</strong> attestations.</p>
<p>This raises the question, “<em>why does this occur with a delay of more than 32 slots?</em>”</p>
<p>In <strong>percentage</strong> terms, we can see the <em>first inclusion</em> share reducing over an increasing delay:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/e/5e9f624dcd52ea92cbfedbd5d0da5ebf1f2113e3.png" title="correct_head_delay_reinclusion_per"><img alt="correct_head_delay_reinclusion_per" height="316" src="https://ethresear.ch/uploads/default/optimized/3X/5/e/5e9f624dcd52ea92cbfedbd5d0da5ebf1f2113e3_2_690x316.png" width="690" /></a></div><p></p>
<p>To dig deeper into this reinclusion finding, let’s check the <strong>CL clients that built the blocks</strong> that included attestations with &gt;32 slots delay:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/2/62e38ffb13e5375a76e8d567990bbf2dfcd0c9e6.png" title="correct_head_delay_clients_proposers"><img alt="correct_head_delay_clients_proposers" height="316" src="https://ethresear.ch/uploads/default/optimized/3X/6/2/62e38ffb13e5375a76e8d567990bbf2dfcd0c9e6_2_690x316.png" width="690" /></a></div><p></p>
<p>We can see quite clearly that it’s mainly Prysm proposers who include attestations that have already been included earlier, which is very likely a bug.</p>
<blockquote>
<p>The fact that the plot also shows other clients affected might stem from inaccuracies in classifying clients probabilisticly.</p>
</blockquote>
<p><em>The Prysm team was notified.</em></p>
<p><strong>Edit</strong>: <em>The Prysm team was faster in fixing the bug than I was in finishing this post.</em></p>
<p><strong>Fix</strong>: <a class="inline-onebox" href="https://github.com/prysmaticlabs/prysm/pull/14156#event-13323121631">Increase attestation seen cache exp time to two epochs by terencechain · Pull Request #14156 · prysmaticlabs/prysm · GitHub</a></p>
<h2><a class="anchor" href="https://ethresear.ch#missedfailed-attestations-7" name="missedfailed-attestations-7"></a>Missed/Failed Attestations</h2>
<h3><a class="anchor" href="https://ethresear.ch#missedfailed-head-votes-8" name="missedfailed-head-votes-8"></a>Missed/Failed Head Votes</h3>
<p><strong>Head votes</strong> are the <strong>most difficult</strong> part of an attestation. They need to be cast correctly and timely. Per <a href="https://github.com/ethereum/consensus-specs/blob/1642610bd5994d344fb1b6a9f44ec0e14a527580/specs/phase0/validator.md#attesting">honest validator spec</a>, <strong>validators have 4 seconds</strong> to receive and validate a block for the current slot. If no block is received until second 4, validators attest to the block in the previous slot. <strong>Timeliness in the context of head votes means 1 slot.</strong> Although older head votes can be included, there is no reward for the respective validator.</p>
<blockquote>
<p>The legend is ordered in descending order by the sum of missed votes.<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/3/2317707179ceb2e3d59cebbda6ca85119edb590e.png" title="missed_head_votes_over_date"><img alt="missed_head_votes_over_date" height="380" src="https://ethresear.ch/uploads/default/optimized/3X/2/3/2317707179ceb2e3d59cebbda6ca85119edb590e_2_690x380.png" width="690" /></a></div><p></p>
</blockquote>
<p><strong>On average, we observe around ~500 missed or wrong head votes out of ~32k validators per slot and ~16k, out of ~1m, per epoch. This represents around 1.56%.</strong></p>
<blockquote>
<p>The entity labeled as <em>unidentified</em> may consist of multiple independent parties, including solo stakers and entities that haven’t been identified yet, and it has a total market share of 20% of all validators.</p>
</blockquote>
<p>Assuming every node operator performs equally, the market share of each entity should reflect its share of missed head votes. However, this is not the case and we see certain node operators being superior compared to others.</p>
<p><strong>The following chart visualizes the delta in the expected number of missed head votes based on market share and the actual number of missed attestations.</strong><br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/7/971dec860541e8e89421505e5ebabcbe71d6154a.png" title="delta_missed_head_votes"><img alt="delta_missed_head_votes" height="335" src="https://ethresear.ch/uploads/default/optimized/3X/9/7/971dec860541e8e89421505e5ebabcbe71d6154a_2_690x335.png" width="690" /></a></div><p></p>
<p>While entities such as <em>Kiln</em>, <em>Ether_fi</em>, <em>Lido</em>, <em>Renzo</em>, <em>Figment</em>, and <em>Stakefish</em> perform better than the average, we observe that Rocketpool validators, Kraken validators, and solo stakers miss up to 3% more head votes than their market share.</p>
<p><strong>Focusing on the slot indices in epochs, we distinguish between missing a head vote due to being offline and voting for the wrong head.</strong></p>
<p>The following chart shows the average number of missed/wrong head votes over the slots of an epoch:<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/9/f975b9422a722e62e609aa65aaa40ecc1f722ac7.png" title="failed_missed_head_votes"><img alt="failed_missed_head_votes" height="316" src="https://ethresear.ch/uploads/default/optimized/3X/f/9/f975b9422a722e62e609aa65aaa40ecc1f722ac7_2_690x316.png" width="690" /></a></div><p></p>
<p>From the above chart, we can infer:</p>
<ul>
<li>There is a fairly <strong>constant number of <em>missed</em> head votes</strong>.
<ul>
<li><strong>This is expected</strong> as <em>lost-key validators</em> contribute a constant portion to that category.</li>
</ul>
</li>
<li>The beginning of an epoch, particularly the first slot, has significantly more wrong head votes than the rest.
<ul>
<li><strong>This is expected</strong> because the proposer in the first slot has to carry out the <strong>epoch transition</strong>. It must then broadcast that block to reach all attesters. T</li>
</ul>
</li>
<li>The average amount of missed/wrong head votes is <strong>3 times larger</strong> in the first slot of an epoch than in the epochs 2-32.</li>
</ul>
<p>Focusing on missed head votes and CL clients, we cannot see anything suspicious in the following chart:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/7/f7cbecfb8118dcd5ca0e37e04df641e8c1c994e4.png" title="failed_missed_head_votes_over_clclient"><img alt="failed_missed_head_votes_over_clclient" height="287" src="https://ethresear.ch/uploads/default/optimized/3X/f/7/f7cbecfb8118dcd5ca0e37e04df641e8c1c994e4_2_690x287.png" width="690" /></a></div><p></p>
<p>In general, it looks like all CL clients are affected by early-in-epoch misses the same:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/5/75c8e90cf46709ceb37998916b3ea77c7b9dfe88.png" title="failed_missed_head_votes_over_clclient_over_slot"><img alt="failed_missed_head_votes_over_clclient_over_slot" height="316" src="https://ethresear.ch/uploads/default/optimized/3X/7/5/75c8e90cf46709ceb37998916b3ea77c7b9dfe88_2_690x316.png" width="690" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#missedfailed-target-votes-9" name="missedfailed-target-votes-9"></a>Missed/Failed Target Votes</h3>
<p>Target votes are already easier to get right. The only exception is the first slot of an epoch that follows the <strong>epoch boundary</strong>: In such cases, the head vote equals the target vote and validators having their target vote wrong tend to vote for the parent block (=the block in the last slot of the previous epoch) instead.</p>
<p>On average, we observe around 150 missed target votes per slot and 4,800 per epoch. This represents around <strong>0.48%</strong> of all validators.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/7/570b69e8ccefa7585e259961fa4afa09206a1663.png" title="missed_target_votes_over_date"><img alt="missed_target_votes_over_date" height="380" src="https://ethresear.ch/uploads/default/optimized/3X/5/7/570b69e8ccefa7585e259961fa4afa09206a1663_2_690x380.png" width="690" /></a></div><p></p>
<p>Visualizing the same over the different CL clients, we see all clients affected to extents close to their market share.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/8/b8284e30b6f3f61ed8a95d5989a6417c275f82bc.png" title="failed_missed_target_votes_over_clclient"><img alt="failed_missed_target_votes_over_clclient" height="287" src="https://ethresear.ch/uploads/default/optimized/3X/b/8/b8284e30b6f3f61ed8a95d5989a6417c275f82bc_2_690x287.png" width="690" /></a></div><p></p>
<p>Looking at the entities that perform better than others, we again see operators such as Lido, Renzo, Mantle, Coinbase, etc. outperforming the average.</p>
<blockquote>
<p>Notably, Lido isn’t a single NO but consists of multiple operators that I combined for simplicity.</p>
</blockquote>
<p>On the other hand, Rocketpool validators and solo stakers perform worse and miss up to 3% more target votes than expected.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/7/17818eccecb8186179f09dbfb5b0fc3de023f5dc.png" title="delta_missed_target_votes"><img alt="delta_missed_target_votes" height="335" src="https://ethresear.ch/uploads/default/optimized/3X/1/7/17818eccecb8186179f09dbfb5b0fc3de023f5dc_2_690x335.png" width="690" /></a></div><p></p>
<p>As seen in <a href="https://ethresear.ch/t/the-second-slot-itch-statistical-analysis-of-reorgs/16333">previous analysis</a> on reorgs, epoch boundaries can cause troubles for certain validators when it comes to proposing a block.<br />
<strong>Blocks are more frequently reorged if they are proposed in the first or second slot of an epoch.</strong> Thus, we would expect those blocks to be responsible for the largest split-views among validators, causing some to attest to the current block, and others to the parent block.</p>
<p>Even though expected, we can see that the slot index in an epoch has a major impact on failed target votes:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/0/20b331e0885aaa2efed7972a75ecbe489ab8dd26.png" title="failed_missed_target_votes"><img alt="failed_missed_target_votes" height="316" src="https://ethresear.ch/uploads/default/optimized/3X/2/0/20b331e0885aaa2efed7972a75ecbe489ab8dd26_2_690x316.png" width="690" /></a></div><p></p>
<p><strong>Target votes are the hardest to get right at the beginning of an epoch.</strong> This is visible in the above diagram showing the <strong>first slot of an epoch with 18x more wrong target votes</strong> than other slots. The thing is, timely and correct target votes bring twice as many rewards than head or source votes.</p>
<p>Although looking problematic, I’d argue this isn’t a big issue. A target vote at the beginning of an epoch is essentially just a head vote, and the relative share of failures in the first slot at 6.4% is still relatively low. Furthermore, it is a known fact that epoch boundaries come with many different cascading effects including missed slots, which also contributes to the above finding.</p>
<blockquote>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/c/2c3c18e5acdc7080ad46101a7f05e775a50edb35.png" title="failed_missed_target_votes_over_clclient_over_slot"><img alt="failed_missed_target_votes_over_clclient_over_slot" height="316" src="https://ethresear.ch/uploads/default/optimized/3X/2/c/2c3c18e5acdc7080ad46101a7f05e775a50edb35_2_690x316.png" width="690" /></a></div><br />
This phenomenon seems to be agnostic to CL clients.<p></p>
</blockquote>
<h3><a class="anchor" href="https://ethresear.ch#missedfailed-source-votes-10" name="missedfailed-source-votes-10"></a>Missed/Failed Source Votes</h3>
<p>Source votes are easy to get correct and even validators that are slightly out of sync have a good chance to vote for the right source checkpoint. This is because the to-be-voted-for checkpoint is at least 6.4 minutes (<em>=1 epoch</em>) in the past. Wrong source votes indicate that the validator is either out of sync or on a completely different chain. Thus, target and head votes must be incorrect if the source vote is wrong.</p>
<blockquote>
<p>For source votes one cannot differentiate between <em>missed</em> and <em>failed</em> because wrong source votes never make it onchain and are ignored by proposers/validators.</p>
</blockquote>
<p>On average, we observe around 100 missed source votes per slot, 3,200 per epoch.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/1/61f31f5346adcf9f2b85495f01d371c64e2bff69.png" title="missed_source_votes_over_date"><img alt="missed_source_votes_over_date" height="380" src="https://ethresear.ch/uploads/default/optimized/3X/6/1/61f31f5346adcf9f2b85495f01d371c64e2bff69_2_690x380.png" width="690" /></a></div><p></p>
<p>Similar to head and target votes, we observe an increased number of missed source votes at the beginning of an epoch. This MIGHT be related to the increased reorg probability at the beginning of an epoch but more analysis would be needed to confirm that.<br />
In general, validators usually have ample time (at least 32 slots) to cast their source vote. However, if their head vote is incorrect, it might result in the entire attestation being ignored by an aggregator and, consequently, not being recorded onchain.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/f/1f1e51c85696870bc5d10d561a9814172f0ef750.png" title="missed_source_votes_over_slot"><img alt="missed_source_votes_over_slot" height="380" src="https://ethresear.ch/uploads/default/optimized/3X/1/f/1f1e51c85696870bc5d10d561a9814172f0ef750_2_690x380.png" width="690" /></a></div><p></p>
<h2><a class="anchor" href="https://ethresear.ch#best-and-worst-validators-11" name="best-and-worst-validators-11"></a>Best and Worst Validators</h2>
<p>Validators cast a vote in every epoch and quickly checking <a href="https://beaconcha.in/">beaconcha.in</a>, more than 99.9% of validators are active in every epoch.</p>
<p>By summing up correct head votes, we can determine the best and worst-performing validators.</p>
<p><strong>The following chart visualizes the average missed/failed head votes per slot over the validator IDs:</strong></p>
<blockquote>
<p>Withdrawn validators are excluded.<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/4/94c96b54be8347449d0d060223ba0a9333dd3013.png" title="head_votes_over_validator_ids"><img alt="head_votes_over_validator_ids" height="316" src="https://ethresear.ch/uploads/default/optimized/3X/9/4/94c96b54be8347449d0d060223ba0a9333dd3013_2_690x316.png" width="690" /></a></div><p></p>
</blockquote>
<p>We can see that the missed slot rate is slightly <strong>increasing with increasing validator IDs,</strong> with outliers for the validators with IDs 0-30k, 300k-330k, and 780k-790k.<br />
The best validators are the group with IDs from 50k-60k.</p>
<p><strong>Over four weeks, most validators miss around 20-30 head votes:</strong></p>
<p>The following chart has a <strong>logarithmic y-axis</strong> to make sure we can also see the last bar on the very right that consists of validators that have never attested in the 4 weeks analyzed.<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/7/2757547120b4ca4f9068e3fe491289fa8dec1f06.png" title="failed_missed_head_per_validator_dist_per"><img alt="failed_missed_head_per_validator_dist_per" height="316" src="https://ethresear.ch/uploads/default/optimized/3X/2/7/2757547120b4ca4f9068e3fe491289fa8dec1f06_2_690x316.png" width="690" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#the-peak-of-performance-best-validators-12" name="the-peak-of-performance-best-validators-12"></a>The peak of performance (best validators)</h3>
<p>For the following, I use data ranging from epoch 292,655 to epoch 293,105, not the entire time frame analyzed, due to the sheer amount of data involved.</p>
<p><em><strong>High-performers</strong></em> are defined as validators who haven’t missed voting for the correct head during a time frame of 3 days, starting from the last slot analyzed and going backward.</p>
<p>The following table shows the largest node operators (sorted in descending order by market share) and the percentage of high-performing validators within 3 days compared to the total number of validators for each entity:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/7/5713faed8326eeee5afcdddcf300a1bbdd15e691.jpeg" title="performer_table"><img alt="performer_table" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/5/7/5713faed8326eeee5afcdddcf300a1bbdd15e691_2_422x500.jpeg" width="422" /></a></div><p></p>
<p>^ The entities in <em><strong>green</strong></em> have <em><strong>more</strong></em> high-performing validators than the average.</p>
<p><strong>The shares visualized using a bar chart look like the following:</strong></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/0/50ed0ae6c74385c61c57a0e9c89ac9beef1e5d64.png" title="topperformer_percentage"><img alt="topperformer_percentage" height="316" src="https://ethresear.ch/uploads/default/optimized/3X/5/0/50ed0ae6c74385c61c57a0e9c89ac9beef1e5d64_2_690x316.png" width="690" /></a></div><p></p>
<p>We can see that the average high-performer rate is around 0-5% for the shown entities.<br />
<strong>The outliers are <em>Everstake</em>, <em>Frax Finance</em> and <em>Rockx</em>.</strong></p>
<p><em>So, what are those 3 parties doing differently than others?</em></p>
<p><strong>There are two strategies an entity might apply:</strong></p>
<ol>
<li><em>Attest early</em> to ensure their vote has enough time to travel through the network and reach the next proposer for inclusion.</li>
<li><em>Attest late</em> to ensure they vote for the correct head of the chain. The longer a validator waits, the easier it is to determine the head of the chain as other validators have already voted -&gt; <em>the risk is that the vote might not reach the next proposer in time</em>.</li>
</ol>
<p>The latter strategy may be referred to as <em><strong><a href="https://ethresear.ch/t/timing-games-implications-and-possible-mitigations/17612#attester-timing-games-9">attester timing games</a></strong></em>.</p>
<p><em>But what is better?</em></p>
<p><img alt="Screenshot from 2024-06-27 20-22-27" height="311" src="https://ethresear.ch/uploads/default/original/3X/a/1/a19d7ddccff2f47d6d73e80b5e2b5f1b96572091.png" width="587" /></p>
<p>I asked my Twitter friends, and the majority voted for ‘seen later,’ indicating validators are playing timing games for increased attestation accuracy.</p>
<p>In truth, both are right.</p>
<p>The following chart shows the distributions of attestation-seen timestamps of high-performing validators vs. the rest (non-high-performing validators):</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/a/8acfc992ada1a3b0a22ce9f1cfb036a7f191ab8e.png" title="high_performer_vs_rest_timing"><img alt="high_performer_vs_rest_timing" height="304" src="https://ethresear.ch/uploads/default/optimized/3X/8/a/8acfc992ada1a3b0a22ce9f1cfb036a7f191ab8e_2_690x304.png" width="690" /></a></div><p></p>
<p>We can see that the largest share of head votes from <strong>high-performers</strong> is seen between <strong>second 2 and 3</strong> in the slot. We observe another spike right <strong>after second 5</strong> in the slot. For all other validators (cf. <em>rest</em>), the majority of head votes arrive between <strong>second 4 and 5</strong>.</p>
<p><strong>This points towards:</strong></p>
<ul>
<li>Most attesters are exceptionally good because they are <strong>faster</strong> than others.</li>
<li>Some attesters are exceptionally good because they might <strong>wait longer</strong> for more accuracy.</li>
</ul>
<p><strong>&gt; Early attestations by high-performing validators are seen some milliseconds earlier than the rest.<br />
&gt; Late attestations by high-performing validators are seen about 0.5 seconds later than the rest.</strong></p>
<p>It is worth noting that every high-performing validator can be part of both groups, e.g., attesting late to ‘weak’ blocks (cf. epoch boundaries) and early for ‘strong’ blocks.<br />
Validators with great network connectivity can afford to wait slightly longer. Furthermore, at any second in the slot, validators with great connectivity have more information available than other validators.</p>
<blockquote>
<p>A simple example is Coinbase: Technically, every Coinbase validator can be made aware of the votes of other Coinbase validators before voting. With a 10% market share, this provides significant additional security when voting on the correct head.</p>
</blockquote>
<p>By examining the head votes received/seen timings among the largest entities, we can clearly observe the differences. The best performers—Everstake, Frax Finance, and Rockx—typically attest between 4 and 6 seconds into the slot. While these entities outperform others, the following chart does not necessarily indicate a specific strategy being applied.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/2/c22c5adbedc46936379f5e7d1a775f4ad183cb0c.jpeg" title="image"><img alt="image" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/c/2/c22c5adbedc46936379f5e7d1a775f4ad183cb0c_2_300x500.jpeg" width="300" /></a></div><p></p>
<blockquote>
<p>And for a deeper dive into this topic check out <a href="https://ethereum.github.io/beaconrunner/notebooks/thunderdome/thunderdome.html">this simulation</a> by Barnabé that goes into the depth of strategic attesting behavior.</p>
</blockquote>
<p><strong>Finally, we get the following timings for the attestations over different CL clients:</strong><br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/0/e01321bc4823cb594799d2c67012c599e352cfcd.png" title="head_timing_cl_clients"><img alt="head_timing_cl_clients" height="345" src="https://ethresear.ch/uploads/default/optimized/3X/e/0/e01321bc4823cb594799d2c67012c599e352cfcd_2_690x345.png" width="690" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#we-like-them-for-what-they-are-worst-validators-13" name="we-like-them-for-what-they-are-worst-validators-13"></a>We like them for what they are… (worst validators)</h3>
<p>Other validators are <em>less performant</em> than others. This becomes obvious by looking at the number of missed attestations over time.</p>
<p>First, let’s consider the validators who are offline. There are various reasons for validators to go offline, and occasionally, random validators might experience brief outages. However, there is a small subset of validators that are very likely to remain permanently offline.</p>
<p><img alt="lost_keys" height="193" src="https://ethresear.ch/uploads/default/original/3X/5/7/57134b955a3de1691a15cf2678146f66c1e04126.png" width="456" /></p>
<p>We observed 139 validators, representing 0.014% of all validators, who were permanently offline in the 4 weeks analyzed.<br />
Now, one can argue that being offline for over 4 weeks doesn’t mean the validator is permanently offline. While this is fair, validators who have never cast any vote provide a good upper-bound estimate for the number of permanently offline validators who might have lost their keys.</p>
<p>Within those offline validators, we identify 12 solo stakers, 37 rocketpool validators, and 90 belonging to the category unidentified (=<em>20% market share, including many many actual solo stakers</em>).</p>
<p><strong>Most offline validators have low validator IDs:</strong><br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/d/2d0d6b73c9b05e0a3a4bc6ee7b5d020b4c98f24b.png" title="head_votes_over_offline_validator_ids"><img alt="head_votes_over_offline_validator_ids" height="316" src="https://ethresear.ch/uploads/default/optimized/3X/2/d/2d0d6b73c9b05e0a3a4bc6ee7b5d020b4c98f24b_2_690x316.png" width="690" /></a></div><p></p>
<p>We can see spikes around 730k and 870k, but the <strong>largest portion comes from OG validators</strong> with low IDs, those activated before the Merge. This is both expected and unexpected:</p>
<ul>
<li>OG stakers are generally crypto-native individuals who can securely manage private keys.</li>
<li>OG stakers are generally solo stakers who are less sophisticated.</li>
</ul>
<p>Based on the above, it seems the latter is more likely to hold true.</p>
<p><img alt="ogvalidator" height="411" src="https://ethresear.ch/uploads/default/original/3X/e/7/e70bfc6416b8a68b815f9835f7fe8e5076a340d5.png" width="457" /></p>
<br />
<p>Moving the focus to the bad validators that miss more than the mean but not all slots in the analyzed time frame, the bar chart looks like the following:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/5/e535223a575fe5764af70e6d31676c70b7c09e3c.png" title="head_votes_over_bad_validator_ids"><img alt="head_votes_over_bad_validator_ids" height="316" src="https://ethresear.ch/uploads/default/optimized/3X/e/5/e535223a575fe5764af70e6d31676c70b7c09e3c_2_690x316.png" width="690" /></a></div><p></p>
<p><strong>If low-id validators aren’t offline they perform quite well.</strong> Looking at the above graph, the largest share of “bad” validators can be found at the IDs 900k-1m.</p>
<h2><a class="anchor" href="https://ethresear.ch#attestations-big-blocks-and-blobs-14" name="attestations-big-blocks-and-blobs-14"></a>Attestations, Big Blocks, and Blobs</h2>
<p><strong>Big blocks and blocks with many blocks are expected to receive fewer attestations.</strong> This is because certain validators might struggle to download and validate the block fast enough and therefore vote for another block.</p>
<p>With <a href="https://www.eip4844.com/">EIP-4844</a> going live, the <a href="https://ethresear.ch/t/on-block-sizes-gas-limits-and-scalability/18444">block size</a> consists of 3 parts:</p>
<ul>
<li>EL Payload (~85 KB)</li>
<li>Beacon Block (excl. EL payload) / CL Part (~5 KB)</li>
<li>Blobs (~384 KB)</li>
</ul>
<p>Previous analysis showed that the average beacon block size excl. blobs is around 90 KiB. One blob has a size of 128 KiB. As a result, on average, we get blocks (incl. blobs) of size <code>nr_blobs * 128 + 90</code>, with the blob being the main contributor to the size of a block.</p>
<p><strong>More blobs mean more data that needs to be transmitted across the globe. Thus, we can expect more failed head votes for blocks with 6 blobs than those with one blob.</strong></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/6/76d5ade603b46d1fe148a918bda24f4fe3866fd6.png" title="failed_missed_head_size_boxplot"><img alt="failed_missed_head_size_boxplot" height="316" src="https://ethresear.ch/uploads/default/optimized/3X/7/6/76d5ade603b46d1fe148a918bda24f4fe3866fd6_2_690x316.png" width="690" /></a></div><p></p>
<p>This expectation holds when looking at the above boxplot diagram:<br />
 → <strong>The median missed head votes doubles going from 0 to 6 blobs.</strong></p>
<p><em><strong>Let’s get more granular…</strong></em></p>
<p>The following visualizes the block size incl. blobs in MiB over the failed head votes per slot.</p>
<blockquote>
<p>This chart shows only wrong/failed head votes and excludes offline validators.<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/f/4ffabf4ebdf70341da81e69e95094040f0f6591e.png" title="failed_missed_head_size_scatter"><img alt="failed_missed_head_size_scatter" height="316" src="https://ethresear.ch/uploads/default/optimized/3X/4/f/4ffabf4ebdf70341da81e69e95094040f0f6591e_2_690x316.png" width="690" /></a></div><p></p>
</blockquote>
<p><strong>For the sizes above 0.8 MiB, which are most likely blocks with 6 blobs, we can see more weak blocks than for 0 blob blocks. “Weak” because up to 32k attesters of that slot, up to 99%, voted for a different block.</strong><br />
The only way that block still made it into the canonical chain is the next validator building on top of it instead of reorging that block out.</p>
<p>In the analyzed month, we observe 401 blocks with &gt;31k attesters voting for different blocks that still made it into the canonical chain. 233 of them carried 6 blobs. Assuming most validators attest at the latest at second 4 of a slot, those blocks must have been propagated very late such that validators already attested to a different block before seeing it.<br />
This can be confirmed by plotting the “first seen” time of those weak blocks over the seconds in a slot, comparing it to all other blocks:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/3/b3e8a77964031d2b79681810e8f527c7c95a1699.png" title="hist_late_performer"><img alt="hist_late_performer" height="304" src="https://ethresear.ch/uploads/default/optimized/3X/b/3/b3e8a77964031d2b79681810e8f527c7c95a1699_2_690x304.png" width="690" /></a></div><p></p>
<p>The chart shows that most blocks are seen between second 1 and 2 in the slot. For those weak blocks, it’s between second 4 and 5, right after the attestation deadline.</p>
<p>We can confirm this by looking at the attestation timing over the seconds in a slot. We can see that 80% of the attestations are seen 5 seconds into the slot. A block propagated at second 4 in the slot will likely miss out on at least ~40% of all possible attestations, no matter how fast it propagates through the network.<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/2/42fe46361f7b2a22bd61c0195f719a57df04d64d.png" title="attestations_cdf"><img alt="attestations_cdf" height="304" src="https://ethresear.ch/uploads/default/optimized/3X/4/2/42fe46361f7b2a22bd61c0195f719a57df04d64d_2_690x304.png" width="690" /></a></div><p></p>
<p><em><strong>Are blobs the problem?</strong></em></p>
<p>The following chart shows the first seen time of 1-blob blocks vs 6-blob blocks:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/d/9d9090203cef36a8e47a8b59e7a8e3b54be815ae.png" title="hist_late_performer_blobs"><img alt="hist_late_performer_blobs" height="304" src="https://ethresear.ch/uploads/default/optimized/3X/9/d/9d9090203cef36a8e47a8b59e7a8e3b54be815ae_2_690x304.png" width="690" /></a></div><p></p>
<p>We can see that despite 6-blobs blocks being seen later in the slot, the delta is rather small, not to say negligible. At the time of the block arriving, the blobs should have already been seen.</p>
<p>In the past, the fact that a user was (not) using <strong><a href="https://github.com/flashbots/mev-boost">MEV-Boost</a></strong> impacted different performance metrics. Thus, let’s plot MEV-Boost users vs. local builders for completeness:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/9/c9c92b7f095017be8e86eac8b4a486a6c1dfdaec.png" title="hist_late_performer_mevboost"><img alt="hist_late_performer_mevboost" height="304" src="https://ethresear.ch/uploads/default/optimized/3X/c/9/c9c92b7f095017be8e86eac8b4a486a6c1dfdaec_2_690x304.png" width="690" /></a></div><p></p>
<p>Finally, comparing three of the largest relays, we get the following image:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/6/56eedd8200ca5e5c778261aee74f9405630c1677.png" title="hist_mevboost_relays"><img alt="hist_mevboost_relays" height="304" src="https://ethresear.ch/uploads/default/optimized/3X/5/6/56eedd8200ca5e5c778261aee74f9405630c1677_2_690x304.png" width="690" /></a></div><p></p>
<p>While most relays such as Ultra Sound, BloXroute, Agnostic Gnosis, or Flashbots show a very similar curve, we can see the Titan relay having two peaks instead of just one.<br />
This means that some blocks going through the Titan relay are first seen in the p2p network between 2.5-3 seconds into the slot, which is very late.</p>
<p>Notable, those late blocks of Titan still became canonical, pointing towards proposer timing games.</p>
<h2><a class="anchor" href="https://ethresear.ch#attestations-and-proposer-timing-games-15" name="attestations-and-proposer-timing-games-15"></a>Attestations and Proposer Timing Games</h2>
<p>Next, let’s look at the impact of Proposer Timing Games on attestations.<br />
We refer to Proposer Timing Games (see <a href="https://eprint.iacr.org/2023/760">[1]</a>, <a href="https://arxiv.org/abs/2305.09032">[2]</a>) if block proposers delay their block proposal to give the builders more time for MEV extraction.<br />
Instead of asking the relay for a block in second 0 in the block, a proposer can delay this, e.g. until second 2 in the slot, and maximize profits. This comes with the risk of not getting enough attestations and being reorged out.</p>
<p><em>Find some real-time visuals on timing games at <a href="https://timing.pics/">timing.pics</a>.</em></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/5/c563eb0043362562f84cb3fb2f823a14c92dce14.png" title="timing_games2"><img alt="timing_games2" height="499" src="https://ethresear.ch/uploads/default/optimized/3X/c/5/c563eb0043362562f84cb3fb2f823a14c92dce14_2_373x499.png" width="373" /></a></div><p></p>
<p><strong>Proposer timing games are expected to negatively impact validators’ attestation performance</strong>, although this hasn’t been thoroughly analyzed yet. The concern is that <strong>proposer timing games could have cascading effects</strong>: attesters might slightly <strong>delay their attestations</strong> to ensure they vote for the correct head of the chain. Knowing proposers are playing timing games, it might be rational to delay the attestation too. <strong>Such strategies can be harmful to the network’s overall health.</strong></p>
<blockquote>
<p>For more info on the impact of proposer timing games on attestations, check out <a href="https://ethresear.ch/t/timing-games-implications-and-possible-mitigations/17612">Caspar’s post</a> on it.</p>
</blockquote>
<p>The following graph shows the average number of missed head votes over the seconds in a slot. The <a href="https://github.com/flashbots/relay-specs">relays’ Data API</a> (<em>bidsReceived</em> endpoint) was used for the in-slot timestamps.</p>
<blockquote>
<p>Multiple prior analyses showed that using the bidsReceived timestamps provides a <em>good enough</em> approximation of actual propagation timings. Notably, bidReceived <strong>must come earlier</strong> than the block’s propagation timing.</p>
</blockquote>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/6/763882ecc1e817832856f802783d3aa9643ce8ca.png" title="failed_missed_head_votes_over_timing"><img alt="failed_missed_head_votes_over_timing" height="316" src="https://ethresear.ch/uploads/default/optimized/3X/7/6/763882ecc1e817832856f802783d3aa9643ce8ca_2_690x316.png" width="690" /></a></div><p></p>
<p>The above chart shows that the number of missed head votes increases rapidly with being 1 - 1.2 seconds into the slot. The longer a proposer waits the fewer attestations its block is expected to receive.</p>
<p><strong>We can see that the number of missed head votes per slot increases to an average of &gt;4k (12.5% of the committee) for late blocks published more than 1.7 seconds into the slot.</strong><br />
This sounds bad although the numbers are still relatively low compared to the 32k validators that attest in each slot.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/4/540d12e9110a2a48ced72b152688d78d5ab1cc8a.png" title="failed_missed_head_votes_over_timing_per"><img alt="failed_missed_head_votes_over_timing_per" height="316" src="https://ethresear.ch/uploads/default/optimized/3X/5/4/540d12e9110a2a48ced72b152688d78d5ab1cc8a_2_690x316.png" width="690" /></a></div><p></p>
<p>Proposing a block with a <em>bid received</em> timestamp of over 2 seconds causes an average of 5k attestations to be missed. This represents about 15% of the committee.</p>
<h2><a class="anchor" href="https://ethresear.ch#next-steps-16" name="next-steps-16"></a>Next Steps</h2>
<p>In this final section I want to quickly present the idea behing anti-correlation penalties and <a href="https://eips.ethereum.org/EIPS/eip-7716">EIP-7716</a> in specific.</p>
<p>Operating multiple validators comes with <strong>economies of scale</strong>. Theoretically, one can run thousands of validators from a single node, and validators running on the same node have the same view of the chain and cast the same votes.</p>
<p><img alt="same_node_validators" height="257" src="https://ethresear.ch/uploads/default/original/3X/0/2/02262692e73fd82f5ee6f0ff9a62fc4619db6746.png" width="455" /></p>
<p>Having <strong>multiple validators</strong> on <strong>one node</strong> isn’t the only reason for <strong>correlated failures</strong>: Using the same cloud/ISP provider, the same hardware, running nodes from the same geo-location or having the node be maintained by the same group of individuals; everything that leverages economies of scale increases the correlation among validators. Beautiful.</p>
<p>We should therefore use that knowledge to design economic incentives in a way that makes it harder/less profitable to leverage economies of scale.<br />
Without requiring the protocol to know about the node operators behind validators, correlations present a great way to distinguish small solo stakers from professional operators.<br />
One concrete proposal for improved diversification within the validator set is <a href="https://eips.ethereum.org/EIPS/eip-7716">EIP-7716 - Anti-Correlation Attestation Penalties</a>.</p>
<h3><a class="anchor" href="https://ethresear.ch#eip-7716-anti-correlation-attestation-penalties-17" name="eip-7716-anti-correlation-attestation-penalties-17"></a>EIP-7716 - Anti-Correlation Attestation Penalties</h3>
<p><img alt="7716_correlation" height="317" src="https://ethresear.ch/uploads/default/original/3X/0/5/05134246699e0567331369387e90bc395b260135.png" width="459" /></p>
<p><a href="https://eips.ethereum.org/EIPS/eip-7716">EIP-7716</a> was first described by Vitalik in an <a href="https://ethresear.ch/t/a-concrete-proposal-for-correlated-attester-penalties/19341">ethresearch post</a>. After some initial analysis and a more concrete proposal available the EIP is now at the point where everyone is invited to look into the inner workings of correlated penalties and leave feedback.</p>
<p>In short, the EIP proposes the following:<br />
Multiply the missed source and missed target votes by a penalty factor that ranges from 0 to 4 but remains at 1 on average.</p>
<ul>
<li>If the balance of non-attesting validators increases, the penalty factor does so too.</li>
<li>If the balance of non-attesting validators remains the same, the penalty factor approaches 1.</li>
<li>If the balance of non-attesting validators decreases, the penalty factor can go below 1 for some time and approaches 1 afterward.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#how-does-7716-work-18" name="how-does-7716-work-18"></a>How does 7716 work?</h4>
<div class="md-table">
<table>
<thead>
<tr>
<th>Abbreviation</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="math">p</span></td>
<td>Penalty factor</td>
</tr>
<tr>
<td><span class="math">p_{exc}</span></td>
<td>Net excess penalties</td>
</tr>
<tr>
<td><span class="math">balance_{NA}</span></td>
<td>Non attesting balance</td>
</tr>
<tr>
<td><span class="math">p_{adj}</span></td>
<td>Penalty adjustment factor</td>
</tr>
<tr>
<td><span class="math">balance_{TOTAL}</span></td>
<td>Total active balance</td>
</tr>
<tr>
<td><span class="math">p_{max}</span></td>
<td>Maximum penalty factor</td>
</tr>
</tbody>
</table>
</div><p>The penalty factor scales the slot penalties to a maximum of <span class="math">p_{max}</span>, or down. It’s determined the following:</p>
<p><span class="math">p = \min\left(\frac{balance_{NA} \times p_{adj}}{p_{exc} \times balance_{TOTAL} + 1}, p_{max} \right)</span></p>
<p>The <span class="math">nep</span> is updated at the end of each slot using:</p>
<p><span class="math">p_{exc} = \max(1, p_{exc} + p) - 1</span></p>
<p><strong>The formula calculates the penalty factor as a ratio of the “penalty weight” of non-attesting validators to the total scaled balance of all validators. A higher non-attesting balance or penalty adjustment factor increases the penalty factor. Conversely, a higher net excess penalty or total active balance reduces the penalty factor.</strong></p>
<p>When  <span class="math">balance_{NA}</span> continuously increases for several rounds, also the penalty factor, as well as the net excess penalty increases. This continues until  <span class="math">balance_{N}</span> stops decreasing. Then, the net excess penalty starts decreasing together with the penalty factor.</p>
<p>With the net excess penalties, <span class="math">p_{exc}</span>, keeping track of the excess penalties of past epochs, the formula can self-regulate what is a “large” number of misses, and what is not.</p>
<p>This mechanism ensures that the sum of penalties doesn’t change with this EIP - only the distribution does.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/deep-diving-attestations-a-quantitative-analysis/20020">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 09 Jul 2024 08:02:31 +0000</pubDate>
</item>
<item>
<title>Mantis: Driving Ethereum’s Cross-Domain Future</title>
<link>https://ethresear.ch/t/mantis-driving-ethereum-s-cross-domain-future/20009</link>
<guid>https://ethresear.ch/t/mantis-driving-ethereum-s-cross-domain-future/20009</guid>
<content:encoded><![CDATA[
<div> 关键词：Mantis、Inter-Blockchain Communication (IBC)、Ethereum、Trust-minimized Bridging、Multi-chain Agnostic Trust-minimized Intent Settlement

总结:
Mantis是一个新兴的协议，旨在优化跨链操作并解决DeFi空间中的挑战。它通过与IBC（Inter-Blockchain Communication）协议和Picasso Network的整合，实现了信任最小化的跨链连接，首次将这种技术扩展到以太坊之外的IBC兼容链，如Cosmos Hub、Polkadot和Solana等。Mantis的架构包括竞赛式的解决方案、拍卖机制和与Ethereum的集成，提供了链间资产交换、意图执行和结算的一站式服务。通过Mantis，Ethereum得以加强其在去中心化金融中的核心地位，并促进新用户和流动性流入。此外，Mantis简化了跨链交易体验，为去中心化应用程序开发者提供工具。未来，Mantis计划进一步开发拍卖系统和可信承诺机制，以提升整体系统的福利。 <div>
<p>Author: <a href="https://x.com/0xbrainjar" rel="noopener nofollow ugc">0xbrainjar</a></p>
<p>Reviewers: <a href="https://x.com/ComposableSyd" rel="noopener nofollow ugc">Sydney Sweck</a> &amp; <a href="https://x.com/0xBrMazoRoig" rel="noopener nofollow ugc">Bruno Mazorra</a></p>
<h1><a class="anchor" href="https://ethresear.ch#summary-1" name="summary-1"></a>Summary</h1>
<p>Recently, Composable <a href="https://twitter.com/Picasso_Network/status/1775512007963500772" rel="noopener nofollow ugc">launched its IBC Ethereum mainnet connection</a>. The <a href="https://www.ibcprotocol.dev/" rel="noopener nofollow ugc">IBC Protocol</a> is emerging as the gold standard for cross-chain communication, as we have previously explored in our comparison analysis <a href="https://medium.com/@Picasso_Network/ibc-as-the-end-game-of-bridging-a-comparison-analysis-on-trust-dcc01e0d9377" rel="noopener nofollow ugc">here</a>. IBC’s trust levels parallel that of ZK bridging, which is limited to the Ethereum ecosystem and its layer 2s. Originally, the IBC Protocol was also limited to one ecosystem: the Interchain, which includes Cosmos SDK chains and the Cosmos Hub. However, IBC has now been expanded outside of the Interchain/Cosmos ecosystem for the first time by Composable’s Picasso Network.</p>
<p>IBC Ethereum a significant milestone, marking the first time that trust-minimized bridging is available between Ethereum and other IBC-enabled chains including the Cosmos hub, Cosmos SDK chains, Polkadot and Kusama parachains, Solana, and more ecosystems soon. Moreover, this was a huge technological feat, given that this connection required architecting a light client on Ethereum. While various projects were exploring the concept of Ethereum light clients at the time, there were no light clients fully available on Ethereum when we began development.</p>
<p>Now, Composable is in the process of launching a product that aims to bring more utility to cross-domain Ethereum operations: Multi-chain Agnostic Trust-minimized Intent Settlement, or Mantis. This framework serves as a vertically integrated intent pipeline, complete with expression, execution, and settlement. Ultimately, Mantis strives to establish a decentralized market for cross-domain intent expression through a permissionless solver network and intent-settlement framework. Through Ethereum IBC and now Mantis, Ethereum will be optimally positioned to continue in its role as the leading hub of DeFi; new cross-chain use cases to and from Ethereum will be generated, enabling the flow of new liquidity and users to Ethereum, with all of the complexities abstracted away to improve the user experience.</p>
<p>The present article thus summarizes Mantis from our recently-published Mantis Whitepaper and Litepaper. Moreover, this post details how Mantis can benefit Ethereum and other IBC-enabled ecosystems.</p>
<h1><a class="anchor" href="https://ethresear.ch#about-mantis-2" name="about-mantis-2"></a>About Mantis</h1>
<h2><a class="anchor" href="https://ethresear.ch#the-industry-need-3" name="the-industry-need-3"></a>The Industry Need</h2>
<p>Mantis is a relevant protocol within the present DeFi space for a number of reasons, as it aims to fulfill a number of challenges currently facing the space:</p>
<ul>
<li><strong>Optimizing UX and Execution:</strong> There has always been a need in the space to optimize both user experience (UX) and execution. If this is accomplished, capital efficiency and value accrual can be maximized for all participants.</li>
<li><strong>Combatting Centralization Trends:</strong> In the multi-chain bridging space, there has been an increased reliance upon centralized structures. Unfortunately, there has been a lack of decentralized solutions that rival the speed and cost of centralized structures.</li>
<li><strong>Facilitating Trust-Minimized Interoperability:</strong> Many bridging structures in place today require putting trust in third-party intermediaries, making them vulnerable to attack. However, new technologies are being introduced with the launch of trust-minimized bridging structures like the <a href="https://www.ibcprotocol.dev/" rel="noopener nofollow ugc">IBC Protocol</a>, which powers the <a href="http://picasso.xyz" rel="noopener nofollow ugc">Picasso Network</a>. These developments enable generalized message passing and synchronization of protocols and applications across multiple blockchain ecosystems.</li>
<li><strong>Delivering Intent-Centricity:</strong> Intents are another new area of development in the DeFi space that are positioned to further assist in resolving user experience and execution issues. However, many intents solutions are not cross-chain interoperable, and are not vertically integrated with execution and settlement solutions, rendering them unable to accrue value from pay for orderflow.</li>
</ul>
<p>With Mantis, Composable addresses these present unmet needs in the DeFi space. Overall, our thesis is that cross-domain interoperability widens the intent solution space. We hypothesize that this increased choice in solutions results in value in the form of better user outcomes.</p>
<h2><a class="anchor" href="https://ethresear.ch#architecture-4" name="architecture-4"></a>Architecture</h2>
<p>Mantis accomplishes its functionalities via the Mantis protocol and rollup, a cross-domain auction mechanism, as well as their synergies with the Inter-Blockchain Communication (IBC) Protocol and the Picasso Network. Moreover, a commitment mechanism between chains allows conditions in the other parts of the architecture to be carried out cross-domain.</p>
<h3><a class="anchor" href="https://ethresear.ch#the-mantis-protocol-rollup-5" name="the-mantis-protocol-rollup-5"></a>The Mantis Protocol &amp; Rollup</h3>
<p>The Mantis protocol facilitates optimal execution of cross-domain intents via a competition of solvers. Users sign intents, which are contained on a private rollup mempool. Solvers are staked agents that can a) observe the transactions on the mempool and b) post solutions in the auctioneer contract. The auctioneer contract scores the solutions in terms of users utility maximization. The winner of the auction is responsible for settling the outcome of the intent to the solution settlement contracts in the final chain expressed by the intent.</p>
<p>The Mantis protocol lives on the Mantis Solana Virtual Machine (SVM) rollup. This rollup serves as a coordination and settlement layer for cross-domain intents, in addition providing a framework for cross-domain block proposals and credible commitments. The rollup further allows for assets to be staked and restaked to provide crypto-economic security along the proof-of-stake model. This includes staking both the native token of Solana (SOL) as well as liquid staked token versions of SOL. These assets are staked into the bridge contract of the rollup, which then sends them to the proper place for staking or restaking.</p>
<p>The Mantis rollup also provides developers with a simplified mechanism for designing cross-domain decentralized applications (cdApps), which are defined by their inclusion of scoring, solvers, solution settlement, and cross-domain integrity proofs. An SDK is provided to further enhance the development and integration process.</p>
<h3><a class="anchor" href="https://ethresear.ch#cross-domain-auctions-6" name="cross-domain-auctions-6"></a>Cross-Domain Auctions</h3>
<p>Mantis plans to introduce cross-domain combinatorial auctions, with the goal of accomplishing the following:</p>
<ul>
<li>Optimized cross-domain MEV extraction*</li>
<li>Cross-domain intent solution atomic settlement</li>
<li>Efficient blockspace allocation</li>
<li>Increased distribution of revenue to validators selling items separately</li>
</ul>
<p>*We would like to take a moment here to reflect on MEV and our goals surrounding this concept. MEV is an evolving term with a number of interpretations. Initially MEV stood for miner extractable value, representing the maximum profit an agent (miner or validator) in proof-of-work blockchain systems could incur from its monopolistic rights over transaction inclusion. With the advent of proof-of-stake systems, MEV has become more often described as maximal extractible value, as miners are largely obsolete. Maximal extractible value still refers to the value that agents derive from strategically reordering and including transactions, but now these agents are frequently searchers.</p>
<p>A number of negative ramifications have been reported from these MEV extraction mechanisms. Thus, Flashbots introduced MEV-geth to Ethereum, which implemented a centralized combinatorial auction where searchers can express complex preferences in bundles. Then, this auction system was decentralized by MEV-Boost, allowing anyone to propose their block by bidding at auction. With the introduction of proposer-builder separation, validators on Ethereum now derive value from their monopolistic power over their slots.</p>
<p>As one can see, value from rearranging and including/excluding transactions can now be carried out by a number of parties in a number of manners. In addition to the extraction by validators, miners, and searchers, builders can also derive profits and users themselves can derive financial benefits from these mechanisms by using protocols such as Flashbots Protect, MEV blocker and Cow Protocol . Therefore, it becomes difficult to define exactly what value accrual mechanisms can be considered MEV.</p>
<p>Another complicating factor in the definition of MEV is that some of the aforementioned value accrual mechanisms have an inverse relationship. Most importantly, there is tension between the profits made by validators and other sellers from MEV and the overall welfare of the system (i.e. total value accrued to all users of the system, including end users, solvers, searchers, stakers, etc.). When overall profits to sellers are maximized, overall welfare goes down.</p>
<p>Thus, the goal of Mantis is not necessarily to maximize MEV extraction. Rather, the goal is to maximize overall welfare.One way in which we hope to achieve this is via our mechanisms designed to allocate blockspace efficiently to the users valuing it the most, such as our cross-domain auctions.</p>
<p>Initially, these auctions will be just-in-time to allow builders to express atomically. For two domains, this will involve two simultaneous English auctions with a unique combinatorial block take-it-or-leave it offer. Buyers can place send blocks with bids for the independent blocks and combinatorial blocks. The problems with this approach are the risk of double-signing and the high level of trust placed in the relay.</p>
<p>Therefore, Mantis aims to later introduce a future combinatorial blockspace market, where the rights to future blockspace on multiple domains can be bought and sold. The new crypto-economic primitive of restaking (such as that being facilitated by the Picasso Network) enables block proposers to issue credible commitments about future block construction. These are promises to build blocks in accordance with specific conditions laid out by execution ticket holders if certain payment thresholds are met. Tickets exist outside of a domain’s consensus protocol and will be exchanged via a combinatorial batch auction where buyers express combinatorial valuations over the tickets and sellers express reserve prices. Then, tickets can be traded or sold in a secondary market. This aims to decrease the monopoly of block sellers while increasing market efficiency.</p>
<h3><a class="anchor" href="https://ethresear.ch#the-ibc-protocol-7" name="the-ibc-protocol-7"></a>The IBC Protocol</h3>
<p>The <a href="https://www.ibcprotocol.dev/" rel="noopener nofollow ugc">IBC Protocol</a> facilitates communication between different blockchain ecosystems. <a href="https://medium.com/picasso-network/why-ibc-everywhere-is-the-key-to-cross-chain-defi-041bed829acd" rel="noopener nofollow ugc">Compared to other cross-chain communication protocols</a>, the benefits of IBC are that it is <a href="https://medium.com/@Picasso_Network/ibc-as-the-end-game-of-bridging-a-comparison-analysis-on-trust-dcc01e0d9377" rel="noopener nofollow ugc">trustless</a>, secure, censorship-resistant, permissionless, fast, cost-effective, and natively interoperable. For these reasons, Mantis has opted to use IBC as its mechanism for cross-chain communication.</p>
<p>Composable has expanded the reach of the IBC so that it not only connects the <a href="https://hub.cosmos.network/" rel="noopener nofollow ugc">Cosmos Hub</a> and <a href="https://v1.cosmos.network/sdk" rel="noopener nofollow ugc">Cosmos SDK</a> chains that it originally linked, but also interoperates with <a href="https://polkadot.network/" rel="noopener nofollow ugc">Polkadot</a> and <a href="https://kusama.network/" rel="noopener nofollow ugc">Kusama</a> parachains, <a href="https://ethereum.org/en/" rel="noopener nofollow ugc">Ethereum</a>, and <a href="https://solana.com/" rel="noopener nofollow ugc">Solana</a>. Creating these novel connections required a significant amount of technical development, given that many blockchains lack different components needed for IBC-compatibility.</p>
<p>In the case of Ethereum, the following components needed to be architected in order to enable IBC-compatibility:</p>
<ul>
<li><strong>ZK Circuit:</strong> This program is able to output a proof given a set of inputs. This proof can then be easily verified to ensure that each computational step that was run inside the circuit was done so correctly. In Picasso’s solution, the ZK circuit connects SNARK ED-25519 signatures to a prover. ED-25519 is a digital signature algorithm (DSA) that offers small key and signature sizes and fast computation being impervious to many common attacks to other DSAs.</li>
<li><strong>Tendermint Light Client on Ethereum:</strong> We constructed a <a href="https://docs.tendermint.com/v0.34/tendermint-core/light-client.html" rel="noopener nofollow ugc">Tendermint light client</a> on Ethereum, which lives as an Ethereum smart contract and is able to communicate over IBC with the light client on Picasso.</li>
<li><strong>Ethereum Light Client on the Picasso Chain:</strong> We also created a CosmWasm contract in the Wasm client of the Picasso Cosmos SDK chain to complete the Ethereum IBC connection.</li>
<li><strong>IBC Stack on Ethereum:</strong> We created a modified IBC stack for Ethereum that consists of Solidity smart contracts on Ethereum. Through this IBC stack, all BC components can operate on Ethereum, facilitating Ethereum’s interoperability with IBC.</li>
<li><strong>Hyperspace Relayer:</strong> The Composable Foundation’s <a href="https://informal.systems/blog/comparing-hyperspace-hermes" rel="noopener nofollow ugc">Hyperspace relayer</a> connects the two light clients involved in Ethereum IBC by transferring IBC packets between them. Hyperspace is the first event-driven, chain-agnostic IBC relayer that is based on ibc-rs (the Rust implementation of IBC). Hyperspace can thus relay packets between any IBC-enabled chains.</li>
<li><strong>Prover:</strong> This entity interacts with the relayer and proves to the verifier that something is true without revealing other information. On Picasso, what is being proved is various transactions sent between Ethereum and IBC. In particular, this prover is a rapid SNARK prover living on the Picasso Cosmos SDK chain.</li>
<li><strong>Verifier:</strong> Verifiers receive a proof from provers and validate this claim. This prover-verifier relationship results in the production of zero-knowledge proofs, as Ethereum explains <a href="https://ethereum.org/en/zero-knowledge-proofs/" rel="noopener nofollow ugc">here</a>.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#the-picasso-network-its-restaking-pool-8" name="the-picasso-network-its-restaking-pool-8"></a>The Picasso Network &amp; Its Restaking Pool</h3>
<p>The <a href="https://picasso.xyz/" rel="noopener nofollow ugc">Picasso Network</a> aims to deliver ecosystem-agnosticism to DeFi. It executes on this vision via the Picasso Layer 1, a Cosmos SDK blockchain that acts as an IBC hub between Cosmos and non-Cosmos IBC-enabled chains.</p>
<p>Picasso is the first censorship-resistant, natively-secured cross-ecosystem interoperability solution. The Picasso Network further emphasizes trust-minimization by drawing on the trustless IBC protocol. While a multisig is initially being used for upgradability of IBC contracts on Picasso, the end goal is to transition to decentralized governance.</p>
<p>Picasso is a critical component of Mantis as it allows the Mantis framework to be cross-chain capable over IBC. Specifically, Mantis transactions are grouped into IBC bundles for shipment based on domain. These bundles are then sent from the Mantis rollup over Picasso IBC and out to relevant blockchains for settlement.</p>
<p>Moreover, a restaking pool on Picasso coordinates the agents that have a combination of stake in different chains. Commitments formed between these actors draw upon this restaking pool.</p>
<h2><a class="anchor" href="https://ethresear.ch#development-roadmap-9" name="development-roadmap-9"></a>Development Roadmap</h2>
<p>The development for Mantis will be carried out in the following steps:</p>
<ol>
<li>
<p><strong>Enabling cross-domain swaps:</strong> integrating with IBC bridges and automated market makers across different chains to facilitate seamless asset swaps</p>
</li>
<li>
<p><strong>Setting up the foundational architecture:</strong> establishing a robust framework that includes the initial design of the Mantis architecture and the development of standards for scoring mechanisms and IBC for intent-based mechanisms</p>
</li>
<li>
<p><strong>Implementing cross-domain intent-based mechanisms:</strong> developing application programming interfaces (APIs) and software development kits (SDKs) that enable users to create and manage cross-domain intents, along with implementing an open-source solver that solves these intents</p>
</li>
<li>
<p><strong>Enriching the restaking layer:</strong> building out the restaking layer of Mantis to have additional functionality (simultaneously to step 3)</p>
</li>
<li>
<p><strong>Creating cross-domain MEV auctions:</strong> developing an auction system that efficiently allocates blockspace (simultaneously to step 3)</p>
</li>
<li>
<p><strong>Deploying block proposal commitments:</strong> enhancing the infrastructure for block proposals and establishing a credible commitment mechanism across domains, including robust fraud-proof mechanisms to maintain trust and security.</p>
</li>
<li>
<p><strong>Completing public launch and scaling:</strong> focusing on officially releasing all functionalities and documentation for Mantis</p>
</li>
</ol>
<h1><a class="anchor" href="https://ethresear.ch#benefits-to-ethereum-10" name="benefits-to-ethereum-10"></a>Benefits to Ethereum</h1>
<p>Mantis supports Ethereum’s continued role as a leader in DeFi as the space becomes increasingly cross-chain. Composable has already connected Ethereum to the IBC, and therefore, to our trust-minimized bridge. This connection will drive new usership and liquidity to Ethereum from Solana, Cosmos, Polkadot, and Kusama. It will also enable the development of new use cases for ETH outside of Ethereum and on these other networks. Through such new use cases in new locations, DeFi users who do not currently hold ETH will likely be incentivized to do so, and existing users may be incentivized to hold more ETH. Thus, the Ethereum network is positioned to expand its reach even further into the cross-domain DeFi landscape, helping the ecosystem to maintain its reputation as a leader in the space.</p>
<p>Another benefit Mantis aims to deliver is chain abstraction. Mantis provides a mechanism for Ethereum and other domains to easily be participants in cross-chain DeFi without the blockchain, its layer 2s, or any protocols in the ecosystem needing to make significant modifications. Now that Ethereum is integrated with IBC, its innumerable DeFi protocols and applications can be leveraged from within Mantis. A user simply puts their intent for a transaction into the Mantis user interface, and the rest is handled for them. For example, A user may be looking to swap ETH for USDC. Once they input this intent, Solvers on Mantis compete to come up with the best execution route. For the sake of this example, perhaps the best price for this swap is through an ETH-USDC pool on Uniswap. The solver who has proposed the best settlement route wins the rights to settle the solution, routing the funds through Uniswap for the swap, and then back to the user. Once the transaction is settled as specified, the solver is rewarded. In this manner, all parties benefit: new traffic is routed through Uniswap in this example (or more generally, any other protocol or protocols providing best execution), the user has a streamlined experience with optimized settlement, and the solver is rewarded for their role.</p>
<h1><a class="anchor" href="https://ethresear.ch#conclusion-11" name="conclusion-11"></a>Conclusion</h1>
<p>Mantis provides the architecture needed for IBC-enabled chains like Ethereum to easily participate in the cross-domain future. This will help Ethereum continue its role at the forefront of DeFi as the industry continues to embrace multi-domain operations.</p>
<h1><a class="anchor" href="https://ethresear.ch#references-more-about-composable-12" name="references-more-about-composable-12"></a>References &amp; More About Composable</h1>
<p>Composable is dedicated to improving DeFi’s accessibility, quality, transparency, efficiency, and security. Our ultimate vision is for the Composable ecosystem to become an execution hub for chain-agnostic transactions. We are actualizing our mission by working to unite the DeFi space, building an ecosystem and a range of infrastructure to support trustless cross-chain operations.</p>
<ul>
<li><a href="https://assets.website-files.com/65b28e756a8eda2e91e76ca4/6656289f21123d6215091555_MANTIS%20Whitepaper.pdf" rel="noopener nofollow ugc">Mantis Whitepaper</a></li>
<li><a href="https://assets.website-files.com/65b28e756a8eda2e91e76ca4/6655e8e69277b97e9c11c793_MANTIS%20Litepaper.pdf" rel="noopener nofollow ugc">Mantis Litepaper</a></li>
<li><a href="https://www.mantis.app/" rel="noopener nofollow ugc">Mantis app</a></li>
<li><a href="https://www.composable.finance/" rel="noopener nofollow ugc">Composable website</a></li>
<li><a href="https://twitter.com/ComposableFin" rel="noopener nofollow ugc">Composable X/twitter</a></li>
<li><a href="http://discord.gg/composable" rel="noopener nofollow ugc">Composable Discord</a></li>
<li><a href="https://t.me/composable_chat" rel="noopener nofollow ugc">Composable Telegram</a></li>
<li><a href="https://github.com/ComposableFi/" rel="noopener nofollow ugc">Composable GitHub</a></li>
</ul>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/mantis-driving-ethereum-s-cross-domain-future/20009">Read full topic</a></p>
]]></content:encoded>
<pubDate>Mon, 08 Jul 2024 13:16:05 +0000</pubDate>
</item>
<item>
<title>Maximum Viable Security: A New Framing for Ethereum Issuance</title>
<link>https://ethresear.ch/t/maximum-viable-security-a-new-framing-for-ethereum-issuance/19992</link>
<guid>https://ethresear.ch/t/maximum-viable-security-a-new-framing-for-ethereum-issuance/19992</guid>
<content:encoded><![CDATA[
<h1><a class="anchor" href="https://ethresear.ch#maximum-viable-security-a-new-framing-for-ethereum-issuance-1" name="maximum-viable-security-a-new-framing-for-ethereum-issuance-1"></a><strong>Maximum Viable Security: A New Framing for Ethereum Issuance</strong></h1>
<p><em>by <a href="http://x.com/artofkot" rel="noopener nofollow ugc">@artofkot</a>, <a href="http://x.com/damcnuta" rel="noopener nofollow ugc">@damcnuta</a>, <a href="http://x.com/sonyasunkim" rel="noopener nofollow ugc">@sonyasunkim</a>, <a href="http://x.com/adcv_" rel="noopener nofollow ugc">@adcv_</a></em></p>
<p><em>Appreciate feedback from <a href="http://x.com/ppclunghe" rel="noopener nofollow ugc">@ppclunghe</a>, <a href="https://x.com/ks_kulk" rel="noopener nofollow ugc">@ks_kulk</a>, <a href="http://x.com/lazyleger" rel="noopener nofollow ugc">@lazyleger</a>, <a href="https://cryptecon.org/team-detail-ce/items/juan-beccuti.html" rel="noopener nofollow ugc">Juan Beccuti</a>, <a href="https://x.com/entigdd" rel="noopener nofollow ugc">enti</a>, <a href="https://x.com/stakesaurus" rel="noopener nofollow ugc">Stakesaurus</a>, <a href="http://x.com/hasufl" rel="noopener nofollow ugc">@hasufl</a>, <a href="http://x.com/lex_node" rel="noopener nofollow ugc">@lex_node</a>, <a href="https://x.com/_vshapovalov" rel="noopener nofollow ugc">@_vshapolapov</a>, <a href="http://x.com/brettpalatiello" rel="noopener nofollow ugc">@brettpalatiello</a></em></p>
<hr />
<p><strong>Table of Contents</strong></p>
<ul>
<li><a href="https://ethresear.ch/t/maximum-viable-security-a-new-framing-for-ethereum-issuance/19992#tldr-embrace-security-2">TLDR: Embrace security</a></li>
<li><a href="https://ethresear.ch/t/maximum-viable-security-a-new-framing-for-ethereum-issuance/19992#h-1-the-foundations-of-the-maximum-viable-security-mvs-framework-3">1. The foundations of the Maximum Viable Security (“MVS”) framework</a>
<ul>
<li><a href="https://ethresear.ch/t/maximum-viable-security-a-new-framing-for-ethereum-issuance/19992#h-11-ethereum-has-a-clear-goal-build-a-secure-and-sovereign-distributed-system-for-everyone-4">1.1. Ethereum has a clear goal: build a secure and sovereign distributed system for everyone</a></li>
<li><a href="https://ethresear.ch/t/maximum-viable-security-a-new-framing-for-ethereum-issuance/19992#h-12-a-diverse-staking-economy-is-key-5">1.2. A diverse staking economy is key</a>
<ul>
<li><a href="https://ethresear.ch/t/maximum-viable-security-a-new-framing-for-ethereum-issuance/19992#h-121-stakers-6">1.2.1. Stakers</a></li>
<li><a href="https://ethresear.ch/t/maximum-viable-security-a-new-framing-for-ethereum-issuance/19992#h-122-validating-entities-7">1.2.2. Validating entities</a></li>
<li><a href="https://ethresear.ch/t/maximum-viable-security-a-new-framing-for-ethereum-issuance/19992#h-123-entities-decentralization-8">1.2.3. Entities’ decentralization</a></li>
</ul>
</li>
<li><a href="https://ethresear.ch/t/maximum-viable-security-a-new-framing-for-ethereum-issuance/19992#h-13-there-is-no-future-proof-safe-level-of-security-9">1.3. There is no future-proof safe level of Security</a></li>
<li><a href="https://ethresear.ch/t/maximum-viable-security-a-new-framing-for-ethereum-issuance/19992#h-14-reframing-the-discourse-expansion-over-efficiency-10">1.4. Reframing the discourse: expansion over efficiency</a></li>
</ul>
</li>
<li><a href="https://ethresear.ch/t/maximum-viable-security-a-new-framing-for-ethereum-issuance/19992#h-2-analysis-of-ethereum-issuance-reduction-proposal-within-the-mvs-framework-11">2. Analysis of Ethereum Issuance reduction proposal within the MVS framework</a>
<ul>
<li><a href="https://ethresear.ch/t/maximum-viable-security-a-new-framing-for-ethereum-issuance/19992#h-21-the-assumption-that-ethereum-overpays-for-security-is-wrong-less-issuance-may-lead-to-centralization-of-the-validator-set-12">2.1. The assumption that Ethereum overpays for security is wrong: less issuance may lead to centralization of the validator set</a>
<ul>
<li><a href="https://ethresear.ch/t/maximum-viable-security-a-new-framing-for-ethereum-issuance/19992#h-211-etf-inflows-would-exacerbate-centralization-in-the-context-of-a-33-stake-cap-13">2.1.1 ETF inflows would exacerbate centralization in the context of a 33% stake cap</a></li>
<li><a href="https://ethresear.ch/t/maximum-viable-security-a-new-framing-for-ethereum-issuance/19992#h-212-staked-eth-concentration-with-cexs-doesnt-necessarily-have-to-happen-with-a-higher-stake-cap-14">2.1.2 Staked ETH concentration with CEXs doesn’t necessarily have to happen with a higher stake cap</a></li>
<li><a href="https://ethresear.ch/t/maximum-viable-security-a-new-framing-for-ethereum-issuance/19992#h-213-mvi-effect-on-the-ratio-of-solo-stakers-15">2.1.3 MVI effect on the ratio of solo stakers</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="https://ethresear.ch/t/maximum-viable-security-a-new-framing-for-ethereum-issuance/19992#h-22-lst-dominance-and-cost-modeling-are-inadequate-arguments-for-issuance-reduction-19">2.2. LST dominance and cost-modeling are inadequate arguments for issuance reduction</a>
<ul>
<li><a href="https://ethresear.ch/t/maximum-viable-security-a-new-framing-for-ethereum-issuance/19992#h-221-issuance-as-a-cost-is-a-reductive-framing-20">2.2.1. Issuance as a cost is a reductive framing</a></li>
<li><a href="https://ethresear.ch/t/maximum-viable-security-a-new-framing-for-ethereum-issuance/19992#h-222-stakers-getting-higher-real-vs-nominal-yield-is-not-significant-21">2.2.2. Stakers getting higher real vs nominal yield is not significant</a></li>
<li><a href="https://ethresear.ch/t/maximum-viable-security-a-new-framing-for-ethereum-issuance/19992#h-223-reducing-lst-dominance-shouldnt-be-a-primary-objective-of-ethereums-monetary-policy-22">2.2.3. Reducing LST dominance shouldn’t be a primary objective of Ethereum’s monetary policy</a></li>
</ul>
</li>
<li><a href="https://ethresear.ch/t/maximum-viable-security-a-new-framing-for-ethereum-issuance/19992#h-3-putting-it-all-together-23">3. Putting it all together</a></li>
</ul>
<hr />
<h2><a class="anchor" href="https://ethresear.ch#tldr-embrace-security-2" name="tldr-embrace-security-2"></a>TLDR: Embrace security</h2>
<p>Given Ethereum’s goal of building a secure and sovereign distributed system, we believe viewing Ethereum’s monetary policy through the lens of Minimum Viable Issuance (MVI) is not appropriate. Instead, we propose Maximum Viable Security (MVS) as a new framework for the community to consider in the Ethereum issuance debate. That is,</p>
<p>From: <strong>Minimum Viable Issuance (MVI)</strong> – minimize issuance, without compromising security.<br />
→<br />
To: <strong>Maximum Viable Security (MVS)</strong> – maximize security, without compromising scarcity.</p>
<p>After covering the motivation and foundations behind MVS, we evaluate Ethereum issuance reduction proposals through the MVS lens. We show that issuance reduction can compromise security and neutrality in a direct way, through staked ETH concentration with Centralized Exchanges – and this effect, on balance, far outweighs the advantages of cutting the issuance.</p>
<h2><a class="anchor" href="https://ethresear.ch#h-1-the-foundations-of-the-maximum-viable-security-mvs-framework-3" name="h-1-the-foundations-of-the-maximum-viable-security-mvs-framework-3"></a>1. The foundations of the Maximum Viable Security (“MVS”) framework</h2>
<h3><a class="anchor" href="https://ethresear.ch#h-11-ethereum-has-a-clear-goal-build-a-secure-and-sovereign-distributed-system-for-everyone-4" name="h-11-ethereum-has-a-clear-goal-build-a-secure-and-sovereign-distributed-system-for-everyone-4"></a>1.1. Ethereum has a clear goal: build a secure and sovereign distributed system for everyone</h3>
<blockquote>
<p><code>There are many goals of this project; one key goal is to facilitate transactions between consenting individuals who would otherwise have no means to trust one another.</code><br />
<em>Source: Ethereum Yellow Paper (<a href="https://ethereum.github.io/yellowpaper/paper.pdf" rel="noopener nofollow ugc">link</a>)</em></p>
</blockquote>
<p>The growth of Ethereum’s market capitalization from 0 to $400bn today underscores the market’s confidence in its current and future potential. This value hinges on Ethereum’s ability to validate state changes transparently, securely, and sovereignly.</p>
<p>Security is a crucial part of the value proposition. Without sybil resistance and slashing defense (programmable or social) against 34% double-signing attacks, a settlement layer would not be trusted by participants. A secure validation layer is the most scalable (<a href="https://unenumerated.blogspot.com/2017/02/money-blockchains-and-social-scalability.html" rel="noopener nofollow ugc">link</a>) foundation for providing transaction settlement with incorruptible finality.</p>
<p>Sovereignty is equally important – Ethereum should be able to defend against more subtle 51% attacks such as short-range reorgs and censoring (<a href="https://ethereum.org/en/developers/docs/consensus-mechanisms/pos/attack-and-defense/#attackers-with-50-stake" rel="noopener nofollow ugc">link</a>), and should be able to resist coercion by state actors. If Ethereum loses sovereignty (aka autonomy), it loses its value as a neutral settlement mechanism:</p>
<blockquote>
<p><code>"Decentralization" is the broad distribution of a system's intrinsic/accepted forms of power, protecting users against arbitrary exercises of power from the recognized legitimate 'authorities' within the system's logic (e.g., validators). "Autonomy" is the system's resistance against extrinsic/unaccepted forms of power, protecting users against all exercises of power from authorities outside the system's logic (e.g., government authorities).</code><br />
<em>Source: lex_node (<a href="https://twitter.com/lex_node/status/1799489646042165662" rel="noopener nofollow ugc">link</a>)</em></p>
</blockquote>
<p>While 34% attacks are costly and 51% attacks are to some extent bounded by reputation and social slashing, a gradual coercion by state actors on independent validators is more feasible, and can even be unintentional. For instance, the European Securities and Markets Authority (ESMA) recently suggested (<a href="https://www.esma.europa.eu/press-news/consultations/consultation-technical-standards-specifying-certain-requirements-mica-3rd#responses" rel="noopener nofollow ugc">link</a>) viewing MEV as a form of market manipulation subject to notification requirements from validators. Such regulations could make it impracticable for node operators to continue to function in Europe. In a worst-case outcome, these regulations could propagate to the rest of the world and impose artificial restrictions on how the consensus algorithm works.</p>
<p>High autonomy is therefore maintained through robust decentralization among validators, which includes:</p>
<ul>
<li><strong>Client software diversity</strong>: running different types of validator software to avoid concentration risk from bugs.</li>
<li><strong>Node operator diversity</strong>: different, independent entities running validator software to prevent individual node operators reaching higher levels of control.</li>
<li><strong>Geographic and jurisdictional diversity</strong>: different levels of base-level infrastructure — such as connectivity to the internet, power supply, law authorities and jurisdictions — that are capable of influencing node operators.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#h-12-a-diverse-staking-economy-is-key-5" name="h-12-a-diverse-staking-economy-is-key-5"></a>1.2. A diverse staking economy is key</h3>
<h4><a class="anchor" href="https://ethresear.ch#h-121-stakers-6" name="h-121-stakers-6"></a>1.2.1. Stakers</h4>
<p>Stakers fall into three main categories:</p>
<ol>
<li>Retail and Institutions: These participants delegate their staking to Centralized Exchanges (CEXs)</li>
<li>On-chain Actors: They delegate their staking to Decentralized Staking Middleware (DSM), such as Liquid Staking Tokens (LSTs) or decentralized pools, as well as Liquid Restaking Token protocols (LRTs) and Centralized Staking Providers (CSPs).</li>
<li>Solo Stakers: These users choose not to delegate and run validators independently</li>
</ol>
<h4><a class="anchor" href="https://ethresear.ch#h-122-validating-entities-7" name="h-122-validating-entities-7"></a>1.2.2. Validating entities</h4>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/4/e4e64e83cbc59b58fdfe0316e37c8b548dfb52d8.jpeg" title="tg_image_4158519118"><img alt="tg_image_4158519118" height="469" src="https://ethresear.ch/uploads/default/optimized/3X/e/4/e4e64e83cbc59b58fdfe0316e37c8b548dfb52d8_2_690x469.jpeg" width="690" /></a></div><br />
<em>Note: CSP numbers do not include capital delegated from DSM/LRTs. The above numbers are approximate and for illustration purposes; they are our best estimates from Dune (<a href="https://dune.com/hildobby/eth2-staking" rel="noopener nofollow ugc">1</a>, <a href="https://dune.com/lido/eth-deposits-stats" rel="noopener nofollow ugc">2</a>), as of June 30th 2024.</em><p></p>
<p>A hypothetical scenario where most ETF Ether is staked with custodial services, like Coinbase, suggests that this is where most of future inflows will likely originate. Recent Bitcoin ETFs have seen ~$15b of inflows. Proportionally applied to Ethereum, this could mean about 4m ETH. Notably, 8 out of 11 Bitcoin ETFs use Coinbase as their custodian, a pattern that may repeat with ETH.</p>
<h4><a class="anchor" href="https://ethresear.ch#h-123-entities-decentralization-8" name="h-123-entities-decentralization-8"></a>1.2.3. Entities’ decentralization</h4>
<p>Contributions to decentralization and thus censorship resistance and neutrality can be approximated as follows: Solo Stakers &gt; Decentralized Staking Middleware &gt; Liquid Restaking Protocols &gt; Centralized Staking Providers &gt; CEXs.</p>
<ul>
<li><strong>Solo Stakers</strong>: Contribute the most to decentralization because each adds an additional validator</li>
<li><strong>DSM</strong>: Efficiently distribute delegated stake among many parties, bonded via reputation (Lido) or collateral (Rocket Pool, Lido’s Community Staking Module). Their impact on Ethereum’s decentralization is measurable and significant, with data on operational diversity publicly available and regularly updated (<a href="https://app.hex.tech/8dedcd99-17f4-49d8-944e-4857a355b90a/app/3f7d6967-3ef6-4e69-8f7b-d02d903f045b/latest" rel="noopener nofollow ugc">link</a>). The Herfindahl-Hirschmann Index (HHI) can also provide a useful proxy on the effect on validation concentration (<a href="https://dune.com/steakhouse/hhi" rel="noopener nofollow ugc">link</a>)</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/4/b4d2e88d63be2c26d7397166220ad2752e954a34.png" title="dune_hhi"><img alt="dune_hhi" height="411" src="https://ethresear.ch/uploads/default/optimized/3X/b/4/b4d2e88d63be2c26d7397166220ad2752e954a34_2_690x411.png" width="690" /></a></div><p></p>
<ul>
<li><strong>Restaking Infrastructure</strong>: While not cost-optimized for native staking, these protocols distribute stake among fewer node operators without aggregating it under one entity</li>
<li><strong>Centralized Staking Providers</strong>: Risk aggregating large amounts of stake, but competition among them can bolster decentralization if many can sustain independent businesses</li>
<li><strong>CEXs</strong>: Benefit the most from the power law distribution of AUM, often driving staked ETH concentration. Coinbase, for instance, is the largest node operator with nearly 15% market share.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#h-13-there-is-no-future-proof-safe-level-of-security-9" name="h-13-there-is-no-future-proof-safe-level-of-security-9"></a>1.3. There is no future-proof safe level of Security</h3>
<p>Anders Lowsson suggests (<a href="https://ethresear.ch/t/reward-curve-with-tempered-issuance-eip-research-post/">link</a>) that Ethereum should reduce its issuance, arguing that “excessive incentives for staking, beyond what is necessary for security, can unfortunately over time turn into perverse subsidies, with many downsides.” However, this raises the question of what constitutes “adequate incentives for staking” and what level of security is truly necessary.</p>
<blockquote>
<p><code>What exactly is "neutrality"? I see that term being used in handwavy fashion, especially when scaling comes up, and it's hard to know what we mean by "preserving credible neutrality" at the moment. Would be nice to get some info there. :)</code><br />
<em>Source: eawosikaa (<a href="https://x.com/eawosikaa/status/1808005717976047799" rel="noopener nofollow ugc">link</a>)</em></p>
</blockquote>
<p>Today’s global capital markets are valued in the hundreds of trillions of dollars, while Ethereum represents only a tiny fraction of that. For Ethereum to become a neutral settlement layer for the world, its cost of corruption would need to be in the hundreds of billions, if not trillions, of dollars, to capture the value that could be extracted in a possible attack. For context, large value payment systems (excluding retail payments) cleared quadrillions of dollars in value in 2022 (<a href="https://data.bis.org/topics/CPMI_FMI/tables-and-dashboards/BIS,CPMI_T9,1.0?view=value&amp;dimensions=REP_CTY%3AUS" rel="noopener nofollow ugc">link</a>). In comparison, over the past 12mos, stablecoin transfer value on Ethereum just about cleared $8tn, or 0.5% (<a href="https://www.theblock.co/data/stablecoins/usd-pegged/adjusted-on-chain-volume-of-stablecoins-monthly" rel="noopener nofollow ugc">link</a>). This is consistent with the proportion of market capitalization of Ethereum relative to global capital markets (well under 1% as well).</p>
<p>The slightest risk of insufficient security would stagnate Ethereum’s growth – decentralization and the resulting neutrality is Ethereum’s <span class="hashtag-raw">#1</span> competitive advantage. No risk should be taken to erode that, and instead, we should seek to strengthen it even further. To answer Emmanuel’s question, in our framing, we would use “neutrality” interchangeably with “sovereignty” and “autonomy”: ability to defend against censorship and coercion attacks (<a href="https://nakamoto.com/credible-neutrality/" rel="noopener nofollow ugc">link</a>). Such that the cost of “coercion” is always higher than the benefit from manipulating the state.</p>
<p>Anders’ argument assumes that a 34% double-singing attack is so costly and 51% censorship attack is so unlikely today, that the network can afford to focus on strengthening other layers. If Ethereum were already a major part of the world’s capital markets, this argument might hold more weight, as incremental risks would be smaller. However, reducing today the network’s most crucial features—security and sovereignty—would compromise the network’s ability to grow.</p>
<p>Currently, Ethereum’s social layer serves as the final defense (<a href="https://ercwl.medium.com/the-case-for-social-slashing-59277ff4d9c7" rel="noopener nofollow ugc">link</a>) against norm violations that threaten its credible neutrality. However, this social layer is structurally fragile. It requires constant vigilance from the community so that enforcement can occur on a daily basis. Yet, as Ethereum grows, massive new inflows might bypass today’s social layer altogether. If a large bank, say, staked $1tn worth of Ether with a CEX, what chance does a community of open source developers have to enforce social norms? The key question, as Emmanuel points out, is: What is the threshold for security that Ethereum needs today and in the future? The MVI proposal, in our view, fails to address this critical question, focusing instead on the other effects of reducing the security budget.</p>
<h3><a class="anchor" href="https://ethresear.ch#h-14-reframing-the-discourse-expansion-over-efficiency-10" name="h-14-reframing-the-discourse-expansion-over-efficiency-10"></a>1.4. Reframing the discourse: expansion over efficiency</h3>
<p>Ethereum should balance incentives for all stakeholders to ensure the highest level of security. This balance involves weighing long-term sustainability and expansion vs short-term efficiency to create enduring security value.</p>
<p>MVS suggests that instead of asking “how much could we reduce issuance for staking efficiency”, we should be asking “how much network incentivisation do we need to perpetuate decentralization to maintain and expand security”.</p>
<p>Strategically, MVI and MVS represent two different paths for Ethereum’s growth. MVI focuses on minimizing costs, benefiting ETH holders in the short term. MVS, on the other hand, emphasizes building a long-lasting moat around the network, optimizing long-term value creation for all stakeholders, including ETH holders.</p>
<p>Ethereum’s unique appeal lies in its secure, credibly neutral blockspace. Unlike commodity blockspace, which competes on price, secure blockspace competes on features. Similar to the advanced chip industry, where success de<br />
ffpends on computational ability rather than price, Ethereum should compete on the magnitude of security it offers. This security creates an enduring competitive advantage, accelerating value creation across the ecosystem.</p>
<p>There is a subtlety in that the market cap of Ethereum is a variable that contributes to security, and so minimizing issuance can be seen as bolstering security. Superficially, there is a reflexive effect, where Ethereum’s security both causes and is driven by its market cap. However, we believe that Ethereum’s security making ETH valuable is the primary causation, and therefore security needs to be prioritized. Below we illustrate diagrammatically the alternative value creation paths for Ethereum contributors deciding between MVS and MVI.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/f/1f2963c356b0018f378fbf4fe73ef79e641aa362.jpeg" title="tg_image_2418175601"><img alt="tg_image_2418175601" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/1/f/1f2963c356b0018f378fbf4fe73ef79e641aa362_2_530x500.jpeg" width="530" /></a></div><p></p>
<h2><a class="anchor" href="https://ethresear.ch#h-2-analysis-of-ethereum-issuance-reduction-proposal-within-the-mvs-framework-11" name="h-2-analysis-of-ethereum-issuance-reduction-proposal-within-the-mvs-framework-11"></a>2. Analysis of Ethereum Issuance reduction proposal within the MVS framework</h2>
<p>We posit that, under the MVS framework, Ethereum issuance reduction proposals risk creating downstream effects that would compromise Ethereum’s security value. Overall, we believe that ETH’s moneyness stands to increase with greater security and autonomy, to a degree that far outweighs the downsides of issuance or externalities such as capital gains taxes.</p>
<h3><a class="anchor" href="https://ethresear.ch#h-21-the-assumption-that-ethereum-overpays-for-security-is-wrong-less-issuance-may-lead-to-centralization-of-the-validator-set-12" name="h-21-the-assumption-that-ethereum-overpays-for-security-is-wrong-less-issuance-may-lead-to-centralization-of-the-validator-set-12"></a>2.1. The assumption that Ethereum overpays for security is wrong: less issuance may lead to centralization of the validator set</h3>
<h4><a class="anchor" href="https://ethresear.ch#h-211-etf-inflows-would-exacerbate-centralization-in-the-context-of-a-33-stake-cap-13" name="h-211-etf-inflows-would-exacerbate-centralization-in-the-context-of-a-33-stake-cap-13"></a>2.1.1 ETF inflows would exacerbate centralization in the context of a 33% stake cap</h4>
<p>Lowering the target stake ratio (<a href="https://ethresear.ch/t/endgame-staking-economics-a-case-for-targeting/18751">link</a>) could lead to a concentration of staked ETH with Centralized Exchanges (CEXs), driving capital away from decentralized alternatives.</p>
<p>Consider a scenario where a 33% cap (equivalent to 40.6 million staked ETH) is implemented, and the curve enacts a sharp drop of yield to zero as stake ratio increases from 30% (36.6 million ETH) to 33% (40.6 million ETH). Suppose Ether ETFs are launched in the US, attracting significant capital inflows. If these ETFs use Coinbase as their custodian (as 8 out of 11 BTC ETF issuers do), this could lead to $15 billion in inflows, adding approximately 4.5 million ETH to Coinbase’s custody. The simulated impact on the validation market might look like this; the 40.1m max staked ETH being slightly lower then 40.6 represents the fact that when yield becomes extremely low there is no marginal staker at all on the market.</p>
<div class="md-table">
<table>
<thead>
<tr>
<th><strong>Illustrative impact on validation market with a 33% MVI limit</strong></th>
<th style="text-align: center;"><strong>Current composition</strong></th>
<th style="text-align: center;"><strong>Effect in 4 years</strong></th>
<th style="text-align: center;"><strong>Future composition</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>ETH staked</td>
<td style="text-align: center;">33.1m</td>
<td style="text-align: center;">+7m</td>
<td style="text-align: center;">40.1m</td>
</tr>
<tr>
<td>ETH held with Coinbase</td>
<td style="text-align: center;">17.5m</td>
<td style="text-align: center;">+4.5m (ETFs)</td>
<td style="text-align: center;">22m</td>
</tr>
<tr>
<td>ETH held &amp; staked with Coinbase</td>
<td style="text-align: center;">4.3m</td>
<td style="text-align: center;">+10m</td>
<td style="text-align: center;">14.3m</td>
</tr>
<tr>
<td>ETH staked on-chain via LSTs/LRTs</td>
<td style="text-align: center;">13.7m</td>
<td style="text-align: center;">-2m</td>
<td style="text-align: center;">11.7</td>
</tr>
<tr>
<td>ETH staked by other entities</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">-1m</td>
<td style="text-align: center;">14.1</td>
</tr>
</tbody>
</table>
</div><ol>
<li>Market forces and fiduciary duties ensure that CEXs like Coinbase squeeze the maximum amount of profit from staking-as-a-service (for their customers and ETF issuers), and long-term the majority of their holdings are staked.</li>
</ol>
<p>We model the above impact by assigning a 10m staked ETH inflow to Coinbase. When Coinbase’s stake reaches 7.8 million, total staked ETH will be about 36.6 million, causing rewards to drop sharply. Consequently:</p>
<ol start="2">
<li>Lido stETH and other LST/LRT users, being sophisticated on-chain actors, will seek higher rewards elsewhere. The switching cost of moving capital on-chain is extremely low, so there is no incentive for capital to stay – the capital will leave for higher yields in DeFi.</li>
<li>CSPs will exit these protocols since the 5% fee from middleware won’t cover their costs.</li>
</ol>
<p>We model the above two impacts by assigning a 2 million ETH outflow to LSTs/LRTs and a 1 million ETH outflow to other entities.</p>
<ol start="4">
<li>Meanwhile, CEXs like Coinbase can continue offering staking products because their marginal costs are extremely low, and can even be offset by other business segments. Their customers may remain loyal or lack alternatives due to regulations or unsophisticated nature of the user base. This can happen despite Coinbase having higher fees (25%) compared to better-performing alternatives (5-15%).</li>
</ol>
<p>In this scenario, Coinbase could control 14.3 million ETH, surpassing the 33% network control threshold independently, while Lido and other DSMs lose market share.</p>
<h4><a class="anchor" href="https://ethresear.ch#h-212-staked-eth-concentration-with-cexs-doesnt-necessarily-have-to-happen-with-a-higher-stake-cap-14" name="h-212-staked-eth-concentration-with-cexs-doesnt-necessarily-have-to-happen-with-a-higher-stake-cap-14"></a>2.1.2 Staked ETH concentration with CEXs doesn’t necessarily have to happen with a higher stake cap</h4>
<p>Without the cap, both CEXs and on-chain market segments could coexist without putting pressure on each other due to sufficient demand for staking. LSTs, LRTs and CSPs wouldn’t face the dramatic yield decrease that would occur when Coinbase’s stake reaches 7.8 million ETH. Some might argue that Coinbase would undercut other staking providers by lowering its 25% fee. However, this is uncertain. Coinbase’s customer base seems inelastic, meaning the most profitable strategy might be to maintain or even increase their fees. In addition, even if Coinbase goes after the on-chain market and lowers their fees, the market may not be fully efficient – some people might prefer to stick with LSTs due to their decentralization preference.</p>
<p>In a highly segmented market, margins don’t need to uniformly compress, leaving space for both CEXs/CSPs and LSTs/restaking segments to thrive. LSTs and CEXs serve distinct market segments. For CEXs, the most profitable approach is to charge high fees from retail and institutional clients (e.g., Coinbase’s 25%) without directly competing with LSTs. Targeting stake ratios could stifle the market for on-chain actors but not significantly affect the market for retail and institutional clients.</p>
<p>Thus, in the absence of a stake cap, the coexistence of various staking actors could lead to a more balanced distribution of staked ETH across different market segments.</p>
<h4><a class="anchor" href="https://ethresear.ch#h-213-mvi-effect-on-the-ratio-of-solo-stakers-15" name="h-213-mvi-effect-on-the-ratio-of-solo-stakers-15"></a>2.1.3 MVI effect on the ratio of solo stakers</h4>
<h5><a class="anchor" href="https://ethresear.ch#the-importance-of-this-effect-is-overrated-16" name="the-importance-of-this-effect-is-overrated-16"></a>The importance of this effect is overrated</h5>
<p>Approximately 30 million ETH is delegated, while only 3 million is solo staked. It is evident that delegation dominates as a modality of staking. The key issue is ETH concentration with CEXs, rather than the interaction between solo stakers and LSTs.</p>
<div class="md-table">
<table>
<thead>
<tr>
<th style="text-align: center;"><strong>Grouping</strong></th>
<th style="text-align: center;"><strong>Approximate stake</strong></th>
<th style="text-align: center;"><strong>Type</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CEXs</td>
<td style="text-align: center;">10m</td>
<td style="text-align: center;">Delegated</td>
</tr>
<tr>
<td style="text-align: center;">LSTs, LRTs, CSPs</td>
<td style="text-align: center;">20m</td>
<td style="text-align: center;">Delegated</td>
</tr>
<tr>
<td style="text-align: center;">Solo stakers</td>
<td style="text-align: center;">3m</td>
<td style="text-align: center;">Solo staked</td>
</tr>
</tbody>
</table>
</div><h5><a class="anchor" href="https://ethresear.ch#lsts-and-csps-can-also-contribute-to-overall-network-quality-17" name="lsts-and-csps-can-also-contribute-to-overall-network-quality-17"></a>LSTs and CSPs can also contribute to overall network quality</h5>
<p>While solo stakers are often seen as the backbone of Ethereum’s network security, the contributions of LSTs and centralized staking providers are undervalued.</p>
<p>There is a lot of nuance to the emergent risks of malicious actors emerging from LSTs such as Lido. There certainly are risks (cf. Mike Neuder’s extensive post on the subject, <a href="https://notes.ethereum.org/@mikeneuder/magnitude-and-direction" rel="noopener nofollow ugc">link</a>). However, there are also many benefits to deterministic stake allocation to professional or larger node operators. It’s possible for solo stakers to have different motivations than an LST whose main objective is to decentralize Ethereum validation (<a href="https://research.lido.fi/t/lido-dao-vibe-alignment-purpose-mission-vision/" rel="noopener nofollow ugc">link</a>). Some of the most noteworthy examples of malicious proposers, for example, have come from solo validators, such as those involved in the April 3rd, 2023 MEV Boost exploit (<a href="https://collective.flashbots.net/t/post-mortem-april-3rd-2023-mev-boost-relay-incident-and-related-timing-issue/" rel="noopener nofollow ugc">link</a>).</p>
<p>Centralized staking providers and LSTs are quantifiably more performant validators than solo stakers. There is significant existing data (<a href="http://rated.network/" rel="noopener nofollow ugc">link</a>) today to quantify proposer effectiveness and attester effectiveness, which drive fewer missed slots and attestations, faster block propagation and chain finalization. Overall the network is much more stable and responsive with professional validators than it would otherwise be, but also more decentralized.</p>
<h5><a class="anchor" href="https://ethresear.ch#issuance-reductions-would-likely-decrease-the-share-of-solo-stakers-18" name="issuance-reductions-would-likely-decrease-the-share-of-solo-stakers-18"></a>Issuance reductions would likely decrease the share of solo stakers</h5>
<p>Some argue that solo stakers are less elastic with respect to yield, because they are as a cohort more heterogeneous than other validating entities, and hence have a steeper supply curve.</p>
<p>However, our simplified analysis of Ethereum validator economics shows this argument is flawed. Solo stakers in fact have much higher fixed costs, making them much less adaptable to a low issuance rates compared to larger node operators. Specifically,</p>
<p>For solo stakers:</p>
<ul>
<li>Staking APR is lower and closer to the Median staking APR (i.e. 2.4% per Rated, <a href="https://explorer.rated.network/network?network=mainnet&amp;timeWindow=1d&amp;rewardsMetric=average&amp;geoDistType=all&amp;hostDistType=all&amp;soloProDist=stake" rel="noopener nofollow ugc">link</a>) than scale node operators due to the unpredictability of proposer rewards, tips and MEV</li>
<li>The costs of running a single validator include hardware (32 GB RAM, 4 TB SSD) and electricity. Home internet plans are sufficient for solo stakers, so broadband cost is assumed to be 0 (no incremental cost).</li>
<li>In this set up, 100% of solo staker’s total costs are fixed costs. Assuming hardware depreciation of 5 years, profit margins are &gt;90% to solo stakers</li>
<li>We exclude the need to reserve 32 ETH in capital as collateral, which brings the capital outlay (though not outright investment) significantly higher</li>
</ul>
<p>Then consider, on the opposite end of the spectrum, a large centralized node operator with 100,000 validators:</p>
<ul>
<li>Staking APR is higher and closer to the Average staking APR (i.e. 3.3% per Rated, <a href="https://explorer.rated.network/network?network=mainnet&amp;timeWindow=1d&amp;rewardsMetric=average&amp;geoDistType=all&amp;hostDistType=all&amp;soloProDist=stake" rel="noopener nofollow ugc">link</a>) as stake pooling smoothes the unpredictable components of both CL (proposer rewards) and EL (tips + MEV) rewards</li>
<li>Costs include hardware but also significant operational costs including technical and marketing staff</li>
<li>Hardware and internet are fixed costs, electricity is a variable cost and staff costs can be seen as a semi-variable cost
<ul>
<li>Employment footprint can be eventually adjusted should the top line be negatively impacted</li>
</ul>
</li>
<li>Counting half of the maintenance &amp; growth spend as fixed and the other half as variable, we arrive at fixed costs representing 64% of the large node operators’ total costs (i.e. much less than solo stakers). Profit margins are also lower than those of solo stakers</li>
</ul>
<div class="md-table">
<table>
<thead>
<tr>
<th>Assumptions</th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td>ETH ($)</td>
<td style="text-align: right;">3,500</td>
</tr>
<tr>
<td>Average staking APR</td>
<td style="text-align: right;">3.3%</td>
</tr>
<tr>
<td>Median staking APR</td>
<td style="text-align: right;">2.4%</td>
</tr>
<tr>
<td>MVI reduction assumed</td>
<td style="text-align: right;">2.0%</td>
</tr>
</tbody>
</table>
</div><div class="md-table">
<table>
<thead>
<tr>
<th><strong>Illustrative Annual P/L</strong></th>
<th style="text-align: center;"><strong>Current</strong></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td style="text-align: center;"><strong>Solo Staker</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>Large Node Operator</strong></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td></td>
<td style="text-align: center;"><strong>Quantity</strong></td>
<td style="text-align: center;"><strong>$</strong></td>
<td style="text-align: center;"><strong>Quantity</strong></td>
<td style="text-align: center;"><strong>$</strong></td>
</tr>
<tr>
<td># of validators</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">112,000</td>
<td style="text-align: center;">100,000</td>
<td style="text-align: center;">11,200,000,000</td>
</tr>
<tr>
<td>Staking APR</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.4%</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3.3%</td>
</tr>
<tr>
<td>Staking income</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2,677</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">367,360,000</td>
</tr>
<tr>
<td>Commission</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10%</td>
<td style="text-align: center;">36,736,000</td>
</tr>
<tr>
<td></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td><strong>Hardware cost</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>800</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>7,750,000</strong></td>
</tr>
<tr>
<td>Computer/servers</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">800</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">7,000,000</td>
</tr>
<tr>
<td>Backup servers</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">750,000</td>
</tr>
<tr>
<td></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td><strong>Operational cost</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>74</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>19,794,780</strong></td>
</tr>
<tr>
<td>Electricity</td>
<td style="text-align: center;">70Wh, $0.12/kWh</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">750Wh/server, $0.12/kWh</td>
<td style="text-align: center;">354,780</td>
</tr>
<tr>
<td>Internet connection</td>
<td style="text-align: center;">No incremental cost</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">540GB/month/val @ $0.03/GB</td>
<td style="text-align: center;">19,440,000</td>
</tr>
<tr>
<td></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td><strong>Maintenance &amp; growth</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>0</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>11,400,000</strong></td>
</tr>
<tr>
<td>Technical staff</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">8,400,000</td>
</tr>
<tr>
<td>Marketing/admin staff</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">3,000,000</td>
</tr>
<tr>
<td>Cybersecurity/miscellaneous</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1,000,000</td>
</tr>
<tr>
<td></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td><strong>Total cost (assume 5Y hardware depreciation)</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>234</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>32,744,780</strong></td>
</tr>
<tr>
<td>o/w fixed cost</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">100%</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">64%</td>
</tr>
<tr>
<td>o/w variable cost</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0%</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">36%</td>
</tr>
<tr>
<td><strong>Payback period on capex (months)</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>3.9</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>23.3</strong></td>
</tr>
<tr>
<td><strong>Annual income/loss</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>2,443</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>3,991,220</strong></td>
</tr>
<tr>
<td><strong>Profit margin (excl. ETH at stake)</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>91.3%</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>10.9%</strong></td>
</tr>
</tbody>
</table>
</div><p>In the event that MVI reduces staking APR for all stakers (e.g. -200bps), the below scenario analysis helps visualize how different stakers may be impacted differently. High level:</p>
<ul>
<li>Solo stakers have very limited, if no, way of adjusting their underlying costs. 100% of the reduced staking rewards will fall through to the bottom line, resulting in a dramatic reduction in profit margin. As a result, the payback period on capex (i.e. hardware) multiplies from 3.9 months to 47.2 months in our example, without considering the need to raise 32 ETH to activate a validator to begin with. This raises the question of whether incremental demand from new solo stakers could be sustained in the post-MVI world</li>
<li>Meanwhile, large node operators have more levers to pull to protect their profits and capex payback periods
<ul>
<li>As in Scenario 1, node operators can raise their commission</li>
<li>As in Scenario 2, node operators can raise their commission and reduce variable costs, notably staff costs</li>
<li>With very minor changes to their structure they can come back to prior levels of profit</li>
</ul>
</li>
</ul>
<div class="md-table">
<table>
<thead>
<tr>
<th><strong>Illustrative Annual P/L</strong></th>
<th style="text-align: center;"><strong>If staking APR reduces by 200bps</strong></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td style="text-align: center;"><strong>Solo Staker</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>Large Node Operator - Scenario 1</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>Large Node Operator - Scenario 2</strong></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td></td>
<td style="text-align: center;"><strong>Quantity</strong></td>
<td style="text-align: center;"><strong>$</strong></td>
<td style="text-align: center;"><strong>Quantity</strong></td>
<td style="text-align: center;"><strong>$</strong></td>
<td style="text-align: center;"><strong>Quantity</strong></td>
<td style="text-align: center;"><strong>$</strong></td>
</tr>
<tr>
<td># of validators</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">112,000</td>
<td style="text-align: center;">100,000</td>
<td style="text-align: center;">11,200,000,000</td>
<td style="text-align: center;">100,000</td>
<td style="text-align: center;">11,200,000,000</td>
</tr>
<tr>
<td>Staking APR</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong><em>0.4%</em></strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong><em>1.3%</em></strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong><em>1.3%</em></strong></td>
</tr>
<tr>
<td>Staking income</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">437</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">143,360,000</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">143,360,000</td>
</tr>
<tr>
<td>Commission</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong><em>25%</em></strong></td>
<td style="text-align: center;">35,840,000</td>
<td style="text-align: center;"><strong><em>25%</em></strong></td>
<td style="text-align: center;">35,840,000</td>
</tr>
<tr>
<td></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td><strong>Hardware cost</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>800</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>7,750,000</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>7,750,000</strong></td>
</tr>
<tr>
<td>Computer/servers</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">800</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">7,000,000</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">7,000,000</td>
</tr>
<tr>
<td>Backup servers</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">750,000</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">750,000</td>
</tr>
<tr>
<td></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td><strong>Operational cost</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>74</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>19,794,780</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>19,794,780</strong></td>
</tr>
<tr>
<td>Electricity</td>
<td style="text-align: center;">70Wh, $0.12/kWh</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">354,780</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">354,780</td>
</tr>
<tr>
<td>Internet connection</td>
<td style="text-align: center;">No incremental cost</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">19,440,000</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">19,440,000</td>
</tr>
<tr>
<td></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td><strong>Maintenance &amp; growth</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>0</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>11,400,000</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>10,504,000</strong></td>
</tr>
<tr>
<td>Technical staff</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">8,400,000</td>
<td style="text-align: center;"><strong><em>64</em></strong></td>
<td style="text-align: center;">7,739,789</td>
</tr>
<tr>
<td>Marketing/admin staff</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">3,000,000</td>
<td style="text-align: center;"><strong><em>28</em></strong></td>
<td style="text-align: center;">2,764,211</td>
</tr>
<tr>
<td>Cybersecurity/miscellaneous</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1,000,000</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1,000,000</td>
</tr>
<tr>
<td></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td><strong>Total cost (assume 5Y hardware depreciation)</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>234</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>32,744,780</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>31,848,780</strong></td>
</tr>
<tr>
<td>o/w fixed cost</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">100%</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">64%</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">66%</td>
</tr>
<tr>
<td>o/w variable cost</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0%</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">36%</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">34%</td>
</tr>
<tr>
<td><strong>Payback period on capex (months)</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>47.2</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>30.0</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>23.3</strong></td>
</tr>
<tr>
<td><strong>Annual income/loss</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>203</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>3,095,220</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>3,991,220</strong></td>
</tr>
<tr>
<td><strong>Profit margin (excl. ETH at stake)</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>46.5%</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>8.6%</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>11.1%</strong></td>
</tr>
</tbody>
</table>
</div><p><em>Illustrative figures can be found <a href="https://docs.google.com/spreadsheets/d/1tr7VJqzJLiywf34_debHa20wfjU5d8db1eYrJWU0i3Q/edit?gid=0#gid=0" rel="noopener nofollow ugc">here</a></em></p>
<p>Due to the presence of a higher proportion of fixed costs, solo stakers (and smaller node operators alike) will show higher sensitivity to changes in staking rewards compared to larger node operators. The corollary is that as MVI reduces staking reward APR, the marginal players may be priced out, leading to a greater centralization of stake. This would exacerbate the already decreasing trend of solo stakers alongside Ethereum’s issuance compression over time.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/7/c75ca71bdb5bbc1207a30f9439fd1dc937b2aa59.png" title="dune_marketshare"><img alt="dune_marketshare" height="411" src="https://ethresear.ch/uploads/default/optimized/3X/c/7/c75ca71bdb5bbc1207a30f9439fd1dc937b2aa59_2_690x411.png" width="690" /></a></div><br />
<em>Source: Dune (<a href="https://dune.com/queries/3852057/6478867" rel="noopener nofollow ugc">link</a>)</em><p></p>
<h3><a class="anchor" href="https://ethresear.ch#h-22-lst-dominance-and-cost-modeling-are-inadequate-arguments-for-issuance-reduction-19" name="h-22-lst-dominance-and-cost-modeling-are-inadequate-arguments-for-issuance-reduction-19"></a>2.2. LST dominance and cost-modeling are inadequate arguments for issuance reduction</h3>
<h4><a class="anchor" href="https://ethresear.ch#h-221-issuance-as-a-cost-is-a-reductive-framing-20" name="h-221-issuance-as-a-cost-is-a-reductive-framing-20"></a>2.2.1. Issuance as a cost is a reductive framing</h4>
<p>“Issuance as a cost” concerns the dilution effect on native ETH holders and the potential welfare loss due to externalities like taxes.</p>
<p>The first component focuses on the direct impact of issuance. It redistributes network ownership from unstaked ETH holders to staked ETH holders. High issuance rates force ETH holders to stake to avoid dilution. This increases Ethereum’s security and neutrality but comes with inconvenience and some risk for native ETH holders – which, under MVS, doesn’t qualify as strongly undesirable. Moreover, the cumulative effect could even be seen as beneficial, to the extent that more stake landing with a distributed set of validators justifies investors’ inconvenience.</p>
<p>The second component addresses additional costs for stakers due to taxes. ETH holders who earn staking rewards may face tax obligations, creating additional sell pressure. However, this concern is specific to certain jurisdictions and points in time. Furthermore, the impact of this sell pressure on Ethereum’s overall functionality is questionable. Assuming 3.5% staking rewards, a $400bn ETH market cap, and 30% average taxes paid by all stakers, we get $4.2bn in annual sell pressure. Given Ethereum’s daily trading volume is in billions, absorbing 1% sell pressure over a year seems immaterial. Furthermore, LSTs such as wstETH provide an efficient way to postpone the tax payments, since the tax event is triggered only when wstETH is sold.</p>
<p>Even though ETH market cap is significant in determining attack costs, the relatively minor effect of sell pressure does not provide enough security benefits to justify reducing issuance. The trade-offs include potential staked ETH concentration, loss of sovereignty, and a more substantial decrease in market cap as a result.</p>
<h4><a class="anchor" href="https://ethresear.ch#h-222-stakers-getting-higher-real-vs-nominal-yield-is-not-significant-21" name="h-222-stakers-getting-higher-real-vs-nominal-yield-is-not-significant-21"></a>2.2.2. Stakers getting higher real vs nominal yield is not significant</h4>
<p>This argument, while mathematically beautiful (<a href="https://notes.ethereum.org/@mikeneuder/subsol#3-Scaled-Root-Curve-alternative-issuance" rel="noopener nofollow ugc">link</a>), is not significant in magnitude. It does not affect security and neutrality in any way; in fact, it is not at all clear if there is any benefit to Ethereum in fewer stakers getting higher real yield compared to more stakers getting less real yield. In addition, this analysis assumes concave supply curves with respect to nominal yield, while it is possible that at a higher staking ratio we should adjust our analysis to concave supply curves with respect to real yield.</p>
<h4><a class="anchor" href="https://ethresear.ch#h-223-reducing-lst-dominance-shouldnt-be-a-primary-objective-of-ethereums-monetary-policy-22" name="h-223-reducing-lst-dominance-shouldnt-be-a-primary-objective-of-ethereums-monetary-policy-22"></a>2.2.3. Reducing LST dominance shouldn’t be a primary objective of Ethereum’s monetary policy</h4>
<p>This argument is directly related to security and neutrality, and thus can be analyzed under a security-maximizing framework.</p>
<p>In his article (<a href="https://notes.ethereum.org/@mikeneuder/magnitude-and-direction" rel="noopener nofollow ugc">link</a>) Mike Neuder analyzed various directions and magnitudes of possible Lido attacks on Ethereum in the future. While there are several potential attacks, all of them have a corresponding mitigation plan. Dual governance is at the heart of many of those mitigations. DG is a mechanism that allows stETH holders to slow down Lido’s governance and exit from the protocol before any decision is made. This mechanism is in active and final stages of development (<a href="https://research.lido.fi/t/dual-governance-design-and-implementation-proposal" rel="noopener nofollow ugc">link</a>).</p>
<p>Another argument for issuance reduction is that stETH risks substituting ETH as the de-facto money and collateral. While there is certainly a possibility that LSTs wind up replacing a lot of ETH functionality in DeFi, it does not diminish the moneyness of ETH – all LSTs are underscored by ETH, and thus derive their value from ETH. In order to execute any of these transactions, users will still need ETH to pay for gas, at the very least. Furthermore, ETH will continue to be bridged to various L2s either way, so at a baseline ETH velocity will already decline with broader adoption of L2s, without compromising its moneyness.</p>
<p>Finally, there are unintended consequences to targeting individual applications in an opinionated manner in order to manipulate the viability of ETH as collateral or as commodity money. The long-term roadmap of Ethereum should not be hostage to short-term tactical considerations, least of all on the application layer. The growth of LSTs has allowed the growth of user activity on Ethereum and has also increased the velocity and usage of Ether itself.</p>
<h2><a class="anchor" href="https://ethresear.ch#h-3-putting-it-all-together-23" name="h-3-putting-it-all-together-23"></a>3. Putting it all together</h2>
<p>MVI, as a framework, ultimately suggests to squeeze as much as possible out of staking, so that stakers’ cost and revenue are more or less at the same low rate. The major problem of this approach is that market forces structurally do not reward decentralization, and ultimately drive stake concentration to CEXs, which are entities with the lowest cost validators and the most inelastic customer base. Thus the downside of MVI is undermining the security and neutrality of Ethereum. In our view, the benefits of MVI, such as decreasing the selling pressure from taxes, do not justify taking this risk on balance.</p>
<p>MVS, on the other hand, suggests evaluating monetary policies primarily through the lens of how it affects security and neutrality, the core value propositions of Ethereum. One of the arguments for issuance reduction, namely to tackle LST dominance, indeed falls into MVS focus. However, the security and neutrality concerns of LST dominance are of second order in nature (“if dual governance doesn’t work”, “LST becomes an additional risk layer for all users”, etc). Meanwhile, stake accumulating in CEXs rather than in LSTs, LRTs or even CSPs creates a very real risk of staked ETH concentration with one single entity. As such, we do not see the case where LST dominance risk outweighs the risk of stake concentration with CEXs.</p>
<p>While we presented the MVS framework, and accordingly evaluated the issuance reduction proposal, the natural question stands: what would be the right issuance policy under the MVS approach? This is an incredibly complex and deep question that we would like to explore in future. Some of the directions that we have in mind include:</p>
<ul>
<li>How do we quantifiably measure security? Is there a way for a protocol to see its security? Credit to the contributions from the StakeSure (<a href="https://arxiv.org/abs/2401.05797" rel="noopener nofollow ugc">link</a>) paper in this direction.</li>
<li>Guided by MVS, rather than focusing on value creation through cost reductions, we should instead consider the value creation by improving security and neutrality. There is a heuristic argument that increasing issuance can improve security through making the network more complex via a more diverse validator set. Is there a way to make this precise? How do we make sure that the extra issued ETH strictly improves security and neutrality?</li>
</ul>
<p>Is there a case for a marginal improvement analysis: the more diverse the validator set is, the more complex the network becomes, and improvements to security could have increasing marginal contributions. (Similar to how complexity contributes to entropy and layered security, <a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf" rel="noopener nofollow ugc">link</a>)</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/f/1f6daf84bc2dfabd3b7747f049d71b9597079ddb.jpeg" title="image"><img alt="image" height="389" src="https://ethresear.ch/uploads/default/optimized/3X/1/f/1f6daf84bc2dfabd3b7747f049d71b9597079ddb_2_690x389.jpeg" width="690" /></a></div><p></p>
<hr />
<p><em>Disclosure: authors are variously affiliated with cyber.fund, Lido DAO, Steakhouse Financial, Progrmd Capital</em></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/maximum-viable-security-a-new-framing-for-ethereum-issuance/19992">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sat, 06 Jul 2024 21:56:34 +0000</pubDate>
</item>
<item>
<title>Releasing Constantine v0.1.0, a modular cryptography stack for Ethereum</title>
<link>https://ethresear.ch/t/releasing-constantine-v0-1-0-a-modular-cryptography-stack-for-ethereum/19990</link>
<guid>https://ethresear.ch/t/releasing-constantine-v0-1-0-a-modular-cryptography-stack-for-ethereum/19990</guid>
<content:encoded><![CDATA[
<p>I am very proud to release the very first version of <a href="https://github.com/mratsim/constantine" rel="noopener nofollow ugc">Constantine</a>, a high-performance modular cryptography stack for blockchains and proof systems.<br />
It is currently as of July 2024 the fastest implementation of Ethereum-specific cryptographic primitives:</p>
<ul>
<li>BLS signatures</li>
<li>BN254 precompiles (EIP-196 and EIP-197, repriced in EIP-1108)</li>
<li>BLS12-381 precompiles (EIP-2537)</li>
<li>KZG Polynomial commitments (EIP-4844)</li>
</ul>
<p>Constantine has bindings in C, Go, Nim and Rust.</p>
<h2><a class="anchor" href="https://ethresear.ch#history-1" name="history-1"></a>History</h2>
<p>Constantine is written in <a href="https://nim-lang.org/" rel="noopener nofollow ugc">Nim</a>, the language was chosen by Status for Nimbus for its expressiveness, its type system strength, the ease to wrap C and C++ and syntactic closeness to Python so that ethereum/research and PyEVM could be ported with ease.</p>
<p>In February 2018, after woes with C++ in Nimbus, the first library I built was a fixed precision big integer library for uint256.<br />
Then we (at Status) realized that we would also need elliptic curves for secp256k1 and BN254 (also known as BN256 or alt_bn128).</p>
<p>How hard could it be to implement elliptic curves, with cryptographic hardening, once you know how to write big integers?</p>
<p>Turned out it was too hard, after a week or so another approach was taken for time-to-market and correctness reasons:</p>
<ul>
<li>Use libsecp256k1 from Bitcoin</li>
<li>Port 1-1 bncurves from Zcash for BN254</li>
<li>Use Apache Milagro for BLS12-381</li>
</ul>
<p>It was then restarted as a personal side-project in February 2020 after learning a lot from implementing hashing-to-curve and Ethereum BLS signatures and identifying significant performance gap. <em>Note that this predates BLST which was initially released in June 2020.</em></p>
<p>Since then Constantine has seen regular contributions (sometimes with couple months gap) up to where it is today.</p>
<h2><a class="anchor" href="https://ethresear.ch#performance-2" name="performance-2"></a>Performance</h2>
<h3><a class="anchor" href="https://ethresear.ch#ethereum-bls-signatures-consensus-layer-3" name="ethereum-bls-signatures-consensus-layer-3"></a>Ethereum BLS signatures (Consensus Layer)</h3>
<p>Benchmarks are done on an AMD Ryzen 7840U, a low-power ultra-mobile 8-core CPU from 2023.</p>
<h4><a class="anchor" href="https://ethresear.ch#blst-through-nim-blscurve-4" name="blst-through-nim-blscurve-4"></a>BLST (through nim-blscurve)</h4>
<p>Nim-blscurve is the backend of Nimbus-eth2. As Nim compiles to machine code through C (or C++), calling C has zero-overhead from Nim.</p>
<p>Repro.<br />
Install the latest Nim version, Nim v2.0.8.</p>
<pre><code class="lang-auto">git clone https://github.com/status-im/nim-blscurve
cd nim-blscurve
git submodule update --init
nimble bench
</code></pre>
<p>2 benchmarks will be done with 2 different memory management solutions (different implementations of refcounting)<br />
BLST is as-of v0.3.12 (May 2024) with runtime CPU features detection</p>
<pre><code class="lang-auto">Backend: BLST, mode: 64-bit
====================================================================================================================================

Scalar multiplication G1 (255-bit, constant-time)                             10332.180 ops/s        96785 ns/op       318784 cycles
Scalar multiplication G2 (255-bit, constant-time)                              4622.247 ops/s       216345 ns/op       712585 cycles
EC add G1 (constant-time)                                                   1795332.136 ops/s          557 ns/op         1836 cycles
EC add G2 (constant-time)                                                    713775.874 ops/s         1401 ns/op         4617 cycles
------------------------------------------------------------------------------------------------------------------------------------
Pairing (Miller loop + Final Exponentiation)                                   1484.823 ops/s       673481 ns/op      2218271 cycles
------------------------------------------------------------------------------------------------------------------------------------
Hash to G2 (Draft #9) + affine conversion                                      6795.232 ops/s       147162 ns/op       484712 cycles
------------------------------------------------------------------------------------------------------------------------------------
BLS signature                                                                  3490.864 ops/s       286462 ns/op       943532 cycles
BLS verification                                                               1212.302 ops/s       824877 ns/op      2716928 cycles
BLS agg verif of 1 msg by 128 pubkeys                                          1139.886 ops/s       877281 ns/op      2889519 cycles
------------------------------------------------------------------------------------------------------------------------------------
BLS verif of 6 msgs by 6 pubkeys                                                203.231 ops/s      4920498 ns/op     16206824 cycles
Serial batch verify 6 msgs by 6 pubkeys (with blinding)                         359.968 ops/s      2778025 ns/op      9150078 cycles
Parallel batch verify of 6 msgs by 6 pubkeys (with blinding)                    615.452 ops/s      1624822 ns/op      5351722 cycles
------------------------------------------------------------------------------------------------------------------------------------
BLS verif of 60 msgs by 60 pubkeys                                               20.310 ops/s     49236672 ns/op    162172626 cycles
Serial batch verify 60 msgs by 60 pubkeys (with blinding)                        42.709 ops/s     23414406 ns/op     77120772 cycles
Parallel batch verify of 60 msgs by 60 pubkeys (with blinding)                  250.298 ops/s      3995236 ns/op     13159139 cycles
------------------------------------------------------------------------------------------------------------------------------------
BLS verif of 180 msgs by 180 pubkeys                                              6.746 ops/s    148237745 ns/op    488256390 cycles
Serial batch verify 180 msgs by 180 pubkeys (with blinding)                      14.419 ops/s     69354258 ns/op    228434104 cycles
Parallel batch verify of 180 msgs by 180 pubkeys (with blinding)                 99.467 ops/s     10053540 ns/op     33113513 cycles
------------------------------------------------------------------------------------------------------------------------------------

Using nthreads = 16. The number of threads can be changed with TP_NUM_THREADS environment variable.
</code></pre>
<h4><a class="anchor" href="https://ethresear.ch#constantine-5" name="constantine-5"></a>Constantine</h4>
<p>GCC generates poor code everwhere assembly is not used, hence we force Clang as a compiler.</p>
<pre><code class="lang-auto">git clone https://github.com/mratsim/constantine
cd constantine
CC=clang nimble bench_eth_bls_signatures
</code></pre>
<pre><code class="lang-auto">--------------------------------------------------------------------------------------------------------------------------------------------------
Pubkey deserialization (full checks)                                                     BLS12_381 G1          22295.550 ops/s         44852 ns/op        147729 CPU cycles (approx)
Pubkey deserialization (skip checks)                                                     BLS12_381 G1          92515.496 ops/s         10809 ns/op         35602 CPU cycles (approx)
Signature deserialization (full checks)                                                  BLS12_381 G2          16808.418 ops/s         59494 ns/op        195958 CPU cycles (approx)
Signature deserialization (skip checks)                                                  BLS12_381 G2          46453.291 ops/s         21527 ns/op         70906 CPU cycles (approx)
--------------------------------------------------------------------------------------------------------------------------------------------------
BLS signature                                                                            BLS12_381 G2           4005.078 ops/s        249683 ns/op        822392 CPU cycles (approx)
BLS verification                                                                         BLS12_381              1498.960 ops/s        667129 ns/op       2197347 CPU cycles (approx)
--------------------------------------------------------------------------------------------------------------------------------------------------
BLS agg verif of 1 msg by 128 pubkeys                                                    BLS12_381              1423.694 ops/s        702398 ns/op       2313504 CPU cycles (approx)
--------------------------------------------------------------------------------------------------------------------------------------------------
BLS verif of 6 msgs by 6 pubkeys                                                         BLS12_381               249.683 ops/s       4005085 ns/op      13191614 CPU cycles (approx)
BLS serial batch verify of 6 msgs by 6 pubkeys (with blinding)                           BLS12_381               420.912 ops/s       2375795 ns/op       7825187 CPU cycles (approx)
BLS parallel batch verify (16 threads) of 6 msgs by 6 pubkeys (with blinding)            BLS12_381               683.399 ops/s       1463273 ns/op       4819062 CPU cycles (approx)
--------------------------------------------------------------------------------------------------------------------------------------------------
BLS verif of 60 msgs by 60 pubkeys                                                       BLS12_381                24.863 ops/s      40220998 ns/op     132477024 CPU cycles (approx)
BLS serial batch verify of 60 msgs by 60 pubkeys (with blinding)                         BLS12_381                48.878 ops/s      20459201 ns/op      67387049 CPU cycles (approx)
BLS parallel batch verify (16 threads) of 60 msgs by 60 pubkeys (with blinding)          BLS12_381               280.961 ops/s       3559207 ns/op      11722847 CPU cycles (approx)
--------------------------------------------------------------------------------------------------------------------------------------------------
BLS verif of 180 msgs by 180 pubkeys                                                     BLS12_381                 8.334 ops/s     119995222 ns/op     395232558 CPU cycles (approx)
BLS serial batch verify of 180 msgs by 180 pubkeys (with blinding)                       BLS12_381                16.488 ops/s      60650899 ns/op     199767961 CPU cycles (approx)
BLS parallel batch verify (16 threads) of 180 msgs by 180 pubkeys (with blinding)        BLS12_381               112.215 ops/s       8911481 ns/op      29351939 CPU cycles (approx)
--------------------------------------------------------------------------------------------------------------------------------------------------
</code></pre>
<h4><a class="anchor" href="https://ethresear.ch#analysis-6" name="analysis-6"></a>Analysis</h4>
<ul>
<li>15% performance improvement on signatures</li>
<li>24% performance improvement on verification</li>
</ul>
<p>Furthermore, it is in theory possible to achieve a 2x performance improvement for signing if there is a need for it.</p>
<h3><a class="anchor" href="https://ethresear.ch#kzg-polynomial-commitment-for-eip-4844-consensus-layer-7" name="kzg-polynomial-commitment-for-eip-4844-consensus-layer-7"></a>KZG Polynomial commitment for EIP-4844 (Consensus Layer)</h3>
<p>I will reuse my benchmarks from Dec, 2023: <a class="inline-onebox" href="https://github.com/mratsim/constantine/pull/304#issuecomment-1844795359" rel="noopener nofollow ugc">Productionize KZG EIP-4844 by mratsim · Pull Request #304 · mratsim/constantine · GitHub</a></p>
<div class="md-table">
<table>
<thead>
<tr>
<th style="text-align: center;">Bench</th>
<th style="text-align: center;">c-kzg-4844 (serial)</th>
<th style="text-align: center;">go-kzg-4844 (serial)</th>
<th style="text-align: center;">go-kzg-4844 (parallel)</th>
<th style="text-align: center;">constantine (serial)</th>
<th style="text-align: center;">constantine (parallel)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">blob_to_kzg_commitment</td>
<td style="text-align: center;">37.773 ms</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5.823 ms</td>
<td style="text-align: center;">23.765 ms</td>
<td style="text-align: center;">4.425 ms</td>
</tr>
<tr>
<td style="text-align: center;">compute_kzg_proof</td>
<td style="text-align: center;">39.945 ms</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">7.146 ms</td>
<td style="text-align: center;">24.255 ms</td>
<td style="text-align: center;">4.710 ms</td>
</tr>
<tr>
<td style="text-align: center;">compute_blob_kzg_proof</td>
<td style="text-align: center;">40.212 ms</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">7.205 ms</td>
<td style="text-align: center;">24.288 ms</td>
<td style="text-align: center;">4.794 ms</td>
</tr>
<tr>
<td style="text-align: center;">verify_kzg_proof</td>
<td style="text-align: center;">0.915 ms</td>
<td style="text-align: center;">0.923 ms</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.782 ms</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">verify_blob_kzg_proof</td>
<td style="text-align: center;">1.531 ms</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.390 ms</td>
<td style="text-align: center;">1.266 ms</td>
<td style="text-align: center;">1.113 ms</td>
</tr>
<tr>
<td style="text-align: center;">verify_blob_kzg_proof_batch 1</td>
<td style="text-align: center;">1.528 ms</td>
<td style="text-align: center;">1.392 ms</td>
<td style="text-align: center;">1.405 ms</td>
<td style="text-align: center;">1.286 ms</td>
<td style="text-align: center;">1.130 ms</td>
</tr>
<tr>
<td style="text-align: center;">verify_blob_kzg_proof_batch 2</td>
<td style="text-align: center;">2.589 ms</td>
<td style="text-align: center;">3.233 ms</td>
<td style="text-align: center;">1.591 ms</td>
<td style="text-align: center;">2.006 ms</td>
<td style="text-align: center;">1.152 ms</td>
</tr>
<tr>
<td style="text-align: center;">verify_blob_kzg_proof_batch 4</td>
<td style="text-align: center;">4.553 ms</td>
<td style="text-align: center;">4.671 ms</td>
<td style="text-align: center;">1.914 ms</td>
<td style="text-align: center;">3.437 ms</td>
<td style="text-align: center;">1.250 ms</td>
</tr>
<tr>
<td style="text-align: center;">verify_blob_kzg_proof_batch 8</td>
<td style="text-align: center;">8.446 ms</td>
<td style="text-align: center;">7.410 ms</td>
<td style="text-align: center;">2.738 ms</td>
<td style="text-align: center;">6.115 ms</td>
<td style="text-align: center;">1.891 ms</td>
</tr>
<tr>
<td style="text-align: center;">verify_blob_kzg_proof_batch 16</td>
<td style="text-align: center;">16.228 ms</td>
<td style="text-align: center;">12.734 ms</td>
<td style="text-align: center;">3.542 ms</td>
<td style="text-align: center;">11.567 ms</td>
<td style="text-align: center;">3.091 ms</td>
</tr>
<tr>
<td style="text-align: center;">verify_blob_kzg_proof_batch 32</td>
<td style="text-align: center;">32.016 ms</td>
<td style="text-align: center;">23.048 ms</td>
<td style="text-align: center;">7.215 ms</td>
<td style="text-align: center;">21.779 ms</td>
<td style="text-align: center;">6.764 ms</td>
</tr>
<tr>
<td style="text-align: center;">verify_blob_kzg_proof_batch 64</td>
<td style="text-align: center;">63.415 ms</td>
<td style="text-align: center;">43.224 ms</td>
<td style="text-align: center;">14.438 ms</td>
<td style="text-align: center;">43.099 ms</td>
<td style="text-align: center;">11.538 ms</td>
</tr>
</tbody>
</table>
</div><ul>
<li>A 37% performance improvement over c-kzg-4844 for serial commitment</li>
<li>A 39% improvement for proof generation</li>
<li>A 17% improvement for a single blob verification</li>
<li>A 32% improvement for 64 blob verification</li>
</ul>
<p>And Constantine offers paralellization to improve those numbers 4~6x on my 8-core machine.</p>
<h3><a class="anchor" href="https://ethresear.ch#evm-precompiles-execution-layer-8" name="evm-precompiles-execution-layer-8"></a>EVM precompiles (Execution Layer)</h3>
<p>Note:</p>
<ul>
<li>Constantine also offers a fast MODEXP precompile that reaches 80% to 110% of GMP, without assembly.</li>
<li>SHA256 is faster than OpenSSL and BLST for data size less than 4MB and within 3% otherwise.</li>
</ul>
<pre><code class="lang-auto">git clone https://github.com/mratsim/constantine
cd constantine
CC=clang nimble bench_eth_evm_precompiles
</code></pre>
<pre><code class="lang-auto">--------------------------------------------------------------------------------------------------------------------------------
SHA256 -  32 bytes            72 gas    1714.29 MGas/s    23809523.810 ops/s           42 ns/op          140 CPU cycles (approx)
SHA256 -  64 bytes            84 gas    1584.91 MGas/s    18867924.528 ops/s           53 ns/op          176 CPU cycles (approx)
SHA256 -  96 bytes            96 gas    1777.78 MGas/s    18518518.519 ops/s           54 ns/op          179 CPU cycles (approx)
SHA256 - 128 bytes           108 gas    1333.33 MGas/s    12345679.012 ops/s           81 ns/op          267 CPU cycles (approx)
SHA256 - 160 bytes           120 gas    1481.48 MGas/s    12345679.012 ops/s           81 ns/op          268 CPU cycles (approx)
SHA256 - 192 bytes           132 gas    1233.64 MGas/s     9345794.393 ops/s          107 ns/op          353 CPU cycles (approx)
SHA256 - 224 bytes           144 gas    1321.10 MGas/s     9174311.927 ops/s          109 ns/op          359 CPU cycles (approx)
SHA256 - 256 bytes           156 gas    1130.43 MGas/s     7246376.812 ops/s          138 ns/op          454 CPU cycles (approx)
--------------------------------------------------------------------------------------------------------------------------------
BN254_G1ADD                  150 gas      87.41 MGas/s      582750.583 ops/s         1716 ns/op         5652 CPU cycles (approx)
BN254_G1MUL                 6000 gas     229.66 MGas/s       38276.047 ops/s        26126 ns/op        86050 CPU cycles (approx)
--------------------------------------------------------------------------------------------------------------------------------
BN254_PAIRINGCHECK 1       79000 gas     166.99 MGas/s        2113.754 ops/s       473092 ns/op      1558009 CPU cycles (approx)
BN254_PAIRINGCHECK 2      113000 gas     191.99 MGas/s        1699.056 ops/s       588562 ns/op      1938370 CPU cycles (approx)
BN254_PAIRINGCHECK 3      147000 gas     183.15 MGas/s        1245.930 ops/s       802613 ns/op      2642801 CPU cycles (approx)
BN254_PAIRINGCHECK 4      181000 gas     191.76 MGas/s        1059.434 ops/s       943900 ns/op      3108745 CPU cycles (approx)
BN254_PAIRINGCHECK 5      215000 gas     169.72 MGas/s         789.374 ops/s      1266827 ns/op      4171120 CPU cycles (approx)
BN254_PAIRINGCHECK 6      249000 gas     181.10 MGas/s         727.321 ops/s      1374909 ns/op      4528210 CPU cycles (approx)
BN254_PAIRINGCHECK 7      283000 gas     189.03 MGas/s         667.965 ops/s      1497084 ns/op      4930714 CPU cycles (approx)
BN254_PAIRINGCHECK 8      317000 gas     204.18 MGas/s         644.095 ops/s      1552566 ns/op      5113680 CPU cycles (approx)
--------------------------------------------------------------------------------------------------------------------------------
BLS12_G1ADD                  500 gas     164.10 MGas/s      328191.664 ops/s         3047 ns/op        10034 CPU cycles (approx)
BLS12_G2ADD                  800 gas     161.75 MGas/s      202183.583 ops/s         4946 ns/op        16289 CPU cycles (approx)
BLS12_G1MUL                12000 gas     141.66 MGas/s       11805.400 ops/s        84707 ns/op       279001 CPU cycles (approx)
BLS12_G2MUL                45000 gas     325.51 MGas/s        7233.639 ops/s       138243 ns/op       455333 CPU cycles (approx)
BLS12_MAP_FP_TO_G1          5500 gas     161.82 MGas/s       29422.149 ops/s        33988 ns/op       111947 CPU cycles (approx)
BLS12_MAP_FP2_TO_G2        75000 gas     659.96 MGas/s        8799.486 ops/s       113643 ns/op       374305 CPU cycles (approx)
--------------------------------------------------------------------------------------------------------------------------------
BLS12_PAIRINGCHECK 1      108000 gas     216.83 MGas/s        2007.665 ops/s       498091 ns/op      1640562 CPU cycles (approx)
BLS12_PAIRINGCHECK 2      151000 gas     222.00 MGas/s        1470.214 ops/s       680173 ns/op      2240287 CPU cycles (approx)
BLS12_PAIRINGCHECK 3      194000 gas     219.98 MGas/s        1133.901 ops/s       881911 ns/op      2904762 CPU cycles (approx)
BLS12_PAIRINGCHECK 4      237000 gas     222.97 MGas/s         940.782 ops/s      1062946 ns/op      3500927 CPU cycles (approx)
BLS12_PAIRINGCHECK 5      280000 gas     221.08 MGas/s         789.576 ops/s      1266502 ns/op      4171417 CPU cycles (approx)
BLS12_PAIRINGCHECK 6      323000 gas     223.09 MGas/s         690.679 ops/s      1447851 ns/op      4768780 CPU cycles (approx)
BLS12_PAIRINGCHECK 7      366000 gas     222.28 MGas/s         607.311 ops/s      1646603 ns/op      5423299 CPU cycles (approx)
BLS12_PAIRINGCHECK 8      409000 gas     221.94 MGas/s         542.640 ops/s      1842844 ns/op      6069597 CPU cycles (approx)
--------------------------------------------------------------------------------------------------------------------------------
BLS12_G1MSM   2            21312 gas     120.40 MGas/s        5649.430 ops/s       177009 ns/op       583004 CPU cycles (approx)
BLS12_G1MSM   4            30768 gas     101.53 MGas/s        3299.960 ops/s       303034 ns/op       998108 CPU cycles (approx)
BLS12_G1MSM   8            43488 gas      81.23 MGas/s        1867.787 ops/s       535393 ns/op      1763434 CPU cycles (approx)
BLS12_G1MSM  16            64128 gas      66.43 MGas/s        1035.864 ops/s       965378 ns/op      3179510 CPU cycles (approx)
BLS12_G1MSM  32           103296 gas      57.99 MGas/s         561.362 ops/s      1781382 ns/op      5867248 CPU cycles (approx)
BLS12_G1MSM  64           170496 gas      50.89 MGas/s         298.504 ops/s      3350039 ns/op     11034035 CPU cycles (approx)
BLS12_G1MSM 128           267264 gas      42.24 MGas/s         158.035 ops/s      6327700 ns/op     20841720 CPU cycles (approx)
--------------------------------------------------------------------------------------------------------------------------------
BLS12_G2MSM   2            79920 gas     269.62 MGas/s        3373.637 ops/s       296416 ns/op       976301 CPU cycles (approx)
BLS12_G2MSM   4           115380 gas     225.12 MGas/s        1951.109 ops/s       512529 ns/op      1688121 CPU cycles (approx)
BLS12_G2MSM   8           163080 gas     177.21 MGas/s        1086.654 ops/s       920256 ns/op      3031066 CPU cycles (approx)
BLS12_G2MSM  16           240480 gas     130.56 MGas/s         542.920 ops/s      1841892 ns/op      6066436 CPU cycles (approx)
BLS12_G2MSM  32           387360 gas     126.36 MGas/s         326.195 ops/s      3065648 ns/op     10097244 CPU cycles (approx)
BLS12_G2MSM  64           639360 gas     118.26 MGas/s         184.965 ops/s      5406423 ns/op     17807268 CPU cycles (approx)
BLS12_G2MSM 128          1002240 gas     100.70 MGas/s         100.471 ops/s      9953136 ns/op     32782906 CPU cycles (approx)
--------------------------------------------------------------------------------------------------------------------------------
</code></pre>
<p>Constantine achieves over 200Mgas/s for a wide range of cryptographic precompiles on a laptop CPU with restricted power consumption (7840U, 15W to 30W)</p>
<p>note, I suggest a repricing for EIP-2537 to help SNARKS applications.</p>
<h2><a class="anchor" href="https://ethresear.ch#security-9" name="security-9"></a>Security</h2>
<p>Constantine, as it names indicates, as a strong focus on security and especially constant-time cryptography is used by default in the core of the library.<br />
It HAS NOT been audited yet, but it has undergone extensive fuzzing by Guido Vranken, thanks to the sponsoring of the Ethereum Foundation in Summer 2023. It has also been added to OSS-Fuzz (<a class="inline-onebox" href="https://github.com/google/oss-fuzz/pull/10710" rel="noopener nofollow ugc">[bls-signatures] Remove Chia, add Constantine by guidovranken · Pull Request #10710 · google/oss-fuzz · GitHub</a>), the Google 24/7 open-source fuzzing initiative.</p>
<h2><a class="anchor" href="https://ethresear.ch#the-future-10" name="the-future-10"></a>The Future</h2>
<p>Constantine will follow and support future Ethereum cryptographic needs. In particular I thank the Ethereum Foundation Fellowship Program and Status for sponsoring work on implementing Verkle Tries in Constantine the past year.</p>
<p>Constantine also supports accelerating Zero-Knowledge proof systems, for example it is possible to use it through PSE (Privacy Scaling Explorations, a branch of the EF) Halo2: <a class="inline-onebox" href="https://github.com/mratsim/constantine/pull/308" rel="noopener nofollow ugc">ZAL: ZK Accel Layer by mratsim · Pull Request #308 · mratsim/constantine · GitHub</a>.</p>
<p>Constantine is has the fastest MSM on x86, all libraries benchmarked as of July 2024 (Arkworks, Barretenberg, Bellman, Gnark, Halo2) and by a factor 2x over popular Rust libraries Arkworks and Halo2. And I do plan to build proof systems on top.</p>
<p>Hidden in Constantine is a compiler for GPU code generation and there are plans for accelerating ARM.</p>
<p>Now I don’t know what a snarkified EVM will look like, but I certainly hope to contribute to make it a reality.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/releasing-constantine-v0-1-0-a-modular-cryptography-stack-for-ethereum/19990">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sat, 06 Jul 2024 11:01:41 +0000</pubDate>
</item>
<item>
<title>Reputation-Centric Light Client Framework for Optimistic Rollups</title>
<link>https://ethresear.ch/t/reputation-centric-light-client-framework-for-optimistic-rollups/19988</link>
<guid>https://ethresear.ch/t/reputation-centric-light-client-framework-for-optimistic-rollups/19988</guid>
<content:encoded><![CDATA[
<div> 关键词：声誉系统、乐观rollup、Herodotus数据处理器、存储证明、快速最终性

总结:<br />
本文提出了一种基于声誉的轻客户端框架，旨在优化乐观rollup（ORU）的性能，特别是缩短最终性时间。框架核心是Herodotus数据处理器，通过计算sequencer的历史行为信誉分数，如无挑战记录和输出根的有效性，允许轻客户端信任信誉良好的sequencer的输出根，无需等待完整的争议期。系统还包括惩罚机制和备份机制，以确保安全性和可靠性。该方法有望改善用户体验并推动L2生态发展。 <div>
<p>Authors: <a href="https://x.com/0xmarcello" rel="noopener nofollow ugc">Marcello Bardus</a> (<a href="https://x.com/HerodotusDev" rel="noopener nofollow ugc">Herodotus</a>), <a href="https://x.com/kacperkozi" rel="noopener nofollow ugc">Kacper Koziol</a> (<a href="https://x.com/HerodotusDev" rel="noopener nofollow ugc">Herodotus</a>)</p>
<p>Thanks for early feedback: <a href="https://x.com/bonustrack87" rel="noopener nofollow ugc">bonustrack87</a> (<a href="https://x.com/SnapshotLabs" rel="noopener nofollow ugc">Snapshot Labs</a>), <a href="https://x.com/lsukernik" rel="noopener nofollow ugc">Larry Sukernik</a> (<a href="https://x.com/hi_reverie" rel="noopener nofollow ugc">Reverie</a>), <a href="https://x.com/piapark_eth" rel="noopener nofollow ugc">Pia Park</a> (<a href="https://x.com/HerodotusDev" rel="noopener nofollow ugc">Herodotus</a>), <a href="https://x.com/wojtekwtf" rel="noopener nofollow ugc">Wojtek</a> (<a href="https://www.supercast.xyz/" rel="noopener nofollow ugc">Supercast</a>),</p>
<h1><a class="anchor" href="https://ethresear.ch#summary-1" name="summary-1"></a>Summary:</h1>
<p>This post introduces a conceptual framework for a reputation-centric light client system designed to address critical challenges in Optimistic Rollups (ORUs), with a primary focus on enabling fast finality for accessing ORU data from Ethereum, ORUs and other Ethereum layers. At its core, the system leverages the Herodotus Data Processor to compute sequencer reputation scores based on the sequencer’s historical behavior, including their track record of submitting valid output roots and avoiding successful challenges. This allows light clients to trust output roots only from sequencers with an impeccable track record without waiting for the full dispute period. This approach significantly reduces finality time while maintaining security. The framework includes a punitive measure that resets a sequencer’s reputation upon successful challenges, ensuring system integrity. Additionally, a fallback mechanism reverts to the standard seven-day dispute period in cases of unresolved conflicts or detected irregularities.</p>
<h3><a class="anchor" href="https://ethresear.ch#reputation-centric-light-client-framework-for-optimistic-rollups-2" name="reputation-centric-light-client-framework-for-optimistic-rollups-2"></a>Reputation-Centric Light Client Framework for Optimistic Rollups</h3>
<p>Optimistic Rollups have seen significant adoption, however, they encounter several challenges, particularly in terms of finality time and data verification. This post introduces a conceptual framework for a reputation-centric light client system that aims to address these issues, enabling fast finality for accessing ORU data from Ethereum, and from other Ethereum layers.</p>
<p>OP Stack and other Optimistic Rollups (ORUs) have a security model based on fraud proofs. In this model, anyone can act as a sequencer, also known as a proposer. The sequencer first proposes the rollup state to Layer 1 (L1), after which a seven-day window is opened. During this period, anyone can challenge the correctness of the proposed state.</p>
<p>In ORU implementations such as OP Stack, proposers periodically submit output roots to L1. These output roots are a hash of certain L2 state information, including the state root, block number, and timestamp of the latest L2 block. OP Stack incorporates a specification for a fault proof system with bonding, which creates incentives for proposers to submit correct output roots.</p>
<p>This mechanism imposes a long finality time for ORUs, which is problematic for Storage Proofs, a secure on-chain data access solution that Herodotus has previously developed for Optimism and several other ORU ecosystems. This is especially problematic following recent upgrades that introduced permissionless fraud proofs on Optimism. With these upgrades, no assumptions can be made about where a valid state root can be found.</p>
<h2><a class="anchor" href="https://ethresear.ch#secure-data-access-and-processing-3" name="secure-data-access-and-processing-3"></a>Secure Data Access and Processing</h2>
<p>The framework incorporates two crucial components:</p>
<h3><a class="anchor" href="https://ethresear.ch#storage-proofs-4" name="storage-proofs-4"></a>Storage Proofs</h3>
<p>Storage Proofs are a secure on-chain data access primitive utilized by the Herodotus Data Processor that enables the cryptographic proving of the provenance of on-chain data. They allow for the verification of any data available on Ethereum, including current and historical balances, transactions, user interactions, liquidations, and more. Storage Proofs also enable the trustless and secure reading of data from arbitrary Ethereum Layers.</p>
<p>By utilizing Storage Proofs, the Herodotus Data Processor can ensure the integrity and authenticity of the on-chain data it processes, providing a foundation of trust for its computations.</p>
<h3><a class="anchor" href="https://ethresear.ch#data-processing-component-5" name="data-processing-component-5"></a>Data Processing Component</h3>
<p>This would leverage the Herodotus Data Processor (HDP) to compute sequencer reputation scores efficiently. HDP can be thought of as a zk-coprocessor, capable of performing computations on verified data. Storage Proofs guarantee the integrity of the input data to HDP. Custom computations can be defined using HDP Modules, which can later process the verified historical data and update reputation scores based on the defined criteria, such as the consistency of avoiding challenges and the validity of proposed output roots over time.</p>
<h2><a class="anchor" href="https://ethresear.ch#reputation-based-light-client-6" name="reputation-based-light-client-6"></a>Reputation based light client</h2>
<p>In our design, a sequencer, identified by an Ethereum address, would be assumed to be the most trustworthy based on the following criteria:</p>
<ul>
<li>The sequencer consistently avoids challenges, or any initiated challenges against them are unsuccessful.</li>
<li>The validity of the sequencer’s proposed output roots over time, as proven by the lack of successful fault proofs against their outputs.</li>
</ul>
<p>In OP Stack implementations like Bedrock, and potentially in other ORUs, output roots represent a compact summary of the L2 state at a specific block. These output roots are not Merkle roots of the entire canonical L2 chain, but rather a hash of certain L2 state information. Bonded proposers periodically submit these output roots to L1.</p>
<p>The output root typically includes a hash of the following information:</p>
<ol>
<li>The state root of the L2 block</li>
<li>The L2 block number</li>
<li>The timestamp of the L2 block</li>
<li>The hash of the L2 block itself</li>
</ol>
<p>This structure allows for efficient verification of specific L2 state information without requiring the entire L2 chain data on L1.</p>
<p>Once a highly reputable sequencer posts a claimed output root to L1, a Light Client contract would assume the claim is valid and treat it as final. This approach ensures that only sequencers with an impeccable track record are trusted, significantly reducing the finality time while relying on the cryptographically proven historical reliability of the sequencer rather than waiting for the full dispute period.</p>
<p>The Light Client contract would store the full output roots proposed by reputable sequencers, not just the block hash, enabling trustless proof of claims like withdrawals against the output roots directly.</p>
<p>The reputation of the sequencer can be periodically updated using the Herodotus Data Processor. This involves assessing historical data to ensure the sequencer continues to meet the criteria of reliability and activity. By continuously evaluating the sequencer’s performance and updating their reputation at fixed intervals, the Light Client can maintain a high level of trust and accuracy in the state roots it accepts.</p>
<h2><a class="anchor" href="https://ethresear.ch#system-architecture-7" name="system-architecture-7"></a>System Architecture</h2>
<h2><a class="anchor" href="https://ethresear.ch#h-326x4111935069506181uploadpmywafztk4c8h3awxejv5ig72gppng-8" name="h-326x4111935069506181uploadpmywafztk4c8h3awxejv5ig72gppng-8"></a><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/4/b4b3807ab51eeeb259a9c7c08890b13cd0910b23.png" title="|326x411.1935069506181"><img alt="|326x411.1935069506181" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/b/4/b4b3807ab51eeeb259a9c7c08890b13cd0910b23_2_396x500.png" width="396" /></a></div></h2>
<p>The proposed system would operate as follows:</p>
<ol>
<li>Proposers submit output root proposals to the appropriate ORU contracts on L1, based on the state of the ORU L2 chain.</li>
<li>The ORU L1 contracts handle both output root proposals and challenges/fault proofs against these proposals.</li>
<li>The Herodotus Data Processor retrieves and processes data from the ORU L1 contracts, including output root proposals and challenges/fault proofs.</li>
<li>The reputation-based light client contract uses the processed data from the Herodotus Data Processor to track sequencer reputation scores and store trusted output roots. A custom reputation calculation formula can be implemented, allowing for flexible and adaptable assessment of sequencer reliability based on various factors and weighting systems as deemed appropriate for the specific ORU implementation.</li>
<li>The light client interface allows other contracts to query the state root of the L2 chain based on the most reputable sequencer’s output roots.</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#handling-successful-challenges-9" name="handling-successful-challenges-9"></a>Handling Successful Challenges</h2>
<p>In the event that any challenge against a sequencer is successful, the reputation of the sequencer would immediately reset to zero in the light client. This punitive measure ensures that only sequencers with an impeccable track record maintain trusted status.</p>
<p>With fault proof systems like those in OP Stack’s Bedrock, the Light Client contract would automatically reset a sequencer’s reputation to zero if a fault proof is successfully submitted and verified, showing an invalid output root proposed by that sequencer. This automated process ensures swift and consistent enforcement of the reputation system.</p>
<p>The permissionless output proposal mechanism provides an objective way to track sequencer reputation over time and identify potentially malicious outputs. Simultaneously, the output roots proposed by sequencers enable the verification of Storage Proofs against these proposed L2 state roots when using the Light Client. Ultimately, this approach creates a self-regulating system that not only incentivizes honest behavior but also ensures quick penalization of any attempts at fraud, thereby maintaining the overall reliability and security of the network.</p>
<h3><a class="anchor" href="https://ethresear.ch#fallback-mechanism-10" name="fallback-mechanism-10"></a>Fallback Mechanism</h3>
<p>In cases of unresolved conflicts or when the system detects any irregularities, it would automatically fall back to the conservative seven-day dispute period. This would ensure that the system remains secure and trustworthy, even in the face of unexpected challenges or disagreements among reputable sequencers.</p>
<h2><a class="anchor" href="https://ethresear.ch#potential-impact-and-future-directions-11" name="potential-impact-and-future-directions-11"></a>Potential Impact and Future Directions</h2>
<p>We believe that this reputation-based light client framework has the potential to significantly decrease duration to finality for ORUs. By reducing finality times while maintaining security, it could substantially improve the user experience and enable new use cases in L2 ecosystems.</p>
<p>As we continue to explore and refine this concept, we welcome input from the community. The next steps would involve further theoretical analysis, simulations, and potentially, prototype implementations.</p>
<h1><a class="anchor" href="https://ethresear.ch#references-12" name="references-12"></a>References</h1>
<p>Optimism Bedrock Documentation: <a class="inline-onebox" href="https://community.optimism.io/docs/developers/bedrock" rel="noopener nofollow ugc">Bedrock Explainer | Optimism Docs</a></p>
<p>L2 Output Root Proposals Specification: <a class="inline-onebox" href="https://github.com/ethereum-optimism/optimism/blob/65ec61dde94ffa93342728d324fecf474d228e1f/specs/proposals.md" rel="noopener nofollow ugc">optimism/specs/proposals.md at 65ec61dde94ffa93342728d324fecf474d228e1f · ethereum-optimism/optimism · GitHub</a></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/reputation-centric-light-client-framework-for-optimistic-rollups/19988">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 05 Jul 2024 22:01:41 +0000</pubDate>
</item>
<item>
<title>Protocol Asset: canonical tokenized asset with the most social consensus preference of protocol</title>
<link>https://ethresear.ch/t/protocol-asset-canonical-tokenized-asset-with-the-most-social-consensus-preference-of-protocol/19983</link>
<guid>https://ethresear.ch/t/protocol-asset-canonical-tokenized-asset-with-the-most-social-consensus-preference-of-protocol/19983</guid>
<content:encoded><![CDATA[
<div> 关键词：Protocol Asset、社交共识、兼容性、Tokenized、价值体现

总结:
Protocol Asset是一种新型概念，它代表了开放协议或社区标准中最受欢迎的社会共识首选的代币化资产。这类资产如ORDI（Ordinal协议的代表）和Pandora（ERC-404的代表），特点是被广泛接受、完全兼容其底层协议，并承载着该协议的价值。它们不仅是理论与实践的结合体，而且其价值和安全性依赖于社会共识，而非仅仅依赖区块链网络。社交层的共识确保了这些资产作为各自标准的权威象征。 <div>
<p>Idea initiated by <a href="https://x.com/0xozeth" rel="noopener nofollow ugc">0xOZ.eth</a>. Thanks <a href="https://twitter.com/mkkb2156" rel="noopener nofollow ugc">Makd</a> for discussion.</p>
<p>We introduced a new concept: Protocol Asset. Protocol Asset represents canonical tokenized asset with the most social consensus preference of open protocol or community standard.</p>
<p>For example, ORDI is the protocol asset of Ordinal protocol, and Pandora is the protocol asset of ERC-404.</p>
<h2><a class="anchor" href="https://ethresear.ch#background-1" name="background-1"></a>Background</h2>
<p>The evolution of blockchain and crypto has led to the emergence of various standards and protocols, each designed to address specific challenges or enable new functionalities. Notable examples include the Ethereum Improvement Proposals (EIPs) and Ethereum Request for Comments (ERCs) standards, and Ordinal theory with its Inscription protocol.</p>
<p>Despite the establishment of these standards, identifying the materialized asset associated with an open standard remains challenging. For instance, even in the case of ERC standards, which often include a reference implementation, pinpointing a specific instance deployed based on this implementation is not straightforward.</p>
<h2><a class="anchor" href="https://ethresear.ch#concept-2" name="concept-2"></a>Concept</h2>
<p>A Protocol Asset represents a canonical tokenized asset that aligns with the most socially accepted preferences of an open protocol or community standard.</p>
<h3><a class="anchor" href="https://ethresear.ch#characteristics-of-protocol-assets-3" name="characteristics-of-protocol-assets-3"></a>Characteristics of Protocol Assets</h3>
<p>To qualify as a Protocol Asset, it must be:</p>
<ul>
<li><strong>Tokenized and Ownable</strong>: The asset should be a materialized token that can be owned.</li>
<li><strong>Fully Compatible</strong>: The asset must be entirely compatible with the underlying protocol and standard.</li>
<li><strong>Socially Favored</strong>: The asset should be canonically preferred from a social consensus perspective.</li>
</ul>
<p>Typically, the Protocol Asset is the first implementation or instance of the protocol or standard.</p>
<h3><a class="anchor" href="https://ethresear.ch#value-encapsulation-4" name="value-encapsulation-4"></a>Value Encapsulation</h3>
<p>Protocol Assets encapsulate the value inherent in the protocols and standards they represent. They serve as the tangible manifestation of the protocol’s principles, ensuring that the theoretical and practical aspects of the protocol are embodied in a specific, widely recognized asset.</p>
<h3><a class="anchor" href="https://ethresear.ch#social-consensus-and-security-5" name="social-consensus-and-security-5"></a>Social Consensus and Security</h3>
<p>It’s crucial to understand that the security and recognition of token standards, such as ERC-20, are fundamentally based on social consensus rather than being directly secured by the Ethereum network. As Dankrad Feist aptly pointed out, “You have been lied to about ERC-20s. They aren’t secured by Ethereum. It’s just social consensus; any ERC-20 community can just fork away.”</p>
<p>This highlights that the value and trust in these tokens derive from the community’s agreement and collective support. This social layer of consensus is what ultimately secures the token, making it the canonical representation of its respective protocol.</p>
<h2><a class="anchor" href="https://ethresear.ch#examples-6" name="examples-6"></a>Examples</h2>
<h3><a class="anchor" href="https://ethresear.ch#ordi-7" name="ordi-7"></a>ORDI</h3>
<p>ORDI is the Protocol Asset of the Ordinal protocol. It represents the canonical tokenized asset for this protocol, favored by social consensus and adhering to the standards set forth by the Ordinal protocol.</p>
<h3><a class="anchor" href="https://ethresear.ch#pandora-8" name="pandora-8"></a>Pandora</h3>
<p>Pandora serves as the Protocol Asset for ERC-404. As with ORDI, Pandora is the materialized token that aligns with the social consensus and standards of the ERC-404 protocol, representing its values and functionalities in a tangible form.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/protocol-asset-canonical-tokenized-asset-with-the-most-social-consensus-preference-of-protocol/19983">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 05 Jul 2024 17:54:05 +0000</pubDate>
</item>
<item>
<title>Gossipsub Message Propagation Latency</title>
<link>https://ethresear.ch/t/gossipsub-message-propagation-latency/19982</link>
<guid>https://ethresear.ch/t/gossipsub-message-propagation-latency/19982</guid>
<content:encoded><![CDATA[
<div> 关键词：Gossipsub、Ethereum、Message propagation latency、Hermes、Lodestar

总结:
研究团队ProbeLab通过工具Hermes对Ethereum P2P网络中的Gossipsub消息传播延迟进行了调查，以确定哪些协议组件消耗了最大的网络带宽。结果显示，98%的消息能在4秒内送达，而Lodestar客户端的接收时间相对较慢，但可能与其特定实现有关。节点位置靠近网络核心的节点通常能更快接收消息，但过度地理集中可能会加剧这种差异。尽管如此，总体上各节点的表现符合4秒内的要求，显示出网络的稳定性。 <div>
<h1><a class="anchor" href="https://ethresear.ch#summary-and-tldr-1" name="summary-and-tldr-1"></a>Summary and TL;DR</h1>
<p>The ProbeLab team (<a href="https://probelab.io/" rel="noopener nofollow ugc">probelab.io </a>) is carrying out a study on the performance of Gossipsub in Ethereum’s P2P network. Following from our previous post on the <a href="https://ethresear.ch/t/ethereum-node-message-propagation-bandwidth-consumption/19952">Ethereum Node Message Propagation Bandwidth Consumption</a>, in this post we investigate <strong>Gossipsub’s message propagation latency</strong>, i.e., how long it takes to have a message delivered to all nodes in the network. The target of the study is to identify the protocol components that consume the biggest share of network bandwidth. The study has been co-authored by <a class="mention" href="https://ethresear.ch/u/cortze">@cortze</a> and <a class="mention" href="https://ethresear.ch/u/yiannisbot">@yiannisbot</a>.</p>
<p>For the purposes of this study, we have built a tool called <strong>Hermes, which acts as a GossipSub listener and tracer</strong> (<a href="https://github.com/probe-lab/hermes/" rel="noopener nofollow ugc">GitHub - probe-lab/hermes: A Gossipsub listener and tracer.</a>). Hermes subscribes to all relevant pubsub topics and traces all protocol interactions.</p>
<p><strong>Study Description:</strong> Message propagation and arrival times are sensitive metrics for blockchain networks. We assume that the message is going to arrive to each peer “as fast as possible”, but in some cases, just because the core of the network achieved fast message delivery time, doesn’t mean that  message propagation to the entire network was done in time.</p>
<p>In the particular case of Ethereum, with such strict message delivery deadlines, ensuring the messages arrive within a 4-second window is essential to reduce the probability of forks.</p>
<p>In this study, we will approximate the average message propagation latency throughout the whole network.</p>
<p><strong>TL;DR:</strong> Despite a relatively short dataset of 3 days, we could observe with high confidence that:</p>
<ul>
<li>98% of messages arrive prior to the 4-second mark.</li>
<li>Lodestar seems to be the slowest client in terms of message arrival time, although this could also be related to when the arrivals are traced in the particular implementation.</li>
<li>Nodes located in or near the core of the network (<a href="https://probelab.io/ethereum/discv5/2024-25/#geolocation" rel="noopener nofollow ugc">NA or EU</a>) do have certain advantages when it comes to receiving messages sooner. Although the traced locations do not show any worrying behaviour, it is worth pointing out that extra geographical centralization could exacerbate the differences even further.</li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#results-2" name="results-2"></a>Results</h1>
<p>The results in this report have been gathered from EF’s Xatu public datasets. We’ve fetched 3 days’ worth of data from the <code>beacon_api_eth_v2_beacon_block</code> table (from the 14th to the 16th of June).</p>
<h2><a class="anchor" href="https://ethresear.ch#arrival-cdf-times-within-the-slot-3" name="arrival-cdf-times-within-the-slot-3"></a>Arrival CDF times within the slot</h2>
<p>The study starts by calculating the arrival time of all the blocks within the slot that they belong to. The calculation is done based on the slot number and the time since genesis, given that each slot lasts 12 seconds:</p>
<pre><code class="lang-go">time_within_slot = arrival_time - (genesis_time + (slot * 12))
</code></pre>
<p>This measurement is crucial, as any block arrival beyond the 4 second mark is likely to generate a fork in some part of the network (as it can start receiving attestations of a non-proposed block).</p>
<p>In this first graph, we observe that 98% of the blocks arrived within the 4-second mark, leaving only the remaining 2% of blocks exceeding it.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/8/d8861bec5a5ad5613b752e126153afffcd236c23.png" title="CDF-propagation-latency"><img alt="CDF-propagation-latency" height="309" src="https://ethresear.ch/uploads/default/optimized/3X/d/8/d8861bec5a5ad5613b752e126153afffcd236c23_2_517x309.png" width="517" /></a></div><p></p>
<p>The data was originated from the sentry nodes that are under the control of the Ethereum Foundation. These nodes include all the main client implementations in each of the locations, as shown in the following table:</p>
<div class="md-table">
<table>
<thead>
<tr>
<th>Continent</th>
<th>Country</th>
<th>Client</th>
</tr>
</thead>
<tbody>
<tr>
<td>EU</td>
<td>FI</td>
<td>lighthouse</td>
</tr>
<tr>
<td></td>
<td></td>
<td>lodestar</td>
</tr>
<tr>
<td></td>
<td></td>
<td>nimbus</td>
</tr>
<tr>
<td></td>
<td></td>
<td>prysm</td>
</tr>
<tr>
<td></td>
<td></td>
<td>teku</td>
</tr>
<tr>
<td>NA</td>
<td>US</td>
<td>lighthouse</td>
</tr>
<tr>
<td></td>
<td></td>
<td>lodestar</td>
</tr>
<tr>
<td></td>
<td></td>
<td>nimbus</td>
</tr>
<tr>
<td></td>
<td></td>
<td>prysm</td>
</tr>
<tr>
<td></td>
<td></td>
<td>teku</td>
</tr>
<tr>
<td>OC</td>
<td>AU</td>
<td>lighthouse</td>
</tr>
<tr>
<td></td>
<td></td>
<td>lodestar</td>
</tr>
<tr>
<td></td>
<td></td>
<td>nimbus</td>
</tr>
<tr>
<td></td>
<td></td>
<td>prysm</td>
</tr>
<tr>
<td></td>
<td></td>
<td>teku</td>
</tr>
</tbody>
</table>
</div><p>When comparing the arrival times over the different sentry nodes (figure below), we do see slight differences between them. The exception of <code>Lodestar</code> catches our attention, as it has a less uniform tail in its distribution. However, the rest of the clients follow a similar trend, with 99% of messages arriving within the first 4 seconds.</p>
<p>Since this data has been collected from the standard <a href="https://ethereum.github.io/beacon-APIs/#/Events/eventstream" rel="noopener nofollow ugc">event streamer Beacon API</a>, it is hard to explain the differences within each of the client implementations, as not only the libp2p codebase is written in different languages, but the message arrivals could also be timestamped at different moments of the message validation logic.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/0/60a95219771e0ec9bdc64265b87b0054bc877b83.png" title="gossipsub_arrival_times_within_slot_by_agent_on_mainnet_beacon_block"><img alt="gossipsub_arrival_times_within_slot_by_agent_on_mainnet_beacon_block" height="309" src="https://ethresear.ch/uploads/default/optimized/3X/6/0/60a95219771e0ec9bdc64265b87b0054bc877b83_2_517x309.png" width="517" /></a></div><p></p>
<p>We were expecting to see different arrival times from different geographic locations, as the network geographical distribution seems to be concentrated within European and North American countries (<a href="https://probelab.io/ethereum/discv5/2024-24/#geolocation" rel="noopener nofollow ugc">link to the distribution</a>). The following graphs show that although there are indeed differences between countries or continents, they are minimal, with all the CDF distributions showing 98-99% of the block arrivals completing within the 4-second mark.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/e/bec7e4974200c56b0e35fe8e118e90b97ea4b873.png" title="gossipsub_arrival_times_within_slot_on_by_country_mainnet_beacon_block"><img alt="gossipsub_arrival_times_within_slot_on_by_country_mainnet_beacon_block" height="309" src="https://ethresear.ch/uploads/default/optimized/3X/b/e/bec7e4974200c56b0e35fe8e118e90b97ea4b873_2_517x309.png" width="517" /></a></div><p></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/d/bdd5349b57b1f35d0484f1d82e09c9bea1c358fe.png" title="gossipsub_arrival_times_within_slot_on_by_continent_mainnet_beacon_block"><img alt="gossipsub_arrival_times_within_slot_on_by_continent_mainnet_beacon_block" height="309" src="https://ethresear.ch/uploads/default/optimized/3X/b/d/bdd5349b57b1f35d0484f1d82e09c9bea1c358fe_2_517x309.png" width="517" /></a></div><p></p>
<h2><a class="anchor" href="https://ethresear.ch#arrival-times-4" name="arrival-times-4"></a>Arrival times</h2>
<p>The previous CDFs show that almost all the block arrivals happen within the expected time range. However, the plots do not reveal outliers, as CDFs are not sensitive to sudden network perturbations.</p>
<p>Thus, the following graphs show the <code>maximum</code>, <code>median</code>, <code>mean</code>, and <code>minimum</code> block arrival times on time windows of 4 epochs (1536 seconds).</p>
<p>We do not find large variations in the <code>minimum</code>, <code>mean</code> and the <code>median</code> distributions over the 3 day period. However, we do see that the maximum arrival time does vary quite significantly. We can observe that arrival times vary from 4 seconds to almost 12 seconds, almost exceeding the entire slot duration.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/2/92b9349d287acecc0903a14950d4876f1df18370.png" title="msg-arrival-overall"><img alt="msg-arrival-overall" height="309" src="https://ethresear.ch/uploads/default/optimized/3X/9/2/92b9349d287acecc0903a14950d4876f1df18370_2_517x309.png" width="517" /></a></div><p></p>
<p>Interestingly, there are differences when comparing the mean arrival times of the different client implementations. Lodestar seems to be the latest one receiving the messages in the mesh and presents quite a high variance, while Teku seems to be the one receiving the messages first, followed by Prysm.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/6/c65b77648c36d9692a84fa65efd21ac3043aadd9.png" title="msg-arrival-by-agent"><img alt="msg-arrival-by-agent" height="309" src="https://ethresear.ch/uploads/default/optimized/3X/c/6/c65b77648c36d9692a84fa65efd21ac3043aadd9_2_517x309.png" width="517" /></a></div><p></p>
<p>A similar pattern is also observed for the arrival time distribution by continent. As we could anticipate, European nodes receive slightly sooner messages than the North American and the Oceania ones. Although the difference is not significant, 0.6 seconds still keeps the arrival within the safety margins. However, this still showcases that there are some latency incentives to locate nodes in regions with lower latency, or in other words, around the core of the network (which, however, will, in turn, lead to more geographic centralization).</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/f/bf6284e07ea3c5f9c4473be8b629d95b514371ed.png" title="msg-arrival-by-continent"><img alt="msg-arrival-by-continent" height="309" src="https://ethresear.ch/uploads/default/optimized/3X/b/f/bf6284e07ea3c5f9c4473be8b629d95b514371ed_2_517x309.png" width="517" /></a></div><p></p>
<h2><a class="anchor" href="https://ethresear.ch#correlation-between-arrival-times-and-size-of-the-messages-5" name="correlation-between-arrival-times-and-size-of-the-messages-5"></a>Correlation between arrival times and size of the messages</h2>
<p>When attempting to correlate our findings to ones described in the previous <a href="http://ethresear.ch">ethresear.ch</a> <a href="https://ethresear.ch/t/big-block-diffusion-and-organic-big-blocks-on-ethereum/17346">blog post</a> that investigated this issue in particular, we haven’t been able to see any major correlation between size and the arrival time of the blocks. Although the block size distribution achieved in three days isn’t fully representative, the following graph shows that most blocks stay within the 50KB to 150KB range with a similar arrival time of 1 to 3 seconds.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/0/9/0905bc564c76545c3e9fc2c983edcbc280925d47.png" title="msg-arrival-vs-size"><img alt="msg-arrival-vs-size" height="375" src="https://ethresear.ch/uploads/default/optimized/3X/0/9/0905bc564c76545c3e9fc2c983edcbc280925d47_2_375x375.png" width="375" /></a></div><p></p>
<h1><a class="anchor" href="https://ethresear.ch#conclusions-and-takeaways-6" name="conclusions-and-takeaways-6"></a>Conclusions and Takeaways</h1>
<p>Despite a relatively short dataset of 3.5hrs, we could observe with high confidende that:</p>
<ul>
<li>98% of messages arrive prior to the 4-second mark.</li>
<li>Lodestar seems to be the slowest client in terms of message arrival time, although this could also be related to when the arrivals are traced in the particular implementation.</li>
<li>Nodes located in or near the core of the network (<a href="https://probelab.io/ethereum/discv5/2024-25/#geolocation" rel="noopener nofollow ugc">NA or EU</a>) do have certain advantages when it comes to receiving messages sooner. Although the traced locations do not show any worrying behaviour, it is worth pointing out that extra geographical centralization could exacerbate the differences even further.</li>
</ul>
<p>For more details and <strong>weekly network health reports on Ethereum’s discv5 DHT network</strong> head over to <a href="https://probelab.io/" rel="noopener nofollow ugc">probelab.io</a>.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/gossipsub-message-propagation-latency/19982">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 05 Jul 2024 14:03:33 +0000</pubDate>
</item>
<item>
<title>EVM in Motoko for Trustless Execution Environments</title>
<link>https://ethresear.ch/t/evm-in-motoko-for-trustless-execution-environments/19981</link>
<guid>https://ethresear.ch/t/evm-in-motoko-for-trustless-execution-environments/19981</guid>
<content:encoded><![CDATA[
<div> 关键词：Motoko、Internet Computer (IC)、EVM实现、教育目标、微EVMs

总结:<br />
文章介绍了作者管理的组织正在资助一个在Motoko中构建的EVM，目标是运行在Internet Computer上，未来可能扩展到AO领域。项目已通过GG19和GG20资金支持，主要目的是实现无信任执行和共识代理。目前完成了第一个里程碑——算术函数，寻求经验丰富的开发人员提供反馈和优化建议。这个项目还有助于教育，提供EVM工作原理的学习资源，并作为其他链上智能合约的微EVM基础，支持跨链交互。开发者正在GitHub上开源项目文件和测试代码，以促进改进和集成。 <div>
<p>Hello ethResearch <img alt=":wave:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/wave.png?v=12" title=":wave:" width="20" /> it has been a while since I posted. Thanks for your patience as I wade back into the eth universe.</p>
<p>An organization that I’m running called <a href="https://icdevs.org" rel="noopener nofollow ugc">https://icdevs.org</a> is funding an evm built in Motoko that is targeted to run on the Internet Computer(and that we think will slide well into the AO universe as well). We should have started this 3 years ago, but there is no time like the present. To date this work has been funded through <span class="hashtag-raw">#GG19</span> and <span class="hashtag-raw">#GG20</span>. The eventuality of this project is trustless execution and consensus agents and the ability to monitor and relay messages between EVMs(and other chains) in a trustless manner.</p>
<p>The bounty has reached its first milestone and we’re looking for experienced EVM implementors to tell us what we’ve missed and how to make it better. I realize this forum seems to have moved on to bigger and harder scaling challenges, but I’m hoping you all can point me in the right direction to find the right audience. It is a bit too technical for r/ethereum but may be too basic for this forum and not quite an EIP.</p>
<p>Why we are looking to build out an EVM execution layer for the Intenet computer(from our thread at <a class="inline-onebox" href="https://forum.dfinity.org/t/open-icdevs-org-bounty-63-evm-opcodes-motoko-1-9-cketh/27592?u=skilesare" rel="noopener nofollow ugc">Open - ICDevs.org Bounty #63 - EVM OpCodes - Motoko - 1.9 ckETH - Bounties - Internet Computer Developer Forum</a> )</p>
<ol>
<li>The obvious - we can’t build an EVM in motoko without the op-codes. Now building an evm in motoko isn’t particularly a priority at the moment, but long term the Ethereum Foundation has made it a priority to have EVMs in as many languages as possible as a security feature. Would it make sense to have IC canisters as evm nodes for other chains? Probably depends on network config and a few other things, but I could certainly see it being of value long term. Having the op-codes defined separates the execution concerns from any future project that might want to wire up the rest of the EVM machinery. From building from the ground up you get an EVM that takes the IC’s compute pattern and restrictions into account in ways that existing EVMs written in other languages would need significant rewrites to support.</li>
<li>General education - These opcodes are an awesome way to learn about stacks, memories, and crypto primitives. Education is the primary goal of <a href="http://icdevs.org/" rel="noopener nofollow ugc">ICDevs.org </a> and we feel like Motoko versions of these libraries would make a really interesting set of examples for people learning about how EVMs work, why they work, and what concepts mirror over into the IC(and which ones don’t).</li>
<li>Libraries and Integrations - these libraries build on top of a number of other Bounties that we’ve funded that could use some burn-in and integration testing to improve them and make sure they are working properly. <a href="https://github.com/f0i/merkle-patricia-trie.mo" rel="noopener nofollow ugc">GitHub - f0i/merkle-patricia-trie.mo: A Merkle Patricia Trie implementation in Motoko </a> <a href="https://github.com/relaxed04/rlp-motoko" rel="noopener nofollow ugc">GitHub - relaxed04/rlp-motoko: RLP implementation on motoko</a>. In addition, some of the op codes implement core functionality that we’ll need to do cross-chain like ecrecover which would be important for a motoko canister trying to verify a signature from the evm universe.</li>
<li>Micro EVMs - In one universe bitfinity EVMs proliferate and we end up with a garden of highly specialized evms on the IC that interact and interoperate in unique ways. These libraries would allow you to pull in the memory, storage, etc from those EVMs and run transaction simulations to check for opportunities or to automate actions against them using things like the event logs. The always-on nature of IC canisters makes them ideal for writing bots/agents that seek opportunities and execute on them by signing tecdsa messages and relaying them.</li>
</ol>
<p>Our bounty hunter has completed the first milestone, arithmetic functions.</p>
<p>project file:</p><aside class="onebox allowlistedgeneric">
  <header class="source">
      <img class="site-icon" height="32" src="https://ethresear.ch/uploads/default/original/2X/b/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg" width="32" />

      <a href="https://github.com/icdevsorg/evm.mo" rel="noopener nofollow ugc" target="_blank">GitHub</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image"><img class="thumbnail" height="345" src="https://ethresear.ch/uploads/default/optimized/3X/5/4/546c435f8f13ad709696b772052a5b091237e94b_2_690x345.png" width="690" /></div>

<h3><a href="https://github.com/icdevsorg/evm.mo" rel="noopener nofollow ugc" target="_blank">GitHub - icdevsorg/evm.mo: EVM Based Libraries for Motoko</a></h3>

  <p>EVM Based Libraries for Motoko. Contribute to icdevsorg/evm.mo development by creating an account on GitHub.</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both;"></div>
</aside>

<p>main code file:</p><aside class="onebox githubblob">
  <header class="source">

      <a href="https://github.com/icdevsorg/evm.mo/blob/5b3870c2454caf5cb1506010c8ab6796214ff307/src/evm_mo_backend/main.mo" rel="noopener nofollow ugc" target="_blank">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/icdevsorg/evm.mo/blob/5b3870c2454caf5cb1506010c8ab6796214ff307/src/evm_mo_backend/main.mo" rel="noopener nofollow ugc" target="_blank">icdevsorg/evm.mo/blob/5b3870c2454caf5cb1506010c8ab6796214ff307/src/evm_mo_backend/main.mo</a></h4>


      <pre><code class="lang-mo">import Array "mo:base/Array";
import Nat "mo:base/Nat";
import Nat8 "mo:base/Nat8";
import Nat64 "mo:base/Nat64";
import Int "mo:base/Int";
import Trie "mo:base/Trie";
import Iter "mo:base/Iter";
import Debug "mo:base/Debug";
import Vec "mo:vector"; // see https://github.com/research-ag/vector
import Map "mo:map/Map"; // see https://mops.one/map
import EVMStack "evmStack";
import T "types";

module {
  
  type Result&lt;Ok, Err&gt; = { #ok: Ok; #err: Err};
  type Engine = [(T.ExecutionContext, T.ExecutionVariables) -&gt; Result&lt;T.ExecutionVariables, Text&gt;];
  type Vec&lt;X&gt; = Vec.Vector&lt;X&gt;;
  type Map&lt;K, V&gt; = Map.Map&lt;K, V&gt;;
  type Trie&lt;K, V&gt; = Trie.Trie&lt;K, V&gt;;
</code></pre>



  This file has been truncated. <a href="https://github.com/icdevsorg/evm.mo/blob/5b3870c2454caf5cb1506010c8ab6796214ff307/src/evm_mo_backend/main.mo" rel="noopener nofollow ugc" target="_blank">show original</a>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both;"></div>
</aside>

<p>tests:</p><aside class="onebox githubblob">
  <header class="source">

      <a href="https://github.com/icdevsorg/evm.mo/blob/5b3870c2454caf5cb1506010c8ab6796214ff307/test/main.test.mo" rel="noopener nofollow ugc" target="_blank">github.com</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/icdevsorg/evm.mo/blob/5b3870c2454caf5cb1506010c8ab6796214ff307/test/main.test.mo" rel="noopener nofollow ugc" target="_blank">icdevsorg/evm.mo/blob/5b3870c2454caf5cb1506010c8ab6796214ff307/test/main.test.mo</a></h4>


      <pre><code class="lang-mo">import { test; skip } "mo:test/async"; // see https://mops.one/test

import { stateTransition } "../src/evm_mo_backend/main";

import Array "mo:base/Array";
import Nat "mo:base/Nat";
import Nat8 "mo:base/Nat8";
import Nat64 "mo:base/Nat64";
import Int "mo:base/Int";
import Trie "mo:base/Trie";
import Debug "mo:base/Debug";
import Vec "mo:vector";
import Map "mo:map/Map";
import EVMStack "../src/evm_mo_backend/evmStack";
import T "../src/evm_mo_backend/types";

let dummyTransaction: T.Transaction = {
    caller = "\00\aa\00\aa\00\aa\00\aa\00\aa\00\aa\00\aa\00\aa\00\aa\00\aa";
    nonce = 2;
    gasPriceTx = 5;
</code></pre>



  This file has been truncated. <a href="https://github.com/icdevsorg/evm.mo/blob/5b3870c2454caf5cb1506010c8ab6796214ff307/test/main.test.mo" rel="noopener nofollow ugc" target="_blank">show original</a>

  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both;"></div>
</aside>

            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/evm-in-motoko-for-trustless-execution-environments/19981">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 05 Jul 2024 13:37:42 +0000</pubDate>
</item>
<item>
<title>Preconfirmations under the NO lens</title>
<link>https://ethresear.ch/t/preconfirmations-under-the-no-lens/19975</link>
<guid>https://ethresear.ch/t/preconfirmations-under-the-no-lens/19975</guid>
<content:encoded><![CDATA[
<p>by <a href="https://twitter.com/umb_nat" rel="noopener nofollow ugc">U. Natale</a>.</p>
<p><strong>Acknowledgements</strong><br />
This research has been granted by <a href="https://chorus.one/" rel="noopener nofollow ugc">Chorus One</a>. We are grateful to <a href="https://twitter.com/plc_hld" rel="noopener nofollow ugc">M. Moser</a>, <a href="https://x.com/crainbf" rel="noopener nofollow ugc">B. Crain</a>, and <a href="https://x.com/Yannimoto" rel="noopener nofollow ugc">Y. Socolov</a> for useful discussions and comments. We also thanks <a href="https://x.com/mempirate" rel="noopener nofollow ugc">J. Bostoen</a> and <a href="https://x.com/fra_mosterts" rel="noopener nofollow ugc">F. Mosterts</a> from <a href="https://x.com/chainbound_" rel="noopener nofollow ugc">Chainbound</a> team for reviewing the entire document (review ≠ endorsement).</p>
<h1><a class="anchor" href="https://ethresear.ch#preconfirmations-landscape-1" name="preconfirmations-landscape-1"></a>Preconfirmations landscape</h1>
<p>In the context of PBS, bargaining between proposer and relay start at around 1s. This means that users submitting transactions after 1s have to wait for the next slot to know if the transaction is included or not. Even in the context of timing games and assuming some aggressive player, there is a hard cut-off at &lt; 4s due to attestation deadline.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/b/8b20ed4fa8795f4a096dc0d24d975d1b02f78d72.png" title="Screenshot 2024-06-21 alle 11.35.54"><img alt="Screenshot 2024-06-21 alle 11.35.54" height="255" src="https://ethresear.ch/uploads/default/optimized/3X/8/b/8b20ed4fa8795f4a096dc0d24d975d1b02f78d72_2_690x255.png" width="690" /></a></div><br />
<strong>Fig. 1:</strong> The current setup under PBS.<p></p>
<p>With preconfirmation, users have the possibility to access the dead time space between two blocks via a <a href="https://mirror.xyz/preconf.eth/sgcuSbd1jgaRXj9odSJW-_OlWIg6jcDREw1hUJnXtgI" rel="noopener nofollow ugc">credible heads-up before a confirmation happens</a>. However, at the moment, preconfirmations give no guarantees on execution.</p>
<p>For example, imagine 2 users submit 2 conflicting transactions (e.g. a swap against the same pool), but both get a preconfirmation. What happens in slot N+1 is that both transactions land in some place into the slot, but one of the two fails.</p>
<p>From the provider of preconfirmations perspective, the original agreement was respected, however one of the two users next time will think twice before paying for a preconfirmation. The same scenario can happen even if there is only one preconf, but this transaction lands in some place into the slot after other conflicting transactions.</p>
<p>This poses some questions on who can preconfirm a transaction and who can’t. From the two example above it is evident that unsophisticated players can’t play this game and provide a real improvement for the Ethereum ecosystem. It is clear that the burden would be reduced if the preconfs were intended only for transactions that do not touch contentious state — e.g. transfers of tokens and NFTs, dApps with “batching” architecture, L2 settlements, etc. In this case no sophistication is needed so we will exclude it from the goal of this analysis.</p>
<h1><a class="anchor" href="https://ethresear.ch#proposer-as-preconf-provider-2" name="proposer-as-preconf-provider-2"></a>Proposer as preconf provider</h1>
<p>Preconfirmations should be managed in a manner which is similarly decentralized to the current PBS setup; they should not give rise to a centralization bottleneck that exceeds the current builder dominance.</p>
<p>Fundamentally, the PBS transaction pipeline is an auction. Preconfirmations under a gateway architecture follow a delegation scheme, where node operators (NOs) select a third party to select transactions for future inclusion. Therefore, the gateway design is not a spot market, and a generally less competitive scheme as the cost of switching is considerably higher. Indeed, the gateway architecture expects each validator to sign an on-chain transaction to deposit collateral, meanwhile this operation under PBS is done completely off-chain, meaning that there is no cost to switching builders.</p>
<p>This may directly reflect in multi-block MEV, where gateways will be able to provide increasingly more competitive partnership offers to node operators as they scale their dominance over the network. In difference to PBS builders, these gateways will have certainty over the slots for which they hold a mandate. Therefore, a gateway architecture is likely to manifest as a heavily centralized setup, where multi-block MEV is the central return to scale, and switching costs are high. Overall, as it is not an auction or a spot market, the gateway architecture is more likely to manifest a centralization bottleneck which exceeds PBS builder dominance.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/9/193ec08347bb058b66d9b5ead9de72df69a1636d.png" title="stake penetration vs slots in a row"><img alt="stake penetration vs slots in a row" height="421" src="https://ethresear.ch/uploads/default/optimized/3X/1/9/193ec08347bb058b66d9b5ead9de72df69a1636d_2_690x421.png" width="690" /></a></div><br />
<strong>Fig. 2:</strong> Rate of N slots in a row over total Ethereum slots in a year as a function of stake penetration. Growth is higher than linear, and the case of builders/gateways dominance in block production is just an extension of above.<p></p>
<p>A preferable scheme would be the proposer selecting preconfirmations itself. Even in the case of the largest proposers, their ability to engage in multi-block MEV is capped by their voting power (see Fig. 2), which is in turn is capped by the proposer market (i.e. access to capital). Even under PBS, proposers could theoretically already engage in multi-block MEV, but refrain to do so, for a variety of reasons ranging from access to capital and organisational setup, to legal liability. These same patterns would likely extend to a preconfirmation setup.</p>
<p>In this section we are going to analyze some scenarios that may arise if the proposer of the slot is the one providing preconfirmation for transactions. We further assume that the NO is a sophisticated player, since the more transactions an unsophisticated preconfirmations provider includes in the preconfirmation list, the more difficult it is for block builders to create a block with all transactions being successful. This implies an execution guarantee on preconfirmed transactions.</p>
<p>If the majority of preconfirmed transactions fail, the market becomes less attractive, making preconfirmations a difficult tool to use. Including conflicting transactions can also damage the NOs credibility, negatively impacting the brand.</p>
<h2><a class="anchor" href="https://ethresear.ch#information-edge-from-private-order-flow-3" name="information-edge-from-private-order-flow-3"></a>Information edge from private order flow</h2>
<p>In this section we are going to show the different information edge builders have in the current PBS framework. As we did in <a href="https://arxiv.org/pdf/2312.09654" rel="noopener nofollow ugc">The cost of artificial latency in the PBS context</a>, we can define a standardized parameter that allows for a comparison of bids irrespective of their absolute size. This corresponds to the ratio between a given bid and the maximum bid in the auction for a particular slot. That is</p>
<div class="math">
\begin{equation}
R = \frac{b_s(t_E)}{\textrm{max}_{t_E}b_S(t)}\,,\qquad(1)
\end{equation}
</div>
<p>where <span class="math">b</span> is the bid value, <span class="math">s</span> indicate the corresponding slot, and <span class="math">t_E</span> is the time at which the bid was made eligible. This allows us to compare builders bidding strategy over all slot proposed by Chorus One since 2024-03-13.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/8/e83b3b463951c5f72911dad23a05e70b9868c23c.png" title="Builders edge on private order flow"><img alt="Builders edge on private order flow" height="345" src="https://ethresear.ch/uploads/default/optimized/3X/e/8/e83b3b463951c5f72911dad23a05e70b9868c23c_2_690x345.png" width="690" /></a></div><br />
<strong>Fig. 3:</strong> Builder bidding strategy standardized over all slots proposed by Chorus One since 2024-03-13.<p></p>
<p>If we select 6 of the top builders by entity (according to <a href="https://mevboost.pics/" rel="noopener nofollow ugc">mevboost.pic</a>), we can clearly see a difference in the overall strategy, see Fig. 3. For example, we can see how some builders start to deliver bids “late” into the slot, others seems to start much earlier. Furthermore, some builders have a clear linear trend in bid increase per unit of time, others seems to start being careful near the end of the auction.</p>
<p>Although this is independent from the information edge coming from private order flow, we can see how different builders propagates different values of <span class="math">R</span>. Precisely, at 1s into the slot (that is the median value for bid selection in current PBS framework) we have</p>
<ul>
<li><strong>Builder:</strong> 0xb211df4…, <strong>Median:</strong> 0.91, <strong>0.25-quantile:</strong> 0.83, <strong>0.95-quantile:</strong> 0.99</li>
<li><strong>Builder:</strong> 0x83d3495…, <strong>Median:</strong> 0.86, <strong>0.25-quantile:</strong> 0.75, <strong>0.95-quantile:</strong> 0.97</li>
<li><strong>Builder:</strong> 0xa32aadb…, <strong>Median:</strong> 0.90, <strong>0.25-quantile:</strong> 0.83, <strong>0.95-quantile:</strong> 0.98</li>
<li><strong>Builder:</strong> 0xa03a000…, <strong>Median:</strong> 0.81, <strong>0.25-quantile:</strong> 0.74, <strong>0.95-quantile:</strong> 0.95</li>
<li><strong>Builder:</strong> 0xa91d3e5…, <strong>Median:</strong> 0.79, <strong>0.25-quantile:</strong> 0.65, <strong>0.95-quantile:</strong> 0.93</li>
<li><strong>Builder:</strong> 0xb783f81…, <strong>Median:</strong> 0.88, <strong>0.25-quantile:</strong> 0.82, <strong>0.95-quantile:</strong> 0.96</li>
</ul>
<p>that indicates how different entities arrive to the most-likely-end of the auction with less/higher bid values.</p>
<p>From the NO perspective, a difference in information edge could lead to a mispricing of MEV txs, thus increasing the risk of producing less valuable blocks. In general, builders in the current MEV-Boost framework have a comprehensive view of all transactions and typically include those that maximize the block value. However, with validators as preconfirmation providers, proposers must select transactions in advance, often without knowledge of transactions occurring on private channels. The primary metric available to validators in this scenario is the base fee. Specifically, if a transaction pays the base fee (BF) plus a priority fee (PF), it is considered valid in principle. But if the priority fee is the lowest compared to transactions in the private order flow from builders, the block value could decrease. This is because builders are now required to include the preconfirmed transaction instead of a potentially more valuable one. Here sophisticated NOs are in advance since they can develop models to probabilistically evaluate transactions and perform an opinionated selection.</p>
<p>It is worth noting, that validators with private transaction flows could be incompatible with preconfirmations, depending on implementation. Private transaction flows can also manifest by virtue of network jitter. Indeed, if a proposer gives a preconfirmation on a transaction from private transaction flow (or on a transaction from an RPC that’s close to the proposer, but far away from the builder), there could be a non-zero likelihood this transaction is not known by the builders, which may find it difficult to build a valid block (i.e. with the preconfirmed tx). The solution is that the proposer <a href="https://chainbound.github.io/bolt-docs/api/builder-api" rel="noopener nofollow ugc">sends the full transaction to builders</a>. Concerns about privacy are clearly excluded since the proposer already committed to certain execution, and the builder can’t really do anything about that.</p>
<h2><a class="anchor" href="https://ethresear.ch#enforced-early-timing-games-4" name="enforced-early-timing-games-4"></a>Enforced early timing-games</h2>
<p>Arbitrageurs often engage in short-term trading due to competitive pressures. When they opt to delay immediate gains in hopes of capturing a greater mispricing, they run the risk of losing the lucrative opportunity to other traders. This issue is particularly critical within Ethereum’s Proposer-Builder Separation (PBS) mechanism, where searchers must strategically balance their bidding approaches.</p>
<p>Consider an arbitrage opportunity that arises relative to an external source, such as a centralized exchange (CEX), at t=4 seconds into slot N. Since the on-chain price is stale and searchers are uncertain whether the opportunity will vanish on the CEX side, they may prefer to execute the first leg of the trade on the CEX immediately and wait the canonical 12 seconds to see their transaction confirmed on-chain. In the PBS context, however, if a searcher immediately bids their maximum willingness to pay for the opportunity, there is a non-zero likelihood that other searchers may outbid them, effectively frontrunning the original strategy. Conversely, if the searcher bids aggressively too late, the closing trade may fail to be included on-chain since the proposer has already committed to a block that excludes this particular transaction. This scenario creates an auction dynamic that hinges on accurately pricing the time within the slot. The same applies for a DEX &lt;&gt; DEX opportunity, since other arbitrageurs may offer a higher share of MEV for the same opportunity and then seeing their bundle being selected.</p>
<p>Therefore, searchers must strategize not only about how much to bid, but also about the optimal timing of their bids. Bidding too early or too late can both result in a loss of the arbitrage opportunity. The delicate balance between these factors is crucial for optimizing their strategies in such competitive and time-sensitive environments. This study models this behavior and evaluates various strategies to understand the optimal bidding dynamics in Ethereum’s PBS framework.</p>
<h3><a class="anchor" href="https://ethresear.ch#model-description-5" name="model-description-5"></a>Model Description</h3>
<p>To investigate how the introduction of preconfirmations might influence the auction dynamics in PBS, we conducted simulations using an Agent-Based Modeling (ABM) framework. The model is designed to simulate the behavior of searchers participating in PBS auctions under varying conditions, incorporating elements of competitive bidding and strategic timing. In our model, we assume that searchers at step N are aware of the bids at step N-1. While this might seem at odds with the usual dynamics in MEV-Boost, where the auction is not publicly visible, we can reconcile this assumption with two scenarios:</p>
<ol>
<li><strong>Historical Data Adjustment</strong>: Searchers adjust their bidding strategies based on the share of MEV extracted as a function of past data. In this scenario, at each step N, searchers are informed about the behavior of searchers at the corresponding step N-1 from the previous slot. Thus, the predictive model is grounded in the historical data of past auctions.</li>
<li><strong>Vertically Integrated Builders</strong>: In this scenario, searchers are considered as vertically integrated builders. Here, we can imagine a block as a composition of transactions that produce a certain value for the MEV, with the bidding phase representing the exact competition between builders.</li>
</ol>
<p>By incorporating these scenarios, our model aims to provide a simplistic but comprehensive understanding of how searchers might operate within the PBS auction mechanism under the influence of preconfirmations.</p>
<h3><a class="anchor" href="https://ethresear.ch#searchers-behaviour-6" name="searchers-behaviour-6"></a>Searchers behaviour</h3>
<p>In the model, each agent represents a searcher with a specific profit margin, aggressivity parameter, and fear-of-missing-out (FOMO) factor. These agents operate in a simulated environment that mimics the Ethereum PBS auction mechanism. Each agent’s decision-making process is influenced by the bids placed in previous auction steps, representing the competitive nature of the environment.</p>
<p>Agents update their bids in each step based on a combination of their internal parameters and the observed bids from the previous step. The bid update process is governed by a logistic growth model, where the increment of the bid follows a logistic function, adjusted by the agent’s aggressivity parameter and FOMO factor. This approach ensures that agents increase their bids more cautiously in the early stages and more rapidly as the auction progresses, reflecting the strategic balance between the risk of being outbid and the urgency of capturing the arbitrage opportunity. This dynamic allows the agents to optimize their bidding strategies over time, aiming to reach the maximum bid value closer to the end of the auction period.</p>
<p>Additionally, agents take into account the probability that the auction may terminate at any given step. This probability is derived from a fictitious empirical distribution of auction durations, modelled using a truncated normal distribution to generate realistic auction durations, cfr. Fig. 4. In case of preconfirmation, we add a half-normal distribution to the previous one, cfr. Fig. 5. The termination probability influences the agents’ urgency in placing bids, as they must balance the risk of the auction ending unexpectedly with the potential benefits of waiting for a more opportune moment to bid. This probabilistic approach ensures that agents are not only competing against each other but also managing the inherent uncertainty of the auction’s duration.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/0/6/06e7143081bbd056f94a6b8f8e3873b675a2a89f.png" title="Auction Time - no preconf - PDF &amp; CDF"><img alt="Auction Time - no preconf - PDF &amp; CDF" height="230" src="https://ethresear.ch/uploads/default/optimized/3X/0/6/06e7143081bbd056f94a6b8f8e3873b675a2a89f_2_690x230.png" width="690" /></a></div><br />
<strong>Fig. 4:</strong> Single instances of a fictitious empirical distribution for the transaction selection time into the auction in the absence of preconfirmations.<p></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/3/d3d2b8c36e3810f0175271e67a2b49d9804afd44.png" title="Auction Time - preconf - PDF &amp; CDF"><img alt="Auction Time - preconf - PDF &amp; CDF" height="230" src="https://ethresear.ch/uploads/default/optimized/3X/d/3/d3d2b8c36e3810f0175271e67a2b49d9804afd44_2_690x230.png" width="690" /></a></div><br />
<strong>Fig. 5:</strong> Single instances of a fictitious empirical distribution for the transaction selection time into the auction in the presence of preconfirmations.<p></p>
<p>The model incorporates four different types of searchers, each using a different predictive model to estimate the bid for the next step before applying their respective increment:</p>
<ol>
<li><strong>Predictive Model 1</strong>: This model predicts the next bid as simply the maximum bid observed so far. It assumes that the current trend will continue without significant changes.</li>
<li><strong>Predictive Model 2</strong>: This model uses a linear regression based on the bid history to predict the next bid. It fits a linear model to the previous bids and uses the resulting slope and intercept to estimate the next bid. This approach assumes that the bid growth can be approximated by a linear trend.</li>
<li><strong>Predictive Model 3</strong>: This model calculates the average increment of the bids from previous steps and adds this average increment to the current maximum bid. This model assumes that past increments provide a good estimate for future increases.</li>
<li><strong>Predictive Model 4</strong>: This model uses a logarithmic fit based on the bid history to predict the next bid. It fits a logarithmic model to the previous bids and uses the resulting parameters to estimate the next bid. This approach assumes that the bid growth follows a decelerating trend, reflecting a more conservative strategy as the auction progresses.</li>
</ol>
<p>By incorporating these diverse predictive models, the simulation captures a wide range of bidding behaviors and strategies, providing a more comprehensive understanding of how different types of searchers might operate within the Ethereum PBS auction mechanism.</p>
<h3><a class="anchor" href="https://ethresear.ch#results-7" name="results-7"></a>Results</h3>
<p>Analyzing the results in Fig. 6, we observe that in scenarios where preconfirmations on transactions are possible, searchers begin to increase their share of captured MEV earlier. This suggests that to increase MEV share received, a node operator might opt to run the version that allows for preconfirmations but never actually selects any MEV transactions. This creates a situation where searchers bid higher because the auction might end sooner. However, since no preconfirmations are offered (as the validator does not select any), the auction continues, and searchers find themselves starting from a higher base bid.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/b/5be75738445af00fdf93f6b1e455b905c46a23c2.png" title="Auction war - combined"><img alt="Auction war - combined" height="230" src="https://ethresear.ch/uploads/default/optimized/3X/5/b/5be75738445af00fdf93f6b1e455b905c46a23c2_2_690x230.png" width="690" /></a></div><br />
<strong>Fig. 6:</strong> Distribution of different bidding strategy extracted from a set of simulation using the ABM described in previous section. Simulations including a preconfirmation phase are in red, simulations without a preconfirmation phase are in blue.<p></p>
<p>This situation can be likened to a modified version of the prisoner’s dilemma. In this strategic game, each searcher (or prisoner) must decide whether to bid aggressively early (cooperate) or wait for a more opportune moment (defect). If all searchers bid aggressively early, they collectively drive up the MEV share and risk overbidding. Conversely, if they all wait, the auction proceeds normally, and they can potentially secure MEV shares at a lower cost. However, if some searchers bid aggressively while others wait, the aggressive bidders might secure a higher share early, pushing the late bidders to increase their bids even further as the auction continues.</p>
<p>This dynamic creates a tension between the searchers: each must decide whether to trust that others will not bid aggressively early or to secure their position by doing so themselves. The presence of preconfirmations adds an additional layer of complexity, as the threat of an early auction end prompts higher early bids, even when no actual preconfirmations are selected.</p>
<p>In summary, the introduction of preconfirmations influences searchers’ bidding behavior, leading to higher initial bids due to the perceived risk of an early auction end. This strategic interplay resembles the prisoner’s dilemma, where individual decisions to bid early or wait impact the collective outcome, highlighting the intricate balance between cooperation and competition in optimizing MEV shares.</p>
<p>In other words, with preconfirmations, searchers competing for the same opportunity can no longer rely on the probability that a certain builder will win a slot. If a competing searcher’s transaction is preconfirmed, even if the transaction is accepted by the winning builder, the builder must prioritize the preconfirmed one.</p>
<h2><a class="anchor" href="https://ethresear.ch#reversal-timing-game-8" name="reversal-timing-game-8"></a>Reversal timing-game</h2>
<p>Currently, the dynamics involve searchers relying on private auctions through builders, who have a certain probability of winning the block. Builders construct a block based on the privately received transactions and subsequently compete with other builders (through a public auction) to determine the winning block.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/8/d86ee43d92e0249f4368147a23f5f2f24a1560bf.png" title="latency_vs_ntxs"><img alt="latency_vs_ntxs" height="230" src="https://ethresear.ch/uploads/default/optimized/3X/d/8/d86ee43d92e0249f4368147a23f5f2f24a1560bf_2_690x230.png" width="690" /></a></div><br />
<strong>Fig. 7:</strong> Dependency in current auction dynamic between number of transactions included in the block and bid value, source from <a href="https://ethresear.ch/t/the-cost-of-artificial-latency-in-the-pbs-context/17847">The cost of artificial latency in the PBS context</a>.<p></p>
<p>Empirical data shows that the number of transactions included in blocks proposed by builders and the value of the block increase linearly, cfr. Fig 7 and [The cost of artificial latency in the PBS context](<a href="https://ethresear.ch/t/the-cost-of-artificial-latency-in-the-pbs-context/17847**.">https://ethresear.ch/t/the-cost-of-artificial-latency-in-the-pbs-context/17847**.</a>** This implies that once an arbitrage opportunity between CEX and DEX is identified, stat-arbitrageurs submit their transaction, which is not further modified, and the additional value builders obtain comes from a greater inclusion of transactions. In fact, searchers prefer to submit their transaction immediately as the opportunity might vanish on the CEX, and there is a form of preconfirmation due to the historical probability of a builder winning an auction. Therefore, it is highly likely that the rebalancing on the CEX occurs in the early stages of the block.</p>
<p>By modeling the price difference between CEX and DEX as a Markovian jump-diffusion process, we can derive the expression for the probability that searchers can execute a profitable trade (i.e. that the price difference is greater than the fees needed to execute the trade). This probability, <span class="math">P</span>,  is given by (see Appendix for a derivation):</p>
<div class="math">
\begin{equation}
P = \frac{1}{1+\frac{\sqrt{2\lambda}\gamma}{\sigma}}\,,\qquad(2)
\end{equation}
</div>
<p>where <span class="math">\gamma</span> represents the fee of the trade,  <span class="math">\lambda</span> is such that the time mean interval between trades is <span class="math">\bar{t} = \lambda^{-1}</span>, <span class="math">\sigma</span> is the volatility of the price difference.</p>
<p>Equation (2) allows us to define a new dynamic for stat-arbs under the preconfirmation framework. Indeed, when the time interval between trades is small (i.e. high values of <span class="math">\lambda</span>), the probability of having a profitable trade decrease. On the other hand, if volatility becomes predominant, the dynamic may change. Preconfirmations allows arbitrageurs to tune the time interval <span class="math">\lambda^{-1}</span> in order to maximize the probability of being in the trading regime on a volatility based strategy.<br />
Precisely, the time between trades is determined by the time at which the previous slot selected transactions and the time at which new transactions are selected for current block. With current PBS design this corresponds to 12s. Indeed, even if the builder knows he won the slot at t=4s into the slot N-1, he now has to wait 12s (i.e. 4s into the slot N) before knowing if he wins the slot N. With preconfirmations the frequency of transaction selections is a dynamic variable, because you know that your transaction is selected at different time wrt. the usual 4s into the slot. Clearly, by alternating preconfirmations with normal block inclusion, the parameter <span class="math">\lambda</span> is non-constant.</p>
<p>If now the objective is to minimize the ratio <span class="math">\sqrt{2\lambda}/\sigma</span>, if the volatility is low searchers can start to increase the frequency of trades submission (i.e. participate in preconfirmation auction) in order to maintain <span class="math">\sqrt{2\lambda}/\sigma &lt;&lt; 1</span>.</p>
<p>This modeling is consistent with the hypothesis that searchers may be interested in submit their transaction at the beginning of the block. This creates a dynamic potentially opposite to the timing games observed in the MEV-Boost context, where now searchers strive to compete from the early stages in the preconfirmation market.</p>
<h2><a class="anchor" href="https://ethresear.ch#capturing-on-chain-mev-9" name="capturing-on-chain-mev-9"></a>Capturing on-chain MEV</h2>
<p>With node operators as preconf provider, preconfirmations give validators the power back to decide on some transaction that have to be included in the slot. This means that NO can add new transactions on top of the current MEV-Boost pipeline, meaning that the ways of capturing MEV augment. Indeed, if we stay in the assumption that preconf transactions are likely to be executed as valid transactions, each time a validator is selected to propose a slot, it can preconf on his own transactions. This means that some types of on-chain MEV, in principle, can be captured by NO using preconfirmations, without renouncing to CEX &lt;&gt; DEX arbitrage, that might result more complicated for NOs.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/1/41859fc9aeca6897bb9ff6312cc64a64d9c2414a.png" title="Screenshot 2024-06-20 alle 12.13.39"><img alt="Screenshot 2024-06-20 alle 12.13.39" height="327" src="https://ethresear.ch/uploads/default/optimized/3X/4/1/41859fc9aeca6897bb9ff6312cc64a64d9c2414a_2_690x327.png" width="690" /></a></div><br />
<strong>Fig. 8:</strong> Daily extracted MEV in 30 days by profit. Source <a href="https://eigenphi.io/" rel="noopener nofollow ugc">EigenPhi</a>.<p></p>
<p>Given the importance on the order of transaction execution, only arbitrage and liquidation could be captured using preconfirmation. According to <a href="https://eigenphi.io/" rel="noopener nofollow ugc">EigenPhi</a>, arbitrages and liquidations produced revenue of $3M profit in 30 days. From the Top 12 leaderboard on arbitrageurs, we can see that only 66% of captured MEV is shared with builders. Clearly, also builders retain a portion of MEV, but due to lack of data, we exclude this from our calculation, which at the end will provide a lower bound on extra revenue a NO can make.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/5/e5f6e65454a19f24711d87a2fac6c81cbf950962.png" title="Screenshot 2024-06-20 alle 12.13.52"><img alt="Screenshot 2024-06-20 alle 12.13.52" height="162" src="https://ethresear.ch/uploads/default/optimized/3X/e/5/e5f6e65454a19f24711d87a2fac6c81cbf950962_2_690x162.png" width="690" /></a></div><br />
<strong>Fig. 9:</strong> Leaderboard of top 12 on-chain arbitrageurs in 30 days. Source <a href="https://eigenphi.io/" rel="noopener nofollow ugc">EigenPhi</a>.<p></p>
<p>If we assume that a NO with 1% of stake penetration captures 1% of this extra MEV, there is an extra $345,600 in a year. Since the median MEV revenue for a NO with such share is ETH 392.31 (cfr. Fig.  10), assuming a price per ETH of $3,500 this (98.74 ETH extra MEV) corresponds to a 25.17% increase from MEV revenue in a year. It is worth mentioning that <a href="https://adagio.chorus.one/" rel="noopener nofollow ugc">current timing games provide ~10% extra MEV</a>.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/8/a869e4823ee39f779a3b39c028ff5e4d784e2066.png" title="MEV yearly size - NO size 10000 over 1005387"><img alt="MEV yearly size - NO size 10000 over 1005387" height="230" src="https://ethresear.ch/uploads/default/optimized/3X/a/8/a869e4823ee39f779a3b39c028ff5e4d784e2066_2_690x230.png" width="690" /></a></div><br />
<strong>Fig. 10:</strong> Probability Distribution Function of MEV proceeds in a year for a node operator with 1% of stake penetration.<p></p>
<h1><a class="anchor" href="https://ethresear.ch#conclusions-10" name="conclusions-10"></a>Conclusions</h1>
<p>Implementing a sophisticated system for preconfirmations within Ethereum’s PBS framework is far from trivial. This complexity opens new avenues for sophisticated NOs to enhance their revenues from MEV.</p>
<p>Our study has demonstrated that preconfirmations introduce a significant layer of strategic depth to the PBS auction mechanism. By providing searchers with a credible heads-up before a transaction is confirmed, preconfirmations alter the timing and aggressiveness of bids. This shift is particularly pronounced in scenarios where NOs, acting as preconfirmation providers, selectively include transactions that maximize their revenue while ensuring successful block proposals.</p>
<p>This analysis highlighted the varied strategies employed by different builders in the current PBS framework, revealing significant differences in how they time their bids. This information asymmetry can lead to mispricing of MEV transactions, potentially reducing the overall value of blocks for node operators.</p>
<p>The presence of preconfirmations forces searchers to engage in more sophisticated bidding strategies. They must carefully balance the risk of being outbid by competitors against the potential for an early auction termination, which could prevent their transactions from being included. This dynamic is akin to a modified prisoner’s dilemma, where searchers must decide between bidding aggressively early or waiting for a more opportune moment, knowing that their decisions impact the overall auction outcome. Overall, this may push for a new type of timing games, where now searchers will compete more aggressively in the first part of the preconfirmation interval.</p>
<p>Complex implementation of preconfirmations provide NOs with a powerful tool to capture on-chain MEV directly. By preconfirming their transactions, NOs can ensure the inclusion of high-value arbitrage and liquidation opportunities, significantly boosting their MEV revenue. Our calculations indicate that a NO with a 1% stake penetration could see a 25.17% increase in annual MEV revenue through strategic use of preconfirmations. This increase is substantial compared to the ~10% extra MEV derived from current timing games.</p>
<p>Despite the potential benefits, the implementation of preconfirmations must be carefully managed to avoid centralization risks. A decentralized approach, where proposers themselves manage preconfirmations, is preferable to a gateway architecture that could lead to undue centralization and higher switching costs.</p>
<h1><a class="anchor" href="https://ethresear.ch#appendix-a-11" name="appendix-a-11"></a>Appendix A</h1>
<h2><a class="anchor" href="https://ethresear.ch#deriving-the-trade-probability-12" name="deriving-the-trade-probability-12"></a>Deriving the trade probability</h2>
<p>To see where Eq. (2) comes from, let’s model the price difference between CEX and DEX  as a Markovian jump-diffusion process. This allows us to derive the expression for the probability that stat-arbitrageurs can execute a profitable trade, i.e. that the price difference is higher than the fees needed to execute the trade.</p>
<p>If we assume that DEX and CEX prices follows a Brownian motion, since the difference between two Brownian motion is still a Brownian motion, we can model the price difference as a Brownian motion with volatility <span class="math">\sigma</span></p>
<div class="math">
\begin{equation}
dM(t) = \mu_Mdt+\sigma dW(t)\,,
\end{equation}
</div>
<p>where <span class="math">\mu_M</span> represents the drift of motion. In the presence of discrete time arrival for trades (i.e. jumps) modelled as a Poisson process with rate <span class="math">\lambda</span>,  we get</p>
<div class="math">
\begin{equation}
dM(t) = \mu_Mdt+\sigma dW(t) + j(M_{t-1}) dN(t)\,,
\end{equation}
</div>
<p>where <span class="math">j(M_{t-1}) dN(t)</span> is the contribution from jumps (depending only on immediately previous state <span class="math">j(M_{t-1})</span>, that’s where the Markovian approximation comes in). The density <span class="math">p(x,t)</span> of the process <span class="math">M(t)</span> is governed by Fokker-Planck equation</p>
<div class="math">
\begin{align}
\partial_t p(x,t) &amp;= -\mu_M\partial_x p(x,t) + \frac{\sigma^2}{2}\partial^2_xp(x,t)+\lambda\left[\int_{-\infty}^{+\infty}p(x-y,t)\delta(y-j)dy - p(x,t)\right]\\
&amp;=-\mu_M\partial_x p(x,t) + \frac{\sigma^2}{2}\partial^2_xp(x,t)+\lambda\left[p(x-j,t)-p(x,t)\right]\,,
\end{align}
</div>
<p>where the Dirac <span class="math">\delta</span> determine the dimension of the jump (we are assuming constant jumps) and <span class="math">\lambda</span> is the mean dimension of jumps in the price difference. In the absence of drift, the equation of the process is</p>
<div class="math">
\begin{equation}
\partial_tp(x,t)=\frac{\sigma^2}{2}\partial^2_xp(x,t)+\lambda\left[p(x-j,t)-p(x,t)\right]\,.
\end{equation}
</div>
<p>To find the stationary distribution (i.e. <span class="math">p(x)</span>), we can consider the case with <span class="math">\partial_tp(x,t)=0</span>, such that Fokker-Planck equation becomes</p>
<div class="math">
\begin{equation}
0=\frac{\sigma^2}{2}\partial^2_xp(x)+\lambda\left[p(x-j)-p(x)\right]\,.
\end{equation}
</div>
<p>Now, if we consider the Taylor expansion of <span class="math">p(x)</span> for small <span class="math">j</span> we obtain</p>
<div class="math">
\begin{equation}
p(x-j)\sim p(x)-j\partial_xp(x)+\frac{\lambda j^2}{2}\partial_x^2p(x)+\ldots\,,
\end{equation}
</div>
<p>which gives</p>
<div class="math">
\begin{equation}
0=\frac{1}{2}\left(\sigma^2+\lambda j^2\right)\partial^2_xp(x,t)-\lambda j\partial_xp(x)\,.
\end{equation}
</div>
<p>If we now observe that for</p>
<div class="math">
j\ll\frac{\sigma}{\sqrt{\lambda}}\,,
</div>
<p>we can neglect second order terms in <span class="math">j</span>. For the next part of the paper we’ll use</p>
<div class="math">
j=\frac{\sigma}{\sqrt{2\lambda}}=\sigma\sqrt{\frac{\bar{t}}{2}}\,,
</div>
<p>which means the dimension of the jump between trades is given by the volatility of price difference times the square root of half the time interval between trades. Under these assumptions our Fokker-Planck equation becomes</p>
<div class="math">
\begin{equation}
0=\frac{\sigma^2}{2}\partial^2_xp(x,t)-\frac{\sqrt{\lambda}\sigma}{\sqrt{2}}\partial_xp(x)\,.
\end{equation}
</div>
<p>This is a second order differential equation, with solution of the form</p>
<div class="math">
\begin{equation}
p(x)=Ae^{r_1x}+Be^{r_2x}\,,
\end{equation}
</div>
<p>where <span class="math">r_1</span> and <span class="math">r_2</span> are the solution of</p>
<div class="math">
\begin{equation}
r^2-\frac{\sqrt{2\lambda}}{\sigma}r=0\,.
\end{equation}
</div>
<p>It follows that</p>
<div class="math">
\begin{equation}
p(x)=A+Be^{\frac{\sqrt{2\lambda}}{\sigma}x}\,.
\end{equation}
</div>
<p>Since <span class="math">p(x)</span> is a density, it has to be normalized and not diverging for <span class="math">x\to\pm\infty</span>. This means that the solution has to be</p>
<div class="math">
\begin{equation}
p(x)=p_1(x|x\in[-\gamma,\gamma])+p_2(x|x\in(-\infty,-\gamma)\,\cup\,(\gamma,\infty))\,,
\end{equation}
</div>
<p>where</p>
<div class="math">
\begin{align}
&amp;p_1(x) = A\,,\qquad\qquad\qquad\,\,\,\, x\in[-\gamma,\gamma]\\
&amp;p_2(x) = Be^{-\frac{\sqrt{2\lambda}}{\sigma}(|x|-\gamma)}\,,\qquad x\in(-\infty,\gamma] \cup [\gamma,\infty)\,.
\end{align}
</div>
<p>The nature of <span class="math">p_2(x)</span> is that it is null at infinity. Now, if we impose continuity of <span class="math">p(x)</span> at boundaries, we have</p>
<div class="math">
p_1(\gamma)=p_2(\gamma)\Rightarrow A = B e^0 = B\,.
</div>
<p>It follows that, by imposing the symmetry condition and the fact that <span class="math">p(x)</span> is a density we get</p>
<div class="math">
\begin{align*}
&amp;2\int_0^\infty p(x)dx = 1 \\
&amp;\to \left.2Ax\right|_0^\gamma-\left.2\frac{B\sigma}{\sqrt{2\lambda}}e^{-\frac{\sqrt{2\lambda}}{\sigma}(|x|-\gamma)}\right|_\gamma^\infty=1\\
&amp;\to 2A\gamma\left(1+\frac{1}{\xi}\right)=1\,,
\end{align*}
</div>
<p>where we introduced the parameter <span class="math">\xi=\frac{\sqrt{2\lambda}\gamma}{\sigma}</span>, characterizing the behaviour of the price difference process. By solving for A, it follows that</p>
<div class="math">
\begin{split}
&amp;p_1(x) = \frac{1}{2\gamma}\frac{\xi}{1+\xi}\,,\qquad\qquad\qquad\,\,\,\, x\in[-\gamma,\gamma]\\
&amp;p_2(x) = \frac{1}{1+\xi}\frac{\xi}{2\gamma}e^{-\frac{\xi}{\gamma}(|x|-\gamma)}\,,\qquad x\in(-\infty,\gamma] \cup [\gamma,\infty)\,.
\end{split}
</div>
<p>Now, we are interested in computing the probability of the trade area. This has as density</p>
<div class="math">
\begin{equation}
p_2(x|x\in(-\infty,-\gamma))+p_2(x|x\in(\gamma,\infty))=\frac{1}{1+\xi}\frac{\xi}{\gamma}e^{-\frac{\xi}{\gamma}(|x|-\gamma)}\,,
\end{equation}
</div>
<p>and since</p>
<div class="math">
\frac{\xi}{\gamma}e^{-\frac{\xi}{\gamma}(|x|-\gamma)}\,,
</div>
<p>is the density of an exponential distribution and that is the only part dependent from <span class="math">x</span>, we have that the invariant for the trade region probability is</p>
<div class="math">
P=\frac{1}{1+\xi}=\frac{1}{1+\frac{\sqrt{2\lambda}\gamma}{\sigma}}\,,
</div>
<p>that is the result used in Eq. (2). Note that this result is consistent with what presented in <a href="https://arxiv.org/pdf/2305.14604" rel="noopener nofollow ugc">Milionis et al</a>, even if the derivation is different.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/preconfirmations-under-the-no-lens/19975">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 05 Jul 2024 09:39:35 +0000</pubDate>
</item>
<item>
<title>Leaderless and Leader-Based Preconfirmations</title>
<link>https://ethresear.ch/t/leaderless-and-leader-based-preconfirmations/19971</link>
<guid>https://ethresear.ch/t/leaderless-and-leader-based-preconfirmations/19971</guid>
<content:encoded><![CDATA[
<div> 关键词：preconfirmation (预确认), leader-based, leaderless, sourcing leaders, mev-boost

总结:
本文讨论了两种类型的预确认系统：领导型和无领导型。领导型预确认由单一权威提供，确保较高保证但可能导致集中；无领导型通过竞争提供价格发现，可能创造更有价值的区块，但存在不确定性。文章提出“ sourcing leaders”作为结合两者的优势，他们从竞争性提供商处获取预确认并为用户提供100%保证。文章还探讨了领导选举机制、拍卖与抽奖等方法，以及如何优化价格结构以防止集中。未来研究方向包括游戏理论分析、定价策略和集成现有MEV提升基础设施。 <div>
<p><em>Joint work with <a class="mention" href="https://ethresear.ch/u/murat">@murat</a>. Thanks to <a class="mention" href="https://ethresear.ch/u/the-ctra1n">@The-CTra1n</a> and <a class="mention" href="https://ethresear.ch/u/bemagri">@bemagri</a> for reviewing and providing valuable feedback.</em></p>
<h1><a class="anchor" href="https://ethresear.ch#introduction-1" name="introduction-1"></a>Introduction</h1>
<p>A preconfirmation (preconf) for the context of this article refers to a promise about a given set of transactions relative to a future block, e.g., execution of a transaction in the next block or placing transactions at the top of the block. Entities who want to obtain a preconf can bid a certain amount indicating how much they are willing to pay for a preconf.</p>
<p>One dimension in which preconfs can be distinguished is whether there exists a unique preconf provider for every L1 block (a preconf leader), or whether there can be multiple competing preconf providers for every L1 block, without a leader. We here discuss the two approaches, their respective advantages and disadvantages, and how they can be combined. A particularly promising approach for combining the two concepts to obtain the best of both worlds appears to be using “sourcing leaders”, which are operating in a leaderless setting and collect preconfs from competing preconf providers.</p>
<h1><a class="anchor" href="https://ethresear.ch#overview-and-definitions-2" name="overview-and-definitions-2"></a>Overview and Definitions</h1>
<h2><a class="anchor" href="https://ethresear.ch#leader-based-preconfs-3" name="leader-based-preconfs-3"></a>Leader-Based Preconfs</h2>
<p>The simplest form of preconfs are ones issued by an appointed leader. This leader must have the authority to issue preconfs and have some means to enforce them. It is not necessary to have a single leader overall, as long as there is a unique, predetermined, and publicly known leader at each point in time. A straightforward way to choose leaders is using the current L1 proposer, as, e.g., in <a href="https://github.com/Commit-Boost/commit-boost-client" rel="noopener nofollow ugc">commit-boost</a>. More sophisticated leader-election methods are discussed below, and can be employed by <a href="https://docs.primev.xyz/concepts/what-is-mev-commit" rel="noopener nofollow ugc">mev-commit</a>.</p>
<h2><a class="anchor" href="https://ethresear.ch#leaderless-preconfs-4" name="leaderless-preconfs-4"></a>Leaderless Preconfs</h2>
<p>An alternative to leader-based preconfs is to have multiple preconf providers simultaneously. The most natural instantiation of this is to have the block builders act as preconf providers, leveraging the strengths of the existing mev-boost landscape. This mechanism is <a href="https://docs.primev.xyz/concepts/what-is-mev-commit" rel="noopener nofollow ugc">used by mev-commit</a>. In this case, a single preconf provider cannot provide an authoritative preconf; in case the block builders are preconf providers, a single builder can only promise to honor the preconf for the blocks this block builder builds.</p>
<p>A preconf from a single block builder thus constitutes a probabilistic preconf in the sense that the preconf is conditioned on the issuing block builder winning the corresponding block. This can already be useful, e.g., for arbitrage searchers. A proper preconf with a 100% guarantee is obtained if all block builders preconfirm. A subtlety of this is that the set of all possible block builders must be known, which is not the case in a permissionless setting. This is solved by mev-commit by letting <a href="https://docs.primev.xyz/get-started/providers/registering-a-provider" rel="noopener nofollow ugc">block builders register</a> as providers and <a href="https://docs.primev.xyz/get-started/validators" rel="noopener nofollow ugc">proposers and relays opt-in</a> to only deliver blocks from registered block builders. Analyzing the game-theoretic interplay between bidders and multiple preconf providers is an interesting open problem.</p>
<h1><a class="anchor" href="https://ethresear.ch#comparison-5" name="comparison-5"></a>Comparison</h1>
<p>Both approaches have their advantages and disadvantages, which we discuss below.</p>
<h2><a class="anchor" href="https://ethresear.ch#advantages-of-leader-based-preconfs-6" name="advantages-of-leader-based-preconfs-6"></a>Advantages of Leader-Based Preconfs</h2>
<p>The most obvious advantage of leader-based preconfs is that a single preconf already constitutes almost a 100% guarantee (almost because the slot may be missed or the chain reorged). This simplifies the protocol interaction and also possibly provides faster feedback. Note that reorg risks are the same for all types of preconfs, so we do not discuss them further here.</p>
<h2><a class="anchor" href="https://ethresear.ch#advantages-of-leaderless-preconfs-7" name="advantages-of-leaderless-preconfs-7"></a>Advantages of Leaderless Preconfs</h2>
<p>Having multiple simultaneous preconf providers creates a competitive environment, allowing for efficient preconf price discovery and thereby optimizing validator yield. A single provider having a preconf monopoly, on the other hand, can dictate the prices arbitrarily.</p>
<p>Further advantages come from letting the block builders be the preconf providers. First, block builders have sufficient sophistication to properly price preconfs. Secondly, builders are building the blocks and thus are the only entities that can issue preconfs without interfering with block production and adding latency: If another party issues a preconf, it must be communicated to the block builders such that they can build compatible blocks, and failure to receive the preconf in time leads to the block builder building a block violating the preconf. This also means that there is some delay between issuing the preconf and the builders learning about it in a leader based approach, which is particularly problematic towards the end of a slot, where builders may learn too late about the preconf. This also creates an advantage for block builders with fast connections to the preconf leaders, potentially leading to further centralization. Furthermore, receiving a preconf from a separate entity interferes with the block building strategy of the builders and thus can potentially lead to substantially less valuable blocks. Finally, leaderless preconfs can be integrated more easily into the existing mev-boost infrastructure.</p>
<h1><a class="anchor" href="https://ethresear.ch#leader-election-8" name="leader-election-8"></a>Leader Election</h1>
<p>As mentioned above, the simplest way to elect a preconf leader is to choose the current L1 proposer. This, however, requires additional sophistication from the proposer and likely leads to economic inefficiencies. It is therefore likely that proposers want to outsource preconfs similarly to how proposers outsource block building in PBS, even though this might raise concerns such as increased complexity due to additional actors, and potentially more centralization. A crucial difference from PBS is that preconf leaders need to be chosen in advance, i.e., before preconf bids are available. Thus, when the right to become a preconf leader is auctioned off, the potential leaders need to place their bids in the leader election without knowing the value they can derive from becoming a leader. This means their bids can only be based on expected values rather than actual amounts as in PBS, similarly to <a href="https://ethresear.ch/t/execution-tickets/17944">execution tickets</a>. A notable exemption to this are scenarios in which preconfs are not time critical such as preconfs for blob inclusion bids. In this case, the auction can be run after all preconf bids have been issued and thus the auction can be based on the actual value instead of the expected one (cf. <a href="https://ethresear.ch/t/blob-preconfirmations-with-inclusion-lists-to-mitigate-blob-contention-and-censorship/19150">Ethereum Research - Blob Preconfirmations with Inclusion Lists to Mitigate Blob Contention and Censorship</a>).</p>
<p>One concern with an expected-value-based auction is that this value likely remains relatively stable over time and thus a possible scenario is that a single entity that is very good at pricing wins an overwhelming fraction of the auctions, leading to centralization and a preconf monopoly (cf. <a href="https://collective.flashbots.net/t/when-to-sell-your-blocks/2814/1" rel="noopener nofollow ugc">The Flashbots Collective - When To Sell Your Blocks</a>). A possible mitigation to this problem is to instead of running an auction, sell lottery tickets and choose the leader randomly as the holder of the winning ticket. This is akin to a similar mechanism recently proposed by <a href="https://hackmd.io/@EspressoSystems/market-design" rel="noopener nofollow ugc">Espresso Systems in a related context</a>. Further research is required to determine an optimal pricing structure for such lotteries.</p>
<h1><a class="anchor" href="https://ethresear.ch#combining-leaderless-and-leader-based-preconfs-9" name="combining-leaderless-and-leader-based-preconfs-9"></a>Combining Leaderless and Leader-Based Preconfs</h1>
<p>To obtain the best of both worlds, one can combine a leaderless with a leader-based approach. We discuss some options how to achieve this below.</p>
<h2><a class="anchor" href="https://ethresear.ch#simultaneous-leaders-and-leaderless-providers-10" name="simultaneous-leaders-and-leaderless-providers-10"></a>Simultaneous Leaders and Leaderless Providers</h2>
<p>One option to combine leaderless and leader-based preconfs is to have a dedicated preconf leader, but let this leader operate simultaneously with multiple non-leader preconf providers. We assume below that the non-leader providers are block builders. In such a scheme, both the leader and the builders can issue preconfs at any point in time. When the leader issues a preconf, it must be communicated to the block builders, who then need to honor them when building their blocks. At this point, block builders cannot commit to the already committed bid anymore (since such commitment would not add any value). On the other hand, if a builder issues a preconf first, the leader can still commit to the same bid, turning the preconf from the builder into a 100% guaranteed preconf.</p>
<p>While this approach might appear conceptually simple, it comes with several challenges. One issue is that it is probably very hard, if not impossible, for the leader to issue execution preconfs that are compatible with execution preconfs of the block builders. This approach might therefore be limited to inclusion preconfs. Another issue is the timing of preconfs: For the mechanism to work, a total order among preconfs needs to be established, since a builder should only be rewarded for a preconf on a bid that also been committed to by the leader if the builder committed first. This total order can be established by a dedicated side-chain, such as the mev-commit chain. Nevertheless, there is room for leaders to play games with the competing builders by delaying their preconfs or trying to frontrun the builders. Yet another difficulty of this approach are the more complex incentives. Who should be paid how much in case multiple preconfs are issued? Developing a fair mechanism that leads to good preconf prices requires further research.</p>
<h2><a class="anchor" href="https://ethresear.ch#sourcing-leaders-11" name="sourcing-leaders-11"></a>Sourcing Leaders</h2>
<p>An alternative is to have leaders that themselves have no authority to enforce preconfs. Instead, the leaders receive bids from end users and subsequently try to obtain preconfs from the preconf providers. We call such leaders “sourcing leaders”. Once the sourcing leader has obtained preconfs from all providers, they issue a preconf to the end user. A sourcing leader is not strictly speaking a leader as defined above, but can provide the same advantage of a leader, namely issuing preconfs that themselves provide a 100% guarantee to the end user.</p>
<p>The role of a sourcing leader can be taken on by sophisticated actors such as solvers. A sourcing leader can in this case also offer preconfs before all providers have issued one and charge a premium to take on the risk that the preconf is violated. It is furthermore possible to have multiple competing sourcing leaders that offer preconfs with different prices at different speeds, where users can choose the best one for their purposes.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/d/1df247e0731521b812fdecf14287017b6e1a772b.png" title="Sourcing Leader"><img alt="Sourcing Leader" height="219" src="https://ethresear.ch/uploads/default/optimized/3X/1/d/1df247e0731521b812fdecf14287017b6e1a772b_2_690x219.png" width="690" /></a></div><br />
<strong>Figure 1:</strong> Illustration of the interaction between an end user, a sourcing leader running a bidder node, and three preconf providers.<p></p>
<h1><a class="anchor" href="https://ethresear.ch#conclusion-and-open-problems-12" name="conclusion-and-open-problems-12"></a>Conclusion and Open Problems</h1>
<p>Both leader-based and leaderless preconfs offer unique advantages and challenges. Leader-based preconfs offer a 100% guarantee (ignoring missed slots and reorgs) with a single preconfirmation, whereas leaderless ones create a competitive environment, enabling efficient price discovery, and potentially leading to more valuable blocks. Different methods for leader election also have their own trade-offs, with options ranging from auctions to lotteries.</p>
<p>Combining leaderless and leader-based preconfs can provide the benefits of both systems. One approach is to have a dedicated preconf leader operating alongside non-leader providers. Another approach is to use sourcing leaders who have no enforcement authority themselves, but attempt to obtain preconfs from providers. Both approaches allow for a high degree of competition, but also pose additional challenges.</p>
<p>There are still several unresolved research problems uncovered in this article. One of them is to analyze the game-theoretic interplay between bidders and multiple preconf providers in a leaderless preconf system. For a leader-based approach, relevant open problems are determining an optimal pricing structure for preconf leader lotteries to mitigate the risk of centralization and a preconf monopoly, and how to integrate with the existing mev-boost infrastructure. For combining leaderless and leader-based preconfs, designing fair mechanisms for the interaction between both types of preconf providers is left for future research. Finally, an important open question is how the approach with a sourcing leader compares to the others in terms of obtaining fair prices.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/leaderless-and-leader-based-preconfirmations/19971">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 04 Jul 2024 20:41:05 +0000</pubDate>
</item>
<item>
<title>Execution Consensus Separation</title>
<link>https://ethresear.ch/t/execution-consensus-separation/19964</link>
<guid>https://ethresear.ch/t/execution-consensus-separation/19964</guid>
<content:encoded><![CDATA[
<div> 关键词：MEV、共识层、执行层、应用层、多提案者共识（MCP）

总结:<br />
文章讨论了执行一致性分离在解决以太坊上的交易优先权执行问题（MEV）中的关键作用。首先，需要改善共识层的抗审查能力，引入多提案者共识（MCP），允许多个提案者同时提出交易，增强交易的包容性。其次，执行层需实现延迟执行和确定性调度规则，确保交易按照规则有序进行。最后，应用层应发展为无序机制，例如通过链上拍卖避免价格猜测导致的价值损失。这些升级将使以太坊对开发者和用户更加友好，研究者可共同推进这一安全协议的改进。 <div>
<h2><a class="anchor" href="https://ethresear.ch#execution-consensus-separation-1" name="execution-consensus-separation-1"></a>Execution Consensus Separation</h2>
<p><strong></strong></p><div class="lightbox-wrapper"><strong><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/b/4b9eb4b7bcec14c8b9a8aee948a332d9d48013e4.jpeg" title=""><img alt="" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/4/b/4b9eb4b7bcec14c8b9a8aee948a332d9d48013e4_2_500x500.jpeg" width="500" /></a></strong></div><br />
MEV is fundamentally about control. The proposer has control of which transactions make it into blocks and which order they appear in. In other words MEV is all about censorship and reordering. All of the goals on the Ethereum roadmap related to MEV are therefore impossible without fixing these things. The good news is that fixing these things is possible, the bad news is that the solution requires us to work together to study and prove the security of some meaningful upgrades to both consensus and execution.<p></p>
<p>Current work on the <a href="https://x.com/VitalikButerin/status/1741190491578810445" rel="noopener nofollow ugc">“Scourge” section</a> of the <a href="https://ethereum.org/en/roadmap/" rel="noopener nofollow ugc">Ethereum roadmap</a> has been siloed. People work on individual problems and sometimes lose the broader scope of what we are ultimately trying to achieve. <a href="https://ethresear.ch/t/epbs-design-constraints/18728">ePBS</a>, <a href="https://ethereum-magicians.org/t/eip-7547-inclusion-lists/17474" rel="noopener nofollow ugc">Inclusion Lists</a>, <a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590">MEV Burn</a>, <a href="https://github.com/flashbots/mev-boost/issues/139" rel="noopener nofollow ugc">Distributed Block Building</a>, and <a href="https://x.com/VitalikButerin/status/1741190491578810445" rel="noopener nofollow ugc">Application-layer MEV minimization</a>, are examples of ideas that require censorship resistance and control over ordering, but we haven’t yet addressed the pre-requisites. Solving these allows us to kill 5 birds with 1 stone. But to do this we need to think from first principles and work on the underlying root causes rather than tinkering with a thin veneer on top of the protocol.</p>
<p>Solving MEV at the protocol level requires buy in from all three levels of the chain:</p>
<ol>
<li><strong>Consensus Layer:</strong> Multiple concurrent proposers.</li>
<li><strong>Execution Layer:</strong> Delayed execution and deterministic scheduling rules.</li>
<li><strong>Application Layer:</strong> Order-agnostic applications.</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#consensus-layer-2" name="consensus-layer-2"></a>Consensus layer</h2>
<p>We cannot get anywhere without vastly improved censorship resistance at the consensus layer. This is what allows us to hold auctions and prevent censorship of competing bids. The root cause of Ethereum’s weak censorship resistance is the fact that only a single entity can include transactions during each 12 second slot. <strong>Multiple concurrent proposers (MCP)</strong> fixes this problem. Instead of coming to consensus on an ordered block of transactions from a single block proposer, each of the K proposers propose a set of transactions at the same time. The protocol then aggregates these proposals using a <strong>common subset</strong> primitive (or a similar algorithm, this is an active area of research), yielding an unordered set of transactions which are to be included in the block.</p>
<p>MCP solves the problem of censorship-resistant inclusion, achieving the goals of <a href="https://ethereum-magicians.org/t/eip-7547-inclusion-lists/17474" rel="noopener nofollow ugc">Inclusion Lists</a> in a more natural way. The output is an unordered set of transactions, so it does not solve the problem of reordering. That will be the responsibility of the execution layer.</p>
<p>MCP is an area of active study and we encourage people to get involved. See SMG <a href="https://mechanism.org/spec/01" rel="noopener nofollow ugc">SPEC-01</a> for a theoretical description of MCP. Work is currently underway at SMG to formally specify MCP and create a proposed implementation of a gadget for use in the Ethereum protocol. Contact us if you are interested in working on this.</p>
<h2><a class="anchor" href="https://ethresear.ch#execution-layer-3" name="execution-layer-3"></a>Execution layer</h2>
<p>Ethereum’s execution layer must be upgraded to solve the problem of transaction reordering. To do this, we must delay the calculation of the state root to the next block so that the execution layer has time to implement a deterministic ordering rule.</p>
<p>Once it has the transactions, the execution layer has a new important job: figuring out how to order them. To do this, we need to select a <strong>deterministic scheduling rule</strong>. This is an area of active study where we encourage people to get involved. There are many promising candidates: <a href="https://www.paradigm.xyz/2024/06/priority-is-all-you-need" rel="noopener nofollow ugc">priority fee ordering</a>, as-needed execution, and <a href="https://github.com/flashbots/mev-boost/issues/139" rel="noopener nofollow ugc">distributed block building</a>. We will elaborate on the last two in an upcoming article.</p>
<p>With delayed execution and a deterministic scheduling rule, Ethereum’s execution layer will determine the order of transactions in a block, allowing it to achieve the same goals as <a href="https://github.com/flashbots/mev-boost/issues/139" rel="noopener nofollow ugc">distributed block building</a> and <a href="https://ethresear.ch/t/epbs-design-constraints/18728">ePBS</a> in a more natural way. In addition, since the ordering is enforced by the logic of the protocol, not by the goodwill of any particular validator, the protocol can burn all the fees at this stage, achieving the goals of <a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590">MEV Burn</a>.</p>
<h2><a class="anchor" href="https://ethresear.ch#application-layer-4" name="application-layer-4"></a>Application layer</h2>
<p>Assuming we succeed in the above upgrades, Ethereum’s application layer will be free to upgrade their applications to be natively MEV-resistant while remaining totally onchain. We call the class of things they will do <strong>order-agnostic applications</strong> or order-agnostic mechanisms.</p>
<p>For example take the problem of liquidation MEV. For the sake of argument, suppose we have 1000 ETH that needs to be liquidated for DAI. We don’t know what the appropriate price is for the ETH, so we have two options: we can guess the right price and have a posted price available to the first person who claims it, which is how Compound and Aave work, and leads to tremendous value leaked to liquidation races, reducing UX. Or, we can hold a Dutch auction, which leads to slightly less value leakage, but doesn’t allow us to clear the distressed debt right away. But now, with MCP and deterministic scheduling, these protocols can simply hold an onchain auction for the right to liquidate 1000 ETH and elicit the price that way.</p>
<p>Order agnostic application design has a number of benefits, and there are many more examples of places where MEV leaks that can be solved. Future posts will elaborate on this.</p>
<h2><a class="anchor" href="https://ethresear.ch#conclusion-5" name="conclusion-5"></a>Conclusion</h2>
<p>The successful implementation of these upgrades will result in a much friendlier Ethereum for both developers and users. The first step of this research program is fleshing out and proving the security of a multi proposer design with simultaneous release. Other blockchains have multiple proposers, but are not designed in the same way or for the same purpose. If you are a consensus researcher interested in working on this topic, please reach out, we have funding available for this.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/execution-consensus-separation/19964">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 03 Jul 2024 19:09:20 +0000</pubDate>
</item>
<item>
<title>Fork Choice compliance test suites &amp; test generator</title>
<link>https://ethresear.ch/t/fork-choice-compliance-test-suites-test-generator/19954</link>
<guid>https://ethresear.ch/t/fork-choice-compliance-test-suites-test-generator/19954</guid>
<content:encoded><![CDATA[
<div> 关键词：Fork Choice、测试生成器、测试套件、性能优化、未来步骤

总结:<br />
Fork Choice合规性测试生成器已完成初步实施，能够为客户端开发团队提供测试模板。已经生成了三个测试套件（Tiny、Small和Standard），涵盖不同规模的测试目的。尽管生成Extended测试套件需要时间，但支持多进程以提高效率。目前的测试速度较慢，未来将优化性能并增加模型灵活性，计划进行覆盖率导向的模糊测试和新的测试向量格式。整体目标是简化测试采用并确保协议的正确实现。 <div>
<p>This is a preliminary announcement, we’ll officially announce during the next All Core Devs call.</p>
<p>We (TxRx team, ConsenSys) have implemented a Fork Choice compliance test generator as well as have generated Fork Choice compliance test suites.</p>
<p>Overall F/C compliance testing methodology is described <a href="https://hackmd.io/@ericsson49/fork-choice-implementation-vs-spec-testing" rel="noopener nofollow ugc">here</a>.</p>
<p>In this report we briefly describe the results of the initial implementation phase (i.e. the F/C test generator and F/C test suites).  A more detailed description of the work is TBD.</p>
<p>This work was supported by a grant from the Ethereum Foundation.</p>
<h1><a class="anchor" href="https://ethresear.ch#implementation-status-1" name="implementation-status-1"></a>Implementation status</h1>
<h2><a class="anchor" href="https://ethresear.ch#test-generator-2" name="test-generator-2"></a>Test generator</h2>
<p>The initial version of the Fork Choice tests generator is implemented and currently available as a draft <a href="https://github.com/ethereum/consensus-specs/pull/3831" rel="noopener nofollow ugc">consensus-specs PR</a>. We have been focusing on minimizing efforts for client implementer teams to adopt the generated tests. The only a small change to the existing <a href="https://github.com/ethereum/consensus-specs/tree/dev/tests/formats/fork_choice" rel="noopener nofollow ugc">FC test format</a> is the addition of a <a href="https://github.com/ericsson49/eth2.0-specs/tree/fc-compliance2/tests/formats/fork_choice#checks-step" rel="noopener nofollow ugc">new check</a>, which is safe to ignore initially.</p>
<h2><a class="anchor" href="https://ethresear.ch#test-suites-3" name="test-suites-3"></a>Test suites</h2>
<p>We have developed test generation parameters for three suites at the moment.</p>
<div class="md-table">
<table>
<thead>
<tr>
<th>Test suite</th>
<th>size</th>
<th>Purpose</th>
<th>Status</th>
<th>Link</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tiny</td>
<td>135 tests</td>
<td>Demonstration, smoke testing</td>
<td>Done</td>
<td><a href="https://drive.google.com/file/d/1dWbkJY27fhOVX8i4aUuZ7VBkH3P_Vr1i/view?usp=drive_link" rel="noopener nofollow ugc">link</a></td>
</tr>
<tr>
<td>Small</td>
<td>1472 tests</td>
<td>Initial adoption, smoke testing</td>
<td>Done</td>
<td><a href="https://drive.google.com/file/d/1EAIeTL5F3zelXK5pBkSLuawJjuLWQx3r/view?usp=drive_link" rel="noopener nofollow ugc">link</a></td>
</tr>
<tr>
<td>Standard</td>
<td>13240 tests</td>
<td>Main testing</td>
<td>Done</td>
<td><a href="https://drive.google.com/file/d/1_56JObwYWHARgm5QYmE4vrkcLEru854G/view?usp=drive_link" rel="noopener nofollow ugc">link</a></td>
</tr>
<tr>
<td>Extended</td>
<td>about 100K tests</td>
<td>Extended testing</td>
<td>TBD</td>
<td></td>
</tr>
</tbody>
</table>
</div><p><strong>Note</strong>: We are able to generate the Extended test suite. However, it will take significant time (about a week), therefore, we have delayed actual test suite generation until it will be demanded.</p>
<p>It should be possible to generate test suites for any fork (Altair, Capella, Deneb) and preset (mainnet or minimal). However, test generation for mainnet is very slow. We have tested minimal/altair and minimal/deneb.</p>
<p>Test generation currently is slow (about 10-15 seconds per test on average). However, a multiprocessing mode is supported (about 2 seconds per test on Apple M1). Generation of the Standard test suite takes about 8 hours (multiprocessing mode) or two days (single process mode).</p>
<p>The reasons of slow performance are known and are to be alleviated in future. Currently, our top priority is to simplify adoption of the new test suites.</p>
<h2><a class="anchor" href="https://ethresear.ch#testing-the-tests-4" name="testing-the-tests-4"></a>Testing the tests</h2>
<p>We have run the generated tests against <a href="https://github.com/Consensys/teku" rel="noopener nofollow ugc">Teku</a>, using Teku test runner and against the official executable Fork Choice spec (minimal/deneb), using a simple Python <a href="https://github.com/ericsson49/eth2.0-specs/blob/4a0745bd7c0ec6d6a216a8baf81bcb80c30ccaa3/tests/generators/fork_choice_generated/test_run.py" rel="noopener nofollow ugc">test runner</a>.</p>
<h1><a class="anchor" href="https://ethresear.ch#test-generation-approach-5" name="test-generation-approach-5"></a>Test generation approach</h1>
<p>The test generation approach is a mix of model-based and fuzz testing.</p>
<p>Principles:</p>
<ul>
<li>the Fork Choice spec is virtually “decomposed” into two parts: topological sorting of events and actual event processing</li>
<li>tests are generated for the event processing part, the topological sorting part is addressed via event shuffling (time shift plus drop/duplication)</li>
<li>models are used to describe the spec aspects that we want to cover. There are two flavors: trees of various shapes (for block trees and super-majority link trees) and predicates to be covered (<code>filter_block_tree</code>)</li>
<li>for each model there can be multiple solutions, each solution can be seen as a template (e.g. SM link tree + block tree) which can be instantiated in multiple ways (varying validator actions)</li>
<li>each test case can be mutated multiple times</li>
</ul>
<p>Tests are generated with four steps:</p>
<ol>
<li>Models (implemented using MiniZinc), describing abstract coverage aspects that we want to cover. Currently there are three models: <a href="https://github.com/ericsson49/eth2.0-specs/blob/4a0745bd7c0ec6d6a216a8baf81bcb80c30ccaa3/tests/generators/fork_choice_generated/model/minizinc/SM_links.mzn" rel="noopener nofollow ugc">SM link</a> (super-majority link) tree model, <a href="https://github.com/ericsson49/eth2.0-specs/blob/4a0745bd7c0ec6d6a216a8baf81bcb80c30ccaa3/tests/generators/fork_choice_generated/model/minizinc/Block_tree.mzn" rel="noopener nofollow ugc">Block tree</a> model and <a href="https://github.com/ericsson49/eth2.0-specs/blob/4a0745bd7c0ec6d6a216a8baf81bcb80c30ccaa3/tests/generators/fork_choice_generated/model/minizinc/Block_cover3.mzn" rel="noopener nofollow ugc">Block cover</a> model.</li>
<li>For each model a set of solutions is produced. The models are parameterized, which affects the size of solution set generated.
<ul>
<li>SM link and block tree solutions are combined into a single block tree.</li>
</ul>
</li>
<li>Each solution is instantiated using two test instantiators (<a href="https://github.com/ericsson49/eth2.0-specs/blob/4a0745bd7c0ec6d6a216a8baf81bcb80c30ccaa3/tests/generators/fork_choice_generated/instantiators/block_tree.py" rel="noopener nofollow ugc">block tree</a> and <a href="https://github.com/ericsson49/eth2.0-specs/blob/4a0745bd7c0ec6d6a216a8baf81bcb80c30ccaa3/tests/generators/fork_choice_generated/instantiators/block_cover.py" rel="noopener nofollow ugc">block cover</a>). The instantiation is randomized, i.e. a coin is flipped on each decision point. This results in a complete Fork Choice test case (i.e. <em>anchor state</em> plus a sequence of <em>tick</em> | <em>block</em> | <em>attestation</em> | <em>attester_slashing</em> events).</li>
<li>Each test case is mutated via <a href="https://github.com/ericsson49/eth2.0-specs/blob/4a0745bd7c0ec6d6a216a8baf81bcb80c30ccaa3/tests/generators/fork_choice_generated/instantiators/mutation_operators.py" rel="noopener nofollow ugc">mutation</a> (shuffling) operators. Currently, there are thee mutation operator: time shift, drop and duplicate (with consequent shifting).</li>
</ol>
<p>The models are developed manually.<br />
Solutions to the models are produced with a special <a href="https://github.com/ericsson49/eth2.0-specs/blob/4a0745bd7c0ec6d6a216a8baf81bcb80c30ccaa3/tests/generators/fork_choice_generated/generate_test_instances.py" rel="noopener nofollow ugc">generator</a>.<br />
Test instantiators and mutations are performed with <a href="https://github.com/ericsson49/eth2.0-specs/blob/4a0745bd7c0ec6d6a216a8baf81bcb80c30ccaa3/tests/generators/fork_choice_generated/test_gen.py" rel="noopener nofollow ugc">test_gen.py</a>.</p>
<p>After tests are generated, one can validate the produced test steps using <a href="https://github.com/ericsson49/eth2.0-specs/blob/4a0745bd7c0ec6d6a216a8baf81bcb80c30ccaa3/tests/generators/fork_choice_generated/test_run.py" rel="noopener nofollow ugc">test_run.py</a> script, which executes the steps using the pyspecs, performing prescribed checks.</p>
<h1><a class="anchor" href="https://ethresear.ch#test-structure-6" name="test-structure-6"></a>Test structure</h1>
<div class="md-table">
<table>
<thead>
<tr>
<th>Test group</th>
<th>size (standard suite)</th>
<th>parameters (solutions + variations + mutations)</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Block tree</td>
<td>4096 tests</td>
<td>1024*2*(1+1)</td>
<td>focus on trees of varying shapes</td>
</tr>
<tr>
<td>Block weight</td>
<td>2048 tests</td>
<td>8*64*(1+3)</td>
<td>focus on producing block trees with varying weights</td>
</tr>
<tr>
<td>Shuffling</td>
<td>2048 tests</td>
<td>8*4*(1+63)</td>
<td>focus on shuffling/mutation operators</td>
</tr>
<tr>
<td>Attester slashing</td>
<td>1024 tests</td>
<td>8*16*(1+7)</td>
<td>focus on attester slashing</td>
</tr>
<tr>
<td>Invalid messages</td>
<td>1024 tests</td>
<td>8*32*(1+3)</td>
<td>focus on invalid messages</td>
</tr>
<tr>
<td>Block cover</td>
<td>3000 tests</td>
<td>60*5*(1+9)</td>
<td>cover various combinations of predicates from the <code>filter_block_tree</code> method</td>
</tr>
</tbody>
</table>
</div><h1><a class="anchor" href="https://ethresear.ch#future-steps-7" name="future-steps-7"></a>Future steps</h1>
<ul>
<li>improve performance. Performance is adequate right now (for the initial adoption phase). But is the main blocker otherwise.</li>
<li>more flexible test generation. More and better models, better instantiators, better mutation operators.</li>
<li>coverage-guided fuzzing</li>
<li>new test vector format (don’t need full test cases for fuzz testing, as need to compare against the FC spec anyway)</li>
</ul>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/fork-choice-compliance-test-suites-test-generator/19954">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 02 Jul 2024 20:00:49 +0000</pubDate>
</item>
<item>
<title>Ethereum Node Message Propagation Bandwidth Consumption</title>
<link>https://ethresear.ch/t/ethereum-node-message-propagation-bandwidth-consumption/19952</link>
<guid>https://ethresear.ch/t/ethereum-node-message-propagation-bandwidth-consumption/19952</guid>
<content:encoded><![CDATA[
<div> 关键词：GossipSub, Bandwidth consumption, Hermes, Optimization, Ethereum P2P network

总结:
GossipSub在以太坊P2P网络中的性能研究发现，SENT_IHAVE和RECV_IHAVE消息消耗了大量带宽，分别占总带宽的23.4%和10%，对出站和入站流量影响显著。研究建议改进机制以减少重复消息，这将节省约42%的总带宽。尽管Hermes节点配置非标准，但研究结果证实了优化空间。与其他节点的比较显示，当前以太坊节点的带宽使用率相对较低，仍有提升余地。总的来说，这项工作旨在通过深入了解GossipSub的性能，为协议优化提供依据。 <div>
<h1><a class="anchor" href="https://ethresear.ch#summary-tldr-1" name="summary-tldr-1"></a>Summary &amp; TL;DR</h1>
<p>The ProbeLab team (<a href="https://probelab.io/" rel="noopener nofollow ugc">probelab.io </a>) is carrying out a study on the performance of Gossipsub in Ethereum’s P2P network. Following from our previous post on the <a class="inline-onebox" href="https://ethresear.ch/t/number-duplicate-messages-in-ethereums-gossipsub-network/19921">Number Duplicate Messages in Ethereum's Gossipsub Network</a>, in this post we investigate bandwidth consumption at the GossipSub level, i.e., bandwidth consumption for message propagation. The target of the study is to identify the protocol components that consume the biggest share of network bandwidth. The study has been co-authored by <a class="mention" href="https://ethresear.ch/u/cortze">@cortze</a> and <a class="mention" href="https://ethresear.ch/u/yiannisbot">@yiannisbot</a>.</p>
<p>For the purposes of this study, we have built a tool called <strong>Hermes, which acts as a GossipSub listener and tracer</strong> (<a href="https://github.com/probe-lab/hermes/" rel="noopener nofollow ugc">GitHub - probe-lab/hermes: A Gossipsub listener and tracer. </a>). Hermes subscribes to all relevant pubsub topics and traces all protocol interactions. The results reported here are from a 3.5hr trace.</p>
<p><strong>Study Description:</strong> The distributed nature of p2p systems makes them generally less effective in computational, latency, and bandwidth consumption. This is due to the extra interactions between nodes needed to organize a p2p network without a central authority that bridges between peers. Thus, taking care of processes, such as peer or content discovery, content sharing, and message broadcasting often become a challenge, or bottleneck.</p>
<p>Ethereum is not different in that respect. Message propagation takes a large portion of the network bandwidth used by a node in the Ethereum network. This study investigates bandwidth consumption at the GossipSub level. The target is to identify the protocol components that consume the biggest share of network bandwidth.</p>
<p><strong>TL;DR:</strong> Despite the fact that the configuration of our <code>Hermes</code> node, which, in this case, doesn’t represent a standard node in the Ethereum network, the bandwidth consumption numbers of GossipSub validate that there’s plenty of space for optimization.</p>
<p>We observed that a significant portion of bandwidth is spent on <code>SENT_IHAVE</code> messages (23.4% of the total bandwidth and 30% of the total outgoing bandwidth) and <code>RECV_IHAVE</code> messages (10% of the total bandwidth, and 42% of the total inbound bandwidth).</p>
<p>More than anything, these findings validate the improvement recommendations made during our previous study on the “Effectiveness of Gossipsub’s gossip mechanism”: <a class="inline-onebox" href="https://ethresear.ch/t/gossip-iwant-ihave-effectiveness-in-ethereums-gossipsusb-network/19686">Gossip IWANT/IHAVE Effectiveness in Ethereum's Gossipsusb network</a></p>
<p>Taking into account that a node doesn’t only receive duplicated messages but also generates duplicates to others, we strongly recommend pushing the <a href="https://github.com/libp2p/specs/pull/560" rel="noopener nofollow ugc">GossipSub1.2</a> initiative, as it will effectively eliminate the bandwidth wasted on receiving or generating duplicates, which amounts to ~42% of total bandwidth.</p>
<h1><a class="anchor" href="https://ethresear.ch#results-on-bandwidth-consumption-2" name="results-on-bandwidth-consumption-2"></a>Results on Bandwidth Consumption</h1>
<blockquote>
<p><img alt=":eyes:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/eyes.png?v=12" title=":eyes:" width="20" /> NOTES: The bandwidth usage displayed in this study is limited to:</p>
<ul>
<li>The <code>Holesky</code> network</li>
<li>The GossipSub RPC calls</li>
<li>The following GossipSub topics:
<ul>
<li><code>beacon_block</code></li>
<li><code>beacon_aggregate_and_proof</code></li>
<li><code>sync_commmittee_contribution_and_proof</code></li>
<li><code>attester_slashing</code></li>
<li><code>proposer_slashing</code></li>
<li><code>voluntary_exit</code> * (check <code>Hermes</code> issue → <a class="inline-onebox" href="https://github.com/probe-lab/hermes/issues/24" rel="noopener nofollow ugc">Broadcasting of invalid `voluntary_exit` messages to mesh peers · Issue #24 · probe-lab/hermes · GitHub</a>)</li>
<li><code>bls_to_execution_change</code></li>
</ul>
</li>
<li>The bandwidth of <code>SENT_IHAVE</code> and <code>RECV_IHAVE</code> RPC calls has been calculated based on the number of bytes per <code>topic</code>  strings and <code>msg_ids</code> that were inside.</li>
</ul>
</blockquote>
<h2><a class="anchor" href="https://ethresear.ch#netin-vs-netout-3" name="netin-vs-netout-3"></a>NetIn vs NetOut</h2>
<p>The study starts with a general overview of what is the ratio of sent vs received bandwidth consumption. The following graph shows that on the <code>Hermes</code> node, the biggest share of the bandwidth comes from the data that we send out to the connected peers.</p>
<p>The total outbound bandwidth is around 3 to 4 times higher than the inbound. Note that <code>Hermes</code> differs from a standard node in that it keeps more peer connections (around 250 peers). This clearly has a significant impact on bandwidth usage. That said, although the numbers are not representative of the bandwidth usage of a normal node in absolute terms, the percentage split still represents that of a normal node.</p>
<p>Narrowing down, we observe a ratio of 700-800 KB/s for outgoing traffic and 200 KB/s for incoming traffic.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/f/4fb2e194b5b97ea9f84092740d059ad4447d2061.jpeg" title="bandwidth-in-out"><img alt="bandwidth-in-out" height="309" src="https://ethresear.ch/uploads/default/optimized/3X/4/f/4fb2e194b5b97ea9f84092740d059ad4447d2061_2_517x309.jpeg" width="517" /></a></div><p></p>
<h2><a class="anchor" href="https://ethresear.ch#bandwidth-based-on-each-event-type-4" name="bandwidth-based-on-each-event-type-4"></a>Bandwidth based on each event type</h2>
<p>GossipSub sends multiple types of messages with different purposes. From control messages to keep the mesh stable to pure messages or gossip  <code>IHAVE</code> / <code>IWANT</code>  messages to ensure that the host didn’t miss any message. Each of these message types requires sending RPC calls, adding up to the total of sent and received network traffic.</p>
<p>The following graphs isolate the bandwidth attributed to each of the events. The first one shows the raw KB/s over time, and the second one shows the percentage of each event over the aggregated total.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/3/b352576dc2b9350a99470bde3eb0710d0e710d3c.jpeg" title="bandwidth-by-event"><img alt="bandwidth-by-event" height="309" src="https://ethresear.ch/uploads/default/optimized/3X/b/3/b352576dc2b9350a99470bde3eb0710d0e710d3c_2_517x309.jpeg" width="517" /></a></div><p></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/b/ab8bda76fba67303a9b9991902f5ff1805c63175.jpeg" title="bandwidth-ratio-by-event"><img alt="bandwidth-ratio-by-event" height="309" src="https://ethresear.ch/uploads/default/optimized/3X/a/b/ab8bda76fba67303a9b9991902f5ff1805c63175_2_517x309.jpeg" width="517" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#percentage-table-5" name="percentage-table-5"></a>Percentage Table</h3>
<pre><code>| Event | % of total BW | % of Received BW | % of Sent BW |
| --- | --- | --- | --- |
| RECV_GRAFT | 0.000367 | 0.001565 | ———————— |
| RECV_IHAVE | 9.974349 | 42.537746 | ———————— |
| RECV_IWANT | 2.368042 | 10.099021 | ———————— |
| RECV_MSG  (duplicated) | 7.347250 | 31.333920 | ———————— |
| RECV_MSG | 3.640691 | 15.526507 | ———————— |
| RECV_PRUNE | 0.002973 | 0.012678 | ———————— |
| RECV_SUBS | 0.114559 | 0.488562 | ———————— |
| SENT_GRAFT | 0.002863 | ———————— | 0.003740 |
| SENT_IHAVE | 23.404913 | ———————— | 30.573967 |
| SENT_IWANT | 0.094569 | ———————— | 0.123536 |
| SENT_MSG | 53.049257 | ———————— | 69.298539 |
| SENT_PRUNE | 0.000164 | ———————— | 0.000214 |
| SENT_SUBS | 0.000003 | ———————— | 0.000004 |
</code></pre>
<p>From the above graphs, we can observe that:</p>
<ul>
<li>The <code>SENT_MSG</code> event is the one that consumes the most network traffic, with a total of 53% of the total network traffic and 69% of the total sent traffic.<br />
It has a spiky oscillation between 500 to 700 KB/s, and it is clearly the most bandwidth consuming event.<br />
It is hard to define which is the ratio of duplicates that all those sent messages generate on the remote side. However, we could assume that it would follow a similar pattern to the <code>RECV_MSG</code> one (2 duplicate bytes per 1 original byte).</li>
<li>Surprisingly, the <code>SENT_IHAVE</code> event follows <code>SENT_MSG</code>s in terms of consumed bandwidth with a total of 23.4% of the total bandwidth and 30% of the total outgoing bandwidth. Interestingly, subscribing to topics with a high frequency of messages (even if they are small in size), does have an impact on the bandwidth that we use sending those <code>IHAVE</code> messages.<br />
Each <code>IHAVE</code> is limited to <code>5,000</code> message IDs; however, with a message ID of 40 bytes, it still adds up to a maximum of 200KBs in message IDs on every heartbeat (0.7s in the case of Ethereum).</li>
<li><code>RECV_IHAVES</code> represent 10% of the total bandwidth, and 42% of the total inbound bandwidth, with an inbound network bandwidth requirement of 100KB/s.</li>
<li>The above two points showcase that, far from being negligible on the overall value they provide, the total bandwidth used on <code>IHAVE</code> messages represents almost 400KB/s, consuming 23% of the total outgoing bandwidth and more than 40% of the incoming bandwidth.</li>
<li>The <code>RECV_MSG</code> events remain in the fourth position with a representation of 11% of the total consumed bandwidth, where only 3.6% belong to unique or original messages, and the remaining 7.3% belong to duplicates. In terms of the overall inbound bandwidth, they represent 15% and 31%, respectively, for original and duplicated received messages.</li>
<li>On a much lower ratio, the whole list of <code>RECV_IWANT</code> messages stays within a lower 2.3% of the total bandwidth usage, which represents 10% of the total received bytes.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#comparison-with-live-nodes-6" name="comparison-with-live-nodes-6"></a>Comparison with live nodes</h2>
<p>In order to validate the previous measurements taken from the GossipSub module at <code>Hermes</code>, we’ve compared the bandwidth usage ratios with standard running Ethereum nodes:</p>
<ul>
<li>Local Prysm node at home setup (Holesky) reports an average received network traffic of 386KB/s and a sent network traffic of 580KB/s.<br />
Although the numbers might be slightly different, these measurements take the whole traffic of the Beacon Node docker container, which includes:
<ul>
<li>Peer discovery</li>
<li>Requests/Responses like <code>beacon_blocs</code> or <code>blobs</code> by range or by root</li>
</ul>
</li>
</ul>
<p>The MigaLabs <a href="https://monitoreth.io/node_metrics#network-in-out" rel="noopener nofollow ugc">public dashboard</a> at <a href="https://monitoreth.io/node_metrics" rel="noopener nofollow ugc">monitor.eth</a> shows slightly bigger bandwidth usage than the ones we measured. However, it is unclear whether the measurement includes the Execution Layer. The reported bandwidth reports an average of 290KB/s inbound and 1.2MB/s outbound, although it doesn’t include many data points (5 points per hour) and the variation is noticeable.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/7/c7cf74da4a25ce8b98d757eeaebf56696d2c6aa6.jpeg" title="migalabs"><img alt="migalabs" height="315" src="https://ethresear.ch/uploads/default/optimized/3X/c/7/c7cf74da4a25ce8b98d757eeaebf56696d2c6aa6_2_517x315.jpeg" width="517" /></a></div><p></p>
<h1><a class="anchor" href="https://ethresear.ch#conclusions-and-takeaways-7" name="conclusions-and-takeaways-7"></a>Conclusions and takeaways</h1>
<p>Despite the fact that the configuration of our <code>Hermes</code> node, which, in this case, doesn’t represent a standard node in the Ethereum network, the bandwidth consumption numbers of GossipSub validate that there’s plenty of space for optimization.</p>
<p>We observed that <strong>a significant portion of bandwidth is spent on <code>SENT_IHAVE</code> (23.4% of the total bandwidth and 30% of the total outgoing bandwidth) and <code>RECV_IHAVE</code> (10% of the total bandwidth, and 42% of the total inbound bandwidth)</strong>.</p>
<p>More than anything, these findings validate the improvement recommendations made during our previous study on the “Effectiveness of Gossipsub’s gossip mechanism”: <a class="inline-onebox" href="https://ethresear.ch/t/gossip-iwant-ihave-effectiveness-in-ethereums-gossipsusb-network/19686">Gossip IWANT/IHAVE Effectiveness in Ethereum's Gossipsusb network</a></p>
<p>Taking into account that a node doesn’t only receive duplicated messages but also generates duplicates to others, we strongly recommend pushing the <a href="https://github.com/libp2p/specs/pull/560" rel="noopener nofollow ugc">GossipSub1.2</a> initiative, as it will effectively eliminate the bandwidth wasted on receiving or generating duplicates, which amounts to ~42% of total bandwidth.</p>
<p>Even currently though, the network bandwidth usage of a host in the Ethereum network (around 300 KB/s inbound and 1.1 MB/s outbound, including the EL) still constitutes a small percentage of the <a href="https://fairinternetreport.com/research/internet-speed-by-country/" rel="noopener nofollow ugc">average household</a> bandwidth availability, which varies between 8MB/s and 26MB/s depending on the region.</p>
<p>For more details and <strong>weekly network health reports on Ethereum’s discv5 DHT network</strong> head over to <a href="https://probelab.io/" rel="noopener nofollow ugc">probelab.io</a>.</p>
            <p><small>2 posts - 2 participants</small></p>
            <p><a href="https://ethresear.ch/t/ethereum-node-message-propagation-bandwidth-consumption/19952">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 02 Jul 2024 14:39:15 +0000</pubDate>
</item>
<item>
<title>Fork Choice Attacks and Protections in EPBS</title>
<link>https://ethresear.ch/t/fork-choice-attacks-and-protections-in-epbs/19951</link>
<guid>https://ethresear.ch/t/fork-choice-attacks-and-protections-in-epbs/19951</guid>
<content:encoded><![CDATA[
<div> 关键词：ePBS、fork choice attack、proposer boost、builder boost、ex-anti attack

总结:
本文探讨了ePBS（增强版持久拜占庭容错）中的分叉选择攻击，重点关注新的分叉选择提升参数设计。文章分析了两种主要攻击类型（后抗攻击和外抗攻击），以及在引入builder块后，ePBS中出现的新攻击情景，如攻击者与建设者合谋的情况。作者提到，尽管存在削弱的防御，但攻击者需要更高的恶意比例才能成功，例如40%的proposer boost和20%的attacker committee。文章还讨论了各种攻击的动机、优势和潜在后果，以及相应的防范措施，包括Reveal Boost和Withheld Boost。最后，文章提出了可能的改进方向，如考虑网络同步性和多槽活度的影响。 <div>
<h2><a class="anchor" href="https://ethresear.ch#introduction-1" name="introduction-1"></a>Introduction</h2>
<p>This post explores fork choice attacks through the perspective of ePBS, focusing on the new fork choice boost parameters and the rationale behind their design. We’ll begin by examining why these parameters are crucial, followed by a review of the existing designs. For background reading, I recommend reading <a href="https://ethresear.ch/t/payload-boosts-in-epbs/18769">Payload Boosts in ePBS</a> by Potuz. Additionally, for a deeper understanding of how the LMD GHOST fork choice operates today, consider Ben Edgington’s section on fork choice in his book, <a href="https://eth2book.info/capella/part3/forkchoice/" rel="noopener nofollow ugc">Upgrading Ethereum</a>. Let’s dive in!</p>
<h4><a class="anchor" href="https://ethresear.ch#references-2" name="references-2"></a>References</h4>
<p><a href="https://ethresear.ch/t/payload-boosts-in-epbs/18769">Payload boosts in ePBS</a> - Feb/2024 By Potuz<br />
<a href="https://ethresear.ch/t/sandwitch-attacks-on-epbs/19538/1">Sandwitch attacks on ePBS</a> - May/2024 By Potuz</p>
<h2><a class="anchor" href="https://ethresear.ch#fork-choice-attacks-today-3" name="fork-choice-attacks-today-3"></a>Fork choice attacks today</h2>
<p>We analyze these scenarios from both the attacker’s and the victim’s perspectives, focusing on two consecutive proposal slots, each with distinct proposers. Two primary types of attacks can emerge:</p>
<ol>
<li>The proposer of slot <span class="math">n+1</span> attacks the proposer of slot <span class="math">n</span>.</li>
<li>The proposer of slot <span class="math">n</span> attacks the proposer of slot <span class="math">n+1</span>.</li>
</ol>
<p>To clarify, by “attack,” we mean an attempt to reorg the block out of the canonical chain. The motives behind such a reorg typically include:</p>
<ol>
<li>Stealing the content of the block.</li>
<li>Increasing the time available to build the block, making a 24-second block more valuable than a 12-second one.</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#post-anti-attack-4" name="post-anti-attack-4"></a>Post-anti attack</h3>
<p>The first type of attack is a post-anti attack, where the proposer of slot <span class="math">n+1</span> attempts to reorg the block from slot <span class="math">n</span>. In this scenario, the proposer of <span class="math">n+1</span> utilizes the <a href="https://eth2book.info/capella/part3/forkchoice/phase0/#proposer-boost" rel="noopener nofollow ugc">proposer boost</a> to gain an advantage and potentially reorg the block from slot <span class="math">n</span>. Currently, the proposer boost is set at 40%. This means that as long as the block at slot <span class="math">n</span> receives votes from more than 40% of the beacon committee, it is safe against a reorg. Typically, we define the percentage of the beacon committee that belongs to the attacker as <span class="math">\delta</span>. An attacker can successfully reorg a block if <span class="math">\delta &gt; 1 - PB</span>, which is 60% under today’s parameters.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/f/3f93dac13fced8dbaa43115f19cd6ae45668406d.png" title="Screenshot 2024-06-26 at 12.57.24 PM"><img alt="Screenshot 2024-06-26 at 12.57.24 PM" height="297" src="https://ethresear.ch/uploads/default/optimized/3X/3/f/3f93dac13fced8dbaa43115f19cd6ae45668406d_2_690x297.png" width="690" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#ex-anti-attack-5" name="ex-anti-attack-5"></a>Ex-anti attack</h3>
<p>The second type of attack is known as the ex-anti attack, where the proposer of slot <span class="math">n</span> attempts to reorg the block from slot <span class="math">n+1</span>. This type of attack is inherently difficult to pull off because the proposer boost grants a 40% advantage to the block at slot <span class="math">n+1</span>. To successfully carry out this attack, the attacker’s beacon committee must withhold their attestations and block then release them synchronously which occurs shortly after the block at slot <span class="math">n+1</span> is published. To reorg the block at slot <span class="math">n+1</span>, the attacker’s beacon committee support must exceed the proposer boost. We can assert that an attacker can reorg a block if <span class="math">\delta &gt; PB</span>, which is 40% under today’s parameter.</p>
<p>It is worth mentioning in ex-anti attack, attackers who propose multiple consecutive slots have an added advantage. For two slots, the effectiveness of the attack can be simplified to the expression <span class="math">\delta / 2 &gt; PB</span>, requiring only 20% of the stake per slot to reorg an honest block.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/c/ccb28f1581907f015d5780dae95dd78444ba59d8.png" title="Screenshot 2024-06-26 at 12.57.38 PM"><img alt="Screenshot 2024-06-26 at 12.57.38 PM" height="340" src="https://ethresear.ch/uploads/default/optimized/3X/c/c/ccb28f1581907f015d5780dae95dd78444ba59d8_2_690x340.png" width="690" /></a></div><p></p>
<h2><a class="anchor" href="https://ethresear.ch#fork-choice-attacks-in-epbs-6" name="fork-choice-attacks-in-epbs-6"></a>Fork choice attacks in ePBS</h2>
<p>In the ePBS model, the introduction of a <strong>builder block between two proposer blocks</strong> complicates the landscape of potential attacks beyond what we see today. This addition expands the array of possible attack scenarios:</p>
<h3><a class="anchor" href="https://ethresear.ch#pre-epbs-scenarios-7" name="pre-epbs-scenarios-7"></a>Pre-ePBS Scenarios:</h3>
<ol>
<li><strong>Proposer <span class="math">n+1</span> attacking proposer <span class="math">n</span></strong> - This scenario concerns post-anti reorg safety.</li>
<li><strong>Proposer <span class="math">n</span> attacking proposer <span class="math">n+1</span></strong> - This scenario concerns ex-anti reorg safety.</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#post-epbs-scenarios-8" name="post-epbs-scenarios-8"></a>Post-ePBS Scenarios:</h3>
<ol>
<li><strong>Proposer of <span class="math">n+1</span> and builder of <span class="math">n</span> collude and attack proposer of <span class="math">n</span></strong> - This scenario concerns proposer post-anti reorg safety.</li>
<li><strong>Proposer and builder of <span class="math">n</span> collude and attack proposer of <span class="math">n+1</span></strong> - This scenario concerns proposer ex-anti reorg safety.</li>
<li><strong>Proposer of <span class="math">n+1</span> and <span class="math">n</span> collude and attack builder of <span class="math">n</span></strong> - This scenario introduces builder safety, including reorg safety and withholding safety.</li>
</ol>
<p>Before we go into the specific attack scenarios under the ePBS framework, it’s important to establish the incentives for honest builder behavior. Similar to the proposer boost, builders are also incentivized through boosts for honest actions through <a href="https://ethresear.ch/t/payload-timeliness-committee-ptc-an-epbs-design/16054">payload timeliness committee</a>.</p>
<ul>
<li><strong>Reveal Boost (<span class="math">RB</span>)</strong>: Awarded to builders who timely reveal their payloads.</li>
<li><strong>Withheld Boost (<span class="math">WB</span>)</strong>: Granted if a builder, feeling unsafe about revealing the payload, opts to release a withheld message. This boost gives weight to the parent block of the committed consensus block.</li>
</ul>
<p>These boosts also ensure both builder <strong>reveal</strong> and <strong>withhold</strong> safety. Builder reveal safety means that if the builder acted honestly and revealed a payload in a timely fashion (as attested by the PTC), then the revealed payload should be on-chain. Builder withhold safety means that if a beacon block containing a builder’s header is withheld or revealed late, then that beacon block should not be the canonical head of the blockchain in the view of honest validators.</p>
<p>To ensure clarity and maintain focus throughout our discussion, we will designate the boosts as follows: Reveal Boost (<span class="math">RB</span>), Withheld Boost (<span class="math">WB</span>), and Proposer Boost (<span class="math">PB</span>). The specific values of these boosts will be displayed towards the end of the post. Now, let’s explore the first scenario: the proposer post-anti attack in ePBS.</p>
<h3><a class="anchor" href="https://ethresear.ch#proposer-post-anti-attack-9" name="proposer-post-anti-attack-9"></a>Proposer post-anti attack</h3>
<p>As you may have noted, this scenario is similar to the post-anti attack today, except that the builder of <span class="math">n</span> colludes with the proposer of <span class="math">n+1</span>. We also assume that a portion of the beacon committee is part of the malicious team, represented by <span class="math">\delta</span>. The post-anti attack is successful if <span class="math">WB + PB + \delta &gt; 1 - \delta</span>. This indicates that post-anti attack resistance is weaker in ePBS due to the added power of the withheld boost from the colluding builder.</p>
<p>Let’s examine the benefits for the attacker in a successful attack:</p>
<ul>
<li>The block at <span class="math">n+1</span> gains two slots worth of transactions by reorg out <span class="math">n</span>, resulting in more time and more transactions, thereby increasing its block value.</li>
<li>Since <span class="math">n</span>'s payload was revealed as withheld, and both <span class="math">n</span>'s builder and <span class="math">n+1</span>'s proposer collude, there is no opportunity to steal <span class="math">n</span>'s payload transaction content. They are all on the same team.</li>
<li>From <span class="math">n</span>'s proposer’s perspective, the loss includes the opportunity to propose a beacon block, and from the protocol’s perspective, it results in the loss of one slot worth of consensus liveness.</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/c/bceef4d59d54e6590addac9fc886b98f4d419f29.png" title="Screenshot 2024-06-26 at 12.57.48 PM"><img alt="Screenshot 2024-06-26 at 12.57.48 PM" height="225" src="https://ethresear.ch/uploads/default/optimized/3X/b/c/bceef4d59d54e6590addac9fc886b98f4d419f29_2_690x225.png" width="690" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#proposer-ex-anti-attack-10" name="proposer-ex-anti-attack-10"></a>Proposer ex-anti attack</h3>
<p>Let’s move on to the second scenario: the proposer ex-anti attack in ePBS. In this scenario, we will examine the most extreme version where the builder’s Reveal Boost (<span class="math">RB</span>) is leveraged for the ex-anti attack. What does this attack look like?</p>
<p>The proposer of slot <span class="math">n</span> withholds the block and the beacon committee, represented by <span class="math">\delta</span>, withholds the attestations. The attacking builder of slot <span class="math">n</span> releases the payload on time to gain the <span class="math">RB</span>. The ex-anti attack is successful if <span class="math">RB + \delta &gt; PB</span>. However, realistically, the proposer will try to split the beacon committee into portions seen (<span class="math">x</span>) and not seen (<span class="math">1-x</span>). This modifies the equation to <span class="math">RB + x + \delta &gt; PB + 1-x</span>.</p>
<p>Let’s examine the benefits for the attacker in a successful attack:</p>
<ul>
<li>The block at slot <span class="math">n</span> reorgs out slot <span class="math">n+1</span>. Unlike a post-anti attack, the builder of slot <span class="math">n</span> must commit and release the payload on time to gain the <span class="math">RB</span>. Due to this commitment:
<ul>
<li>Even if the attack is successful, it only provides one slot of transactions without leading to more time and more transactions. The proposer of slot <span class="math">n+2</span> benefits here.</li>
<li>It cannot steal slot <span class="math">n+1</span>'s transactions because the payload is pre-committed, leaving nothing to steal from the consensus block.</li>
</ul>
</li>
</ul>
<p>In other words, the ex-anti attack is less valuable than the post-anti attack if we assume the worst-case scenario for both.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/0/f/0fb72294c98dbb6a1be55419420ebf8144cfaa62.png" title="Screenshot 2024-06-26 at 12.57.58 PM"><img alt="Screenshot 2024-06-26 at 12.57.58 PM" height="263" src="https://ethresear.ch/uploads/default/optimized/3X/0/f/0fb72294c98dbb6a1be55419420ebf8144cfaa62_2_690x263.png" width="690" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#builder-attacks-11" name="builder-attacks-11"></a>Builder attacks</h3>
<p>Finally, let’s move to the last section: proposers of <span class="math">n</span> and <span class="math">n+1</span> collude to attack the builder of <span class="math">n</span>. We will divide this section into two parts. The first part will focus on reorg out the builder’s payload, and the second part will focus on making the payload part of the canonical chain even if the builder chooses to withhold it.</p>
<h4><a class="anchor" href="https://ethresear.ch#payload-reorging-attack-12" name="payload-reorging-attack-12"></a>Payload reorging attack</h4>
<p>Let’s examine the first part. The proposer of slot <span class="math">n</span> releases the block late / attempts to split the beacon committee view, resulting in <span class="math">x</span> beacon committee members voting for the block and <span class="math">1-x</span> not voting for it. The builder reveals the payload on time and gains a <span class="math">RB</span>. The proposer of slot <span class="math">n+1</span> could then reorg the payload by reorg the entire proposer block of slot <span class="math">n</span>, which is more powerful than just reorganizing the payload itself. The attack is successful if <span class="math">PB + 1 - x &gt; RB + x</span>.</p>
<p>What does a successful attack provide to the attacker?</p>
<ul>
<li>Given that proposers of slots <span class="math">n</span> and <span class="math">n+1</span> are colluding, there is no extra slot advantage gained by reorg out the block at slot $n. It is essentially the same as not proposing a block at slot <span class="math">n</span>.</li>
<li>A minor advantage of the collusion between the two proposers is the ability to steal the payload transactions from slot <span class="math">n</span>. This issue is mitigated if transactions are protected by binding them to slot $n. This scenario is known as a next-slot unbundling attack, which differs from same-slot unbundling. Same-slot unbundling is impossible in ePBS.</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/5/e50f3d20a2b304825cb05436a2a3e610fbddcf4f.png" title="Screenshot 2024-06-26 at 12.58.35 PM"><img alt="Screenshot 2024-06-26 at 12.58.35 PM" height="210" src="https://ethresear.ch/uploads/default/optimized/3X/e/5/e50f3d20a2b304825cb05436a2a3e610fbddcf4f_2_690x210.png" width="690" /></a></div><p></p>
<h4><a class="anchor" href="https://ethresear.ch#payload-withholding-attack-13" name="payload-withholding-attack-13"></a>Payload withholding attack</h4>
<p>Let’s look at the second part. The proposer of slot <span class="math">n</span> releases the block late or tries to split the beacon committee view, resulting in <span class="math">x</span> beacon committee members voting for the block and <span class="math">1-x</span> not voting for it. The builder withholds the payload on time and gains a Withheld Boost (<span class="math">WB</span>). The proposer of slot <span class="math">n+1</span> could attempt to force the builder to fulfill unconditional payment by making the block at slot <span class="math">n</span> canonical, which from the chain’s perspective, appears as if the builder did not release the payload. The attack is successful if <span class="math">PB + x &gt; WB + 1 - x</span>.</p>
<p>What does a successful attack provide to the attacker?</p>
<ul>
<li>The only plausible scenario for this attack is when the builder is not confident in revealing the payload and hence withholds it. In this case, the proposer of slot <span class="math">n+1</span>, colluding with the proposer of slot <span class="math">n</span>, wants to take the builder’s payment regardless.</li>
<li>Another primary advantage is that the block at slot <span class="math">n+1</span> can contain two slots’ worth of transactions since the builder submits an empty payload by withholding.</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/f/5f04c5fc36d5b005e7dcc6c7839e90f40a4a0af7.png" title="Screenshot 2024-06-26 at 12.58.09 PM"><img alt="Screenshot 2024-06-26 at 12.58.09 PM" height="266" src="https://ethresear.ch/uploads/default/optimized/3X/5/f/5f04c5fc36d5b005e7dcc6c7839e90f40a4a0af7_2_690x266.png" width="690" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#boost-numbers-14" name="boost-numbers-14"></a>Boost numbers</h3>
<p>Finally, let’s summarize the equations for each worst-case attack scenario if the attacker wins:</p>
<ol>
<li><strong>Proposers post-anti attack</strong>: <span class="math">WB + PB + \delta &gt; 1 - \delta</span></li>
<li><strong>Proposers ex-anti attack</strong>: <span class="math">RB + x + \delta &gt; PB + 1 - x</span></li>
<li><strong>Builder reorg payload attack</strong>: <span class="math">PB + 1 - x &gt; RB + x</span></li>
<li><strong>Builder withhold payload attack</strong>: <span class="math">PB + x &gt; WB + 1 - x</span></li>
</ol>
<p>Overall, we can derive that the parameters are approximately <span class="math">PB = 20\%</span>, <span class="math">WB = 40\%</span>, <span class="math">RB = 40\%</span>, and <span class="math">\delta = 20\%</span>. This means we can tolerate a malicious beacon committee up to 20%, whereas today, this tolerance is 40%.</p>
<p>The real question to ask is whether the worst-case scenario of a 20% attack even makes sense, as in the ex-anti attack, the builder must release the payload to perform the attack. Nevertheless, it certainly represents a degradation in fork choice. A 20% attack is significantly more dangerous in the post-anti attack than in the ex-anti attack due to the additional time available.</p>
<p>Something we haven’t analyzed here is how multi-slot liveness may play a role in this context. Given (block, slot) voting and under worse network asynchrony conditions, we may experience prolonged empty slots, making recovery difficult. Solutions like a backoff scheme have been proposed, which require further thought and analysis.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/fork-choice-attacks-and-protections-in-epbs/19951">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 02 Jul 2024 14:24:34 +0000</pubDate>
</item>
<item>
<title>P2P ZK Light Client Bridge between Tron and Ethereum L2s</title>
<link>https://ethresear.ch/t/p2p-zk-light-client-bridge-between-tron-and-ethereum-l2s/19931</link>
<guid>https://ethresear.ch/t/p2p-zk-light-client-bridge-between-tron-and-ethereum-l2s/19931</guid>
<content:encoded><![CDATA[
<div> 关键词：Tron USDT、Ethereum L2、Zero-knowledge proofs、Cross-chain bridge、Decentralization

总结:<br />
这篇文章讨论了Tron网络上的USDT在第三世界国家的广泛应用，但其高度中心化的控制导致高额交易费和生态系统孤立。为解决这些问题，作者提出了一种设计，即利用零知识轻验证的跨链桥，将USDT TRC20与以太坊L2网络低成本连接起来。这种设计旨在减少交易费用、促进去中心化并增强两个生态系统的流动性。通过零知识证明确保交易安全，同时简化用户操作，使得从Tron无缝接入Ethereum成为可能，从而降低集中风险并推动全球去中心化金融基础设施的发展。 <div>
<p><em>By <a href="https://x.com/alexhooketh" rel="noopener nofollow ugc">Alex Hook</a>. Thanks to these people for inspiration, feedback and suggestions: <a href="https://x.com/alexanderchopan" rel="noopener nofollow ugc">accountless.eth</a>, <a href="https://x.com/pseudotheos" rel="noopener nofollow ugc">pseudotheos</a>, <a href="https://x.com/domothy" rel="noopener nofollow ugc">Domothy</a>, <a href="https://x.com/DoganEth" rel="noopener nofollow ugc">Dogan Alpaslan</a>, <a href="https://zkp2p.xyz" rel="noopener nofollow ugc">ZKP2P team</a></em></p>
<hr />
<p><strong>Abstract.</strong> USDT on the Tron Network has emerged as a dominant crypto application in the Third World countries. However, the current cartelized control of the Tron Network results in elevated transaction fees, capital concentration, and an ecosystem isolated from other crypto networks. We propose a design for a cost-effective, peer-to-peer bridge from USDT TRC20 to Ethereum L2 networks, utilizing zero-knowledge light verification of the Tron blockchain.</p>
<h1><a class="anchor" href="https://ethresear.ch#introduction-1" name="introduction-1"></a>Introduction</h1>
<p>According to <a href="https://tokenterminal.com/terminal/datasets/stablecoins" rel="noopener nofollow ugc">Token Terminal</a>, USDT on Tron has achieved preeminence by several metrics, including outstanding supply, 30d transfer volume, number of transfers, and number of holders. At the time of writing, the second place by volume, DAI on Ethereum, has only ~20% less volume than it, but two orders of magnitude fewer holders and number of transfers. The second place by number of transfers, USDC on Base, has 5x fewer transfers.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/4/6476c0c44ca7866bfea5a4f35205a47fa7c74204.png" title="Screenshot 2024-06-27 at 8.00.11 PM"><img alt="Screenshot 2024-06-27 at 8.00.11 PM" height="459" src="https://ethresear.ch/uploads/default/optimized/3X/6/4/6476c0c44ca7866bfea5a4f35205a47fa7c74204_2_690x459.png" width="690" /></a></div><p></p>
<p>This shows Tron USDT’s monstrous levels of payment usage among individuals. Unsurprising—Tron team has done an extensive advertisement campaign for its payment solution in Africa and Latin America. Shortly after, the network effect spread it to the developing countries in Asia and Post-Soviet area.</p>
<p>If we look at the areas of the largest prevalence of Tron USDT, a noteworthy pattern can be noticed. Tron USDT is largely used in the countries with weak economies and unsustainable local currencies: Türkiye, Lebanon, Zimbabwe, Venezuela, Argentina, and more. In these countries, traditional banking doesn’t provide people with options for reliable store of value and means of payment, as local currencies are unreliable, and foreign currencies are either banned for payment use or subject to strict control.</p>
<h3><a class="anchor" href="https://ethresear.ch#problems-2" name="problems-2"></a>Problems</h3>
<p>It is fair to say that USDT on Tron is one of the largest crypto applications by usage today. Millions of people around the world are interacting with it every day. It’s massively used as a store of value, acts as a medium of exchange in isolated economies such as Northern Cyprus, Cuba, and Vietnam. <a href="https://mirror.xyz/0x8958D0c419BCDFB8A86b8c0089552bE015fbe364/ODhOuYjK80atc9_jGprXotSo3PNobT1PRLFtorXHBrA" rel="noopener nofollow ugc">Local P2P platforms are building their infrastructure around USDT on Tron</a>. However, its dominance presents certain challenges for the broader Web3 community:</p>
<ul>
<li>
<p><em>A primary concern is the high degree of centralization</em> within the Tron Network. <a href="https://github.com/alexhooketh/tron-light-client/blob/cc3e037c5b71ba5e6216c8d19fee4dfe9d254e69/README.md" rel="noopener nofollow ugc">According to our research</a>, over the past 250 days there were only 28 unique block producers. The same entities are constantly winning the DPoS election due to delegations from the largest TRX holders. Most of these Super Representatives (block producers in Tron) <a href="https://tronscan.org/#/sr/representatives" rel="noopener nofollow ugc">lack any public information</a> beyond their status as block producers.</p>
</li>
<li>
<p>Despite this centralization, <em>transaction fees on Tron remain among the highest in crypto</em>—<a href="https://developers.tron.network/docs/resource-model#energy" rel="noopener nofollow ugc">420 sun</a> (1 sun = 1e-6 TRX) per gas. At the <a href="https://coinmarketcap.com/currencies/tron/" rel="noopener nofollow ugc">TRX’s price of ~0.000035 ETH</a>, this roughly corresponds to Ethereum L1’s gas price of 14.7 gwei. The usual fee for USDT transfers in Tron is <strong>$1-1.5 in TRX</strong>, rendering small transfers barely economical. However, the usage is still very high, as Tron’s ecosystem is isolated and there’s no convenient way to interact with other ecosystems from it.</p>
</li>
</ul>
<p>In contrast, the Ethereum ecosystem continues to thrive. Following the Dencun upgrade, transaction fees on rollups have drastically decreased to <a href="https://www.growthepie.xyz/fundamentals/transaction-costs" rel="noopener nofollow ugc">less than a cent</a> per ERC20 transfer. Combined with L2s, Ethereum DeFi <a href="https://defillama.com/chains" rel="noopener nofollow ugc">now comprises &gt;80% of the entire DeFi TVL</a>. <a href="https://l2beat.com/scaling/activity" rel="noopener nofollow ugc">Rollups alone consistently handle upwards of 100 TPS</a>, <a href="https://mirror.xyz/alexhook.eth/y9PTlM6tVr0H8X68r1LV2UwAnT9D6u1MEEiUFvcpyG0" rel="noopener nofollow ugc">with theoretical limits of 400-800 TPS</a> depending on the specific rollup. OP Mainnet has upgraded to Stage 1 trustlessness with all OP Chains and ZKsync catching up this summer. Arbitrum is working towards Stage 2.</p>
<p>People in developed countries are already integrated with Ethereum. By allowing ones from developing countries to seamlessly move into it from Tron, we can unite these disparate ecosystems and mitigate the risks associated with increasing centralization and monopolization of Sun’s machine.</p>
<h1><a class="anchor" href="https://ethresear.ch#rationale-3" name="rationale-3"></a>Rationale</h1>
<p>The protocol for cross-chain transfers from Tron should ideally possess the following characteristics:</p>
<ul>
<li><strong>Trust-minimized:</strong> The system should preclude the provision of incorrect information about the Tron blockchain or the theft of locked funds, except in the event of an attack on Tron’s consensus. In such a case, the security council authorized to stop the system can be established.</li>
<li><strong>Permissionless liquidity supply:</strong> The protocol should allow any entity to provide liquidity at their preferred rate. This fosters fair competition among providers, potentially resulting in more favorable and flexible exchange rates based on order size.</li>
<li><strong>Permissionless operation:</strong> While a centralized relay for light client updates and order fulfillment is acceptable, provided there exists a self-proposing mechanism in case of liveness failure, the relay must not serve as a source of trust. When feasible, on-chain operations should be implemented instead (e.g., a paymaster for gasless order claims).</li>
<li><strong>As simple as possible from the Tron side:</strong> Gas fees on Tron are extremely high, so it may be not affordable for users to execute more than necessary on-chain. Moreover, USDT Tron users are mostly using wallets such as Trust Wallet, Exodus, hardware wallets, and local exchange accounts, that do not support contract calls or token approvals. The only Tron wallet with these features, TronLink, is not common among USDT Tron users.</li>
<li><strong>Reasonably cheap on the destination side:</strong> Zero-knowledge proofs should be employed where possible to minimize costs. While the system can be more extensive than on the Tron side, it should still be optimized to keep user claim costs low.</li>
<li><strong>Single liquidity hub with enshrined bridging:</strong> The protocol should be deployed on a single L2 network to prevent liquidity fragmentation. To mitigate protocol isolation, cross-chain token bridges can be integrated at the UI level, <a href="https://zkp2p.xyz/send" rel="noopener nofollow ugc">similarly to ZKP2P</a>.</li>
<li><strong>USDC-native:</strong> Given USDC’s prevalence in the Ethereum ecosystem, the protocol can be based on USDC, effectively providing USDT-USDC swaps. However, USDC is virtually unknown in areas of extensive USDT Tron usage, so this difference should be addressed on UX level to reduce user distrust.</li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#trons-consensus-and-protocol-101-4" name="trons-consensus-and-protocol-101-4"></a>Tron’s consensus and protocol 101</h1>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/1/81e8ead1ee5585f245d51ac55f4f1db43f3785d2.png" title="image"><img alt="image" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/8/1/81e8ead1ee5585f245d51ac55f4f1db43f3785d2_2_540x500.png" width="540" /></a></div><p></p>
<p>Every 6 hours (7200 blocks), network participants delegate their TRX to validator candidates. The 27 candidates accumulating the most votes are elected as Super Representatives (SRs), who are then responsible for block production. Block producer selection follows a deterministic round-robin pattern. A block is considered finalized after receiving 18 confirmations, 2/3 of the SR set.</p>
<p>The block production is an ECDSA signature over the SHA256 hash of the protobuf-encoded block header. That is, one block = one signature. The top 128 representatives, beyond the 27 SRs, are designated as Super Representative Partners, voting on blocks produced by SRs. However, <a href="https://developers.tron.network/docs/concensus#block-generation-mechanism" rel="noopener nofollow ugc">as producers are predictable and the longest-chain rule is applied</a>, there is no necessity in validating votes.</p>
<p>Block header consists of the following elements:</p>
<pre><code class="lang-auto">message BlockHeader {
  message raw {
    int64 timestamp = 1;
    bytes txTrieRoot = 2;
    bytes parentHash = 3;
    int64 number = 7;
    bytes witness_address = 9;
    int32 version = 10;
  }
  raw raw_data = 1;
  bytes witness_signature = 2;
}
</code></pre>
<p>Even though state root is formally specified in the protocol, it’s not added to the header. We assume this is for backward-compatibility purposes, as the current version of Tron Network does not merkleize state.</p>
<p>The signature is made over a SHA256 hash of the serialized <code>raw_data</code> element. That is, by utilizing light verification, we can access only one transaction-specific element—the Merkle root of the transaction tree. However, in Tron, transactions carry their execution status, so we don’t need to access the state to validate the success of one-transaction operations, such as TRC20 transfer().</p>
<pre><code class="lang-auto">message Transaction {
  ...
  message Result {
    enum code {
      SUCESS = 0;
      FAILED = 1;
    }
    enum contractResult {
      DEFAULT = 0;
      SUCCESS = 1;
      REVERT = 2;
      BAD_JUMP_DESTINATION = 3;
      OUT_OF_MEMORY = 4;
      PRECOMPILED_CONTRACT = 5;
      STACK_TOO_SMALL = 6;
      STACK_TOO_LARGE = 7;
      ILLEGAL_OPERATION = 8;
      STACK_OVERFLOW = 9;
      OUT_OF_ENERGY = 10;
      OUT_OF_TIME = 11;
      JVM_STACK_OVER_FLOW = 12;
      UNKNOWN = 13;
      TRANSFER_FAILED = 14;
      INVALID_CODE = 15;
    }
    int64 fee = 1;
    code ret = 2;
    contractResult contractRet = 3;
    ...
}
</code></pre>
<p>Votes for witnesses (representatives) are of a specific transaction type. This means that in order to calculate the votes, the light client has to download all transactions and re-execute ones of this type.</p>
<pre><code class="lang-auto">message Transaction {
  message Contract {
    enum ContractType {
      ...
      VoteWitnessContract = 4;
      ...
}
</code></pre>
<p>However, considering the fact that the SR set is almost static, we believe that it would be computationally cheaper to delegate choosing the canonical set to DAO or enshrine the set into the circuit.</p>
<p>Normal contract calls, such as TRC20 transfer, have the <code>TriggerSmartContract</code> type and are nearly identical to ERC20 transactions. This means that we can prove the USDT transfer on Tron network using only the transaction root, which can be safely accessed on-chain using ZK light client relay.</p>
<h1><a class="anchor" href="https://ethresear.ch#design-proposal-5" name="design-proposal-5"></a>Design proposal</h1>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/b/4b4b8d0d74dfe0a7dd5991bce974eac97c8621fc.jpeg" title="image"><img alt="image" height="402" src="https://ethresear.ch/uploads/default/optimized/3X/4/b/4b4b8d0d74dfe0a7dd5991bce974eac97c8621fc_2_690x402.jpeg" width="690" /></a></div><p></p>
<p>The proposed cross-chain swap mechanism involves three primary entities: the <em>User</em>, the <em>Buyer</em> (or liquidity provider), and the <em>Relayer</em>. The process unfolds as follows:</p>
<ol>
<li>
<p><em>The Buyer</em> locks USDC into the swap contract on the L2, specifying their exchange rate and Tron address for transfers.</p>
</li>
<li>
<p><em>The User</em> selects a <em>Buyer</em> offering the most favorable rate with sufficient liquidity. <em>The User</em> then initiates a transaction on the L2 to temporarily lock a portion of the <em>Buyer’s</em> USDC. This step prevents liquidity depletion before order fulfillment. If supported by the L2, this transaction may be funded by a paymaster.</p>
</li>
<li>
<p><em>The User</em> transfers the corresponding amount of Tron USDT to the <em>Buyer’s</em> specified address.</p>
</li>
<li>
<p>Following 18 block confirmations (~54 seconds, ensuring finality), the <em>Relayer</em> retrieves the latest Tron block headers and generates a ZK proof to them. The circuit for light verification must contain the transaction root from the header as the public input so that it’s known to the relay contract. This proof is needed to efficiently prove the new Tron blocks and their transaction roots <em>to the relay contract</em>.</p>
</li>
<li>
<p><em>The Relayer</em> obtains the finalized transaction from the Tron blockchain and generates a zero-knowledge proof of transaction inclusion against the transaction root. This proof is needed to efficiently prove the order fulfillment <em>to the swap contract</em>. Just like light client proofs, transaction proofs can be aggregated to minimize the costs of on-chain proof verification.</p>
</li>
<li>
<p><em>The Relayer</em> submits these proofs to the respective smart contracts on the L2. Upon verification, the swap contract releases the funds to the <em>User</em> and allocates a small portion to the <em>Relayer</em> as compensation. In case of liveness failure, the <em>User</em> can generate and relay proofs themselves, removing the need for relayer fees.</p>
</li>
<li>
<p><em>The Buyer</em> can exchange their acquired Tron USDT for USDC on the L2 through various means, including direct 1:1 exchange with issuers, and reinvest in the swap contract.</p>
</li>
</ol>
<p>This system streamlines the user experience to just two primary actions: committing to the order on the destination chain and transferring Tron USDT to a specified address. The User receives the equivalent USDC on the L2 within approximately one minute. This system can even be used to accept payments in USDT Tron, requiring only a web browser with a connected wallet for order creation.</p>
<p>For Buyers, liquidity provision is fully automated. They create a Tron wallet, and supply USDC with specified Tron address to the smart contract. When their liquidity is out, it is automatically removed. Received USDT can be spent and exchanged back to USDC at any time. This system is expected to provide higher exchange rates than the existing P2P platforms, as the rate is competitive and there’s no need to cover the costs of KYC and other web2-specific processes.</p>
<p>Relayers require only a server running relayer and ZK prover software. As relayers do not serve as the source of trust, this role can be either permissionless or delegated to the development team, provided self-proposing functionality is supported.</p>
<h1><a class="anchor" href="https://ethresear.ch#zk-light-client-poc-6" name="zk-light-client-poc-6"></a>ZK Light Client PoC</h1>
<p>We’ve written a proof-of-concept of ZK light verification of Tron blocks in Noir language. It receives the previous and new block IDs with a transaction root as the public input, and the block header as the private input. It does not implement round-robin checks and election mechanism for efficiency purposes, and the SR set is hardcoded into the circuit. The proof is generated in about 35 seconds on an M1 machine.</p>
<p>For the production version of this system, it may be necessary to rewrite the circuits to STARK-based proof systems and/or implement GPU proving to improve proving speed.</p>
<p>The source code can be found here: <a class="inline-onebox" href="https://github.com/alexhooketh/zktron" rel="noopener nofollow ugc">GitHub - alexhooketh/zktron: ZK light client for Tron Network written in Noir</a></p>
<h1><a class="anchor" href="https://ethresear.ch#conclusion-7" name="conclusion-7"></a>Conclusion</h1>
<p>The proposed P2P ZK Light Client Bridge between Tron and Ethereum L2s is a significant advancement in addressing the problems of Tron Network in a Web3 way. By leveraging zero-knowledge proofs and efficient light client verification, this system offers a trust-minimized, permissionless, and cost-effective solution for bridging the gap between these two prominent blockchain ecosystems.</p>
<p>This bridge design addresses several key challenges:</p>
<ol>
<li>It mitigates the risks associated with the centralization of the Tron Network by providing users with seamless access to the more decentralized and robust Ethereum ecosystem.</li>
<li>It significantly reduces transaction costs for users, particularly benefiting those in developing economies who rely heavily on USDT for daily transactions and value storage.</li>
<li>It enhances liquidity and interoperability between Tron and Ethereum, expanding Ethereum ecosystem to the areas of extensive Tron usage.</li>
<li>It maintains a high level of security through the use of ZK proofs, ensuring the integrity of cross-chain transactions without compromising on efficiency.</li>
</ol>
<p>By bridging these ecosystems, we can solve the problem of increasing influence of Tron, taking a significant step towards realizing the vision of a truly global, decentralized financial infrastructure that can benefit users across all economic backgrounds.</p>
<p>We welcome feedback and questions from the community. Feel free to leave your comments, suggestions, or inquiries in the comments section below. <strong>Thank you for reading.</strong></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/p2p-zk-light-client-bridge-between-tron-and-ethereum-l2s/19931">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 28 Jun 2024 20:03:10 +0000</pubDate>
</item>
<item>
<title>Orbit SSF: solo-staking-friendly validator set management for SSF</title>
<link>https://ethresear.ch/t/orbit-ssf-solo-staking-friendly-validator-set-management-for-ssf/19928</link>
<guid>https://ethresear.ch/t/orbit-ssf-solo-staking-friendly-validator-set-management-for-ssf/19928</guid>
<content:encoded><![CDATA[
<p><em>Much of the post came together during a week of in-person whiteboarding with <a href="https://rig.ethereum.org" rel="noopener nofollow ugc">RIG</a> and wannabe RIGs like myself, Ansgar and Toni. Thanks in particular to <a href="https://twitter.com/weboftrees" rel="noopener nofollow ugc">Anders</a>, <a href="https://twitter.com/adietrichs" rel="noopener nofollow ugc">Ansgar</a>, <a href="https://twitter.com/barnabemonnot" rel="noopener nofollow ugc">Barnabé</a>, <a href="https://twitter.com/soispoke" rel="noopener nofollow ugc">Thomas</a> for continued discussions and feedback, again to Anders for most of the ideas around individual incentives, and again to Barnabé for the diagrams about finality. The core idea that the post explores was originally proposed by Vitalik in <a href="https://ethresear.ch/t/sticking-to-8192-signatures-per-slot-post-ssf-how-and-why/17989#approach-3-rotating-participation-ie-committees-but-accountable-5">this post</a></em>.</p>
<h2><a class="anchor" href="https://ethresear.ch#where-we-are-1" name="where-we-are-1"></a>Where we are</h2>
<p>The <a href="https://notes.ethereum.org/@vbuterin/single_slot_finality" rel="noopener nofollow ugc">Single Slot Finality (SSF) roadmap</a> has <a href="https://notes.ethereum.org/@vbuterin/single_slot_finality#What-are-the-key-questions-we-need-to-solve-to-implement-single-slot-finality" rel="noopener nofollow ugc">three main components</a>:</p>
<ul>
<li>Consensus algorithm</li>
<li>Signature aggregation</li>
<li>Validator set economics</li>
</ul>
<p>Since the previously linked post, there has been a lot of progress on the consensus algorithm side, with <a href="https://ethresear.ch/t/a-simple-single-slot-finality-protocol/14920">multiple</a> <a href="https://arxiv.org/abs/2310.11331" rel="noopener nofollow ugc">candidate</a> <a href="https://notes.ethereum.org/@fradamt/chained-3sf" rel="noopener nofollow ugc">protocols</a> and <a href="https://github.com/fradamt/ssf/tree/main/high_level" rel="noopener nofollow ugc">the beginning of a specification effort</a>. There have also been some effort to explore the design space of signature aggregation, both with a <a href="https://ethresear.ch/t/horn-collecting-signatures-for-faster-finality/14219">networking</a> <a href="https://ethresear.ch/t/flooding-protocol-for-collecting-attestations-in-a-single-slot/17553">focus</a> and a <a href="https://ethresear.ch/t/signature-merging-for-large-scale-consensus/17386">cryptographic focus</a>. Still, we are likely not close to being able to reliably aggregate millions of signatures per slot, without increasing the slot time or validator requirements significantly. On the staking economics side, there has been lots of work over the last year, but for the most part focused on understanding <a href="https://mirror.xyz/barnabe.eth/v7W2CsSVYW6I_9bbHFDqvqShQ6gTX3weAtwkaVAzAL4" rel="noopener nofollow ugc">liquid staking</a> and <a href="https://mirror.xyz/barnabe.eth/96MD_A194uXLLjcOWePW3O2N3P-JG-SHtNxU0b40o50" rel="noopener nofollow ugc">restaking</a>, and on <em>stake</em> capping, i.e., constraining the amount of ETH staked (if you’re reading this, you’re probably already at least at a surface level familiar with the issuance conversation, in which case you might want to dig deeper and check out these posts <a href="https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448/1">[1]</a> <a href="https://ethresear.ch/t/endgame-staking-economics-a-case-for-targeting/18751">[2]</a>). Here, we are instead interested in <em>validator capping</em>, i.e., constraining the amount of validator identities in the system, or at least the ones actively participating at any given time, to satisfy technical constraints. Some ideas in this direction can be found in this <a href="https://ethresear.ch/t/sticking-to-8192-signatures-per-slot-post-ssf-how-and-why/17989#what-would-8192-signatures-per-slot-under-ssf-look-like-2">recent post</a>, and in fact <a href="https://ethresear.ch/t/sticking-to-8192-signatures-per-slot-post-ssf-how-and-why/17989#approach-3-rotating-participation-ie-committees-but-accountable-5">approach 3</a> from the post provides the foundation for this post. Moreover, a recent important development is that <a href="https://eips.ethereum.org/EIPS/eip-7251" rel="noopener nofollow ugc">EIP-7251 (MaxEB)</a> has been included in the <a href="https://github.com/ethereum/consensus-specs/blob/a3a6c916b236c9e8904090303f0c38ae49db1002/specs/electra/beacon-chain.md" rel="noopener nofollow ugc">Electra fork</a>. Validator effective balances will be allowed to be as high as 2048 ETH, enabling staking pools to <a href="https://notes.ethereum.org/@fradamt/maxeb-consolidation" rel="noopener nofollow ugc">consolidate their validators</a>, a new capability which we can leverage in our designs.</p>

<h2><a class="anchor" href="https://ethresear.ch#goals-2" name="goals-2"></a>Goals</h2>
<p>With the goal of finding a design which can make its way into the protocol in a reasonable timeline, we are here going to focus on solutions that <em>do not</em> rely on large improvements in the signature aggregation process. We also do not think it is very realistic to propose significant increases of the slot time, which have many externalities. Given these technical constraints, let’s focus on a minimal set of properties that we ideally want to achieve:</p>
<ul>
<li><strong>Validator capping</strong>: at most <span class="math">N</span> <em>active</em> validators at a time. For example, <span class="math">N = 2^{15} \approx 32k</span>, which we know we can handle because it is the size of a committee today. If we wanted to completely remove attestation aggregation, we would likely need to drop this number to a few thousands.</li>
<li><strong>Solo staking viability</strong>: staking with 32 ETH is <em>guaranteed</em> to still be possible, <em>and</em> the solo staking yield should still not compare unfavorably to delegated staking yields.</li>
<li><strong>High eventual economic security</strong>: More than <span class="math">D_f</span> stake provides economic security, at least <em>eventually</em>. For example <span class="math">D_f = 20M</span> ETH. Ideally, we also do not have to wait longer than today for it (two epochs).</li>
<li><strong>Fast finality</strong>: at least <em>some</em> amount of economic security is available shortly after a block is proposed (think: 10 to 30 seconds, not over 10 minutes).</li>
<li><strong>Optimally secure consensus protocol</strong>: the consensus protocol is (provably) resilient to ~1/2 adversaries under network synchrony, and 1/3 under partial synchrony.</li>
</ul>
<p>Without solo staking viability as defined here, we could simply raise the minimum balance, or go with approaches that allow for a low minimum balance but do not <em>guarantee</em> it, for example in the face of large stakers intentionally splitting their stake. Such solutions would likely have to lean on <a href="https://ethresear.ch/t/sticking-to-8192-signatures-per-slot-post-ssf-how-and-why/17989#approach-1-go-all-in-on-decentralized-staking-pools-3">decentralized staking pools</a> or <a href="https://ethresear.ch/t/sticking-to-8192-signatures-per-slot-post-ssf-how-and-why/17989#approach-2-two-tiered-staking-4">two-tier staking</a> to preserve the accessible nature of staking as it is today, or perhaps even to carve out a more tailored role for smaller stakers, as is suggested by <a href="https://ethresear.ch/t/unbundling-staking-towards-rainbow-staking/18683">rainbow staking</a>. While there is merit to these approaches and we believe they (and more generally the role of solo staking/broad consensus participation) deserve further exploration, we are choosing here to only explore designs that are compatible with this narrow interpretation of solo staking viability.</p>
<h2><a class="anchor" href="https://ethresear.ch#overview-of-approaches-3" name="overview-of-approaches-3"></a>Overview of approaches</h2>
<p>Validator capping, solo staking viability and high economic security immediately raise an issue: high economic security requires millions of ETH to participate in finalizing and a minimum balance of 32 ETH then implies a worst case of hundreds of thousands or millions of validators (~1M at the time of writing), which seems to conflict with validator capping. There are two main classes of approaches that attempt to deal with this problem:</p>
<ul>
<li><strong>Validator set rotation</strong>: the full validator set is allowed to be large, but only a subset is actively participating at any given time.</li>
<li><strong>Economic validator set capping</strong>: the size of the full validator set is constrained through economic incentives. To <em>guarantee</em> a small validator set size we can for example <a href="https://notes.ethereum.org/@vbuterin/single_slot_finality#Economic-capping-of-total-validator-count" rel="noopener nofollow ugc">reduce issuance past the target validator count</a>. However, this leaves all stakers open to a cheap griefing attack, where a small amount of stake can have a disproportionate negative impact on issuance.</li>
</ul>
<p>In this post we are not going to focus on the latter approach <em>in isolation</em>, but we are going to propose a way to combine economic incentives with a form of validator set rotation.</p>
<h2><a class="anchor" href="https://ethresear.ch#validator-set-rotation-4" name="validator-set-rotation-4"></a>Validator set rotation</h2>
<p>The current protocol also has to deal with the issue we have outlined in the previous section, and the chosen “solution” is precisely validator set rotation: only 1/32 of the validator set votes in any given slot. This design <a href="https://notes.ethereum.org/@vbuterin/serenity_design_rationale#Why-32-ETH-validator-sizes" rel="noopener nofollow ugc">trades off finality time</a>, and fails to satisfy our desired property of fast finality.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/3/83646288afbace1b452239618b34e83319531d0a.png" title=""><img alt="" height="388" src="https://ethresear.ch/uploads/default/optimized/3X/8/3/83646288afbace1b452239618b34e83319531d0a_2_690x388.png" width="690" /></a></div><p></p>
<p>Let’s then explore whether we can use validator set rotation without giving up on fast finality or other properties.</p>
<h3><a class="anchor" href="https://ethresear.ch#fast-rotation-5" name="fast-rotation-5"></a>Fast rotation</h3>
<p>One way to go about validator set rotation is to have committees which rotate fast, as in the current protocol. In order to avoid a high time-to-finality, we can use a different consensus protocol allowing for <a href="https://ethresear.ch/t/a-model-for-cumulative-committee-based-finality/10259">committee-based finality</a>, i.e., such that even a committee can provide economic security proportional to its stake. In fact, the post linked above deals with <em>cumulative</em> committee-based finality, where the consensus protocol even allows for accumulation of economic security over multiple finalizations, such that <span class="math">k</span> committees finalizing in a row results in <span class="math">k</span> times the economic security that a single committee can provide. We get two birds with one stone, getting both fast (partial) finality and full <em>eventual</em> economic security. In particular, we could have full finality <em>in the same time as today</em> (which gives enough time for each committee to do its own finalization by voting twice), but with the big improvement that economic security accrues every slot, rather than all at once after two epochs.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/6/b6816fefcf5021bb672ed76029c844d1f311047d.png" title=""><img alt="" height="388" src="https://ethresear.ch/uploads/default/optimized/3X/b/6/b6816fefcf5021bb672ed76029c844d1f311047d_2_690x388.png" width="690" /></a></div><p></p>
<p>This seems like a clear improvement on today’s protocol, so why are we not just doing it? An answer comes from the consensus protocol design space: it is not clear at this point how to have an optimally secure dynamically available protocol with <em>fast</em> rotating committees. In fact, much of the problems with the LMD-GHOST component of today’s protocol, or at least the <a href="https://ethresear.ch/t/reorg-resilience-and-security-in-post-ssf-lmd-ghost/14164#introduction-2">fundamental ones</a>, come precisely from the interaction of multiple committees. In short, an adversary can accumulate weight across multiple committees, and use it to reorg honest blocks that have a single committee supporting them.</p>
<p>For interested readers, there actually are optimally secure dynamically available consensus protocols that allow for committees (<a href="https://arxiv.org/abs/2209.03255" rel="noopener nofollow ugc">[1]</a> <a href="https://eprint.iacr.org/2022/1448.pdf" rel="noopener nofollow ugc">[2]</a> for example), but all known ones suffer from catastrophic failures under even short-lived asynchrony (<a href="https://arxiv.org/abs/2302.11326" rel="noopener nofollow ugc">[1]</a> <a href="https://arxiv.org/abs/2309.05347" rel="noopener nofollow ugc">[2]</a>). It is not known whether this is a fundamental limitation, but at least so far we do not know any protocol that gets around it.</p>
<h3><a class="anchor" href="https://ethresear.ch#slow-rotation-6" name="slow-rotation-6"></a>Slow rotation</h3>
<p>There is however a simple way to avoid the problem altogether: giving up on <em>fast</em> committee rotation, and instead having a committee which rotates out slowly, for example by replacing a small percentage of it every slot. The upshot is that such a committee effectively acts as a full validator set, in the sense that its actions do not interact with those of other committees, as would be the case with fast rotation. We can in principle take any protocol that works when the whole validator set is able to participate at once, and make it work with this mechanism by slowing down the rotation sufficiently.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/c/4c21c0ca0871b1e7c6440325b3a3174df016793f.png" title=""><img alt="" height="389" src="https://ethresear.ch/uploads/default/optimized/3X/4/c/4c21c0ca0871b1e7c6440325b3a3174df016793f_2_690x389.png" width="690" /></a></div><p></p>
<p>At a first glance, one obvious downside is that a full rotation of the validator set would be much slower than today, and thus so would finality. However, we could do things a bit differently, by decoupling the committee which votes for the available chain (LMD-GHOST votes) from those which vote for finality (Casper FFG votes). Only LMD-GHOST has problems with fast committee rotation, so we could have a slowly rotating committee whose votes count for LMD-GHOST and in parallel also fast rotating committees whose votes accumulate economic security over time, up to full finality in no more than today’s two epochs.</p>
<p>Besides some amount of extra complexity in the consensus protocol, one remaining downside is that we leave a single committee “in charge” of LMD-GHOST for extended periods of time. Moreover, while linearly accumulating finality is a strict improvement over today’s step function finality, we do not achieve something even stronger, namely getting a high level of economic security immediately. This is of course impossible to achieve given the constraints we have laid out, <em>unless we make some assumptions about the stake distribution</em>, for example that it is <a href="https://notes.ethereum.org/@vbuterin/single_slot_finality#The-good-news-gains-from-enabling-voluntary-validator-balance-consolidation" rel="noopener nofollow ugc">Zipfian</a>, or anyway such that a large portion of the stake is concentrated in the first few thousand entities.</p>

<h2><a class="anchor" href="https://ethresear.ch#orbit-ssf-stable-core-rotating-periphery-7" name="orbit-ssf-stable-core-rotating-periphery-7"></a>Orbit SSF: Stable core, rotating periphery</h2>
<p>Our starting point is <a href="https://ethresear.ch/t/sticking-to-8192-signatures-per-slot-post-ssf-how-and-why/17989#approach-3-rotating-participation-ie-committees-but-accountable-5">approach 3 from this post</a>, where validators are (roughly) sampled by balance, so that validators with a lot of stake are always in the validator set. Contrast this with the previously considered validator set rotation approaches where validators were (implicitly) sampled by indices, as we do today, which results in each committee having small weight regardless of what the stake distribution looks like.</p>
<p>We then consider adding consolidation incentives, to have stronger guarantees about the level of finality that we can reach with a single committee. The rotating parts of the committee can then rotate slowly, and we do not need to take on the extra consensus complexity of decoupling voting for the available chain and for the finality gadget. Moreover, there is never a small committee (in terms of stake) in charge of a critical consensus component: at all times, we can expect the active validator set to hold a meaningful fraction of the whole stake.</p>
<h3><a class="anchor" href="https://ethresear.ch#active-validator-set-management-8" name="active-validator-set-management-8"></a>Active validator set management</h3>
<p>There are two key components here:</p>
<ul>
<li><em>Active validator set selection</em>: We set a stake threshold <span class="math">T</span> (in principle it could also be set dynamically), and then define the probability of validator with stake <span class="math">S</span> to be sampled in the active set to be <span class="math">p(S) = \min(\frac{S}{T}, 1) = 
\begin{cases}
\frac{S}{T} &amp; S \le T \\ 
1 &amp; S \ge T 
\end{cases}</span><br />
A validator with stake <span class="math">S \le T</span> is selected with probability <span class="math">\frac{S}{T}</span> proportional to its stake, whereas validators with stake <span class="math">S \ge T</span> are <em>always</em> in the validator set. The idea here is of course that it is helpful to have a stable core of large validators always in the active set, because they contribute a lot of economic security but still only add the same load as any other validator.</li>
<li><em>Reward adjustment</em>: We adjust attestation rewards so that all validators still get compensated proportionally to their stake, regardless of whether they fall below or above the threshold <span class="math">T</span>. To define the reward function, we take as reference the maximum attestation reward <span class="math">R</span> that the protocol gives to a validator with stake <span class="math">T</span>, for a single attestation (<span class="math">R</span> can of course vary depending on the overall issuance level). Given <span class="math">R</span>, the maximum reward for an attestation by a validator with stake <span class="math">S</span> is <span class="math">r(S) = R\cdot\max(1, \frac{S}{T}) = 
\begin{cases} 
R &amp; S \le T \\
R \cdot \frac{S}{T} &amp;S \ge T
\end{cases}</span><br />
Overall, the <em>expected</em> rewards of a validator with stake <span class="math">S</span> are then <span class="math">p(S)\cdot r(S) = R\cdot\frac{S}{T} = \frac{R}{T} \cdot S</span>. In other words, <span class="math">\frac{R}{T}</span> per unit of stake, regardless of how it is distributed.  To help visualize this, here’s a plot of <span class="math">p(S)</span>, <span class="math">r(S)</span> and <span class="math">p(S)\cdot r(S)</span>, for <span class="math">R = 2</span> (arbitrary value just for the plot) and <span class="math">T = 1024</span>. Validators with less than <span class="math">T</span> stake do have higher variance, because they only participate <span class="math">\frac{S}{T}</span> of the time, but over longer periods of time the variance will still very low, since attestation rewards are constant and very frequent.</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/9/69e8f45397940a38df6d146660314fa29c2f790c.png" title=""><img alt="" height="355" src="https://ethresear.ch/uploads/default/optimized/3X/6/9/69e8f45397940a38df6d146660314fa29c2f790c_2_690x355.png" width="690" /></a></div><p></p>
<h4><a class="anchor" href="https://ethresear.ch#validator-capping-9" name="validator-capping-9"></a>Validator capping</h4>
<p>We can easily compute the expected size of an active validator set <span class="math">V_a</span> that is sampled this way from a full validator set <span class="math">V</span> whose total deposit size is <span class="math">D</span>:<br />
<span class="math">\mathbb{E}[|V_a|] = \sum_{i \in V} p(S_i) = \sum_{i \in V} \min(\frac{S_i}{T}, 1) = \frac{1}{T}\sum_{i \in V} \min(S_i, T) \le \frac{1}{T}\sum_{i \in V} S_i = \frac{D}{T}</span></p>
<p>Basically, any validator with stake <span class="math">S \le T</span> contributes exactly <span class="math">\frac{S}{T}</span> to the expectation. Crucially, these contribution scale linearly in <span class="math">S</span>: the only effect of splitting up to <span class="math">T</span> stake into small validators is to increase the variance of the active validator set size, without affecting the expectation. As for validators with stake <span class="math">S &gt; T</span>, they even decrease the expectation compared to the worst case, which is <span class="math">\frac{D}{T}</span>, equal to the full validator set size if all validators had <span class="math">T</span> stake.</p>
<p>For example, we can set <span class="math">T = 4096</span> ETH, giving us a <em>maximum</em> expected active validator set size of <span class="math">\frac{120M}{4096} \approx 30k</span>. If we were to employ stake capping (we will later discuss how to do so in this context) to ensure (or have high assurances) that <span class="math">D</span> is bounded by (for example) <span class="math">2^{25}M</span> ETH, we could even set <span class="math">T = 1024</span> and still have an expected active validator set size of at most ~32k. There can of course be deviations from this expected size, but with high probability the actual active validator set size would always fall within reasonably narrow bounds, so we can have very strong guarantees about the maximum load that we would need to be able to handle. We look at this in more detail <a href="https://ethresear.ch#Validator-capping-active-validator-set-variance">in the appendix</a>.</p>
<h3><a class="anchor" href="https://ethresear.ch#incentivizing-consolidation-10" name="incentivizing-consolidation-10"></a>Incentivizing consolidation</h3>
<p>Let <span class="math">D_a</span> be the active deposit size, i.e., the total stake of the active validator set, contrasted with the total deposit size <span class="math">D</span>, the stake of the whole validator set. Optimistically, as long as there is sufficient consolidation, <span class="math">D_a</span> will be high, a clear improvement over the <a href="https://ethresear.ch#Slow-rotation">previous slow rotation approach</a>. Still, we would like this to be more than an optimistic property. The question we are left to answer is then how we can ensure, or at least highly incentivize, a high <span class="math">\frac{D_a}{D}</span> ratio. For example, we want to prevent that all validators keep 32 ETH balances (no one consolidates), which would result in <span class="math">\mathbb{E}[D_a] = \frac{32}{T} D</span>, e.g., only <span class="math">\frac{D}{32}</span> with <span class="math">T = 1024</span>. With today’s <span class="math">D = 32M</span> ETH, the expected active deposit size would only be <span class="math">1M</span> ETH. On the other hand, we do not want to reward consolidated validators disproportionately compared to small validators.</p>
<p>We explore two complementary approaches:</p>
<ul>
<li><strong>Collective consolidation incentives</strong>, growing the size of the pie for the whole validator set when the set is more consolidated.</li>
<li><strong>Individual consolidation incentives</strong>, accounting for the extra risk accruing from further individual consolidation.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#collective-consolidation-incentives-11" name="collective-consolidation-incentives-11"></a>Collective consolidation incentives</h4>
<p>The first approach we explore is to reward <em>everyone</em> for consolidation, spreading out the benefits beyond the consolidating validators so as to maintain rewards undifferentiated, while still providing an incentive to consolidate.</p>
<p>A first proposal in this direction is to set the rewards based on <span class="math">D_a</span>, rather than <span class="math">D</span>. For example, we could use the same issuance curve <span class="math">I</span> we use today, but where the deposit size used as input is <span class="math">D_a</span> instead of <span class="math">D</span>: the cumulative issuance would then be <span class="math">I(D_a)</span>, and the resulting yield per unit of stake <span class="math">\frac{I(D_a)}{D}</span>. Notably, <span class="math">I</span> is monotonically increasing, so, whenever <span class="math">D_a &lt; D</span>, the cumulative issuance <span class="math">I(D_a)</span> is less than the maximum issuance <span class="math">I(D)</span> that would be possible at this deposit size, with full consolidation. The yield gap <span class="math">\frac{I(D) - I(D_a)}{D}</span> between the current yield and the yield with full consolidation then acts as a consolidation incentive.</p>
<p>Consolidation incentives aside, another way to think about this proposal is that we simply pay for the economic security we get, at least from a single committee: if today our security budget for <span class="math">X</span> amount of deposits is <span class="math">Y</span>, as expressed by <span class="math">I(X) = Y</span>, we would now be wiling to pay <span class="math">Y</span> in order to get <span class="math">X</span> amount of <em>active</em> deposits. To get an idea of what this looks like in practice, here’s a color plot of the yield for <span class="math">(D, \frac{D_a}{D})</span> (starting from <span class="math">D = 1</span> to help the visualization).</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/f/afced2f656e5db107f33e22007ab6b5fdd5859fc.png" title=""><img alt="" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/a/f/afced2f656e5db107f33e22007ab6b5fdd5859fc_2_623x500.png" width="623" /></a></div><p></p>
<h4><a class="anchor" href="https://ethresear.ch#individual-consolidation-incentives-12" name="individual-consolidation-incentives-12"></a>Individual consolidation incentives</h4>
<p><em>Credit to Anders for raising the issue of differentiated risk and for proposing the kind of individual incentives we explore here</em></p>
<p>Though our exploration of collective incentives has found them to be decently strong, there is one factor we have not considered: validators with stake <span class="math">\ll T</span> have a better risk profile than validators with stake <span class="math">\ge T</span>. This is because they are in the active only a fraction of the time, which means two things:</p>
<ul>
<li>In a staking pool, accidental slashing caused by a bad setup can be caught early with only a fraction of the validators being subject to it</li>
<li>Tail risk of mass slashing or leaking, for example due to client bugs, is much lower, as in many cases this would only affect the active set. For a staking pool, this effectively caps the pool’s slashing exposure to a fraction of the stake, in almost all scenarios.</li>
</ul>
<p>We might then be unwilling to solely rely on collective incentives, which cannot properly account for the risk differentiation between consolidated and non consolidated validators, itself an individual anti-consolidation incentive. On the other hand, we are hesitant to use individual consolidation incentives, because differentiated rewards threaten our goal of solo staking viability. Faced with this dilemma, a potential approach to mitigate the consequences on solo staking viability is to try to set individual consolidation incentives that just offset the added risk from consolidation. The goal is for risk-adjusted rewards to be roughly equivalent for consolidated and non consolidated validators, so that the available choices of higher risk, higher reward and lower risk, lower rewards are similarly attractive. In particular, it is then at least in principle possible (though not guaranteed) to have a validator set where both setups coexist, so that we can aspire to both have a high consolidation ratio and solo staking viability.</p>
<p>Concretely, here’s a way we could go about this. Given the base yield <span class="math">y(D_a, D) = \frac{I(D_a, D)}{D}</span>, we can adjust the yield of a validator with <span class="math">S</span> stake to be <span class="math">y(D_a, D)(1 + \frac{\min(S,T)}{T}g(\frac{D_a}{D}))</span>, where <span class="math">g(x)</span> is decreasing and <span class="math">g(1) = 0</span>. In other words, a validator with <span class="math">S</span> stake gets additional <em>consolidation yield</em> <span class="math">y_c(D_a, D, S) = \frac{\min(S,T)}{T}g(\frac{D_a}{D})y(D_a, D)</span>, or equivalently its yield increases by a factor of <span class="math">\frac{\min(S,T)}{T}g(\frac{D_a}{D})</span>, up to <span class="math">g(\frac{D_a}{D})</span> for fully consolidated validators with <span class="math">S = T</span>. This factor decreases as <span class="math">\frac{D_a}{D}</span> grows, because there are diminishing returns to further consolidation (same reason why the staking yield falls as the total deposit size grows). In particular, it falls all the way to <span class="math">0</span> if <span class="math">\frac{D_a}{D}</span> goes to <span class="math">1</span>, restoring the base yield <span class="math">y(D_a, D)</span> for everyone, and generally making the rewards less and less differentiated as more consolidation occurs. The idea is that an equilibrium will be reached where <span class="math">g(\frac{D_a}{D})</span> just about compensates for the additional risk from consolidating, and further consolidation is not incentivized. We can even set <span class="math">g</span> to reach <span class="math">0</span> at some lower level of consolidation that we are happy with, leaving more space for staking with non consolidated validators to be economically viable. For example, if <span class="math">g(0.8) = 0</span>, then a validator with 32 ETH gets the same yield, and less risk, as a validator with 1024 ETH, even if 20% of the stake is made up of 32 ETH validators.</p>
<p>Let’s now look at a specific form of <span class="math">g</span>. The simplest possible choice is a linear function, which is fully determined by <span class="math">g(0)</span>, the initial yield increase factor when there is no consolidation at all. The function is then simply <span class="math">g(x) = g(0)(1 - x)</span>. For example <span class="math">g(x) = \frac{1-x}{4}</span>, where the maximum yield increase is 25%. The extra yield of a validator with stake <span class="math">S</span> is:<br />
<span class="math">y_c(D_a, D, S) = g(0)\frac{\min(S,T)}{T} \cdot y(D_a, D) \cdot (1 - \frac{D_a}{D})</span></p>
<p>Let’s see what this looks like in combination with the collective incentives introduced <a href="https://ethresear.ch#Collective-consolidation-incentives">in the previous section</a>, where issuance is based on <span class="math">D_a</span>, i.e., <span class="math">y(D_a, D) = \frac{I(D_a)}{D}</span>, and <span class="math">I</span> is the current issuance curve <span class="math">I(x) = c\sqrt{x}</span>. The maximum consolidation yield, or the yield advantage of a consolidated validator over a regular one, is:</p>
<p><span class="math">y_c(D_a, D, S=T) = g(0) \cdot y(D_a, D) (1 - \frac{D_a}{D})  = \\
= g(0) \cdot c \cdot \frac{\sqrt{D_a}}{D}(1 - \frac{D_a}{D}) = \\ g(0) \cdot c \cdot \frac{1}{\sqrt{D_a}} \frac{D_a}{D}(1 - \frac{D_a}{D})</span></p>
<p>The next color plot shows <span class="math">y_c(D_a, D, S=T)</span> as a function of <span class="math">\frac{D_a}{D}</span> and <span class="math">D_a</span>, for <span class="math">g(0) = \frac{1}{4}</span> (some portion on the upper left corner is infeasible, because <span class="math">D</span> would be <span class="math">&gt; 120M</span>). Horizontally, the function looks like <span class="math">x(1-x)</span>: the consolidation yield is low at low consolidation levels, when collective incentives are strong, and at high consolidation levels, when we don’t have a strong requirement for more consolidation and we are more worried about the economic viability of running non consolidated validators. Vertically it looks like <span class="math">\frac{1}{\sqrt{y}}</span>, with the consolidation yield slowly falling off as <span class="math">D_a</span> grows and we have less need for consolidation in general, since the economic security of the active set is determined by <span class="math">D_a</span>.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/c/fc72bbb9885865bf40afe632e841d2ab2ff06e70.png" title=""><img alt="" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/f/c/fc72bbb9885865bf40afe632e841d2ab2ff06e70_2_586x500.png" width="586" /></a></div><p></p>
<p>We can of course very easily modify any such function <span class="math">g</span> so that the incentives fall to <span class="math">0</span> after a certain consolidation level <span class="math">r_0 \in [0,1]</span>, by replacing <span class="math">g</span> with <span class="math">\tilde{g}(x) = \max(g(\frac{x}{r_0}), 0)</span>, which squeezes <span class="math">g</span> in the range <span class="math">[0,r_0]</span> and sets the consolidation yield to <span class="math">0</span> afterwards. For example, this is the consolidation yield with <span class="math">r_0 = 80\%</span>, starting from the previous <span class="math">g(x) = \frac{1-x}{4}</span>.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/b/1bf37fb8df61c4442a5054ccdaf8d55b02c351f9.png" title=""><img alt="" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/1/b/1bf37fb8df61c4442a5054ccdaf8d55b02c351f9_2_573x500.png" width="573" /></a></div><p></p>



<h2><a class="anchor" href="https://ethresear.ch#appendix-13" name="appendix-13"></a>Appendix</h2>
<h3><a class="anchor" href="https://ethresear.ch#validator-capping-active-validator-set-variance-14" name="validator-capping-active-validator-set-variance-14"></a>Validator capping: active validator set variance</h3>
<p>Let’s also get an upper bound on the variance of the active validator set size. <span class="math">\mathbb{V}[|V_a|] = \mathbb{V}[\sum_{i\in V} \chi_{\{i \in V_a\}}] = \sum_{i\in V} \mathbb{V}[\chi_{\{i \in V_a\}}]</span>, since each validator is sampled independently. Moreover, <span class="math">\mathbb{V}[\chi_{\{i \in V_a\}}] = 0</span> whenever <span class="math">S_i \ge T</span>, since validator <span class="math">i</span> is then always in <span class="math">V_a</span>.<br />
For <span class="math">S_i &lt; T</span>, the variance is <span class="math">\mathbb{V}[\chi_{\{i \in V_a\}}] = p(S_i)(1 - p(S_i)) = \frac{S_i}{T}(1 - \frac{S_i}{T})</span>, which is maximized when <span class="math">p(S_i) = \frac{1}{2}</span>, or equivalently when <span class="math">S_i = \frac{T}{2}</span>, in which case <span class="math">\mathbb{V}[\chi_{\{i \in V_a\}}] = \frac{1}{4}</span>. Therefore, <span class="math">\mathbb{V}[|V_a|] \le \frac{1}{4}|V|</span>.</p>
<p>Concretely, say we keep a minimum balance of 32 ETH, so that the maximum validator set size <span class="math">|V|</span> is ~4M. The standard deviation of <span class="math">|V_a|</span> is then bounded by <span class="math">\frac{\sqrt{|V|}}{2} \approx 1000</span>. The probability of deviations beyond 10k is then vanishingly low. We can then even explicitly cap the active validator set size, say at 40k validators. Doing so introduces only a tiny correlation to the sampling of different validators, because sampling is completely unaffected other than in the exceedingly rare events of massive deviations.</p>
<h3><a class="anchor" href="https://ethresear.ch#collective-incentives-15" name="collective-incentives-15"></a>Collective incentives</h3>
<h4><a class="anchor" href="https://ethresear.ch#quantifying-the-individual-effect-of-collective-consolidation-incentives-16" name="quantifying-the-individual-effect-of-collective-consolidation-incentives-16"></a>Quantifying the individual effect of collective consolidation incentives</h4>
<p>Let’s look into the consolidation incentives a bit more quantitatively. While it is true that there is always some consolidation incentive whenever consolidation is at all possible, we should also consider how strong these incentives are for various stakers. In particular, the strength of the incentives varies based on how large a staker is, because a consolidation increases yield <em>for everyone</em>, not just for the party which peforms it. In other words, the gains of a consolidation are socialized, to avoid having a sort of consolidation reward, which would effectively disadvantage smaller validators that cannot access it. Consolidation incentives are therefore stronger the larger a validator is. On the one hand, this means that sufficiently large validators have a strong incentive to consolidate, which means we should expect <span class="math">D_a</span> to always represent at least a meaningful portion of the total stake <span class="math">D</span>. On the other hand, it means that small but still meaningfully sized stakers (e.g. 1%) might not be particularly incentivized to consolidate.</p>
<p>To quantify this, let’s look at how much of an issuance increase there is in the event of the full consolidation of a staker having a fraction <span class="math">p</span> of the total stake <span class="math">D</span>, when <span class="math">\frac{D_a}{D} = r</span>. Here we assume that the stake <span class="math">pD</span> in question is initially not consolidated at all, and neglect the small effect it has on <span class="math">D_a</span> (e.g. if <span class="math">T = 1024</span>, a minimum balance validator only increases <span class="math">D_a</span> by 1/32 of its stake). Issuance, and thus yield, increases by a factor of <span class="math">\frac{I(D_a + pD) - I(D_a)}{I(D_a)} = \frac{I((r + p)D)}{I(rD)} - 1</span>. Plugging in the definition of <span class="math">I</span>, we can simplify this to <span class="math">\sqrt{1 + \frac{p}{r}} - 1</span>. As expected, the consolidation incentives grow with <span class="math">p</span>. It is also expected that they fall with <span class="math">r</span>, since the issuance curve <span class="math">I</span> is concave. As it turns out, there’s no dependency on <span class="math">D</span> for this particular form of <span class="math">I</span>.</p>
<p>We now plot <span class="math">100(\sqrt{1 + \frac{p}{r}} - 1)</span>, the <em>percentage</em> of yield increase that every validator experiences when a fraction <span class="math">p</span> of the stake is fully consolidated, starting from <span class="math">D_a = rD</span>. We restrict <span class="math">r</span> to the range <span class="math">[0.1, 1]</span> for ease of visualization, because the consolidation incentives blow up for <span class="math">r</span> near <span class="math">0</span>, as we would wish. Notice that the minimum <span class="math">r</span> is actually <span class="math">1/32</span> for <span class="math">T = 1024</span> and minimum balance 32 ETH.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/c/ccde75a7bf33c1105849424713dceee8fd5b151d.png" title=""><img alt="" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/c/c/ccde75a7bf33c1105849424713dceee8fd5b151d_2_598x500.png" width="598" /></a></div><p></p>
<p>On the other hand, the <em>absolute</em> yield increase <span class="math">100\cdot\frac{I(D_a + pD) - I(D_a)}{D_a}</span> is not independent of <span class="math">D</span>. We plot it here specifically for <span class="math">D = 30M</span> ETH. For lower values of <span class="math">D</span>, the consolidation incentives only get stronger in absolute terms.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/3/53bb8a566b7597b844788fa776551f98df5b36c3.png" title=""><img alt="" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/5/3/53bb8a566b7597b844788fa776551f98df5b36c3_2_567x500.png" width="567" /></a></div><p></p>
<p>Finally, we also plot the yearly ETH returns from consolidation, <span class="math">(I(D_a + pD) - I(D_a))\cdot \frac{p}{r}</span>, again for <span class="math">D = 30M</span> ETH.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/8/68a7e7faee22708549d1b4c2738e1016de2cf661.png" title=""><img alt="" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/6/8/68a7e7faee22708549d1b4c2738e1016de2cf661_2_578x500.png" width="578" /></a></div><p></p>
<h4><a class="anchor" href="https://ethresear.ch#generalizing-collective-incentives-17" name="generalizing-collective-incentives-17"></a>Generalizing collective incentives</h4>
<p>When <span class="math">I</span> is our current issuance curve, <span class="math">I(x) = c\sqrt{x}</span>, we have that <span class="math">I(D_a) = c\sqrt{D_a} = c \sqrt{D} \sqrt{\frac{D_a}{D}} = I(D)\sqrt{\frac{D_a}{D}}</span>. In other words, we can think about the previous proposal as incentivizing a high <span class="math">\frac{D_a}{D}</span> ratio by directly a  applying an issuance penalty based on it. More generally, we can let the issuance be <span class="math">I(D_a, D) = I(D) \cdot \delta(\frac{D_a}{D})</span> for any <span class="math">\delta</span> such that <span class="math">\delta(0) = 0</span> and <span class="math">\delta(1) = 1</span>. With this, the yield increase from consolidating is exactly <span class="math">\frac{I(D)}{D} \cdot (\delta(r + p) - \delta(p))</span>, i.e., a fraction <span class="math">\delta(r + p) - \delta(r)</span> of the maximum yield available at deposit size <span class="math">D</span>. The percentage yield increase is instead <span class="math">\frac{\delta(r + p) - \delta(r)}{\delta(r)}</span>. The simplest case is <span class="math">\delta(r) = r</span>, where <span class="math">I(D_a, D) = I(D) \cdot \frac{D_a}{D}</span>, in which case the yield increase is simply <span class="math">p \frac{I(D)}{D}</span>, constant in <span class="math">r</span>, and the percentage yield increase is <span class="math">\frac{p}{r}</span>.</p>
<p>In this form, we can more clearly separate the design of incentives to stake from that of incentives to consolidate the stake: <span class="math">I(D)</span> provides the <em>maximum</em> possible incentive to stake at a given total deposit level <span class="math">D</span>, while <span class="math">\delta</span> regulates the incentive to consolidate at a given ratio <span class="math">\frac{D_a}{D}</span>. We can for example have <span class="math">I</span> being concave, as it is currently, but <span class="math">\delta</span> linear as in the previous example: the protocol then considers stake deposits to have diminishing returns, while it believes consolidation to be equally valuable regardless of where <span class="math">\frac{D_a}{D}</span> currently sits.</p>
<h4><a class="anchor" href="https://ethresear.ch#discouragement-attacks-18" name="discouragement-attacks-18"></a>Discouragement attacks</h4>
<p>At any point, it is possible to increase <span class="math">D</span> while barely increasing <span class="math">D_a</span>, by activating validators with minimum balance. Thus, the issuance <span class="math">I(D_a)</span> is approximately constant, but distributed to more stake. This is the same <a href="https://ethresear.ch/t/reward-curve-with-tempered-issuance-eip-research-post/19171#h-53-discouragement-attacks-32">discouragement attack</a> that would be possible with a constant issuance curve, or with issuance capped at some maximum value, where the yield also decreases like <span class="math">\frac{1}{D}</span>. While worse than today, where it decreases like <span class="math">\frac{1}{\sqrt{D}}</span>, this discouragement attack is nothing like the ultra cheap griefing vector that would arise with if we were to <a href="https://notes.ethereum.org/@vbuterin/single_slot_finality#Economic-capping-of-total-validator-count" rel="noopener nofollow ugc">use issuance to target a validator count</a>. For example, say we started reducing issuance past our ideal target of ~30k validators, and were to go negative at 40k. Then, activating a few thousands minimum balance validators, in the order of 0.01% to 0.1% of the stake, would be enough to make yields go negative. On the other hand, in the context of the discouragement attack we are considering here, reducing yield by a factor of <span class="math">k</span> requires increasing the deposit size by a factor of <span class="math">k</span>. For example, halving issuance when <span class="math">D = </span> 20M requires depositing another 20M.</p>
<h4><a class="anchor" href="https://ethresear.ch#stake-capping-19" name="stake-capping-19"></a>Stake capping</h4>
<p>If we were to set the issuance based on <span class="math">D_a</span>, we would not be able to immediately adopt any issuance curve that reduces issuance past some deposit size, like the ones discussed <a href="https://ethresear.ch/t/reward-curve-with-tempered-issuance-eip-research-post/19171/1">here</a> and <a href="https://ethresear.ch/t/endgame-staking-economics-a-case-for-targeting/18751">here</a>. The reason for that is simple: if issuance goes down past a certain value of <span class="math">D_a</span>, but it turns out that the yield at that point is still attractive, the incentives are such that <span class="math">D</span> would still grow (more stake wants yield at these levels) while <span class="math">D_a</span> would not (growing <span class="math">D_a</span> lowers yield). In fact, instead of consolidation incentives, we end up having incentives for splitting up stake over multiple validators, so as to decrease <span class="math">D_a</span> and keep it at the point of maximum issuance! Meanwhile, stake capping is not achieved, at least not any more than we would already achieve it by capping issuance at the maximum value, rather than having it decrease afterwards.</p>
<p>If we did want to adopt some form of stake capping, we would then need to do things a bit differently. We could let the issuance be <span class="math">I(D_a, D) = I(D_a) - f(D)</span>, where <span class="math">f</span> acts to reduce the issuance past some critical <em>total</em> deposit size. Intuitively, the goal is to try to ensure two things at once: that we have enough <span class="math">D_a</span>, and that we do not have too much <span class="math">D</span>. For example:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/8/c83ba4769d0fc0c8db841d28c0210dfdd3ab53d2.png" title=""><img alt="" height="227" src="https://ethresear.ch/uploads/default/optimized/3X/c/8/c83ba4769d0fc0c8db841d28c0210dfdd3ab53d2_2_690x227.png" width="690" /></a></div><p></p>
<p>To help visualizing the effect of this further, here are the cumulative issuance and yield while holding <span class="math">\frac{D_a}{D} = 0.8</span>.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/0/4/044c55ed764d7dfddac4c2df5be73783c8017800.png" title=""><img alt="" height="230" src="https://ethresear.ch/uploads/default/optimized/3X/0/4/044c55ed764d7dfddac4c2df5be73783c8017800_2_690x230.png" width="690" /></a></div><p></p>
<p>Finally, here is a color plot of the yield in the <span class="math">(D, \frac{D_a}{D})</span> space. <span class="math">D</span> starts at 2 to help the visualization be effective.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/4/744db3435b9b033af6e55309e19068c435be1ffb.png" title=""><img alt="" height="463" src="https://ethresear.ch/uploads/default/optimized/3X/7/4/744db3435b9b033af6e55309e19068c435be1ffb_2_690x463.png" width="690" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#individual-incentives-20" name="individual-incentives-20"></a>Individual incentives</h3>
<h4><a class="anchor" href="https://ethresear.ch#total-issuance-21" name="total-issuance-21"></a>Total issuance</h4>
<p>The total <em>extra</em> issuance is:</p>
<p><span class="math">I_c(D_a, D) = \sum_{i \in V} y_c(D_a, D, S_i) S_i = g(0) y(D_a, D)(1 - \frac{D_a}{D}) \sum_{i \in V} p(S_i)S_i = \\ = g(0)y(D_a, D)\cdot D_a(1 - \frac{D_a}{D}) = g(0)I(D_a)\cdot \frac{D_a}{D}(1 - \frac{D_a}{D}) = \\
= g(0) \sqrt{D} \sqrt{\frac{D_a}{D}}\cdot \frac{D_a}{D}(1 - \frac{D_a}{D}) = g(0)I(D) \sqrt{\frac{D_a}{D}}\cdot \frac{D_a}{D}(1 - \frac{D_a}{D})</span></p>
<p>The total issuance then is:</p>
<p><span class="math">I_T(D_a, D) = I(D_a) + I_c(D_a, D) = I(D_a)(1 + g(0) \frac{D_a}{D}(1 - \frac{D_a}{D})) = \\
= c \sqrt{D} \sqrt{\frac{D_a}{D}}(1 + g(0)\cdot \frac{D_a}{D}(1 - \frac{D_a}{D})) = \\
= I(D) \sqrt{\frac{D_a}{D}} (1 + g(0)\frac{D_a}{D}(1 - \frac{D_a}{D})) = I(D) \cdot h(\frac{D_a}{D})</span>, where <span class="math">h(x) = \sqrt{x}(1 + g(0)x(1-x))</span>. For <span class="math">g(0) = \frac{1}{4}</span>, we have that <span class="math">h(x) \le 1</span> for <span class="math">x \in [0,1]</span>, so <span class="math">I(D)</span> remains an upper bound on the total issuance.</p>
<p><img alt="" height="435" src="https://ethresear.ch/uploads/default/original/3X/5/4/543c7981de3b9feeff11aff29ac6556c3f9ad5cf.png" width="547" /></p>
<h4><a class="anchor" href="https://ethresear.ch#generalizing-individual-consolidation-incentives-22" name="generalizing-individual-consolidation-incentives-22"></a>Generalizing individual consolidation incentives</h4>
<p>More generally, we can choose any consolidation yield curve <span class="math">y_c(D_a, D, S) = \frac{\min(S,T)}{T} y_c(D_a, D)</span>, not necessarily depending on <span class="math">y(D_a, D)</span>, or even any curve <span class="math">y_c(D_a, D, S)</span> with a different kind of dependency on <span class="math">S</span>. An interesting example of the first kind is the curve <span class="math">y_c(D_a, D, S) = \frac{\min(S,T)}{T} (y(D) - y(D_a, D))</span>, where <span class="math">y_c(D_a, D, S)</span> essentially interpolates between the yield <span class="math">y(D) = y(D_a = D, D)</span> that would be paid out to a fully consolidated validator set at deposit size <span class="math">D</span>, and the base yield <span class="math">y(D_a, D)</span> paid out at the current consolidation level. In other words, a validator with <span class="math">T</span> stake always gets paid the maximum possible yield for deposit size <span class="math">D</span>, <span class="math">y(D)</span>, regardless of the consolidation level achieved by the whole validator set, while validators with minimum stake get paid closer to the base yield <span class="math">y(D_a, D)</span>, and their yield linearly increases to <span class="math">y(D)</span> as they consolidate. In this case, the consolidation incentives are quite a bit stronger at lower consolidation levels.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/2/5229207a5722061af774a3d38e53aa0d28a08a89.png" title=""><img alt="" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/5/2/5229207a5722061af774a3d38e53aa0d28a08a89_2_573x500.png" width="573" /></a></div><p></p>
<p>While the absolute yield increase falls with <span class="math">D_a</span>, the percentage yield increase from consolidating does not. As it turns out, it only depends on <span class="math">\frac{D_a}{D}</span>:<br />
<span class="math">\frac{y_c(D_a, D)}{y(D_a, D)} = \frac{y(D) - y(D_a, D)}{y(D_a, D)} =
\frac{y(D)}{y(D_a, D)} - 1 = \sqrt{\frac{D}{D_a}} - 1 </span></p>
<p>In other words, this also fits the previous form <span class="math">y_c(D_a, D, S) = \frac{\min(S,T)}{T} g(\frac{D_a}{D}) y(D_a, D)</span>, with <span class="math">g(x) = \sqrt{\frac{1}{x}} - 1</span> instead of a linear function.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/a/4ab4a1cd832f6644730fd660799e1167f71af570.png" title=""><img alt="" height="293" src="https://ethresear.ch/uploads/default/optimized/3X/4/a/4ab4a1cd832f6644730fd660799e1167f71af570_2_690x293.png" width="690" /></a></div><p></p>
<p>Since <span class="math">y(D_a, D) + y_c(D_a, D, S) \le y(D)</span>, it still holds that <span class="math">I(D)</span> is a bound on the total issuance. In fact, the total issuance can be worked out to be <span class="math">I_T(D_a, D) = I(D_a) + I_c(D_a, D) = I(D) \sqrt{\frac{D_a}{D}}(1 + \sqrt{\frac{D_a}{D}} (1 - \sqrt{\frac{D_a}{D}})) = I(D) h(\frac{D_a}{D})</span>, with <span class="math">h(x) = \sqrt{x}(1 + \sqrt{x}(1 - \sqrt{x})))</span>, which we compare here to the previous example:</p>
<p><img alt="" height="435" src="https://ethresear.ch/uploads/default/original/3X/8/e/8e42613ed48e495edae62b74cecc8f994f7499a9.png" width="547" /></p>
            <p><small>3 posts - 3 participants</small></p>
            <p><a href="https://ethresear.ch/t/orbit-ssf-solo-staking-friendly-validator-set-management-for-ssf/19928">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 28 Jun 2024 07:22:10 +0000</pubDate>
</item>
<item>
<title>Number Duplicate Messages in Ethereum's Gossipsub Network</title>
<link>https://ethresear.ch/t/number-duplicate-messages-in-ethereums-gossipsub-network/19921</link>
<guid>https://ethresear.ch/t/number-duplicate-messages-in-ethereums-gossipsub-network/19921</guid>
<content:encoded><![CDATA[
<div> 关键词：GossipSub、Ethereum、消息重复、Hermes、优化建议

总结:<br />
GossipSub是Ethereum P2P网络中的消息广播协议，其设计允许一定程度的消息重复。研究团队使用工具Hermes追踪GossipSub性能，发现正常情况下每个消息最多接收3次重复（不包括通过IHAVE/IWANT控制消息传播的情况）。研究发现，通过网格的重复消息保持在3次或以下，但通过Gossip机制可能会有额外的重复。团队提出两点优化建议：限制并发IWANT请求的数量（类似Kademlia的alpha参数）和降低心跳频率，以减少IHAVE消息和额外重复。最后，尽管存在一些边缘情况，但大部分重复消息并不构成重大问题。 <div>
<h1><a class="anchor" href="https://ethresear.ch#summary-tldr-1" name="summary-tldr-1"></a>Summary &amp; TL;DR</h1>
<p>The ProbeLab team (<a href="https://probelab.io/" rel="noopener nofollow ugc">probelab.io</a>) is carrying out a study on the performance of Gossipsub in Ethereum’s P2P network. Following from our previous post on the “<a href="https://ethresear.ch/t/gossipsub-network-dynamicity-through-grafts-and-prunes/19750">Gossipsub Network Dynamicity through GRAFTs and PRUNEs</a>” in this post we investigate the number of messages and duplicated messages seen by our node, per topic. There is no public data on the overhead that broadcasting messages and control data over the network imply on each participating node.</p>
<p>For the purposes of this study, we have built a tool called <strong>Hermes, which acts as a GossipSub listener and tracer</strong> (<a href="https://github.com/probe-lab/hermes/" rel="noopener nofollow ugc">GitHub - probe-lab/hermes: A Gossipsub listener and tracer.</a>). Hermes subscribes to all relevant pubsub topics and traces all protocol interactions. The results reported here are from a 3.5hr trace.</p>
<p><strong>Study Description:</strong> Gossipsub’s design is inherently allowing for message duplicates. A brief model we develop shows that it’s normal to receive each message up to 3 extra times (as a duplicate). This excludes the gossip mechanism which propagates messages through the IHAVE/IWANT control message sequence.</p>
<p><strong>TL;DR:</strong> We find that indeed duplicates through mesh stay in the order of 3 per message or below, which, however, doesn’t count for duplicates through gossip. For instance, there are edge cases where a message is requested (and responded to) through an IWANT message while the actual message is already in transit. Eventually, this results in an extra duplicate. We make two recommendations:</p>
<ol>
<li><strong>Reduce the number of concurrent <code>IWANT</code> messages we send through a limiting factor</strong> (somewhat similar to kademlia’s <code>alpha</code> parameter).</li>
<li><strong>Lower the current <code>heartbeat</code> frequency (i.e., increasing the <code>heartbeat</code> interval) from 0.7 seconds to 1 second</strong> (as per the original protocol spec and recommendation). This would reduce the excessive <code>IHAVE</code> messages and reduce the chances of generating extra duplicates.</li>
</ol>
<h1><a class="anchor" href="https://ethresear.ch#background-2" name="background-2"></a>Background</h1>
<p><a href="https://github.com/libp2p/specs/blob/f25d0c22e5ef045c8c050bc91c297468de35f720/pubsub/gossipsub/gossipsub-v1.1.md" rel="noopener nofollow ugc">GossipSub</a> is a routing system that can be enabled on libp2p’s <a href="https://github.com/libp2p/specs/blob/f25d0c22e5ef045c8c050bc91c297468de35f720/pubsub/README.md" rel="noopener nofollow ugc">PubSub</a> message broadcasting protocol. This protocol organizes the message broadcasting channels on what is commonly known as Topics, where peers subscribed to a given topic keep a particular subset of connected peers for that particular topic. This subset of peer connections per topic is also known as “mesh”.</p>
<p>In the case of GossipSub, the standard broadcasting mechanism of PubSub is extended with a few sets of enhancements that make it:</p>
<ul>
<li>more efficient than what is commonly called flooding, reducing the protocol’s bandwidth usage</li>
<li>more resilient, as the protocol:
<ul>
<li>shares metadata of seen messages over sporadic Gossip messages (for censorship or Sybil attacks)</li>
<li>keeps a local score for each mesh-connected peer to ensure healthy and useful connections, where each peer keeps connections with the highest scoring neighbours</li>
<li>avoids sharing a message with peers that already sent the message to us</li>
</ul>
</li>
</ul>
<p>This all looks good on paper. However, there is still no public data on the overhead that broadcasting messages and control data over the network imply on each participating node. Even more importantly, how much room for improvement exists within the protocol and the implementations to make it more optimal.</p>
<h2><a class="anchor" href="https://ethresear.ch#expected-results-3" name="expected-results-3"></a>Expected Results</h2>
<p>Message propagation through the GossipSub’s mesh considers some occasional duplicates that can arrive as the message might come from different peers within the mesh:</p>
<p>Given:</p>
<ul>
<li><code>n</code> as the number of nodes in the graph</li>
<li><code>k</code> as the mesh degree</li>
<li><code>l</code> as the number of connections (links) between two nodes <span class="math">l = \frac{nk}{2}</span></li>
</ul>
<p>The number of links used to propagate a message to all nodes in the graph can be defined as <code>n-1 ~= n</code>. The links form a spanning tree with the message origin as root (<code>n</code> is big enough compared to the initial sender link, so that it can be considered negligible).</p>
<p>The number of links not used to propagate a specific message corresponds to <span class="math">l-n = \frac{n(k-2)}{2}</span>.</p>
<p>This means that on average each node will have 1 link used to receive a message, 1 to propagate it to a peer that doesn’t have it yet. And the rest <code>k-2</code>, to either send or receive the duplicate message.</p>
<p>Assuming that <span class="math">\frac{k-2}{2}</span> links are used to send the message to peers that already have it, it means that we receive <span class="math">\frac{k-2}{2}</span> duplicate messages.</p>
<p>In the case of Ethereum, <code>k=8</code>, and therefore, it follows that <span class="math">\frac{k-2}{2} = 3</span>. So, <strong>the expected value is to receive 3 duplicate messages for each message</strong>.</p>
<h1><a class="anchor" href="https://ethresear.ch#results-4" name="results-4"></a>Results</h1>
<p>As previously introduced, this report aims to provide insights on:</p>
<ul>
<li>the number of duplicate messages that we receive per each shared message in the network,</li>
<li>the extra bandwidth that we are spending on duplicates,</li>
<li>any existing unexpected behavior or potential optimization that could be applied on GossipSub.</li>
</ul>
<blockquote>
<p>NOTES:<br />
The numbers presented in the following sections belong to the same 3.5 hours run of <code>Hermes</code> as the previous studies, with the following extra configuration:</p>
<ul>
<li>The experiment is ran on the <code>Holesky</code> network</li>
<li>Our node was subscribed to the following topics:
<ul>
<li><code>beacon_block</code></li>
<li><code>beacon_aggregate_and_proof</code></li>
<li><code>sync_commmittee_contribution_and_proof</code></li>
<li><code>attester_slashing</code></li>
<li><code>proposer_slashing</code></li>
<li><code>voluntary_exit</code> * (check <code>Hermes</code> issue → <a class="inline-onebox" href="https://github.com/probe-lab/hermes/issues/24" rel="noopener nofollow ugc">Broadcasting of invalid `voluntary_exit` messages to mesh peers · Issue #24 · probe-lab/hermes · GitHub</a>)</li>
<li><code>bls_to_execution_change</code></li>
</ul>
</li>
</ul>
</blockquote>
<h2><a class="anchor" href="https://ethresear.ch#overall-number-of-messages-5" name="overall-number-of-messages-5"></a>Overall Number of Messages</h2>
<p>To give a little bit of context, the report starts by taking a look at the number of messages and the respective duplicates received over time. The following graph shows the number of <code>HANDLED</code> events by the libp2p-host in comparison with the <code>DELIVERED</code> and <code>DUPLICATED</code> ones.</p>
<blockquote>
<p>NOTE: In this report we will consider the <code>DELIVER</code> events as unique identifier of the arrival of a message. This is because the internal event tracer at the libp2p host notifies of the arrival of a unique message at multiple levels, which in turn, makes the <code>HANDLED</code> and <code>DELIVER</code> events at the arrival of a new message the exact same notification, just at different levels of the host.</p>
</blockquote>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/5/65a05809dfbc2915a07ceadedbf9cd8d85f16fe8.jpeg" title="overall-number-of-events"><img alt="overall-number-of-events" height="309" src="https://ethresear.ch/uploads/default/optimized/3X/6/5/65a05809dfbc2915a07ceadedbf9cd8d85f16fe8_2_517x309.jpeg" width="517" /></a></div><p></p>
<ul>
<li>The number of unique messages (i.e., <code>HANDLE_MESSAGE</code>) stays steady around the 3,000 and 3,200 unique messages per minute.</li>
<li>By looking closer into the messages per topic (not shown here), we observe that the topic with the highest message frequency is the <code>beacon_aggregate_and_proof</code> one, receiving over 90% of the tracked unique messages.</li>
<li>There are some duplicated spikes at the <code>beacon_block</code> topic that reach up to 60 duplicates  in some occasions.</li>
<li>The number of duplicates seems to vary quite wildly over time, which can be related to the number of connections per mesh (as per the analysis done further up which showed that 3 duplicates per message are expected).</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#number-of-duplicate-messages-6" name="number-of-duplicate-messages-6"></a>Number of Duplicate Messages</h2>
<p>When it comes to the actual number of <code>DUPLICATE</code> messages, the following figures show that number of duplicates can oscillate over time.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/d/edacb4d1d050448d2a5b17ef6c67ed0cb3ca42e0.png" title="duplicates-per-topic"><img alt="duplicates-per-topic" height="309" src="https://ethresear.ch/uploads/default/optimized/3X/e/d/edacb4d1d050448d2a5b17ef6c67ed0cb3ca42e0_2_517x309.png" width="517" /></a></div><p></p>
<p>Clearly, the <code>beacon_block</code> topic seems to be the only one generating the largest number of spikes at times.</p>
<h2><a class="anchor" href="https://ethresear.ch#cdf-of-duplicate-messages-7" name="cdf-of-duplicate-messages-7"></a>CDF of Duplicate Messages</h2>
<p>The following graph shows the Cumulative Distribution Function (CDF) of the duplicates per message per topic. In the graph, we can see that:</p>
<ul>
<li>smaller but more frequent messages like the <code>beacon_ggregate_and_proof</code> and <code>sync_commitee_contributions</code> do have fewer duplicates.
<ul>
<li>between 32% and 45% of the messages do not have any duplicates.</li>
<li>50% of the messages are received with less than 2 duplicate messages, keeping the mean lower than the theoretical target of <code>3</code> duplicates per message.</li>
<li>the upper tail shows that less than 10% of the messages get more than 4 duplicates, with a cap at 8-10 duplicates (i.e., the node’s mesh size, <code>D</code>).</li>
</ul>
</li>
<li>the case of the <code>beacon_blocks</code> is completely different.
<ul>
<li>there are almost no recorded messages without duplicates (1%-2%).</li>
<li>54% of the messages report the expected <code>3</code>  duplicates from the mesh</li>
<li>Taking look at the tail of the CDF (shown in the dropdown plot further down) there are a few messages that were received up to 34 or 40 times.</li>
</ul>
</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/0/1/0153b674d22c5c90c7fee45cbf880ec5b865d548.png" title="CDF-duplicates"><img alt="CDF-duplicates" height="309" src="https://ethresear.ch/uploads/default/optimized/3X/0/1/0153b674d22c5c90c7fee45cbf880ec5b865d548_2_517x309.png" width="517" /></a></div><p></p>
<h2><a class="anchor" href="https://ethresear.ch#correlation-between-message-size-and-number-of-duplicates-8" name="correlation-between-message-size-and-number-of-duplicates-8"></a>Correlation between Message Size and Number of Duplicates</h2>
<p>From the CDF above there seems to be a pattern of “the bigger the size of the message, the more duplicates it has”. So we went a step further to investigate if there is indeed a correlation. The following graph shows that the correlation between the size of a message and the number of duplicates is somewhat present but is not a norm or at least doesn’t follow any fixed pattern.</p>
<p>The figure is complemented by two auxiliary quartile plots or “boxplots”, which represent the given distribution of points of their respective axis, helping us understand that:</p>
<ul>
<li><code>sync_commmittee_contribution_and_proof</code> messages are the smallest ones in size, which also correlates with the smallest ratio of duplicate messages.</li>
<li><code>beacon_aggregate_and_proof</code> messages are the second ones in size, having also a bigger tail of duplicates on the Y concentration plot.</li>
<li><code>beacon_block</code> messages, despite being the ones with the widest variation in size, do not follow any particular pattern that could correlate the message size with the number of duplicates.</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/b/fb40e85a2381cd48f38c553e329c3f0083a27196.png" title="msg-size-number-of-duplicates"><img alt="msg-size-number-of-duplicates" height="374" src="https://ethresear.ch/uploads/default/optimized/3X/f/b/fb40e85a2381cd48f38c553e329c3f0083a27196_2_383x374.png" width="383" /></a></div><p></p>
<p>As such, we conclude that <strong>there is no correlation between message size and number of duplicates</strong>.</p>
<h2><a class="anchor" href="https://ethresear.ch#arrival-time-of-duplicates-9" name="arrival-time-of-duplicates-9"></a>Arrival Time of Duplicates</h2>
<p>Reducing the number of duplicates has already been a topic of discussion in the community. There are already some proposals like <a href="https://github.com/libp2p/specs/blob/f25d0c22e5ef045c8c050bc91c297468de35f720/pubsub/gossipsub/gossipsub-v1.2.md" rel="noopener nofollow ugc">gossipsub1.2 </a> that spotted this large number of duplicated messages previously, proposing the addition of a new control <code>IDONTWANT</code> message that could not only notify other peers that we already got a message, but also cancel the <code>IWANT</code> ongoing messages.</p>
<p>In order to see how effective the <code>IDONTWANT</code> control message would be, we’ve computed the time between the first delivery of each message and their respective first duplicate. This is done to validate that there is enough time to send the <code>IDONTWANT</code> message once a new message is received (prior to the message validation) and before the duplicate starts being sent over.</p>
<p>The following graph gives the time between the delivery time of a message and the time to the first duplicated message in seconds.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/8/e87ebed2ebacfb8abd79473ceb14e2af58bc7b82.png" title="arrival-cdf"><img alt="arrival-cdf" height="309" src="https://ethresear.ch/uploads/default/optimized/3X/e/8/e87ebed2ebacfb8abd79473ceb14e2af58bc7b82_2_517x309.png" width="517" /></a></div><p></p>
<p>Results show that 50% of the duplicated beacon blocks arrive within 73 milliseconds, roughly an entire Round Trip Time (RTT) with a well connected peer. In practice, this means that <strong>the <code>IDONTWANT</code> message could prevent at least the other 50% of messages that arrive between 73 milliseconds and 2 seconds of the first arrival</strong>.</p>
<p>We’ve spotted that a big part of the duplicated messages arrive from <code>IWANT</code> messages that we sent milliseconds before the arrival of the same message though the mesh.<br />
The <a href="https://github.com/libp2p/specs/blob/f25d0c22e5ef045c8c050bc91c297468de35f720/pubsub/gossipsub/gossipsub-v1.2.md" rel="noopener nofollow ugc">gossipsub1.2</a> proposal already contemplates <a href="https://github.com/libp2p/specs/blob/f25d0c22e5ef045c8c050bc91c297468de35f720/pubsub/gossipsub/gossipsub-v1.2.md#cancelling-iwant" rel="noopener nofollow ugc">this scenario</a>, where the same <code>IDONTWANT</code> message could break or stop any ongoing responses to <code>IWANT</code> messages for that <code>msgID</code>.</p>
<p>In summary, we conclude that <strong>the <code>IDONTWANT</code> control message addition to Gossipsub will be a valuable enhancement that can indeed prevent the vast majority of duplicate messages</strong>.</p>
<h1><a class="anchor" href="https://ethresear.ch#conclusions-and-takeaways-10" name="conclusions-and-takeaways-10"></a>Conclusions and takeaways</h1>
<blockquote>
<p>This set of conclusions have been extracted from running the <code>go-libp2p</code>  implementation and, although it also involves the traces of how other implementations interact with Hermes, it might be a biased conclusion from the point of view of the Go implementation.</p>
</blockquote>
<ol>
<li>
<p>We have identified that there is no limit on the number of peers that we simultaneously send <code>IWANT</code> messages to for the same <code>msgID</code>.<br />
We identify that this has some benefits:</p>
<ul>
<li>Concurrently fetches the message from multiple actors.</li>
<li>Bypasses bandwidth limitations of peer(s) we have sent <code>IWANT</code> messages to, since we have forwarded the <code>IWANT</code> message to multiple peers.</li>
</ul>
<p>However, it also has obvious downsides:</p>
<ul>
<li>
<p>We receive multiple duplicates from the peers that respond to our simultaneous <code>IWANT</code> request, consuming more bandwidth on both ends.</p>
</li>
<li>
<p>The message could be already on the wire through the mesh connections, so when the <code>IWANT</code> message responses arrive, the message was already delivered through the mesh.</p>
</li>
<li>
<p>There is no track of who we contacted for a given message, given that Gossipsub is:</p>
<ul>
<li>forwarding the message only the first time we see it, and</li>
<li>removing the peer that sent us the message from the list of peers we’re broadcasting the message to and forgetting about that peer.</li>
</ul>
<p>This makes the entire broadcasting process unaware of who sent us that message in <code>IHAVE</code>s, or who we are already contacting for a particular message - resulting in multiple duplicates.</p>
</li>
</ul>
<p><a href="https://github.com/libp2p/specs/blob/f25d0c22e5ef045c8c050bc91c297468de35f720/pubsub/gossipsub/gossipsub-v1.2.md#cancelling-iwant" rel="noopener nofollow ugc">Canceling ongoing <code>IWANT</code>messages</a> with <code>IDONTWANT</code> messages, which is a proposal included in <a href="https://github.com/libp2p/specs/blob/f25d0c22e5ef045c8c050bc91c297468de35f720/pubsub/gossipsub/gossipsub-v1.2.md" rel="noopener nofollow ugc">gossipsub1.2</a> is a valuable enhancement that will limit the number of duplicates.</p>
<h3><a class="anchor" href="https://ethresear.ch#recommendation-1-11" name="recommendation-1-11"></a><strong>Recommendation 1</strong></h3>
<p>We propose having a limiting factor (somewhat similar to kademlia’s <code>alpha</code> parameter), which would limit the number of concurrent <code>IWANT</code> messages we send for the same <code>msgID</code>.</p>
<hr />
<hr />
</li>
<li>
<p>The gossiping mechanism of Gossipsub acts as a backup mechanism to the broadcasting/mesh propagation part of the protocol for those messages that didn’t manage to reach all nodes in the network. The more frequent gossiping is, the higher its contribution becomes to message propagation (i.e., more messages are being requested through <code>IWANT</code> requests because they have not reached the entirety of the network).</p>
<p>An edge case that results from very frequent gossiping (i.e., small <code>heartbeat</code> interval) is that messages that are already in transit, but have not been downloaded completely, are being requested through an <code>IWANT</code> message. This inevitably results in a duplicate message once both messages arrive at their destination.</p>
<p>It is hard to quantify how often the message responses to <code>IWANT</code> messages are indeed future duplicates, but it is still worth pointing out that high heartbeat frequency increases the chances of those edge cases.</p>
<h3><a class="anchor" href="https://ethresear.ch#recommendation-2-12" name="recommendation-2-12"></a>Recommendation 2</h3>
<p>A quick and straightforward optimization is to <strong>lower the current <code>heartbeat</code> frequency (i.e., increasing the <code>heartbeat</code> interval) from 0.7 seconds to 1 second</strong> (as per the original protocol spec and recommendation). This would reduce the excessive <code>IHAVE</code> messages and reduce the chances of generating extra duplicates.</p>
<hr />
<hr />
</li>
<li>
<p>We have spotted some edge cases that may occur due to the “lack” of control over the triggered events at GossipSub (<code>IHAVE</code>/ <code>IWANT</code>).</p>
<p>It isn’t easy to judge from the logs whether those cases are just a matter of timing, as GossipSub replies to those events as interruptions (at least in the Go implementation), or if some of those cases are caused by a bug in one of the implementations.</p>
<p>We found that <strong>the number of messages where we received multiple duplicates from the same peer to just 1% of the total number of <code>beacon_blocks</code> received</strong>. We, therefore, conclude that this is not critical or an issue that requires further investigation.</p>
</li>
</ol>
<p>For more details and <strong>weekly network health reports on Ethereum’s discv5 DHT network</strong> head over to <a href="https://probelab.io/" rel="noopener nofollow ugc">probelab.io</a>.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/number-duplicate-messages-in-ethereums-gossipsub-network/19921">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 27 Jun 2024 08:48:46 +0000</pubDate>
</item>
<item>
<title>Estimating Validator Decentralization Using p2p Data</title>
<link>https://ethresear.ch/t/estimating-validator-decentralization-using-p2p-data/19920</link>
<guid>https://ethresear.ch/t/estimating-validator-decentralization-using-p2p-data/19920</guid>
<content:encoded><![CDATA[
<div> 关键词：地理分布、验证器、Ethereum、共识层网络、节点连接

总结:<br />
文章探讨了Ethereum区块链中验证器的地理分布问题，重点关注了验证器客户端与 beacon 节点的分离。研究者通过分析验证器的职责、随机分配的委员会以及使用的网络协议，确定了验证器在短-lived attestation subnets上的活动作为调查核心。方法论包括监听节点订阅请求、收集和分析元数据，尤其是订阅的子网数量。然而，由于某些客户端的行为策略，实际观察到的短-lived子网订阅较少，限制了验证器数量的准确估计。结果仅提供了部分验证器的地理分布信息，且存在一些局限性，如最大估计值为62个验证器等。 <div>
<blockquote>
<p>Written by <a href="https://x.com/mempirate" rel="noopener nofollow ugc">Jonas</a> &amp; <a href="https://x.com/namn_grg" rel="noopener nofollow ugc">Naman</a> from <a href="https://x.com/chainbound_" rel="noopener nofollow ugc">Chainbound</a>.<br /><br />
This research was funded by the Robust Incentives Group at the Ethereum Foundation. This work is specifically related to ROP-8. Additional information can be found <a href="https://www.notion.so/bad7233658cc41f38b26e7b4f6cf6e8b?pvs=21" rel="noopener nofollow ugc">here</a>. We want to thank <a href="https://x.com/soispoke" rel="noopener nofollow ugc">soispoke</a>, the <a href="https://x.com/EthPandaOps" rel="noopener nofollow ugc">EF DevOps team</a>, <a href="https://migalabs.io/" rel="noopener nofollow ugc">MigaLabs</a> and <a href="https://probelab.io/" rel="noopener nofollow ugc">ProbeLab</a> for their advice and contributions!</p>
</blockquote>
<h2><a class="anchor" href="https://ethresear.ch#table-of-contents-1" name="table-of-contents-1"></a>Table of Contents</h2>
<ul>
<li><a href="https://ethresear.ch#introduction">Introduction</a></li>
<li><a href="https://ethresear.ch#anatomy-of-a-validator">Anatomy of a validator</a></li>
<li><a href="https://ethresear.ch#attestation-duties-and-committees">Attestation duties and committees</a></li>
<li><a href="https://ethresear.ch#attestation-subnets">Attestation subnets</a>
<ul>
<li><a href="https://ethresear.ch#subnet-types">Subnet types</a></li>
</ul>
</li>
<li><a href="https://ethresear.ch#validator-footprints">Validator footprints</a></li>
<li><a href="https://ethresear.ch#methodology">Methodology</a>
<ul>
<li><a href="https://ethresear.ch#long-lived-subnets">Long-lived subnets &amp; node metadata</a></li>
<li><a href="https://ethresear.ch#short-lived-subnets">Short-lived subnets</a></li>
<li><a href="https://ethresear.ch#estimating-validator-counts">Estimating validator counts</a></li>
</ul>
</li>
<li><a href="https://ethresear.ch#architecture">Architecture</a>
<ul>
<li><a href="https://ethresear.ch#crawler">Crawler</a></li>
<li><a href="https://ethresear.ch#consumer">Consumer</a></li>
</ul>
</li>
<li><a href="https://ethresear.ch#results">Result</a></li>
<li><a href="https://ethresear.ch#limitations">Limitations</a></li>
<li><a href="https://ethresear.ch#references">References</a></li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#introduction-2" name="introduction-2"></a>Introduction</h2>
<p>The geographical distribution of a validator set is <a href="https://collective.flashbots.net/t/decentralized-crypto-needs-you-to-be-a-geographical-decentralization-maxi/1385" rel="noopener nofollow ugc">one of the most critical factors</a> in determining a blockchain’s level of decentralization. Validator decentralization is vital for Ethereum. It enhances network security, resilience, and censorship resistance by distributing control and minimizing the risk of single points of failure or malicious attacks.</p>
<p>It is well known that Ethereum has a <a href="https://beaconcha.in/charts/validators" rel="noopener nofollow ugc">very large</a> validator set, but <strong>is this validator set geographically distributed?</strong> Ethereum has a substantial amount  of beacon nodes running on the consensus layer network, with current estimates at around ~12,000 active nodes (<a href="https://nodewatch.io/" rel="noopener nofollow ugc">source</a>). A beacon node serves as a <em>potential</em> entrypoint into the network for validators, but it is not representative of the actual validator distribution.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/f/8f1dda810cb6cc5f9d3db8c3c592d8167d16710e.jpeg" title="image"><img alt="image" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/8/f/8f1dda810cb6cc5f9d3db8c3c592d8167d16710e_2_500x500.jpeg" width="500" /></a></div><br />
<small><em>Probably not.</em></small><p></p>
<p>In this article, we present the methodology and results of an investigation aiming to address this question. We start with some context about the logical components making up a validator, then proceed with some potential methods of identifying validators on the beacon P2P network. We then expand on our chosen methodology and finally present the results.</p>
<h2><a class="anchor" href="https://ethresear.ch#anatomy-of-a-validator-3" name="anatomy-of-a-validator-3"></a>Anatomy of a validator</h2>
<p>An Ethereum validator is a virtual entity that consists of a balance, public key and other properties on the beacon chain. They are roughly responsible for 4 things:</p>
<ol>
<li>Proposing new blocks</li>
<li>Voting on other block proposals (attesting)</li>
<li>Aggregating attestations</li>
<li>Slashing other validators in case they commit faults</li>
</ol>
<p>A <em>validator client</em> is the piece of software that executes these responsibilities for each of its registered validator keys (which can be many). But a validator client on its own cannot connect to the P2P beacon network to talk directly to other validators. Instead, it connects to an entity known as a <em>beacon node</em>, which is a standalone client that maintains the beacon chain and communicates with other beacon nodes.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/8/68536fc182f09a1eb2c1e4b89f380dd4aca9c326.jpeg" title="image"><img alt="image" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/6/8/68536fc182f09a1eb2c1e4b89f380dd4aca9c326_2_495x500.jpeg" width="495" /></a></div><br />
<small><em>Schematic of validator clients and a beacon node</em></small><p></p>
<p>Beacon nodes can have a number of validators attached to them that ranges from zero to thousands. In fact, <a href="https://medium.com/@grandine/grandine-0-4-1-released-fb98daef6d60" rel="noopener nofollow ugc">it’s been reported</a> that in some Ethereum testnets client developers have been running upwards of 50k validators on a single machine. This separation of concerns makes our investigation somewhat harder: a simple crawl of the P2P network might give us a good overview of the set of online beacon nodes in real time, but this is not representative of the overall validator client distribution at all. Before we address this problem, we’ll take a closer look at validator duties and their footprint on the network.</p>
<h2><a class="anchor" href="https://ethresear.ch#attestation-duties-and-committees-4" name="attestation-duties-and-committees-4"></a>Attestation duties and committees</h2>
<p>As mentioned above, one of the main responsibilities of a validator is voting on blocks by broadcasting <em>attestations</em>. These attestations express the view of a validator about which chain they think is correct. In more detail, they actually cast 2 different votes: one to express their view of the current head block, and one to help finalize past blocks. This is because Ethereum’s  consensus is a combination of <a href="https://arxiv.org/pdf/2003.03052" rel="noopener nofollow ugc">2 subprotocols</a>: LMD GHOST, a fork-choice rule, and a finality gadget called Casper FFG.</p>
<p>These duties are assigned randomly every epoch (with some <a href="https://github.com/ethereum/consensus-specs/blob/29f39487de964683bbe13d11c7c58b3fe13dca10/specs/phase0/validator.md#lookahead" rel="noopener nofollow ugc">lookahead</a>) with RANDAO as the source of randomness. Validators get assigned to one slot per epoch at which they have to cast their attestation, which is just a message with the votes that is signed over with the validator BLS private key. These votes are then to be packed and stored in the next beacon block. However, if <a href="https://beaconcha.in/charts/validators" rel="noopener nofollow ugc">all 1 million validators</a> were to attest for every block, the network would be flooded with messages, and the proposer that is supposed to pack these attestations into their block would have trouble verifying all of those signatures in time. This would make Ethereum’s design goal of low resource validation unfeasible.</p>
<p>To address these issues, the beacon network is subdivided into <em>committees</em>, which are subsets of the active validator set that distribute the overall workload. Committees have a minimum size of <span class="math">128</span> validators, and there are <span class="math">64</span> committees that are assigned per slot. But how is this achieved in practice? What network primitives do we require to enable such a logical separation?</p>
<h2><a class="anchor" href="https://ethresear.ch#attestation-subnets-5" name="attestation-subnets-5"></a>Attestation subnets</h2>
<p>The Ethereum consensus P2P network is built with <a href="https://github.com/libp2p/specs/tree/master/pubsub/gossipsub" rel="noopener nofollow ugc">GossipSub</a>, a scalable pubsub protocol running on libp2p. Being a pubsub protocol, GossipSub supports publish/subscribe patterns and the segmentation of networks into logical components called <em>topics</em> (aka P2P overlays)<em>.</em> These are the networking primitives that underpin beacon committees.</p>
<p>One example of a topic is the <a href="https://github.com/ethereum/consensus-specs/blob/29f39487de964683bbe13d11c7c58b3fe13dca10/specs/phase0/p2p-interface.md#beacon_block" rel="noopener nofollow ugc"><code>beacon_block</code></a> topic, which is a <em>global topic</em> on which new beacon blocks are broadcast. Every validator must subscribe to this topic in order to update their local view of the chain and perform their duties.</p>
<p>The attestation overlays look quite a bit different. For each committee, we derive a subnet ID based on the committee index (0-64). The topic for the respective subnets then becomes <a href="https://github.com/ethereum/consensus-specs/blob/29f39487de964683bbe13d11c7c58b3fe13dca10/specs/phase0/p2p-interface.md#beacon_attestation_subnet_id" rel="noopener nofollow ugc"><code>beacon_attestation_{subnet_id}</code></a>. Every validator knows their upcoming attestation duties at least 1 epoch ahead of time and can join the correct subnet in advance. When they have to make an attestation, they broadcast it on this subnet.</p>
<p>As mentioned before, these attestations are eventually supposed to make it into a beacon block. But since upcoming proposers might not be subscribed to these subnets, how does that work? This is where <em>attestation aggregators</em> come in. These are a subset of the beacon committees that are responsible for <em>aggregating</em> all of the attestations they see and broadcasting the aggregate attestations on the global <a href="https://github.com/ethereum/consensus-specs/blob/29f39487de964683bbe13d11c7c58b3fe13dca10/specs/phase0/p2p-interface.md#beacon_attestation_subnet_id" rel="noopener nofollow ugc"><code>beacon_aggregate_and_proof</code></a> topic. This topic is again a mandatory global topic that all validators will be subscribed to, thus providing a way for local unaggregated attestations to make it into the global view of the network. Per committee, there’s a target number of aggregators of <span class="math">16</span>.</p>
<h3><a class="anchor" href="https://ethresear.ch#subnet-types-6" name="subnet-types-6"></a>Subnet types</h3>
<p>These attestation subnets described above are ephemeral and directly tied to the validator duties. We call these <strong>short-lived</strong> attestation subnets. The problem with these ephemeral subnets is that they are not very robust, and could result in lost messages. To deal with this issue, the notion of a “<a href="https://github.com/ethereum/consensus-specs/issues/2749" rel="noopener nofollow ugc">subnet backbone</a>” was introduced.</p>
<p>This backbone consists of <strong>long-lived</strong>, persistent subnet subscriptions that are not tied to validator duties but rather a <a href="https://github.com/ethereum/consensus-specs/blob/29f39487de964683bbe13d11c7c58b3fe13dca10/specs/phase0/p2p-interface.md#attestation-subnet-subscription" rel="noopener nofollow ugc">deterministic function</a> of the beacon node’s unique ID and the current epoch. These long-lived subnets are maintained for <span class="math">256</span> epochs, or around 27 hours, and each beacon node has to subscribe to 2 of them. They are also advertised on the discovery layer, making it easier for beacon nodes with certain duties to find peers on the relevant subnets.</p>
<h2><a class="anchor" href="https://ethresear.ch#validator-footprints-7" name="validator-footprints-7"></a>Validator footprints</h2>
<p>Returning to the separation of the beacon node and validator clients, there’s now a clear footprint that validators leave on the beacon node’s network identity: their short-lived subnet subscriptions. This will be the core of our methodology.</p>
<h2><a class="anchor" href="https://ethresear.ch#methodology-8" name="methodology-8"></a>Methodology</h2>
<p>Generally, the beacon network consists of 3 domains:</p>
<ul>
<li>The discovery domain</li>
<li>The Req/Resp domain</li>
<li>The gossip domain</li>
</ul>
<p>Each of these domains provides some information about a beacon node.</p>
<h3><a class="anchor" href="https://ethresear.ch#long-lived-subnets-node-metadata-9" name="long-lived-subnets-node-metadata-9"></a>Long-lived subnets &amp; node metadata</h3>
<p>At the <strong>discovery layer</strong> (<a href="https://github.com/ethereum/devp2p/blob/5713591d0366da78a913a811c7502d9ca91d29a8/discv5/discv5.md" rel="noopener nofollow ugc">discv5</a>), a beacon node’s identity consists of an <a href="https://github.com/ethereum/consensus-specs/blob/29f39487de964683bbe13d11c7c58b3fe13dca10/specs/phase0/p2p-interface.md#enr-structure" rel="noopener nofollow ugc">ENR</a> with some additional metadata. This metadata can roughly be represented as the following object:</p>
<pre><code class="lang-js">{ 
	peer_id, 
	ip, 
	tcp_port, 
	udp_port, 
	attnets, // Important
	fork_digest, 
	next_fork_version, 
	next_fork_epoch 
}
</code></pre>
<p>This metadata helps other peers connect to peers that are relevant to them, indeed, one of the extra metadata fields are the (long-lived) attestation subnets that this node is subscribed to!</p>
<p>The <strong>Req/Resp domain</strong> is where the actual handshake happens. This is where nodes exchange <code>Status</code> messages that look like the following in order to establish a connection:</p>
<pre><code class="lang-js">(
  fork_digest: ForkDigest
  finalized_root: Root
  finalized_epoch: Epoch
  head_root: Root
  head_slot: Slot
)
</code></pre>
<p>The underlying protocol used for the Req/Resp domain is (again) libp2p. On the lower levels, additional information like <code>client_version</code> is also exchanged when connections are set up.</p>
<p>It is at this level that peers can also exchange <code>MetaData</code> objects to identify each other’s most up to date long-lived subnet subscriptions. The <a href="https://github.com/ethereum/consensus-specs/blob/29f39487de964683bbe13d11c7c58b3fe13dca10/specs/phase0/p2p-interface.md#metadata" rel="noopener nofollow ugc"><code>MetaData</code></a> object looks like this:</p>
<pre><code class="lang-js">(
  seq_number: uint64
  attnets: Bitvector[ATTESTATION_SUBNET_COUNT]
  ...
)
</code></pre>
<h3><a class="anchor" href="https://ethresear.ch#short-lived-subnets-10" name="short-lived-subnets-10"></a>Short-lived subnets</h3>
<p>So far, we’ve only seen how nodes exchange metadata and their long-lived subnet subscriptions, which tell us nothing about potential validators. For that, we need the short-lived subnets, which we can only collect on the gossip domain. Our initial strategy was doing just that:</p>
<ol>
<li>Listen to incoming topic subscription requests</li>
<li>Save and index them</li>
</ol>
<p>However, on an initial review of the data, we saw way too many beacon nodes that didn’t subscribe to any additional subnets besides their long-lived, mandatory subscriptions.</p>
<p>Our assumption was that in order to publish data on a gossipsub topic, one needed to be subscribed to it. It turns out that this is not the case, and many clients have different behaviour to minimize bandwidth and CPU usage. Rather than subscribing to the subnet directly, the peer finds other peers that are subscribed to the required subnet beforehand and shares the attestation with them. The subscribed peers make sure to verify and forward these attestations. Remember that in theory, only attestation aggregators need to be listening to all incoming attestations in order to do their jobs. This is exactly what was happening, and explains why we had so little short-lived subnet observations.</p>
<p>With this understanding, we could now tune our assumptions:</p>
<ul>
<li>For each subnet, there’s a target of <code>TARGET_AGGREGATORS_PER_COMMITTEE=16</code> aggregators per committee</li>
<li>This means that on average, there will only be <span class="math">16</span> validators per committee that will be subscribed to an additional short-lived subnet for the duration of an epoch</li>
<li>This results in a maximum of <span class="math">16 * 32 * 64 = 32768</span> useful observations per epoch</li>
</ul>
<p>With these assumptions in mind, we can start estimating validator counts.</p>
<h3><a class="anchor" href="https://ethresear.ch#estimating-validator-counts-11" name="estimating-validator-counts-11"></a>Estimating validator counts</h3>
<p>For each observation, we subtract the number of long-lived subnets <span class="math">S_l</span> from all subscribed subnets <span class="math">S_{all}</span> to arrive at the number of short-lived subnets <span class="math">S_s</span>:</p>
<div class="math">
S_s = S_{all} - S_l
</div>
<p>Since we know aggregators are subscribed to one additional subnet per epoch, <span class="math">S_s</span> will result in an estimated validator count for a certain beacon node in this epoch. Note that just one observation will not be enough to get an accurate estimate, because of the following reasons:</p>
<ul>
<li>It could be that a validator is not an aggregator for this epoch, and thus won’t subscribe to any subnets</li>
<li>There could be overlap between the long-lived and short-lived subnets</li>
</ul>
<p>Due to this reason, we continuously try to collect observations for each known beacon node per epoch, and save the maximum estimated validator counts. Note also that the ceiling for validator estimations is at <span class="math">64 - 2</span>, because that’s the maximum amount of short-lived subnets we can record. This is important! It means that for beacon nodes with more than <span class="math">62</span> validators, we can not estimate how many there are, and just record the ceiling. We want to highlight again that this is just an estimation and won’t be a very accurate representation of the total number of validators.</p>
<h2><a class="anchor" href="https://ethresear.ch#architecture-12" name="architecture-12"></a>Architecture</h2>
<p>In this section we’ll dive a bit deeper into the architecture. All the code for this is open source and can be found in this repository: <a class="inline-onebox" href="https://github.com/chainbound/valtrack" rel="noopener nofollow ugc">GitHub - chainbound/valtrack: An Ethereum validator crawler</a>. A lot of the crawler code is based on projects like <a href="https://github.com/probe-lab/hermes" rel="noopener nofollow ugc">Hermes</a> and <a href="https://github.com/migalabs/armiarma/" rel="noopener nofollow ugc">Armiarma</a>. An overview can be seen here:<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/8/e856490bf2dd28b100abeb8c0f37e50f389e882b.jpeg" title="image"><img alt="image" height="305" src="https://ethresear.ch/uploads/default/optimized/3X/e/8/e856490bf2dd28b100abeb8c0f37e50f389e882b_2_690x305.jpeg" width="690" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#crawler-13" name="crawler-13"></a>Crawler</h3>
<p>The crawler is the core component of the system. It will crawl the discv5 discovery DHT, find nodes that are on the correct network by looking at the metadata in their ENRs, and then try to connect with them. It will keep a local cache of known peers and try to reconnect every epoch to get updated observations.</p>
<p>We outline 2 types of events (observations): <code>PeerDiscoveryEvent</code> and <code>MetadataReceivedEvent</code>. The second one is most relevant and contains the following fields:</p>
<pre><code class="lang-go">type MetadataReceivedEvent struct {
	ENR               string          `json:"enr"`
	ID                string          `json:"id"`
	Multiaddr         string          `json:"multiaddr"`
	Epoch             int             `json:"epoch"`
	MetaData          *eth.MetaDataV1 `json:"metadata"`
	SubscribedSubnets []int64         `json:"subscribed_subnets"`
	ClientVersion     string          `json:"client_version"`
	CrawlerID         string          `json:"crawler_id"`
	CrawlerLoc        string          `json:"crawler_location"`
	Timestamp         int64           `json:"timestamp"` // Timestamp in UNIX milliseconds
}
</code></pre>
<p>Along with some metadata, this contains all of the fields required to apply the previously described methodology: <code>SubscribedSubnets</code> contains the actually subscribed subnets, obtained by listening on the GossipSub domain, and <code>MetaData</code> contains the peer’s long-lived subnets.</p>
<p>All of these events are then sent to a persistent message queue, where they are stored until they’re read by the consumer.</p>
<h3><a class="anchor" href="https://ethresear.ch#consumer-14" name="consumer-14"></a>Consumer</h3>
<p>The consumer turns the event logs into a stateful view of the network by implementing the methodology described above. It parses the short-lived subnets from the metadata events to get the estimated validator counts, and updates any existing entries in its stateful view. This stateful view is saved in a local sqlite database, which we expose over an API. The table schema roughly looks like this:</p>
<pre><code class="lang-sql">validator_tracker (
	peer_id TEXT PRIMARY KEY,
	enr TEXT,
	multiaddr TEXT,
	ip TEXT,
	port INTEGER,
	last_seen INTEGER,
	last_epoch INTEGER,
	client_version TEXT,
	possible_validator BOOLEAN,
	max_validator_count INTEGER,
	num_observations INTEGER,
	hostname TEXT,
	city TEXT,
	region TEXT,
	country TEXT,
	latitude REAL,
	longitude REAL,
	postal_code TEXT,
	asn TEXT,	
	asn_organization TEXT,
	asn_type TEXT
)
</code></pre>
<p>We then join this data together with an IP location dataset to provide more information about geographical distribution.</p>
<h2><a class="anchor" href="https://ethresear.ch#results-15" name="results-15"></a>Results</h2>
<p><a href="https://www.chainbound.io/" rel="noopener nofollow ugc">Chainbound</a> runs a <a class="inline-onebox" href="https://github.com/chainbound/valtrack" rel="noopener nofollow ugc">GitHub - chainbound/valtrack: An Ethereum validator crawler</a> deployment that pushes all data to Dune every 24 hours.</p>
<blockquote>
<p><img alt=":bulb:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/bulb.png?v=12" title=":bulb:" width="20" /> Dune table link: <a href="https://dune.com/data/dune.rig_ef.validator_metadata" rel="noopener nofollow ugc">https://dune.com/data/dune.rig_ef.validator_metadata</a>.</p>
</blockquote>
<p><em>This data has been stripped of sensitive information such as IP addresses and exact coordinates. However, it retains information like city, coordinates with a precision of a 10km radius, and ASN information.</em></p>
<p>An example dashboard leveraging this information can be seen <a href="https://chainbound.grafana.net/dashboard/snapshot/AmuaGRjfOrARoc7BWY9L43dD5jIgsgnf?orgId=1" rel="noopener nofollow ugc">here</a>.<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/5/e5a141037b8bcb98e8247ccb94f3daeeb2d143ce.jpeg" title="image"><img alt="image" height="360" src="https://ethresear.ch/uploads/default/optimized/3X/e/5/e5a141037b8bcb98e8247ccb94f3daeeb2d143ce_2_690x360.jpeg" width="690" /></a></div><p></p>
<p>We also store the individual event logs, like PeerDiscoveryEvent and MetadataReceivedEvent. These are available on demand by sending an email to <a href="mailto:admin@chainbound.io">admin@chainbound.io</a>.</p>
<h2><a class="anchor" href="https://ethresear.ch#limitations-16" name="limitations-16"></a>Limitations</h2>
<ul>
<li>The maximum number of validators we can estimate with this methodology per beacon node is 62, due to that being the maximum amount of short-lived subnet subscriptions. This will result in a significantly underreported total number of validators, but should still be able to provide a reasonable estimation of the geographical distribution.</li>
<li>We failed to gather any meaningful data on Teku nodes over the 30-day period, which could signify an error in our P2P implementation and impact the results.</li>
<li>These results will be skewed towards validators attached to beacon nodes that have opened P2P networking ports in their firewall, which will mostly be beacon nodes running on cloud providers. The reason for this is that our crawler can more easily connect to nodes that have exposed ports.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#references-17" name="references-17"></a>References</h2>
<ul>
<li><a class="inline-onebox" href="https://eth2book.info/capella/" rel="noopener nofollow ugc">Upgrading Ethereum</a></li>
<li><a class="inline-onebox" href="https://hackmd.io/@dmarz/ethereum_overlays" rel="noopener nofollow ugc">The Hitchhiker's Guide to P2P Overlays in Ethereum Consensus - HackMD</a></li>
<li><a class="inline-onebox" href="https://github.com/ethereum/consensus-specs/tree/dev" rel="noopener nofollow ugc">GitHub - ethereum/consensus-specs: Ethereum Proof-of-Stake Consensus Specifications</a></li>
</ul>
            <p><small>4 posts - 4 participants</small></p>
            <p><a href="https://ethresear.ch/t/estimating-validator-decentralization-using-p2p-data/19920">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 27 Jun 2024 08:33:30 +0000</pubDate>
</item>
<item>
<title>Presenting Klaster - rethinking chain abstraction</title>
<link>https://ethresear.ch/t/presenting-klaster-rethinking-chain-abstraction/19910</link>
<guid>https://ethresear.ch/t/presenting-klaster-rethinking-chain-abstraction/19910</guid>
<content:encoded><![CDATA[
<div> 关键词：Klaster、Interchain Transaction (iTx)、Transaction Commitment Layer、Smart Accounts、Cross-chain transaction flow

总结:
Klaster是一个旨在解决区块链碎片化问题的协议，通过引入网络节点（Klaster Nodes）作为用户和链之间的中介。核心概念包括iTx（跨链交易捆绑），它是一系列可能相互依赖的交易，跨越多个链。Klaster利用智能账户和ERC-4337 Entrypoint，通过经济激励建立可靠的节点网络，允许开发者构建链抽象应用，用户只需一次签名即可执行跨链事务。协议提供了一站式服务，简化复杂操作，如资产转移、交换等，提升了用户体验。Klaster正在测试阶段，未来将实现去中心化，增强网络可靠性。 <div>
<h1><a class="anchor" href="https://ethresear.ch#introduction-1" name="introduction-1"></a>Introduction</h1>
<p>We are witnessing an ever-growing list of new chains popping out, and attracting a high level of activity and transactions. Ethereum is also scaling nicely, and with the EIP-4844 it’s becoming increasingly cheaper to onboard as a user and start interacting with chains.</p>
<p>This introduces fragmentation, which in our opinion is here to stay especially in a world where there will be hundreds of chains, users will demand fragmentation to be solved for. If we build solutions that kind of aggregate different assets in some sort of “centralized” service only to make all chains look like one and make it easy to move across chains, then we haven’t accomplished much.</p>
<p>We propose a solution which abstracts away chains and solves for fragmentation by introducing <strong>Klaster</strong> - a network of nodes placed between the users and chains. This layer wraps multiple blockchain networks and makes it easy for users to execute complex transaction flows spanning across one or more chains - all of that approved by the single off-chain signature.</p>
<p>By introducing the Klaster Nodes as a generic execution network, and defining how cross-chain transactions are being bundled and approved, we hope to set the standard for building chain abstracted applications. This goes beyond just a simple balance abstraction - spend your funds from one chain by interacting on another chain. It provides a “full” chain abstraction by allowing any arbitrary flow to be defined and executed.</p>
<h1><a class="anchor" href="https://ethresear.ch#tldr-2" name="tldr-2"></a>TL;DR</h1>
<p>Klaster Protocol aims to position itself as a chain abstraction framework which allows dApps or Wallets to build complex cross-chain transaction bundles and let the users sign only once to execute these bundles across one or more blockchain networks.</p>
<p>We introduce two key concepts:</p>
<ul>
<li><strong>iTx bundles</strong>: series of (possibly dependent) transactions spanning across many chains</li>
<li><strong>Transaction Commitment Layer</strong>: network of nodes providing execution guarantees and offering orchestrated iTx execution across many blockchain networks</li>
</ul>
<p>Klaster Protocol leans on Smart Accounts and ERC-4337 EntryPoint and by introducing the economic incentives provides a reliable network of Klaster Nodes which anyone can use to build truly chain abstracted dApps while not sacrificing on the security, or taking the control from the user.</p>
<h1><a class="anchor" href="https://ethresear.ch#klaster-3" name="klaster-3"></a>Klaster</h1>
<p>Klaster provides an infrastructure for building chain abstracted apps. Klaster does this by introducing a network of Nodes, which act as a <strong>Transaction Commitment Layer</strong>. This layer is placed between the dApp and multiple blockchain networks, It talks to the outside world (users, dApps) via <strong>interchain transactions (iTx)</strong>.</p>
<p>Developers can use these primitives to:</p>
<ul>
<li>Build chain abstracted dApps (no switch network button)</li>
<li>Define complex flows involving multiple chains without having to think of the specifics of how the flow will get executed</li>
<li>Automate the execution of the dependent actions spanning across many chains</li>
<li>Onboard the users from different chains and ecosystems into their dApp with a single user signature</li>
</ul>
<p>Users on the other hand:</p>
<ul>
<li>Can interact with chain abstracted dApps using any wallet they prefer</li>
<li>Don’t have to care of where their funds are, the dApp will be able to spend their funds from other chains with a single user signature</li>
<li>Don’t have to “lock” their funds in order for the dApp to consume their funds</li>
<li>Can use any asset on any chain to pay for gas cost of the full iTx execution involving many transactions on different chains</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#core-concepts-4" name="core-concepts-4"></a>Core concepts</h2>
<p>At its core, Klaster leans on its unique approach of <strong>separating transaction signing from<br />
the transaction execution</strong>.</p>
<p>If we think about how the EOA is executing a transaction on an EVM - it’s all bundled in the same operation - sign &amp; execute happening simultaneously with the user having one EOA wallet popup and interacting with the chain/RPC.</p>
<p>A more advanced approach can be seen with the Account Abstraction (ERC-4337), where users can approve their UserOp and then hand it over to the Bundler for execution. This approach is still bounded to one single chain - the one where the user’s smart account is deployed.</p>
<p>Klaster Model breaks the boundaries of a single chain, and allows an account owner to approve a complex series of (possibly dependent) UserOps targeting different blockchain networks - with a single off-chain signature! This signature is then provided to the Klaster Node (what would be a bundler in AA), for orchestrating an execution across all the different chains.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/5/755b51c20b470de874fc70cf3589d99577681458.jpeg" title="photo_2024-06-26_14-25-17"><img alt="photo_2024-06-26_14-25-17" height="388" src="https://ethresear.ch/uploads/default/optimized/3X/7/5/755b51c20b470de874fc70cf3589d99577681458_2_690x388.jpeg" width="690" /></a></div><p></p>
<p>As seen from the illustration above, if the user wanted to bridge funds and then swap on the destination chain, they would usually execute two transactions, on two different applications (Bridge app &amp; then DEX app), while also having to pay for gas fees on two different chains.</p>
<p>By splitting the signature from the execution, Klaster is able to convert two actions into one <strong>iTx bundle</strong> and then execute them through the Klaster Node. Klaster node will figure out the ordering of transactions, and execute them as user intended, while also covering for execution fees.</p>
<h2><a class="anchor" href="https://ethresear.ch#interchain-transaction-itx-bundle-5" name="interchain-transaction-itx-bundle-5"></a>Interchain Transaction (iTx bundle)</h2>
<p><strong>Interchain Transaction (iTx)</strong> is the fundamental working unit used within the Klaster protocol. It’s a bundle of one or more blockchain transactions spanning across one or more blockchain networks. It fully describes what the user or the dApp is trying to achieve. One iTx, consisting of two transactions, might be: “bridge assets from chain A using some 3rd party bridge to chain B, and then swap bridged assets for something else on chain B”.</p>
<p>From the Klaster Protocol perspective, one iTx bundle is actually a Merkle Tree of all the UserOps as leaves, and is defined by its Merkle Root hash (iTx hash): <strong>one iTx bundle = one unique iTx hash</strong>.</p>
<p>Any on-chain interaction on any blockchain network can be converted to the UserOp and placed as a part of a bigger iTx Merkle Tree - meaning the iTx tree approach can be used to basically define any complex operation spanning across multiple blockchain networks provided that there’s at least some liquidity services connecting the chains.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/a/2a26e4bc6a55b451b319a567816c3f9fd11c8b5a.jpeg" title="photo_2024-06-26_14-27-23"><img alt="photo_2024-06-26_14-27-23" height="388" src="https://ethresear.ch/uploads/default/optimized/3X/2/a/2a26e4bc6a55b451b319a567816c3f9fd11c8b5a_2_690x388.jpeg" width="690" /></a></div><p></p>
<p>Transaction Commitment Layer takes unsigned iTx requests, and <strong>commits</strong> to execute them in the specific time frame - and therefore provides a reliable execution layer capable of executing the parts of the iTx on different blockchain networks. This involves strategically determining the optimal order for executing the individual transactions within the bundle. For instance, if a transaction on Polygon relies on assets being transferred from Ethereum first, the node will ensure that the Ethereum transfer is finalized before proceeding with the Polygon transaction.</p>
<h2><a class="anchor" href="https://ethresear.ch#high-level-protocol-overview-6" name="high-level-protocol-overview-6"></a>High Level Protocol Overview</h2>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/4/44e74558671473aa21b67bb54151e8dea1ce9070.jpeg" title="photo_2024-06-26_14-28-18"><img alt="photo_2024-06-26_14-28-18" height="388" src="https://ethresear.ch/uploads/default/optimized/3X/4/4/44e74558671473aa21b67bb54151e8dea1ce9070_2_690x388.jpeg" width="690" /></a></div><p></p>
<p>The following steps are involved for the user/dApp to interact with the Klaster Protocol:</p>
<ol>
<li>dApp defines a list of operations to be executed across one or many chains and bundle them together into the iTx</li>
<li>dApp asks the Klaster Network for a quote (fee) for executing an iTx</li>
<li>dApp receives back the iTx with included fee amount and cryptographic execution guarantees given by the Klaster Network</li>
<li>User signs the iTx by signing its root iTx hash and then broadcasts the signed iTx back to the Klaster Network</li>
</ol>
<p>Once the Klaster Network receives the signed iTx, it will charge the user upfront by pulling the fee amount as defined in the quote, and it will start processing the transactions from the iTx bundle, executing them on the different blockchain networks in the correct order. The specifics of how the fee is being calculated and charged upfront is outlined in the technical breakdown section.</p>
<h2><a class="anchor" href="https://ethresear.ch#chain-abstraction-vs-balance-abstraction-aave-example-7" name="chain-abstraction-vs-balance-abstraction-aave-example-7"></a>Chain Abstraction vs Balance Abstraction (AAVE example)</h2>
<p>While balance abstraction is a great step forward in solving for liquidity fragmentation, it’s not covering all bases. Let’s say we want to build a chain abstracted version of AAVE, where users can interact with the dApp not only by having the “balance” abstracted away (supply assets from one chain to AAVE deployed on another chain), but also having an <strong>AAVE “position” abstracted</strong> away which is a more dApp specific use-case.</p>
<p>For example, a user might have 100 USDC supplied on AAVE on Optimism, but they want to switch the position to Base, and supply USDC there, because of better rates. Or there’s a bot that wants to do this periodically, in the user’s name and with the user’s approval.</p>
<p>Right now, the user would have to unwind their position, find a bridge to use, move liquidity to Base and then resupply the USDC. This involves signing multiple transactions and switching between multiple frontends and blockchain networks / RPCs, not to mention also having some gas dust on these chains to be able to execute transactions in the first place. We believe this is unsustainable and there has to be a way of “standardizing” these interactions &amp; making life easier on the user facing side.</p>
<p>By using Klaster protocol, this complicated “position” rebalancing operation can be converted to one simple iTx bundle containing three UserOps:</p>
<ul>
<li>[<em>Optimism</em>] UserOp1: unwinds AAVE USDC position on Optimism</li>
<li>[<em>Optimism</em>] UserOp2: bridges 100 USDC to Base using some third party bridge (across bridge, for example)</li>
<li>[<em>Base</em>] UserOp3: supplies 100 USDC on AAVE</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/a/aaccd85ed18ac9bacdc8cfe3806eb872019b0363.jpeg" title="photo_2024-06-26_14-35-03"><img alt="photo_2024-06-26_14-35-03" height="390" src="https://ethresear.ch/uploads/default/optimized/3X/a/a/aaccd85ed18ac9bacdc8cfe3806eb872019b0363_2_690x390.jpeg" width="690" /></a></div><p></p>
<p>The only thing the user would have to do from their side is provide one signature for the iTx and the balance reposition would be handled by the Klaster Protocol automatically. No gas required on the destination chain. No different apps involved. One simple signature. And we bet that if the developers are provided with tools like Klaster, many more other interesting use-cases might emerge other than the one we’re describing here.</p>
<h1><a class="anchor" href="https://ethresear.ch#technical-breakdown-i-want-to-know-more-8" name="technical-breakdown-i-want-to-know-more-8"></a>Technical Breakdown (I want to know more)</h1>
<h2><a class="anchor" href="https://ethresear.ch#smart-accounts-itx-module-9" name="smart-accounts-itx-module-9"></a>Smart Accounts - iTx Module</h2>
<p>Using an iTx bundle in combination with Smart Contract Accounts allows for one very powerful feature to be implemented - and is there to help on the UX side: <strong>single signature iTx approvals</strong>.</p>
<p>Smart Account modular architecture allows for building a standardized ERC-7579 module which “understands” iTx bundles and can be installed on top of existing smart account wallets or used to initialize new wallets as the UserOp model allows for providing the wallet initialization data as a UserOp parameter.</p>
<p>A smart account owner can approve the whole iTx bundle of many chain transactions by only <strong>signing once</strong> - one off-chain signature of the iTx Merkle Root hash can be used to approve for executing all the transactions across many chains.</p>
<p>As mentioned earlier, the tree is defined by its Merkle root hash - iTx hash. The smart contract owner signs the iTx hash with a signer. This typically is an EOA which is a common owner of all the smart accounts across different chains where the assets are being bridged and consumed, and by providing one signature, all of these operations are immediately executable.</p>
<p>If the user doesn’t have a smart contract account on one or more blockchain networks - the accounts can be “lazy deployed” for the user - meaning, the iTx bundle can contain an operation which bridges some amount of funds to the “not yet created” account as the address of the smart account can be precomputed.</p>
<p>By having the iTx validation module as a standardized module - Klaster protocol remains neutral &amp; unopiniated - it can work with different smart account providers.</p>
<h2><a class="anchor" href="https://ethresear.ch#klaster-fees-node-selection-10" name="klaster-fees-node-selection-10"></a>Klaster Fees &amp; Node Selection</h2>
<p>The Klaster Transaction Commitment Layer consists of many Klaster Nodes - all of them being equal. Every Node is defined by its wallet address, and in order for nodes to join the network, they have to stake capital - this is how the nodes provide uptime &amp; execution guarantees.</p>
<p>Klaster Nodes are taking care of the following:</p>
<ol>
<li>Estimating iTx fees &amp; responding to quote requests</li>
<li>Committing to iTx execution (or rejecting the request)</li>
<li>Executing fully signed iTx (if previously committed to execution)</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#estimating-itx-fees-responding-to-quote-requests-11" name="estimating-itx-fees-responding-to-quote-requests-11"></a>Estimating iTx fees &amp; responding to quote requests</h3>
<p>When the user or the dApp asks the protocol for quotes, every node will estimate the total cost of executing the iTx on different chains. The node adds its own fee on top of the total cost, including the <strong>success execution tip</strong> (more on this in the “optimistic execution” chapter) and responds back with the full cost the user will have to pay in order for the node to do the job.</p>
<h3><a class="anchor" href="https://ethresear.ch#committing-to-itx-execution-or-rejecting-the-request-12" name="committing-to-itx-execution-or-rejecting-the-request-12"></a>Committing to iTx execution (or rejecting the request)</h3>
<p>The user or the dApp chooses the best received quote by taking into account the total execution cost offered by each of the nodes, and their reputation. The dApp then connects directly with the selected node, and asks for a commitment - a guarantee from the node that they are going to execute the iTx in full, provided that the user pays for what the node asks for.</p>
<p>The node commits to the iTx execution by</p>
<ol>
<li>Prepending the payment tx* in the list of the transactions in the iTx bundle</li>
<li>Signing the root iTx hash with its own private key - essentially binding itself to the execution of the iTx</li>
</ol>
<p>*<em>A payment transaction generated and prepended by the node transfers some liquid asset from the user’s account to the node wallet address. The asset is selected by the user and the amount is calculated by the node to cover for all the execution costs + the node fee. This means that the user can pay for the execution on any chain and in any asset supported by the node.</em></p>
<h3><a class="anchor" href="https://ethresear.ch#executing-fully-signed-itx-if-previously-committed-to-execution-13" name="executing-fully-signed-itx-if-previously-committed-to-execution-13"></a>Executing fully signed iTx (if previously committed to execution)</h3>
<p>Once the dApp receives the iTx which includes the payment transaction and the node commitment, the user is finally prompted to approve the full iTx bundle by signing the root iTx hash - essentially approving the execution of all the transactions contained in the bundle. The iTx bundle, whose root iTx hash has been signed by both the node (commitment) &amp; user (execution approval) is sent to back the selected node which:</p>
<ol>
<li>Verifies the iTx bundle integrity (calculates &amp; verifies merkle root)</li>
<li>Verifies the commitment signature (make sure the node really did commit to this iTx)</li>
<li>Verifies the user signature</li>
<li>Collects the payment from the user (the first transaction in the iTx bundle)</li>
<li>Once the payment is complete, proceeds to execute the rest of the operations by performing the optimistic execution algorithm</li>
</ol>
<p>If the node fails to execute the iTx bundle, the user can use the node commitment (node iTx signature) to initiate a slashing request and prove on-chain that the node actually promised to execute the iTx but failed to do so in a given timeframe.</p>
<h2><a class="anchor" href="https://ethresear.ch#meta-paymaster-and-multichain-gas-refunds-14" name="meta-paymaster-and-multichain-gas-refunds-14"></a>Meta Paymaster and Multichain Gas Refunds</h2>
<p>For the node to be fully operational, they have to own the native coin balance on their wallet address for every chain they support - in order to be able to pay for gas and execute UserOps as a part of iTx bundle. By accepting the upfront payment from the user in one token and one chain, and then executing the transactions and subsidizing gas on one or more chains, Klaster Node acts in a way as a Meta Paymaster.</p>
<p>The node executes UserOps contained in the iTx by routing them through the official ERC-4337 EntryPoint on different chains, and after receiving post-operation execution callbacks with the actual gas consumption data, the node will execute refunds for every processed UserOp, that is if actual UserOp cost (including the Klaster Node fee) was less than the maximum UserOp that was prepaid by the user. The process is illustrated below:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/c/9ca11c122eee71670ddf3e95a2ecee88cf427bcb.png" title="klaster-meta-paymaster-latest"><img alt="klaster-meta-paymaster-latest" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/9/c/9ca11c122eee71670ddf3e95a2ecee88cf427bcb_2_569x500.png" width="569" /></a></div><p></p>
<p>By relying on the official ERC-4337 EntryPoint for UserOp routing, the Klaster Protocol is staying compliant with the AA space, since most of the AA wallets today choose to trust and give control to one EntryPoint contract. Any existing AA wallet could technically activate the Klaster iTx module and gain cross-chain capabilities.</p>
<h2><a class="anchor" href="https://ethresear.ch#optimistic-itx-execution-15" name="optimistic-itx-execution-15"></a>Optimistic iTx Execution</h2>
<p>Klaster Node is incentivized to execute UserOps from a given iTx bundle in the right order of events, without the user having to explicitly provide the order of events.</p>
<p>The right order of events is implicitly deduced by the Klaster Node, by repeatedly simulating every UserOp, between the timestamp deadlines set by the user when defining UserOp, and waiting for the simulation to yield 0 <em>REVERT</em> opcodes in the simulated execution breakdown. Once this happens, Klaster Node “knows” all the preconditions have been met (whatever they may be) and will proceed to execute the UserOp as this maxmizes the profits for the Klaster Node.</p>
<p>In our AAVE example from above, Klaster Node will wait for the bridge action to complete without having to be aware of which bridge is being used and what the estimated bridge time to destination might be. It’s not even aware of the context of any UserOp or the potential dependencies between those. The execution flow would look like this:</p>
<ol start="0">
<li>
<p>The Node executes the Payment UserOp (at index 0 in the list of UserOps). That way the Node charges for the full execution of all the other UserOps upfront and can proceed with the next steps</p>
</li>
<li>
<p>The Node “sees” that out of three UserOps (<strong>unwind, bridge, supply</strong>), the only one with 0 REVERTs is the <strong>unwind</strong> operation, and it proceeds to execute the UserOp successfully (on Optimism)</p>
</li>
<li>
<p>Afterward, another operation that yields 0 REVERTs is the bridge operation as the funds are now there to be bridged (unwinded position), so it proceeds to execute <strong>bridge</strong> action (on Optimism)</p>
</li>
<li>
<p>Finally, once the funds arrive at the user’s dest chain smart account (whenever that may be, depending on the 3rd party bridge being used), the <strong>supply</strong> operation is executed which marks the full iTx execution as complete.</p>
</li>
</ol>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/b/bb781b0246065508b7a832ee5060408e61d20a87.jpeg" title="photo_2024-06-26_14-45-59"><img alt="photo_2024-06-26_14-45-59" height="388" src="https://ethresear.ch/uploads/default/optimized/3X/b/b/bb781b0246065508b7a832ee5060408e61d20a87_2_690x388.jpeg" width="690" /></a></div><p></p>
<p>By having this generic approach of not being aware of the context, iTx bundles can express pretty much any complex cross-chain flow. To incentivize the node to wait for the simulation success (0 REVERTs), but then also to execute the UserOp <strong>as soon as 0 REVERT is detected,</strong> Klaster fee will include the <strong>diminishing success tip</strong>. This fee can be collected by the Node only if the UserOp was executed with 0 REVERT status and the tip is fading to 0 as the UserOp execution moment is closing to the upper bound execution timestamp.</p>
<p><em>It is still possible for some of the UserOps to fail, for example, 3rd party bridge not working properly. In that case - the node has fulfilled its obligation, as it’s recorded on-chain that the node “attempted” to execute the UserOp, although the funds haven’t reached the destination chain. In that case, the node is protected from slashing, while the user experienced a partially executed iTx. The funds are still owned by the user, and have remained on their wallet on one of the chains where the UserOp failed.</em></p>
<h1><a class="anchor" href="https://ethresear.ch#integration-16" name="integration-16"></a>Integration</h1>
<p>dApp/Wallet developers will soon have access to the SDK, which in turn will allow for building chain abstracted applications much more efficiently, while staying neutral and not locking the developer to having to use any specific technology.</p>
<p>The developers are free to use any bridges or 3rd party services as a part of the iTx bundle - depending on the level of security/speed they require, and to rely on different AA wallet providers as the smart account wallets used behind the scenes.</p>
<p>On the user side, they have to sign once, and see their cross-chain intent being executed step by step without having to do any other action or even own gas funds on any of the chains they interact with.</p>
<p>This is how we see it developed further and how dApps might integrate the SDK in order to provide cross-chain experience to the end user:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/5/65e8e1fa54439727f6d9a820b1d86e740f228025.jpeg" title="photo_2024-06-18_13-18-43"><img alt="photo_2024-06-18_13-18-43" height="315" src="https://ethresear.ch/uploads/default/optimized/3X/6/5/65e8e1fa54439727f6d9a820b1d86e740f228025_2_690x315.jpeg" width="690" /></a></div><p></p>
<h1><a class="anchor" href="https://ethresear.ch#demo-use-cases-17" name="demo-use-cases-17"></a>Demo &amp; Use Cases</h1>
<p>At the moment, we’re building a chain abstracted AAVE dApp - to showcase what the protocol can do in terms of UX improvements.</p>
<p>The frontend will only contain two buttons: “supply” &amp; “borrow” without specifying the chains. When executing borrow or supply, user’s funds will be routed to any chain where the AAVE market’s rates are most favorable, regardless of the fact which chain the user’s funds are on.</p>
<p>If the user wants to rebalance the existing position, again, it’s a one-click interaction for the user, but in the background, iTx is being executed by the Klaster Nodes.</p>
<p>Some other interesting use cases:</p>
<ul>
<li>streamlined checkout flows</li>
<li>easier onboarding to the SocialFi L2/L3 apps, as Klaster protocol works with AA by default, and many of these apps choose to have embedded wallets generated for the users behind the scenes</li>
<li>building chain abstracted flavors of dapps that are natively multichain (DEXs, lending markets, NFT marketplaces)</li>
<li>single-chain dApps can use the Klaster Stack to streamline onboarding flows, attracting users from various chains. With Klaster Stack, users can interact with the dApp in just one click, regardless of their original blockchain</li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#faq-18" name="faq-18"></a>FAQ</h1>
<p><strong>Q: Is Klaster a Blockchain Network?</strong><br />
A: No.</p>
<p><strong>Q: What’s the current development status?</strong></p>
<p>A: Centralized Klaster Node including the SDK and the docs is in the testing phase and will be launched very soon. The decentralization phase including the slashing and multichain staking which in turn makes the network more reliable will most likely be rolled out later this year.</p>
<p><strong>Q: What are the dangers of using Klaster Protocol?</strong></p>
<p>A: Dangers are mostly related to the impaired UX.</p>
<p>For example, malicous Klaster Node can refuse to process an iTx bundle in full - they only execute the Payment UserOp part of the iTx bundle. The user can still replay their UserOps manually and achieve the same effect but will have to pay for gas execution themselves. Nodes on the other hand get slashed in the decentralized model because the user can submit a proof of Node commiting to execute the iTx but failing to do so in a given timeframe - which is fully verifiable on-chain. As the AA wallets are used behind the scenes, the user is in full control of their funds, and the security is reduced to security of bridges used as an intermediary steps to move assets between chains. Klaster’s iTx-enabled AA wallet module is pending audits and the reports will be shared soon.</p>
<p><strong>Q: Can I run my own node, and what are the advantages of running a node?</strong></p>
<p>A: Klaster Protocol will host its own public node, with the implementation publicly available for everyone to take a look and verify the inner workings of the Node itself. While the initial version of the protocol is not decentralized in a sense of having a p2p networking implemented between public Klaster nodes, anyone can still choose to run their own Klaster Node either for their own purposes (only handling one single dApp) or even providing this node for others to connect to.</p>
<p>New chains can spin up their own Klaster node to easily onboard users from other chains.</p>
<p>Klaster Node if operational earns a % of the total gas processed and is another revenue stream for Node operators. To set up a node, one needs to have a wallet connected to the node, and funded with native coin on every chain which the Node operator decides to support.</p>
<p><strong>Q: How does Klaster compare to other chain abstraction solutions?</strong></p>
<p>A: For a start, we think we have a unique approach here in being highly focused on the UX part. We’re trying to stay as generic and as neutral as possible, and we’ve developed something that can be used today to fix the UX in some ways. Comparing to some other approaches we see being built in this space, Klaster’s main difference is that Klaster doesn’t work with liquidity nor does it require the Node operators to provide liquidity - meaning it’s easier to run the network and gain an initial base of Node operators. It doesn’t try to be “one solution fits all” which hides away blockchains completely, but rather a framework where, given the fact that the cross-chain action details are known upfront - it enables developers to easily define and build the action, and for the user to sign once and see the effects happening on different chains.</p>
<p><strong>Q: Where does Klaster Protocol fit in the CAKE framework?</strong></p>
<p>A: According to the CAKE Layer definitions, we’d say Klaster comes somewhere in the Settlement Layer (Execution part).</p>
<p><strong>Q: Is Klaster Protocol a bridge?</strong></p>
<p>A: Not really. Klaster Protocol can <em>wrap</em> bridges and other services to create a true cross-chain experience by having bridge action only there as a one step of the more comple iTx interaction.</p>
<p><strong>Q: I want to know more about the slashing process. Why do the Nodes have to stake capital, and how does slashing work?</strong></p>
<p>A: Klaster Nodes have to execute iTx bundles if they previously “promised” to the user they will do so. There has to be a way of punishing the Node for not doing their job - or even worse, collecting the fee payment from the user but never executing their desired intent. To make this possible, Klaster Nodes have to stake capital in order to be accepted by the network and allowed to execute iTx bundles on user’s behalf.</p>
<p>Every UserOp contains lower and upper bound timestamps, and the interval between these are when the UserOp is considered valid and can be executed on-chain. When the Node builds a full iTx tree, and signs the root iTx hash with their private key - we say the Node is “commited” to the iTx. The user has received the full quote including the Node commitment, and can use this commitment to initiate a slashing procedure if the nonce of the user’s smart account was not increased by one in the given timeframe, on any chain where their UserOp was <strong>not executed</strong>.</p>
<p><strong>Q: Is Klaster Protocol actually an Intent Solver network?</strong></p>
<p>A: Not really. Intents mean the user describes the end-result state and <em>someone somehow</em> finds the solution to the steps (txs) required to achieve the desired outcome. Klaster takes a completely opposite approach. The design space of the intent solvers is just too big and solving for all cases using intents is simply too complicated. We say - let’s make the system more exact, in a sense that, we assume that the developers of either dApps or wallets will always know upfront what exactly they want to achieve - and then let’s give them tools and means of how to express this interaction (iTx bundle) while making it easy for users to approve and execute these iTx bundles.</p>
<p>Klaster Protocol though is a great tool for Intent Solvers to express &amp; execute their “paths of execution” once they solve for some specific user’s request.</p>
<p><strong>Q: What’s the role of AA Wallets in the Klaster Protocol?</strong></p>
<p>A: AA Wallet is the only viable option for Klaster Protocol to work. Since we need to be able to have the user authorize many actions with only one signature - the only possibility for this to work is to actually use programmable smart contract wallets.</p>
<p><strong>Q: How is the Node protected from users? How are the users protected from the Node?</strong></p>
<p>A: The Node charges for its service fee plus all the other execution gas costs upfront. This way, the node might overcharge for the gas spendings, but the user will still get charged fairly if the actual gas spent was lower than what the node calculated. The Node will not commit to execute the iTx if the iTx looks risky - too short timespans for the UserOp execution, or the UserOp execution window which starts far away in the future (gas price spike risks).</p>
<p>The user is protected from the Node by being the only owner of the AA Wallet which used to execute iTx steps. Even if the Klaster Network dies completely, the user can still access and manage funds. The Klaster Node can only do what the user explicitly signs &amp; approves.</p>
<p><strong>Q: Does the user need to own the funds on the AA Wallet to interact with the Klaster Protocol in the first place?</strong></p>
<p>A: Unfortunately yes. If the user’s coming with an EOA wallet and assets are held by this EOA, the user will have to execute at least one EOA transaction and move funds from this wallet to an iTx enabled AA wallet to be able to use Klaster for chain abstraction / gas abstraction purposes. Luckily, the EIP-7702 which is confirmed will improve this flow substantially.</p>
            <p><small>3 posts - 2 participants</small></p>
            <p><a href="https://ethresear.ch/t/presenting-klaster-rethinking-chain-abstraction/19910">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 26 Jun 2024 13:13:35 +0000</pubDate>
</item>
<item>
<title>Pricing Gas Fee Derivatives</title>
<link>https://ethresear.ch/t/pricing-gas-fee-derivatives/19898</link>
<guid>https://ethresear.ch/t/pricing-gas-fee-derivatives/19898</guid>
<content:encoded><![CDATA[
<p><em>Thanks to Nethermind, <a class="mention" href="https://ethresear.ch/u/tkstanczak">@tkstanczak</a>, <a class="mention" href="https://ethresear.ch/u/swapnilraj">@swapnilraj</a> , <a class="mention" href="https://ethresear.ch/u/dapplion">@dapplion</a>, Martin Koppelmann and <a class="mention" href="https://ethresear.ch/u/drewvanderwerff">@DrewVanderWerff</a> for discussion, feedback and review.</em></p>
<p><strong>This is the first instalment in a series of posts where I will outline a methodology for understanding and pricing gas derivatives. The following approach for pricing will be valuable for gas hedging and can also be applied to develop a subscription model for Ethereum.</strong></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/0/2075264e1980cd984ec67a32af4bbe1689af4874.jpeg" title="There-Will-Be-Blood1-ezgif.com-webp-to-jpg-converter"><img alt="There-Will-Be-Blood1-ezgif.com-webp-to-jpg-converter" height="460" src="https://ethresear.ch/uploads/default/optimized/3X/2/0/2075264e1980cd984ec67a32af4bbe1689af4874_2_690x460.jpeg" width="690" /></a></div><p></p>
<p><em>There Will Be Blood” (2007) - are we going to be unwitting extras in the ‘digital oil’ sequel?</em></p>
<p><strong>TLDR:</strong> I show how a two-factor model can be used to price base fee options, of both European and American type. A developed gas derivatives market would be highly beneficial for participants looking to hedge against volatile operational expenses on gas or for those aiming to speculate on future gas fee trends.</p>
<p><strong>Why Price Base Fee Derivatives?</strong></p>
<p>Gas expenditure is a substantial portion of operational costs within blockchain ecosystems. Whether it involves L2 sequencers committing transactions to L1, the running of a DeFi protocols keeper, interacting with oracle contracts, rebalancing liquidity on platforms like Uniswap, verifying proofs, or conducting arbitrage, gas fees are an unpredictable expense. This inherent volatility poses challenges for financial planning and budgeting in blockchain operations. Gas hedging, analogous to its counterpart in traditional financial markets, provides a mechanism to manage and mitigate this uncertainty.</p>
<p>By purchasing gas derivatives, stakeholders can secure current gas fee levels for future transactions, effectively insuring against unforeseen spikes in gas prices. Additionally, with a strike price of zero on a call option, one can fully prepay for gas, paving the way for a ‘gas subscription’ model on Ethereum—assuming a delivery mechanism is established. This research could be particularly useful for pre-confirmations, where blockspace is purchased in advance.</p>
<p>The following post breaks down the importance of understanding base fees, examining their volatility, and proposing a detailed model for pricing base fee derivatives. The first section delves into the calculation of base fees, while the second outlines their structure. In the third section, a model incorporating both deterministic and stochastic components is detailed to simulate base fees using a Monte Carlo process. The fourth and final section explains how to use these Monte Carlo generated paths to price base fee options, including both European and American types. Use cases of this research include participants that want to examine the fair value of a base fee option, a task helpful for those interacting with derivative protocols like Oiler’s Pitch lake <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4123018" rel="noopener nofollow ugc">[1]</a>.</p>
<p>This post is the first in a series on the topic, aimed at sparking interest among both Ethereum researchers and traders. My goal is to engage quants in the conversation around Ethereum infrastructure.</p>
<p><strong>Understanding Base Fees</strong></p>
<p>Before pricing base fees, we should understand its constituent parts. EIP-1559, implemented in the London hard fork of Ethereum in August 2021, introduced a significant overhaul to the transaction fee mechanism on the Ethereum network. The proposal aimed to improve the predictability and efficiency of transaction fees, addressing several issues inherent in the previous auction-based system. Under EIP-1559, each block has a base fee, which is dynamically adjusted according to network congestion. When demand for block space increases, the base fee rises, and when demand decreases, the base fee falls. This mechanism helps to stabilize transaction fees and makes them more predictable for users.</p>
<p>The specific adjustment rule proposed in the EIP-1559 spec computes the base fee <span class="math">BF_{\text{cur}}</span> for the current block from the base fee <span class="math">BF_{\text{pred}}</span> and size <span class="math">s_{\text{pred}}</span> of the predecessor block using the following formula, where <span class="math">s_{\text{target}}</span> denotes the target block size:</p>
<div class="math">
BF_{\text{cur}} := BF_{\text{pred}} \cdot \left( 1 + \frac{1}{8} \cdot \frac{s_{\text{pred}} - s_{\text{target}}}{s_{\text{target}}} \right)

</div>
<p>In short, the next base fee is adjusted by a percentage that equals one-eighth of the difference to the target percentage - meaning base fees are within the bounds of 12.5% higher or lower than the previous block. As is visible below when comparing gas usage, full blocks of 30M are most frequent, with gas usage of just below 13M second most frequent.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/3/f3ef180d0b94b634c3727e72c56bcf12cb7ccbfa.png" title="Histogram"><img alt="Histogram" height="500" src="https://ethresear.ch/uploads/default/original/3X/f/3/f3ef180d0b94b634c3727e72c56bcf12cb7ccbfa.png" width="657" /></a></div><p></p>
<p><strong>Those Are Some Large Percentage Changes… Why Are Base Fees So Volatile?</strong></p>
<p>Several factors contribute to the high volatility of base fees, with the non-storability of base fees and the limited block-space supply being the most significant. Since block-space has a strict limit to 30M gas, which cannot be stored or transferred over time, supply in each time period is fixed, while demand can fluctuate based on usage needs. Base fees for transactions is thus demand inelastic. Consequently, during periods of low demand, the base fee remains relatively stable. However, during peak times, the relative insensitivity of demand to price changes can lead to significant volatility in short-term base fee prices—‘Jumps’. This situation is similar to the electricity market, where demand remains high regardless of price, causing extreme price volatility during peak usage. In the blockchain context, the necessity of paying the base fee to conduct transactions ensures somewhat steady demand, even as prices fluctuate dramatically.</p>
<p>Additionally, since base fees are influenced by the previous base fee, a block with high demand for blockspace—such as one resulting from the deployment of a large simultaneous smart-contract architecture—will primarily impact users in the following block. Users pay for blockspace based on the previous blocks usage, not the current one. Consequently, in the short term, a deployer will not always experience the negative externalities of increasing gas fees for near-future blockchain users. This creates a scenario where users are indifferent to increasing base fees by the cap of 12.5%. Current discussions, as well researched by SMG <a href="https://www.mechanism.org/spec/04" rel="noopener nofollow ugc">[2]</a>, suggest to minimise this externality, and subsequent short-term volatility by modifying the denominator to a higher value. The change would reduce sensitivity to randomness in gas usage and better align the process with underlying trends, improving stability and predictability.</p>
<p><strong>Characteristics of Base Fees</strong></p>
<p>Base fees can be simulated through understanding the structure of gas usage, before feeding the resulting parameters into the <span class="math">BF_{\text{cur}}</span> equation to find the base fees, or observing base fees themself. The model I propose focuses on the latter, as a result of my focus being on hourly averaged base fees for use in 1 day plus dated options, and a focus on gas usage would also mean accounting for variable gas limits. The model I am proposing is very flexible, and allow us to simultaneously include trends, seasonality, mean reversion, volatility and jumps.</p>
<p><strong>Overarching Trend</strong></p>
<p>Unlike electricity, wherein supply can vary, base fees relate directly to the demand side usage of a product, a blockchain. When a new EIP is passed as to change the structure of base fees, or if gas usage dramatically decreases. Unlike electricity, trends in gas usage are far shorter and dissimilar to one another, trends should thus be likened to regimes. For example, one regime may be a simple horizontal linear trend in periods of stable usage,  where another is an exponential trend downwards as a result of blobs being recently introduced.</p>
<p><strong>Seasonality</strong></p>
<p>Base Fee demand is heavily influenced by varying usage of economic and business activities of agents on the underlying blockchain. Different kinds of seasonality appear in the data; intra-daily and weekly. As it is usual in this type of research, I assume that seasonality is generated by deterministic factors and since I use the average hourly prices.</p>
<p><strong>Mean-reversion</strong></p>
<p>In the short term, during periods of high demand, base fees spike, discouraging excessive gas usage, which in turn reduces congestion and drives fees back down. Conversely, during low demand periods, lower fees encourage more transactions, increasing congestion and pushing fees back up. This cyclical nature of congestion creates a mean-reverting behaviour in base fees. Essentially, despite short-term fluctuations, base fees tend to stabilize around an average level, influenced by the balance of demand and network capacity.</p>
<p><strong>Jumps and volatility</strong></p>
<p>By simple eye inspection of base fees over time, there is clear existence of important jumps in the behaviour of base fees, as a result of sequential filling of blocks. One of the characteristics of evolution of these jumps is that the base fees do not stay in the new level, to which it jumps, but revert to the previous level rapidly. Such a behaviour can be captured by introducing a jump-diffusion component to a simulation model.</p>
<h2><a class="anchor" href="https://ethresear.ch#pricing-base-fees-1" name="pricing-base-fees-1"></a><strong>Pricing Base Fees</strong></h2>
<p>How should one price option premiums? Determining the appropriate pricing model for gas fees involves understanding their nature and selecting an appropriate financial framework. Gas fees could be treated as equities, interest rates, or commodities, each with distinct modelling approaches. Popular analytical models like the Black-Scholes closed-form model, often used for equities, offer theoretical insights into price movements and volatility - but in the case of base fees, the underlying assumption of normality in returns would be too naive, and the mean reversion and seasonality present in base fees wouldn’t be accounted for. Alternatively, numerical methods such as finite difference and Monte Carlo simulations would provide far more flexible and robust techniques for capturing the stochastic and path dependent nature of base fees. I therefore opt for the Monte Carlo simulation approach.</p>
<h2><a class="anchor" href="https://ethresear.ch#model-specification-and-estimation-2" name="model-specification-and-estimation-2"></a><strong>Model Specification and Estimation</strong></h2>
<p><strong>Exponential Trend Examination</strong></p>
<p>I first assess the presence of an exponential trend by computing the weekly statistics of the mean and standard deviation. If the spot price series exhibits an exponential trend, then the means and standard deviations, computed over time periods, should be correlated with a statistically significant slope.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/f/ffd93a4891dbf0193c6c5b3a00494d51f2569a4a.jpeg" title="Figure_1"><img alt="Figure_1" height="449" src="https://ethresear.ch/uploads/default/optimized/3X/f/f/ffd93a4891dbf0193c6c5b3a00494d51f2569a4a_2_690x449.jpeg" width="690" /></a></div><p></p>
<p>Demonstrated by a p-value close to zero (4.43e-18), an exponential trend is evident in the data. When base fees are high, base fees are volatile. Consequently, to simplify modelling and work with linear trends, I from hereon out use the logarithm of the base fee.</p>
<h3><a class="anchor" href="https://ethresear.ch#deterministic-model-specification-3" name="deterministic-model-specification-3"></a><strong>Deterministic Model Specification</strong></h3>
<p>We have seen in the previous section that a reasonable model for base fee prices should allow for the existence of deterministic seasonality, the possibility of mean-reversion, seasonality jumps, and volatility (randomness). Therefore, I propose a model that simultaneously incorporates all these factors in a flexible way.</p>
<p>The combined long-term model can be written as the composite of a deterministic component <span class="math">f(t)</span> and a stochastic component <span class="math">X_t</span>:</p>
<div class="math">
\log(BF_t) = f(t) + X_t
</div>
<p><strong>Estimation of Deterministic Component</strong> <span class="math">f(t)</span></p>
<p>The deterministic component  <span class="math">f(t)</span> is given by the sum of piecewise regime-based quadratic polynomial trends and sinusoidal functions corresponding to different harmonics and periods:</p>
<div class="math">
f(t) = \sum_{i=1}^{m} \mathbb{I}{\{t \in R_i\}} \left( \gamma{i,0} + \gamma_{i,1} t + \gamma_{i,2} t^2 \right) + \beta_1 \sin\left(\frac{2\pi t}{24}\right) + \beta_2 \cos\left(\frac{2\pi t}{24}\right) \\
 + \beta_3 \sin\left(\frac{4\pi t}{24}\right) + \beta_4 \cos\left(\frac{4\pi t}{24}\right) + \beta_5 \sin\left(\frac{8\pi t}{24}\right) + \beta_6 \cos\left(\frac{8\pi t}{24}\right) \\
 + \beta_7 \sin\left(\frac{2\pi t}{168}\right) + \beta_8 \cos\left(\frac{2\pi t}{168}\right) + \beta_9 \sin\left(\frac{4\pi t}{168}\right) + \beta_{10} \cos\left(\frac{4\pi t}{168}\right) \\
 + \beta_{11} \sin\left(\frac{8\pi t}{168}\right) + \beta_{12} \cos\left(\frac{8\pi t}{168}\right) + \xi_t
</div>
<p>Where:</p>
<ul>
<li><span class="math">\log BF_t</span> is the logarithm of the base fees per gasat time (t).</li>
<li><span class="math">t</span> is the time in hours since the start of the sample.</li>
<li><span class="math">\beta_1</span> and <span class="math">\beta_2</span> are the coefficients for the fundamental daily seasonal components (1 day period).</li>
<li><span class="math">\beta_3</span> and <span class="math">\beta_4</span>  are the coefficients for the first harmonic daily seasonal components (1 day period).</li>
<li><span class="math">\beta_5</span> and <span class="math">\beta_6</span> are the coefficients for the second harmonic daily seasonal components (1 day period).</li>
<li><span class="math">\beta_7</span> and <span class="math">\beta_8</span> are the coefficients for the fundamental weekly seasonal components (7 day period).</li>
<li><span class="math">\beta_9</span> and <span class="math">\beta_{10}</span>  are the coefficients for the first harmonic weekly seasonal components (7 day period).</li>
<li><span class="math">\beta_{11}</span> and <span class="math">\beta_{12}</span> are the coefficients for the second harmonic weekly seasonal components (7 day period).</li>
<li><span class="math">\mathbb{I}_{\{t \in R_i\}}</span> is an indicator function that equals 1 if <span class="math">t</span> is within regime <span class="math">R_i</span> and 0 otherwise.</li>
<li><span class="math">\gamma_{i,0}</span>, <span class="math">\gamma_{i,1}</span>, and <span class="math">\gamma_{i,2}</span> are the coefficients for the piecewise polynomial trend within regime <span class="math">R_i</span>.</li>
<li><span class="math">\xi_t</span> is the error term.</li>
</ul>
<p>For each regime <span class="math">R_i</span>, I fit a quadratic model of the form:</p>
<div class="math">
z_i(t) = \gamma_{i,0} + \gamma_{i,1} t + \gamma_{i,2} t^2
</div>
<p>To discover the boundaries of each regime, I utilise binary segmentation for detection of change points within the time series. This technique employs a piecewise model, identifying changes based on the L2 norm (Euclidean distance). Initially, the algorithm treats the entire time series as a single segment, searching for a point that maximises the cost function by minimising the residual sum of squares. When a significant change point is identified, the segment is split, and the algorithm recursively continues this process until no further significant change points are found.</p>
<p>In our dataset of roughly two years, and discovered partially heuristically, I identify 16 change points, 17 regimes, with an initial regime immediately after EIP-1559’s release. This finding indicates that the underlying trend in base fees shifts approximately every half to two months. Such shifts are likely influenced by market events such as changes in market sentiment, or other critical factors, all of which could warrant their own focused research to fully understand their cause. I then discover the parameters <span class="math">\gamma_{i,0}</span>, <span class="math">\gamma_{i,1}</span>, <span class="math">\gamma_{i,2}</span> for each regime <span class="math">R_i</span>, using the least squares method. This involves minimising the sum of the squared differences between the observed values <span class="math">BF_t</span> and the predicted values <span class="math">z_i(t)</span> within each regime <span class="math">R_i</span>:</p>
<div class="math">
\min_{\gamma_{i,0}, \gamma_{i,1}, \gamma_{i,2}} \sum_{t \in R_i} \left( BF_t - (\gamma_{i,0} + \gamma_{i,1} t + \gamma_{i,2} t^2) \right)^2
</div>
<p>To solve this minimisation problem, I set up the following normal equations by taking partial derivatives with respect to each parameter and setting them to zero:</p>
<div class="math">
\frac{\partial}{\partial \hat{\gamma}_{i,0}} \sum{t \in R_i} \left( y_t - (\hat{\gamma_{i,0}} + \hat{\gamma}_{i,1}) + \hat{\gamma}_{i,2} t^2) \right)^2 = 0 \\
\frac{\partial}{\partial \hat{\gamma}_{i,1}} \sum_{t \in R_i} \left( y_t - (\hat{\gamma}_{i,0} + \hat{\gamma}_{i,1} t + \hat{\gamma}_{i,2} t^2) \right)^2 = 0 \\
\frac{\partial}{\partial \hat{\gamma}_{i,2}} \sum_{t \in R_i} \left( y_t - (\hat{\gamma}_{i,0} + \hat{\gamma}_{i,1} t + \hat{\gamma}_{i,2} t^2) \right)^2 = 0
</div>
<p>Solving these equations yields the least squares estimates for <span class="math">\hat{\gamma}_{i,0}</span><em>, <span class="math">\hat{\gamma}_{i,1}</span></em>, <span class="math">\hat{\gamma}_{i,2}</span>  for each regime <span class="math">R_i</span>. The below figure displays the resulting regimes and trend curves. The regression analysis has a low <span class="math">R^2</span> for the majority of regimes, indicating that the trends are well-fitted to the data. Notably, the current regime, characterised by the implementation of EIP-4844, differs markedly from the previous two regimes, as base fees dramatically decrease immediately after the fork, and are recovering upwards, likely due to the recent bull market activity. I eagerly ask for a discussion with respect to the underlying reasons for each trend.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/9/4935f9d2b9492a72761b825cbab25df5fc18342c.jpeg" title="Trends"><img alt="Trends" height="412" src="https://ethresear.ch/uploads/default/optimized/3X/4/9/4935f9d2b9492a72761b825cbab25df5fc18342c_2_690x412.jpeg" width="690" /></a></div><p></p>
<p>To calibrate the seasonality components, I first analyse the seasonality present within the data. To do so, I performed a spectral analysis using the Fast Fourier Transform (FFT). The FFT decomposes the time-domain signal into its constituent frequencies, allowing us to compute the power spectrum, which represents the signal’s power distribution across different frequencies. I focused on the positive half of the spectrum and converted frequencies to periods in hours. Significant periodic components were identified by locating peaks in the power spectrum. The identified cycles, with the most prominent displaying daily (24 hours) and weekly (168 hours) seasonality, reinforcing the sinusoidal functions specified above. I visualise the results below to highlight the dominant seasonal patterns in the data.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/c/fc3302537e3548a88afc036869c60322fd6efed6.png" title="Period"><img alt="Period" height="413" src="https://ethresear.ch/uploads/default/original/3X/f/c/fc3302537e3548a88afc036869c60322fd6efed6.png" width="690" /></a></div><p></p>
<p>In estimating the <span class="math">\beta</span> parameters, I also use a least squares optimisation method. Given the observed log base fees <span class="math">BF</span> and the seasonality matrix <span class="math">C</span>, the objective is to estimate the seasonality parameters <span class="math">\beta</span> that minimise the sum of squared residuals. This is formulated as:</p>
<div class="math">
\min_{\beta} \| log(BF)_{detrended} - C \beta \|^2

</div>
<p>where:</p>
<ul>
<li><span class="math">log(BF)_{detrended}</span>  is the vector of de-trended log base fees,</li>
<li><span class="math">C</span>  is the matrix containing the seasonality functions (sine, cosine),</li>
<li><span class="math">\beta</span>  is the vector of seasonality parameters to be estimated.</li>
</ul>
<p>The expanded form of the objective function is:</p>
<div class="math">
\min_{\beta} \sum_{I=1}^{n} (log(BF_i)_{detrended} - C_i \beta)^2
</div>
<p>where  <span class="math">log(BF_i)_{detrended}</span> is the <span class="math">i</span> -th observed log base fee and <span class="math">C_i</span>  is the  <span class="math">i</span>-th row of the seasonality matrix. To find the least squares solution, I set the gradient of the objective function with respect to <span class="math">\beta</span> to zero, yielding the normal equations:</p>
<div class="math">
\frac{\partial}{\partial \beta} \left( \sum_{i=1}^{n} (log(BF_i)_{detrended} - C_i \beta)^2 \right) = -2 C^T (log(BF_i)_{detrended} - C \hat{\beta}) = 0
</div>
<p>Simplifying, I obtain:</p>
<div class="math">
C^T C \hat{\beta} = C^T log(BF)_{detrended}
</div>
<p>Solving the normal equations for  \beta  provides the least squares estimates:</p>
<div class="math">
\hat{\beta} = (C^T C)^{-1} C^T log(BF)_{detrended}
</div>
<p>where  <span class="math">(C^T C)^{-1} C^T</span>  is the Moore-Penrose pseudoinverse of <span class="math">C</span>. The de-seasonalized and de-trended log prices are then calculated by subtracting the combined seasonality components from the observed log prices, as is shown in the figure below.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/f/8f324d5ecfcda3c65cc3aebdbfbd09ac4e6bf621.png" title="Seasoned"><img alt="Seasoned" height="459" src="https://ethresear.ch/uploads/default/optimized/3X/8/f/8f324d5ecfcda3c65cc3aebdbfbd09ac4e6bf621_2_690x459.png" width="690" /></a></div><p></p>
<p>Visibly, base fees in the Ethereum network fluctuate due to weekly and daily patterns of usage. During the week, gas usage is typically higher as business activities, market trading, and development deployments peaks. This weekly seasonality contrasts with daily patterns where peak periods of gas usage occur in the morning and evening as global participants, including Europe and North America, overlap in activity.</p>
<p>Despite removing trend and overarching seasonality, I observe that autocorrelative structure is still present in our time series. To address this, I explore the autocorrelation function (ACF) and the partial autocorrelation function (PACF) of the data. The ACF helps identify the correlation between observations at different lags of base fees, providing insights into the persistence of shocks over time. The PACF isolates the direct effect of a lagged observation by controlling for the contributions of intermediate lags, aiding in the identification of the order of the autoregressive (AR) terms in an ARIMA model. By observing the graph below, we observe 34 autocorrelated lags, and 5 partially autocorrelated lags.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/1/519bf7d93d2812acc39ec47dc5879a39ec44dca8.png" title="Autocorrelation"><img alt="Autocorrelation" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/5/1/519bf7d93d2812acc39ec47dc5879a39ec44dca8_2_532x500.png" width="532" /></a></div><p></p>
<p>To address these lags, I fit an ARIMA(34, 5, 5) model to the de-seasonalized and de-trended data. This model captures the autoregressive and moving average components along with differencing. By fitting this model, we obtain the residuals, which represent the underlying structure of the noise after accounting for these components. Upon examining the residuals, we find that the distribution of the noise is sharply centered around zero with fat tails. The transition between the central peak and the tails appears to follow an exponential pattern. Consequently, we fit a Laplace distribution to the residuals and compare the observed values against the expected values, where the Laplace distribution is defined as:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/2/520b0b7803ad250ea93fc680b158f762eb2571b0.png" title="StandardisedResidules"><img alt="StandardisedResidules" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/5/2/520b0b7803ad250ea93fc680b158f762eb2571b0_2_673x500.png" width="673" /></a></div><p></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/5/65238658d70c03f7293ed41bb385d7eb95d65c7d.png" title="PDF"><img alt="PDF" height="499" src="https://ethresear.ch/uploads/default/original/3X/6/5/65238658d70c03f7293ed41bb385d7eb95d65c7d.png" width="690" /></a></div><p></p>
<div class="math">
f(x; \mu, b, \kappa) =
\begin{cases}
\frac{\kappa}{b} \exp \left( \frac{x - \mu}{b} \kappa \right), &amp; \text{if } x \leq \mu \\
\frac{1}{b \kappa} \exp \left( -\frac{x - \mu}{b \kappa} \right), &amp; \text{if } x &gt; \mu
\end{cases}
</div>
<ul>
<li><span class="math">\mu</span>  is the location parameter (mean),</li>
<li><span class="math">b &gt; 0</span>  is the scale parameter,</li>
<li><span class="math">\kappa &gt; 0</span>  controls the asymmetry of the distribution. a Kappa equal to 1 produces a symmetrical Laplace distribution.</li>
</ul>
<p>A Laplace distribution is significant because it characterises the distribution as having exponential decay on both sides of the peak. In the context of Ethereum base fees, changes often occur as a percentage of the previous base fee rather than by fixed amounts. This implies that large deviations from the mean are more probable than would be expected under a normal distribution, leading to the characteristic heavy tails of the Laplace distribution. The sharp central peak of the Laplace distribution indicates a high probability of small changes around the long term mean. The fat tails, on the other hand, reflect the occasional large percentage changes, driven by significant network, a series of full block events or structural shifts in demand for block space. One would expect a higher denominator parameter (reducing sensitivity of base fees to gas usage) to create a ‘tighter’ distribution with a lower standard deviation.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/3/8359c539f8d3aae5ec27461b63895a852255410e.png" title="QQ"><img alt="QQ" height="482" src="https://ethresear.ch/uploads/default/original/3X/8/3/8359c539f8d3aae5ec27461b63895a852255410e.png" width="690" /></a></div><p></p>
<p>I plot the theoretical quantiles of the Laplace distribution against the observed residual quantiles in a QQ plot. The plot revealed that a few outlier observations exhibited fatter tails than predicted by the Laplace distribution, indicating that the residuals have more extreme values than our model accounts for. Nevertheless, the majority of the data points followed the expected distribution, suggesting that the Laplace distribution still provides a reasonable fit for the central portion of the residuals.</p>
<p><strong>Stochastic Component Specification</strong></p>
<div class="math">
X_t = \log BF_t - f(t)
</div>
<p><strong>Cox, Ingersoll &amp; Ross Poisson Asymmetrical Laplace calibrated model</strong></p>
<p>Removing the trend and observing the noise, the stochastic component <span class="math">X_t</span>  is modelled as an Ornstein-Uhlenbeck process (mean-reverting) with jumps, incorporating a Laplace distribution for both the noise term and the jump size</p>
<div class="math">
dX_t = (\alpha - \kappa X_t) \, dt + \sigma \, dL_t + J(\mu_J, \sigma_J) \, d\Pi(\lambda),
</div>
<p>where:</p>
<ul>
<li><span class="math">α</span> is the drift parameter,</li>
<li><span class="math">κ</span>  is the rate of mean reversion,</li>
<li><span class="math">σ</span> is the volatility,</li>
<li><span class="math">L_t</span> is a noise term following a Laplace distribution,</li>
<li><span class="math">J(μ_J , σ_J )</span> is the jump size, following a Laplace distribution with mean  <span class="math">μ_J</span> and scale parameter <span class="math">σ_J</span> ,</li>
<li><span class="math">Π(λ)</span> is a Poisson process with jump intensity <span class="math">λ</span>.</li>
</ul>
<p>The transition probabilities for base fee equilibrium prices follow a Poisson-Laplace process. This can be expressed as:</p>
<div class="math">

p(X_t | X_{t-1}) = \lambda \frac{1}{2b} \exp\left(-\frac{|X_t - (a \Delta t + \phi X_{t-1} + \mu_J)|}{\sqrt{v_t (\sigma^2 + \sigma_J^2)}}\right)

</div>
<div class="math">

• (1-\lambda) \cdot \frac{1}{2b} \exp\left(-\frac{|X_t - (a \Delta t + \phi X_{t-1})|}{\sqrt{v_t \sigma^2}}\right)

</div>
<p>where:</p>
<ul>
<li><span class="math">∆t</span> is the time increment,</li>
<li><span class="math">a</span> is the drift term,</li>
<li><span class="math">φ</span> is the autoregressive coefficient,</li>
<li><span class="math">b</span> is the scale parameter of the Laplace distribution.</li>
</ul>
<p>The parameters <span class="math">Q = {α, κ, σ, a, b, μ_J , σ_J , λ}</span> can first be estimated by Maximum Likelihood (ML). This approach ensures that the parameters <span class="math">Q</span> are optimised to best fit the observed data under the specified model. The results of the simulation, displayed below, demonstrate the simulated log base fees for the next month. Notably, considering the most recent regime of exponential increase, I adopt a neutral market approach by assuming a linear (horizontal) trend. For pricing derivative products, consideration of what the future trend will be should be taken into account. After retrieving calibrated parameters, a range of future hourly dates is produced, before trend and seasonality is added back in, and the exponential is taken to revert the log.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/f/af172a8475671fb615b0494f03c0e53f7d01c0fb.png" title="Simulation"><img alt="Simulation" height="500" src="https://ethresear.ch/uploads/default/original/3X/a/f/af172a8475671fb615b0494f03c0e53f7d01c0fb.png" width="592" /></a></div><p></p>
<p><strong>ARIMA Monte Carlo Method</strong></p>
<p>An alternative method, which requires less computation simulates a monte-carlo process by modelling standardised residuals directly from a Laplace distribution, augmenting these standardised residuals back de-normalising them, before generating future paths as the composite of both this noise and ARIMA forecasts. A simulated residual path is shown below, producing the resulting simulation.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/0/70411cba63138e2322e32a338e350acc9dbeebe9.png" title="Residules"><img alt="Residules" height="500" src="https://ethresear.ch/uploads/default/original/3X/7/0/70411cba63138e2322e32a338e350acc9dbeebe9.png" width="651" /></a></div><p></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/7/a71fae0b0032a14bc4099ebbd6b20f28c69bfd5a.png" title="Sim2"><img alt="Sim2" height="479" src="https://ethresear.ch/uploads/default/optimized/3X/a/7/a71fae0b0032a14bc4099ebbd6b20f28c69bfd5a_2_690x479.png" width="690" /></a></div><p></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/e/ce326060566053721576245372a025708af87d0b.jpeg" title="Screenshot 2024-06-14 at 12.50.40"><img alt="Screenshot 2024-06-14 at 12.50.40" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/c/e/ce326060566053721576245372a025708af87d0b_2_687x500.jpeg" width="687" /></a></div><p></p>
<p><strong>Pricing with these simulated values</strong></p>
<p>For the purpose of hedging, pricing a European call or put option based on the simulated paths can be expressed as follows. Let <span class="math">BF_T^i</span> represent base fees per gas at maturity <span class="math">T</span> for the <span class="math">i</span>-th simulated path, where <span class="math">i = 1, 2, \ldots, N</span>, and denote the strike price by <span class="math">K</span>. The payoff for the European call option at maturity for the <span class="math">i</span>-th path is given by <span class="math">\max(BF_T^i - K, 0)</span>, and for the European put option, it is given by <span class="math">\max(K - BF_T^i, 0)</span>. To find the option price, I first calculate the average payoff across all <span class="math">N</span> simulated paths: <span class="math">\frac{1}{N} \sum_{i=1}^N \max(BF_T^i - K, 0)</span> for the call option, and <span class="math">\frac{1}{N} \sum_{i=1}^N \max(K - BF_T^i, 0)</span> for the put option. These average payoffs are then discounted to the present value using the risk-free rate <span class="math">r</span>, giving the price of the European call option at time 0 as <span class="math">C_0 = e^{-rT} \times \frac{1}{N} \sum_{i=1}^N \max(BF_T^i - K, 0)</span> and the price of the European put option at time 0 as <span class="math">P_0 = e^{-rT} \times \frac{1}{N} \sum_{i=1}^N \max(K - BF_T^i, 0)</span>.</p>
<p>In pricing a gas plan where the underwriter subsidises the entire unit of gas at any point before <span class="math">T</span> I can model this as an American call option on gas fees with a strike price of zero and implement the Longstaff and Schwartz Regression Approach. This method involves calculating the payoff at the final period <span class="math">T</span> as the gas fee <span class="math">BF_T^i</span> for each path <span class="math">i</span>, assuming no transaction has been exercised before this time. Moving one time step backward, one regress the discounted future payoffs against the current gas fees <span class="math">BF_t^i</span> to estimate continuation values. This regression, known as a basis function, typically includes terms like a constant, <span class="math">BF_t^i</span>, and <span class="math">(BF_t^i)^2</span>. At each time step <span class="math">t</span>, we compare the immediate exercise value <span class="math">BF_t^i</span> with the estimated continuation value <span class="math">\hat{C}_t^i</span> from the regression. If the exercise value is higher, one updates the cash flow to reflect exercising the option; otherwise, we carry forward the discounted cash flow. Finally, the option price at time 0 (now) is obtained by averaging the discounted cash flows across all paths and all time periods.</p>
<p><strong>Utility in Oiler Pitch Lake</strong><br />
Oiler Pitch Lake is a protocol designed to allow liquidity providers (LPs) by pooling their assets to act as sellers in time-weighted moving average (TWAP) base fee cash-settled call options. Utilizing Starknet STARKS and the Fossil coprocessor for verifiability, Pitch Lake determines option payoffs based on the average base fee over a specified time interval, akin to an Asian option.</p>
<p>Given that LPs put up a limited amount of collateral, there is a cap on each option’s payoff. To protect LPs, a reserve price can be set, which is the minimum price at which the option must be sold. Using the aforementioned methodology, this reserve price can be calculated with both accuracy and verifiability, ensuring that LPs are safeguarded. Pitch Lake is currently in development and is expected to launch in the coming months.</p>
<p><strong>Next Steps</strong></p>
<ul>
<li><strong>Call for reproduction</strong>: Please reproduce my analysis and methodology. Working with complex financial mathematics can be prone to assumptions that may be easily violated. View this as an initial attempt to dive into the gas fee pricing topic</li>
<li><strong>What about the blobs market and L2 fee markets? I</strong> am currently working on similar analysis for the blob fee market place, L2 fees, and other blockchains. Expect more results soon.</li>
</ul>
<p><strong>Bibliography</strong><br />
[1] Oiler Pitch Lake. <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4123018" rel="noopener nofollow ugc">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4123018</a><br />
[2] SMG Spec <a class="inline-onebox" href="https://www.mechanism.org/spec/04" rel="noopener nofollow ugc">04</a><br />
[3] [2] Lucia, Julio J., Schwartz, Eduaro. “Electricity Prices and Power Derivatives: Evidence from the Nordic Power Exchange.” Review of Derivatives Research. Vol. 5, Issue 1, pp 5-50, 2002. <a class="inline-onebox" href="https://link.springer.com/article/10.1023/A:1013846631785" rel="noopener nofollow ugc">Electricity Prices and Power Derivatives: Evidence from the Nordic Power Exchange | Review of Derivatives Research</a><br />
[4] Seifert, Jan, Uhrig-Homburg, Marliese. “Modelling Jumps in Electricity Prices: Theory and Empirical Evidence.” <em>Review of Derivatives Research</em>. Vol. 10, pp 59-85, 2007. <a class="inline-onebox" href="https://uk.mathworks.com/help/fininst/simulating-electricity-prices-with-mean-reversion-and-jump-diffusion.html" rel="noopener nofollow ugc">Simulating Electricity Prices with Mean-Reversion and Jump-Diffusion - MATLAB &amp; Simulink - MathWorks United Kingdom</a><br />
[5] Escribano, Alvaro, Pena, Juan Ignacio, Villaplana, Pablo. “Modeling Electricity Prices: International Evidence.” Universidad Carloes III de Madrid, Working Paper 02-27, 2002. Modeling Electricity Prices: International Evidence <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=299360" rel="noopener nofollow ugc">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=299360</a></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/pricing-gas-fee-derivatives/19898">Read full topic</a></p>
]]></content:encoded>
<pubDate>Mon, 24 Jun 2024 20:59:59 +0000</pubDate>
</item>
<item>
<title>Execution Auctions as an Alternative to Execution Tickets</title>
<link>https://ethresear.ch/t/execution-auctions-as-an-alternative-to-execution-tickets/19894</link>
<guid>https://ethresear.ch/t/execution-auctions-as-an-alternative-to-execution-tickets/19894</guid>
<content:encoded><![CDATA[
<div> 关键词：执行拍卖（Execution Auctions, EAs）、执行门票（Execution Tickets, ETs）、MEV、中央化、价值分配

总结:<br />
文章比较了两种解决以太坊中矿工效用提升（MEV）问题的机制：执行拍卖和执行门票。这两种方法旨在通过出售执行权来解决协议中的代理人问题，提高效率并减少外部性。文章分析了两种机制的经济差异，包括预期现值（NPV）、风险折扣、成本控制和中央化风险。执行拍卖简单易实现，但可能导致集中化；执行门票利用随机性，虽能防止集中，但可能涉及复杂性。最后，文章认为执行拍卖在实践中可能更优，尽管执行门票在某些方面提供保护。 <div>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/2/128696dce49653e3211f52d33f86612013a883c4.jpeg" title="ealien"><img alt="ealien" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/1/2/128696dce49653e3211f52d33f86612013a883c4_2_500x500.jpeg" width="500" /></a></div><p></p>
<p><br />
<em>By <a href="https://twitter.com/_JonahB_">Jonah Burian</a> &amp; <a href="https://twitter.com/DavideCrapis">Davide Crapis</a></em></p>
<p><em>Special thanks to <a href="https://x.com/weboftrees">Anders Elowsson</a>, <a href="https://twitter.com/barnabemonnot">Barnabé Monnot</a>, <a href="https://twitter.com/drakefjustin">Justin Drake</a> and <a href="https://twitter.com/mikeneuder">Mike Neuder</a> for the feedback and review.</em></p>
<h1><a class="anchor" href="https://ethresear.ch#introduction-1" name="introduction-1"></a>Introduction</h1>
<p>There is a principal-agent problem in Ethereum. While the protocol creates MEV, it leaks it to proposers. Moreover, MEV in its current state exposes the protocol to other externalities, such as <a href="https://arxiv.org/abs/2305.09032">timing games</a>. It is widely held in the research community that capturing and properly redistributing MEV is an important step in the evolution of Ethereum, to make the protocol more resilient and efficient (<em>note: there are some people who <a href="https://www.nano210.blog/infinite-blockspace-equilibrium/">disagree</a>)</em>. The only way to solve this principal-agent problem is for the protocol to sell the rights to earn the MEV with a credible and efficient mechanism.</p>
<p>After many years of research, two approaches have recently emerged as potential avenues for solving MEV-burn. These are mechanisms where the right to propose an execution payload is not given for free to the <em>beacon proposer</em>, but it is instead sold separately to an <em>execution proposer</em>.</p>
<ul>
<li><strong>Execution Auctions (EAs):</strong> The right to propose an execution payload is <em>deterministically allocated</em> in advance for each slot, the slot execution proposer can purchase this right by bidding in a slot auction held beforehand, for example <a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ#:~:text=introduce%20it%20next.-,Slot%2Dauction%2B32%20ePBS,-We%20suggest%20now">32 slots earlier</a>.</li>
<li><strong>Execution Tickets (ETs):</strong> the execution proposer right is not deterministically allocated, proposers can purchase a lottery ticket in advance and then, before each slot, a winner is drawn at random from the ticket pool and gets the right to propose.
<ul>
<li>The simple version of the protocol gives the winner the right to propose the following block. This was the focus of <a href="https://ethresear.ch/t/economic-analysis-of-execution-tickets/18894">Economic Analysis of Execution Tickets</a>.</li>
<li>The original execution ticket post <a href="https://ethresear.ch/t/execution-tickets/17944#:~:text=There%20exists%20a,market%20function%20smoothly.">suggested</a> a general version of the protocol where the winner has the right to propose <span class="math">m</span> slots later (e.g., 32). The intuition for why winners are given slots multiple slots in advance as opposed to immediately is that the solution allows for winners to offer preconfs.</li>
</ul>
</li>
</ul>
<p>The mechanisms have the same objective but important differences. The goal of this post is to compare the two solutions.</p>
<h1><a class="anchor" href="https://ethresear.ch#setup-2" name="setup-2"></a>Setup</h1>
<p>We will introduce formulas throughout this post to outline the key economic differences between the protocols. We will also explain the practical nuances, so if you want to skip the math, no worries! For the extra curious, we lay out the proof of the formulas in the appendix.</p>
<h2><a class="anchor" href="https://ethresear.ch#terms-3" name="terms-3"></a>Terms</h2>
<ul>
<li><span class="math">t</span> — discrete time intervals (slot).</li>
<li><span class="math">n</span> — the number of tickets.</li>
<li><span class="math">d</span> is the inter-slot discount rate used to calculate the present value of future prizes.
<ul>
<li>Note: assuming that the vanilla staking rate is the risk-free rate in Ethereum, <span class="math">d \approx 10^{-8}</span></li>
</ul>
</li>
<li><span class="math">\mathcal{R}</span> is a random variable representing the value of controlling an execution payload at time <span class="math">t</span>.
<ul>
<li>We term this the Execution Layer Reward (EL Reward) which equals MEV + fees in slot <span class="math">t</span>.</li>
<li>We assume that <span class="math">\mathcal{R}</span> has a distribution that does not vary with time and that each draw is independent. (This is usually not the case in practice, as EL rewards are time-varying and correlated, but it allows for a less complicated analysis that can be expanded later.)</li>
</ul>
</li>
<li><span class="math">\mu_{\mathcal{R}}</span> is the expected value of <span class="math">\mathcal{R}</span>.</li>
<li><span class="math">V_{ticket}</span> — Net Present Value (NPV) of a single ticket.
<ul>
<li>Note: present value of some value <span class="math">X</span> realized at time <span class="math">t</span> is calculated as <span class="math">\frac{X}{(1+d)^t}</span>.</li>
</ul>
</li>
<li><span class="math">m</span> — number of slots t after winning that the right to propose is given (e.g., <span class="math">m=32</span>).</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#northstar-4" name="northstar-4"></a>Northstar</h2>
<p>The expected net present value of all future EL Rewards is</p>
<div class="math">
NPV_{\mathcal{R}} = \frac{\mu_{\mathcal{R}}}{d}
</div>
<p>This is the total value of all block space from now into the future of Ethereum. Given that the goal of the Execution Auctions and Execution Tickets is to capture the value of block space and redistribute the value to align with the protocol’s goals, all solutions must be analyzed in terms of how well they capture <span class="math">NPV_{\mathcal{R}}</span>.</p>
<p><em>Note: Capturing all the value depends on the selling mechanism. In this analysis, we assume that the selling mechanism is efficient. Detailed analysis of the selling mechanism in a dynamic/repeated strategic interaction context is an open problem currently under research.</em></p>
<h1><a class="anchor" href="https://ethresear.ch#execution-auctions-5" name="execution-auctions-5"></a>Execution Auctions</h1>
<p>Execution Auctions (EAs) are essentially slot auctions carried out in advance:</p>
<ul>
<li><strong>Proposer right allocation:</strong> the <em>execution proposer right</em> for slot <span class="math">k+m</span> is sold <span class="math">m</span> slots in advance, at slot <span class="math">k</span>.</li>
<li><strong>Selling mechanism:</strong> the beacon proposer of slot <span class="math">k</span> receives bids for that right and commits to the highest bid, attesters vote.</li>
</ul>
<p>A <strong>secondary market</strong> will most likely develop where an EA ticket winner can resell their proposer right before their turn to propose. Even if the protocol does not allow them to transfer that right, this can be easily done via an out-of-protocol gadget.</p>
<h1><a class="anchor" href="https://ethresear.ch#execution-tickets-6" name="execution-tickets-6"></a>Execution Tickets</h1>
<p>Execution Tickets (ETs) have a lottery component that adds uncertainty on the specific block a holder will be able to propose in the future, this can be resolved closer to the time of proposing or further ahead of time.</p>
<ul>
<li><strong>Proposer right allocation:</strong> the <em>execution proposer right</em> for <em>a slot</em> in the future is sold in the form of a lottery ticket.</li>
<li><strong>Selling mechanism:</strong> assume there are already <span class="math">n</span> tickets in the lottery pool, at each slot a ticket is selected as lottery winner (e.g., at the end of the slot using RANDAO) and a new ticket is sold to enter the pool starting from the next slot.
<ul>
<li><strong>Pricing:</strong> We assume an English Auction for comparison with EAs.</li>
<li><strong>Uncertainty resolution:</strong> we can have a next-slot execution lottery, where at the end of slot <span class="math">k</span> we select proposer for slot <span class="math">k+1</span> (we term these sETs i.e, simple ETs), or a future-slot execution lottery, where at the end of slot <span class="math">k</span> we select proposer for slot <span class="math">k+m</span> (we term these ETs).</li>
</ul>
</li>
</ul>
<p>Similarly to EAs, a secondary market will likely emerge where a ticket holder or a winning ticket holder can resell their right to participate in the lottery or propose.</p>
<p><em>Note: In the initial post <a href="https://ethresear.ch/t/economic-analysis-of-execution-tickets/18894">Economic Analysis of Execution Tickets</a> we did not yet make the distinction between sETs and ETs. That post was about sETs (a special case of ETs).</em></p>
<p><em>Note 2: <a href="https://twitter.com/drakefjustin">Justin </a> perceptively pointed out that we don’t know how to achieve low-latency randomness using RANDAO, and VDFs wouldn’t help either. Low-latency RANDAO would be biasable (as well as fully predictable when you control two slots in a row).</em></p>
<h1><a class="anchor" href="https://ethresear.ch#analysis-7" name="analysis-7"></a>Analysis</h1>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/0/c088b53c1b671ac4704f3ff5c99e4b792264c861.png" title="Chart 1"><img alt="Chart 1" height="174" src="https://ethresear.ch/uploads/default/optimized/3X/c/0/c088b53c1b671ac4704f3ff5c99e4b792264c861_2_690x174.png" width="690" /></a></div><p></p>
<p><em>Note: All approximations assume <span class="math">m</span> (time from when the ticket wins to when the right is conferred) and <span class="math">n</span> (number of ETs) are not large. Given that <span class="math">d</span> is nearly zero, we are able to simplify the equations.</em></p>
<p><em>Note 2: Without using the approximation,</em> EA tickets <em>and ETs have some Dead Weight Loss associated with the fact that winning tickets cannot be immediately used, i.e., there is some loss given the time discount. The intuition for the approximation is that given <span class="math">d</span> is small, this value loss due to the time discount is nominal.</em></p>
<p><em>Note 3: While we assume in the approximation for the variance of sETs and ETs that <span class="math">n</span> is small, we discussed in “<a href="https://ethresear.ch/t/economic-analysis-of-execution-tickets/18894">Economic Analysis of Execution Tickets</a>” that a large <span class="math">n</span> leads to less centralization risk and more democratization in terms of who can afford a ticket. That said, a large <span class="math">n</span> creates valuation complexity and adds the additional complexity of having to run a large sale at the beginning of the lottery to bootstrap the ticket pool. (Read the article to learn more.)</em></p>
<p>Here is a simplified version of the chart assuming the approximation.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/a/dad825aa8d8c8324d64726716ed5a0fd84508ffe.png" title="Chart 2"><img alt="Chart 2" height="186" src="https://ethresear.ch/uploads/default/optimized/3X/d/a/dad825aa8d8c8324d64726716ed5a0fd84508ffe_2_517x186.png" width="517" /></a></div><p></p>
<p>Notice that all three approaches using the approximation arrive at the same conclusion: we can effectively capture (assuming an efficient auction) all the value associated with block space. Moreover, in each design, the tickets have a simple explanation: they are worth about the value associated with proposing an execution payload.</p>
<p>The variance of the ticket value is the variance of the rewards per slot, which is about as good as you are going to get given that the rights to propose are sold in advance, namely prior to the block’s construction.</p>
<h1><a class="anchor" href="https://ethresear.ch#sure-thing-vs-future-possibility-comparing-eas-and-setsets-8" name="sure-thing-vs-future-possibility-comparing-eas-and-setsets-8"></a>Sure Thing vs Future Possibility: Comparing EAs and sETs/ETs</h1>
<p>We now turn to comparing EAs and sETs/ETs to elucidate trade-offs when thinking about implementing such mechanisms in practice. It should be noted that most of the tradeoffs stand from the fundamental difference between EAs and sETs/ETs - the former is a deterministic protocol while the latter leverages nondeterminism.</p>
<ul>
<li><strong>Implementation Simplicity:</strong> EAs are simpler to implement, the tickets do not require randomness so there is no need to worry about RANDAO bias. Moreover, it is unclear how to implement the randomness for sETs. The secondary market for proposer rights with EA tickets will be much simpler than with sETs/ETs, no need to worry about ticket MEV. Moreover, there seems to be a <a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ">clear path to implement EAs via ePBS</a> and bypassability not an issue since we’re selling future slots.</li>
<li><strong>Simpler Assets:</strong> It is easier to reason about a deterministic asset than a random one, which makes EA better than sETs and ETS. That said, buyers in the protocol are most likely sophisticated, and the current paradigm for selecting validators relies on randomness, meaning maintaining non-determinism won’t be a substantial break from the status quo. However, a counterargument is that current proposers might not be buyers of tickets.</li>
<li><strong>Variance could affect valuation and EA tickets are less exposed:</strong> It is reasonable for ticket holders to apply a risk discount to the tickets; that is, they might value the tickets less given risk aversion. While EA tickets are only exposed to the variance in the value of the EL rewards in slot <span class="math">t+m</span>, both sETs and ETs are exposed to the variance in EL rewards and the variance in when a ticket wins. Intuitively, EA would therefore have the lowest risk discount.</li>
<li><strong>Efficiency:</strong> From the protocol’s POV, sETs are more efficient because proposer rights are sold closer to the slot of the MEV in expectation while EAs and ETs have dead weight loss in theory. That said, when factoring in risk aversion, EAs might be more efficient.</li>
<li><strong>Preconfs</strong>: Preconfs require there to be a lookahead, meaning the protocol must know in advance who will control the rights to the execution payload. While EAs and ETs allow for preconfs, sETs do not, as winners are decided at each block.</li>
<li><strong>Cost-of-control:</strong>
<ul>
<li><strong>In EA</strong>
<ul>
<li>EAs put <em>transaction liveness</em> risk on Ethereum—namely, the cost of monopolizing block space is disjoint from the security budget of Ethereum, and the cost of controlling consecutive blocks has a fixed value. Controlling <span class="math">x</span> blocks in a row costs approximately <strong><span class="math">\approx x\mu_{\mathcal{R}}</span></strong>. Luckily, new <a href="https://eips.ethereum.org/EIPS/eip-7547">IL designs</a> could rectify this. Even with ILs, relying heavily on them is suboptimal (they are designed to be a last resort, not commonplace—this can be argued). Importantly as well, the ability to consistently control multiple slots means that well-capitalized parties will perpetually win more block space. This could lead to centralization of the execution payload construction pipeline, exacerbating the current centralization challenges within this pipeline. (See the Multi-block MEV section in “<a href="https://arxiv.org/abs/2404.04262">Future of MEV</a>”).</li>
<li><a href="https://twitter.com/barnabemonnot">Barnabé</a> aptly noted to us that saying the “the cost of monopolizing block space is disjoint from the security budget of Ethereum” is no different from the existing setup where validators can sell building rights. Currently, validators can sell multiple consecutive blocks in a row. <em>This does not mean that the centralization argument is incorrect but indicates that EAs are not a substantial break from the status quo.</em></li>
</ul>
</li>
<li><strong>In sETs (and ETs):</strong>
<ul>
<li>
<p>While the cost of monopolizing block space is disjoint from the security budget with sETs (and ETs), it is substantially more expensive and less likely that a single party can control multiple consecutive blocks in a row. Non-determinism prevents guaranteed control over block space reducing the likelihood of control. Randomness serves as a defense against centralization.</p>
<ul>
<li>
<p>The first chart provides an intuitive understanding of this principle. It shows a scenario with 100 outstanding sETs/ETs. If someone owns 95% of the initial outstanding tickets (remember, one is subsequently minted per block), the probability of winning 20 slots in a row is approximately 4%, while the probability of winning 35 in a row is almost impossible.<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/3/63786be4729c8134b1fa101788af325a31fb7427.png" title="Graph 1"><img alt="Graph 1" height="412" src="https://ethresear.ch/uploads/default/optimized/3X/6/3/63786be4729c8134b1fa101788af325a31fb7427_2_690x412.png" width="690" /></a></div><p></p>
</li>
<li>
<p>Moreover, the costs of controlling <span class="math">P\%</span> of the blocks increases in <span class="math">n</span> (See: <a href="https://ethresear.ch/t/economic-analysis-of-execution-tickets/18894">Economic Analysis of Execution Tickets</a>)<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/5/b5e0ee59790fe044c3adbb4ae4f5ae4408e0694c.png" title="Graph 2"><img alt="Graph 2" height="186" src="https://ethresear.ch/uploads/default/optimized/3X/b/5/b5e0ee59790fe044c3adbb4ae4f5ae4408e0694c_2_690x186.png" width="690" /></a></div><p></p>
</li>
</ul>
</li>
<li>
<p><strong>Where ETs differ:</strong></p>
<ul>
<li>While an attacker in sETs must rely on chance to win consecutive blocks, a clever ETs user can buy a sequence of winning tickets for <span class="math">t+m</span> to <span class="math">t+m+x</span> on the secondary market, making the centralization in ETs similar to the centralization problem with EAs. One can argue that sETs are subject to the same risk as an out-of-protocol auction for control of the sETs winners’ rights can happen. That said, there may be honest actors who don’t sell rights to execution payload construction. If one of these holders wins, they end the sequence of winning blocks for a sET attacker, meaning that a sET attack, even with the out-of-protocol option, is exposed to uncertainty.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><a class="anchor" href="https://ethresear.ch#concluding-remarks-9" name="concluding-remarks-9"></a>Concluding Remarks</h1>
<p>EAs dominate in simplicity, while sETs protect from centralization but at the expense of allowing for preconfs. sETs may also be unimplementable in the Ethereum Protocol today given the RANDAO problem. ILs can curb centralization concerns with EAs, and the secondary market for sETs/ETs can nullify their protective benefits. Moreover, EAs are not a substantial break from the status quo in terms of centralization.</p>
<p><em>While there are still open questions around implementing EAs and their efficiency, EAs seem to be superior to sETs and ETs for the Ethereum protocol.</em></p>
<h1><a class="anchor" href="https://ethresear.ch#related-work-10" name="related-work-10"></a><strong>Related work</strong></h1>
<p><em>This list is copied and pasted from</em> <a href="https://ethresear.ch/t/on-block-space-distribution-mechanisms/19764">On block-space distribution mechanisms</a> with the addition of <a href="https://ethresear.ch/t/on-block-space-distribution-mechanisms/19764">On block-space distribution mechanisms</a>. lol</p>
<ol>
<li><em>mev-boost &amp; relays</em>
<ul>
<li><a href="https://ethresear.ch/t/mev-boost-merge-ready-flashbots-architecture/11177"><em>MEV-Boost: Merge ready Flashbots Architecture</em></a>; Flashbots team</li>
<li><a href="https://ethresear.ch/t/relays-in-a-post-epbs-world/16278"><em>Relays in a post-ePBS world</em></a>; Mike, Jon, Hasu, Tomasz, Chris, Toni</li>
</ul>
</li>
<li><em>mev-burn / mev-smoothing</em>
<ul>
<li><a href="https://ethresear.ch/t/burning-mev-through-block-proposer-auctions/14029"><em>Burning MEV through block proposer auctions</em></a>; Domothy</li>
<li><a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590"><em>MEV burn – a simple design</em></a>; Justin</li>
<li><a href="https://ethresear.ch/t/committee-driven-mev-smoothing/10408"><em>Committee-driven MEV smoothing</em></a>; Francesco</li>
<li><a href="https://ethresear.ch/t/dr-changestuff-or-how-i-learned-to-stop-worrying-and-love-mev-burn/17384"><em>Dr. changestuff or: how I learned to stop worrying and love mev-burn</em></a>; Mike, Toni, Justin</li>
</ul>
</li>
<li><em>enshrined Proposer-Builder Separation (ePBS)</em>
<ul>
<li><a href="https://ethresear.ch/t/two-slot-proposer-builder-separation/10980"><em>Two-slot proposer/builder separation</em></a>; Vitalik</li>
<li><a href="https://ethresear.ch/t/unbundling-pbs-towards-protocol-enforced-proposer-commitments-pepc/13879"><em>Unbundling PBS: towards protocol-enforced proposer commitments (PEPC)</em></a>; Barnabé</li>
<li><a href="https://barnabe.substack.com/p/pbs"><em>Notes on Proposer-Builder Separation</em></a>; Barnabé</li>
<li><a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ"><em>More pictures about proposers and builders</em></a>; Barnabé</li>
<li><a href="https://ethresear.ch/t/why-enshrine-proposer-builder-separation-a-viable-path-to-epbs/15710"><em>Why enshrine Proposer-Builder Separation?</em></a>; Mike, Justin</li>
<li><a href="https://ethresear.ch/t/epbs-design-constraints/18728"><em>ePBS design constraints</em></a>; Potuz</li>
<li><a href="https://mirror.xyz/barnabe.eth/LJUb_TpANS0VWi3TOwGx_fgomBvqPaQ39anVj3mnCOg"><em>Reconsidering the market structure of PBS</em></a>; Barnabé</li>
</ul>
</li>
<li><em>block-space futures</em>
<ul>
<li><a href="https://mirror.xyz/0x03c29504CEcCa30B93FF5774183a1358D41fbeB1/CPYI91s98cp9zKFkanKs_qotYzw09kWvouaAa9GXBrQ"><em>Block vs. Slot Auction PBS</em></a>; Julian</li>
<li><a href="https://frontier.tech/ethereums-blockspace-future"><em>Opportunities and Considerations of Ethereum’s Blockspace Future</em></a>; Drew, Ankit</li>
<li><a href="https://collective.flashbots.net/t/when-to-sell-your-blocks/2814"><em>When to sell your blocks</em></a>; Quintus, Conor</li>
</ul>
</li>
<li><em>execution tickets</em>
<ul>
<li><a href="https://www.youtube.com/watch?v=MtvbGuBbNqI"><em>Attester-proposer separation</em></a>; Justin</li>
<li><a href="https://ethresear.ch/t/execution-tickets/17944"><em>Execution tickets</em></a>; Justin, Mike</li>
<li><a href="https://ethresear.ch/t/economic-analysis-of-execution-tickets/18894"><em>Economic Analysis of Execution Tickets</em></a>; Jonah, Davide</li>
<li><a href="https://ethresear.ch/t/block-auction-epbs-versus-execution-ticket/19232"><em>Block-auction ePBS versus Execution Ticket</em></a>; Terence</li>
</ul>
</li>
<li><em><a href="https://ethresear.ch/t/on-block-space-distribution-mechanisms/19764">On block-space distribution mechanisms</a>; Mike, Pranav, &amp; Dr. Tim Roughgarden</em></li>
</ol>
<p><em>This post has a similar goal to</em> <a href="https://x.com/mikeneuder">Mike</a>, <a href="https://x.com/PGarimidi">Pranav</a>, &amp; <a href="https://x.com/Tim_Roughgarden">Tim</a>’s recent work titled <a href="https://ethresear.ch/t/on-block-space-distribution-mechanisms/19764">On block-space distribution mechanisms</a>: <em>comparing new mechanisms for execution rights allocation.</em> However, there are a few key differences in our analysis that we highlight here:</p>
<ol>
<li>They use a modified ET model (i.e., a model where all tickets are burned between slots). This model, while easier to implement, does not lead to an efficient allocation (as those with lower valuations for block space can still be allocated it).</li>
<li>They focus on a Tullock Contest model, while our model resembles a fixed-income model.</li>
<li>Their analysis focuses on the trade-off between the quality of the in-protocol MEV oracle and the fairness of the mechanism, while we focus on other trade-offs such as implementation ease, risk discounts, centralization control, and economic efficiency.</li>
</ol>
<h1><a class="anchor" href="https://ethresear.ch#appendix-11" name="appendix-11"></a>Appendix</h1>
<p><strong>Calculating the discount rate:</strong></p>
<p>The staking rate at the time of this article is <code>~3.4%</code> (<a href="https://www.coindesk.com/indices/ether/cesr">source</a>).</p>
<p><span class="math">1.34=(1+d)^{\text{number of slots in a year}}=(1+d)^{365 * 24 * 60 * 60 / 12}</span> <img alt=":arrow_right:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/arrow_right.png?v=12" title=":arrow_right:" width="20" />  <span class="math">d=1.27e-08 \approx 10^{-8}</span></p>
<p><strong>The expected net present value of all future EL Rewards:</strong></p>
<p>See this <a href="https://arxiv.org/abs/2404.04262">paper</a> for the proof</p>
<p><strong>Calculating:</strong> <span class="math">E[V_{\text{EA ticket}}]</span></p>
<div class="math">
E[V_{\text{EA ticket}}] =  \frac{\mu_{\mathcal{R}}}{(1+d)^m}
</div>
<p>This is because the value is recognized <span class="math">m</span> slots later so you need to discount the MEV received in <span class="math">m</span> blocks by the discount rate <span class="math">d</span>.</p>
<p><strong>Calculating <span class="math">E[V_{\text{all EA tickets}}]</span></strong></p>
<div class="math">
\begin{align*}
    E[V_{\text{all EA tickets}}] &amp;=
    \sum_{t=1}^{\infty} \frac{ E[V_{\text{EA ticket}}]}{(1+d)^t} \\
    &amp;= \sum_{t=1}^{\infty} \frac{\mu_{\mathcal{R}}}{(1+d)^{m+t}} \\
    &amp;= \frac{1}{(1+d)^{m}} \sum_{t=1}^{\infty} \frac{\mu_{\mathcal{R}}}{(1+d)^{t}} \\
    &amp;= \frac{1}{(1+d)^{m}} NPV_{\mathcal{R}} 
\end{align*}
</div>
<p><strong>Calculating</strong> <span class="math">\text{Var}(V_{\text{EA ticket}})</span></p>
<div class="math">
\text{Var}(V_{\text{EA ticket}}) =  \text{Var}\left(\mathcal{\frac{R}{(1+d)^m}}\right) =  \frac{\text{Var}(\mathcal{R})}{(1+d)^{2m}}
</div>
<p><strong>Calculating</strong> <span class="math">NPV_{\mathcal{R}}</span>, <span class="math">E[V_{\text{sET}}]</span>, <span class="math">E[V_{\text{all sETs}}]</span> and  <span class="math">\text{Var}(V_{\text{sET}})</span></p>
<p>The proofs can be found in Jonah’s “<a href="https://arxiv.org/abs/2404.04262">Future of MEV</a>” paper. Remember, the paper does not make the sET vs. ET distinction.</p>
<p><strong>Calculating</strong> <span class="math">E[V_{\text{ET}}]</span>, <span class="math">E[V_{\text{all ETs}}]</span> and  <span class="math">\text{Var}(V_{\text{ET}})</span>,</p>
<p>These are simple modifications to the sET calculations using the <span class="math">m</span> slot discount.</p>
<p><strong>Calculating Graph 1:</strong></p>
<p>The probability of winning <span class="math">m</span> consecutive slots when holding <span class="math">p</span> percent of the sETs/ETs initially (without rebuying a ticket each block) is determined by the product of the probabilities of winning each individual draw:</p>
<div class="math">
\begin{align*}W &amp;= \left(\frac{pn}{n}\right) \cdot \left(\frac{pn-1}{n}\right) \cdot \left(\frac{pn-2}{n}\right) \cdots \left(\frac{pn-(m-1)}{n}\right) \\&amp;= \frac{(pn)!}{(pn-m)! n^m}\end{align*}
</div>
<p><strong>Calculating Graph 2:</strong></p>
<p>See section 4.4 in the “<a href="https://arxiv.org/abs/2404.04262">Future of MEV</a>” paper.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/execution-auctions-as-an-alternative-to-execution-tickets/19894">Read full topic</a></p>
]]></content:encoded>
<pubDate>Mon, 24 Jun 2024 15:40:47 +0000</pubDate>
</item>
<item>
<title>Avoiding Accidental Liveness Faults for Based Preconfs</title>
<link>https://ethresear.ch/t/avoiding-accidental-liveness-faults-for-based-preconfs/19888</link>
<guid>https://ethresear.ch/t/avoiding-accidental-liveness-faults-for-based-preconfs/19888</guid>
<content:encoded><![CDATA[
<div> 关键词：基于预确认、意外活期性故障、保护、链条化、预验证

总结:<br />
本文讨论了在引入基于预确认（based preconfirmations）的以太坊网络中，如何解决提案者面临的意外活期性故障（liveness faults）导致的潜在损失问题。作者提出了一种利用预验证链条（preconf chaining）的方法，该机制无需修改现有协议设计，旨在防止个体提案者因偶然事件导致的活期性故障被罚。通过链条化的预验证请求，依赖关系和智能的切割条件，可以确保即使一个提案者出现问题，后续的链条也能保证预验证的履行，从而减少误罚。此外，文章还探讨了如何通过激励措施鼓励链条化行为，如共享提示和建立良好的声誉预期。总的来说，链条化预验证为提案者提供了更好的保护，增强了预验证的可靠性，促进了系统的整体稳定性。 <div>
<h1><a class="anchor" href="https://ethresear.ch#avoiding-accidental-liveness-faults-for-based-preconfs-1" name="avoiding-accidental-liveness-faults-for-based-preconfs-1"></a>Avoiding Accidental Liveness Faults for Based Preconfs</h1>
<p><em>thanks to <a href="https://x.com/drakefjustin" rel="noopener nofollow ugc">Justin Drake</a>, <a href="https://x.com/jon_charb" rel="noopener nofollow ugc">Jon Charbonneau</a>, <a href="https://x.com/lvdaniels" rel="noopener nofollow ugc">Ladislaus</a>, <a href="https://x.com/aimxhaisse" rel="noopener nofollow ugc">Sébastien Rannou</a>, <a href="https://x.com/lazyleger" rel="noopener nofollow ugc">sacha</a>, <a href="https://x.com/DrewVdW" rel="noopener nofollow ugc">Drew van Der Werff</a>, and Max Wilde from <a href="https://x.com/aestusrelay" rel="noopener nofollow ugc">Aestus</a> for thinking and review</em><br />
.<br />
.<br />
<em><strong>tl;dr:</strong> We solve one of the largest problems with based preconf opt-in from proposers: accidental liveness slashing. The mechanism we introduce requires no changes to existing based preconf protocol designs and has been under our noses the whole time. We use preconf chaining to protect individual proposers from being slashed for liveness failures.</em><br />
.<br />
.</p>
<h2><a class="anchor" href="https://ethresear.ch#background-2" name="background-2"></a>Background</h2>
<p>On Ethereum today, liveness issues with block proposals are largely accepted, and penalties are minimal. When we introduce based preconfirmations, liveness issues can mean different consequences.</p>
<p>When dealing with preconfs: from the user’s perspective, liveness faults (missing a block proposal) and safety faults (proposing a block that does not fulfill preconf commitments) are the same thing. In both scenarios, a user experiences a situation where their preconfirmation is not fulfilled.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/d/9d996b95a0c2f74f54edf8f3d3b89beda26956db.png" title="liveness faults are safety faults"><img alt="liveness faults are safety faults" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/9/d/9d996b95a0c2f74f54edf8f3d3b89beda26956db_2_406x500.png" width="406" /></a></div><p></p>
<p>Now, from the perspective of the proposer, liveness faults and safety faults are two very different things. Liveness faults may occur from a multitude of external, accidental circumstances (like power outages, wifi downtime, reorgs, spontaneous combustion) that many proposers just aren’t prepared for. On the other hand, safety faults can only occur when some party (the proposer or some delegate) acts maliciously.</p>
<p>Additionally, attributing liveness faults is difficult. Many actors within the block supply chain may be responsible for a liveness fault occurring. The complexity involved with this attribution would be nice to avoid.</p>
<p>To make proposers feel more comfortable with putting up potentially high amounts of collateral, being slashed for accidental liveness faults should be very rare if not impossible.</p>
<h2><a class="anchor" href="https://ethresear.ch#preconf-chaining-3" name="preconf-chaining-3"></a>Preconf Chaining</h2>
<p><img alt="preconf chaining" height="500" src="https://ethresear.ch/uploads/default/original/3X/5/2/524180b5ef3a6673ab62d02d5afdc1a4d0d94fe5.png" width="500" /></p>
<h3><a class="anchor" href="https://ethresear.ch#brief-assumptions-4" name="brief-assumptions-4"></a>Brief Assumptions:</h3>
<ul>
<li>(we are talking about based preconfs here, not L1 preconfs)</li>
<li>slashing conditions are expressive</li>
<li>preconf requests include L2 block number</li>
<li>“active preconfer” refers to the current preconfer (an L1 proposer or delegate in the lookahead), “next active preconfer” refers to the entity who will be the next preconfer.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#slashing-conditions-construction-5" name="slashing-conditions-construction-5"></a>Slashing Conditions Construction:</h3>
<p>We assume a slashing conditions paradigm that is similar to the one presented in <a href="https://ethresear.ch/t/credibly-neutral-preconfirmation-collateral-the-preconfirmation-registry/19634">The Preconf Registry.</a> Specifically, that slashing conditions are “smart” and expressive enough to represent the following constructions.</p>
<p>The slashing conditions are designed so that a preconfer is slashed if:</p>
<ul>
<li>they sign a preconf request about a transaction <code>A</code> and block <code>B</code>, where <code>B</code> is a future L2 block. Also signed is a list of “dependents”, a list of other preconfers (by address or other ID).</li>
<li><code>A</code> is not fulfilled in <code>B</code>, or was not fulfilled in a block prior to <code>B</code></li>
<li>All dependents have signed the same preconf request (commitments/signatures from these are required) and have not been slashed (a challenge/cooldown period is useful here).</li>
</ul>
<p>This dependent design enables a preconfer to conditionally preconfirm a transaction, based on the choices of other preconfer.</p>
<h3><a class="anchor" href="https://ethresear.ch#preconf-flow-6" name="preconf-flow-6"></a>Preconf Flow</h3>
<ul>
<li>Alice (a based L2 user) wants an inclusion preconf for a transaction <code>A</code></li>
<li>Alice <a href="https://ethresear.ch/t/the-preconfirmation-gateway-unlocking-preconfirmations-from-user-to-preconfer/18812">delivers a preconf request to the active preconfer</a></li>
<li>Some entity who obtains a preconf commitment from the active preconfer (Alice, a gateway, or even the active preconfer itself) forwards Alice’s preconf request to the next active preconfer (with a dependent on the active preconfer added) and also forwards the active preconfer’s commitment.</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/4/84a8b7a1158dfc8f844abfc5447b193b3d35f12e.png" title="diagram representing how any actor can send a chained preconf request to the next active preconfer"><img alt="diagram representing how any actor can send a chained preconf request to the next active preconfer" height="387" src="https://ethresear.ch/uploads/default/optimized/3X/8/4/84a8b7a1158dfc8f844abfc5447b193b3d35f12e_2_690x387.png" width="690" /></a></div><p></p>
<p><strong>Any actor with access to a preconf commitment may construct a chained preconf and forward it to the next active preconfer.</strong></p>
<p>Note that incentives for doing this vary:</p>
<ul>
<li><strong>preconf RPC:</strong> aka <a href="https://ethresear.ch/t/the-preconfirmation-gateway-unlocking-preconfirmations-from-user-to-preconfer/18812">The Preconfirmation Gateway</a> might chain preconfs as a public good for proposers.</li>
<li><strong>gateway:</strong> A gateway might also chain preconfs as a public good for proposers, but may also use this as a feature to attract proposers (maybe called “liveness fault protection”).</li>
<li><strong>proposers:</strong> A proposer (or node operator) might also chain preconfs themselves. Their incentive is obviously to avoid being slashed for liveness faults.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#determining-penalties-7" name="determining-penalties-7"></a>Determining Penalties</h3>
<ul>
<li>In the case where the active preconfer represents a proposer that has a liveness failure and proposes no L2 block, they wouldn’t be slashed because the preconf could still be fulfilled by the next preconfer (and the preconf request block number would match).</li>
<li>If the active preconfer proposes a block and does not fulfill the preconf request, they would be slashed for a safety fault.</li>
<li>If the active preconfer does not propose a block and the next preconfer does but does not fulfill the preconf request, the second preconfer is slashed for a safety fault.</li>
<li>If both preconfers have liveness issues, both are slashed for a safety fault. (This can be avoided by chaining beyond 2.)</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#incentivizing-chaining-8" name="incentivizing-chaining-8"></a>Incentivizing Chaining</h3>
<p>To incentivize a future active preconfer to chain preconfs, an active preconfer might share tips. Also, a reputational expectation to chain preconfs can encourage more chaining.</p>
<p>One possible way to get chaining adoption is to simply require that chaining happens. To make this practical, the future active preconfers must be able to access the preconf commitments of previous preconfers. The DA problem must be solved to make this practical, and this could be done with an external DA layer. Notably, using an external DA layer introduces dependencies on another sequencer: the DA sequencer. TBD how designs of different DA layers can work around this issue and potential censorship that might occur.</p>
<h2><a class="anchor" href="https://ethresear.ch#conclusion-9" name="conclusion-9"></a>Conclusion</h2>
<p>In this post, we focus on the benefits of chaining for proposers. Widespread chaining also increases the guarantees that users get for preconfirmations, making preconfs even more valuable. It’s a win-win!</p>
<p>Whether forced or opt-in, preconf chaining can protect proposers from being slashed for accidental liveness faults. This system can help proposers feel more comfortable opting into higher collateral requirements.</p>
<p><img alt="preconf chaining protects proposers from penalties for liveness faults" height="451" src="https://ethresear.ch/uploads/default/original/3X/c/c/cc0abe035c50a142437976c953764a60e774427a.png" width="553" /></p>
<h4><a class="anchor" href="https://ethresear.ch#references-10" name="references-10"></a>References</h4>
<ul>
<li><a href="https://ethresear.ch/t/the-preconfirmation-gateway-unlocking-preconfirmations-from-user-to-preconfer/18812#chained-preconfirmations-13">The Preconfirmation Gateway</a> by <a href="https://x.com/mteamisloading" rel="noopener nofollow ugc">mteam (me)</a> mentions chained preconfirmations as better liveness guarantees for users.</li>
<li><a href="https://ethresear.ch/t/based-preconfirmations/17353">Based preconfirmations</a> by <a href="https://x.com/drakefjustin" rel="noopener nofollow ugc">Justin Drake</a> introduces a simple design for based preconfs.</li>
<li><a href="https://ethresear.ch/t/pre-confirmation-liveness-slashing-penalties-from-the-proposers-perspective/19884">Pre-confirmation Liveness Slashing Penalties from the Proposer’s Perspective</a> by <a href="https://x.com/aimxhaisse" rel="noopener nofollow ugc">Sébastien Rannou</a> touches on the liveness slashing problem and explains how it decreases the economic viability of preconfs for proposers.</li>
</ul>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/avoiding-accidental-liveness-faults-for-based-preconfs/19888">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sat, 22 Jun 2024 18:46:11 +0000</pubDate>
</item>
<item>
<title>Pre-confirmation Liveness Slashing Penalties from the Proposer's Perspective</title>
<link>https://ethresear.ch/t/pre-confirmation-liveness-slashing-penalties-from-the-proposers-perspective/19884</link>
<guid>https://ethresear.ch/t/pre-confirmation-liveness-slashing-penalties-from-the-proposers-perspective/19884</guid>
<content:encoded><![CDATA[
<div> 关键词：liveness penalty, proposer, pre-confirmations, economic viability, missed blocks

总结:
这篇文章探讨了以太坊网络中预确认（pre-confirmations）的预设罚金对提案者（proposer）经济可行性的影响。文章指出，约0.54%的区块在过去7天内被错过，为了收支平衡，提案者需要从预确认接收的额外小费（tip）至少为错过块罚金（0.0054 ETH）的一半。模型显示，随着错过率增加，预确认小费需相应提高。文章提出了几种可能的解决方案，如基于网络错过率自动调整罚金、用户自定义罚金和小费，以及只在经济上可行时才参与预确认。然而，模型缺乏激励机制，不确定是否能鼓励提案者积极参与。 <div>
<p>Current designs around pre-confirmations involve a slashing penalty on liveness, that is if a proposer who commited to pre-confirmations misses its proposal, part of its collateral is burned or redistributed to the user that sent the pre-confirmation as a payback.</p>
<p>This post explores the liveness penalty from the point of view of proposers from an economical perspective.</p>
<h2><a class="anchor" href="https://ethresear.ch#sources-of-liveness-issues-1" name="sources-of-liveness-issues-1"></a>Sources of Liveness Issues</h2>
<p>Liveness issues are complex and can come from different actors or sources, part of them are the result of the proposer’s actions or choices, part of them don’t depend on the proposer. For example:</p>
<ul>
<li>proposing a block in time but being reorg by the next proposer,</li>
<li>failure from the relayer to send the header in time,</li>
<li>failure from the relayer to propagate the signed header in time and reveal the block to the proposer.</li>
</ul>
<p>As a result, the decision on whether to opt-in or not from a proposer perspective has to take into account an <strong>inherent</strong> risk outside of its actions. Using a statistical approach on network history sounds like an easy starting point.</p>
<h2><a class="anchor" href="https://ethresear.ch#economical-minimal-viability-2" name="economical-minimal-viability-2"></a>Economical Minimal Viability</h2>
<p>In the last 7 days on the network, about <code>0.54%</code> of slots were missed, to break-even economically (that is, for an operator to not lose or win anything in the long run), assuming the liveness fault is <code>1 ETH</code>, the minimal extra-tip of a pre-confirmation would be <code>0.0054 ETH</code>.</p>
<p>To put it in perspective, the median execution reward in the last 7 days is <code>~0.048 ETH</code>, so with <code>1 ETH</code> of collateral, the pre-confirmations would need to be about <code>10%</code> of the block’s value with the current network conditions. Using <code>P(miss)</code> as the probability to miss a block, the break-even formula is:</p>
<div class="math">
(1 - (P(miss))) * tip = P(miss) * penalty
</div>
<p>And so the minimal tip:</p>
<div class="math">
tip = {(P(miss) * penalty) \over (1 - P(miss))}
</div>
<p>With <code>1 ETH</code> as a collateral, here is the model for low probabilites of missed block with <code>P(miss) &lt; 0.025</code>:</p>
<p><img alt="download" height="455" src="https://ethresear.ch/uploads/default/original/3X/e/a/ea574d8ff641f0e75064bfc788d672f031b6a3cb.png" width="626" /></p>
<p>Zooming out up with <code>P(miss) &lt; 0.5</code>:</p>
<p><img alt="download" height="455" src="https://ethresear.ch/uploads/default/original/3X/0/6/0638f8a59181327ccce1392f2bd48663d52562aa.png" width="608" /></p>
<h2><a class="anchor" href="https://ethresear.ch#opt-in-if-economically-viable-3" name="opt-in-if-economically-viable-3"></a>Opt-in if Economically Viable</h2>
<p>One idea to make it viable at scale with little effort from proposers would be for the pre-confirmation sidecar on the proposer side to opt-in to pre-confirmations only it if the tip is above what’s economically sound given the current rate of misses on the network. For example, if in the last 24 hours the average missed block proposal is <code>0.5%</code>, only commit to pre-confirmations which tip is above <code>0.005 ETH</code>.</p>
<p>This approach requires the relayer to pass the pre-confirmation tip information to the proposer to decide whether or not to commit to pre-confirmations, or the proposer to send the minimal-tip to the builder so it can provide a block that match it.</p>
<p>The advantage of this approach is if the network is struggling at scale, the risk for a proposer to miss a slot increases, and so it makes sense for proposers to opt-out of pre-confirmations until the situation resolves. Increasing the pre-confirmer bid under such conditions makes sense as more risk is taken.</p>
<p>A disavantage is that the missed block proposal rate is an approximation: it doesn’t account for totally offline validators, or for the extra-cost involved in validating the pre-confirmation on the proposer side which can take time and increase the risks of missing the slot.</p>
<h2><a class="anchor" href="https://ethresear.ch#alternatives-4" name="alternatives-4"></a>Alternatives</h2>
<h4><a class="anchor" href="https://ethresear.ch#adjusted-liveness-penalty-5" name="adjusted-liveness-penalty-5"></a>Adjusted Liveness Penalty</h4>
<p>Instead of using a minimal tip as a way to decide if it’s viable, the liveness penalty could be dynamically adjusted to what is the minimal viable condition. The tip could then be a fixed value.</p>
<h4><a class="anchor" href="https://ethresear.ch#user-defined-liveness-penalty-6" name="user-defined-liveness-penalty-6"></a>User-Defined Liveness Penalty</h4>
<p>The user sending the pre-confirmation could also decide both the liveness penalty and the tip as suggested in <a href="https://ethresear.ch/t/user-defined-penalties-ensuring-honest-preconf-behavior/19545">User-Defined Penalties: Ensuring Honest Preconf Behavior</a>, and adjust it to what the current state of the network is/what validators accept. The assumption here is maybe for some pre-confirmations the goal is to be as soon as possible on the L1, and so, reducing the liveness penalty would increase their probabilities of being pre-confirmed. On the other hand an arbitrage pre-confirmation could prefer to opt-in for a larger liveness penalty as its opportunity would be lost if the block is missed.</p>
<h2><a class="anchor" href="https://ethresear.ch#caveats-7" name="caveats-7"></a>Caveats</h2>
<p>This simple break-even model on the proposer side has no incentive, it is unclear if it will motivate proposers to opt-in.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/pre-confirmation-liveness-slashing-penalties-from-the-proposers-perspective/19884">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 21 Jun 2024 11:44:23 +0000</pubDate>
</item>
<item>
<title>Blob Usage Strategies by Rollups and Non-rollup Applications</title>
<link>https://ethresear.ch/t/blob-usage-strategies-by-rollups-and-non-rollup-applications/19874</link>
<guid>https://ethresear.ch/t/blob-usage-strategies-by-rollups-and-non-rollup-applications/19874</guid>
<content:encoded><![CDATA[
<div> 关键词：rollup, non-rollup, blobs, type 3 transactions, cost-effectiveness

总结:
这篇文章深入研究了以太坊升级后type 3交易中携带blob的应用策略，主要关注rollup和non-rollup应用的差异。rollup应用倾向于根据自身需求选择不同的blob使用策略，如blob数量、利用率和提交频率，以平衡可用数据费用和延迟成本。非-rollup应用通常用于上传完整内容，其blob策略与rollup不同，交易频繁且单blob利用率较高。

文章分析了blob成本效益、gas价格波动对不同应用的影响，以及blob与块重组的关系。结果表明，虽然blob作为数据可用性解决方案通常更经济，但在某些情况下，calldata可能更便宜。此外，blob gas价格的短期波动主要受non-rollup应用驱动，而非rollup应用对价格变动的响应并不明显。

总结:<br />rollup和non-rollup应用的blob策略各有特点，rollup通过调整blob数量和利用率来平衡成本；non-rollup则以高频次、低利用率的方式使用blob。文章还探讨了blob成本、gas价格和块重组之间的关系，发现blob滥用的识别对设计反滥用机制至关重要。 <div>
<p><a href="https://0xpantarhei.substack.com/p/blob-usage-strategies" rel="noopener nofollow ugc">Full Report</a></p>
<h2><a class="anchor" href="https://ethresear.ch#tdlr-1" name="tdlr-1"></a>TDLR</h2>
<ol>
<li>The main applications using blobs are rollups, accounting for approximately 87%. Non-rollup applications mainly include <a href="https://blobscan.com/tx/0x3ff787f16ad6d65dc8d6e45a3ea95440fca2da2c0e344e76a6e509857673da01" rel="noopener nofollow ugc">Blobscriptions</a> and <a href="https://blobscan.com/tx/0x1956039b71bbc1c5de31ceafb27782eb2e8a07c9299d1d534e470bcf35f9835a" rel="noopener nofollow ugc">customized type 3 transactions</a>.</li>
<li>Rollup applications choose different blob usage strategies according to their own situations. The strategies will consider the number of blobs carried by type 3 transactions, blob utilization, and blob submission frequency to balance the costs of availability data fees and delay costs.</li>
<li>Non-rollup applications can be characterized and distinguished from rollup applications by the number of blobs carried by type 3 transactions, blob utilization, and blob submission frequency. These features help identify scenarios of blob abuse, allowing for the design of corresponding anti-abuse mechanisms.</li>
<li>In most cases, using blobs as a data availability solution is more cost-effective than calldata. However, there are a few scenarios where calldata is cheaper: blob gas price spikes and blob utilization is extremely low.</li>
<li>Short-term fluctuations in blob gas price is mainly influenced by the demand from non-rollup applications. Rollup applications have a relatively inelastic demand for blobs, so they do not significantly impact short-term fluctuations in blob gas prices.</li>
<li>Currently, rollup applications do not seem to consider blob gas price as a reference factor in their blob usage strategies.</li>
<li>The probability of blocks containing type 3 transactions being reorganized is extremely low. Additionally, carrying more blobs does not increase the probability of block reorganization. However, there is a clustering phenomenon in block height for blocks containing type 3 transactions.</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#introduction-2" name="introduction-2"></a>Introduction</h2>
<p>This report provides an in-depth analysis of type 3 transactions used for carrying blobs from the time of the Ethereum Decun upgrade until May 22, 2024. It focuses on blob usage strategies of rollup and non-rollup applications. The dataset, data processing programs, and visualization code for this report are <a href="https://github.com/doublespending/EIP-4844-Data-Analysis" rel="noopener nofollow ugc">open source</a>, detailed in the following “Dataset” section.</p>
<h2><a class="anchor" href="https://ethresear.ch#type-3-transactions-blobs-share-by-applications-3" name="type-3-transactions-blobs-share-by-applications-3"></a>Type 3 Transactions &amp; Blobs Share by Applications</h2>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/1/9101b16c984217aa1e5a51a59e7c0024aa6e8e18.jpeg" title="image"><img alt="image" height="363" src="https://ethresear.ch/uploads/default/optimized/3X/9/1/9101b16c984217aa1e5a51a59e7c0024aa6e8e18_2_690x363.jpeg" width="690" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#rollup-applications-4" name="rollup-applications-4"></a>Rollup Applications</h3>
<p>Observations from Figure 1 on the proportion of type 3 transactions:</p>
<ul>
<li>Base, Scroll, Linea, and Starknet are in the same tier, having the highest transaction proportions.</li>
<li>Arbitrum, Optimism, and Zksync are in the next tier, having the second-highest transaction proportions.</li>
</ul>
<p>This phenomenon seems counterintuitive as Arbitrum and Optimism have higher TPS than Scroll, Linea, and Starknet and should have a higher proportion of type 3 transactions.</p>
<p>Figure 2 shows that counterintuitive phenomenon is caused by different rollup strategies in the number of blobs carried by type 3 transactions.</p>
<p>Observations from Figure 2 on the proportion of blobs:</p>
<ul>
<li>Base stands alone, having the highest proportion of blobs.</li>
<li>Arbitrum and Optimism are in the same tier, having the second-highest proportion of blobs.</li>
<li>Scroll, Linea, Starknet, and Zksync are in the same tier, having a medium proportion of blobs.</li>
</ul>
<p>This phenomenon aligns more with intuition: blob proportions are directly related to the scale of rollup’s availability data, thus showing a positive correlation with rollup TPS.</p>
<p>The difference between the proportion of type 3 transactions (31%) and blobs (14%) for non-rollup applications indicates that non-rollup applications and rollup applications have different needs.</p>
<h3><a class="anchor" href="https://ethresear.ch#non-rollup-applications-5" name="non-rollup-applications-5"></a>Non-Rollup Applications</h3>
<ul>
<li>Rollup applications are B2B businesses aiming to fill fine-grained Layer 2 transaction availability data, so their type 3 transactions are not limited to carrying only 1 blob.</li>
<li>Non-rollup applications are B2C businesses aiming to upload complete text, images, etc., so their type 3 transactions usually carry only 1 blob to meet their needs.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#rollup-blob-usage-strategies-6" name="rollup-blob-usage-strategies-6"></a>Rollup Blob Usage Strategies</h2>
<h3><a class="anchor" href="https://ethresear.ch#rollup-strategy-model-7" name="rollup-strategy-model-7"></a>Rollup Strategy Model</h3>
<p>This section models the rollup blob usage strategies with</p>
<ol>
<li><code>blobNumber</code>, i.e. the number of blobs carried by type 3 transactions</li>
<li><code>blobUtilization</code>, i.e. blob space utilization</li>
<li><code>blobInterval</code>, i.e. the blob submission interval</li>
</ol>
<h4><a class="anchor" href="https://ethresear.ch#fee-cost-8" name="fee-cost-8"></a>Fee Cost</h4>
<p>The fee cost per transaction for rollups is expressed as:</p>
<div class="math">
\begin{equation}
feeCost = \frac{1}{k}(\frac{blobCost}{blobUtilization}+\frac{fixedCost}{blobNumber*blobUtilization})
\end{equation}
</div>
<ul>
<li><code>fixedCost</code>: the fixed cost of a type 3 transaction</li>
<li><code>blobCost</code>: the cost of a single blob</li>
<li>The larger the <code>blobUtilization</code>, the lower the amortized cost of the blob fee <span class="math">\frac{blobCost}{blobUtilization}</span> and the fixed cost <span class="math">\frac{fixedCost}{blobNumber*blobUtilization}</span>, resulting in a lower fee cost <code>feeCost</code>.</li>
<li>The larger the <code>blobNumber</code>, the lower the amortized cost of the fixed cost <span class="math">\frac{fixedCost}{blobNumber*blobUtilization}</span>, resulting in a lower fee cost <code>feeCost</code>.</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#delay-cost-9" name="delay-cost-9"></a>Delay Cost</h4>
<p><strong>The delay cost per transaction for rollups is expressed as:</strong></p>
<div class="math">
\begin{equation}
delayCost = F(\frac{blobNumber*blobUtilization*k}{tps})
\end{equation}
</div>
<ul>
<li>The larger the <code>blobUtilization</code>, the larger the delay cost <code>delayCost</code>.</li>
<li>The larger the <code>blobNumber</code>, the larger the delay cost <code>delayCost</code>.</li>
<li>The larger the <code>tps</code>, the smaller the delay cost <code>delayCost</code>.</li>
</ul>
<blockquote>
<p>The derivation of the formula can be found in the <a href="https://0xpantarhei.substack.com/p/blob-usage-strategies" rel="noopener nofollow ugc">full version</a>.</p>
</blockquote>
<h3><a class="anchor" href="https://ethresear.ch#rollup-strategy-analysis-10" name="rollup-strategy-analysis-10"></a>Rollup Strategy Analysis</h3>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/c/cc978c93e42157bd63c06de9c0637fc887dccced.png" title="image"><img alt="image" height="260" src="https://ethresear.ch/uploads/default/optimized/3X/c/c/cc978c93e42157bd63c06de9c0637fc887dccced_2_690x260.png" width="690" /></a></div><p></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/2/32521830fd7aab2cbd7f19d504720344afb2eff7.jpeg" title="image"><img alt="image" height="499" src="https://ethresear.ch/uploads/default/optimized/3X/3/2/32521830fd7aab2cbd7f19d504720344afb2eff7_2_574x499.jpeg" width="574" /></a></div><p></p>
<h3><a class="anchor" href="https://ethresear.ch#non-rollup-blob-strategies-11" name="non-rollup-blob-strategies-11"></a>Non-Rollup Blob Strategies</h3>
<p>Rollup applications are B2B, while non-rollup applications are B2C. Therefore, non-rollup applications differ from the rollup strategy model. For non-rollup applications:</p>
<ul>
<li>The number of blobs carried by type 3 transactions depends on the size of the content (texts/images) stored in the blobs.</li>
<li>Blob utilization depends on the size of the content (texts/images) stored in the blobs.</li>
<li>Blob submission intervals depend on the immediate needs of C-end users, with no delay costs involved.</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/c/cc978c93e42157bd63c06de9c0637fc887dccced.png" title="image"><img alt="image" height="260" src="https://ethresear.ch/uploads/default/optimized/3X/c/c/cc978c93e42157bd63c06de9c0637fc887dccced_2_690x260.png" width="690" /></a></div><p></p>
<ul>
<li>
<p>According to Figure 5 (<strong>Others</strong> ), 1 blob can meet the needs of most non-rollup applications.</p>
</li>
<li>
<p>According to Figure 6 (<strong>Others</strong> ), the blob utilization is concentrated between 20% and 40%, indicating that non-rollup applications generally cannot fill the blob, with the data size mainly between 25.6 kB and 51.2 kB.</p>
</li>
<li>
<p>According to Figure 7 (<strong>Others</strong> ), about 83% of blobs have a submission interval of less than 1 minute, indicating a relative high frequency of user demand for non-rollup applications.</p>
</li>
</ul>
<p>In summary, the type 3 transactions for non-rollup applications can be characterized as: <strong>high-frequency transactions carrying 1 low-utilization blob</strong> .</p>
<p>The essence of this characterization is that non-rollup applications are driven by immediate needs and are less concerned about the fee cost per data byte compared to rollup applications.</p>
<p>This characterization allows for the identification of non-rollup applications, which in turn helps design mechanisms to limit blob abuse by non-rollup applications.</p>
<h2><a class="anchor" href="https://ethresear.ch#is-using-blobs-always-more-cost-effective-than-calldata-12" name="is-using-blobs-always-more-cost-effective-than-calldata-12"></a>Is Using Blobs Always More Cost-effective than Calldata?</h2>
<p>Introducing <code>feeRatio</code> to measure the relative advantages of the two solutions:</p>
<div class="math">
\begin{equation}
feeRatio = \frac{calldataFeeCost }{blobFeeCost}
\end{equation}
</div>
<ul>
<li>When <code>feeRatio</code> ≥ 1, it indicates that using blobs as a data availability solution is not worse than calldata.</li>
<li>When <code>feeRatio</code> &lt; 1, it indicates that using blobs as a data availability solution is worse than calldata.</li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/5/b5e1ca6b02490795bf2e742ecb92586d1b18e685.jpeg" title="image"><img alt="image" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/b/5/b5e1ca6b02490795bf2e742ecb92586d1b18e685_2_472x500.jpeg" width="472" /></a></div><p></p>
<p>Figure 8 also shows a few cases where <code>feeRatio</code> &lt; 1 (red), indicating that calldata is more cost-effective than blobs:</p>
<ul>
<li>Mostly in non-Rollup applications (<strong>Others</strong>):
<ul>
<li>Non-rollup applications generally do not care about the cost differences between blobs and calldata; they care about using blobs itself, such as in Blobscriptions.</li>
</ul>
</li>
<li>A few in Metal rollup:
<ul>
<li>Rollup application Metal seems not to have considered switching between blobs and calldata in its strategy, leading to suboptimal choices in some extreme cases.</li>
<li>Extreme cases are mainly due to Metal’s low blob utilization (see Figure 6) coinciding with a spike in blob gas prices.</li>
<li>However, given that extreme scenarios are rare and maintaining two data availability solutions is costly, Metal’s suboptimal strategy in extreme cases seems acceptable.</li>
</ul>
</li>
</ul>
<blockquote>
<p>The analysis of blob and calldata solutions in this section only considers fee costs and not delay costs. Considering delay costs, calldata has an actual advantage.</p>
</blockquote>
<h2><a class="anchor" href="https://ethresear.ch#blob-gas-price-and-blob-usage-strategies-13" name="blob-gas-price-and-blob-usage-strategies-13"></a>Blob Gas Price and Blob Usage Strategies</h2>
<h3><a class="anchor" href="https://ethresear.ch#analysis-of-blob-gas-price-fluctuations-14" name="analysis-of-blob-gas-price-fluctuations-14"></a>Analysis of Blob Gas Price Fluctuations</h3>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/8/58dd45e0e206936eb5eb2b32fc343a70322254c1.jpeg" title="image"><img alt="image" height="363" src="https://ethresear.ch/uploads/default/optimized/3X/5/8/58dd45e0e206936eb5eb2b32fc343a70322254c1_2_690x363.jpeg" width="690" /></a></div><br />
Figures 9 and 10 show that in scenarios of high blob gas prices (&gt; 10), the proportion of non-rollup applications (<strong>Others</strong>) is significantly higher than in scenarios of low blob gas prices (&lt; 10).<p></p>
<p>Therefore, it can be concluded that the surge in blob gas prices is mainly driven by the demand from non-rollup applications, rather than rollup applications. Otherwise, the proportion of rollup and non-rollup applications should remain stable.</p>
<h3><a class="anchor" href="https://ethresear.ch#how-rollups-respond-to-blob-gas-price-fluctuations-15" name="how-rollups-respond-to-blob-gas-price-fluctuations-15"></a>How Rollups Respond to Blob Gas Price Fluctuations</h3>
<p><em>Hypothesis 1: The higher the blob gas price, to reduce fee costs, applications should carry more blobs in type 3 transactions, i.e., the number of blobs should be positively correlated with blob gas prices.</em><br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/f/b/fb34ad1ab0fcf6250662d82b007a763309889ef7.jpeg" title="image"><img alt="image" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/f/b/fb34ad1ab0fcf6250662d82b007a763309889ef7_2_505x500.jpeg" width="505" /></a></div><p></p>
<p>Figure 14 shows that the hypothesis does not hold.</p>
<p><em>Hypothesis 2: The higher the blob gas price, to reduce fee costs, applications should increase blob utilization, i.e., blob utilization should be positively correlated with blob gas prices.</em><br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/2/521bb465406b224d50b0117150169a5991c5029c.jpeg" title="image"><img alt="image" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/5/2/521bb465406b224d50b0117150169a5991c5029c_2_498x500.jpeg" width="498" /></a></div><p></p>
<p>Figure 15 shows that the hypothesis does not hold.</p>
<p><em>Hypothesis 3: The higher the blob gas price, to reduce fee costs, applications should delay blob submissions, i.e., blob submission intervals should be positively correlated with blob gas prices.</em></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/e/9e1dd1bbb0bad1163b9eaf7f8d61f340279bb0fd.jpeg" title="image"><img alt="image" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/9/e/9e1dd1bbb0bad1163b9eaf7f8d61f340279bb0fd_2_514x500.jpeg" width="514" /></a></div><p></p>
<p>Figure 16 shows that the hypothesis does not hold.</p>
<blockquote>
<p>In Figures 9 and 10, readers might notice that some rollup applications seem to respond to high blob gas prices. Scroll seems to suspend blob submissions under high blob gas prices. However, this conclusion is incorrect. The reason is that not all rollups immediately used blobs after the EIP-4844 upgrade.</p>
</blockquote>
<h2><a class="anchor" href="https://ethresear.ch#blobs-and-block-reorg-16" name="blobs-and-block-reorg-16"></a>Blobs and Block Reorg</h2>
<p>From the Decun upgrade to May 22, there were 171 type 3 transactions included in the forked blocks and 348,121 included in the canonical blocks. Therefore, the proportion of type 3 transactions being forked is approximately 0.049%. This section explores the relationship between block reorg and blob.</p>
<h3><a class="anchor" href="https://ethresear.ch#blob-number-distribution-in-the-canonical-and-forked-blocks-with-blobs-17" name="blob-number-distribution-in-the-canonical-and-forked-blocks-with-blobs-17"></a>Blob Number Distribution in the Canonical and Forked Blocks with Blobs</h3>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/e/bef1c025b4ae7c6990e2c7968acf12a6eccba1a2.jpeg" title="image"><img alt="image" height="403" src="https://ethresear.ch/uploads/default/optimized/3X/b/e/bef1c025b4ae7c6990e2c7968acf12a6eccba1a2_2_690x403.jpeg" width="690" /></a></div><p></p>
<p><em>Hypothesis: More blobs increase the probability of block reorganizations.</em></p>
<p>If the hypothesis holds, the following inequality should be satisfied:</p>
<div class="math">
\begin{equation}
P(reorg|blob=n)  &gt; P(reorg|blob=n-1)
\end{equation}
</div>
<p>According to <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener nofollow ugc">Bayes’ theorem</a>, inequality above is equivalent to:</p>
<div class="math">
\begin{equation}
\frac{P(blob=n|reorg)}{P(blob=n)}  &gt; \frac{P(blob=n-1|reorg)}{P(blob=n-1)}
\end{equation}
</div>
<p>We check whether the actual data satisfies inequality and obtain the following table:<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/c/ec253f7881bbf00cf3b5a37a8635dfb0181309ee.png" title="image"><img alt="image" height="201" src="https://ethresear.ch/uploads/default/optimized/3X/e/c/ec253f7881bbf00cf3b5a37a8635dfb0181309ee_2_690x201.png" width="690" /></a></div><p></p>
<p>The table above shows that equation (10) does not hold for all <code>n</code>. Therefore, the hypothesis does not hold, indicating that more blobs are not significantly related to block reorganizations.</p>
<h3><a class="anchor" href="https://ethresear.ch#distribution-of-type-3-transactions-and-blobs-by-applications-in-the-canonical-and-forked-blocks-with-blobs-18" name="distribution-of-type-3-transactions-and-blobs-by-applications-in-the-canonical-and-forked-blocks-with-blobs-18"></a>Distribution of Type 3 Transactions and Blobs by Applications in the Canonical and Forked Blocks with Blobs</h3>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/d/ddfc7f0d5d5b2a90aaf6efff87b6d7a3733c2aff.jpeg" title="image"><img alt="image" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/d/d/ddfc7f0d5d5b2a90aaf6efff87b6d7a3733c2aff_2_425x500.jpeg" width="425" /></a></div><br />
Figures 18 and 19 show that the proportion of type 3 transactions/blobs for Zksync and Scroll in forked blocks is significantly higher than in the canonical blocks.<p></p>
<p>Applications seem to have some connection with block reorganizations, possibly related to differences in blob usage strategies by applications:</p>
<ul>
<li>Zksync and Scroll are less strategic in selecting the timing of submitting type 3 transactions, targeting block heights prone to reorganization.</li>
<li>The unique characteristics of Zksync and Scroll’s type 3 transactions make the blocks containing them more likely to be reorganized.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#clustering-phenomenon-of-forked-blocks-with-blobs-19" name="clustering-phenomenon-of-forked-blocks-with-blobs-19"></a>Clustering Phenomenon of Forked Blocks with Blobs</h3>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/2/8/281e3d3c49f900b77406ef467f2c32a1b08331eb.jpeg" title="image"><img alt="image" height="286" src="https://ethresear.ch/uploads/default/optimized/3X/2/8/281e3d3c49f900b77406ef467f2c32a1b08331eb_2_690x286.jpeg" width="690" /></a></div><br />
If each block has the same probability of being reorganized, the forked blocks should be evenly distributed across the block height range. However, Figure 20 shows a clustering phenomenon in block heights for forked blocks, possibly related to network conditions.<p></p>
<p>In addition, the clustering phenomenon that occurs in block reorganization seems to be somewhat related to the applications that submit blobs. For example, type 3 transactions for non rollup applications are only included in forked blocks between 19500000 and 19600000.</p>
<aside class="onebox allowlistedgeneric">
  <header class="source">
      <img class="site-icon" height="64" src="https://ethresear.ch/uploads/default/original/3X/3/0/30aea33d55bd45ce96fab5bf70ecd7a3d0178f9d.png" width="64" />

      <a href="https://0xpantarhei.substack.com/p/blob-usage-strategies" rel="noopener nofollow ugc" target="_blank">0xpantarhei.substack.com</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image"><img class="thumbnail" height="345" src="https://ethresear.ch/uploads/default/optimized/3X/f/7/f7108d3b335a7303d071fa1c0b9afa898ea3fa24_2_690x345.jpeg" width="690" /></div>

<h3><a href="https://0xpantarhei.substack.com/p/blob-usage-strategies" rel="noopener nofollow ugc" target="_blank">Blob Usage Strategies by Rollups and Non-rollup Applications</a></h3>

  <p>This report provides an in-depth analysis of type 3 transactions used for carrying blobs.</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both;"></div>
</aside>

            <p><small>3 posts - 2 participants</small></p>
            <p><a href="https://ethresear.ch/t/blob-usage-strategies-by-rollups-and-non-rollup-applications/19874">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 20 Jun 2024 03:39:04 +0000</pubDate>
</item>
<item>
<title>Block Building is not just knapsack!</title>
<link>https://ethresear.ch/t/block-building-is-not-just-knapsack/19871</link>
<guid>https://ethresear.ch/t/block-building-is-not-just-knapsack/19871</guid>
<content:encoded><![CDATA[
<div> 关键词：区块链、块构建、NP-hard、贪婪算法、交易冲突

总结:
本文探讨了区块链中块构建的复杂性，通过将问题与背包问题和最大独立集问题关联，揭示其NP-hard性质。研究者提出几种贪婪算法，包括经典和改进版本，以及如何考虑交易间的冲突。实验结果显示，通过结合背包约束优化的贪婪算法比现有方法能多赚约15%的费用。文章还讨论了模型对以太坊矿工的实际意义，以及未来研究方向，如更精确的序列化问题和交易效用影响的建模。 <div>
<p>Authors: <a class="mention" href="https://ethresear.ch/u/mikerah">@Mikerah</a> Afonso <a class="mention" href="https://ethresear.ch/u/sarisht">@sarisht</a></p>
<p>Shoutout to Gabearro Ventalitan Nerla Yun Qi and Surya for all the vibes and discussions!</p>
<p>This project was done as a Hackathon Project at IC3 camp last week.</p>
<h2><a class="anchor" href="https://ethresear.ch#tldr-1" name="tldr-1"></a>TL;DR</h2>
<p>We present a formal model or block building in blockchains. We show that block building is at least a combination of the Knapsack problem and the Maximum Independent Set problem, thus showing that block building is an NP-hard problem. Next, we provide various greedy algorithms with different tradeoffs. Then, we show simulation results to justify the algorithms and benchmarks. Our results show that tweaking the greedy solution with the results of the known knapsack constraint outperforms the currently used greedy algorithm by ~15% in terms of fee earned. Finally, we discuss how this is relevant for block builders in Ethereum in practice and directions for future research.</p>
<h2><a class="anchor" href="https://ethresear.ch#introduction-2" name="introduction-2"></a>Introduction</h2>
<p>Block building in Ethereum has evolved into a multimillion-dollar industry, particularly with the introduction of MEV-Boost. This has significantly increased the revenue earned by the builders. However, the builders’ algorithm for selecting transactions and transaction bundles needs more study. In collaboration with Flashbots, Mikerah (group lead for the project) has recently worked on a project that <a href="https://collective.flashbots.net/t/frp-10-distributed-blockbuilding-networks-via-secure-knapsack-auctions/1955" rel="noopener nofollow ugc">formalizes the model for block building as a knapsack problem</a>. This model considers each transaction’s utility (the fee offered by the transaction) and cost (the gas used by the transaction), with a budget for the maximum price that can be paid (the gas limit for the block). The practical relevance of this research is evident, as it addresses a significant limitation of the current model, where not all transactions are independent of each other.</p>
<h2><a class="anchor" href="https://ethresear.ch#the-problem-3" name="the-problem-3"></a>The Problem</h2>
<p>Let’s delve into the heart of the matter by examining why transactions are not independent, a key challenge in block building.</p>
<h3><a class="anchor" href="https://ethresear.ch#bitcoin-blockchain-4" name="bitcoin-blockchain-4"></a>Bitcoin Blockchain</h3>
<p>The most critical problem described in the Satoshi Nakamoto blockchain paper was catching double-spending. If two transactions try to spend the same UTXO, only one of them should make it on-chain. Thus, we can see that some transactions are dependent on each other. However, that is not all; some transactions that interact with Bitcoin’s OP-Code design can also depend on each other. A classic example of this would be that in an HTLC, either a refund transaction (released by revealing a pre-image of a hash) or payment (released when the timelock on the transaction expires) can go through. If both transactions are simultaneously in the mempool, then the transactions conflict with each other.</p>
<h3><a class="anchor" href="https://ethresear.ch#ethereum-blockchain-5" name="ethereum-blockchain-5"></a>Ethereum Blockchain</h3>
<p>Ethereum inherits the double-spending transaction problem, but owing to its smart contract and gas fee design, it only partially suffers from the other type of conflict since the fee is paid based on the gas used. This causes the model to shift slightly, where the fee paid and the gas used depends on other chain transactions. Further, in the presence of searchers, some transactions are bundled such that multiple bundles contain the same transaction and thus cannot be included in the block simultaneously.</p>
<h2><a class="anchor" href="https://ethresear.ch#model-6" name="model-6"></a>Model</h2>
<p>We first introduce the assumptions we make before describing the mathematical formulations.</p>
<h4><a class="anchor" href="https://ethresear.ch#assumptions-7" name="assumptions-7"></a>Assumptions</h4>
<ul>
<li>Dependent fees and gas are hard to model since we cannot have a boolean representation. Thus, we only consider “Conflicts” and touch upon “Dependency.” Conflicts are situations in which the transactions cannot occur together, and dependency is when one transaction requires another transaction to be executed before it is valid.</li>
<li>We further ignore the optimal ordering of transactions inside a block. Ordering transactions in a particular order can lead to higher profits due to MEV, which we ignore for the same reason as above.</li>
<li>For Ethereum, under the conditions of EIP 1559, the fee considered is the part above the base fee. Any transaction with a negative fee is ignored.</li>
</ul>
<p>Given these assumptions, we now model the binary allocation problem with constraints and dependencies as follows:<br />
Let <span class="math">T</span> be the set of transactions. A transaction in <span class="math">T</span> is denoted by <span class="math">tx_i</span>.<br />
Let <span class="math">f_i</span> denote the fee associated with a transaction <span class="math">tx_i</span>.<br />
Let <span class="math">g_i</span> denote the gas associated with a transaction <span class="math">t_i</span><br />
Let <span class="math">B</span> be the maximum block gas limit.</p>
<p>Then, we have the following optimization problem<br />
Maximise</p>
<div class="math">
\sum_{i\in n} f_ix_i 
</div>
<p>Subject to</p>
<div class="math">
\begin{align*}
 &amp;\sum_{i\in n} x_ig_i \leq B \\
 &amp; x_i+x_j \leq C_{ij}, \forall i\neq j \in n\\
 &amp; x_j - x_i \leq M_{ij}, \forall i\neq j \in n\\
 &amp; x_i \in \{0,1\}
 \end{align*}
</div>
<p>where,</p>
<ul>
<li><span class="math">C_{ij} = 1</span> if <span class="math">t_i</span> and <span class="math">t_j</span> are conflicting transactions, 2 otherwise</li>
<li><span class="math">M_{ij} = 0</span> if <span class="math">t_j</span> depends on <span class="math">t_i</span> and can only be allocated after <span class="math">t_i</span>, 1 otherwise</li>
</ul>
<p>Since, in practice, it is hard for a block builder to infer the 3rd condition (without executing all of the transactions) within a limit snapshot of the transactions within their order flow pools, we can omit the 3rd constraint to simplify the problem. If the builder comes across such a transaction, it would be considered invalid.</p>
<p>As such, we can obtain the following simplified optimization problem<br />
Maximise</p>
<div class="math">
\sum_{i\in n} f_ix_i
</div>
<p>Subject to</p>
<div class="math">
\begin{align*}
 &amp;\sum_{i\in n} x_ig_i \leq BL \\
 &amp; x_i+x_j \leq C_{ij}, \forall i\neq j \in n\\
 &amp; x_i \in \{0,1\}
 \end{align*}
</div>
<p>where,</p>
<ul>
<li><span class="math">C_{ij} = 1</span> if <span class="math">t_i</span> and <span class="math">t_j</span> are conflicting transactions, 2 otherwise</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#reductions-8" name="reductions-8"></a>Reductions</h3>
<p>Now, we present formal arguments as to why block building is an instance of the knapsack problem and the maximum independent set problem.</p>
<h4><a class="anchor" href="https://ethresear.ch#reduction-to-knapsack-9" name="reduction-to-knapsack-9"></a>Reduction to knapsack</h4>
<p>The reduction of the above problem to knapsack is easy to see. We assume no conflicts arise amongst any transactions. In that case, the problem is the same as solving a knapsack problem, with the utility as the fee paid by the transaction, space occupied as the gas used by a transaction, and finally, the block’s gas limit determines the knapsack size. Thus, the block-building problem is at least as hard as the knapsack problem.</p>
<h4><a class="anchor" href="https://ethresear.ch#reduction-to-maximum-independent-set-10" name="reduction-to-maximum-independent-set-10"></a>Reduction to Maximum Independent Set</h4>
<p>If we can solve the above instance of block building problem without any constraint that limits the size of the block in polynomials, then consider the following instance where the block gas limit is set to the sum of gas of all transactions in the mempool. This would imply enough space for all the transactions in the mempool to fit in the block. This problem is now equivalent to finding the maximum weighted independent set because we can consider all transactions as vertices, and an edge exists between two vertices if the corresponding transactions conflict. The above reduction creates the instantiation of the maximum weighted independent set problem, which is again known as NP-hard.</p>
<h2><a class="anchor" href="https://ethresear.ch#algorithms-for-approximate-result-11" name="algorithms-for-approximate-result-11"></a>Algorithms for approximate result</h2>
<p>As we mentioned above, block building is an NP-hard problem with reductions to both the knapsack problem and the maximum weighted independent set problem. Since we know that the maximum weighted independent set problem doesn’t have a C-approximation, this implies that the block-building problem also doesn’t have a C-approximation.</p>
<p>As such, we devise several greedy algorithms in order to solve the block-building problem in practice.</p>

<h3><a class="anchor" href="https://ethresear.ch#greedy-classic-gc-12" name="greedy-classic-gc-12"></a>Greedy Classic (GC)</h3>
<p>We expect today’s builders to use the first algorithm we present. It follows the most widely used knapsack greedy solution, where all objects are sorted according to the ratio of their utility to cost, and then greedily allocate space to each object until you can no longer allocate more space. Due to the added conflict constraint, the builder must check for conflict with any transaction already added to the block. Thus, the algorithm works as follows:</p>
<p>Algorithm input: <span class="math">T = \{t_i\}, F = \{f_i\}, G = \{g_i\}</span><br />
Algorithm output: An ordered block with gas used less than BL<br />
Algorithm description:</p>
<pre><code class="lang-auto">Sort T by corresponding F/G
Let B  := {}
Let BS := 0
For each t in T, f in F, g in G do:
    if t has any conflict with tx in B: continue;
    if g + BS &lt; BL: B.append(t); BS += g
return B
</code></pre>
<p>In practice, the conflict between transactions is only known if simulated sequentially. We propose two constraints on how this conflict can be modeled.</p>
<ul>
<li>Two transactions <span class="math">t_1</span> and <span class="math">t_2</span> conflict if the transactions cannot be executed together. This can happen if some address is trying to double-spend some money it has or if two searcher bundles try to extract MEV from the transaction. We call this conflict a “Real” conflict.</li>
<li>Two transactions <span class="math">t_1</span> and <span class="math">t_2</span> conflict if they interact with the same address. We call this conflict an “All” condition. These transactions do not necessarily invalidate each other. Still, we keep this as a potential conflict condition since this conflict is more straightforward to determine (constant size operation) than the other constraint (gas size operation), and thus can be helpful for builders optimizing based on the time computing is used.</li>
</ul>
<p>Note: In the solution simulation, we assume that <span class="math">p=0.95</span> of transactions in the “All” conflict are not in the “Real” conflict.</p>
<p>Based on the definition of conflicts, we present the two baseline greedy solutions, which we label CG All and CG Real.</p>
<h2><a class="anchor" href="https://ethresear.ch#knapsack-greedy-13" name="knapsack-greedy-13"></a>Knapsack Greedy</h2>
<p>The greedy solution described above is not a good approximation solution. Looking back at the knapsack problem, we get a 1/2 approximation over the optimal solution by comparing the above classic greedy with the utility of the first object that was not allocated.</p>
<p>The algorithm begins by running an instance of the greedy classic. It then finds the highest paying (highest f/g) transaction and adds it to the block. Adding this transaction would require modification of the block since some transactions in block (B) conflicted with this transaction, or the transaction could not be inserted due to insufficient space. Thus, we remove transactions that conflict with this new addition and then make enough space to add this transaction. After inserting the transaction, we repeat the greedy insertion until the block is again full. We repeat the above algorithm until we have seen each transaction at least once over the greedy solution.</p>
<p>The pseudocode for the solution is as follows:</p>
<pre><code class="lang-auto">Sort T by corresponding F/G
Let B  := {}
Let B_f:= {}
Let S  := {}
Let BS := 0
while S != T: 
    let t := t in T, not in S, with maximum f/g:
    remove any transaction from B that conflicts with t.
    remove smallest f/g txs until there is space to insert t.
    B.append(t)
    S.append(t)
    For each t in T, f in F, g in G do:
        if t has any conflict with tx in B: continue;
        if g + BS &lt; BL: B.append(t); BS += g; S.append(t)
    if sum(B.f) &gt; sum(B_f.f): B_f = B

return B_f

# B.f is the fee corresponding to each transaction in B
</code></pre>
<p>In this greedy protocol, we attempt to enforce the inclusion of a transaction every time. It is still distinct from the greedy knapsack 1/2 approximation, but it tries to replicate what was accomplished by the knapsack greedy but for all items not picked by the greedy algorithm.</p>
<p>This solution will outperform its classic greedy counterpart since it computes maximum over all solutions, one of which is the classic greedy solution. Like the classic greedy solution, we analyze this when conflicts are “Real” and “All”.</p>
<h2><a class="anchor" href="https://ethresear.ch#classic-greedy-informed-solutions-14" name="classic-greedy-informed-solutions-14"></a>Classic Greedy Informed Solutions</h2>
<p>Solving the knapsack problem is very easy compared to all known NP-Hard problems, especially the maximum independent set condition we have been imposing. Thus, we allow the builder to solve the knapsack reasonably accurately and quickly via a BLP solver. The knapsack solution gives the builder some idea about how to build the block, and then when there are conflicting transactions in the chosen block, the “later” transactions are discarded. In this solution, we run a knapsack LP solution. On the output of the LP, we sort the output based on i) f/g ratio ii) f, and finally iii) g. The way greedy works here is that the transactions are picked in the order of the metric, and whenever there is a conflict, the LP solver is recalled, but removing constraints on the already added and the conflicting transaction (<span class="math">x_i</span> is set to 1 for all that have already been chosen and <span class="math">x_i</span> is set to 0 for the conflicting transaction). This is repeated until the block is full.</p>
<pre><code class="lang-auto">Let B  := {}
Let B_c:= {nil}
Let BS := 0
Let C  := {}
while B_c != B:
    B_c = LP.solve(sum(x.f), x.g &lt;= BL, C)
    Sort B_c by "heuristic"
    for t in B_c:
        if t has any conflict with tx in B: 
            C.add(x_t = 0)
            break;
        B.append(t)
        C.add(x_t = 1)

return B


# Replace "heurestic" by f/g for standard, 
                       f for high-value 
# Sorting is in descending order 
</code></pre>
<p>We label these transactions as CGI-f/g and CGI-f. We only analyze the “All” conflicts for this since the time to run the algorithm is potentially higher than for the other Greedy Algorithms.</p>
<h2><a class="anchor" href="https://ethresear.ch#simulation-15" name="simulation-15"></a>Simulation</h2>
<p>Due to our limited time to work on the project, we tried to replicate the transaction data synthetically instead of working with real transactions. To properly simulate Ethereum mempool transactions, we choose the following dataset:</p>
<h3><a class="anchor" href="https://ethresear.ch#dataset-16" name="dataset-16"></a>Dataset</h3>
<p>We choose 2000 transactions under this distribution.</p>
<ul>
<li>80%: SMALL: g ~ N(24k, 3k)  f/g ~ N(16,4) - These low gas-consuming transactions have minimal smart contract interactions and thus use less gas. In almost all cases, the gas fees for these transactions are small since they are usually never a priority transaction.</li>
<li>18%  : LARGE1: g ~ N(200k, 20K)  f/g ~ N(16,4) - These represent transactions that have a significant contract execution; however, in this case, these are still not priority transactions, since the user is okay to wait for some time for the contract execution.</li>
<li>2%  : LARGE2: g ~ N(200k, 20K)  f/g ~ N(40,10) - These are the priority transactions. Usually, these have high gas usage since they mostly interact with, for example, DeFi contracts and want to be executed as soon as possible.</li>
</ul>
<p>We simulate the conflicts among these transactions by randomly choosing transactions such that each transaction has a <span class="math">\sigma</span> number of conflicts. While our preliminary results constitute the same <span class="math">\sigma</span> across all types of transactions, in practice, the larger transactions, especially the high-paying ones, would have a more significant number of conflicts since usually MEV extracting bundles would be constructed around them.</p>
<h3><a class="anchor" href="https://ethresear.ch#results-17" name="results-17"></a>Results</h3>
<p>We ran our simulation over 100 blocks with the mempool created as above.</p>
<p>When we consider <span class="math">\sigma=2</span> number of conflicts per transaction, we see the following results:</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/5/45d6ba5351f45cbc8f51bd30a3637d3f1554c6f5.png" title="s2feeratio"><img alt="s2feeratio" height="499" src="https://ethresear.ch/uploads/default/optimized/3X/4/5/45d6ba5351f45cbc8f51bd30a3637d3f1554c6f5_2_668x499.png" width="668" /></a></div><br />
<div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/4/1430b955e746ba6faf056ac169b049c0e3dded9a.png" title="s2wastedgas"><img alt="s2wastedgas" height="499" src="https://ethresear.ch/uploads/default/optimized/3X/1/4/1430b955e746ba6faf056ac169b049c0e3dded9a_2_668x499.png" width="668" /></a></div><p></p>
<p>Increasing the number of conflicts each transaction had increases the problem’s difficulty. Therefore, the various greedy algorithms have a larger separation in performance:</p>
<p>For <span class="math">\sigma = 10</span>,<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/e/ae043667cfb292234612d06e14e402d2cc86b268.png" title="s10feeratio"><img alt="s10feeratio" height="499" src="https://ethresear.ch/uploads/default/optimized/3X/a/e/ae043667cfb292234612d06e14e402d2cc86b268_2_668x499.png" width="668" /></a></div><p></p>
<p>For <span class="math">\sigma = 20</span>,<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/9/79e0d8ad31f56fcb617c858775285e5e6b5b28fb.png" title="s20feeratio"><img alt="s20feeratio" height="499" src="https://ethresear.ch/uploads/default/optimized/3X/7/9/79e0d8ad31f56fcb617c858775285e5e6b5b28fb_2_668x499.png" width="668" /></a></div><p></p>
<p>For <span class="math">\sigma = 40</span>,<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/3/e333c84dc6daa57f54481113545d081b8bb2af91.png" title="s40feeratio"><img alt="s40feeratio" height="499" src="https://ethresear.ch/uploads/default/optimized/3X/e/3/e333c84dc6daa57f54481113545d081b8bb2af91_2_668x499.png" width="668" /></a></div><p></p>
<h2><a class="anchor" href="https://ethresear.ch#future-research-direction-18" name="future-research-direction-18"></a>Future Research Direction</h2>
<p>Based on our results, solving the block-building problem is an NP-Hard problem, and as long as conflicts exist amongst the transactions, it remains a complex problem.</p>
<p>However, this does not mean that all hope is lost. The block-building problem may have more potential than the Maximum Independent Set problem. Combining the space of Knapsack and Maximum Independent Set gives us a smaller search space to find a satisfactory approximate solution for the issue at hand.</p>
<p>Further, for Ethereum bundles from searchers, if <span class="math">tx_i</span> and <span class="math">tx_j</span> conflict, as well as <span class="math">tx_j</span> and <span class="math">tx_k</span> conflict, then there is a high likelihood that <span class="math">tx_i</span> and <span class="math">tx_k</span> also conflict. This eases the constraints on the solution since, amongst an all-2-all graph of transactions, for MIS, you only need to pick the transaction with the highest utility (also satisfying knapsack).</p>
<p>Another thing to note is that our algorithms can inform how block builders construct blocks in practice. Notably, the Classical Greedy Informed algorithm, in which we sort the transactions by highest fee, is closest to the optimal solution.</p>
<p>That being said, the most exciting extension to this research would be modeling the block-building problem as a job sequencing problem instead and somehow estimating how utility (fee+MEV) from one transaction affects the utility of other transactions sequenced after the first transaction.</p>
<p>On that note, we invite potential collaborators to explore new ideas for building blocks that maximize the builders’ utility.</p>
            <p><small>7 posts - 5 participants</small></p>
            <p><a href="https://ethresear.ch/t/block-building-is-not-just-knapsack/19871">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 19 Jun 2024 17:35:36 +0000</pubDate>
</item>
<item>
<title>Fork-Choice enforced Inclusion Lists (FOCIL): A simple committee-based inclusion list proposal</title>
<link>https://ethresear.ch/t/fork-choice-enforced-inclusion-lists-focil-a-simple-committee-based-inclusion-list-proposal/19870</link>
<guid>https://ethresear.ch/t/fork-choice-enforced-inclusion-lists-focil-a-simple-committee-based-inclusion-list-proposal/19870</guid>
<content:encoded><![CDATA[
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/6/3/639a5b7de796701a13a5759e8f5a1fe393f067f3.jpeg" title="DALL·E 2024-06-05 14.58.08 - A highly realistic illustration of a rock with the Ethereum symbol fossilized into it, set in a cave. The rock should appear weathered and ancient, wi"><img alt="DALL·E 2024-06-05 14.58.08 - A highly realistic illustration of a rock with the Ethereum symbol fossilized into it, set in a cave. The rock should appear weathered and ancient, wi" height="394" src="https://ethresear.ch/uploads/default/optimized/3X/6/3/639a5b7de796701a13a5759e8f5a1fe393f067f3_2_690x394.jpeg" width="690" /></a></div><br />
<em>^focil =&gt; fossil =&gt; protocol ossification</em><p></p>
<p><em>by <a href="https://ethresear.ch/u/soispoke/summary">Thomas</a>, <a href="https://ethresear.ch/u/fradamt/summary">Barnabé</a>, <a href="https://ethresear.ch/u/fradamt/summary">Francesco</a> and <a href="https://ethresear.ch/u/julian/summary">Julian</a></em> - June 19th, 2024</p>
<p><em>This design came together during a small, week long, in-person gathering in Berlin with RIG and friends to discuss censorship resistance, issuance, and Attester-Proposer-Builder-Consensus-Execution-[insert here] Separation.</em></p>
<p><em>Thanks to Luca, Terence, Toni, Ansgar, Alex, Caspar and Anders for discussions, feedback and comments on this proposal.</em></p>
<h1><a class="anchor" href="https://ethresear.ch#tldr-1" name="tldr-1"></a><strong>tldr</strong></h1>
<p>In this post, we introduce Fork-Choice enforced Inclusion Lists (FOCIL), a simple committee-based IL design.</p>
<p>FOCIL is built in three simple steps:</p>
<ol>
<li>Each slot, a set of validators is selected to become <strong>IL committee members.</strong> Each member gossips one <em>local inclusion list</em> according to their subjective view of the mempool.</li>
<li><strong>The block proposer</strong> collects and aggregates available local inclusion lists into a concise <em>aggregate</em>, which is included in its block.</li>
<li><strong>The attesters</strong> evaluate the quality of the <em>aggregate</em> given their own view of the gossiped local lists to ensure the block proposer accurately reports the available local lists.</li>
</ol>
<p>This design ensures a robust and reliable mechanism to uphold Ethereum’s censorship resistance and <a href="https://ethresear.ch/t/uncrowdable-inclusion-lists-the-tension-between-chain-neutrality-preconfirmations-and-proposer-commitments/19372">chain neutrality</a> properties, by guaranteeing timely transaction inclusion.</p>
<h1><a class="anchor" href="https://ethresear.ch#introduction-2" name="introduction-2"></a>Introduction</h1>
<p>In an effort to shield the Ethereum validator set from centralizing forces, the right to build blocks has been auctioned off to specialized entities known as builders. Over the past year, this has resulted in a few sophisticated builders dominating the network’s block production. Economies of scale have further entrenched their position, making it increasingly difficult for new entrants to gain significant market share. A direct consequence of oligopolistic block production is a deterioration of the network’s (weak) censorship resistance properties. Today, <a href="https://censorship.pics/" rel="noopener nofollow ugc">two of the top three builders</a> are actively filtering out transactions interacting with sanctioned addresses from their blocks. In contrast, 90% of the <a href="https://www.ethernodes.org/countries" rel="noopener nofollow ugc">more decentralized and heterogeneous validator set</a> is not engaging in censorship.</p>
<p>This has driven <a href="https://github.com/michaelneuder/mev-bibliography?tab=readme-ov-file#censorship-resistance" rel="noopener nofollow ugc">research</a> toward ways that allow validators to impose constraints on builders by force-including transactions in their blocks. These efforts recently culminated in the first practical implementation of forward <span class="math">\text{ILs}</span> (<span class="math">\text{fILs}</span>) being considered for inclusion in the upcoming Pectra fork (see <a href="https://ethresear.ch/t/no-free-lunch-a-new-inclusion-list-design/16389">design</a>, <a href="https://eips.ethereum.org/EIPS/eip-7547" rel="noopener nofollow ugc">EIP</a>, and <a href="https://notes.ethereum.org/@mikeneuder/il-spec-overview" rel="noopener nofollow ugc">specs</a> <a href="https://gist.github.com/michaelneuder/ba32e608c75d48719a7ecba29ec3d64b" rel="noopener nofollow ugc">here</a>). However, some concerns were raised about the specific mechanism proposed in <a href="https://eips.ethereum.org/EIPS/eip-7547" rel="noopener nofollow ugc">EIP-7547</a>, leading to its rejection.</p>
<p>Here, we introduce FOCIL, a simple committee-based design improving upon previous IL mechanisms (<a href="https://ethresear.ch/t/no-free-lunch-a-new-inclusion-list-design/16389">Forward ILs</a>, <a href="https://ethresear.ch/t/the-more-the-less-censored-introducing-committee-enforced-inclusion-sets-comis-on-ethereum/18835">COMIS</a>) or co-created blocks (<a href="https://ethresear.ch/t/concurrent-block-proposers-in-ethereum/18777">CBP</a>) and addressing issues related to <a href="https://ethresear.ch/t/fun-and-games-with-inclusion-lists/16557">bribing/extortion attacks</a>, IL equivocation, <a href="https://ethereum.org/en/roadmap/account-abstraction/" rel="noopener nofollow ugc">account abstraction</a> (AA) and incentive incompatibilities. Note also Vitalik’s recent proposal “<a href="https://ethresear.ch/t/one-bit-per-attester-inclusion-lists/19797">One-bit-per-attester inclusion lists</a>”, where the committee chosen to build the list is essentially the whole set of attesters.</p>
<h1><a class="anchor" href="https://ethresear.ch#design-3" name="design-3"></a><strong>Design</strong></h1>
<p>In this section, we introduce the core properties of the FOCIL mechanism (see <strong>Figure 1.</strong>).</p>
<h2><a class="anchor" href="https://ethresear.ch#high-level-overview-4" name="high-level-overview-4"></a><strong>High-level overview</strong></h2>
<p>Each slot, a set of validators is randomly selected to become part of an inclusion list (<span class="math">\text{IL}</span>) committee. <span class="math">\text{IL}</span> committee members are responsible for creating local inclusion lists (<span class="math">\text{IL}_\text{local}</span>) of transactions pending in the public mempool. Local <span class="math">\text{ILs}</span> are then broadcast over the global topic, and the block producer must include a canonical aggregate (<span class="math">\text{IL}_\text{agg}</span>) of transactions from the collected local <span class="math">\text{ILs}</span> in its block <span class="math">B</span>. The quality of <span class="math">\text{IL}_\text{agg}</span> is checked by attesters, and conditions the validity of block <span class="math">B</span>.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/e/bedfe43a4319b8ef24f99db6089793aeda7dc3fb.png" title="Screenshot 2024-06-04 at 15.58.51"><img alt="Screenshot 2024-06-04 at 15.58.51" height="375" src="https://ethresear.ch/uploads/default/optimized/3X/b/e/bedfe43a4319b8ef24f99db6089793aeda7dc3fb_2_690x375.png" width="690" /></a></div><p></p>
<blockquote>
<p><strong>Figure 1.</strong> Diagram illustrating the FOCIL mechanism.</p>
</blockquote>
<h2><a class="anchor" href="https://ethresear.ch#mechanism-5" name="mechanism-5"></a><strong>Mechanism</strong></h2>
<ul>
<li><strong>Validator Selection and Local Inclusion Lists</strong>
<ul>
<li>A set of validators is selected from the beacon committee to become <span class="math">\text{IL}</span> committee members for slot <span class="math">n</span>. This set is denoted as <span class="math">\text{IL}_\text{committee}(n) = \{ 1, \dots, m \}</span>, where <span class="math">m</span> is the number of <span class="math">\text{IL}</span> committee members.</li>
<li>Each <span class="math">\text{IL}</span> committee member <span class="math">i \in \text{IL}_\text{committee}(n)</span> releases a local <span class="math">\text{IL}</span>, resulting in a set of local <span class="math">\text{ILs}</span> for slot <span class="math">n</span>, defined as <span class="math">\text{IL}_\text{local}(n) = \{ \text{IL}_1, \dots, \text{IL}_m \}</span>.</li>
<li>Each local <span class="math">\text{IL}_i</span> contains transactions: <span class="math">\text{IL}_i = \{ \text{tx}^1_i, \dots, \text{tx}^{j_i}_i \}</span>, where each <span class="math">\text{tx}</span> is represented as  <span class="math">\text{tx} = (\text{tx}[\text{From}], \text{tx}[\text{Gas Limit}])</span>, and  <span class="math">j_i</span> indicates the number of transactions in <span class="math">\text{IL}_i</span>. The <code>From</code> field represents the sender’s address, and the <code>Gas Limit</code> field represents the maximum gas consumed by a transaction. This is used to check whether a transaction can be included in a block given the <a href="https://ethresear.ch/t/unconditional-inclusion-lists/18500">conditional</a> IL property.</li>
</ul>
</li>
<li><strong>Block Producer’s Role</strong>
<ul>
<li>The block producer of slot <span class="math">n</span>, denoted <span class="math">\text{BP}(n)</span>, must include an <span class="math">\text{IL}</span> aggregate denoted <span class="math">\text{IL}_\text{agg}</span> and a payload in their block  <span class="math">B = (B[\text{IL}_\text{agg}], B[\text{payload}])</span>.</li>
<li><span class="math">\text{IL}_\text{agg}</span> consists of transactions: <span class="math">\text{IL}_\text{agg} = \{ \text{tx}^1_\text{agg}, \dots, \text{tx}^{t_\text{agg}}_\text{agg} \}</span> where each transaction <span class="math">\text{tx}_\text{agg}</span> is defined as <span class="math">(\text{tx}_\text{agg}[\text{tx}], \text{tx}_\text{agg}[\text{bitlist}])</span>, and the <span class="math">\text{payload}</span> must include transactions present in the <span class="math">\text{IL}_\text{agg}</span>.</li>
<li>The bitlist <span class="math">\text{tx}_\text{agg}[\text{bitlist}] \in \{0, 1\}^m</span> indicates which local $\text{IL}$s included a given transaction.</li>
<li>The function <span class="math">\text{Agg}</span> takes the set of available local ILs <span class="math">\text{IL}_\text{local}(n)</span> and outputs a “canonical” aggregate. The proposer aggregate <span class="math">\text{IL}_\text{agg}^\text{proposer}</span> is included in block <span class="math">B</span>, and each attester evaluates it quality by comparing it against its own <span class="math">\text{IL}_\text{agg}^\text{attester}</span>, using the function <span class="math">\text{Eval}(\text{IL}_\text{agg}^\text{attester}, \text{IL}_\text{agg}^\text{proposer}, Δ) \in \{ \text{True}, \text{False} \}</span>.</li>
</ul>
</li>
<li><strong>Attesters’ Role</strong>
<ul>
<li>Attesters for slot <span class="math">n</span> receive the block <span class="math">B</span> and apply a function <span class="math">\text{Valid}(B)</span> to determine the block validity.</li>
<li><span class="math">\text{Valid}</span> encodes the block validity according to the result of <span class="math">\text{Eval}</span>, as well as core IL properties such as <a href="https://ethresear.ch/t/unconditional-inclusion-lists/18500">conditional vs. unconditional</a>.</li>
<li>Here are some scenarios to illustrate <span class="math">\text{IL}</span>-dependent validity conditions:
<ul>
<li>If local <span class="math">\text{ILs}</span> are made available before deadline <span class="math">d</span>, but the proposer doesn’t include an <span class="math">\text{IL}_\text{agg}^\text{proposer}</span>, block <span class="math">B</span> is considered invalid.</li>
<li>If no local <span class="math">\text{ILs}</span> are made available before deadline <span class="math">d</span>, and the proposer doesn’t include an <span class="math">\text{IL}_\text{agg}^\text{proposer}</span>, block <span class="math">B</span> is considered valid.</li>
<li>If block <span class="math">B</span> is full, local $\text{IL}$s were available before <span class="math">d</span>, and the proposer doesn’t include an <span class="math">\text{IL}_\text{agg}^\text{proposer}</span>, block <span class="math">B</span> is still considered valid.</li>
<li>If <span class="math">\text{IL}_\text{agg}^\text{proposer}</span> doesn’t overlap with most of attesters’ <span class="math">\text{IL}_\text{agg}^\text{attester}</span> according to <span class="math">\text{Eval}</span>, block <span class="math">B</span> is considered invalid.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>The core FOCIL mechanism could be defined as:</strong></p>
<div class="math">
\mathcal{M}_\text{FOCIL}= (\text{Agg}, \text{Eval}, \text{Valid})
</div>
<h2><a class="anchor" href="https://ethresear.ch#timeline-6" name="timeline-6"></a>Timeline</h2>
<p>The specific timing is given here as an example, but more research is required to figure out which numbers make sense.</p>
<ul>
<li><strong>Slot</strong> <span class="math">n-1</span><strong>,</strong> <span class="math">t = 6</span><strong>:</strong> The <span class="math">\text{IL}</span> committee releases their local <span class="math">\text{ILs}</span>, knowing the contents of block <span class="math">n-1</span>.</li>
<li><strong>Slot</strong> <span class="math">n-1</span><strong>,</strong> <span class="math">t=9</span><strong>:</strong> There is a local <span class="math">\text{IL}</span> freeze deadline <span class="math">d</span> after which everyone locks their view of the observed local <span class="math">\text{ILs}</span>. The proposer broadcast the <span class="math">\text{IL}_\text{agg}</span> over the global topic.</li>
<li><strong>Slot</strong> <span class="math">n</span><strong>,</strong> <span class="math">t=0</span><strong>:</strong> The block producer of slot <span class="math">n</span> releases their block <span class="math">B</span> which contains both the payload and aggregated <span class="math">\text{IL}_\text{agg}</span>.</li>
<li><strong>Slot</strong> <span class="math">n</span><strong>,</strong> <span class="math">t=4</span><strong>:</strong> The attesters of slot <span class="math">n</span> vote on block <span class="math">B</span>, deciding whether <span class="math">\text{IL}_\text{agg}</span> is “good enough” by comparing the result of computing the <span class="math">\text{Agg}</span> function over their local view of available local <span class="math">\text{ILs}</span> (applying <span class="math">\text{Eval}</span>) and checking if block <span class="math">B</span> is <span class="math">\text{Valid}</span>.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#aggregation-evaluation-and-validation-functions-7" name="aggregation-evaluation-and-validation-functions-7"></a><strong>Aggregation, Evaluation and Validation Functions</strong></h2>
<p>As mentioned in the mechanism section, FOCIL relies on three core functions. Each of these needs to be specified to ensure the mechanism fulfils its purpose.</p>
<ul>
<li>
<p><strong>The <span class="math">\text{Agg}</span> function</strong> is probably the most straightforward to define: Transactions from all collected local <span class="math">\text{ILs}</span> should be deterministically aggregated and deduplicated to construct <span class="math">\text{IL}_\text{agg}</span>. We let:</p>
<ul>
<li><span class="math">\text{IL}_\text{local} = \{\text{IL}_1, \text{IL}_2, \ldots, \text{IL}_m\}</span> be the set of local inclusion lists collected from committee members <span class="math">m</span>.</li>
<li>Each <span class="math">\text{IL}_i = \{\text{tx}_i^1, \text{tx}_i^2, \ldots, \text{tx}_i^{t_i}\}</span><br />
be the transactions in the local inclusion list of the <span class="math">i</span>-th committee member.</li>
<li>Each transaction <span class="math">\text{tx}</span> be defined by <span class="math">(\text{hash}, \text{sender}, \text{nonce})</span></li>
</ul>
<p><span class="math">\text{Agg}(\text{IL}_\text{local})</span>  can be thus defined as:</p>
<div class="math">
\text{Agg}(\text{IL}_\text{local}) = {\text{tx} | \text{tx} \in \bigcup_{i \in m} \text{tx}_{i} }
</div>
</li>
<li>
<p><strong>The <span class="math">\text{Eval}</span> function</strong> is used by each slot <span class="math">n</span> attester to assess the quality of the <span class="math">\text{IL}_\text{agg}</span> included in block <span class="math">B</span>. Each attester calculates the <span class="math">\text{Agg}</span> function over all local <span class="math">\text{ILs}</span> they have observed in their view and then compares their generated <span class="math">\text{IL}_\text{agg}^\text{attester}</span> to the one included by the proposer <span class="math">\text{IL}_\text{agg}^\text{proposer}</span>. The <strong><span class="math">\text{Eval}</span></strong> function can then be defined so that the proposer’s <span class="math">IL_{\text{agg}}^{\text{proposer}}</span> is valid if it includes a sufficient proportion of transactions observed by the attesters, as defined by the parameter <span class="math">Δ</span>:</p>
<div class="math">
\text{Eval}(IL_{\text{agg}}^{\text{attester}}, IL_{\text{agg}}^{\text{proposer}}, \Delta) = 
\begin{cases} 
\text{True} &amp; \text{if } \frac{|IL_{\text{agg}}^{\text{attester}} \cap IL_{\text{agg}}^{\text{proposer}}|}{|IL_{\text{agg}}^{\text{attester}}|} \geq \Delta \\
\text{False} &amp; \text{otherwise}
\end{cases}
</div>
<p><em>Note that the <span class="math">\text{Eval}</span> function, and especially its parameter <span class="math">Δ</span>, will determine the trade-off between <strong>(1) the quality</strong> of the <span class="math">\text{IL}_\text{agg}^\text{proposer}</span> and the agency we are willing to give to proposers, and <strong>(2)</strong> <strong>liveness</strong>, as we might see an increase in missed slots if the criteria are set too strictly.</em></p>
</li>
<li>
<p><strong>The <span class="math">\text{Valid}</span> function</strong> encodes whether the  <span class="math">\text{IL}_\text{agg}</span> conforms to pre-defined core <span class="math">\text{IL}</span> properties, such as:</p>
<ul>
<li><strong>Conditional vs. Unconditional</strong>: Should the proposer include as many <span class="math">\text{IL}</span> transactions in the block as possible as long as there is space left, or is there dedicated block space reserved for <span class="math">\text{IL}</span> transactions?</li>
<li><strong>Where-in-block</strong>: Where should <span class="math">\text{IL}</span> transactions be included in the block? Should they be placed anywhere, at the top of the block, or at the end of the block?</li>
<li><strong>Expiry</strong>: How long do transactions remain in the <span class="math">\text{IL}</span> once they have been included? What happens if a slot is skipped?</li>
</ul>
</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#more-rules-8" name="more-rules-8"></a><strong>More rules</strong></h2>
<p>In the following section, we introduce other rules that could be added to the core mechanism to specify:</p>
<ul>
<li>How users should pay for having their transactions included (<span class="math">\text{Payment}</span>)</li>
<li>How rewards can be distributed across FOCIL participants (<span class="math">\text{Reward}</span>)</li>
<li>How local <span class="math">\text{ILs}</span> are constructed (<strong><span class="math">\text{Inclusion}</span></strong>)</li>
<li>Interactions between <span class="math">\text{IL}</span> and payload transactions (<span class="math">\text{Priority}</span>).</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#user-bidding-textpayment-and-textreward-rules-9" name="user-bidding-textpayment-and-textreward-rules-9"></a><strong>User Bidding,</strong> <span class="math">\text{Payment}</span> <strong>and</strong> <span class="math">\text{Reward}</span> <strong>rules</strong></h3>
<ul>
<li>Users place bids based on the value they assign to having their transactions included in block <span class="math">B</span>. They need to take into consideration the FOCIL mechanism <span class="math">\mathcal{M}_\text{FOCIL}</span>, but also how the <a href="https://eips.ethereum.org/EIPS/eip-1559" rel="noopener nofollow ugc">EIP-1559</a> mechanism works to set their base fees, denoted <span class="math">\mathcal{M}_\text{1559}</span>. For instance, a user <span class="math">t</span> makes a bid <span class="math">b^t(v^t, \mathcal{M}_\text{FOCIL},\mathcal{M}_\text{1559}) = (\delta^t, f^t)</span>, where <span class="math">\delta^t</span> is the maximum priority fee and <span class="math">f^t</span> is the maximum total fee (i.e., base fee <span class="math">r</span> + priority fee <span class="math">\delta^t</span>).</li>
<li>The vector of bids from all users is denoted as <span class="math">\mathbf{b} = (b^1, b^2, \dots, b^T)</span>, where each <span class="math">b^t</span> represents the bid from user <span class="math">t</span>.</li>
<li>The <span class="math">\text{Payment}</span> rule <span class="math">p(\mathbf{b}) = (p_0(\mathbf{b}), p_1(\mathbf{b}), \dots, p_t(\mathbf{b}), \dots, p_m(\mathbf{b}))</span> ensures that users pay no more than their priority fee <span class="math">\hat{\delta}^t = \min(\delta^t, f^t - r)</span>. Here, <span class="math">p_0(\mathbf{b}</span>) represents the payment to the block producer, and <span class="math">p_t(\mathbf{b}</span>) represents the payment made by user <span class="math">t</span> to all other <span class="math">\text{IL}</span> committee members, where the set of users has size <span class="math">m</span> and the block producer is indexed by 0.</li>
</ul>
<p>The <span class="math">\text{Payment}</span> rule defined above is meant to give a general view of how the value paid by users’ transactions can be redistributed across FOCIL participants (e.g., <span class="math">\text{IL}</span> committee members, block producer) to incentivize behavior that is considered good for the network, in this case preserving its censorship-resistant properties. Incentivizing <span class="math">\text{IL}</span> committee members for including transactions strengthens the robustness of the mechanism by increasing the <a href="https://arxiv.org/abs/2301.13321" rel="noopener nofollow ugc">cost of censorship</a>, or the amount a censoring party would have to pay for <span class="math">\text{IL}</span> committee members to exclude transactions from their local <span class="math">\text{ILs}</span>. Delving into the specifics of how the builder and <span class="math">\text{IL}</span> committee members should be rewarded is beyond the scope of this post as distributing rewards in an incentive-compatible way, especially during congestion, gets quite complex.</p>
<p>However, here are three high-level options to consider:</p>
<ul>
<li><strong>Option 1</strong>: All transaction priority fees go to the builder, and <span class="math">\text{IL}</span> committee members are just not incentivized to include transactions in their local <span class="math">\text{ILs}</span>. This simple option doesn’t require any changes to the existing fee market, but entirely relies on altruism from <span class="math">\text{IL}</span> committee members. We could even consider an opt-in version of FOCIL, where validators can choose to be part of a list that may be elected to become <span class="math">\text{IL}</span> committee members and participate in building <span class="math">\text{ILs}</span> altruistically. However, it wouldn’t increase the cost of censorship nor would it make it very appealing for validators to participate in the mechanism. This could also lead to out-of-band payments from users wanted to have their transactions included in local <span class="math">\text{ILs}</span>.</li>
<li><strong>Option 2</strong>: Priority fees from transactions included in the block are given to the <span class="math">\text{IL}</span> committee members. To distribute rewards among members, we could implement a weighted incentive system by defining a <span class="math">\text{Reward}</span> rule to calculate and distribute rewards for each member, considering the quantity (i.e., count) and uniqueness of transactions included in their local lists (see Appendix 1 of the <a href="https://ethresear.ch/t/the-more-the-less-censored-introducing-committee-enforced-inclusion-sets-comis-on-ethereum/18835">COMIS post</a> for more details). If transactions are not part of the <span class="math">\text{IL}_\text{agg}</span>, priority fees go to the builder. However, this approach could be problematic during congestion periods with the conditional <span class="math">\text{IL}</span> property, as builders might be incentivized to fill the block with transactions that are not in the <span class="math">\text{IL}_\text{agg}</span>, even if <span class="math">\text{IL}</span> transactions have higher priority fees. To address this, we might need to design a mechanism that redirects priority fees to the builder during congestion. However, the practical implementation and potential secondary effects need further investigation.</li>
<li><strong>Option 3</strong>: A third option is to introduce a new, separate inclusion fee that always go to IL committee members while priority fees always go to the builder. This would likely address the concerns of <strong>Option 2</strong> related to congestion but would introduce a whole other variable that users need to set. A useful distinction between Option 2 and Option 3 is whether the complexity is pushed upon the IL committee members or the end users.</li>
</ul>
<p>Another interesting question to explore is the impact of fee distribution across <span class="math">\text{IL}</span> committee members on mechanisms like <a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590/4">MEV-burn</a>. <strong>Options 2</strong> and <strong>3</strong> would effectively “reduce the burn” and produce a similar effect as <a href="https://ethresear.ch/t/committee-driven-mev-smoothing/10408">MEV-smoothing</a>, but on a smaller scale limited to the size of the <span class="math">\text{IL}</span> committee (h/t Anders).</p>
<h3><a class="anchor" href="https://ethresear.ch#textinclusion-rule-10" name="textinclusion-rule-10"></a><span class="math">\text{Inclusion}</span> <strong>Rule</strong></h3>
<p>The <span class="math">\text{Inclusion}</span> rule determines the criteria according to which <span class="math">\text{IL}</span> committee members should build their local <span class="math">\text{ILs}</span>. In FOCIL, we define it with the premise that IL committee members will try to maximize their rewards. Assuming <strong>Option 2</strong> for the <span class="math">\text{Payment}</span> rule, the <span class="math">\text{Inclusion}</span> rule could be to include all transactions seen in the public mempool, ordered by priority fees.</p>
<h3><a class="anchor" href="https://ethresear.ch#textpriority-rule-11" name="textpriority-rule-11"></a><span class="math">\text{Priority}</span> <strong>Rule</strong></h3>
<p>We assume the block will be made of two components: a payload and an  <span class="math">\text{IL}_\text{agg}</span> included by the proposer to impose constraints on transactions that need to be included in the builder’s payload. Imposing constraints to the block payload via the  <span class="math">\text{IL}_\text{agg}</span> thus requires a priority rule to determine what happens during congestion. Generally, the priority rule in FOCIL states that transactions in the  <span class="math">\text{IL}_\text{agg}</span> might be excluded if the block can be filled with the builder’s payload transactions. In other words, the block will still be valid even if some transactions in the <span class="math">\text{IL}_\text{agg}</span> are not included, as long as the block is completely full (i.e., the <code>30 M</code> gas limit is reached).</p>
<p><em>Note: Rules are not set in stone and should be interpreted as candidates for FOCIL. Rules also don’t necessarily have to be made explicit. For instance, we can define the <span class="math">\text{Reward}</span> such that the dominant strategy of the <span class="math">\text{IL}</span> committee is to adhere to the <span class="math">\text{Inclusion}</span> rule without any kind of enforcement by the protocol.</em></p>
<h2><a class="anchor" href="https://ethresear.ch#improvements-and-mitigations-12" name="improvements-and-mitigations-12"></a>Improvements and Mitigations</h2>
<p>In this section, we discuss improvements over previous  <span class="math">\text{IL}</span> proposals, focusing on simplification and addressing specific implementation concerns.</p>
<h3><a class="anchor" href="https://ethresear.ch#commitment-attacks-13" name="commitment-attacks-13"></a><strong>Commitment attacks</strong></h3>
<p>One of the main differences between FOCIL and the forward IL (<span class="math">\text{fIL}</span>) design proposed in <a href="https://eips.ethereum.org/EIPS/eip-7547" rel="noopener nofollow ugc">EIP-7547</a> is that FOCIL relies on a committee of multiple validators, rather than a single proposer, to construct and broadcast the <span class="math">\text{IL}</span>. This approach imposes stricter constraints on creating a “good” aggregate list and significantly reduces the surface for bribery attacks. Instead of targeting a single party to influence the exclusion of transactions from the <span class="math">\text{IL}</span>, attackers would now need to bribe an entire <span class="math">\text{IL}</span> committee (e.g., <code>256</code> members), substantially increasing the cost of such attacks. Previous designs (e.g., <a href="https://ethresear.ch/t/the-more-the-less-censored-introducing-committee-enforced-inclusion-sets-comis-on-ethereum/18835">COMIS</a> and <a href="https://ethresear.ch/t/anonymous-inclusion-lists-anon-ils/19627">anon-IL</a>), also involved multiple parties in building inclusion lists but still relied on an aggregator to collect, aggregate, and deduplicate local <span class="math">\text{ILs}</span>. In FOCIL, the entire set of attesters now participates in enforcing and ensuring the quality of the <span class="math">\text{IL}</span> included in the proposer’s block, thus removing single-party dependency other than the proposer. Additionally, it is worth noting that a censoring proposer would have to forego all consensus and execution layer rewards and cause a missed slot to avoid including transactions in the <span class="math">\text{IL}</span>.</p>
<h3><a class="anchor" href="https://ethresear.ch#splitting-attacks-and-il-equivocation-14" name="splitting-attacks-and-il-equivocation-14"></a><strong>Splitting attacks and IL equivocation</strong></h3>
<p>Another concern with <span class="math">\text{fILs}</span> focused on possible “splitting” attacks using <span class="math">\text{ILs}</span>. <a href="https://eprint.iacr.org/2021/1413.pdf" rel="noopener nofollow ugc">Splitting attacks</a> like timed release or “equivocation” occur when malicious participants attempt to divide the honest view of the network to stall consensus. On Ethereum, a validator equivocating by contradicting something it previously advertised to the network is a <a href="https://eth2book.info/capella/part2/incentives/slashing/" rel="noopener nofollow ugc">slashable offense</a>. If there is evidence of the offence being included in a beacon chain block, the malicious validator gets ejected from the validator set. Quick reminder that in the <a href="https://eips.ethereum.org/EIPS/eip-7547" rel="noopener nofollow ugc">EIP-7547</a> design, the proposer for slot <span class="math">n-1</span> is responsible for making the <span class="math">\text{IL}</span> to constrain proposer <span class="math">n</span>, and can broadcast multiple <span class="math">\text{ILs}</span> (check out the <a href="https://ethresear.ch/t/no-free-lunch-a-new-inclusion-list-design/16389">No-free lunch</a> post to see why, and how it relates to solving the free data availability problem). This means a malicious proposer could split the honest view of the network through <span class="math">\text{IL}</span> equivocation without being slashed. However, this is not a concern with FOCIL, since <span class="math">\text{IL}_\text{agg}</span> has to be part of proposer $n$’s block. An <span class="math">\text{IL}</span> equivocation would thus be equivalent to a block equivocation, which is a known, slashable offense from the protocol’s perspective.</p>
<h3><a class="anchor" href="https://ethresear.ch#incentives-incompatibilities-15" name="incentives-incompatibilities-15"></a>Incentives incompatibilities</h3>
<p>Previous <span class="math">\text{fILs}</span> proposals did not consider incentivizing the <span class="math">\text{IL}</span> proposer(s) for including “good” transactions. Relying on altruistic behavior might be fine, but there is always the risk that only very few validators will choose to participate in the mechanism if there is no incentive to gain. There is a strong argument to be made that the adoption of any <span class="math">\text{IL}</span> mechanism might be very low if validators risk being flagged as either non-censoring or censoring entities by revealing their preferences (see the <a href="https://ethresear.ch/t/anonymous-inclusion-lists-anon-ils/19627">Anonymous Inclusion Lists post</a>), and if they are not rewarded for contributing to preserving the network’s censorship resistance properties. In FOCIL, we consider mechanisms to distribute rewards across <span class="math">\text{IL}</span> committee members and mention two options (<strong>Option 2</strong> and Option 3 in the <span class="math">\text{Payment}</span> rule section) for sharing transaction fees based on the quantity (i.e., count) and uniqueness of transactions included in their local lists. We hope to continue working in this direction and to find incentive-compatible ways to increase the costs of censorship.</p>
<h3><a class="anchor" href="https://ethresear.ch#same-slot-censorship-resistance-16" name="same-slot-censorship-resistance-16"></a>Same-slot censorship resistance</h3>
<p>By having FOCIL run in parallel with block building during slot  <span class="math">n-1</span>, we can impose constraints on the block by including transactions submitted during the same slot in local <span class="math">\text{ILs}</span>. This is a strict improvement over <span class="math">\text{fILs}</span> designs, where the forward property imposes a 1-slot delay on <span class="math">\text{IL}</span> transactions. This property is particularly useful for time-sensitive transactions that might be censored for MEV reasons (see <a href="https://cdn.prod.website-files.com/642f3d0236c604d1022330f2/6499f35e0bd0f43471a95adc_MEV_Auctions_ArXiV_6.pdf" rel="noopener nofollow ugc">Censorship resistance in onchain auctions</a> paper). Admittedly, the mechanism is not exactly real-time because we still need to impose the “local <span class="math">\text{IL}</span> freeze” deadline <span class="math">d</span> so block producers have time to consider <span class="math">\text{IL}_\text{agg}</span> transactions before proposing their block.</p>
<h3><a class="anchor" href="https://ethresear.ch#textil-conditionality-17" name="textil-conditionality-17"></a><span class="math">\text{IL}</span> conditionality</h3>
<p>A core property of <span class="math">\text{ILs}</span> is their conditionality, which determines whether ILs should have dedicated block space for their transactions (<a href="https://ethresear.ch/t/unconditional-inclusion-lists/18500">unconditional</a>) or share block space with the payload and only being included if the block isn’t full (conditional). For FOCIL, we’re leaning towards using conditional <span class="math">\text{ILs}</span> for a couple of reasons. Firstly, it might generally be best to give sophisticated entities like builders the maximum amount of freedom in organizing block space as long as they include <span class="math">\text{IL}</span> transactions. Allowing them to order transactions and fill blocks as they prefer, rather than imposing too many restrictions on their action space, reduces the risk of them using side channels to circumvent overly rigid mechanisms. Specifically, the unconditional property just couldn’t really be enforced effectively with FOCIL, since builders wanting to use <span class="math">\text{IL}</span> dedicated block space could simply “buy up <span class="math">\text{IL}</span> committee seats” from the elected validators to include their transactions via local <span class="math">\text{ILs}</span>. Another reason to opt for conditional <span class="math">\text{ILs}</span> is the flexibility in the size of the list. With unconditional ILs, an added block space must strictly set an arbitrary maximum <span class="math">\text{IL}</span> gas limit (e.g., <code>3M</code> gas). In contrast, conditional <span class="math">\text{ILs}</span> allow for a much more flexible <span class="math">\text{IL}</span> size, depending on the remaining space in the block. The known tradeoff with conditional <span class="math">\text{ILs}</span> is block stuffing: censoring builders might fill their blocks up to the gas limit to keep <span class="math">\text{IL}</span> transactions out. More research is needed to determine the sustainability of block stuffing, as <a href="https://timroughgarden.org/papers/eip1559.pdf" rel="noopener nofollow ugc">consecutive full blocks exponentially increase base fees</a> and the overall cost of this strategy.</p>
<h3><a class="anchor" href="https://ethresear.ch#account-abstraction-accounting-18" name="account-abstraction-accounting-18"></a><strong>Account Abstraction accounting</strong></h3>
<p>In previous proposals, <span class="math">\text{IL}</span> summaries were constructed as structures to constrain blocks without committing to specific raw transactions. Each <span class="math">\text{IL}</span> summary —or <span class="math">\text{IL}_\text{agg}</span> for FOCIL— entry represents a transaction by including the following fields: <code>From</code> and <code>Gas Limit</code>. Satisfying an entry in the <span class="math">IL</span> summary requires that at least <em>some</em> transaction from the <code>From</code> address has been executed, <em>unless</em> the remaining gas in the block is less than <code>Gas Limit</code> . The idea is simple: if a transaction was previously valid and had a sufficiently high basefee, the only two things preventing its inclusion are the lack of sufficient gas in the block or its invalidation, which would require a transaction from the same sender to have been previously executed. Here we rely on a property of Ethereum EOAs: the <code>nonce</code> and <code>balance</code> of an EOA determine the validity of any transaction originating from that EOA, and can only be modified by such a transaction.</p>
<p>However, even limited forms of Account Abstraction that have been considered for inclusion in Electra (e.g., <a href="https://github.com/ethereum/EIPs/blob/43fb1e0ca950c42a09efdf9a85d8acfe260efac1/EIPS/eip-3074.md" rel="noopener nofollow ugc">EIP-3074</a> or <a href="https://github.com/ethereum/EIPs/blob/43fb1e0ca950c42a09efdf9a85d8acfe260efac1/EIPS/eip-7702.md" rel="noopener nofollow ugc">EIP-7702</a>) allow a transaction to trigger a change in an EOA’s balance, <em>without originating from that EOA</em>. This <a href="https://hackmd.io/@potuz/BkWngLly0#Transactions-that-become-invalid" rel="noopener nofollow ugc">raised concerns</a> regarding previous <span class="math">\text{fIL}</span> proposals, as proposer <span class="math">n</span> is not aware of what is included in builder $n$’s payload when proposing its <span class="math">\text{IL}</span>. This could lead to a scenario where proposer <span class="math">n</span> includes a transaction <span class="math">txn_A</span> from address <span class="math">A</span> in the <span class="math">\text{IL}</span>, while builder <span class="math">n</span> includes an EIP-7702 transaction <span class="math">txn_B</span>, originating from address <span class="math">B</span> but sweeping out all the <code>ETH</code> from address <span class="math">A</span>, and thus invalidating  <span class="math">txn_A</span>. Consequently, builder <span class="math">n+1</span> would no longer be able to include <span class="math">txn_A</span>, though no other transaction from address <span class="math">A</span> has been previously executed. In other words, the <span class="math">IL</span> summary would be unsatisfiable.</p>
<p>In FOCIL, one simplification is that the constraints from the <span class="math">\text{IL}_\text{agg}</span> apply to the block that is being built concurrently. This means a transaction in the <span class="math">\text{IL}_\text{agg}</span> can’t be invalidated because of a transaction in the previous block, as it can in <span class="math">\text{fIL}</span> designs. In other words, we do not need to worry about what happened in the previous block in order to check for satisfaction of the <span class="math">\text{IL}_\text{agg}</span>. However, a builder could still insert EIP-7702 transactions in its payload that invalidate <span class="math">\text{IL}_\text{agg}</span> transactions. To handle this case, we can do the following when validating a block:</p>
<ul>
<li>Before executing the block’s transactions, we store <code>nonce</code> and <code>balance</code> of all <code>From</code> addresses that appear in the <span class="math">\text{IL}_\text{agg}</span>.</li>
<li>After execution, we check the <code>nonce</code> and <code>balance</code> of all <code>From</code> addresses from the <span class="math">\text{IL}_\text{agg}</span> again, and for each (<code>From</code>, <code>Gas Limit</code>) pair in the <span class="math">\text{IL}_\text{agg}</span> we require that either the <code>nonce</code> or the <code>balance</code> has changed, or the <code>Gas Limit</code> is more than the remaining gas.</li>
</ul>
<p>If the <code>nonce</code> has changed, some transaction from that address has been executed. If the <code>balance</code> has changed but the <code>nonce</code> has not, some AA transaction has touched that address. In either case, that address has transacted in the block, and the entry is satisfied.</p>
<p><em>Note: With "full” AA, transactions could have validity that depends on arbitrary state (e.g., the price changing in a Uniswap pool). In such cases, relying on a reduced form of transactions (i.e., entries with <code>From</code> and <code>Gas limit</code> fields) is insufficient, as the full validation logic of the transaction is needed. Due to the <a href="https://notes.ethereum.org/@vbuterin/pbs_censorship_resistance#What-are-the-design-goals-of-any-anti-censorship-scheme" rel="noopener nofollow ugc">free data-availability</a> problem, putting raw transactions on-chain is not an option. Instead, attesters could check this locally since they need to construct their own <span class="math">\text{IL}_\text{agg}^\text{attester}</span> and could, therefore, evaluate the full validation logic. This allows them to verify if the transaction has been invalidated and if its inclusion should be enforced. However, attesters might have <span class="math">\text{IL}_\text{agg}^\text{attester}\text{s}</span> that contain different transactions from the same <code>From</code> address, leading to a situation where one transaction might be invalidated while another is not. This would result in split views and potential attacks</em></p>
            <p><small>10 posts - 5 participants</small></p>
            <p><a href="https://ethresear.ch/t/fork-choice-enforced-inclusion-lists-focil-a-simple-committee-based-inclusion-list-proposal/19870">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 19 Jun 2024 15:42:04 +0000</pubDate>
</item>
<item>
<title>Burn incentives in MEV pricing auctions</title>
<link>https://ethresear.ch/t/burn-incentives-in-mev-pricing-auctions/19856</link>
<guid>https://ethresear.ch/t/burn-incentives-in-mev-pricing-auctions/19856</guid>
<content:encoded><![CDATA[
<div> 关键词：MEV定价拍卖、公共利益、竞争、贿赂、共识机制

总结:
本文分析了MEV定价拍卖中的五种潜在激励，包括公有利益建设者、营利性公有利益建设者、敲诈勒索、攻击性竞争（包括单个和联合攻击）以及与共识机制的风险。这些因素促使参与者竞相烧掉MEV，以确保自身或竞争对手的收益。文章指出，尽管存在对晚投标和缺乏公平性的担忧，但实际博弈中，特别是通过与验证者服务提供商（SSP）的紧密合作，MEV燃烧的动机变得更加强烈。此外，作者提醒要警惕MEV定价拍卖可能对共识机制产生的负面影响，比如attester-builder的整合可能导致共识形成过程中的竞争失衡。因此，设计MEV机制时需平衡各方利益，防止意外破坏网络稳定。 <div>
<h1><a class="anchor" href="https://ethresear.ch#burn-incentives-in-mev-pricing-auctions-1" name="burn-incentives-in-mev-pricing-auctions-1"></a>Burn incentives in MEV pricing auctions</h1>
<p><em>Thanks to Barnabé Monnot, Thomas Thiery and Caspar Schwarz-Schilling for feedback and comments.</em></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/7/1703882e171fbc76c500a2799ebea0ad8dfe61d7.jpeg" title="The process of burning MEV"><img alt="The process of burning MEV" height="375" src="https://ethresear.ch/uploads/default/optimized/3X/1/7/1703882e171fbc76c500a2799ebea0ad8dfe61d7_2_375x375.jpeg" width="375" /></a></div><p></p>
<h2><a class="anchor" href="https://ethresear.ch#introduction-2" name="introduction-2"></a>Introduction</h2>
<h3><a class="anchor" href="https://ethresear.ch#overview-3" name="overview-3"></a>Overview</h3>
<p>This post presents a rudimentary review of incentives for burning MEV under the <a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590">“simple” MEV burn mechanism</a> presented by Justin, as well as its slot auction counterpart, <a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ">“execution auctions”</a> presented by Barnabé. The analysis is also applicable to Francesco’s original <a href="https://ethresear.ch/t/committee-driven-mev-smoothing/10408">MEV smoothing</a> design. These auctions—involving builders bidding, attesters enforcing a base fee floor, and proposers selecting a winning bid—will be defined as MEV pricing auctions (in the author’s view, the “execution auction” moniker could also be extended to cover all MEV pricing auctions).</p>
<p>The post highlights how incentives to drive up the price floor (and thus burn more MEV) can emerge in these designs regardless of any direct profit motive among builders for doing so. Importantly, stakers and staking service providers wish to ensure that competitors do not attain more rewards for selling MEV capture rights than them. They may therefore integrate with builders to bid away competing stakers’ profits. Auctions that set a price floor on proposers’ MEV capture rights will thus be influenced by the overarching staking <a href="https://en.wikipedia.org/wiki/Metagame">metagame</a>. It is only at this layer that griefing attacks against proposers to burn their MEV capture rights can be understood. Adverse competition during the consensus formation process might hypothetically lead attesters to bias their MEV base fee floor during split views, rejecting or admitting blocks depending on how it impacts their bottom line (in their roles as both builders and stakers). This is something to be attentive to. Naturally, burning MEV might also be considered a public good, and such incentives are reviewed in the text as well.</p>
<h3><a class="anchor" href="https://ethresear.ch#mev-pricing-auctions-4" name="mev-pricing-auctions-4"></a>MEV pricing auctions</h3>
<p>In <a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590"><em>MEV burn–a simple design</em></a>, Justin formulated an add-on to <a href="https://ethresear.ch/t/why-enshrine-proposer-builder-separation-a-viable-path-to-epbs/15710">enshrined proposer–builder separation</a> (ePBS), modifying the <a href="https://ethresear.ch/t/committee-driven-mev-smoothing/10408">MEV smoothing</a> design.  Builders can specify a base fee and a tip in their block bids. At some specific time before the slot begins (e.g., 2 seconds), attesters observe the highest base fee among the bids (“observation deadline”) and impose it as a subjective base fee floor when attesting to the proposer’s block. Only bids with a base fee above the floor are accepted, and the base fee is burned.</p>
<p>If builders bid before the observation deadline with the same timing as today, then the mechanism will <a href="https://ethresear.ch/t/in-a-post-mev-burn-world-some-simulations-and-stats/17092">burn substantial MEV</a>. Concerns have however been raised over the risk of <a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590/4">collusion between proposers and builders</a> and lack of <a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590/23">proper incentivization</a>. A <a href="https://ethresear.ch/t/dr-changestuff-or-how-i-learned-to-stop-worrying-and-love-mev-burn/17384">recent write-up</a> on the benefits of the design and MEV burn in general generated similar worries of a <a href="https://ethresear.ch/t/dr-changestuff-or-how-i-learned-to-stop-worrying-and-love-mev-burn/17384/3">stable equilibrium of late bidding</a>.</p>
<p>The design can be further modified to involve auctioning off the rights to the entire slot, 32 slots in advance (“<a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ">execution auction</a>”). A benefit of this design is the ability to offer long-lived preconfirmations and—hypothetically—the reduced value-in-flight during the auction. The same concerns raised for the block auction design can be applied to the slot auction design, because the beacon proposer might still benefit from colluding with builders to form late-bidding cartels when selecting the execution proposer.</p>
<p>A modified MEV pricing auction, <a href="https://ethresear.ch/t/mev-burn-incentivizing-earlier-bidding-in-a-simple-design/17389">MEV burn with builder kickbacks</a>, attempts to compensate builders for bidding early. That design is not the focus of this post, but incentives and side effects in uncompensated MEV pricing auctions will affect its relevance.</p>
<h2><a class="anchor" href="https://ethresear.ch#five-burn-incentives-in-mev-pricing-auctions-5" name="five-burn-incentives-in-mev-pricing-auctions-5"></a>Five burn incentives in MEV pricing auctions</h2>
<p>The outlined concerns of late bidding are valid, but it turns out that it is not possible to analyze MEV burn without incorporating stakers as participating agents. In such an analysis, competition for attaining the most yield will—under equilibrium—drive participants to burn each other’s MEV. Other incentives for burning MEV also exist. The analysis starts from the most idealistic public good example in (A) and gradually builds toward a metagame of active collusion to discourage other stakers in (E) (see Figure 1).</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/a/7a0cc1b660d8b22ac81aff0bbc070505e6f30e7e.jpeg" title="Figure 1"><img alt="Figure 1" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/7/a/7a0cc1b660d8b22ac81aff0bbc070505e6f30e7e_2_500x500.jpeg" width="500" /></a></div><p></p>
<p><strong>Figure 1.</strong> Five types of builders potentially burning MEV in MEV pricing auctions: (A) Public good builder, (B) For-profit public good builder, (C) Extortion racket, (D) Staker-initiated griefing, (E) Staker-initiated griefing cartel. The incentives behind (D) are important to understand (indicated by an arrow).</p>
<h3><a class="anchor" href="https://ethresear.ch#a-public-good-builder-6" name="a-public-good-builder-6"></a>(A) Public good builder</h3>
<p>The first example is a builder that dedicates resources to burning MEV without a direct profit motive. If Ethereum’s users believe that burning MEV is a public good, and in particular if no other incentive is sufficient, they may come together to fund the development and operation of a public good builder. Initiatives to fund public goods are fairly <a href="https://medium.com/ethereum-optimism/retroactive-public-goods-funding-33c9b7d00f0c">prevalent</a> within the Ethereum ecosystem. The public good builder can for example consistently bid according to guaranteed MEV at the observation deadline in the block auction design. This ensures that the MEV is burned while the builder will not suffer any direct losses from the bid. In the slot auction design, the builder would instead need to bid according to its expected MEV for the entire slot and might bid slightly below to stay safe.</p>
<p>The public good builder will likely not be the best and will often be outbid in terms of tips from other builders in the proposer auction (taking place after the observation deadline), in which the proposer selects a winning bid. But the operation can still be very impactful. After all, priority fees are a significant portion of all value (in this post these fees are also treated as MEV), and some further “low-hanging MEV fruits” are potentially available without dedicating too large resources for extraction. While the builder may use any public goods funding received diligently and not strive for any profit, pursuing an idealistic path can still raise the originators’ public profile and provide significant economic benefits in the future (perhaps not even directly related to building blocks).</p>
<h3><a class="anchor" href="https://ethresear.ch#b-for-profit-public-good-builder-7" name="b-for-profit-public-good-builder-7"></a>(B) For-profit public good builder</h3>
<p>A builder that positions itself as providing a public good may also enjoy direct economic benefits from its operation if some validators sympathize with the mission. There may for example be a market fit for builders that do not censor, nor extract various types of toxic MEV. In the block auction design, the builder could keep the MEV base fee in line with the available (non-censorship/non-toxic) MEV during the attester auction, and then pivot to tipping afterward, retaining some small profit margin. The MEV in some blocks is not particularly geared towards specialized searchers, and stakers may not lose that much in tips for some blocks by selecting the public good builder. Therefore, the public good builder could have higher profit margins in the blocks it does eventually get to build than builders that have not positioned themselves as providing a public good. A builder bidding before the observation deadline might of course also hope that its bids are the only ones to reach the proposer in times of degraded network conditions.</p>
<h3><a class="anchor" href="https://ethresear.ch#c-extortion-racket-8" name="c-extortion-racket-8"></a>(C) Extortion racket</h3>
<p>Given the lower effort required for extracting some of the MEV, it seems like (A) and (B) could have a natural position and high impact within the Ethereum ecosystem. But it may very well be that no successful public good builder can be sustained over the long run. After all, many stakers will not be particularly enthusiastic over a builder that burns their MEV opportunities.</p>
<p>Still, consider the importance of a dedicated MEV-burning builder within the staking ecosystem. If the builder is operational, proposers will lose out on a lot of value relative to if it does not operate. Is there a business opportunity here? Perhaps a builder could commit to burning the maximum possible MEV but abstain from doing so if it receives a bribe from the proposer? It seems natural that proposers would be willing to pay for this, since the proposer stands to capture most value from the available MEV if none is burned. But the prospect of competition makes the business model perilous. If a sole extortive builder is profitable, then a few more may try to enter the market as well. There is not much use in paying off two builders if it turns out that a third burned the MEV anyway through a bid. A mechanism for reconciling this ex-post would become rather complex. The validator may then be better off by simply not negotiating with any extortion racket.</p>
<p>While the extortion racket seems unsustainable, it helps to underscore the power that builders have over proposers. The ultimate incentive for burning MEV then emerges when changing the responsible actor from one unaffected by the staking equilibrium (extorting builder) to one that is not (other stakers). The auction will eventually become part of the <a href="https://en.wikipedia.org/wiki/Metagame">metagame</a> of the overarching staking equilibrium.</p>
<h3><a class="anchor" href="https://ethresear.ch#d-metagame-staker-initiated-griefing-9" name="d-metagame-staker-initiated-griefing-9"></a>(D) Metagame—staker-initiated griefing</h3>
<p>Staking service providers (SSPs) compete for delegated stake and derive income by taking a cut of the staking yield when they pass it back to the delegators. An SSP must ensure that the yield it offers delegating stakers is competitive relative to offers from other SSPs. The MEV pricing auction may therefore lead SSPs to burn competing proposers’ MEV by tightly integrating with builders or running them in-house. If a competitor burns an SSP’s MEV, then the SSP must respond in kind or will lose out on delegators and thus income. When considering the metalevel of SSPs, this equilibrium seems more stable than an equilibrium of late bidding leading to little or no MEV burn. All it takes to break the late-bidding cartel is one defecting SSP builder, forcing others to respond.</p>
<p>An SSP that through a builder griefs other stakers without taking any loss executes something comparable to a <a href="https://github.com/ethereum/research/blob/d1d465f658e0024a2010b0a6ad960a76d9c40cac/papers/discouragement/discouragement.pdf">discouragement attack</a> with an infinite griefing factor. This is a very advantageous attack, primarily because delegators will flow to the best performing SSP. In addition, a reduction in overall yield for other stakers pushes down the quantity of supplied stake, bringing up the equilibrium yield. Thus, even if some delegators do not flow to the SSP that burns its competitor’s MEV, the expected staking yield (that the SSP will share in the profit from) will still go up, if the competitor’s customers simply stop delegating. Of course, the cost of running the builder must be accounted for. But large SSPs can amortize that cost across a vast amount of yield-bearing validators.</p>
<p>Yet, directly profiting from the MEV is almost always better than burning it. When an SSP’s builder is able to extract more MEV in a competitor’s slot than any other builder, it will still be better off only bidding to a level that ensures it wins the auction. The SSP must thus make a probabilistic judgment as to the uniqueness of its MEV opportunity in the particular slot before deciding how to proceed (or more precisely, any edge in MEV value <span class="math">V_e</span> relative to the second best builder). An SSP builder must in essence bid before the observation deadline up to the point where the expected payoff from burning the marginal MEV is equal to the expected payoff from waiting and hoping to extract it. There are some game-theoretic nuances to this that here will be set aside, with some aspects discussed in the next section. The point is to assert that there are stronger incentives for builders to bid before the observation deadline than what has been previously understood, because a builder might be run by an SSP that indirectly profits from burning other stakers’ potential MEV revenue.</p>
<p>What happens in the metagame to smaller SSPs and solo stakers? They may not afford to run a builder of their own to ensure that their competitors’ MEV is burned. It is of course possible for solo stakers to try to come together to form a union around a builder, where each contributor is guaranteed to see their validators excluded from MEV base fee bids by the specific builder (and receive full tips during the proposer auction). There is then a question of if they will be able to organize such a union, but also if it really would be necessary. On the one hand, if there are several “griefing builders” running concurrently among the largest SSPs, parties holding less stake may not need to run their own griefing builder. Everyone will see their MEV burned anyway, since the big SSPs burn each other’s and everyone else’s MEV. On the other hand, a party not having a griefing builder readily available may be suboptimally positioned when considering the prospect of cartelization.</p>
<h3><a class="anchor" href="https://ethresear.ch#e-metagame-staker-initiated-griefing-cartel-10" name="e-metagame-staker-initiated-griefing-cartel-10"></a>(E) Metagame—staker-initiated griefing cartel</h3>
<p>Can builders operating at the metalevel collude to selectively burn or selectively <em>not</em> burn MEV, depending on the identity of the slot’s validator? The cartel would strive to ensure that all participating SSPs (or any union of solo stakers) receive the MEV in their validators’ proposed blocks, while minimizing MEV in all other validators’ blocks.</p>
<p>However, if attesters are honest, builders can only cartelize to selectively burn or not burn MEV that they uniquely are able to extract. As long as competing builders are operational, this substantially limits the power of any cartel. Therefore, the advantage of (E) over (D) is not substantial.</p>
<h4><a class="anchor" href="https://ethresear.ch#proposer-is-part-of-the-cartel-11" name="proposer-is-part-of-the-cartel-11"></a>Proposer is part of the cartel</h4>
<p>When the beacon proposer is part of the cartel, members will abstain from bidding before the observation deadline to ensure that as much value as possible flows to the proposer. This type of cartelization has been highlighted as a concern (<a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590/4">1</a>, <a href="https://ethresear.ch/t/dr-changestuff-or-how-i-learned-to-stop-worrying-and-love-mev-burn/17384/3">2</a>) in the debate around MEV pricing auctions. The idea is that participants come to an explicit or implicit agreement to not bid before the observation deadline. Yet the incentive to burn MEV is stronger than previously understood, since stakers outside the cartel will wish to grief cartel members by bidding early (D), and so from this perspective, the risk of late-bidding-cartelization is lower than feared.</p>
<p>It might also be difficult to efficiently uphold cartelization, because it is not possible for members to know which, if any, defected in pursuit of (D). One avenue would be to try to share the profits from every slot to give all participants incentives to hold back bids before the observation deadline. Yet overall, the existence of (A), (B), and (D) means that some value will still reasonably be burned by public good builders or any competitors not part of the cartel.</p>
<h4><a class="anchor" href="https://ethresear.ch#proposer-is-not-part-of-the-cartel-12" name="proposer-is-not-part-of-the-cartel-12"></a>Proposer is not part of the cartel</h4>
<p>When the beacon proposer is outside the cartel, the goal is to deprive it of revenue while still capturing as much of the MEV as possible. It will still be more profitable for the cartel to extract any unique MEV opportunity rather than burn it. Define <span class="math">V_s</span> as the value a builder can attain in the slot auction and <span class="math">V_b</span> as its value for the block auction (from a block built at the observation deadline). When a builder can extract the most MEV, it has an edge <span class="math">V_e</span> over the second-best builder (kept constant for simplicity). Just as in (D), the cartel can bid up to <span class="math">V_b-V_e</span> or <span class="math">V_s-V_e</span>, with the difference that <span class="math">V_e</span> expands if the cartel collectively gains a larger edge against the best builder outside of the cartel. This expansion is what the cartel tries to capitalize on, both when the proposer is part of the cartel (expanding <span class="math">V_e</span> to lower the burn) and when not (expanding <span class="math">V_e</span> to increase builder profits). A challenge—just as in (D)—is that the cartel might not be able to properly estimate <span class="math">V_e</span>. After the observation deadline, the cartel attempts to extract as much value as possible, leaving the MEV either burned or in their hands.</p>
<h4><a class="anchor" href="https://ethresear.ch#collusion-at-other-levels-13" name="collusion-at-other-levels-13"></a>Collusion at other levels</h4>
<p>The presentation so far has been somewhat simplistic. It bears mentioning that collusion need not happen at the level of the builders, but can for example happen at the level of searchers or any out-of-protocol relay that the cartel still finds beneficial to maintain before posting to the P2P layer. In all scenarios of successful cartelization, if some stakers (for example solo stakers) are unable to act collectively, they may end up at the short end of the discouragement dynamic.</p>
<h2><a class="anchor" href="https://ethresear.ch#risks-associated-with-attester-builder-integration-14" name="risks-associated-with-attester-builder-integration-14"></a>Risks associated with attester–builder integration</h2>
<p>The analysis so far indicates that (D) may have a significant effect on its own but that it does not necessarily lead to the riskier cartelization in (E). But what might happen when we give SSPs tools for depriving each other of revenue? While SSPs will always compete, competition in MEV pricing auctions is on the verge of seeping into the consensus formation process. At the consensus level, all participants are expected to behave honestly and are rewarded for good behaviour. Through staker–builder integration in (D)-(E), SSPs will come to actively influence each other’s rewards, cooperating or griefing each other. A risk is that SSPs might navigate down perilous paths in this landscape.</p>
<p>It has been noted that MEV pricing auctions suffer from attesters potentially having <a href="https://ethresear.ch/t/mev-burn-incentivizing-earlier-bidding-in-a-simple-design/17389">split views</a> of the MEV base fee floor. Biasing the outcome in a split view one way or the other might benefit one builder over another, result in a block being forked out to deprive the beacon proposer of all rewards, or allow the proposer to reap higher rewards when selling MEV capture rights. One concern is that SSPs might eventually try to profit by tuning their attestations of the MEV base fee floor to produce favorable outcomes. This can also be done as part of a cartel. The honest majority assumption need not be broken to derive profits, due to split views. It is only necessary to put a thumb on the scale, and a competitive consensus formation might make such behavior more likely.</p>
<p>Of course, stakers who do not honestly attest to which bids they have observed at which specific time point subject themselves to risks of social slashing if malicious behavior can be uncovered. This is always a potential final resort under proof of stake. In essence, just as it is prudent to be cautious of MEV or excessive issuance as strata for cartelization, it also seems prudent to be cautious of MEV pricing auctions as a stratum for consensus adversity.</p>
<h2><a class="anchor" href="https://ethresear.ch#block-vs-slot-auctions-in-terms-of-mev-pricing-15" name="block-vs-slot-auctions-in-terms-of-mev-pricing-15"></a>Block vs. slot auctions in terms of MEV pricing</h2>
<p>Will block auctions or slot auctions burn more MEV? Is one more centralizing than the other? These questions are not easy to answer, because it depends on which burn incentive that comes to dominate, the likelihood of cartelization under different designs, etc. This section will discuss some differences (previous writings on <a href="https://mirror.xyz/0x03c29504CEcCa30B93FF5774183a1358D41fbeB1/CPYI91s98cp9zKFkanKs_qotYzw09kWvouaAa9GXBrQ">block vs. slot auctions</a> provide a broader perspective).</p>
<h3><a class="anchor" href="https://ethresear.ch#block-vs-slot-auctions-concerning-d-16" name="block-vs-slot-auctions-concerning-d-16"></a>Block vs. slot auctions concerning (D)</h3>
<p>Assume that (D) becomes an important incentive for burning MEV. Further, assume a competitive market without cartelization and perfect information about how much MEV each participant can extract. In the block auction design, the builder can bid <span class="math">V_b-V_e</span> for the block at the observation deadline to maximize burn while retaining opportunities to extract value. It then updates its block and bid through tips in the proposer auction up until the slot boundary. There is <span class="math">V_s-V_b</span> worth of value that the proposer hopes to attain through tips, and <span class="math">V_e</span> worth of value left for the builder (under these simplified conditions).</p>
<p>In the slot auction design, the builder can instead bid <span class="math">V_s-V_e</span> already at the observation deadline. It is just buying the rights to build the block, not committing to its content, and that value is an entire slot’s worth of MEV. Naturally, <span class="math">V_s</span> will here just be an estimate, and the risk that builders take on by bidding on an expected value instead of a tangible value might be worth some fraction of the total bid value. But incomplete information around competitors’ eventual final bids will likely serve to pull down the bid value at the observation deadline more. The staker–builder can ideally burn <span class="math">V_s-V_e</span> of a competing beacon proposer’s auctionable MEV, and again retain <span class="math">V_e</span> for itself. The difference in MEV burn between the two designs is then <span class="math">V_s-V_b</span>.</p>
<p>If the staker–builder could estimate <span class="math">V_s</span> also in the block auction design (which nominally is easier since it bids much closer to the deadline), it could bid <span class="math">V_s-V_e-V_g</span> already at the observation deadline. Since the bid is attached to a block containing only <span class="math">V_b</span> of MEV, <span class="math">V_g</span> is reserved as a tip for the proposer auction. If there is no tip, the proposer might elect to pick the block from the observation deadline, depriving the builder of <span class="math">V_s-V_b</span>. However, while the proposer might specifically wish to do so if the same builder bids with low tips also in the proposer auction, a staker can obfuscate its identity by running several builders (the kickback design disincentivizes obfuscation).</p>
<p>In either design, it seems most likely that the burn ends up being lower than these theoretical maxima due to incomplete information in combination with the fact that capturing the MEV is more valuable than burning it. The staker–builder will therefore operate with quite some margin to maximize expected profits.</p>
<h3><a class="anchor" href="https://ethresear.ch#block-vs-slot-auctions-concerning-a-b-17" name="block-vs-slot-auctions-concerning-a-b-17"></a>Block vs. slot auctions concerning (A)-(B)</h3>
<p>The analysis for (D) is to some extent also applicable for (A) and (B). The public good builder could theoretically bid higher in the slot auction than in the block auction. However, the risk associated with overbidding in the slot auction design might be more serious for these builders. In the block auction design, the available value will be much clearer, making it easier for an unsophisticated builder to make low-risk bids.</p>
<h3><a class="anchor" href="https://ethresear.ch#value-of-preconfirmations-18" name="value-of-preconfirmations-18"></a>Value of preconfirmations</h3>
<p>As previously mentioned, the slot auction design facilitates execution layer preconfirmations, which can provide a welfare gain to Ethereum. In addition, their value can be burnt (just as in <a href="https://ethresear.ch/t/execution-tickets/17944#roadmap-compatibility-6">execution tickets</a>), since builders are bidding to attain that value. This increases the burn of the slot auction design.</p>
<h3><a class="anchor" href="https://ethresear.ch#builder-centralization-under-competition-over-expected-mev-19" name="builder-centralization-under-competition-over-expected-mev-19"></a>Builder centralization under competition over expected MEV</h3>
<p>If builders have different strengths and weaknesses, they will intermittently attain the highest <span class="math">V_b</span> in the block auction design. While one builder might be able to extract the highest MEV in expectation, not all blocks will play to its strengths. However, in the slot auction, builders bid on expected MEV, and one specific builder might then always have the highest expected <span class="math">V_s</span>. <a href="https://collective.flashbots.net/t/when-to-sell-your-blocks/2814">This could potentially be a centralizing force</a>, depending on how secondary markets evolve.</p>
<h2><a class="anchor" href="https://ethresear.ch#conclusion-20" name="conclusion-20"></a>Conclusion</h2>
<p>There are strong incentives for burning MEV even in designs that do not directly compensate for it, for example to provide a public good service or to ensure that other participants in the staking metagame do not attain a higher yield. Uncompensated MEV pricing auctions accommodates these incentives. Of particular relevance is staker-initiated griefing (D). It seems clear that SSPs will seek to influence builders’ bidding strategies, and this can lead to staker–builder integration. Still, this form of integration does not necessarily lead to censorship or higher MEV profits; thus not negating sought benefits of proposer–builder separation. If it is desirable to give an outside party an independent incentive to burn MEV, then <a href="https://ethresear.ch/t/mev-burn-incentivizing-earlier-bidding-in-a-simple-design/17389">builder kickbacks</a> are an option. They can also be applied to the slot auction design.</p>
<p>When implementing a MEV burn mechanism, it is important to ensure that the burn mechanism does not accidentally set fire to Ethereum’s consensus mechanism. Giving SSPs tools for griefing each other could lead to adverse competition during the consensus formation process. A particular concern is then if emerging attester–builder integration leads attesters to bias their MEV base fee floor, rejecting or admitting blocks depending on how it impacts their bottom line (in their roles as both builders and stakers). Which of the different scenarios (A-E) that would predominate is seemingly a more important parameter when evaluating the merits of MEV pricing auctions than the mechanism’s ability to burn substantial MEV (which this post suggests it can).</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/burn-incentives-in-mev-pricing-auctions/19856">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 18 Jun 2024 20:58:14 +0000</pubDate>
</item>
<item>
<title>Preconfirmations: On splitting the block, mev-boost compatibility and relays</title>
<link>https://ethresear.ch/t/preconfirmations-on-splitting-the-block-mev-boost-compatibility-and-relays/19837</link>
<guid>https://ethresear.ch/t/preconfirmations-on-splitting-the-block-mev-boost-compatibility-and-relays/19837</guid>
<content:encoded><![CDATA[
<div> 关键词：Preconfirmation, XGA-style, Ethereum, Block Splitting, Relay

总结:
本文讨论了一种名为XGA-style的预确认机制，它为非优先级交易提供有限时间内（2个epoch后）的区块底部预留空间。这种机制将区块分为顶部和底部两部分，顶部用于传统MEV竞拍，底部通过预确认拍卖分配。预确认通过多单位拍卖进行，买家可以锁定区块容量确保交易成功纳入。文章还提到，这有助于缓解竞争性建块者压力，简化预确认定价，以及对Relay角色的重新思考，提出通过保险和奖励机制来保障预确认平台的稳定运行。XGA是首个实现这一设计的L2平台，目前已有主网版本，但正在进行进一步开发以支持更多功能。 <div>
<p>Thanks to <a class="mention" href="https://ethresear.ch/u/fabrizioromanogenove">@FabrizioRomanoGenove</a>, <a class="mention" href="https://ethresear.ch/u/meridian">@meridian</a> and Philipp Zahn for helpful comments and feedback on this post.</p>
<h2><a class="anchor" href="https://ethresear.ch#what-is-a-preconfirmation-1" name="what-is-a-preconfirmation-1"></a><img alt=":question:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/question.png?v=12" title=":question:" width="20" /> What is a Preconfirmation?</h2>
<p>There have been a lot of variations on the definition of preconfirmation going around recently in the Ethereum community. In this post we will keep the definition as simple and broad as possible in order to generate the least amount of confusion and avoid arguing on semantics as much as possible:</p>
<blockquote>
<p>We call a <strong><em>preconfirmation mechanism</em></strong> any mechanism that ensures (non-positional) inclusion of a (bundle of) transaction(s), if execution is successful, in a finite and bounded amount of time from the emission of the preconfirmation.</p>
</blockquote>
<h3><a class="anchor" href="https://ethresear.ch#xga-style-preconfirmations-2" name="xga-style-preconfirmations-2"></a><img alt=":mag:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/mag.png?v=12" title=":mag:" width="20" /> XGA-Style Preconfirmations</h3>
<p>We will analyze a specific kind of preconfirmation mechanism – as hinted to in <a href="https://ethresear.ch/t/a-simple-small-mev-boost-compatible-preconfirmation-idea/19800/3">this post on ethresearch</a> – that we came up with some time ago and have been building since then:</p>
<blockquote>
<p>An <strong><em>XGA-style preconfirmation mechanism</em></strong> is a preconfirmation mechanism that guarantees (non-positional) inclusion of a sized bundle of transactions <strong>in the bottom portion of a predetermined block to be minted 2 epochs after the preconfirmation was emitted</strong>. Maximum bundle size is determined at the time of emission of the preconfirmation.</p>
</blockquote>
<h2><a class="anchor" href="https://ethresear.ch#splitting-the-block-3" name="splitting-the-block-3"></a><img alt=":scissors:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/scissors.png?v=12" title=":scissors:" width="20" /> Splitting the Block</h2>
<p>Looking at the previous definition, I assume the first couple of questions that would come to mind is “what do you mean exactly by the bottom portion of a block?” and “how is the block to include the bundle predetermined?”. Our idea is pretty simple: Partition the block in such a way to keep a top-of-the-block (ToB)<sup class="footnote-ref"><a href="https://ethresear.ch#footnote-48655-1" id="footnote-ref-48655-1">[1]</a></sup>, high-priority section, in which traditional builders do their usual thing and is allocated through a traditional mev-boost auction or whatever the relay running it prefers; and a reserved bottom-of-the-block (BoB) section, which will serve as allocation space for preconfirmations. In this design, preconfirmation bundles will be allocated via a separate auction in the form of <strong><em>forward contracts</em></strong>.</p>
<h3><a class="anchor" href="https://ethresear.ch#a-two-auction-format-4" name="a-two-auction-format-4"></a><img alt=":busts_in_silhouette:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/busts_in_silhouette.png?v=12" title=":busts_in_silhouette:" width="20" /> A Two-Auction Format</h3>
<p>As briefly mentioned above, in the XGA-style split-block design, preconfirmations are allocated in a completely separate way from the traditional mev-boost auction, allowing them to coexist without excessively disrupting the ecosystem. Traditional builders will be able to do their own thing with minimal adjustments, while everyone else can still enjoy the benefits of preconfirmations.</p>
<p>In simple terms: An XGA-style BoB auction is a multi-unit auction selling gas tokens for a specific block <span class="math">B</span> in fixed-size units (e.g. <span class="math">100</span> K gas). These tokens can then be used to submit a bundle<sup class="footnote-ref"><a href="https://ethresear.ch#footnote-48655-2" id="footnote-ref-48655-2">[2]</a></sup> that is guaranteed inclusion in <span class="math">B</span> if execution is successful.</p>
<p>As an example, picture this scenario:</p>
<ul>
<li><img alt=":clock2:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/clock2.png?v=12" title=":clock2:" width="20" /> At the start of epoch <span class="math">N-2</span> we know that the validator <span class="math">V</span>, serving XGA-style preconfirmations, will be the proposer for the <span class="math">K</span>-th slot of epoch <span class="math">N</span>.</li>
<li><img alt=":oil_drum:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/oil_drum.png?v=12" title=":oil_drum:" width="20" /> $5$M gas out of the standard <span class="math">30</span> M will be auctioned off into <span class="math">50</span> gas tokens, each representing a capacity of <span class="math">100</span> K gas.</li>
<li><img alt=":shopping_cart:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/shopping_cart.png?v=12" title=":shopping_cart:" width="20" /> At some fixed time <span class="math">t</span> before the start of slot <span class="math">K</span>, a multi-unit auction allocating the tokens is run. Aki manages to win 5 tokens for <span class="math">K</span>, for a combined capacity of <span class="math">500</span> K.</li>
<li><img alt=":alarm_clock:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/alarm_clock.png?v=12" title=":alarm_clock:" width="20" /> Within the deadline fixed at some time <span class="math">d</span> before the end of <span class="math">K</span>, Aki uses the <span class="math">5</span> tokens to submit a bundle of size just over <span class="math">400</span> K gas.</li>
<li><img alt=":outbox_tray:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/outbox_tray.png?v=12" title=":outbox_tray:" width="20" /> In the meantime, other BoB auction winners submit their own bundles.</li>
<li><img alt=":dollar:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/dollar.png?v=12" title=":dollar:" width="20" /> At the start of <span class="math">K</span>, a traditional mev-boost auction for <span class="math">25</span> M gas is run as usual by all relays, and is won by Bogdan via relay <span class="math">R</span>.</li>
<li><img alt=":brick:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/brick.png?v=12" title=":brick:" width="20" /> After deadline <span class="math">d</span> is reached and the mev-boost auction is over, the BoB part is assembled and attached at the bottom of the max-<span class="math">25</span> M block submitted by Bogdan via relay <span class="math">R</span>.</li>
<li><img alt=":tada:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/tada.png?v=12" title=":tada:" width="20" /> Since Aki’s bundle contained no reverting transactions, it is included without any problem – together with the non-reverting bundles submitted by the other BoB winners – somewhere after the portion built by Bogdan.</li>
<li><img alt=":satellite:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/satellite.png?v=12" title=":satellite:" width="20" /> The block for <span class="math">K</span> gets broadcasted as usual.</li>
<li><img alt=":x:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/x.png?v=12" title=":x:" width="20" /> Excess tokens for <span class="math">K</span> that didn’t get spent can no longer be used.</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#who-builds-the-blocks-then-5" name="who-builds-the-blocks-then-5"></a><img alt=":brick:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/brick.png?v=12" title=":brick:" width="20" /> Who Builds the Blocks, then?</h3>
<p>Block building, in the case of XGA-style preconfirmations, is handled by multiple parties:</p>
<ul>
<li><img alt=":package:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/package.png?v=12" title=":package:" width="20" /> The ToB part is built by traditional mev-boost builders as usual.</li>
<li><img alt=":gift:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/gift.png?v=12" title=":gift:" width="20" /> The BoB part is assembled by the party running the BoB auction.</li>
<li><img alt=":brick:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/brick.png?v=12" title=":brick:" width="20" /> Merging the two parts and sending the block over is handled by the relay.</li>
</ul>
<p>In this setup, the relay takes on more work and responsibilities than it currently does. We will explore a potentially beneficial approach to this change later.</p>
<h3><a class="anchor" href="https://ethresear.ch#what-are-the-economic-advantages-of-preconfirmations-6" name="what-are-the-economic-advantages-of-preconfirmations-6"></a><img alt=":money_with_wings:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/money_with_wings.png?v=12" title=":money_with_wings:" width="20" /> What Are the Economic Advantages of Preconfirmations?</h3>
<p>Well… In general, for the whole range of designs that are being discussed right now this is not clear yet! <strong>Conjecturally</strong>, some of the proposed preconfirmation mechanisms will allow more value to trickle down to validators, but since the preconfirmation design landscape is so broad and confused right now it’s hard to take into account all the possible market effects that could come out of such designs. For example, most of the preconf mechanisms currently being discussed are pretty unfriendly towards what has been one of the main APY-cows for validators since the dawn of mev-boost: competitive builder/searchers.</p>
<h4><a class="anchor" href="https://ethresear.ch#why-are-we-betting-on-xga-style-preconfs-7" name="why-are-we-betting-on-xga-style-preconfs-7"></a><img alt=":game_die:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/game_die.png?v=12" title=":game_die:" width="20" /> Why Are We Betting on XGA-Style Preconfs?</h4>
<p>It seems clear to us that reserving a spot for non-priority-sensitive transactions can offer several benefits:</p>
<ul>
<li>Users and platforms (e.g. rollups) that are not involved in competitive building/searching just doesn’t care about running HFT operations on L1 can greatly benefit from separating their concerns from those of competitive builder/searchers.</li>
<li>On the other end, it eases some of the pressure on the competitive builder/searcher side by removing some of the burden of having to include <em>“filler transactions”</em> to keep their blocks competitive. E.g. freeing them from needing to include blob-bearing transactions that could negatively impact latency.</li>
<li>It makes actually pricing inclusion preconfirmations simpler, since it is still regulated by the usual gas pricing model, and at the same time the preconf inclusion market is kept separate from the traditional priority market for position-sensitive transactions.</li>
<li>Moreover, we believe in gradual change, allowing time for everyone to adapt to and observe the effects of new, potentially disruptive features in a controlled manner. A split-block design compatible with traditional mev-boost block building offers a less intrusive path to adoption.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#rethinking-relays-8" name="rethinking-relays-8"></a><img alt=":bulb:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/bulb.png?v=12" title=":bulb:" width="20" /> Rethinking Relays</h2>
<p>At the moment running a relay naively is mostly a non remunerative gig. Under XGA-style preconfirmations, the relay does significantly more work and takes on more risk than before, e.g. if a block is missed and/or already sold preconfirmation tokens end up not getting included due to the relay malfunctioning, whoever bought them incurs an active loss of assets. While this sounds scary, it is also a good opportunity to rethink the role of relays in the Ethereum ecosystem.</p>
<h3><a class="anchor" href="https://ethresear.ch#insurance-and-reward-mechanisms-for-relays-9" name="insurance-and-reward-mechanisms-for-relays-9"></a><img alt=":shield:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/shield.png?v=12" title=":shield:" width="20" /> Insurance and Reward Mechanisms for Relays</h3>
<p>What we are proposing is that a relay can subscribe to an XGA-style preconf platform by staking a collateral that could be used to offer the damaged parties a refund in case of the relay malfunctioning, while sharing a percentage of the platform revenue each time it submits a successful block that includes XGA-enabled preconfirmations<sup class="footnote-ref"><a href="https://ethresear.ch#footnote-48655-3" id="footnote-ref-48655-3">[3]</a></sup>.</p>
<h2><a class="anchor" href="https://ethresear.ch#introducing-xga-10" name="introducing-xga-10"></a><img alt=":mega:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/mega.png?v=12" title=":mega:" width="20" /> Introducing XGA</h2>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/4/448bd42d21b7642cd38d003f7cec9cb82adfc3b6.png" title="image"><img alt="image" height="99" src="https://ethresear.ch/uploads/default/optimized/3X/4/4/448bd42d21b7642cd38d003f7cec9cb82adfc3b6_2_690x99.png" width="690" /></a></div><br />
XGA – eXtensible Gas Auctions – is the first L2 platform for XGA-style preconfirmations (lol), designed and built by the combined efforts of <a href="https://www.manifoldfinance.com/" rel="noopener nofollow ugc">Manifold Finance</a> and <a href="https://20squares.xyz/" rel="noopener nofollow ugc">20Squares</a>. We’re very willing to make this an open and collaborative effort, so if you have any feedback and/or are interested in building this together with us, please reach out!<p></p>
<p>Right now we have released on mainnet our v1.0 (yes, this is not a beta, <strong>we’re ready to go</strong> and currently onboarding validators), with the caveat that in v1.0, the ToB mev-boost auction can only be run on a single relay. We’re currently working on shipping v2.0, which will allow a <strong>relay-agnostic</strong> auction to be run in the ToB part. You can find more about it at <a href="https://docs.xga.com/" rel="noopener nofollow ugc">docs.xga.com</a>.</p>
<hr class="footnotes-sep" />

<ol class="footnotes-list">
<li class="footnote-item" id="footnote-48655-1"><p>We have specific terms for ToB and BoB auctions, namely α and β-auctions respectively. <a class="footnote-backref" href="https://ethresear.ch#footnote-ref-48655-1">↩︎</a></p>
</li>
<li class="footnote-item" id="footnote-48655-2"><p>Note that this doesn’t exclude the possibility of overwriting an already submitted bundle, if re-submitted before the deadline. <a class="footnote-backref" href="https://ethresear.ch#footnote-ref-48655-2">↩︎</a></p>
</li>
<li class="footnote-item" id="footnote-48655-3"><p>We are already iterating on designs for captive insurance mechanisms for XGA-style platforms. We will upload a new post detailing some of the possible designs soon. <a class="footnote-backref" href="https://ethresear.ch#footnote-ref-48655-3">↩︎</a></p>
</li>
</ol>
            <p><small>4 posts - 2 participants</small></p>
            <p><a href="https://ethresear.ch/t/preconfirmations-on-splitting-the-block-mev-boost-compatibility-and-relays/19837">Read full topic</a></p>
]]></content:encoded>
<pubDate>Mon, 17 Jun 2024 09:40:41 +0000</pubDate>
</item>
<item>
<title>IPv6 vs Ethereum?</title>
<link>https://ethresear.ch/t/ipv6-vs-ethereum/19829</link>
<guid>https://ethresear.ch/t/ipv6-vs-ethereum/19829</guid>
<content:encoded><![CDATA[
<div> 关键词：IPv6、CGA、BCA、Subnet ID、Interface ID

总结: 这篇文章探讨了IPv6地址结构与以太坊区块链网络之间的类比。作者提出将IPv6 Subnet ID和Interface ID的概念应用于以太坊，形成类似VPC（虚拟私有云）的结构，每个链对应不同的Subnet。使用加密技术如Cryptographically Generated Addresses (CGA) 和 Bitcoin Address-based Addresses (BCA)，可以增强节点身份验证和隐私保护。这种设想旨在通过利用IPv6的发现协议和现有机制，简化Solano节点设置，解决网络碎片问题，并增强跨链通信的安全性。 <div>
<p>I started writing this after a few days of unsuccessful attempts to run solo node behind CGNAT, as just a brainbreeze on whether it could be somehow done differently to ease up solo node setup.<br />
So far It does not seem to be an answer, however I want to share some thoughts on analogies seen with ipv6 networking to see if anyone has ideas on how this can be useful . .</p>
<h2><a class="anchor" href="https://ethresear.ch#ipv6-101-1" name="ipv6-101-1"></a>ipv6 101</h2>
<p>An IPv6 address consists of 128 bits, represented as eight groups of four hexadecimal digits separated by colons. Each group is called a hextet. For example:</p>
<p><code>2001:0db8:85a3:0000:0000:8a2e:0370:7334</code></p>
<p>where</p>
<ul>
<li>Global Routing Prefix: 2001:0db8 (Assigned by the Regional Internet Registry)</li>
<li>Subnet ID: 85a3:0000 (Identifies a specific subnet within the network)</li>
<li>Interface ID: 0000:8a2e:0370:7334 (identify the individual interface or device on the subnet)</li>
</ul>
<p>This hierarchical structure allows for efficient routing of IPv6 packets. Routers can quickly determine the destination network based on the global routing prefix, then further refine the path based on the subnet ID.</p>
<p><em>Multiple gateways</em> from ipv6 subnet may exist to public ipv6 space. Addresses within ipv6 sub network may access global ipv6 address space. Routing protocols such as <a href="https://datatracker.ietf.org/doc/html/rfc5340" rel="noopener nofollow ugc">OSPFv3</a> or <a href="https://en.wikipedia.org/wiki/Border_Gateway_Protocol" rel="noopener nofollow ugc">BGP</a> may be used.</p>
<h2><a class="anchor" href="https://ethresear.ch#subnet-gateway-analogy-2" name="subnet-gateway-analogy-2"></a>Subnet Gateway analogy</h2>
<p>Just as an IPv6 router directs traffic to devices within its subnet, an RPC node facilitates communication with nodes and smart contracts within its respective blockchain network.</p>
<p>When we consider the concept of Chain IDs. In blockchain, Chain IDs are unique identifiers for different networks (e.g., Ethereum Mainnet has Chain ID 1, while various testnets have different IDs). Similarly, in IPv6, a subnet is identified by its unique prefix, which is a portion of the IPv6 address.</p>
<h2><a class="anchor" href="https://ethresear.ch#address-analogy-3" name="address-analogy-3"></a>Address analogy</h2>
<p>Since Interface Ids in IPv6 are only 64 bits long, they are too small to fit in 160 bits address of Eth.</p>
<p>However, what could be useful is using InterfaceIds to identify the nodes in the P2P network, forming VPC for Ethereum.</p>
<p>In IPv6, organizations or individuals can assign themselves a unique subnet prefix, effectively creating their own independent addressing space.</p>
<h3><a class="anchor" href="https://ethresear.ch#cryptography-for-ipv6-address-generation-4" name="cryptography-for-ipv6-address-generation-4"></a>Cryptography for IPv6 address generation</h3>
<p><a href="https://en.wikipedia.org/wiki/Secure_Neighbor_Discovery" rel="noopener nofollow ugc">Secure Neighbor Discovery (SEND)</a> is a security extension to the Neighbor Discovery Protocol (NDP) in IPv6, designed to address the vulnerabilities in the original NDP.</p>
<p>There are several papers and RFCs (Requests for Comments) relevant to cryptography for IPv6 address generation, particularly focusing on enhancing privacy and security:</p>
<p><strong><a href="https://datatracker.ietf.org/doc/html/rfc3972" rel="noopener nofollow ugc">RFC 3972</a> - Cryptographically Generated Addresses (CGA)</strong>: This RFC introduces the concept of CGA, where the interface identifier of an IPv6 address is generated using a cryptographic hash function from a public key and other parameters. This approach aims to bind a public key to an address securely, deterring address theft and enhancing authentication.</p>
<p><strong><a href="https://datatracker.ietf.org/doc/html/rfc7721" rel="noopener nofollow ugc">RFC 7721</a> - Security and Privacy Considerations for IPv6 Address Generation Mechanisms</strong>: This RFC discusses the security and privacy implications of different IPv6 address generation mechanisms, including SLAAC, privacy extensions, and CGAs. It provides recommendations for mitigating potential risks and improving privacy protection.</p>
<p><strong><a href="https://www.researchgate.net/publication/350518202_IPv6_Cryptographically_Generated_Address_Analysis_Optimization_and_Protection" rel="noopener nofollow ugc">IPv6 Cryptographically Generated Address: Analysis, Optimization and Protection</a></strong>:  This paper delves into the details of CGAs, analyzing their security and performance characteristics. It proposes optimizations to improve the efficiency of CGA generation and suggests additional security measures to strengthen the protection they offer.</p>
<p><strong><a href="https://arxiv.org/pdf/2311.15842" rel="noopener nofollow ugc">IPv6 Bitcoin-Certified Addresses, Mathieu Ducroux</a></strong>: proposes mechanism for enhancing the security and privacy of IPv6 addresses by leveraging the Bitcoin blockchain.<br />
In essence, BCAs are IPv6 addresses where the interface identifier is derived from a Bitcoin address.</p>
<h2><a class="anchor" href="https://ethresear.ch#how-could-this-be-beneficial-5" name="how-could-this-be-beneficial-5"></a>How could this be beneficial?</h2>
<p>If we can think of ethereum ecosystem as one big VPN where chains are subnet addressable that potentially solves fragmentation issues, allowing to use already established discovery protocols to route traffic between different nodes, use features like <a href="https://en.wikipedia.org/wiki/Multicast_address" rel="noopener nofollow ugc">multicast</a> etc.</p>
            <p><small>3 posts - 2 participants</small></p>
            <p><a href="https://ethresear.ch/t/ipv6-vs-ethereum/19829">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sat, 15 Jun 2024 21:28:45 +0000</pubDate>
</item>
<item>
<title>Slot Inclusion Rates and Blob Market Combinatorics</title>
<link>https://ethresear.ch/t/slot-inclusion-rates-and-blob-market-combinatorics/19817</link>
<guid>https://ethresear.ch/t/slot-inclusion-rates-and-blob-market-combinatorics/19817</guid>
<content:encoded><![CDATA[
<div> 关键词：slot inclusion rate, blob market, integer packing problem, reorg risk, builder censorship

总结:<br />本文探讨了blob市场的slot inclusion rate（区块包含率）问题，指出其存在高波动性和某些Rollup（如Optimism和Base）的高值。文章分析了当前blob提交策略导致的竞争和整数打包问题（integer packing problem），这可能导致更高的slot inclusion rate而非建设者审查。研究发现，虽然市场容量未充分利用，但大blob交易的策略（如Base一次提交多个）导致平均slot inclusion rate较高。文章还提出了优化建议，如调整最大blob数量、动态投标策略和预确认机制，以提高效率并减少竞争中的潜在延迟审查。总的来说，文章强调了blob市场设计对slot inclusion rate影响的重要性，并呼吁进一步研究来改善市场动态。 <div>
<h2><a class="anchor" href="https://ethresear.ch#tldr-1" name="tldr-1"></a>TLDR</h2>
<ul>
<li><strong>Slot inclusion rate</strong>, the number of slots required for a blob to be included in the beacon chain, has a high variance and is higher for some rollups than others.</li>
<li>The current combinatorics of the blob market has an <strong>integer packing problem</strong>. This is a type of combinatorial optimization that generally involves packing objects of different sizes into a finite number of containers or bins.</li>
<li>Data suggests that the integer packing problem is contributing more to higher slot inclusion rates than builder censorship.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#introduction-2" name="introduction-2"></a>Introduction</h2>
<p>This post offers a fresh perspective on the current design and constraints of the blob market, presenting additional data (<a href="https://blobs.primev.xyz/dashboard" rel="noopener nofollow ugc">from a blob tracking dashboard created at Primev</a>) on slot inclusion concerning reorg risks, and a combinatorial analysis of the blob market design, revealing an integer packing problem.</p>
<p>The key metric in this post is the <strong>slot inclusion rate</strong>. The slot inclusion rate indicates the number of slots required for a blob to be included in the beacon chain,<br />
with a higher rate signifying a longer inclusion time.</p>
<p>Recent research on the blob market <a href="https://ethresear.ch/t/big-blocks-blobs-and-reorgs/19674">[1]</a>, <a href="https://ethresear.ch/t/blobs-reorgs-and-the-role-of-mev-boost/19783">[2]</a>, <a href="https://mirror.xyz/preconf.eth/cxUO8pPBfqnqAlzFUzoEUa6sgnr68DRmsNhBWPb2u-c" rel="noopener nofollow ugc">[3]</a> has focused on how larger blobs increase reorg risk due to higher latency. This could incentivize builder censorship to reduce latency by excluding blobs from blocks.</p>
<p>Despite the blob market being under capacity and the base fee remaining at 1 wei, research <a href="https://mirror.xyz/preconf.eth/6lZYL62DR9U14KC7wCC4RHReVdHcBeMy5PKeHVbPq5k" rel="noopener nofollow ugc">[4]</a> shows that rollups like Optimism and Base often have high slot inclusion rates, taking more than five slots to be included. Given the underutilized market, this seems counterintuitive, suggesting possible latency censorship. However, the current blob submission strategies and blob market combinatorics suggest that higher slot inclusion rates may indicate increased competition between blob producers rather than builder censorship.</p>
<h2><a class="anchor" href="https://ethresear.ch#blob-submission-strategies-3" name="blob-submission-strategies-3"></a>Blob Submission Strategies</h2>
<p>The below table <a href="https://analytics.mev-commit.xyz/dashboard" rel="noopener nofollow ugc">from the dashboard</a> shows a 7 day snapshot of the largest blob market participants.</p>
<p>There are now 3 major strategies across the number of blobs:</p>
<ul>
<li>submit the max 5-6 blobs at a time (blast, base, linea, optimism)</li>
<li>submit 3-4 blobs at a time (arbitrum, zksync)</li>
<li>submit 1-2 blobs at a time (taiko, metal, paradex, scroll)<br />
<div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/e/7e768a0ed198c966965d14171b97d1c2600eea7d.png" title="image"><img alt="image" height="244" src="https://ethresear.ch/uploads/default/optimized/3X/7/e/7e768a0ed198c966965d14171b97d1c2600eea7d_2_690x244.png" width="690" /></a></div></li>
</ul>
<p>Aggregating blobs into fewer transactions reduces transaction expenses (base fee, blob fee, priority fee) but increases slot inclusion times. In contrast, smaller blob transactions improve slot inclusion times at the cost of higher transaction expenses.</p>
<h2><a class="anchor" href="https://ethresear.ch#slot-inclusion-rates-4" name="slot-inclusion-rates-4"></a>Slot Inclusion Rates</h2>
<p>The next chart displays a time series overlay of base block demand (total transaction fees and base fee in gwei) with the slot inclusion rate for each blob transaction. It shows high slot inclusion rates, up to 30 slots, even during periods of low blockspace demand.<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/3/832319c888134f3fe0b465411923147c0c85c5fa.png" title="image"><img alt="image" height="258" src="https://ethresear.ch/uploads/default/optimized/3X/8/3/832319c888134f3fe0b465411923147c0c85c5fa_2_690x258.png" width="690" /></a></div><p></p>
<p>The table mentioned earlier above contains the average slot inclusion rate for each rollup. Base, which submits the largest blobs in each transaction has the highest, averaging 13 slots. Taiko has the lowest average at 1.7 slots and submits only single blobs for each transaction right now.</p>
<p><strong>Base slot inclusion rate:</strong><br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/a/5/a5f9d2f0d94388d88993444ae9da999347121e7e.png" title="image"><img alt="image" height="300" src="https://ethresear.ch/uploads/default/optimized/3X/a/5/a5f9d2f0d94388d88993444ae9da999347121e7e_2_690x300.png" width="690" /></a></div><p></p>
<p>taiko slot inclusion rate<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/e/ce84eb73e15b668adaa7dc811f23e8c3606000ee.png" title="image"><img alt="image" height="300" src="https://ethresear.ch/uploads/default/optimized/3X/c/e/ce84eb73e15b668adaa7dc811f23e8c3606000ee_2_690x300.png" width="690" /></a></div><p></p>
<h2><a class="anchor" href="https://ethresear.ch#builder-slot-inclusion-rates-5" name="builder-slot-inclusion-rates-5"></a>Builder Slot Inclusion Rates</h2>
<p>This table examines slot inclusion rates from the builder’s perspective, including the number of blocks, blob transactions, average blob count, and priority fees collected.</p>
<p>A higher slot inclusion rate means a blob has waited longer to be included in a block. An efficiency metric would be to have the lowest possible slot inclusion rate, indicating that builders are including blobs sooner rather than later.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/9/f/9fe6938327d570742a8b7f278788cacfa4df81ca.png" title="image"><img alt="image" height="211" src="https://ethresear.ch/uploads/default/original/3X/9/f/9fe6938327d570742a8b7f278788cacfa4df81ca.png" width="690" /></a></div><p></p>
<p>Builders like Titan and Beaverbuild have more efficient blob slot inclusion rates than vanilla builders. They also have the lowest average blobs per block. This could be due to their efficiency in accepting strategies like Taiko blobs over other block builders.</p>
<h2><a class="anchor" href="https://ethresear.ch#combinatorics-6" name="combinatorics-6"></a>Combinatorics</h2>
<p><a href="https://colab.research.google.com/drive/1EeRpWjb0meIi53IyyyZu7QWmg8HqVAMr#scrollTo=PDAJADyB24Jv" rel="noopener nofollow ugc">This notebook</a> uses dynamic programming to count the number of combinations of blobs for the current blob market. Given the current 6 blob per block capacity and 6 blobs per block, there are 11 possible combinations.</p>
<p><strong>Occurrences of each number:</strong><br />
1: 19<br />
2: 8<br />
3: 4<br />
4: 2<br />
5: 1<br />
6: 1</p>
<p>A trivial observation is that there is only one combination in which a block can fit 5 or 6 blobs. Since 4 out of 10 rollups submit these 5 and 6 blob transactions, there will only be one winner. Additionally, a single 1-blob transaction can “censor” a 6-blob transaction for an entire slot by being accepted first.</p>
<p>The combinatorics of the current blob market size suggest that the small size itself is causing higher slot inclusion problems, rather than blob censorship latency. This indicates that censorship is not from builders but from competition among blob users.</p>
<p>This raises an important question: what is the optimal maximum number of blobs allowed in a block relative to the maximum number that can fit in a block? Would the combinatorics be more favorable if the maximum blob size were 3 instead of 6? Would it be better to allow 9 blobs per block instead of 8? There is an economic incentive to group blobs as large as possible to save on costs, which disproportionately favors larger rollups over smaller ones until blob sharing becomes feasible.</p>
<h2><a class="anchor" href="https://ethresear.ch#bidding-strategies-7" name="bidding-strategies-7"></a>Bidding Strategies</h2>
<p>Currently, blobs use static bidding strategies, generally resubmitting their blobs if their bids sit in the mempool for too long. This shows a certain level of insensitivity to slot inclusion for each rollup. If a blob is delayed for 100 slots, there seem to be no consequences or incentives to increase slot inclusion rates at this time.</p>
<p>The two charts below show sample bidding strategies used by Base and Taiko, just two examples of the rollup strategies available on the dashboard. Base averages a priority fee of 4.5 gwei, while Taiko averages 2.9 gwei. There is no correlation between priority bids and base fee fluctuations.</p>
<p><strong>base:</strong><br />
<img alt="image" height="336" src="https://ethresear.ch/uploads/default/original/3X/8/7/8785ccb0b147a318d6426a694bf7697d3f1a5383.png" width="501" /></p>
<p><strong>taiko:</strong><br />
<img alt="image" height="336" src="https://ethresear.ch/uploads/default/original/3X/9/1/91bab571ac6836399edf78b7c7ce757ad62cf2ed.png" width="501" /></p>
<p>Resubmitting blobs through the mempool is expensive and generally not recommended as a good practice. This creates the problem of how blob producers can become more competitive in their bidding strategies if they need to make their slot inclusion rates more efficient.</p>
<p>One solution is to use preconfirmations. For example, using a protocol such as mev-commit to attach preconf bids to blob transactions would allow rollups to dynamically adjust their bids without having to resubmit blobs into the mempool. A stronger solution would be <a href="https://ethresear.ch/t/blob-preconfirmations-with-inclusion-lists-to-mitigate-blob-contention-and-censorship/19150">to receive preconfirmations from proposers</a> to guarantee that builders wouldn’t be able to censor blobs.</p>
<h3><a class="anchor" href="https://ethresear.ch#conclusion-8" name="conclusion-8"></a>Conclusion</h3>
<p>Analysis of slot inclusion rates and blob market combinatorics reveals a complex interplay between efficient slot inclusion, competition, and potential censorship. While current data suggests that high slot inclusion rates are primarily driven by competition among blob users, there remain several unanswered questions:</p>
<ul>
<li>What is the optimal maximum number of blobs per block to balance efficiency and fairness?</li>
<li>How can blob producers develop more competitive bidding strategies?</li>
<li>Could the implementation of dynamic bidding strategies or preconfirmations significantly reduce slot inclusion times?</li>
<li>What long-term effects might increased competition and potential latency censorship have on the blob market?</li>
</ul>
<p>The combinatorics of the blob market are a fundamental factor affecting slot inclusion efficiency and cost. By understanding and optimizing these combinatorial constraints, it is possible to enhance market dynamics, reduce costs, and improve transaction efficiency for all participants. Further research and experimentation are needed to address these questions and optimize the blob market for all participants.</p>
            <p><small>3 posts - 2 participants</small></p>
            <p><a href="https://ethresear.ch/t/slot-inclusion-rates-and-blob-market-combinatorics/19817">Read full topic</a></p>
]]></content:encoded>
<pubDate>Fri, 14 Jun 2024 16:47:05 +0000</pubDate>
</item>
<item>
<title>A simple, small, mev-boost compatible preconfirmation idea</title>
<link>https://ethresear.ch/t/a-simple-small-mev-boost-compatible-preconfirmation-idea/19800</link>
<guid>https://ethresear.ch/t/a-simple-small-mev-boost-compatible-preconfirmation-idea/19800</guid>
<content:encoded><![CDATA[
<div> 关键词：MEV-boost、preconfs、proposer、relayer、merging policies

总结:<br />
本文提出了一种扩展MEV-boost以支持预配置交易（preconfs）的机制。该想法保持了现有MEV-boost流程的稳定性，仅改变提案阶段，让提案者在开始投票前提供预配置交易列表。提案者发送包含交易要求的签名JSON对象给中继器，如必须包含或排除的交易。中继器根据合并策略处理这些信息，同时保持传统MEV-boost拍卖的兼容性。尽管存在一些挑战，如中继器的额外计算负担和预配置信息的一致性问题，但作者认为这个设计有助于减少生态系统分裂，且具有渐进式替代MEV-boost的潜力。 <div>
<p><strong>Disclaimer</strong>: This post will not contain any nice images, because I am artistically inept.</p>
<p>The reasons why I’m writing this are the following:</p>
<ol>
<li>Preconfs are a very hot topic right now and many people are working on them;</li>
<li>As usual, some of the proposed solutions advocate for punching changes all the way into the main Ethereum protocol. I’m personally not a fan of this, since life is already full of <em>oh my God, what have I done?™</em> moments and <em>more drama™</em> is the least thing everyone probably needs.</li>
<li>MEV-boost is probably the <em>only</em> thing this community has really almost universally agreed upon since MEV has been a thing. So I’d very much try to preserve backwards-compatibility with MEV-boost and generalize on this than coming up with more innovative ways to balkanize our ecosystem even further.</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#a-primer-on-mev-boost-1" name="a-primer-on-mev-boost-1"></a>A primer on MEV-boost</h2>
<p>This section exists just so that everyone is on the same page. Feel free to skip it or to insult me if you think I summarised things stupidly.</p>
<p>In layman terms, MEV-boost works like this:</p>
<ol>
<li>Proposer polls the relayer(s) for their best blocks;</li>
<li>Relayer(s) send their best block headers to proposer;</li>
<li>Proposer picks the best block by comparing the block headers received and the block built in-house.</li>
<li>For an in-house block, proposer just signs and broadcasts. For a mev-boost block, proposer signs the header. Relay will broadcast the complete block revealing the payload.</li>
</ol>
<p>This mechanism is nice because the only party that builders have to trust is relayer: Proposer cannot unbundle blocks and scam builders.</p>
<h2><a class="anchor" href="https://ethresear.ch#the-actual-idea-2" name="the-actual-idea-2"></a>The actual idea</h2>
<p>The idea I have in mind works towards extending mev-boost by allowing for preconfs (and most likely for a lot of other stuff if one wants to). Notably, it does not change points 2,3,4 in the previous section, but only point 1.</p>
<p>Suppose proposer has a stash of preconfed txs on the side. The only thing the idea assumes is the following:</p>
<blockquote>
<p>By the time Proposer starts polling, it needs to have a finalized lists of preconfed txs to include.</p>
</blockquote>
<p>The reason for this will become clear shortly. Having this list at hand, proposer sends a signed JSON object to the relayer when it polls, containing the preconfed txs. This object could look, for instance, like this:</p>
<pre><code class="lang-JSON">{
    proposer: address,
    slotNumber: int,
    gasUsed: int,
    blobsUsed: int.
    mergingPolicy: int,
    mustBeginWith: txBundle,
    mustContain: txBundle,
    mustOmit: txBundle,
    mustEndWith: txBundle,
    otherStuff: JSON,
    signature : signature
}
</code></pre>
<p><strong>This design is just an idea. It is by no means fixed yet and most likely can be improved upon both in conceptual and performance terms, so take it with a grain of salt.</strong><br />
The fields <code>proposer</code> and <code>slotNumber</code> are obvious. The fields <code>mergingPolicy</code>, <code>mustBeginWith</code>, <code>mustContain</code>, <code>mustOmit</code>, <code>mustEndWith</code> can all be empty: They contain bundles of transactions that must (or must not) be included in the block. These fields are, effectively, the ones that proposer can use to signal relayer that 'hey, I need the block to respect these requirements, because of previous agreement I made with other parties."</p>
<p>How the proposer comes to define this json object is not our concern, and is outside of the scope of this idea. Just for the sake of clarity though, let’s consider some examples: For instance, <a href="https://docs.xga.com" rel="noopener nofollow ugc">XGA</a>, one of the projects <code>20[ ]</code> is contributing to, provides preconfs as tokenized bottom-of-block space. As such, XGA-style preconfs will produce objects where only <code>mustEndWith</code> is not empty.</p>
<p>The fields <code>gasUsed</code> and <code>blobsUsed</code> tell the relay how much gas and blobs the ‘preconf space’ already claimed. <code>otherStuff</code> exists to be able to extend this standard in the future without <em>more drama™</em>.</p>
<h3><a class="anchor" href="https://ethresear.ch#merging-policies-3" name="merging-policies-3"></a>Merging policies</h3>
<p>The <code>mergingPolicy</code> fields instructs the relay about how to deal with all this information. This is fundamental because, in the end, the relay will still run a traditional mev-boost auction for the remaining blockspace. As soon as a block is built by more than one party there’s a risk that different parties may step up on each other’s toes. As such, <code>mergingPolicy</code> serves as a well-defined conflict resolution policy. If you need a mental reference, think about git conflicts and automated ways to solve them if you so like.</p>
<p>How to define merging policies is up for debate. The community could agree on a common repository where merging policies are defined, voted and agreed upon, and where merging algos are explicitly provided. So, for instance, one merging policy could be:</p>
<blockquote>
<p>If the payload coming from the builder contains a transaction that also appears in the preconf bundle, deal with it in the following way:</p>
</blockquote>
<p>As said above, XGA sells BOB as preconfs, and leaves TOB open for traditional mev-boost auctions. As such, it has already defined and implemented a merging policy for its bottom of the block case, which will hopefully be open sourced soon.</p>
<h3><a class="anchor" href="https://ethresear.ch#what-does-the-relay-do-4" name="what-does-the-relay-do-4"></a>What does the relay do?</h3>
<p>This is probably already kinda clear at this point, but to make it explicit: The relay receives this signed JSON object when the proposer polls. What should it do with it? First of all, it should make some of these fields public to the builders, such as <code>mergingPolicy</code>, <code>gasUsed</code>, <code>blobsUsed</code> and <code>mustOmit</code>. This way builders will know what they can build.</p>
<p>When a block from a builder is received, the relayer will <strong>unbundle</strong> the block and apply the merging policy to merge it with the preconfed txs. The <strong>relay</strong> will sign the block header, and send it to the proposer.</p>
<p>From the POV of a builder, everything is kinda the same. They create their block using the info provided by the relay (in the simplest case this just means using slightly less gas than limit), and submit it as their bid.</p>
<p>From this point on, everything works as in traditional MEV-boost.</p>
<h2><a class="anchor" href="https://ethresear.ch#analysis-5" name="analysis-5"></a>Analysis</h2>
<p>Ok, so let’s run a rapid analysis of this thing.</p>
<h3><a class="anchor" href="https://ethresear.ch#pros-6" name="pros-6"></a>Pros</h3>
<ol>
<li>
<p>Changes to MEV-boost proper are really minimal. We just need to define an API that MEV-boost must listen to to build the polling payload, and redefine the polling logic.</p>
</li>
<li>
<p>Very little work from Proposer’s side. More work may be needed depending on the preconf system a given proposer wants to use, but then again this is out of the scope of this idea.</p>
</li>
<li>
<p>Very little work from builder’s side unless people go overly crazy with merging policies. I do not think this is necessarily a problem tho as an overly deranged merging policy would result in builders not submitting anything, and most likely in relayers not taking bets in the first place. So I’d bet that this could pretty much evolve as a ‘let the markets decide’ thing.</p>
</li>
<li>
<p>This idea is straightforwardly backwads-compatible with traditional MEV-boost: If the polling payload is empty, we collapse to a traditional MEV-boost auction with no other requisites.</p>
</li>
<li>
<p>This idea allows for gradual phasing out of MEV-boost if the community so decides. For instance, proposers may agree to produce bundles where <code>usedGas</code> is a very low parameter in the beginning (it won’t exceed 5M for XGA, for instance), meaning that the majority of blockspace would come from traditional building, with only a tiny part being preconfs or more generally ‘other stuff’. This parameter may then be increasingly crancked up or varied with time if the community so decides, effectively phasing out traditional block building in favor of ‘something else’. In this respect yes, I know I’m being vague here but when it comes to how this thing could be adopted I can only speculate.</p>
</li>
<li>
<p>This system can be extended in many ways, and it is flexible. Merging policies could be defined democratically, and the polling info could be extended effectively implementing something akin to PEPSI, for instance. Another possible extension/evolution can be using <code>otherStuff</code> to define Jito-style auctions. I mean, there’s really a plethora of ways to go from here.</p>
</li>
<li>
<p>The polling payload is signed by the proposer, and the block header is signed by the relayer. This keeps both parties in check as we accumulate evidence for slashing both. For instance:</p>
<ul>
<li>Imagine I get some preconf guarantee from proposer and that I have evidence of this. Again how this happens is outside of the scope of this post, as this mechanism is agnostic wrt how preconfs are negotiated.</li>
<li>Now suppose furthermore than my preconfed tx does <strong>not</strong> land in the block.</li>
<li>I can use the chain of signed objects to challenge both relayer and proposer. If my tx wasn’t in the polling info signed by proposer, that’s proposer’s fault. On the other hand, if it was, but it wasn’t in the block, then it’s relayer’s fault. I think this is enough to build a slashing mechanism of sorts, which could for instance leverage some already available restaking solution.</li>
</ul>
<p><strong>Note:</strong> If there’s enough interest in this idea, we as 20[  ] can throw some open games at it and simulate the various scenarios. Let me know!</p>
</li>
<li>
<p><strong>Ethereum protocol doesn’t see any of this.</strong> So if it fucks up, we just call it a day and retire in good order without having caused the apocalypse: Relays will only accept empty payloads, proposers will only send empty payloads, and we’ll essentially revert to mev-boost without anyone having to downgrade their infra. I think this is the main selling point of this idea: The amount of ways to make stuff explode in mev-related infraland are countless, so this whole idea was built with a ‘it has to be failsafe’ idea in mind.</p>
</li>
</ol>
<h3><a class="anchor" href="https://ethresear.ch#cons-7" name="cons-7"></a>Cons</h3>
<ol>
<li>
<p>Relayer must unbundle builder blocks to do the merging. I do not think this creates a huge trust issue as relayer can already do this as of now: In general, a relayer that scams builders is a relayer that won’t be used again, and will go out of business quickly.</p>
</li>
<li>
<p>Relayer must do computational work. This is probably the major pain point. This idea entails slightly more latency, as an incoming bid cannot be relayed instantly because <code>mergingPolicy</code> has to be applied. The computational penalty is furthermore heavily dependent on how deranged the merging policy is. As a silver lining, this computational work is <em>provable</em> as both the merging info and the resulting block are signed. The result is that we have <strong>provable evidence to remunerate a relay for its work if we want to</strong>, possibly solving a major pain point for relayers in traditional mev-boost.</p>
</li>
<li>
<p>Relayer is slashable if it screws up. Again, how this should be implemented is outside of the scope of this idea as this mechanism only accounts for the needed trail of evidence to implement slashing, but does not deal with the slashing per sé. Anyway, it is still worth reasoning on the possible consequences of this: If slashing policies are implemented, Relayers will most likely need to provide some collateral or implement some form of captive insurance. Again, this may signify more complexity on one hand but also opportunity on the other, as relayers may for instance decide to tokenize said collateral and develop mechanisms to make money out of these newly created financial instruments. As relayers are private enterprises I’ll leave these considerations to the interested parties.</p>
</li>
<li>
<p><strong>Polling info must stay fixed</strong>. This is related to point 3 above and point 6 of the <a href="https://ethresear.ch#pros">Pros</a> subsection: If the polling info changes all the time, this means huge computational stress for the relayer, and it furthermore allows for malicious behavior from the proposer: For instance, a proposer could send two different polling payloads, and include a given preconfed tx only in one of them. How to resolve these inconsistencies is an open question. In my opinion, the wisest and simplest thing to do would be requiring the polling info to be fixed, meaning that if proposer signs conflicting payloads for the same slot this should be considered akin to equivocation, and thus a slashable offence.</p>
<p>By the way, the consequence of this is that the idea proposed here necessarily excludes some preconf use cases. This is related to my comment <a href="https://ethresear.ch/t/strawmanning-based-preconfirmations/19695/2">here</a> and I think it is unavoidable if we want to keep MEV-boost around. As the majority  of revenue from MEV comes precisely from the bids of very refined, high-time frame searchers, and as I am quite sure that validators don’t want to give this money up at least for now, ‘leaving these players be’ by ruling out such preconf use-cases is in my opinion the most practical option, and exactly the rationale motivating this idea.</p>
</li>
</ol>
<h2><a class="anchor" href="https://ethresear.ch#closing-remarks-8" name="closing-remarks-8"></a>Closing remarks</h2>
<p>That’s it. If the idea is interesting enough let me know, I’ll be happy to start a discussion around it.  The <code>20[ ]</code> team will also be around at EthCC if you want to discuss this in person.</p>
            <p><small>8 posts - 5 participants</small></p>
            <p><a href="https://ethresear.ch/t/a-simple-small-mev-boost-compatible-preconfirmation-idea/19800">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 13 Jun 2024 13:33:55 +0000</pubDate>
</item>
<item>
<title>One-bit-per-attester inclusion lists</title>
<link>https://ethresear.ch/t/one-bit-per-attester-inclusion-lists/19797</link>
<guid>https://ethresear.ch/t/one-bit-per-attester-inclusion-lists/19797</guid>
<content:encoded><![CDATA[
<div> 关键词：Inclusion lists, transaction selection, RANDAO_REVEAL, Reed-Solomon decoding, fork choice rule.

总结:
本文提出了一种新的机制，用于在区块链中实现更去中心化的交易入选列表（Inclusion lists）。机制的核心是利用RANDAO_REVEAL生成随机种子，将验证者分为小组，每个小组负责查找优先级高、费用支付的交易，并通过Erasure编码提供与种子相关联的交易部分。如果多数验证者诚实，Reed-Solomon解码可以确定交易；否则，可能需要使用更复杂的方法恢复交易。验证者的选择和交易的入选受制于区块生产者的决定，但通过调整时间权重和fork choice规则，可以增加对长期未被选中的交易的包容性。这种机制旨在减少集中化风险，提高去中心化程度。 <div>
<p>Inclusion lists are a technology for distributing the authority for choosing which transactions to include into the next block. Currently, the best idea for them is to have an actor that is from a set that is likely to be highly decentralized (eg. consensus block proposers) generate the list. This authority is decoupled from the right to <em>order</em> (or <em>prepend</em>) transactions, which is an inherently economies-of-scale-demanding and so likely to be highly concentrated in practice.</p>
<p>But what if we could avoid putting the responsibility onto a <em>single</em> actor, and instead put it on a <em>large set of actors</em>? In fact, we can even do it in such a way that it’s semi-deniable: from each attester’s contribution, there is no clear evidence of which transaction they included, because one individual piece of provided data could come from multiple possible transactions.</p>
<p>This post proposes a possible way to do this.</p>
<h3><a class="anchor" href="https://ethresear.ch#mechanism-1" name="mechanism-1"></a>Mechanism</h3>
<p>When the block for slot N is published, let <code>seed</code> be the RANDAO_REVEAL of the block. Suppose for convenience that each transaction is under <code>T</code> bytes (eg. <code>T = 500</code>); we can say in this initial proposal that larger transactions are not supported. We put all attesters for that slot into groups of size <code>2 * T</code>, with <code>k = attesters_per_slot / (2 * T)</code> groups.</p>
<p>Each attester is chosen to be the j’th attester of the i’th group. They identify the highest-priority-fee-paying valid transaction which was published before the slot N block, and where <code>hash(seed + tx)</code> is between <code>2**256 / k * i</code> and <code>2**256 / k * (i+1)</code>. They erasure-code that transaction to <code>2T</code> bits, and publish the j’th bit of the erasure encoding as part of their attestation.</p>
<p>When those attestations are included in the next block, an algorithm such as <a href="https://en.wikipedia.org/wiki/Berlekamp%E2%80%93Welch_algorithm">Berlekamp-Welch</a> is used to try to extract the transaction from the provided attester bits.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/d/e/deedccb04e5bb133ccacdbe2c2c17d1e5abdc3ce.png" title="attester_inclusion_list.drawio"><img alt="attester_inclusion_list.drawio" height="271" src="https://ethresear.ch/uploads/default/optimized/3X/d/e/deedccb04e5bb133ccacdbe2c2c17d1e5abdc3ce_2_690x271.png" width="690" /></a></div><p></p>
<p>The Reed-Solomon decoding will fail in two cases:</p>
<ol>
<li>If too many attesters are dishonest</li>
<li>If attesters have different views about whether a particular transaction was published before or after the block, and so they are split between providing bits for two or more different transactions.</li>
</ol>
<p>Note that in case (2), if the transactions are sufficiently small, advanced <a href="https://www.cs.cmu.edu/~venkatg/teaching/codingtheory/notes/notes10.pdf">list decoding algorithms</a> may nevertheless be able to recover several or all of the transactions!</p>
<p>The next block proposer will be able to see which transactions the attestations imply, and so they will be able to block transactions from the list by selectively failing to include attestations. This is an unavoidable limitation of the scheme, though it can be mitigated by having a fork choice rule discount blocks that fail to include enough attestations.</p>
<p>Additionally, the mechanism can be modified so that if a transaction has not been included for 2+ slots, <em>all</em> attesters (or a large fraction thereof) attempt to include it, and so any block that fails to include the transaction would lose the fork choice. One simple way to do this is to score transactions not by <code>priority_fee</code>, but by <code>priority_fee * time_seen</code>, and at the same time have a rule that a transaction that has been seen for <code>k</code> slots is a candidate not just for attester group <code>i</code>, but also for attester group <code>i...i+k-1</code> (wrapping around if needed).</p>
            <p><small>8 posts - 7 participants</small></p>
            <p><a href="https://ethresear.ch/t/one-bit-per-attester-inclusion-lists/19797">Read full topic</a></p>
]]></content:encoded>
<pubDate>Thu, 13 Jun 2024 09:33:36 +0000</pubDate>
</item>
<item>
<title>Torrents and EIP-4444</title>
<link>https://ethresear.ch/t/torrents-and-eip-4444/19788</link>
<guid>https://ethresear.ch/t/torrents-and-eip-4444/19788</guid>
<content:encoded><![CDATA[
<div> 关键词：EIP-4444、Torrents、Ethereum、Pre-merge data、Merkle roots

总结:
EIP-4444目标是减少以太坊节点所需存储的历史数据。文章介绍了使用BitTorrent技术来分发历史数据的方法，通过在geth v1.14.3版本上创建era文件并验证根文件（roots.txt）来实现。这个过程产生了427GB的torrent文件，用于同步pre-merge数据。虽然torrent有依赖多个活跃节点和增加节点网络需求的缺点，但它提供了一种可能的解决方案。客户端可以选择不预下载数据，用户可以通过命令行或特定标志获取。要重现或验证torrent，需同步geth节点、执行era文件导出和创建torrent，以及使用era工具验证数据完整性。 <div>
<h1><a class="anchor" href="https://ethresear.ch#torrents-and-eip-4444-1" name="torrents-and-eip-4444-1"></a>Torrents and EIP-4444</h1>
<h3><a class="anchor" href="https://ethresear.ch#introduction-2" name="introduction-2"></a>Introduction</h3>
<p>EIP-4444 aims to limit the historical data that Ethereum nodes need to store. This EIP has two main problems that require solutions: Format for history archival and Methods to reliably retrieve history. The client teams have agreed on a common <a href="https://ethresear.ch/t/era-archival-files-for-block-and-consensus-data/13526">era files</a> format, solving one half of the problem. The second half of the problem, i.e Method to reliably retrieve history will likely not rely on a single solution. Some client teams may rely on the <a href="https://ethereum.org/en/developers/docs/networking-layer/portal-network/" rel="noopener nofollow ugc">Portal network</a>, some rely on torrents, others might rely on some form of snapshot storage.</p>
<h3><a class="anchor" href="https://ethresear.ch#torrents-for-eip-4444-3" name="torrents-for-eip-4444-3"></a>Torrents for EIP-4444</h3>
<p>Torrents offer us a unique way to distribute this history, torrents as a technology have existed since 2001 and have withstood the test of time. Some client teams, such as <a href="https://github.com/ledgerwatch/erigon" rel="noopener nofollow ugc">Erigon</a> already include a method to sync via torrents that has run in production systems.</p>
<p>In order to make some progress on the Torrent approach of history retrieval, the files would first be required. So an era file export was made on a <a href="https://github.com/ethereum/go-ethereum/" rel="noopener nofollow ugc">geth</a> running version <code>v1.14.3</code> . To explore the initial idea, the torrent approach chose pre-merge data as a target. The merge occurred at block height <a href="https://etherscan.io/block/15537393" rel="noopener nofollow ugc">15537393</a>, meaning all pre-merge data could be archived by choosing a range of 0 to block 15537393. The era files were then created using the command <code> geth --datadir=/data export-history /data/erafiles 0 15537393</code>.</p>
<p>Once the era files were created, they were verified using the command <code>era verify roots.txt</code>, with the source of the <code>roots.txt</code> file being <a href="https://gist.githubusercontent.com/lightclient/528b95ffe434ac7dcbca57bff6dd5bd1/raw/fd660cfedb65cd8f133b510c442287dc8a71660f/roots.txt" rel="noopener nofollow ugc">this</a>. The entire process has been outlined in <a href="https://github.com/ethereum/go-ethereum/pull/26621#issuecomment-1434023464" rel="noopener nofollow ugc">this PR comment</a>. The verification output was found to be this log message: <code>Verifying Era1 files             verified=1896,  elapsed=5h21m49.184s</code></p>
<p>The output era files were then uploaded onto a server and a torrent was created using the software <code>mktorrent</code>. An updated list of trackers was found using the github repo <a href="https://github.com/ngosang/trackerslist" rel="noopener nofollow ugc">trackerslist</a>. The trackers chosen were a mix of http/https/udp in order to allow for maximal compatibility. The chunk size of the torrent was chosen to be 64MB, which was the max allowed and recommended value for a torrent of this size.</p>
<p>The result of this process is now a torrent of size 427GB. This torrent can be imported with <a href="https://ethresear.ch">this magnet link</a>  and a torrent client would be able to pull the entire pre-merge history as era files.</p>
<h4><a class="anchor" href="https://ethresear.ch#tradeoffs-4" name="tradeoffs-4"></a>Tradeoffs</h4>
<p>There are of course some tradeoffs with torrents, as with many of the other EIP-4444 approaches:</p>
<ul>
<li>Torrents rely on a robust set of peers to share the data, there is however no way to incentivise or ensure that this data is served by peers</li>
<li>A torrent client would need to be included in the client releases and some client languages might not have a torrent library</li>
<li>Torrents would de-facto expect the nodes to also seed the content they leech, this would increase node network requirements if they choose to store history</li>
<li>The JSON-RPC response needs to take into account that it may not have the data to return a response in case the user decides to not download pre-merge data</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#conclusion-5" name="conclusion-5"></a>Conclusion</h3>
<p>A client could potentially include this torrent into their releases and avoid syncing pre-merge data by default, which could then be fetched via torrent if a user requests it (perhaps with a flag similar to <code>--preMergeData=True</code>). The client could also hardcode the hash of the expected data, ensuring that the data retrieved matches what they expect.</p>
<h3><a class="anchor" href="https://ethresear.ch#instructions-for-re-creating-torrent-6" name="instructions-for-re-creating-torrent-6"></a>Instructions for re-creating torrent:</h3>
<ul>
<li>Sync a geth node using the latest release</li>
<li>Stop the geth node and run <code>geth --datadir=/data export-history /data/erafiles 0 15537393</code> to export the data in a folder called <code>data/erafiles</code>(Warning, this will use ~427GB of additional space)</li>
<li>Use the <code>mktorrent</code> tool or the <code>rutorrent</code> GUI to create a torrent. Choose the <code>/data/erafiles/</code> folder as the source for the data. Next, obtain the latest open trackers from <a href="https://github.com/ngosang/trackerslist?tab=readme-ov-file" rel="noopener nofollow ugc">this github repository</a>. Choose a healthy mix of udp/http/https trackers and choose the chunk size of the torrent to be 64MB.</li>
<li>The tool should output a <code>.torrent</code> file, the GUI will also allow you to copy a magnet link if that is required</li>
</ul>
<h3><a class="anchor" href="https://ethresear.ch#instructions-for-download-and-verification-of-torrent-data-7" name="instructions-for-download-and-verification-of-torrent-data-7"></a>Instructions for download and verification of torrent data:</h3>
<ul>
<li>Download the torrent data with this magnet link and in a torrent client of your choice: <a href="https://ethresear.ch">link</a></li>
<li>Clone the latest release of <a href="https://github.com/ethereum/go-ethereum/" rel="noopener nofollow ugc">geth</a> and install the dependencies</li>
<li>Run <code>make all</code> in the geth repository to build the <code>era</code> binary</li>
<li>Fetch the <code>roots.txt</code> file with the command: <code>wget https://gist.githubusercontent.com/lightclient/528b95ffe434ac7dcbca57bff6dd5bd1/raw/fd660cfedb65cd8f133b510c442287dc8a71660f/roots.txt</code></li>
<li>Run <code>era verify roots.txt</code> in the folder to verify the integrity of the data</li>
</ul>
            <p><small>15 posts - 5 participants</small></p>
            <p><a href="https://ethresear.ch/t/torrents-and-eip-4444/19788">Read full topic</a></p>
]]></content:encoded>
<pubDate>Wed, 12 Jun 2024 09:35:32 +0000</pubDate>
</item>
<item>
<title>Blobs, Reorgs, and the Role of MEV-Boost</title>
<link>https://ethresear.ch/t/blobs-reorgs-and-the-role-of-mev-boost/19783</link>
<guid>https://ethresear.ch/t/blobs-reorgs-and-the-role-of-mev-boost/19783</guid>
<content:encoded><![CDATA[
<div> 关键词：blobs、MEV-Boost、reorgs、latency、block propagation

总结:
这篇文章探讨了区块中的大对象（blobs）对以太坊网络延迟和重组（reorgs）的影响，特别是与MEV-Boost（矿工提取价值优化）相关的生态系统。非MEV-Boost用户平均包含更多blobs，导致他们区块被重组的概率较高。MEV-Boost用户由于其低延迟连接和专业性，区块被重组的可能性显著较低。研究还指出不同构建者和中继器可能采用策略来处理blobs，比如Rsync-Builder和Flashbots的平均blob数量较少。未来的研究将关注节点能力的提升和减少非MEV-Boost用户的重组率。随着blob市场的发展，其交易提示可能会追平常规交易。 <div>
<h1><a class="anchor" href="https://ethresear.ch#blobs-reorgs-and-the-role-of-mev-boost-1" name="blobs-reorgs-and-the-role-of-mev-boost-1"></a>Blobs, Reorgs, and the Role of MEV-Boost</h1>
<p><strong>The TL;DR is:</strong></p>
<ul>
<li><strong>Builders</strong> might have an incentive to not include blobs because of the higher latency they cause.</li>
<li><strong>Non-MEV-Boost users</strong> include, on average, more blobs in blocks than MEV-Boost builders.</li>
<li><strong>MEV-Boost users</strong> show a significantly lower probability of being reorged than <em>Non-MEV-Boost</em> users (see section <em>MEV-Boost and Reorgs</em> for details).</li>
<li><strong>Rsync-Builder</strong> and <strong>Flashbots</strong> have a lower average number of blobs per block than other builders.</li>
</ul>
<hr />
<p>In a <a href="https://ethresear.ch/t/big-blocks-blobs-and-reorgs/19674">recent analysis on big blocks, blobs and reorgs</a>, we could see the impact of blobs on the reorg probability.</p>
<p><strong>In the following, I want to expand on this by taking the MEV-Boost ecosystem into account.</strong></p>
<p><strong>The fundamental question is…</strong><br />
-&gt; <strong>“<em>Does MEV-Boost impact reorgs, and if so, by how much?</em>”</strong></p>
<p>Blobs are “<em>big</em>” and big objects cause higher latency. Thus, one might expect builders to not include blobs into their blocks in scenarios in which:</p>
<ul>
<li>The builder is submitting its block late in the slot to minimize latency (see timing games).</li>
<li>The builder wants to capture a high MEV opportunity and doesn’t want to risk unavailable blobs invalidating its block.</li>
<li>The proposer is less well connected (because the gossiping starts later in the slot).</li>
</ul>
<p><strong>Builders</strong> might demand to be <strong>compensated</strong> through priority fees for including transactions which might cause blocks to be propagated with higher latency. Until 4844, such transactions have been those with a lot of calldata. As of 4844, blobs are the main drivers of latency.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/d/8db1993891d52c3c8be9d7c6adde8633810ad15b.png" title="tx_type_prio_fee_all (2)"><img alt="tx_type_prio_fee_all (2)" height="345" src="https://ethresear.ch/uploads/default/optimized/3X/8/d/8db1993891d52c3c8be9d7c6adde8633810ad15b_2_690x345.png" width="690" /></a></div><p></p>
<p><strong>As visible in the above chart, blob transactions don’t tip as much as regular Type-2 transactions.</strong><br />
Based on that, blobs don’t give builders a significant edge over other builders competing for the same slot.<br />
Another explanation could be private deals between builders and rollups to secure timely inclusion of blob transactions for a fee paid through side channels.</p>
<h2><a class="anchor" href="https://ethresear.ch#mev-boost-and-reorgs-2" name="mev-boost-and-reorgs-2"></a>MEV-Boost and Reorgs</h2>
<p>The MEV-Boost ecosystem consists of sophisticated parties, <strong>builders</strong> and <strong>relays</strong>, that are well connected and specialized in having low-latency connections to peers.<br />
Thus, it is expected that proposers using MEV-Boost should be reorged less often than ‘Vanilla Builders’ (i.e., users not using MEV-Boost).</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/5/859fee3890096d24a955abd65642fee08ebd141c.png" title="reorgs_mevb_over_blobs (3)"><img alt="reorgs_mevb_over_blobs (3)" height="258" src="https://ethresear.ch/uploads/default/optimized/3X/8/5/859fee3890096d24a955abd65642fee08ebd141c_2_690x258.png" width="690" /></a></div><p></p>
<p>This expectation holds true when looking at the above chart.<br />
<strong>We can see that the reorg probability increases with the number of blobs. However, the reorg probability for MEV-Boost users is much lower than the one for Non-MEV-Boost users (Vanilla Builders).</strong></p>
<p><strong>In this context it’s important to not confuse correlation and causation:<br />
-&gt; <em>Non-MEV-Boost users are on average less sophisticated entities which also contributes to the effect we observe in the above chart.</em></strong></p>
<p>In this context it is interesting to compare the <strong>average number of blobs per block</strong> of MEV-Boost users vs. Non-MEV-Boost users.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/c/3cbac65d110bbf6d535ba55d7dfb62f69206a271.png" title="blobs_over_time (3)"><img alt="blobs_over_time (3)" height="373" src="https://ethresear.ch/uploads/default/optimized/3X/3/c/3cbac65d110bbf6d535ba55d7dfb62f69206a271_2_690x373.png" width="690" /></a></div><p></p>
<p><strong>As visible in the above chart, proposers not using MEV-Boost included on average more blobs into their blocks than MEV-Boost users.</strong><br />
This might point towards MEV-Boost ecosystem participants (relays and builders) applying strategies that go beyond the “<em>include it if there’s space</em>” strategy.</p>
<p><strong>First, let’s look at the builders more closely.</strong></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/b/4bf0a4fe8bc95e88d122c479fe88cf4f32883fbf.png" title="blobs_over_time_builder (4)"><img alt="blobs_over_time_builder (4)" height="258" src="https://ethresear.ch/uploads/default/optimized/3X/4/b/4bf0a4fe8bc95e88d122c479fe88cf4f32883fbf_2_690x258.png" width="690" /></a></div><p></p>
<p>Vanilla Builders (Non-MEV-Boost proposers) are the ones that have the highest blob inclusion rate, followed by Beaverbuild and Titan Builder.</p>
<p>Rsync-Builder seems to include way less blobs in their blocks.<br />
The same applies to the Flashbots builder that seems to have changed its behavior in early May, with the average number of blobs per block approaching zero.</p>
<p><strong>“Is it fair to say 'Builder XY censors blobs!?”</strong><br />
&gt; <strong>No</strong></p>
<blockquote>
<p><em>Different builders follow different strategies. For example a builder such as Rsync-Builder that is generally competitive in slots where low latency and speed matters might end up with winning those blocks where there are no blobs around (c.f. <em>selection bias</em>)</em></p>
</blockquote>
<br />
<p><strong>Next, let’s shift the focus to the relays:</strong><br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/3/7/374ff432477462e6a307a3d83c7da899f3a5b541.png" title="blobs_over_time_relays (4)"><img alt="blobs_over_time_relays (4)" height="258" src="https://ethresear.ch/uploads/default/optimized/3X/3/7/374ff432477462e6a307a3d83c7da899f3a5b541_2_690x258.png" width="690" /></a></div><p></p>
<p>As visible above, Vanilla Builders have on average the highest blob inclusion rate.<br />
The Ultrasound and Agnostic Gnosis relays are second and third, followed by the relays of BloXroute.<br />
The Flashbots relay seems to include the lowest number of blobs.</p>
<p><strong>Importantly, relays are dependent on builders and ultimately it’s the builders that impact the above graph.</strong></p>
<h2><a class="anchor" href="https://ethresear.ch#next-steps-3" name="next-steps-3"></a>Next Steps</h2>
<p>In the context of <a href="https://ethresear.ch/t/peerdas-a-simpler-das-approach-using-battle-tested-p2p-components/16541">PeerDAS</a>, the network will have to rely on nodes that are <em>stronger</em> than others and able to handle way more than 6 blobs per block. Therefore, it’d be super valuable to see more research on that topic happening.</p>
<ul>
<li><strong>Call for reproduction</strong>: It’d be great if someone could verify my results by reproducing this analysis.</li>
<li><strong>Investigate the reasons</strong> why certain builders have a significantly lower blob inclusion rate than others.</li>
<li><strong>Reduce reorg rate for Non-MEV-Boost users</strong>: Relays could offer Non-MEV-Boost users their block propagation services to ensure that fewer of their blocks get reorged.</li>
</ul>
<p>The blob market is still under development and a stable blob price is yet to be discovered. With increasing demand for blob space, tips from blob transaction will likely catch up to regular transactions.</p>
            <p><small>4 posts - 4 participants</small></p>
            <p><a href="https://ethresear.ch/t/blobs-reorgs-and-the-role-of-mev-boost/19783">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 11 Jun 2024 15:46:12 +0000</pubDate>
</item>
<item>
<title>Block Proposing &amp; Validating Timelines for 1.) MEV-Boost, 2.) ePBS, and 3.) ePBS with MEV-Boost</title>
<link>https://ethresear.ch/t/block-proposing-validating-timelines-for-1-mev-boost-2-epbs-and-3-epbs-with-mev-boost/19782</link>
<guid>https://ethresear.ch/t/block-proposing-validating-timelines-for-1-mev-boost-2-epbs-and-3-epbs-with-mev-boost/19782</guid>
<content:encoded><![CDATA[
<div> 关键词：MEV-Boost、ePBS、block release time、validation time、propagation time

总结:<br />
MEV-Boost与ePBS比较的关键点在于验证时间和块释放时间。MEV-Boost中，由于额外的步骤（如提案者获取头和执行块），导致整体释放时间较长（RT^{mevboost}）。相比之下，ePBS通过共识层和执行层的并行处理，减少了验证时间，特别是对于执行块（VT^{EL} 和 SPT）。验证共识块时，ePBS的条件更为宽松（RT^{epbs,cl} + PT^{epbs,cl} + VT^{CL}），而MEV-Boost可能导致因SPT和VT^{EL}的额外延迟而产生重新排序（reorgs）。因此，使用MEV-Boost进行ePBS会比纯ePBS更慢。 <div>
<p>This writeup summarizes the timeline differences between ePBS and MEV-Boost using inequalities. We analyze three models: 1) MEV-Boost, 2) ePBS, and 3) MEV-Boost with relayers on ePBS. We show that MEV-Boost with relayers on ePBS is slower than ePBS alone, which could lead to reorgs.</p>
<h2><a class="anchor" href="https://ethresear.ch#definitions-1" name="definitions-1"></a>Definitions</h2>
<p><span class="math">VT^{CL}</span>: Consensus layer validation time. The time taken by a node to verify the consensus portion of a block.<br />
<span class="math">VT^{EL}</span>: Execution layer validation time. The time taken by a node to verify the execution portion of a block.<br />
<span class="math">RT^{mevboost}</span>: Mev-boost block release time. The time when a block is released from a node or relayer, assuming the MEV-boost setting.<br />
<span class="math">RT^{epbs,cl}</span>: ePBS consensus block release time. The time when a consensus block is released from a node or relayer, assuming the ePBS setting.<br />
<span class="math">RT^{epbs,el}</span>: ePBS execution block release time. The time when an execution block is released from a node or relayer, assuming the ePBS setting.<br />
<span class="math">PT^{mevboost}</span>: Mev-boost block propagation time. The time taken for a block to propagate across the network, assuming the mev-boost setting.<br />
<span class="math">PT^{epbs,cl}</span>: ePBS consensus block propagation time. The time taken for a consensus block to propagate across the network, assuming ePBS setting.<br />
<span class="math">PT^{epbs,el}</span>: ePBS execution block propagation time. The time taken for an execution block to propagate across the network, assuming ePBS setting.<br />
<span class="math">Attestation\_RT^{beacon}</span>: Beacon attestation release time. The time when a beacon attestation is released from a node.<br />
<span class="math">Attestation\_RT^{ptc}</span>: PTC attestation release time. The time when a payload attestation is released from a node, assuming the ePBS setting.<br />
<span class="math">BBT</span>: Proposer build block time. The time taken for a proposer to build consensus portion of a block.<br />
<span class="math">GHT</span>: Proposer get header time. The time taken for a proposer to obtain a header from a relayer (MEV-boost) or builder (ePBS).<br />
<span class="math">GPT</span>: Proposer get payload time. The time a proposer takes to obtain a payload from a relayer (MEV-boost).<br />
<span class="math">SPT</span>: Builder submit payload time. The time taken for a relayer to receive a payload from the builder (MEV-boost).<br />
<span class="math">SBBT</span>: Proposer submit blind block time. The time a proposer takes to submit blind block to the relayer (MEV-boost).</p>
<h2><a class="anchor" href="https://ethresear.ch#proposing-a-mev-boost-block-2" name="proposing-a-mev-boost-block-2"></a>Proposing a mev-boost block</h2>
<p>In Mev-Boost, proposing a block involves two parts. First, the builder sends the block to the relayer. Second, the proposer requests the header and returns the signed block to the relayer. We break down the time it takes in the following subsections, starting with the non-optimistic relayer and then the optimistic relayer. We also assume that everything starts at the 0-second mark of the slot, including the builder sending the execution block to the relayer.</p>
<h3><a class="anchor" href="https://ethresear.ch#non-optimistic-relayer-3" name="non-optimistic-relayer-3"></a>Non optimistic relayer</h3>
<p><span class="math">BRT</span> defines builder to relayer time. This is how much time takes for a builder to submit a block (ie bid) to the relayer and the relayer verifies the block is valid.<br />
<span class="math">BRT = SPT + VT^{EL}</span></p>
<p><span class="math">PRT</span> defines proposer to relayer time. This is how much time takes for a proposer to build block, request header, request payload, and submit blind block.<br />
<span class="math">PRT = BBT + GHT + GPT + SBBT</span></p>
<p><span class="math">RT^{mevboost} = BRT + PRT</span></p>
<p>This assumes everything happens after the slot start because bids become more valuable. Another model is to assume <span class="math">BRT</span> happens before the slot. Then <span class="math">RT^{mevboost} = PRT</span>.</p>
<h3><a class="anchor" href="https://ethresear.ch#optimistic-relayer-4" name="optimistic-relayer-4"></a>Optimistic relayer</h3>
<h4><a class="anchor" href="https://ethresear.ch#relayer-receives-builder-block-time-5" name="relayer-receives-builder-block-time-5"></a>Relayer receives builder block time</h4>
<p><span class="math">BRT = SPT</span></p>
<p><span class="math">PRT</span> is the same as before</p>
<p><span class="math">RT^{mevboost} = BRT + PRT</span></p>
<blockquote>
<p>Using optimistic relayer is faster than non-optimistic relayer by: <span class="math">VT^{EL}</span></p>
</blockquote>
<h2><a class="anchor" href="https://ethresear.ch#validating-a-mev-boost-block-6" name="validating-a-mev-boost-block-6"></a>Validating a mev-boost block</h2>
<p>In MEV-Boost, the block must be processed before <span class="math">Attestation\_RT^{beacon}</span> to be considered canonical. The following equation shows the conditions that need to be met for the block to be considered canonical from the perspective of all nodes.</p>
<p>For a beacon block to be canonical, it should satisfy:<br />
<span class="math">RT^{mevboost} + PT^{mevboost} + VT^{CL} + VT^{EL} &lt; Attestation\_RT^{beacon}</span></p>
<h2><a class="anchor" href="https://ethresear.ch#proposing-an-epbs-block-7" name="proposing-an-epbs-block-7"></a>Proposing an ePBS block</h2>
<p>In ePBS, proposing the consensus block and the execution block are pipelined, where the consensus block commits to the execution block’s header. Block release time becomes two parts 1.) CL block release time and 2.) EL block release time.</p>
<h3><a class="anchor" href="https://ethresear.ch#proposing-the-consensus-block-8" name="proposing-the-consensus-block-8"></a>Proposing the consensus block</h3>
<p>We assume the proposer uses the builder’s RPC to get the header. The proposer could also self-build or use P2P to obtain the header, which is arguably faster. Therefore, there is no need for proposer get header time anymore.</p>
<p><span class="math">RT^{epbs,cl} = GHT + BBT</span></p>
<blockquote>
<p>Using ePBS is faster than mev-boost by: <span class="math">SPT+VT^{EL}+GPT + SBBT</span></p>
</blockquote>
<h3><a class="anchor" href="https://ethresear.ch#proposing-the-execution-block-9" name="proposing-the-execution-block-9"></a>Proposing the execution block</h3>
<p><span class="math">RT^{epbs,el}</span> is when fork choice accumulates sufficient weight (~40%) or 6 seconds into the slot. The builder could propose a “withhold” block to try to reorg consensus layer block so builder does not have to pay the proposer.</p>
<h2><a class="anchor" href="https://ethresear.ch#validating-an-epbs-block-10" name="validating-an-epbs-block-10"></a>Validating an ePBS block</h2>
<p>In ePBS, validating the consensus block and the execution block are pipelined in different stages. The beacon attestation cutoff time has been moved from 4 seconds into the slot to 3 seconds into the slot. However, we can assume that the CL block propagation time is shorter than the block propagation time. EL block validation can be delayed until the subsequent slot, as shown in the equations.</p>
<h3><a class="anchor" href="https://ethresear.ch#validating-the-consensus-block-11" name="validating-the-consensus-block-11"></a>Validating the consensus block</h3>
<p><span class="math">PT^{epbs,cl} &lt; PT^{mevboost}</span><br />
<span class="math">Attestation\_RT^{beacon,epbs} &lt; Attestation\_RT^{beacon,mevboost}</span></p>
<p>For a consensus block to be canonical, it should satisfy:<br />
<span class="math">RT^{epbs,cl} + PT^{epbs,cl} + VT^{CL} &lt; Attestation\_RT^{beacon}</span></p>
<blockquote>
<p>Using ePBS is faster than mev-boost by: <span class="math">PT^{mevboost}-PT^{epbs,cl}+VT^{EL}</span></p>
</blockquote>
<h3><a class="anchor" href="https://ethresear.ch#validating-the-execution-block-12" name="validating-the-execution-block-12"></a>Validating the execution block</h3>
<h4><a class="anchor" href="https://ethresear.ch#as-a-ptc-voting-for-execution-blocks-presence-13" name="as-a-ptc-voting-for-execution-blocks-presence-13"></a>As a PTC voting for execution block’s presence</h4>
<p><span class="math">RT^{epbs,el} + PT^{epbs,el} &lt; Attestation\_RT^{ptc}</span></p>
<h4><a class="anchor" href="https://ethresear.ch#as-a-proposer-proposing-the-next-slots-consensus-block-14" name="as-a-proposer-proposing-the-next-slots-consensus-block-14"></a>As a proposer proposing the next slot’s consensus block</h4>
<p><span class="math">RT^{epbs,el} + PT^{epbs,el} + VT^{EL} &lt; Next\_Slot\_Start\_Time</span></p>
<h4><a class="anchor" href="https://ethresear.ch#everyone-else-15" name="everyone-else-15"></a>Everyone else</h4>
<p><span class="math">RT^{epbs,el} + PT^{epbs,el} + VT^{EL} &lt; Next\_Slot\_Attestation\_RT^{beacon}</span></p>
<h2><a class="anchor" href="https://ethresear.ch#proposing-an-epbs-block-using-mev-boost-16" name="proposing-an-epbs-block-using-mev-boost-16"></a>Proposing an ePBS block using mev-boost</h2>
<p><span class="math">BRT = SPT + VT^{EL}</span><br />
<span class="math">PRT = BBT + GHT</span><br />
<span class="math">RT^{epbs,cl} = BRT + PRT</span></p>
<blockquote>
<p>Using MEV-Boost for ePBS is slower than ePBS by: <span class="math">SPT + VT^{EL}</span><br />
The additional latency occurs because the trusted party must receive and verify the execution block before releasing it to the proposer.</p>
</blockquote>
<h2><a class="anchor" href="https://ethresear.ch#validating-the-consensus-block-17" name="validating-the-consensus-block-17"></a>Validating the consensus block</h2>
<p><span class="math">RT^{epbs,cl} + PT^{epbs,cl} + VT^{CL} &lt; Attestation\_RT^{beacon}</span></p>
<blockquote>
<p>Given <span class="math">Attestation\_RT^{beacon}</span> is shorter than ePBS, an extra <span class="math">SPT + VT^{EL}</span> could lead to additional reorgs.</p>
</blockquote>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/block-proposing-validating-timelines-for-1-mev-boost-2-epbs-and-3-epbs-with-mev-boost/19782">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 11 Jun 2024 13:52:44 +0000</pubDate>
</item>
<item>
<title>SGX as 2FA for FHE/MPC</title>
<link>https://ethresear.ch/t/sgx-as-2fa-for-fhe-mpc/19780</link>
<guid>https://ethresear.ch/t/sgx-as-2fa-for-fhe-mpc/19780</guid>
<content:encoded><![CDATA[
<div> 关键词：SGX、MPC、FHE、Trusted Execution Environment (TEE)、Collusion Risk

总结:
本文探讨了如何利用SGX（安全多方计算）作为FHE（全同态加密）项目的双重身份验证（2FA），特别是在MPC（多方计算）加密管理中增强安全性。文章指出，FHE项目的关键瓶颈在于MPC节点的密钥管理，而将MPC运行在SGX中可以防止节点间的串通风险。SGX作为2FA的优势包括提高安全性、减少信任依赖、保持低延迟和易于扩展。然而，SGX也有其问题，如声誉不佳、可能的误报安全以及缺乏链上远程证明。尽管如此，随着相关项目的发展，如Phala的进展，SGX在隐私保护技术中的应用前景值得关注。 <div>
<p><em>About me: I am <a href="https://x.com/tolak_eth" rel="noopener nofollow ugc">Wenfeng Wang</a>, a builder and researcher at Phala Network, put this topic here and hope to have a comprehensive discussion with the community.</em></p>
<p><strong>TLDR</strong>: Involving SGX introduces a safeguard against the collusion risk inherent in current MPC and FHE systems.</p>
<p>Continuing from Justin Drake’s well-articulated <a href="https://ethresear.ch/t/2fa-zk-rollups-using-sgx/14462">post</a> about SGX as a 2FA for zk-rollups, I aim to expand on the potential of SGX as 2FA in FHE projects, specifically in their MPC encryption management. Despite their distinct applications, both leverage some fundamental features of SGX.</p>
<h2><a class="anchor" href="https://ethresear.ch#mpc-is-the-bottleneck-of-fhe-1" name="mpc-is-the-bottleneck-of-fhe-1"></a>MPC is the bottleneck of FHE</h2>
<p>Lately, the interest in FHE (Fully Homomorphic Encryption) technologies has rejuvenated, especially in the context of Ethereum Virtual Machines (EVMs). What was once merely a concept is now a tangible tool developers can use to write privacy-preserving smart contracts. Interested readers can refer to Vitalik’s early 2020 <a href="https://vitalik.eth.limo/general/2020/07/20/homomorphic.html" rel="noopener nofollow ugc">post</a> about FHE. Now, let’s look at the general architecture of most current FHE projects.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/f/8f17a45e5c32060cd1578a8f2112437f58880327.png" title="image"><img alt="image" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/8/f/8f17a45e5c32060cd1578a8f2112437f58880327_2_663x500.png" width="663" /></a></div><p></p>
<p>I will not dive too deep into FHE itself here, but you can find a notable challenge most FHE designs encounter today lies in the MPC node’s key management. Due to the practice of writing an FHE application, the key is globally used by all users to encrypt the data they send to the FHE server, which will execute under an encryption state. Thus, the whole security of the system relies on the security of the MPC network, and as we all know the truths of the MPC network are:</p>
<ul>
<li>The more nodes you have, the more latency you get</li>
<li>The fewer nodes you have, the more trust assumptions you need</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#tee-as-a-2fa-to-mpc-2" name="tee-as-a-2fa-to-mpc-2"></a>TEE as a 2FA to MPC</h2>
<p>We don’t want to give full trust to MPC nodes because of the possibility of collusion if it is run by humans. Instead, we can add SGX as 2FA to hedge the risk by moving the key management to <a href="https://en.wikipedia.org/wiki/Trusted_execution_environment" rel="noopener nofollow ugc">TEE</a> (Trusted Execution Environments, a technology to run the program in an isolated zone inside CPU, prove program immutable and limited-accessible).</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/1/d/1dc05649e162e2e9de3318a6da112754d5a6cd7e.png" title="image"><img alt="image" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/1/d/1dc05649e162e2e9de3318a6da112754d5a6cd7e_2_608x500.png" width="608" /></a></div><p></p>
<p>As illustrated above, MPC nodes of the FHE system are now running inside TEE, instead of producing TEE proof when acting as 2FA for zk-rollups, here SGX is used to protect the key generation progress in the MPC network, and the whole lifecycle of the key is kept inside TEE and never gonna reveal to the outside world, more importantly, the key can not be touched by human even a single piece. TEE itself can guarantee the program it runs is verifiable, it’s impossible for someone can manipulate the state. Also, the data passing between TEE and the client is secured by TLS communication.<br />
With TEE as a 2FA, it can help reduce the risk in an economic way that:</p>
<ul>
<li>If SGX is not compromised, there is no chance that collusion can happen;</li>
<li>If SGX gets compromised, only when collusion happens between nodes that the system is broken.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#advantagesdisadvantages-of-sgx-as-2fa-for-fhe-3" name="advantagesdisadvantages-of-sgx-as-2fa-for-fhe-3"></a>Advantages/Disadvantages of SGX as 2FA for FHE</h2>
<ul>
<li>
<p>Advantages</p>
<ul>
<li>Security: Remove the possibility of collusion, trust is built on top of machinehood + cryptography instead of humanity.</li>
<li>Safety: By running MPC inside SGX, even a small MPC network can be reasonably secure. Even if TEE is broken, e.g. have bugs in SGX or Intel being malicious, we still fall back to ordinary MPC.</li>
<li>Latency: Using SGX, we can get higher security without introducing more workers. This gives more confident to users to run latency sensitive operations on MPC.</li>
<li>Liveness: SGX didn’t provide extra liveness naturally, but projects like Phala have built a decentralized <a href="https://docs.phala.network/tech-specs/blockchain" rel="noopener nofollow ugc">TEE network</a> that can help make it easy to build an unstoppable network.</li>
<li>Scalability: Scaling the MPC network is hard, but there are a bunch of existing TEE networks that are ready to deploy MPC nodes. So it lowers the cost to build a larger MPC network.</li>
<li>Throughout: There also is no throughput lost, but considering the optimization of latency, throughput can be improved theoretically.</li>
<li>More advantages that can be brought by SGX were well addressed by <a href="https://ethresear.ch/t/2fa-zk-rollups-using-sgx/14462">Justin’s post</a>.</li>
</ul>
</li>
<li>
<p>Disadvantage</p>
<ul>
<li>It’s worth mentioning that SGX also has its own problems, a quote from Justin’s post:</li>
</ul>
<blockquote>
<ul>
<li>SGX has a bad reputation, especially within the blockchain space. Association with the technology may be memetically suboptimal.</li>
<li>false sense of security: Easily-broken SGX 2FA (e.g. if the privkey is easily extractable) may provide a false sense of security.</li>
<li>novelty: No Ethereum application that verifies SGX remote attestations on-chain could be found.</li>
</ul>
</blockquote>
<ul>
<li>As for the last one that SGX remote attestation on-chain doesn’t exist, the latest state is we have a couple of projects working on it, including Puffer, Automata, and also Phala’s <a href="https://github.com/tolak/zk-dcap-verifier" rel="noopener nofollow ugc">zk-dcap-verifier</a>. But considering it hasn’t been deployed on the mainnet, I kept it on the list.</li>
</ul>
</li>
</ul>
<p><em>Special thanks Justin Drake for his research of <a href="https://ethresear.ch/t/2fa-zk-rollups-using-sgx/14462">2FA zk-rollups using SGX</a> and Andrew Miller for this research of TEE in Multi-Proof system, check his <a href="https://docs.google.com/presentation/d/1K96G50S8ICdllQDbEW1su1Ik_eOc5bK9Ih3uvoG-P9Y/edit?usp=sharing" rel="noopener nofollow ugc">presentation</a>.</em></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/sgx-as-2fa-for-fhe-mpc/19780">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 11 Jun 2024 03:25:10 +0000</pubDate>
</item>
<item>
<title>Solutions to the Preconf Fair Exchange Problem</title>
<link>https://ethresear.ch/t/solutions-to-the-preconf-fair-exchange-problem/19779</link>
<guid>https://ethresear.ch/t/solutions-to-the-preconf-fair-exchange-problem/19779</guid>
<content:encoded><![CDATA[
<div> 关键词：公平交换、领导预确认、声誉系统、最后权利、执行承诺

总结:
本文探讨了在领导预确认（preconfirmation）框架中解决公平交换问题的方法。首先，通过建立声誉系统，激励预确认者诚实回应，但依赖于经济条件的稳定性。其次，提出“最后权利”方案，根据预确认者的顺序决定PER的处理权，最后一个处理PER的预确认者有权获得交易提示。这解决了公平问题，且成本随技术进步而降低。最后，还讨论了“第一权利”方案，即反向请求承诺，适用于对执行承诺不那么敏感的场景。文章也提出了结合使用这些方法的可能性，以适应不同规模的交易需求。

< br>总结: <div>
<h3><a class="anchor" href="https://ethresear.ch#tldr-1" name="tldr-1"></a>tldr</h3>
<p>Solutions for dealing with the fair exchange problem in leader-based preconfirmation setups.</p>
<p>Reputation can incentivize preconfers to act honestly.</p>
<p>Alternatively, use order to dictate who gets the PER tip. One can invalidate a PER by sending it to a preconfer with higher priority.</p>
<h1><a class="anchor" href="https://ethresear.ch#fair-exchange-2" name="fair-exchange-2"></a>Fair Exchange?</h1>
<p>The fair exchange problem can be summarized as two untrusted players blindly giving up something in hopes that the other party will do the same. The goal is to try to find a method to ensure that both will cooperate. In the context of preconfirmations, the requesting party (gateway) has no guarantee that their preconfirmation enforcement request (PER) will receive a signed commitment. The preconfer has every right to not return a commitment, hold onto the PER until the last second, and include it if profitable (pocketing the tip for free).</p>
<h1><a class="anchor" href="https://ethresear.ch#solution-1-reputation-3" name="solution-1-reputation-3"></a>Solution 1: Reputation</h1>
<p>One solution to this is by tracking reputation. More specifically, leveraging the promise of future PERs to incentivize preconfers to respond promptly via either commitments or non-commitments (slash-able promises to NOT include). The gateway can throttle or simply ignore preconfers if they misbehave.</p>
<p>Reputation is a tried method and exists today in mev-boost relays (see <a href="https://ethresear.ch/t/the-preconfirmation-sauna/19762">Switchboard’s Sauna Appendix</a>). While this might work, it still requires certain economic conditions for security. If for whatever reason it becomes really profitable to behave dishonestly, the guarantees fall apart.</p>
<h1><a class="anchor" href="https://ethresear.ch#can-we-do-better-4" name="can-we-do-better-4"></a>Can we do better?</h1>
<p>In an ideal scenario, without any limitations of technology, one would simply invalidate the PER if the preconfer takes too long to respond. With blockchains, this is complicated, and time-based approaches require some sort of additional consensus, breaking the based paradigm. However, we can indirectly access “time” by using order. Blocks are ordered, so preconfers can be as well. If we take advantage of this, we arrive at a new solution that avoids the Fair Exchange problem altogether.</p>
<h1><a class="anchor" href="https://ethresear.ch#solution-2-last-right-5" name="solution-2-last-right-5"></a>Solution 2: Last Right</h1>
<p>Determine an order for preconfers. This can be done per block (or even intra-block). Send the PER optimistically to the first preconfer. If they commit, then great. If they return a non-commitment, or do not respond, then send the PER to the next preconfer.</p>
<p>But wait, they can still include my PER and pocket my tip! Yes, they can but they won’t be able to keep the tip. This is due to the central idea of this solution: <strong>the last preconfer to include the PER has the right to the tips</strong>. If two preconfers attempt to include the PER, the second preconfer has the right to the preconf tip. For example, the last preconfer submits a proof and transfers the PER tip to their balance. Other mechanisms are also possible and should be explored.</p>
<p>One consideration here is the cost. If claiming the tip is more expensive than the tip itself, then the model falls apart. The good news is this cost is directly tied to the technology and should decrease exponentially (e.g. zk proof). Preconfirmation tips on the other hand are tied to the value of the transaction itself, which is not as dependent on the tech. So perhaps this mechanism will become more and more economically favorable.</p>
<p>One great side effect of this method is that it preserves the possibility of execution promises. If the first preconfer acts honestly, then it can guarantee the execution state for the PER. Execution guarantees fall apart if there’s any dishonesty (same as Solution 1).</p>
<h1><a class="anchor" href="https://ethresear.ch#solution-3-first-right-6" name="solution-3-first-right-6"></a>Solution 3: First Right</h1>
<p>If we are willing to forgo execution promises, then the gateway can instead request commitments from preconfers in reverse order. Forward the PER to a preconfer down the list, and then move up until one commits. <strong>The first preconfer to include the PER gets the tip.</strong> In the case where L1 proposers are preconfers, this is enforced by the L1 replay protection. This is a much simpler version of Solution 2.</p>
<p>One downside is the “real” latency before the transaction is actually included since the default preconfer is not the current one. But one could argue that for important transactions where L1 settlement is important (e.g. buying a house), preconfirmations in general are probably not a priority.</p>
<p>Note that execution promises are technically still possible if all the state transitions up to the point of inclusion has already been determined. (e.g. All block space has already been filled by PERs or similar.)</p>
<h1><a class="anchor" href="https://ethresear.ch#final-thoughts-7" name="final-thoughts-7"></a>Final Thoughts</h1>
<p>We can even perhaps use these Solutions in tandem. For smaller preconf tips, we can rely on Solution 1, let the first preconfer pocket it and “slash” their reputation. For larger preconf tips, we can fallback to Solution 2 and let the next preconfer steal it back. Or just use them at the same time.</p>
<p>Thanks to <span class="mention">@mteam</span> for getting me up to speed and providing feedback. We at Spire Labs are actively researching preconfirmations and related topics, and building towards a better, unified Ethereum.</p>
            <p><small>4 posts - 3 participants</small></p>
            <p><a href="https://ethresear.ch/t/solutions-to-the-preconf-fair-exchange-problem/19779">Read full topic</a></p>
]]></content:encoded>
<pubDate>Tue, 11 Jun 2024 02:55:02 +0000</pubDate>
</item>
<item>
<title>Inactivity Leak unveiled</title>
<link>https://ethresear.ch/t/inactivity-leak-unveiled/19774</link>
<guid>https://ethresear.ch/t/inactivity-leak-unveiled/19774</guid>
<content:encoded><![CDATA[
<div> 关键词：inactivity leak, finalization, Ethereum PoS, Byzantine validators, safety

总结:
本文探讨了以太坊PoS区块链中的"不活动泄漏"问题，这是一种在灾难性网络故障期间恢复最终性的理论分析。不活动泄漏机制导致链的连续增长，优先保证区块的最终性（活性），但可能牺牲安全。当网络中存在拜占庭验证者时，这一问题更加突出，它们可能导致冲突的最终区块，威胁协议的安全。文章详细描述了不活动分数和惩罚机制，以及在不同行为（活跃和不活跃）下的验证者权益变化。研究发现，拜占庭验证者的恶意行为会加速安全丧失，特别是在初始比例较高时。作者强调了对协议设计中潜在问题的认识对于BFT分析和未来改进的重要性。 <div>
<p>We summarize here the <a href="https://arxiv.org/abs/2404.16363" rel="noopener nofollow ugc">article</a> that presents the first theoretical analysis of the inactivity leak, designed to restore finalization during catastrophic network failures. This work is accepted at DSN2024.</p>
<h1><a class="anchor" href="https://ethresear.ch#tldr-1" name="tldr-1"></a>TL;DR</h1>
<ul>
<li>The inactivity leak is intrinsically problematic for the safety of the protocol. It favors the constant finalization of blocks (<em>liveness</em>) at the expense of having conflicting finalized blocks (<em>safety</em>).</li>
<li>The presence of Byzantine validators -validators that deviate from the protocol- can accelerate the loss of safety.</li>
</ul>
<hr />
<p>The Ethereum PoS blockchain strives for the continuous growth of the finalized chain. In consequence, the protocol incentivizes validators to finalize blocks actively. The inactivity leak is the mechanism used to regain finality. Specifically, the inactivity leak is initiated if a chain has not undergone finalization for four consecutive epochs. The inactivity leak happened for the first time on the mainnet in May 2023.</p>
<p>A good introduction to the inactivity leak is available thanks to the excellent work of Ben Eddington <a href="https://eth2book.info/capella/part2/incentives/inactivity/" rel="noopener nofollow ugc">here</a> (which motivated this work). We formalize the inactivity leak starting by the inactivity score.</p>
<h2><a class="anchor" href="https://ethresear.ch#inactivity-score-2" name="inactivity-score-2"></a>Inactivity Score</h2>
<p>During an inactivity leak, at epoch <span class="math">t</span>, the inactivity score, <span class="math">I_i(t)</span>, of validator <span class="math">i</span> is:</p>
<div class="math">
\begin{cases}
        I_i(t) = I_i(t-1)+4, \text{if $i$ is inactive at epoch $t$} \\
        I_i(t) = \max(I_i(t-1)-1, 0), \text{ otherwise.}
    \end{cases}
</div>
<p>Thus, a validator’s inactivity score increases by <span class="math">4</span> if it is inactive and decreases by <span class="math">1</span> if it is active. The inactivity score is always positive and will be used to penalize validators during the inactivity leak.</p>
<h2><a class="anchor" href="https://ethresear.ch#inactivity-penalties-3" name="inactivity-penalties-3"></a>Inactivity Penalties</h2>
<p>Let <span class="math">s_i(t)</span> represent the stake of validator <span class="math">i</span> at epoch <span class="math">t</span>, and let <span class="math">I_i(t)</span> denote its inactivity score. The penalty at each epoch <span class="math">t</span> is <span class="math">I_i(t-1)\cdot s_i(t-1)/2^{26}</span>. Therefore, the evolution of the stake is expressed by:</p>
<div class="math">
s_i(t)=s_i(t-1)-\frac{I_i(t-1)\cdot s_i(t-1)}{2^{26}}. 
</div>
<h2><a class="anchor" href="https://ethresear.ch#stake-during-the-inactivity-leak-4" name="stake-during-the-inactivity-leak-4"></a>Stake during the Inactivity Leak</h2>
<p>In this work, we model the stake function <span class="math">s</span> as a continuous and differentiable function, yielding the following differential equation:</p>
<div class="math">
s'(t)=-I(t)\cdot s(t)/2^{26}.
</div>
<p>With this equation, we can determine a validator’s stake according to the time by fixing the evolution of its inactivity score. And that is exactly what we do. We define two types of behavior: Active and Inactive.</p>
<ul>
<li>Active validators: they are always active.</li>
<li>Inactive validators: they are always inactive.</li>
</ul>
<p>Validators with these behaviors experience different evolutions in their inactivity scores: (a) Active validators have a constant inactivity score <span class="math">I(t)=0</span>; (b) Inactive validators’ inactivity score increases by 4 every epoch, <span class="math">I(t)=4t</span>. The stake of each type of validator during an inactivity leak:</p>
<ul>
<li>Active validator’s stake: <span class="math"> s(t) = s_0 = 32. </span></li>
<li>Inactive validator’s stake: <span class="math"> s(t) = s_0e^{-t^2/2^{25}}. </span></li>
</ul>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/4/9/496a7e5de461559b800a4d612eacb356a5f3cc84.png" title="image"><img alt="image" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/4/9/496a7e5de461559b800a4d612eacb356a5f3cc84_2_685x500.png" width="685" /></a></div><p></p>
<p>The graph shows the evolution of the stake of validators depending on their activity during the inactivity leak. The expulsion limit is set by the protocol to eject validators that have accumulated too many penalties.</p>

What is an active validator? <a href="https://ethresear.ch/t/inactivity-leak-unveiled/19774/1">(click for more details)</a>
<hr />
<p>This was the formalization of the protocol. Now we make the analysis of the protocol’s property of safety. To do so, we use the following model.</p>
<h2><a class="anchor" href="https://ethresear.ch#model-5" name="model-5"></a>Model</h2>
<ul>
<li><strong>Network</strong>: We assume a partially synchronous system, which transitions from an asynchronous state to a synchronous state after an apriori unknown Global Stabilization Time (GST).</li>
<li><strong>Fault</strong>: Validators are either <em>honest</em> or <em>Byzantine</em> (deviating from the protocol). A Byzantine validator can deviate arbitrarily from the protocol.</li>
<li><strong>Stake</strong>: Each validator starts with 32 ETH.</li>
</ul>
<p>There is no bound on message transfer delay during the asynchronous state.</p>
<h1><a class="anchor" href="https://ethresear.ch#bound-for-safety-6" name="bound-for-safety-6"></a>Bound for safety</h1>
<h2><a class="anchor" href="https://ethresear.ch#with-only-honest-validators-7" name="with-only-honest-validators-7"></a>With only honest validators</h2>
<p>By construction, the inactivity leak will breach safety if a partition occurs for long enough. The question is, how quickly?</p>
<blockquote>
<p><em>Any network partition lasting longer than 4686 epochs (about 3 weeks) will result in a loss of Safety because of conflicting finalization. This is an upper bound for Safety on the duration of the inactivity leak with only honest validators.</em></p>
</blockquote>
<h3><a class="anchor" href="https://ethresear.ch#detailed-analysis-8" name="detailed-analysis-8"></a>Detailed Analysis</h3>
<p>Let us analyze the scenario in which the validators (which are all honest) are partitioned in two. (We are in the asynchronous state according to our model).<br />
The partition will necessarily create a fork, each partition building on the only chain they see. The chains will finalize once the proportion of active validators returns to 2/3rd.</p>
<p>In this case, by understanding the distribution of the validators across the partitions, we can compute the time it takes for the proportion of active validators’ stake to return to 2/3 of the stake on each branch, thus finalizing and breaking safety.</p>
<p>For the analysis, we make the following notations. At the beginning of the inactivity leak:</p>
<ul>
<li><span class="math">n</span> is the total number of validators</li>
<li><span class="math">n_B</span> is the total number of Byzantine validators</li>
<li><span class="math">n_H</span> is the total number of honest validators</li>
<li><span class="math">n_{H_1}</span> is the number of honest validators on branch 1</li>
<li><span class="math">n_{H_2}</span> is the number of honest validators on branch 2</li>
</ul>
<p>There are no Byzantine validators for the first part of our analysis, which implies that <span class="math">n=n_H</span>. Honest validators are only partitioned in two, thus <span class="math">n_H=n_{H_1}+n_{H_2}</span>.</p>
<p><strong>Our goal is to determine when the proportion of honest validators on branch 1 will be superior to 2/3rd of the total stake.</strong>  Which is to say that we look at when the ratio:</p>
<div class="math">
\frac{\text{stake of validator in branch 1}}{\text{stake of validator in branch 1 + stake of validator in branch 2}},
</div>
<p>is superior to 2/3. With our notation, the ratio can be rewritten as:</p>
<div class="math">
\frac{n_{\text H_1}s_{\text H_1}(t)}{n_{\text H_1}s_{\text H_1}(t)+n_{\text H_2}s_{\text H_2}(t)} ,
</div>
<p><span class="math">s_{\text H_1}</span> and <span class="math">s_{\text H_2}</span> are the stakes of honest active and inactive validators, respectively. Since the <span class="math">n_{\text H_1}</span> validators on branch 1 are always active on branch 1, and the <span class="math">n_{\text H_2}</span> validators are always inactive on branch 1 (they are active on branch 2); we know that <span class="math">s_{\text H_1}(t)=s_0</span> and <span class="math">s_{\text H_2}(t)=s_0e^{-t^2/2^{25}}</span>.<br />
Using the notation <span class="math">p_0=n_{\text H_1}/n_H</span>, the ratio of active validators over time is:</p>
<div class="math">
\frac{p_0}{p_0+(1-p_0)e^{-t^2/2^{25}}}. 
</div>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/e/7ec6a1a64318159dada408e4cc0365a1663b28d1.png" title="image"><img alt="image" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/7/e/7ec6a1a64318159dada408e4cc0365a1663b28d1_2_668x500.png" width="668" /></a></div><p></p>
<p>This graph shows the ratio of active validators on branch 1 over time. If finalization hasn’t occurred by epoch <span class="math">t=4685</span>, inactive validators are ejected, causing a jump to 100% active validators.</p>
<h2><a class="anchor" href="https://ethresear.ch#byzantine-validators-9" name="byzantine-validators-9"></a>Byzantine validators</h2>
<p>We now add Byzantine validators.</p>

These Byzantine validators can send messages to each partition without restriction. <a href="https://ethresear.ch/t/inactivity-leak-unveiled/19774/1">(click for more details)</a>
<p>The situation we analyze is now as such:</p>
<ul>
<li>Less than one-third of the stake is held by Byzantine validators (<span class="math">\beta_0=n_{\rm B}/n&lt;1/3</span>).</li>
<li>Honest validators are divided into branches <span class="math">1</span> and <span class="math">2</span>; a proportion <span class="math">p_0=n_{\rm H_1}/n_{\rm H}</span> on branch <span class="math">1</span> and <span class="math">1-p_0=n_{\rm H_2}/n_{\rm H}</span> on branch <span class="math">2</span>.</li>
<li>Byzantine validators can communicate with both branches.</li>
</ul>
<p>Byzantine validators can be active on both branches simultaneously, breaching safety faster. The ratio of active validators on branch 1 is:</p>
<div class="math">
\frac{p_0(1-\beta_0)+\beta_0}{p_0(1-\beta_0)+\beta_0+(1-p_0)(1-\beta_0)e^{-t^2/2^{25}}}.
</div>
<p>This table shows the time it takes to break safety depending on the initial proportion of Byzantine validators (<span class="math">\beta_0</span>):<br />
<img alt="image" height="195" src="https://ethresear.ch/uploads/default/original/3X/3/0/30cda7537ed8ab1493f4beadd138924b6b6408f3.png" width="157" /></p>
<p><em>Byzantine validators can expedite the loss of Safety. If their initial proportion is 0.33, they can make conflicting finalization occur approximately ten times faster than scenarios involving only honest participants.</em></p>
<hr />
<p>The original paper provides more details on the assumptions, scenarios, protocol, and other aspects such as:</p>
<ul>
<li>Ways for Byzantine validators to breach safety without committing slashable behavior.</li>
<li>Methods for Byzantine validators to exceed the 1/3 threshold on both branches of the fork.</li>
<li>An analysis of the probabilistic bouncing attack while considering the inactivity leak. Spoiler alert: this aggravates the attack slightly, but the conditions for the attack to start and persist in time make it highly improbable to be a real threat.</li>
</ul>
<p>For an additional quick peek at the paper’s findings, here is a graphic that presents how quickly Byzantine validators can break safety depending on their initial proportion and whether their behavior is slashable or not.  As you can see, they can have a strong impact even without slashable behavior.<br />
</p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/a/8a447d3888021e7cf6ba7c2b99fd1907ad3a5738.png" title="image"><img alt="image" height="375" src="https://ethresear.ch/uploads/default/optimized/3X/8/a/8a447d3888021e7cf6ba7c2b99fd1907ad3a5738_2_500x375.png" width="500" /></a></div><p></p>
<h1><a class="anchor" href="https://ethresear.ch#conclusion-10" name="conclusion-10"></a>Conclusion</h1>
<p>Our findings highlight the importance of penalty mechanisms in Byzantine Fault Tolerance (BFT) analysis. By identifying potential issues in protocol design, we aim to provide insights for future improvements and tools for further investigation.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://ethresear.ch/t/inactivity-leak-unveiled/19774">Read full topic</a></p>
]]></content:encoded>
<pubDate>Mon, 10 Jun 2024 13:46:18 +0000</pubDate>
</item>
<item>
<title>The contention between preconfs and ePBS</title>
<link>https://ethresear.ch/t/the-contention-between-preconfs-and-epbs/19770</link>
<guid>https://ethresear.ch/t/the-contention-between-preconfs-and-epbs/19770</guid>
<content:encoded><![CDATA[
<div> 关键词：ePBS、preconfirmations、inclusion lists、centralized entity、staked builders

总结:<br />
文章讨论了ePBS（加密预言机拜占庭服务）与不同预确认机制的兼容性。主要观点如下：<br />
1. 使用预确认列表可能会导致交易广播两次，增加网络负担，这与ePBS优化的共识块验证时间不兼容。
2. 预确认系统的中心化性质，通常依赖于集中式实体，如中继，与ePBS的去中心化目标相悖。
3. ePBS下，强制执行预确认可能导致全交易列表广播，挑战了ePBS的优化，即仅验证共识块。
4. 建议利用已有的staked builders作为预确认者，他们在预确认系统中的角色可以像提案者一样受到规则约束。
5. ePBS对restaking（复投）也构成挑战，因为预确认和潜在违规行为可能发生在同一个交易中，需要额外处理。

因此，为保持与ePBS的兼容性，预确认系统的设计需要避免直接在共识块中包含完整交易列表，而是考虑利用staked builders的角色来间接实现预确认。 <div>
<p>This quick note is motivated by a question of <a class="mention" href="https://ethresear.ch/u/hasu.research">@Hasu.research</a> regarding the compatibility of ePBS with the different mechanisms for preconfirmations that are being proposed by independent groups <a href="https://ethresear.ch/t/the-preconfirmation-sauna/19762">1</a> <a href="https://ethresear.ch/t/blob-preconfirmations-with-inclusion-lists-to-mitigate-blob-contention-and-censorship/19150">2</a> <a href="https://chainbound.github.io/bolt-docs/" rel="noopener nofollow ugc">3</a> <a href="https://docs.google.com/presentation/d/1a-0rP2knM11g59UmnKn7I7NH8BlFM5wNhczH35sbkSo/edit#slide=id.g2731bc99d1b_0_0" rel="noopener nofollow ugc">4</a> <a href="https://docs.primev.xyz/get-started/introduction" rel="noopener nofollow ugc">5</a>. The only purpose of this note is to leave a quick written record of the fundamental contention between the enshrinements of preconfirmations and the <a href="https://github.com/potuz/consensus-specs/pull/2" rel="noopener nofollow ugc">current proposal for ePBX</a>.</p>
<h2><a class="anchor" href="https://ethresear.ch#overloading-inclusion-lists-1" name="overloading-inclusion-lists-1"></a>Overloading inclusion lists.</h2>
<p>Even in the very first post on <a href="https://ethresear.ch/t/based-preconfirmations/17353">based preconfirmations</a>, the idea of using <a href="https://eips.ethereum.org/EIPS/eip-7547" rel="noopener nofollow ugc">forced inclusion lists</a> was put forward as a way for proposers to signal their intent of honoring preconfirmations, forcing builders to include these transactions. An extrapolation of this idea led, in one of the original designs for ILs, to propose that inclusion lists may essentially include a complete list of transactions the proposer has in its current mempool. One of the problems with these ideas is that the full list of transactions would need to be broadcast over the P2P network twice: once when the inclusion list is broadcast, and the second time within the payload itself. In all known designs for inclusion lists, validators attest for the existence of the full executable transaction list. This implies in particular that</p>
<ol>
<li>The list must be available at the beacon block validation time.</li>
<li>The list must be executed at the beacon block validation time.</li>
</ol>
<p>This section is not meant to be read as <em>inclusion lists aren’t compatible with ePBS</em> but rather any preconfirmation system (and next block forced inclusion lists by definition are such a system) that relies on the execution and distribution of the transactions at the consensus block validation time, necessarily clashes with the main optimization from ePBS.</p>
<h2><a class="anchor" href="https://ethresear.ch#epbs-validation-optimization-2" name="epbs-validation-optimization-2"></a>ePBS validation optimization</h2>
<p>The above two points are in direct opposition with the main optimization that ePBS brings to block processing, that is that the only hot path to validation is that of the consensus block that has to be fully verified before the attestation deadline. All other validations, like transaction execution, data availability, etc. are deferred to the remainder of the slot and into the next slot.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/f/8f3daa474ae131dfbee7c96cfd9f2a7e4035c06a.jpeg" title="ePBS slot"><img alt="ePBS slot" height="364" src="https://ethresear.ch/uploads/default/optimized/3X/8/f/8f3daa474ae131dfbee7c96cfd9f2a7e4035c06a_2_690x364.jpeg" width="690" /></a></div><p></p>
<p>While ePBS is compatible with inclusion lists, their addition inherently stresses this optimization. Broadcasting a small list of 16 transactions that can be immediately executed in microseconds is not the same as broadcasting a full block, and presumably, even blob transactions as some based rollups would require.</p>
<h2><a class="anchor" href="https://ethresear.ch#the-centralized-nature-of-preconfs-3" name="the-centralized-nature-of-preconfs-3"></a>The centralized nature of preconfs</h2>
<p>There is no current design (that I am aware of) of preconfirmations, that does not rely on a centralized entity. This is natural to expect in the absence of an encrypted public mempool, users can’t send their transactions in the open to the next proposer (although they could <em>encrypt the transactions to the public BLS address of the next proposer</em>), and we can’t enshrine an RPC provider, all systems thus make use on existing centralized entities (for example relays) to act as a preconfer. Decentralization comes in that it is ultimately the proposer who enforces these preconfirmations, by forcing the builder to fullfil them.</p>
<p>Thus, in all proposed systems for preconfirmations, either of L1 transactions or for based rollups, there exist a centralized entity that at the very least is responsible for gathering the transactions and giving out the preconfirmations. Systems differ on how is that these preconfirmations are enforced, they range from new L1 slashing proposals, to restaking proposals (moving the slashing to a separate layer), etc. The point is that preconfirmations can be enforced by the protocol itself, or by a somewhat decentralized party like the subset of validators participating in the preconfirmation scheme. In summary, there is a plethora of options for enforcing the (or penalizing the lack of) inclusion of preconfirmations, in decreasing level of trustlessness:</p>
<ul>
<li>The L1 protocol itself enforces inclusion. For example, forced ILs, with proposer level slashings on missed slots, preconf equivocations, etc.</li>
<li>Some separate committee enforces them. For example a subset of the L1 validators also participate in a sidechain by restaking, and the enforcement/punishment is carried in that sidechain.</li>
<li>A centralized entity enforces them. For example the relay itself only sends bids from builders that have satisfied the required preconfs.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#a-viable-way-compatible-with-epbs-staked-builders-as-preconfirmers-4" name="a-viable-way-compatible-with-epbs-staked-builders-as-preconfirmers-4"></a>A viable way compatible with ePBS: staked builders as preconfirmers.</h2>
<p>Any approach with a full payload being broadcast with the consensus block for preconfirmation enforcement clashes directly with the main scaling optimization of ePBS with regard to block validation. As thus, it seems difficult to expect a working design in which the proposers are in charge of sending and enforcing preconfirmations. The second and third approaches above are fully compatible with ePBS.</p>
<p>One of the features that preconfirmation systems can leverage when ePBS is in place, is that builders themselves are staked validators, thus they can be subject to the same rules that these systems currently require from proposers. For example, those systems that rely on slashings on a restaking scheme could simply add conditions on participating builders. That is, the proposer set participating in the scheme only take bids from builders that are participants of the scheme. The builders and proposers are required to be restaked. There are new penalty conditions for</p>
<ul>
<li>A proposer that does not include a block.</li>
<li>A proposer that includes a block with a commitment to a non-participating builder.</li>
<li>A builder that does not include the payload</li>
<li>A builder that includes a payload does not satisfies the preconf list.</li>
</ul>
<h2><a class="anchor" href="https://ethresear.ch#a-separate-note-on-restaking-5" name="a-separate-note-on-restaking-5"></a>A separate note on restaking</h2>
<p>ePBS also presents a challenge on any restaking scheme: builders can transfer funds in the same payload that they commit a slashable offense. L1 protocol can deal with this by immediately deducting the bid from the builder’s balance at the time of CL block processing, but delaying the credit to the proposer. In case the builder commits a slashable offense, the buffer allows the L1 protocol to implement penalization procedures that can impact those delayed funds accordingly. If the builder is restaked however, the restaking chain does not have access to these funds.</p>
            <p><small>7 posts - 4 participants</small></p>
            <p><a href="https://ethresear.ch/t/the-contention-between-preconfs-and-epbs/19770">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sun, 09 Jun 2024 07:59:33 +0000</pubDate>
</item>
<item>
<title>On block-space distribution mechanisms</title>
<link>https://ethresear.ch/t/on-block-space-distribution-mechanisms/19764</link>
<guid>https://ethresear.ch/t/on-block-space-distribution-mechanisms/19764</guid>
<content:encoded><![CDATA[
<h1><a class="anchor" href="https://ethresear.ch#on-block-space-distribution-mechanisms-1" name="on-block-space-distribution-mechanisms-1"></a>On block-space distribution mechanisms</h1>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/c/3/c3fa4239ef0f468256ba44d0e860fb3d7edaedcf.jpeg" title="upload_3067440b5b4f752379ddba32df7ecf8b"><img alt="upload_3067440b5b4f752379ddba32df7ecf8b" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/c/3/c3fa4239ef0f468256ba44d0e860fb3d7edaedcf_2_499x500.jpeg" width="499" /></a></div><br />
<sub><strong>^p.s. yes, we anthropomorphize the protocol as a ghost because <a href="https://arxiv.org/pdf/1710.09437.pdf" rel="noopener nofollow ugc">Casper</a>.</strong></sub><br />
<sub><strong>^^p.p.s. not sure why the auctioneer ghost looks like he is conducting an orchestra, but here we are ¯\_(ツ)_/¯.</strong></sub><br />
<sub><strong>^^^ p.p.p.s. by the way, if you haven’t seen <a href="https://en.wikipedia.org/wiki/Maestro_(2023_film)" rel="noopener nofollow ugc">Maestro</a>, it’s great.</strong></sub><p></p>
<p><span class="math">\cdot</span><br />
<em>by <a href="https://x.com/mikeneuder" rel="noopener nofollow ugc">Mike</a>, <a href="https://x.com/PGarimidi" rel="noopener nofollow ugc">Pranav</a>, &amp; <a href="https://x.com/Tim_Roughgarden" rel="noopener nofollow ugc">Dr. Tim Roughgarden</a> – June 8, 2024.</em><br />
<span class="math">\cdot</span><br />
<strong>Acknowledgements</strong><br />
<em>Special thanks to <a href="https://x.com/barnabemonnot" rel="noopener nofollow ugc">Barnabé</a>, <a href="https://x.com/_julianma" rel="noopener nofollow ugc">Julian</a>, <a href="https://x.com/_JonahB_" rel="noopener nofollow ugc">Jonah</a>, <a href="https://x.com/DavideCrapis" rel="noopener nofollow ugc">Davide</a>, <a href="https://x.com/soispoke" rel="noopener nofollow ugc">Thomas</a>, <a href="https://x.com/terencechain" rel="noopener nofollow ugc">Terence</a>, <a href="https://x.com/potuz_eth" rel="noopener nofollow ugc">Potuz</a>, &amp; <a href="https://www.nano210.blog/" rel="noopener nofollow ugc">Nate</a> for comments and discussions.</em><br />
<span class="math">\cdot</span><br />
<strong>tl;dr;</strong> <em>Block space, the capacity for transaction inclusion, is the principal resource exported by blockchains. As the crypto ecosystem scales up and professionalizes, the value produced by efficient usage of block space (<a href="https://arxiv.org/abs/1904.05234" rel="noopener nofollow ugc">MEV</a>) has come to play a significant role in the economics of permissionless consensus mechanisms. An immense amount of ink has been spilled by the research community considering what, if anything, protocols should enshrine in response to MEV (see <a href="https://ethresear.ch#related-work-2">Related Work</a>). Indeed, the past few years resemble a <a href="https://en.wikipedia.org/wiki/Blind_men_and_an_elephant" rel="noopener nofollow ugc">Blind Men and the Elephant</a> narrative arc, where many different perspectives, solutions, and theories have been propounded, but each angle can feel disjoint and difficult to compare. The first half of this article aims to present a broad-strokes painting of the “MEV-ephant” by distilling the design space into a core set of questions and exploring how existing proposals answer them. The second half hones in specifically on allocation mechanisms enabled by execution tickets, demonstrating an important new insight – there is a trade-off between the quality of the in-protocol MEV oracle and the fairness of the mechanism.</em></p>
<p><strong>Organization:</strong> <a href="https://ethresear.ch#h-1-motivation-3">Section 1</a> motivates the need for an in-protocol mechanism to handle block-space distribution as part of the “endgame” for Proof-of-Stake. <a href="https://ethresear.ch#h-2-enumeration-6">Section 2</a> enumerates five axes along which block-space distribution mechanisms may be measured, using a familiar set of questions: <em>who, what, when, where, how</em> (abbr. the <code>W^4H questions</code>). <a href="https://ethresear.ch#h-3-interrogation-11">Section 3</a> interrogates how the block builder is selected, focusing on the execution tickets model. <a href="https://ethresear.ch#h-4-extrapolation-18">Section 4</a> extrapolates by concluding and raising open questions that follow from the framework established.</p>
<p><strong>Structural note:</strong> This article is rather long for this format and has some technical elements. We encourage the reader to focus on the portion of the article they are most interested in:</p>
<ul>
<li>Sections <a href="https://ethresear.ch#h-1-motivation-3">1</a>, <a href="https://ethresear.ch#h-2-enumeration-6">2</a>, &amp; <a href="https://ethresear.ch#h-4-extrapolation-18">4</a> provide a broader perspective on the existing proposals and our proposed methodology for analyzing them.</li>
<li><a href="https://ethresear.ch#h-3-interrogation-11">Section 3</a> (which is <span class="math">\approx 44\%</span> of the content, but <a href="https://youtu.be/VDvr08sCPOc?t=111" rel="noopener nofollow ugc"><span class="math">100\%</span></a> of the math) provides a detailed analysis of allocation mechanisms enabled by the execution tickets design. This section can be read in sequence, in isolation, or skipped altogether – up to you!</li>
</ul>
<p><span class="math">\cdot</span><br />
<strong>Contents</strong></p>
<ol>
<li><a href="https://ethresear.ch#h-1-motivation-3"><strong>Motivation</strong></a><br />
<a href="https://ethresear.ch#h-1-what-4"><em>1) What</em></a><br />
<a href="https://ethresear.ch#Block-space-distribution-today"><em>Block-space distribution today through <code>mev-boost</code></em></a></li>
<li><a href="https://ethresear.ch#h-2-enumeration-6"><strong>Enumeration</strong></a><br />
<a href="https://ethresear.ch#the-elementshttpsenwikipediaorgwikieuclid27s_elements-of-block-space-distribution-7"><em>The elements of block-space distribution</em></a><br />
<a href="https://ethresear.ch#execution-tickets-and-other-animals-8"><em>Execution tickets and other animals</em></a><br />
<a href="https://ethresear.ch#applying-w4h-a-comparative-analysis-9"><em>Applying W^4H: a comparative analysis</em></a><br />
<a href="https://ethresear.ch#motivational-interlude-10"><em>Motivational interlude</em></a></li>
<li><a href="https://ethresear.ch#h-3-interrogation-11"><strong>Interrogation</strong></a><br />
<a href="https://ethresear.ch#preliminaries-12"><em>Preliminaries</em></a><br />
<a href="https://ethresear.ch#model-13"><em>Model</em></a><br />
<a href="https://ethresear.ch#familiar-allocation-mechanisms-14"><em>Familiar allocation mechanisms</em></a><br />
<a href="https://ethresear.ch#comparing-the-outcomes-15"><em>Comparing the outcomes</em></a><br />
<a href="https://ethresear.ch#aside-1-calculating-equilibrium-bids-16"><em>Aside #1: Calculating equilibrium bids</em></a><br />
<a href="https://ethresear.ch#aside-2-tullock-contests-17"><em>Aside #2: Tullock Contests</em></a></li>
<li><a href="https://ethresear.ch#h-4-extrapolation-18"><strong>Extrapolation</strong></a></li>
</ol>
<p><span class="math">\cdot</span></p>
<h4><a class="anchor" href="https://ethresear.ch#related-work-2" name="related-work-2"></a><strong>Related work</strong></h4>
<ol>
<li><em>mev-boost &amp; relays</em>
<ul>
<li><a href="https://ethresear.ch/t/mev-boost-merge-ready-flashbots-architecture/11177"><em>MEV-Boost: Merge ready Flashbots Architecture</em></a>; Flashbots team</li>
<li><a href="https://ethresear.ch/t/relays-in-a-post-epbs-world/16278"><em>Relays in a post-ePBS world</em></a>; Mike, Jon, Hasu, Tomasz, Chris, Toni</li>
</ul>
</li>
<li><em>mev-burn / mev-smoothing</em>
<ul>
<li><a href="https://ethresear.ch/t/burning-mev-through-block-proposer-auctions/14029"><em>Burning MEV through block proposer auctions</em></a>; Domothy</li>
<li><a href="https://ethresear.ch/t/mev-burn-a-simple-design/15590"><em>MEV burn – a simple design</em></a>; Justin</li>
<li><a href="https://ethresear.ch/t/committee-driven-mev-smoothing/10408"><em>Committee-driven MEV smoothing</em></a>; Francesco</li>
<li><a href="https://ethresear.ch/t/dr-changestuff-or-how-i-learned-to-stop-worrying-and-love-mev-burn/17384"><em>Dr. changestuff or: how I learned to stop worrying and love mev-burn</em></a>; Mike, Toni, Justin</li>
</ul>
</li>
<li><em>enshrined Proposer-Builder Separation (ePBS)</em>
<ul>
<li><a href="https://ethresear.ch/t/two-slot-proposer-builder-separation/10980"><em>Two-slot proposer/builder separation</em></a>; Vitalik</li>
<li><a href="https://ethresear.ch/t/unbundling-pbs-towards-protocol-enforced-proposer-commitments-pepc/13879"><em>Unbundling PBS: towards protocol-enforced proposer commitments (PEPC)</em></a>; Barnabé</li>
<li><a href="https://barnabe.substack.com/p/pbs" rel="noopener nofollow ugc"><em>Notes on Proposer-Builder Separation</em></a>; Barnabé</li>
<li><a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ" rel="noopener nofollow ugc"><em>More pictures about proposers and builders</em></a>; Barnabé</li>
<li><a href="https://ethresear.ch/t/why-enshrine-proposer-builder-separation-a-viable-path-to-epbs/15710"><em>Why enshrine Proposer-Builder Separation?</em></a>; Mike, Justin</li>
<li><a href="https://ethresear.ch/t/epbs-design-constraints/18728"><em>ePBS design constraints</em></a>; Potuz</li>
<li><a href="https://mirror.xyz/barnabe.eth/LJUb_TpANS0VWi3TOwGx_fgomBvqPaQ39anVj3mnCOg" rel="noopener nofollow ugc"><em>Reconsidering the market structure of PBS</em></a>; Barnabé</li>
</ul>
</li>
<li><em>block-space futures</em>
<ul>
<li><a href="https://mirror.xyz/0x03c29504CEcCa30B93FF5774183a1358D41fbeB1/CPYI91s98cp9zKFkanKs_qotYzw09kWvouaAa9GXBrQ" rel="noopener nofollow ugc"><em>Block vs. Slot Auction PBS</em></a>; Julian</li>
<li><a href="https://frontier.tech/ethereums-blockspace-future" rel="noopener nofollow ugc"><em>Opportunities and Considerations of Ethereum’s Blockspace Future</em></a>; Drew, Ankit</li>
<li><a href="https://collective.flashbots.net/t/when-to-sell-your-blocks/2814" rel="noopener nofollow ugc"><em>When to sell your blocks</em></a>; Quintus, Conor</li>
</ul>
</li>
<li><em>execution tickets</em>
<ul>
<li><a href="https://www.youtube.com/watch?v=MtvbGuBbNqI" rel="noopener nofollow ugc"><em>Attester-proposer separation</em></a>; Justin</li>
<li><a href="https://ethresear.ch/t/execution-tickets/17944"><em>Execution tickets</em></a>; Justin, Mike</li>
<li><a href="https://ethresear.ch/t/economic-analysis-of-execution-tickets/18894"><em>Economic Analysis of Execution Tickets</em></a>; Jonah, Davide</li>
<li><a href="https://ethresear.ch/t/block-auction-epbs-versus-execution-ticket/19232"><em>Block-auction ePBS versus Execution Ticket</em></a>; Terence</li>
</ul>
</li>
</ol>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#h-1-motivation-3" name="h-1-motivation-3"></a>(1) – Motivation</h3>
<p>Before descending into this murky rabbit hole, let’s start by simply motivating the necessity of a block-space distribution mechanism. Validators in Proof-of-Stake protocols are tasked with producing and voting on blocks. The figure below, from Barnabé’s excellent “<a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ" rel="noopener nofollow ugc"><em>More pictures about proposers and builders</em></a>,” describes these as “proposing” and “attesting” rights, respectively.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/7/d/7d85ca7f1812a5822490fa079365301c99733620.png" title="upload_72dad4dc4f8c77f0d57f8f126b3c2e46"><img alt="upload_72dad4dc4f8c77f0d57f8f126b3c2e46" height="219" src="https://ethresear.ch/uploads/default/optimized/3X/7/d/7d85ca7f1812a5822490fa079365301c99733620_2_690x219.png" width="690" /></a></div><p></p>
<h4><a class="anchor" href="https://ethresear.ch#h-1-what-4" name="h-1-what-4"></a>1) What</h4>
<p>(<span class="math">\uparrow</span> <a href="https://twitter.com/SBF_FTX/status/1591989554881658880?lang=en" rel="noopener nofollow ugc">important cultural ref</a>.)</p>
<p>A block-space distribution mechanism is the process by which the protocol determines the owner of the “proposing” or “block construction” rights. Proof-of-Stake protocols typically use some version of the following rules:</p>
<ul>
<li><strong>block-space (proposing) rights</strong> – A random validator is elected as the leader and permitted to create the next block.</li>
<li><strong>voting (attesting) rights</strong> – All validators vote during some time window for the block they see as the canonical head.</li>
</ul>
<p>Validators perform these tasks because they receive rewards for doing so. We categorize the rewards according to their origin in either the consensus layer (the issuance from the protocol – e.g., newly minted <code>ETH</code>) or the execution layer (transaction fees and MEV):</p>
<ol>
<li><strong>Consensus layer</strong><br />
a. <em>Attestation rewards</em> – see <a href="https://github.com/ethereum/annotated-spec/blob/160764ac180eca2cea3581f731ee96ac7098f9f7/phase0/beacon-chain.md#components-of-attestation-deltas" rel="noopener nofollow ugc">attestation deltas</a>.<br />
b. <em>Block rewards</em> – see <a href="https://github.com/ethereum/annotated-spec/blob/160764ac180eca2cea3581f731ee96ac7098f9f7/phase0/beacon-chain.md#rewards-and-penalties-1" rel="noopener nofollow ugc"><code>get_proposer_reward</code></a>.</li>
<li><strong>Execution layer</strong><br />
a. <em>Transaction fees</em> – see <a href="https://etherscan.io/gastracker" rel="noopener nofollow ugc">gas tracker</a>.<br />
b. <em>MEV (transaction ordering)</em> – see <a href="https://mevboost.pics/" rel="noopener nofollow ugc">mevboost.pics</a>.</li>
</ol>
<p>Rewards <code>1a</code>, <code>1b</code>, &amp; <code>2a</code> are well understood and “<a href="https://barnabe.substack.com/p/seeing-like-a-protocol" rel="noopener nofollow ugc">in the view</a>” of the protocol. MEV rewards present a more serious challenge because fully capturing the value realized by transaction ordering is difficult. Unlike the other rewards, even the amount of MEV in a block is unknowable for all intents and purposes (as a permissionless and pseudonymous system, it’s impossible to trace who controls each account and any corresponding offchain activity that may be profitable in tandem). MEV also changes dramatically over time (e.g., as a function of price volatility), resulting in execution layer rewards having a much higher variance than the consensus layer rewards. Further, the Ethereum protocol, as implemented, has no insight into the MEV being produced and extracted by its transactions. To improve protocol visibility into MEV, many mechanisms try to approximate the MEV in a given block; we refer to these as <em>MEV oracles</em>. Block-space distribution mechanisms generally have the potential to produce such an oracle, making the protocol “MEV-aware.”</p>
<p>This suggests the question, <em>why does the protocol care about being MEV-aware?</em> One answer: <strong>MEV awareness may increase the protocol’s ability to preserve the homogeneity of validator rewards, even if validators have varying degrees of sophistication.</strong> For example, if the protocol could accurately burn all MEV, then the validator incentives would be fully in the protocol’s view (just like <code>1a</code>, <code>1b</code>, &amp; <code>2a</code> above). Alternatively, a mechanism that shares all MEV among validators regardless of their sophistication (e.g., <a href="https://ethresear.ch/t/committee-driven-mev-smoothing/10408">mev-smoothing</a>) would seem to promote a larger, more diverse and decentralized validator set, while keeping the MEV rewards as an extra incentivization to stake. Without MEV awareness, the validators best equipped to extract or smooth MEV (e.g., due to relationships with block builders, proprietary algorithms/software, access to exclusive order flow, &amp; economies of scale) may earn disproportionately high rewards and exert significant centralization pressures on the protocol.</p>
<p>Ethereum protocol design strives to keep a decentralized validator set at all costs. It probably goes without saying, but for completeness: <strong>the protocol’s credible neutrality, censorship resistance, and permissionlessness are directly downstream of a decentralized validator set.</strong></p>
<h4><a class="anchor" href="https://ethresear.ch#block-space-distribution-today-5" name="block-space-distribution-today-5"></a>Block-space distribution today</h4>
<p>In Ethereum today, <a href="https://mevboost.pics/" rel="noopener nofollow ugc"><code>mev-boost</code></a> accounts for <span class="math">\approx 90\%</span> of all blocks. Using <code>mev-boost</code>, proposers (the validator randomly selected as the leader) sell their block-building rights to the highest paying bidder through an auction. The figure below demonstrates this flow (we exclude the <a href="https://www.relayscan.io/" rel="noopener nofollow ugc">relays</a> because they functionally serve as an extension of the builders).</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/b/7/b70bdfef2fd8dc26a478c2b870495ea39ebd07bc.png" title="upload_5f698b1a28978bd8f9779e596c471d9a"><img alt="upload_5f698b1a28978bd8f9779e596c471d9a" height="499" src="https://ethresear.ch/uploads/default/optimized/3X/b/7/b70bdfef2fd8dc26a478c2b870495ea39ebd07bc_2_252x499.png" width="252" /></a></div><br />
.<br />
Proposers are incentivized to outsource their block building because builders (the canonical name for MEV-extracting agents specializing in sequencing transactions) pay them more than they would have earned had they built the block themselves. Circling back to our goal of “<strong>preserving the homogeneity of validator rewards in the presence of MEV</strong>,” we see that <code>mev-boost</code> allows access to the builder market for <em>all validators</em>, effectively preserving near-equivalent MEV rewards among solo stakers and professional staking service providers – great! But…<p></p>
<p>Of course, there is a but… <code>mev-boost</code> has issues that continue to rankle some of the Ethereum community. Without being exhaustive, a few of the negative side-effects of taking the <code>mev-boost</code> medicine are:</p>
<ul>
<li><strong>Relays</strong> – These <a href="https://www.relayscan.io/" rel="noopener nofollow ugc">trusted-third parties</a> broker the sale of blocks between proposers and builders. The immense reliance on relays increases the fragility of the protocol as a whole, as demonstrated through <a href="https://collective.flashbots.net/t/disclosure-mitigation-of-block-equivocation-strategy-with-early-getpayload-calls-for-proposers/1705" rel="noopener nofollow ugc">repeated</a>, <a href="https://research.lido.fi/t/bloxroute-feb-6th-post-mortem/6586" rel="noopener nofollow ugc">incidents</a>, <a href="https://gist.github.com/benhenryhunter/5c397db3985a59d14a52816305a6c1b8" rel="noopener nofollow ugc">involving</a>, <a href="https://gist.github.com/benhenryhunter/7b7d9c9e3218aad52f75e3647b83a6cc" rel="noopener nofollow ugc">relays</a>. Further, since relays have no inherent revenue stream, more exotic (and closed-source) methods of capturing margins (e.g., <a href="https://bloxroute.com/pulse/introducing-the-validator-gateway-boost-your-ethereum-validator-rewards/" rel="noopener nofollow ugc">timing games as a service</a> and <a href="https://twitter.com/sui414/status/1778708084510302445" rel="noopener nofollow ugc">bid adjustments</a>) are being implemented.</li>
<li><strong>Out-of-protocol software is brittle</strong> – Beyond the relays, participation in the <code>mev-boost</code> market requires validators to run additional software. The standard suite for solo staking now involves running four binaries: (i) the consensus beacon node, (ii) the consensus validator client, (iii) the execution client, and (iv) mev-boost. Beyond the significant overhead for solo stakers, reliance on this software also provides another potential point of failure during hard forks. See the <a href="https://collective.flashbots.net/t/impact-of-the-prysm-invalid-signature-bug-on-the-mev-boost-ecosystem-at-the-shapella-fork/1623" rel="noopener nofollow ugc">Shapella incident</a> and the <a href="https://writings.flashbots.net/preparing-for-dencun" rel="noopener nofollow ugc">Dencun upgrade</a> for an example of the complexity induced by having more out-of-protocol software.</li>
<li><strong>Builder centralization and censorship</strong> – While this is likely <a href="https://vitalik.eth.limo/general/2021/12/06/endgame.html" rel="noopener nofollow ugc">inevitable</a>, builder centralization was accelerated by the mass adoption of <code>mev-boost</code>. <a href="https://www.relayscan.io/" rel="noopener nofollow ugc">Three builders</a> account for <span class="math">\approx 95\%</span> of <code>mev-boost</code> blocks (<span class="math">85\%</span> of all Ethereum blocks). <code>mev-boost</code> implements an open-outcry, first-price, winner-takes-all auction, leading to high levels of builder concentration and <a href="https://ethresear.ch/t/game-theoretic-model-for-mev-boost-auctions-mma/16206">strategic</a>, <a href="https://ethresear.ch/t/bid-cancellations-considered-harmful/15500">bidding</a>. Without <a href="https://ethresear.ch/t/no-free-lunch-a-new-inclusion-list-design/16389">inclusion lists</a> or another censorship-resistance gadget, builders have extreme influence over transaction inclusion and exclusion – see <a href="https://censorship.pics/" rel="noopener nofollow ugc">censorship.pics</a>.</li>
<li><strong>Timing games</strong> – While <a href="https://arxiv.org/abs/2305.09032" rel="noopener nofollow ugc">timing games</a> are known to be a fundamental issue in Proof-of-Stake protocols, <code>mev-boost</code> pushes staking service providers to compete on thin margins. Additionally, relays (who conduct <code>mev-boost</code> auctions on the proposer’s behalf) serve as sophisticated middlemen facilitating timing games. Thus, we have seen <a href="https://p2p.org/economy/unlock-p2p-orgs-mev-enhancement-feature/" rel="noopener nofollow ugc">marketing</a> endorsing playing timing games to boost the yield from staking with a specific provider.</li>
</ul>
<p><em>“OK, OK … blah blah … we have heard this story before … <a href="https://youtu.be/q8wJqMbr6eY?si=LVryerWbrw3_ge-I" rel="noopener nofollow ugc">tell me something I don’t know</a>.” (<span class="math">\leftarrow</span> h/t Barnabé for the aptly-named, 14k-views on youtube, musical reference.)</em></p>
<h3><a class="anchor" href="https://ethresear.ch#h-2-enumeration-6" name="h-2-enumeration-6"></a>(2) – Enumeration</h3>
<p>Obligatory ‘stage-setting’ out of the way, let’s look a little more carefully at the ~essence~ of a block-space distribution mechanism.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/5/5/55cc9892e24d897856dff5e70b48fe8682903b6e.jpeg" title="upload_cdbf47258422c2a96ea2903ce113a113"><img alt="upload_cdbf47258422c2a96ea2903ce113a113" height="367" src="https://ethresear.ch/uploads/default/optimized/3X/5/5/55cc9892e24d897856dff5e70b48fe8682903b6e_2_552x367.jpeg" width="552" /></a></div><br />
<sub><strong>^ “<a href="https://youtu.be/mvy4YH9--Vw?t=108" rel="noopener nofollow ugc"><em>Is that what I think it is?</em></a>”</strong></sub><p></p>
<h4><a class="anchor" href="https://ethresear.ch#the-elementshttpsenwikipediaorgwikieuclid27s_elements-of-block-space-distribution-7" name="the-elementshttpsenwikipediaorgwikieuclid27s_elements-of-block-space-distribution-7"></a>The <a href="https://en.wikipedia.org/wiki/Euclid%27s_Elements" rel="noopener nofollow ugc">elements</a> of block-space distribution</h4>
<p>Consider the game of acquiring block space; MEV incentivizes agents to participate, while the combination of in-protocol and out-of-protocol software defines the rules. When designing this game, what elements should be considered? To answer this question, we use a familiar rhetorical pattern of “who, what, when, where, &amp; how” (hopefully <a href="https://ethresear.ch#h-1-motivation-3">Section 1</a> sufficiently answered “why”), which we refer to as the <code>W^4H questions</code>. (<span class="math">\leftarrow</span> h/t Barnabé pt. 2 for the connection to “<a href="https://www.goodreads.com/book/show/22749723-who-gets-what-and-why" rel="noopener nofollow ugc"><em>Who Gets What – and Why</em></a>”).</p>
<ul>
<li><em><strong>Who</strong> controls the outcome of the game?</em></li>
<li><em><strong>What</strong> is the good that players are competing for?</em></li>
<li><em><strong>When</strong> does the game take place?</em></li>
<li><em><strong>Where</strong> does the MEV oracle come from?</em></li>
<li><em><strong>How</strong> is the block builder chosen?</em></li>
</ul>
<p>These questions might seem overly simplistic, but when considered in isolation, each can be viewed as an axis in the design space to measure mechanisms. To demonstrate this, we highlight a few different species from the block-space distribution mechanism <a href="https://en.wikipedia.org/wiki/Genus" rel="noopener nofollow ugc">genus</a> that have been explored in the past. While they may feel disjointed and unrelated, their relationship is clarified by understanding how they answer the <code>W^4H questions</code>.</p>
<h4><a class="anchor" href="https://ethresear.ch#execution-tickets-and-other-animals-8" name="execution-tickets-and-other-animals-8"></a>Execution tickets and other animals</h4>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/0/5/05be4a7b2ce343036ba89733f9371dc1cbaa2b5b.jpeg" title="upload_23e73e8aae1d7223973af83053d41ebc"><img alt="upload_23e73e8aae1d7223973af83053d41ebc" height="500" src="https://ethresear.ch/uploads/default/optimized/3X/0/5/05be4a7b2ce343036ba89733f9371dc1cbaa2b5b_2_328x500.jpeg" width="328" /></a></div><br />
<sub><strong>^ fantastic book.</strong></sub><p></p>
<p>We present a compendium of many different proposed mechanisms. Note that this is only a subset of the rather substantial literature around these designs – cf. <a href="https://notes.ethereum.org/@mikeneuder/infinite-buffet" rel="noopener nofollow ugc">infinite buffet</a>. For each of the following, we summarize only the key ideas (see <a href="https://ethresear.ch#related-work-2">related work</a> for more).</p>
<ul>
<li>Execution tickets
<ul>
<li><strong>Key ideas</strong> – Block building and proposing rights are sold directly through “tickets” issued by the protocol. Ticket holders are randomly sampled to become block builders with a fixed lookahead. The ticket holder has the authority to produce a block at the assigned slot.</li>
</ul>
</li>
<li>Block-auction PBS
<ul>
<li><strong>Key ideas</strong> – The protocol bestows block production rights through a random leader-election process. The selected validator can sell their block outright to the builder market or build it locally. The builder must ~commit to a specific block~ when bidding in the auction. <code>mev-boost</code> is an out-of-protocol instantiation of block-auction PBS; enshrined PBS (ePBS), as <a href="https://ethresear.ch/t/two-slot-proposer-builder-separation/10980">originally presented</a>, is the in-protocol equivalent.</li>
</ul>
</li>
<li>MEV-burn/mev-smoothing
<ul>
<li><strong>Key ideas</strong> – A committee is tasked with enforcing a minimum value over the bid the proposer selects in an auction. By requiring the proposer to choose a “large enough” bid, an MEV oracle is created. The MEV is either <em>smoothed</em> between committee members or <em>burned</em> (smoothed over all <code>ETH</code> holders).</li>
</ul>
</li>
<li>Slot-auction PBS
<ul>
<li><strong>Key ideas</strong> – Similar to block-auction PBS but instead sells the <a href="https://mirror.xyz/0x03c29504CEcCa30B93FF5774183a1358D41fbeB1/CPYI91s98cp9zKFkanKs_qotYzw09kWvouaAa9GXBrQ" rel="noopener nofollow ugc">slot</a> to the builder market ~without~ requiring a commitment to a specific block – sometimes referred to as block space futures. By not requiring the builders to commit to a particular block, future slots may be auctioned off ahead of time rather than waiting until the slot itself.</li>
</ul>
</li>
<li>Partial-block auction
<ul>
<li><strong>Key ideas</strong> – Allows a more flexible unit for selling block-space. Instead of selling the full block or slot, allow proposers to sell <em>some</em> of their block, e.g., the top-of-block (which is the most valuable for arbitrageurs), while retaining the rest-of-block construction. Live in other Proof-of-Stake networks, e.g., Jito’s <a href="https://jito-labs.gitbook.io/mev/searcher-resources/block-engine" rel="noopener nofollow ugc">block engine</a> and Skip <a href="https://docs.skip.money/blocksdk/lanes/existing-lanes/mev" rel="noopener nofollow ugc">MEV lane</a>.</li>
</ul>
</li>
<li>APS-burn a.k.a. Execution Auction (nomenclature in flux &amp; the EA acronym has a bit of … <a href="https://en.wikipedia.org/wiki/Effective_altruism" rel="noopener nofollow ugc">baggage</a>)
<ul>
<li><strong>Key ideas</strong> – A brand new proposal from <a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ" rel="noopener nofollow ugc">Barnabé</a> which compels a proposer to auction off the block building and proposing rights ahead of time. The slot is sold ex-ante (a fixed amount of time in advance) without requiring a commitment to a specific block; a committee (à la mev-burn/smoothing) enforces the winning bid is sufficiently large.</li>
</ul>
</li>
</ul>
<p>We know, we know – it’s a lot to keep track of; it’s nearly a full-time job just to stay abreast of all these acronyms. But by comparing these proposals along the axes laid out by the <code>W^4H questions</code>, we can see how they all fit together as different parts of the same design space.</p>
<h4><a class="anchor" href="https://ethresear.ch#applying-w4h-a-comparative-analysis-9" name="applying-w4h-a-comparative-analysis-9"></a>Applying W^4H: a comparative analysis</h4>
<p>For each of the five <code>W^4H questions</code>, we describe different trade-offs made by the aforementioned proposals. For brevity, we don’t analyze each question for each proposal; we instead focus on highlighting key differences arising from each line of questioning.</p>
<ul>
<li><em><strong>Who</strong> controls the outcome of the game?</em>
<ul>
<li>With execution tickets, the protocol dictates the winner of the game by randomly choosing from the set of ticket holders.</li>
<li>With block-auction PBS, the proposer (protocol-elected leader) unilaterally chooses the winner of the game.</li>
<li>With mev-burn, the proposer still chooses the winner, but the winning bid is constrained by the committee, reducing the proposer’s agency.</li>
</ul>
</li>
<li><em><strong>What</strong> is the good that players are competing for?</em>
<ul>
<li>With block-auction PBS, the entire block is sold, but bids must commit to the block contents.</li>
<li>With slot-auction PBS, the entire block is sold, but without any specific block commitment.</li>
<li>With partial-block PBS, a portion of the block is sold.</li>
</ul>
</li>
<li><em><strong>When</strong> does the game take place?</em>
<ul>
<li>With block-auction PBS, the auction takes place during the slot.</li>
<li>With slot-auction PBS, the auction may take place many slots (e.g., 32) ahead of time because there is no block-content commitment.</li>
<li>With execution tickets, the tickets are assigned to slots at a fixed lookahead after being sold ex-ante by the protocol (more on the ticket-selling model we use below).</li>
</ul>
</li>
<li><em><strong>Where</strong> does the MEV oracle come from?</em>
<ul>
<li>With mev-burn/smoothing, a committee enforces that a sufficiently large bid is selected as the winner; this bid size is the oracle.</li>
<li>With execution tickets, the total money spent on tickets serves as the oracle.</li>
</ul>
</li>
<li><em><strong>How</strong> is the block builder chosen?</em>
<ul>
<li>In block-auction PBS, any outsourced block production has a winner-take-all allocation, with the highest bidder granted the block-building rights.</li>
<li>Within execution tickets, many different allocation mechanisms can be implemented. In the original proposal, for example, where a random ticket is selected, the mechanism is ‘proportional-to-ticket-count’; in this case, the highest paying bidder (whoever holds the most tickets) merely has the highest probability of being selected, meaning they are not guaranteed the block building rights.</li>
<li>If that (^) seems opaque, don’t worry. The entire following section is a deep dive into these different allocations.</li>
</ul>
</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#motivational-interlude-10" name="motivational-interlude-10"></a>Motivational interlude</h4>
<p>Before continuing, let’s review our original motivation for block-space distribution mechanisms:</p>
<blockquote>
<p><strong>Block-space distribution mechanisms aim to preserve the homogeneity of validator rewards in the presence of MEV.</strong></p>
</blockquote>
<p>This is a great grounding, but if that is our only goal, why not just continue using <code>mev-boost</code>? Well, remember that <code>mev-boost</code> has some negative side effects that we probably want the endgame protocol to be resilient against. We highlight four other potential design goals of a block-space distribution mechanism:</p>
<ol>
<li><em>Encouraging a wider set of builders to be competitive.</em></li>
<li><em>Allow validators and builders to interact trustlessly.</em></li>
<li><em>Incorporating MEV-awareness into the base layer protocol.</em></li>
<li><em>Removing MEV from validator rewards altogether.</em></li>
</ol>
<p>Note that while (1, 2, &amp; 3) appear relatively uncontroversial (*knock on wood*), (4) is more opinionated (and requires (3) as a pre-condition). The protocol may hope to eliminate MEV rewards from validator rewards as a means to ensure that the consensus layer rewards (what the protocol controls) more accurately reflect the full incentives of the system. This also ties into questions around staking macro-economics and the idea of <a href="https://ethresear.ch/t/endgame-staking-economics-a-case-for-targeting/18751">protocol</a>, <a href="https://notes.ethereum.org/@mikeneuder/iiii" rel="noopener nofollow ugc">issuance</a> – a much more politically-charged discussion. On the other hand, MEV rewards are a byproduct of network usage; MEV could instead be seen as a <a href="https://www.nano210.blog/infinite-blockspace-equilibrium/" rel="noopener nofollow ugc">value capture</a> mechanism for the native token. We aren’t trying to address these questions here but rather explore how different answers to them would shape the design of the mechanism.</p>
<p>What can we do at the protocol-design level to align with these desiderata? As laid out above, there are many trade-offs to consider, but in the following section, we examine “<em>How is the block builder chosen?</em>” to improve on some of these dimensions.</p>
<h3><a class="anchor" href="https://ethresear.ch#h-3-interrogation-11" name="h-3-interrogation-11"></a>(3) – Interrogation</h3>
<p><strong>Editorial note:</strong> As mentioned earlier, this section is longer and more technical than the others – feel free to skip to <a href="https://ethresear.ch#h-4-extrapolation-18">Section 4</a> if you are time (or interest) constrained!</p>
<p><strong>Section goal:</strong> <em>To demonstrate the quantitative trade-off between MEV-oracle quality and the “fairness” of the two most familiar approaches to allocating block proposer rights, which we call <code>Proportional-all-pay</code> and <code>Winner-take-all</code>.</em></p>
<p>We aim to accomplish this with the following subsections:</p>
<ul>
<li><a href="https://ethresear.ch#preliminaries-12"><em>Preliminaries</em></a> – Motivate the fixed-price, unlimited-quantity execution ticket sale mechanism we use.</li>
<li><a href="https://ethresear.ch#model-13"><em>Model</em></a> - Introduce the notation needed to analyze the model.</li>
<li><a href="https://ethresear.ch#familiar-allocation-mechanisms-14"><em>Familiar allocation mechanisms</em></a> - Describe the <code>Proportional-all-pay</code> and <code>Winner-take-all</code> mechanisms using the established framework.</li>
<li><a href="https://ethresear.ch#comparing-the-outcomes-15"><em>Comparing the outcomes</em></a> - Calculate the resulting equilibria in a two-player example.</li>
<li><a href="https://ethresear.ch#aside-1-calculating-equilibrium-bids-16"><em>Aside #1: Calculating equilibrium bids</em></a> - Derive the equilibria in the general case.</li>
<li><a href="https://ethresear.ch#aside-2-tullock-contests-17"><em>Aside #2: Tullock Contests</em></a> - Contextualize the model as a Tullock Contest and draw connections to the existing literature.</li>
</ul>
<p>Let’s <a href="https://youtu.be/GLsCR2RMBak?t=119" rel="noopener nofollow ugc">dig</a> in.</p>
<h4><a class="anchor" href="https://ethresear.ch#preliminaries-12" name="preliminaries-12"></a>Preliminaries</h4>
<p>Before diving into the space of allocation mechanisms made possible with execution tickets, we must first set up the model. Consider a protocol that sells execution tickets with the following rules:</p>
<ol>
<li>the price is fixed at <code>1 WEI</code>, and</li>
<li>unlimited tickets can be bought and sold from the protocol.</li>
</ol>
<p><strong>Note:</strong> <em>this version of execution tickets is effectively equivalent to creating two disjoint staking mechanisms – one each for attesting and proposing. Small changes in the design, e.g., not allowing tickets to be resold to the protocol, may have massive implications for how the market plays out, but that isn’t the focus of this article. Instead, we narrowly explore the question of block-space allocation, given an existing ticket holder set.</em></p>
<p>Notably, the set of block producers is disjoint (from the protocol’s perspective) from the set of attesters – individuals must select which part of the protocol they participate in by deciding whether to stake or buy tickets. The secondary ticket market may evolve as a venue for selling the building rights just in time to the builder market (as is done in <code>mev-boost</code> today).</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/e/4/e4f36ef0b6e6e5d80c4a923d9465bf74212af039.png" title="upload_45a1fa23182dd6a28c2c07dc2479f150"><img alt="upload_45a1fa23182dd6a28c2c07dc2479f150" height="499" src="https://ethresear.ch/uploads/default/optimized/3X/e/4/e4f36ef0b6e6e5d80c4a923d9465bf74212af039_2_486x499.png" width="486" /></a></div><br />
<span class="math">\cdot</span><br />
Separately, builders may choose to interact directly with the protocol by buying execution tickets themselves, but their capital may be better utilized as active liquidity, capturing arbitrage across trading venues. Thus, they may prefer buying block space on the secondary market during the just-in-time auction instead.<p></p>
<p>Why restrict ourselves to this posted-price-unlimited-supply mechanism? Two reasons:</p>
<ol>
<li><em>It’s not clear that a sophisticated market could even be implemented in the consensus layer.</em> The clients are optimized to allow any validator with consumer-grade hardware to participate in the network. This desideratum may be incompatible with fast auctions, bonding curves, or other possible ticket-selling mechanisms. Questions around how many tickets are sold, the MEV around onchain ticket-sale inclusion (meta-MEV?!), and the timing (and timing games) of ticket sales seem closer to execution layer concerns than something that could reasonably be implemented by Ethereum consensus while keeping hardware requirements limited.</li>
</ol>
<blockquote>
<p>“<em>One may imagine the inclusion of ET market-related transactions to possibly induce MEV, whether these transactions are included in the beacon block or the execution payload.</em>” – <strong>Barnabé in</strong> “<a href="https://mirror.xyz/barnabe.eth/QJ6W0mmyOwjec-2zuH6lZb0iEI2aYFB9gE-LHWIMzjQ" rel="noopener nofollow ugc"><em>More pictures about proposers and builders</em></a>.”</p>
</blockquote>
<ol start="2">
<li><em>Even if (a big if) the protocol ~could~ implement a more rigid ticket-selling market, the design space for such a mechanism is immense.</em> Many potential pricing mechanisms have been discussed, e.g., bonding curves, 1559-style dynamic pricing, auctions, etc.; making general claims about these remains outside the scope of this post.</li>
</ol>
<p>Therefore, we focus on the “unlimited, 1 <code>WEI</code> posted-price” version of execution tickets, where the protocol internalizes minimal complexity. With this framing, we can ask the question that is probably <a href="https://youtu.be/5KNEZJ6KkLI?t=53" rel="noopener nofollow ugc">burning</a> you up inside, “<em>given a set of execution ticket holders, how should the winner be selected?</em>” … sounds easy enough, right? Turns out there is a good deal we can say, even with such a seemingly simple question; let’s explore a few different options.</p>
<h4><a class="anchor" href="https://ethresear.ch#model-13" name="model-13"></a>Model</h4>
<p>Consider the repeated game of buying execution tickets to earn MEV rewards for your investment.</p>
<ul>
<li>During each period, each player effectively submits a bid, which is the number of tickets they buy. Denote the vector of bids by <span class="math">\mathbf{b}</span>, where <span class="math">b_i</span> is the bid of the <span class="math">i^{th}</span> player.</li>
<li>Each player has a valuation for winning the block production rights. Denote the vector of valuations by <span class="math">\mathbf{v}</span>, where <span class="math">v_i</span> is the value of the <span class="math">i^{th}</span> player.</li>
<li>At each time step, an allocation mechanism determines each player’s allocation based on the vector of bids. Assuming bidders are risk-neutral (i.e., don’t care between winning 2 <code>ETH</code> with probability <span class="math">0.5</span> vs. 1 <code>ETH</code> with probability <span class="math">1</span>), we can equivalently say that they are each allocated “some portion” of the block, which can be alternatively be interpreted as “the probability that they win a given block.” In an <span class="math">n</span> player game, let <span class="math">x: \mathbf{b} \rightarrow [0,1]^n</span> denote the map implementing an allocation mechanism, where <span class="math">x_i(\mathbf{b})</span> is the allocation of the <span class="math">i^{th}</span> player, under the constraint that <span class="math">\sum_i x_i(\mathbf{b}) =1</span> (i.e., the mechanism fully allocates).</li>
<li>Each player’s payment is collected at each round. Let <span class="math">p: \mathbf{b} \rightarrow \mathbb{R}_{\geq 0}^n</span> denote the payment rule determined by the set of bids, where <span class="math">p_i(\mathbf{b})</span> is the payment of the <span class="math">i^{th}</span> player.</li>
<li>The utility function of each player in the game is, <span class="math">U_i(\mathbf{b}) = v_i x_i(\mathbf{b}) - p_i(\mathbf{b})</span>. The intuition is that “a player’s utility is their value for winning multiplied by the amount they won, less their payment.”</li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#familiar-allocation-mechanisms-14" name="familiar-allocation-mechanisms-14"></a>Familiar allocation mechanisms</h4>
<p>Consider two (quite different) possible mechanisms.</p>
<p><code>Proportional-all-pay</code> (a slight modification to the <a href="https://ethresear.ch/t/execution-tickets/17944">original</a> execution tickets proposal)</p>
<ul>
<li>During each round, all players submit a bid. Denote the vector of bids by <span class="math">\mathbf{b}</span>.</li>
<li>The probability that a bid wins the game is the value of the bid divided by the sum of all the values of the bids,</li>
</ul>
<div class="math">
x_i(\mathbf{b}) = \frac{b_i}{\sum_j b_j}.
</div>
<ul>
<li>Each player pays their bid, no matter the outcome of the game (hence “all-pay”), <span class="math">p_i(\mathbf{b}) = b_i.</span><a href="https://ethresear.ch#fn1dst"><span class="math">^{[1]}</span></a><a href="https://ethresear.ch" name="fn1"></a></li>
</ul>
<p><code>Winner-take-all</code> (the current implementation of PBS)</p>
<ul>
<li>During each round, all players submit a bid. Denote the vector of bids by <span class="math">\mathbf{b}</span>.</li>
<li>The highest bidder wins the game, so <span class="math">x_i(\mathbf{b}) = 1</span> if <span class="math">\max(\mathbf{b}) = b_i</span> and <span class="math">x_i(\mathbf{b}) = 0</span> otherwise (where ties are broken in favor of the lower index bidder, say).</li>
<li>Only the winning player pays the value of their bid, so <span class="math">p_i(\mathbf{b}) = b_i</span> if <span class="math">\max(\mathbf{b}) = b_i</span> and <span class="math">p_i(\mathbf{b}) = 0</span> otherwise (same tie-breaking as above).<a href="https://ethresear.ch#fn2dst"><span class="math">^{[2]}</span></a><a href="https://ethresear.ch" name="fn2"></a></li>
</ul>
<h4><a class="anchor" href="https://ethresear.ch#comparing-the-outcomes-15" name="comparing-the-outcomes-15"></a>Comparing the outcomes</h4>
<p>To demonstrate the different outcomes from these two mechanisms, consider the two-player game where <code>Player 1</code> has a valuation of <span class="math">v_1 = 4</span> and <code>Player 2</code> has a valuation of <span class="math">v_2 = 2.</span> (We consider a complete information setting in which the individual values are common knowledge. To see how the equilibria bid is calculated and for extended discussion, see <a href="https://ethresear.ch#aside-1-calculating-equilibrium-bids-16">Aside 1</a>.)</p>
<ul>
<li><strong><code>Proportional-all-pay</code> outcome:</strong>
<ul>
<li>Equilibrium Bids: <span class="math">\qquad\,\,\,\;\;\; b_1 = 8/9</span>, <span class="math">\,b_2 = 4/9</span></li>
<li>Equilibrium Allocations: <span class="math">\;\;\; x_1 = 2/3</span>, <span class="math">x_2 = 1/3</span></li>
<li>Equilibrium Payments: <span class="math">\;\;\;\; p_1 = 8/9</span>, <span class="math">\,p_2 = 4/9</span></li>
</ul>
</li>
</ul>
<p>This all should feel intuitively correct; with <span class="math">v_1 = 2 \cdot v_2</span> (<code>Player 1</code> has <code>2x</code> the value for the block), <code>Player 1</code> bids, receives and pays twice as much as <code>Player 2</code>.</p>
<ul>
<li><strong><code>Winner-take-all</code> outcome:</strong>
<ul>
<li>Equilibrium Bids: <span class="math">\qquad\,\,\,\;\;\; b_1 = 2+\epsilon</span>, <span class="math">b_2 = 2</span></li>
<li>Equilibrium Allocations: <span class="math">\;\;\; x_1 = 1</span>, <span class="math">\quad\;\; x_2 = 0</span></li>
<li>Equilibrium Payments: <span class="math">\;\;\;\,\, p_1 = 2+\epsilon</span>, <span class="math">p_2 = 0</span></li>
</ul>
</li>
</ul>
<p>This is pretty different. <code>Player 1</code> bids and pays just over <code>Player 2</code>’s value (we use <span class="math">\epsilon</span> to denote a small amount), receiving the entire allocation. <code>Player 2</code> receives nothing and pays nothing.<a href="https://ethresear.ch#fn3dst"><span class="math">^{[3]}</span></a><a href="https://ethresear.ch" name="fn3"></a></p>
<p>Now consider the “revenue” (or the sum of the bids collected by the mechanism) generated from each case:</p>
<ul>
<li><strong><code>Proportional-all-pay</code> revenue:</strong> <span class="math">b_1 + b_2 = 4/3</span></li>
<li><strong><code>Winner-take-all</code> revenue:</strong> <span class="math">\qquad\quad\,\,\,\;\;\;\; b_1 = 2+\epsilon</span></li>
</ul>
<p><code>Winner-take-all</code> has better revenue, corresponding to a more accurate MEV oracle (and thus more MEV burned or smoothed by the protocol) than <code>Proportional-all-pay</code>. Intuitively, by allocating block-production rights to players with lower values (as <code>Proportional-all-pay</code> does), we forgo revenue we would have received had we simply allocated the entire rights to the player with the highest value. We point the interested reader to <a href="https://ethresear.ch#aside-1-calculating-equilibrium-bids-16">Aside 1</a> for a more complete treatment.</p>
<p>Another factor to consider is the “fairness” or “distribution” of the allocation mechanism. For example, suppose we agree on the metric: <span class="math">\text{fairness} = \sqrt{x_1 \cdot x_2}</span> (we use the geometric mean because if <span class="math">x_1 + x_2</span> has a fixed sum, the geometric mean is maximized at <span class="math">x_1 = x_2</span> and zero if either <span class="math">x_1,x_2</span> is zero). Now, let’s look at the fairness outcomes of the two candidate mechanisms:</p>
<ul>
<li><strong><code>Proportional-all-pay</code> fairness:</strong> <span class="math">\sqrt{1/3 \cdot 2/3} \approx 0.471</span></li>
<li><strong><code>Winner-take-all</code> fairness:</strong> <span class="math">\qquad\qquad\;\,\;\sqrt{1 \cdot 0} = 0</span></li>
</ul>
<p>Here, the “performance” of the two mechanisms flips – the <code>Winner-take-all</code> is <em>less fair</em> because <code>Player 2</code> has no chance of winning the game with a lower value. In the <code>Proportional-all-pay</code>, <code>Player 2</code> can hope to win some blocks despite bidding a lower value. As another example, consider the case where <span class="math">v_1=v_2+\epsilon</span>. The <code>Winner-take-all</code> mechanism allocates all the rights to <code>Player 1</code>, while the <code>Proportional-all-pay</code> splits the rights approximately in half.</p>
<blockquote>
<p>Brief note: why might the protocol care about fairness? In a decentralized protocol, a single actor having too much power undermines the credible neutrality of the system. As such, the protocol may be willing to “pay” (in the form of reduced revenue) to ensure that a resource is more evenly distributed among players. Alternatively, we could consider this a measure of “entropy” or even simply randomness being injected into the outcome of the game to try to reduce the influence the most dominant player can have.</p>
</blockquote>
<p>This leads to the punchline from this small example: <strong>a fundamental trade-off exists between MEV-oracle quality and fairness.</strong> The <code>Proportional-all-pay</code> mechanism (and hence the original execution tickets proposal) is fairer because both players win the game with some probability, incentivizing them each (but more importantly, the higher value player) to <a href="https://en.wikipedia.org/wiki/Bid_shading" rel="noopener nofollow ugc">shade</a> their bid accordingly, lowering the revenue, and thus the MEV-oracle accuracy, of the mechanism. The first price mechanism elicits higher bids since bidders only pay if they win the entire block production rights, increasing the revenue, but this <code>Winner-take-all</code> dynamic makes the allocation less fair.</p>
<p><em>Open question: is <code>Proportional-all-pay</code> an “optimal” Sybil-proof mechanism?</em> In the permissionless setting, we only consider Sybil-proof mechanisms, where a player doesn’t benefit from splitting their bid into multiple identities. We posit that the <code>Proportional-all-pay</code> mechanism sits in the <a href="https://en.wikipedia.org/wiki/Habitable_zone" rel="noopener nofollow ugc">Goldilock’s Zone</a> of a Sybil-proof mechanism that gets both good revenue/MEV-oracle accuracy and fairness. We leave as an interesting open problem to determine the extent to which the <code>Proportional-all-pay</code> mechanism’s “optimality” (e.g., we were unable to find another Sybil-proof mechanism that dominates it in both revenue and fairness).</p>
<h4><a class="anchor" href="https://ethresear.ch#aside-1-calculating-equilibrium-bids-16" name="aside-1-calculating-equilibrium-bids-16"></a>Aside <span class="hashtag-raw">#1</span> – Calculating equilibrium bids</h4>
<p><a href="https://ethresear.ch#h-4-extrapolation-18">Convenience link</a> to skip to the conclusion for the less-keen reader <img alt=":wink:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/wink.png?v=12" title=":wink:" width="20" /></p>
<p>In the numerical example above, we provide the equilibrium bids for the <code>Winner-take-all</code> and <code>Proportional-all-pay</code> mechanisms without proof. How can these be determined generally (e.g., continuing to assume that bidders’ values are common knowledge)?<a href="https://ethresear.ch#fn4dst"><span class="math">^{[4]}</span></a><a href="https://ethresear.ch" name="fn4"></a></p>
<p>The <code>Winner-take-all</code> is the familiar <a href="https://www.econport.org/econport/request?page=man_auctions_firstpricesealed" rel="noopener nofollow ugc">First Price Auction</a> setting. In such auctions, the complete information Pure-Nash equilibrium has the two highest-value bidders, each bidding the second-highest bidder’s value, with every other agent bidding below this. In effect, we expect that the highest-value bidder always wins while paying the second highest bidder’s value (we represent this simply as <span class="math">b_1=b_2+\epsilon</span>, though you could equivalently tie-break in favor of the higher-value player).</p>
<p>In the <code>Proportional-all-pay</code> setting, each player has the utility,</p>
<div class="math">
\begin{align}
U_i (\mathbf{b}) &amp;= v_i \cdot x_i(\mathbf{b}) - b_i \\
&amp;= v_i \cdot \frac{b_i}{\sum_j b_j} - b_i.
\end{align}
</div>
<p>To determine the existence of a <a href="https://en.wikipedia.org/wiki/Nash_equilibrium" rel="noopener nofollow ugc">Pure Nash Equilibrium</a>, we consider each player’s first- and second-order conditions. Let <span class="math">\mathbf{b}^*</span> denote the candidate equilibrium set of bids.</p>
<ol>
<li><strong>First-order condition</strong>: <span class="math">\partial U_i / \partial b_i (\mathbf{b^*}) = 0</span> (or <span class="math">\partial U_i / \partial b_i (\mathbf{b^*}) \leq 0, \;\forall i \text{ s.t. } b^*_i=0</span>.)
<ul>
<li>Intuitively, this condition checks a non-zero-bidding player is (to first order) locally indifferent to small changes in its bid.</li>
</ul>
</li>
<li><strong>Second-order condition</strong>: <span class="math">\partial^2 U_i / \partial b_i^2 &lt; 0</span>
<ul>
<li>Intuitively, this condition ensures that the utility function is concave, implying that locally best responses are globally best for all players.</li>
</ul>
</li>
</ol>
<p>In our simple two-player example in the <code>Proportional-all-pay</code> setting, we have the following.</p>
<div class="math">
\begin{align}
\frac{\partial U_1}{\partial b_1}(\mathbf{b}) = \frac{v_1 b_2}{(b_1 + b_2)^2} - 1 = 0 \; , \quad \frac{\partial U_2}{\partial b_2}(\mathbf{b}) = \frac{v_2 b_1}{(b_1 + b_2)^2} - 1 = 0
\end{align}
</div>
<p>This system can be solved to find the equilibrium bids, <span class="math">\mathbf{b}^*</span>,</p>
<div class="math">
\begin{align}
b^*_1 = \frac{v_1^2 v_2}{(v_1 + v_2)^2}\; , \quad b^*_2 = \frac{v_2^2 v_1}{(v_1 + v_2)^2}.
\end{align}
</div>
<p>For our toy example, we have <span class="math">v_1=4, \; v_2=2 \implies b_1^* = 32/36, \; b_2^* = 16/36</span>. We can verify our first-order conditions</p>
<div class="math">
\begin{align}
\frac{4 \cdot 16/36}{16/9} - 1 = 0 \; , \quad \frac{2 \cdot 32/36}{16/9} - 1 = 0 \quad \checkmark
\end{align}
</div>
<p>The second-order conditions can also be verified – this is left as an exercise for the reader <img alt=":wink:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/wink.png?v=12" title=":wink:" width="20" /></p>
<h4><a class="anchor" href="https://ethresear.ch#aside-2-tullock-contests-17" name="aside-2-tullock-contests-17"></a>Aside <span class="hashtag-raw">#2</span> – Tullock Contests</h4>
<p><a href="https://youtu.be/lL2ZwXj1tXM?t=60" rel="noopener nofollow ugc">Last chance</a> to <a href="https://ethresear.ch#h-4-extrapolation-18">skip to the conclusion</a>. (If you continue, by definition, you are the “interested reader” – <a href="https://www.youtube.com/watch?v=SC4xMk98Pdc&amp;t=35s" rel="noopener nofollow ugc">congrats</a>.)</p>
<p>The model described above is established in the algorithmic game theory literature as a <a href="https://www.chapman.edu/ESI/wp/GeneralizedTullockContest-Sheremeta.pdf" rel="noopener nofollow ugc">Tullock Contest</a> – named for Gordon Tullock, who explored the idea in his seminal work, “<a href="https://link.springer.com/chapter/10.1007/978-3-540-79182-9_6" rel="noopener nofollow ugc"><em>Efficient Rent Seeking</em></a>.” He motivates this study by considering situations where investment is made before the outcome is known and where the investments might not transfer easily between participants, e.g., political spending.</p>
<blockquote>
<p>“<em>Suppose, for example, that we organize a lobby in Washington for the purpose of raising the price of milk and are unsuccessful. We cannot simply transfer our collection of contacts, influences, past bribes, and so forth to the steel manufacturers’ lobby. In general, our investments are too specialized, and, in many cases, they are matters of very particular and detailed goodwill to a specific organization. It is true that we could sell the steel lobby our lobbyists with their connections and perhaps our mailing list. But presumably, all these things have been bought by us at their proper cost. Our investment has not paid, but there is nothing left to transfer.</em>” – <strong>Gordon Tullock (1980)</strong></p>
</blockquote>
<p>This allocation mechanism has been applied in the previous crypto literature as well. Back in 2018 (ancient history in crypto-terms), Arnosti and Weinberg wrote “<a href="https://arxiv.org/abs/1811.08572" rel="noopener nofollow ugc"><em>Bitcoin: A natural oligopoly</em></a>,” which demonstrates that even small operating cost advantages among miners in a Proof-of-Work system lead to surprisingly concentrated equilibria. Similarly, Bahrani, Garimidi, and Roughgarden (these names sound familiar :D) explored the centralization effects of heterogeneity in block building skill in “<a href="https://arxiv.org/abs/2401.12120" rel="noopener nofollow ugc"><em>Centralization in Block Building and Proposer-Builder Separation</em></a>.” There appears to be a deep relationship between permissionless crypto-economic systems, where anti-Sybil mechanisms typically require financial investment for participation, and Tullock Contests – more on this <code>Soon™</code> (maybe).</p>
<h3><a class="anchor" href="https://ethresear.ch#h-4-extrapolation-18" name="h-4-extrapolation-18"></a>(4) – Extrapolation</h3>
<p>Phew, thanks for hanging in there; let’s take stock of what we learned. <strong><a href="https://ethresear.ch#h-3-interrogation-11">Section 3</a> demonstrates the fundamental trade-off between MEV-oracle accuracy and fairness of an instantiation of an execution ticket mechanism.</strong> A protocol may be willing to *pay* (in the form of reduced revenue) for more distribution and entropy with the goal of improving and maintaining the protocol’s credible neutrality. Further, using the model to derive equilibrium bids helps inform how we may expect agents to respond to various allocation and payment rules. <a href="https://youtu.be/Hm3JodBR-vs?t=21" rel="noopener nofollow ugc">Neat</a> – our framework led to some interesting and hopefully helpful insights! Maybe we can extend it to other problems in the space as well?</p>
<p>Further questions that this specific model may help answer (returning to three of our <code>W^4 questions</code>):</p>
<ul>
<li><em><strong>What</strong> is the good that players are competing for?</em>
<ul>
<li>Can we extend the model dimensionality, allowing different players to have different values for portions of the block (e.g., an arbitrageur may disproportionately value the top of a block but have zero value for the remainder)?</li>
</ul>
</li>
<li><em><strong>When</strong> does the game take place?</em>
<ul>
<li>How does the MEV-oracle accuracy change if the game takes place far ahead of time versus during the slot itself (e.g., pricing future expected MEV versus present realizable MEV)?</li>
</ul>
</li>
<li><em><strong>How</strong> is the block builder chosen?</em>
<ul>
<li>Are there other Sybil-proof mechanisms that dominate <code>Proportional-all-pay</code> in both revenue and fairness?</li>
<li>Can we more formally characterize the fundamental trade-offs between revenue and fairness?</li>
<li>Given the Sybil-proofness constraint, what alternative allocation and payment rules should be explored (e.g., Tullock contests where the allocation rule is parameterized by <span class="math">\alpha&gt;1</span> where <span class="math">x_i = b_i^\alpha / \sum_j b_j^\alpha</span>), and can we identify the optimal choice?</li>
</ul>
</li>
</ul>
<p>Zooming back out, other versions of the <code>W^4H questions</code> may require different models to reason about.</p>
<ul>
<li><em><strong>Who</strong> controls the outcome of the game?</em>
<ul>
<li>In the committee-enforced version of these mechanisms, how could collusive behavior emerge?</li>
<li>If the just-in-time block auction continues to take place out-of-protocol, should we explicitly describe the secondary market?</li>
</ul>
</li>
<li><em><strong>When</strong> does the game take place?</em>
<ul>
<li>How critical is network latency when considering lookahead block-space sales versus same-slot? Is it worth modeling the <a href="https://dl.acm.org/doi/pdf/10.1145/42282.42283" rel="noopener nofollow ugc">partially-synchronous</a> setting?</li>
<li>How do block builder valuations change if multi-slot MEV is feasible?</li>
</ul>
</li>
<li><em><strong>Where</strong> does the MEV oracle come from?</em>
<ul>
<li>If it comes from the committee, are there incentives for committee members to behave dishonestly?</li>
<li>Do such incentives depend on whether protocol-captured MEV is burned or smoothed?</li>
</ul>
</li>
</ul>
<p>As per usual, open questions abound, but we hope (a) <code>W^4H questions</code> help expand the understanding of block-space distribution mechanisms and (b) the deep dive into allocation mechanisms helps inform the potential design space of execution tickets.</p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://ethresear.ch/uploads/default/original/3X/8/f/8f4ceb270ae099c612cbb2afb4958a9bea1b42d1.jpeg" title="upload_a24cdf5b513fb4e410700573687adcd6"><img alt="upload_a24cdf5b513fb4e410700573687adcd6" height="493" src="https://ethresear.ch/uploads/default/optimized/3X/8/f/8f4ceb270ae099c612cbb2afb4958a9bea1b42d1_2_690x493.jpeg" width="690" /></a></div><br />
<sub><strong>^ <a href="https://youtu.be/WSLMN6g_Od4?t=92" rel="noopener nofollow ugc">The world once we figure out MEV.</a></strong></sub><p></p>
<p>Excited to be here with y’all.</p>
<p><em>— made with <img alt=":heart:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/heart.png?v=12" title=":heart:" width="20" /> by mike, pranav, &amp; dr. tim roughgarden.</em></p>
<hr />
<h3><a class="anchor" href="https://ethresear.ch#footnotes-19" name="footnotes-19"></a>footnotes</h3>
<p><span class="math">^{[1]}</span><a href="https://ethresear.ch" name="fn1dst"></a>: The “all-pay” feature is made possible by burning the price paid for each ticket. <a href="https://ethresear.ch#fn1"><img alt=":leftwards_arrow_with_hook:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/leftwards_arrow_with_hook.png?v=12" title=":leftwards_arrow_with_hook:" width="20" />︎</a></p>
<p><span class="math">^{[2]}</span><a href="https://ethresear.ch" name="fn2dst"></a>: The “winner-pay” version could be done by refunding all non-winning ticket holders their payment at the end of each round. <a href="https://ethresear.ch#fn2"><img alt=":leftwards_arrow_with_hook:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/leftwards_arrow_with_hook.png?v=12" title=":leftwards_arrow_with_hook:" width="20" />︎</a></p>
<p><span class="math">^{[3]}</span><a href="https://ethresear.ch" name="fn3dst"></a>: As mentioned earlier, simply refunding the non-winning tickets instantiates the “winner-pays” property. <a href="https://ethresear.ch#fn3"><img alt=":leftwards_arrow_with_hook:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/leftwards_arrow_with_hook.png?v=12" title=":leftwards_arrow_with_hook:" width="20" />︎</a></p>
<p><span class="math">^{[4]}</span><a href="https://ethresear.ch" name="fn4dst"></a>: This is primarily for tractability in calculating equilibria analytically. Although a strong assumption, it’s not unreasonable in the context of lookahead auctions where bidders might have established prior distributions on their competitor’s valuations. We also view the insights from studying the complete-information equilibria as valuable heuristics for how we may expect these mechanisms to behave in practice. <a href="https://ethresear.ch#fn4"><img alt=":leftwards_arrow_with_hook:" class="emoji" height="20" src="https://ethresear.ch/images/emoji/facebook_messenger/leftwards_arrow_with_hook.png?v=12" title=":leftwards_arrow_with_hook:" width="20" />︎</a></p>
            <p><small>3 posts - 2 participants</small></p>
            <p><a href="https://ethresear.ch/t/on-block-space-distribution-mechanisms/19764">Read full topic</a></p>
]]></content:encoded>
<pubDate>Sat, 08 Jun 2024 12:42:54 +0000</pubDate>
</item>
</channel>
</rss>